def get_vid_from_url ( url ) : return match1 ( url , r'youtu\.be/([^?/]+)' ) or match1 ( url , r'youtube\.com/embed/([^/?]+)' ) or match1 ( url , r'youtube\.com/v/([^/?]+)' ) or match1 ( url , r'youtube\.com/watch/([^/?]+)' ) or parse_query_param ( url , 'v' ) or parse_query_param ( parse_query_param ( url , 'u' ) , 'v' )
def dailymotion_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : html = get_content ( rebuilt_url ( url ) ) info = json . loads ( match1 ( html , r'qualities":({.+?}),"' ) ) title = match1 ( html , r'"video_title"\s*:\s*"([^"]+)"' ) or match1 ( html , r'"title"\s*:\s*"([^"]+)"' ) title = unicodize ( title ) for quality in [ '1080' , '720' , '480' , '380' , '240' , '144' , 'auto' ] : try : real_url = info [ quality ] [ 1 ] [ "url" ] if real_url : break except KeyError : pass mime , ext , size = url_info ( real_url ) print_info ( site_info , title , mime , size ) if not info_only : download_urls ( [ real_url ] , title , ext , size , output_dir = output_dir , merge = merge )
def sina_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : if 'news.sina.com.cn/zxt' in url : sina_zxt ( url , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs ) return vid = match1 ( url , r'vid=(\d+)' ) if vid is None : video_page = get_content ( url ) vid = hd_vid = match1 ( video_page , r'hd_vid\s*:\s*\'([^\']+)\'' ) if hd_vid == '0' : vids = match1 ( video_page , r'[^\w]vid\s*:\s*\'([^\']+)\'' ) . split ( '|' ) vid = vids [ - 1 ] if vid is None : vid = match1 ( video_page , r'vid:"?(\d+)"?' ) if vid : #title = match1(video_page, r'title\s*:\s*\'([^\']+)\'') sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) else : vkey = match1 ( video_page , r'vkey\s*:\s*"([^"]+)"' ) if vkey is None : vid = match1 ( url , r'#(\d+)' ) sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) return title = match1 ( video_page , r'title\s*:\s*"([^"]+)"' ) sina_download_by_vkey ( vkey , title = title , output_dir = output_dir , merge = merge , info_only = info_only )
def get_vid_from_url ( self , url ) : hit = re . search ( r'live.qq.com/(\d+)' , url ) if hit is not None : return hit . group ( 1 ) hit = re . search ( r'live.qq.com/directory/match/(\d+)' , url ) if hit is not None : return self . get_room_id_from_url ( hit . group ( 1 ) ) html = get_content ( url ) room_id = match1 ( html , r'room_id\":(\d+)' ) if room_id is None : log . wtf ( 'Unknown page {}' . format ( url ) ) return room_id
def sprint ( text , * colors ) : return "\33[{}m{content}\33[{}m" . format ( ";" . join ( [ str ( color ) for color in colors ] ) , RESET , content = text ) if IS_ANSI_TERMINAL and colors else text
def print_log ( text , * colors ) : sys . stderr . write ( sprint ( "{}: {}" . format ( script_name , text ) , * colors ) + "\n" )
def e ( message , exit_code = None ) : print_log ( message , YELLOW , BOLD ) if exit_code is not None : sys . exit ( exit_code )
def wtf ( message , exit_code = 1 ) : print_log ( message , RED , BOLD ) if exit_code is not None : sys . exit ( exit_code )
def detect_os ( ) : syst = system ( ) . lower ( ) os = 'unknown' if 'cygwin' in syst : os = 'cygwin' elif 'darwin' in syst : os = 'mac' elif 'linux' in syst : os = 'linux' try : with open ( '/proc/version' , 'r' ) as f : if 'microsoft' in f . read ( ) . lower ( ) : os = 'wsl' except : pass elif 'windows' in syst : os = 'windows' elif 'bsd' in syst : os = 'bsd' return os
def get_vid_from_url ( url ) : vid = match1 ( url , 'https?://www.mgtv.com/(?:b|l)/\d+/(\d+).html' ) if not vid : vid = match1 ( url , 'https?://www.mgtv.com/hz/bdpz/\d+/(\d+).html' ) return vid
def legitimize ( text , os = detect_os ( ) ) : text = text . translate ( { 0 : None , ord ( '/' ) : '-' , ord ( '|' ) : '-' , } ) if os == 'windows' or os == 'cygwin' or os == 'wsl' : text = text . translate ( { ord ( ':' ) : '-' , ord ( '*' ) : '-' , ord ( '?' ) : '-' , ord ( '\\' ) : '-' , ord ( '\"' ) : '\'' , ord ( '+' ) : '-' , ord ( '<' ) : '-' , ord ( '>' ) : '-' , ord ( '[' ) : '(' , ord ( ']' ) : ')' , ord ( '\t' ) : ' ' , } ) else : if os == 'mac' : text = text . translate ( { ord ( ':' ) : '-' , } ) if text . startswith ( "." ) : text = text [ 1 : ] text = text [ : 80 ] return text
def cbs_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : html = get_content ( url ) pid = match1 ( html , r'video\.settings\.pid\s*=\s*\'([^\']+)\'' ) title = match1 ( html , r'video\.settings\.title\s*=\s*\"([^\"]+)\"' ) theplatform_download_by_pid ( pid , title , output_dir = output_dir , merge = merge , info_only = info_only )
def parse_host ( host ) : if re . match ( r'^(\d+)$' , host ) is not None : return ( "0.0.0.0" , int ( host ) ) if re . match ( r'^(\w+)://' , host ) is None : host = "//" + host o = parse . urlparse ( host ) hostname = o . hostname or "0.0.0.0" port = o . port or 0 return ( hostname , port )
def get_conn ( self ) : conn_config = self . _get_conn_params ( ) conn = snowflake . connector . connect ( * * conn_config ) return conn
def bulk_load ( self , table , tmp_file ) : self . copy_expert ( "COPY {table} FROM STDIN" . format ( table = table ) , tmp_file )
def bulk_dump ( self , table , tmp_file ) : self . copy_expert ( "COPY {table} TO STDOUT" . format ( table = table ) , tmp_file )
def execute ( self , context ) : hook = GoogleCloudStorageHook ( google_cloud_storage_conn_id = self . google_cloud_storage_conn_id , delegate_to = self . delegate_to ) hook . upload ( bucket_name = self . bucket , object_name = self . dst , mime_type = self . mime_type , filename = self . src , gzip = self . gzip , )
def get_conn ( self ) : conn = self . get_connection ( self . mysql_conn_id ) conn_config = { "user" : conn . login , "passwd" : conn . password or '' , "host" : conn . host or 'localhost' , "db" : self . schema or conn . schema or '' } if not conn . port : conn_config [ "port" ] = 3306 else : conn_config [ "port" ] = int ( conn . port ) if conn . extra_dejson . get ( 'charset' , False ) : conn_config [ "charset" ] = conn . extra_dejson [ "charset" ] if ( conn_config [ "charset" ] ) . lower ( ) == 'utf8' or ( conn_config [ "charset" ] ) . lower ( ) == 'utf-8' : conn_config [ "use_unicode" ] = True if conn . extra_dejson . get ( 'cursor' , False ) : if ( conn . extra_dejson [ "cursor" ] ) . lower ( ) == 'sscursor' : conn_config [ "cursorclass" ] = MySQLdb . cursors . SSCursor elif ( conn . extra_dejson [ "cursor" ] ) . lower ( ) == 'dictcursor' : conn_config [ "cursorclass" ] = MySQLdb . cursors . DictCursor elif ( conn . extra_dejson [ "cursor" ] ) . lower ( ) == 'ssdictcursor' : conn_config [ "cursorclass" ] = MySQLdb . cursors . SSDictCursor local_infile = conn . extra_dejson . get ( 'local_infile' , False ) if conn . extra_dejson . get ( 'ssl' , False ) : dejson_ssl = conn . extra_dejson [ 'ssl' ] if isinstance ( dejson_ssl , six . string_types ) : dejson_ssl = json . loads ( dejson_ssl ) conn_config [ 'ssl' ] = dejson_ssl if conn . extra_dejson . get ( 'unix_socket' ) : conn_config [ 'unix_socket' ] = conn . extra_dejson [ 'unix_socket' ] if local_infile : conn_config [ "local_infile" ] = 1 conn = MySQLdb . connect ( * * conn_config ) return conn
def bulk_load ( self , table , tmp_file ) : conn = self . get_conn ( ) cur = conn . cursor ( ) cur . execute ( . format ( tmp_file = tmp_file , table = table ) ) conn . commit ( )
def get_proxy_version ( self ) : self . _download_sql_proxy_if_needed ( ) command_to_run = [ self . sql_proxy_path ] command_to_run . extend ( [ '--version' ] ) command_to_run . extend ( self . _get_credential_parameters ( ) ) result = subprocess . check_output ( command_to_run ) . decode ( 'utf-8' ) pattern = re . compile ( "^.*[V|v]ersion ([^;]*);.*$" ) m = pattern . match ( result ) if m : return m . group ( 1 ) else : return None
def cleanup_database_hook ( self ) : if self . database_type == 'postgres' : if hasattr ( self . db_hook , 'conn' ) and self . db_hook . conn and self . db_hook . conn . notices : for output in self . db_hook . conn . notices : self . log . info ( output )
def reserve_free_tcp_port ( self ) : self . reserved_tcp_socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) self . reserved_tcp_socket . bind ( ( '127.0.0.1' , 0 ) ) self . sql_proxy_tcp_port = self . reserved_tcp_socket . getsockname ( ) [ 1 ]
def _get_error_code ( self , e ) : try : matches = self . error_code_pattern . match ( str ( e ) ) code = int ( matches . group ( 0 ) ) return code except ValueError : return e
def _integrate_plugins ( ) : import sys from airflow . plugins_manager import sensors_modules for sensors_module in sensors_modules : sys . modules [ sensors_module . __name__ ] = sensors_module globals ( ) [ sensors_module . _name ] = sensors_module
def clear_dag_runs ( ) : session = settings . Session ( ) drs = session . query ( DagRun ) . filter ( DagRun . dag_id . in_ ( DAG_IDS ) , ) . all ( ) for dr in drs : logging . info ( 'Deleting DagRun :: {}' . format ( dr ) ) session . delete ( dr )
def clear_dag_task_instances ( ) : session = settings . Session ( ) TI = TaskInstance tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) for ti in tis : logging . info ( 'Deleting TaskInstance :: {}' . format ( ti ) ) session . delete ( ti ) session . commit ( )
def set_dags_paused_state ( is_paused ) : session = settings . Session ( ) dms = session . query ( DagModel ) . filter ( DagModel . dag_id . in_ ( DAG_IDS ) ) for dm in dms : logging . info ( 'Setting DAG :: {} is_paused={}' . format ( dm , is_paused ) ) dm . is_paused = is_paused session . commit ( )
def print_stats ( self ) : session = settings . Session ( ) TI = TaskInstance tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) successful_tis = [ x for x in tis if x . state == State . SUCCESS ] ti_perf = [ ( ti . dag_id , ti . task_id , ti . execution_date , ( ti . queued_dttm - self . start_date ) . total_seconds ( ) , ( ti . start_date - self . start_date ) . total_seconds ( ) , ( ti . end_date - self . start_date ) . total_seconds ( ) , ti . duration ) for ti in successful_tis ] ti_perf_df = pd . DataFrame ( ti_perf , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'queue_delay' , 'start_delay' , 'land_time' , 'duration' ] ) print ( 'Performance Results' ) print ( '###################' ) for dag_id in DAG_IDS : print ( 'DAG {}' . format ( dag_id ) ) print ( ti_perf_df [ ti_perf_df [ 'dag_id' ] == dag_id ] ) print ( '###################' ) if len ( tis ) > len ( successful_tis ) : print ( "WARNING!! The following task instances haven't completed" ) print ( pd . DataFrame ( [ ( ti . dag_id , ti . task_id , ti . execution_date , ti . state ) for ti in filter ( lambda x : x . state != State . SUCCESS , tis ) ] , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'state' ] ) ) session . commit ( )
def heartbeat ( self ) : super ( SchedulerMetricsJob , self ) . heartbeat ( ) session = settings . Session ( ) TI = TaskInstance successful_tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . filter ( TI . state . in_ ( [ State . SUCCESS ] ) ) . all ( ) ) session . commit ( ) dagbag = DagBag ( SUBDIR ) dags = [ dagbag . dags [ dag_id ] for dag_id in DAG_IDS ] num_task_instances = sum ( [ ( timezone . utcnow ( ) - task . start_date ) . days for dag in dags for task in dag . tasks ] ) if ( len ( successful_tis ) == num_task_instances or ( timezone . utcnow ( ) - self . start_date ) . total_seconds ( ) > MAX_RUNTIME_SECS ) : if len ( successful_tis ) == num_task_instances : self . log . info ( "All tasks processed! Printing stats." ) else : self . log . info ( "Test timeout reached. Printing available stats." ) self . print_stats ( ) set_dags_paused_state ( True ) sys . exit ( )
def get_dag_run_state ( dag_id , execution_date ) : dagbag = DagBag ( ) if dag_id not in dagbag . dags : error_message = "Dag id {} not found" . format ( dag_id ) raise DagNotFound ( error_message ) dag = dagbag . get_dag ( dag_id ) dagrun = dag . get_dagrun ( execution_date = execution_date ) if not dagrun : error_message = ( 'Dag Run for date {} not found in dag {}' . format ( execution_date , dag_id ) ) raise DagRunNotFound ( error_message ) return { 'state' : dagrun . get_state ( ) }
def get_conn ( self ) : conn = self . get_connection ( self . druid_broker_conn_id ) druid_broker_conn = connect ( host = conn . host , port = conn . port , path = conn . extra_dejson . get ( 'endpoint' , '/druid/v2/sql' ) , scheme = conn . extra_dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to druid broker on %s' , conn . host ) return druid_broker_conn
def create_session ( ) : session = settings . Session ( ) try : yield session session . commit ( ) except Exception : session . rollback ( ) raise finally : session . close ( )
def resetdb ( ) : from airflow import models from alembic . migration import MigrationContext log . info ( "Dropping tables that exist" ) models . base . Base . metadata . drop_all ( settings . engine ) mc = MigrationContext . configure ( settings . engine ) if mc . _version . exists ( settings . engine ) : mc . _version . drop ( settings . engine ) from flask_appbuilder . models . sqla import Base Base . metadata . drop_all ( settings . engine ) initdb ( )
def execute ( self , context ) : hook = WasbHook ( wasb_conn_id = self . wasb_conn_id ) self . log . info ( 'Uploading %s to wasb://%s ' 'as %s' . format ( self . file_path , self . container_name , self . blob_name ) ) hook . load_file ( self . file_path , self . container_name , self . blob_name , * * self . load_options )
def get_conn ( self ) : db = self . get_connection ( self . presto_conn_id ) reqkwargs = None if db . password is not None : reqkwargs = { 'auth' : HTTPBasicAuth ( db . login , db . password ) } return presto . connect ( host = db . host , port = db . port , username = db . login , source = db . extra_dejson . get ( 'source' , 'airflow' ) , protocol = db . extra_dejson . get ( 'protocol' , 'http' ) , catalog = db . extra_dejson . get ( 'catalog' , 'hive' ) , requests_kwargs = reqkwargs , schema = db . schema )
def _get_pretty_exception_message ( e ) : if ( hasattr ( e , 'message' ) and 'errorName' in e . message and 'message' in e . message ) : return ( '{name}: {message}' . format ( name = e . message [ 'errorName' ] , message = e . message [ 'message' ] ) ) else : return str ( e )
def get_records ( self , hql , parameters = None ) : try : return super ( ) . get_records ( self . _strip_sql ( hql ) , parameters ) except DatabaseError as e : raise PrestoException ( self . _get_pretty_exception_message ( e ) )
def get_pandas_df ( self , hql , parameters = None ) : import pandas cursor = self . get_cursor ( ) try : cursor . execute ( self . _strip_sql ( hql ) , parameters ) data = cursor . fetchall ( ) except DatabaseError as e : raise PrestoException ( self . _get_pretty_exception_message ( e ) ) column_descriptions = cursor . description if data : df = pandas . DataFrame ( data ) df . columns = [ c [ 0 ] for c in column_descriptions ] else : df = pandas . DataFrame ( ) return df
def run ( self , hql , parameters = None ) : return super ( ) . run ( self . _strip_sql ( hql ) , parameters )
def get_conn ( self ) : if self . cosmos_client is not None : return self . cosmos_client self . cosmos_client = cosmos_client . CosmosClient ( self . endpoint_uri , { 'masterKey' : self . master_key } ) return self . cosmos_client
def does_collection_exist ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( "Collection name cannot be None." ) existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { "query" : "SELECT * FROM r WHERE r.id=@id" , "parameters" : [ { "name" : "@id" , "value" : collection_name } ] } ) ) if len ( existing_container ) == 0 : return False return True
def create_collection ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( "Collection name cannot be None." ) existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { "query" : "SELECT * FROM r WHERE r.id=@id" , "parameters" : [ { "name" : "@id" , "value" : collection_name } ] } ) ) if len ( existing_container ) == 0 : self . get_conn ( ) . CreateContainer ( get_database_link ( self . __get_database_name ( database_name ) ) , { "id" : collection_name } )
def does_database_exist ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( "Database name cannot be None." ) existing_database = list ( self . get_conn ( ) . QueryDatabases ( { "query" : "SELECT * FROM r WHERE r.id=@id" , "parameters" : [ { "name" : "@id" , "value" : database_name } ] } ) ) if len ( existing_database ) == 0 : return False return True
def create_database ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( "Database name cannot be None." ) existing_database = list ( self . get_conn ( ) . QueryDatabases ( { "query" : "SELECT * FROM r WHERE r.id=@id" , "parameters" : [ { "name" : "@id" , "value" : database_name } ] } ) ) if len ( existing_database ) == 0 : self . get_conn ( ) . CreateDatabase ( { "id" : database_name } )
def delete_database ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( "Database name cannot be None." ) self . get_conn ( ) . DeleteDatabase ( get_database_link ( database_name ) )
def delete_collection ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( "Collection name cannot be None." ) self . get_conn ( ) . DeleteContainer ( get_collection_link ( self . __get_database_name ( database_name ) , collection_name ) )
def insert_documents ( self , documents , database_name = None , collection_name = None ) : if documents is None : raise AirflowBadRequest ( "You cannot insert empty documents" ) created_documents = [ ] for single_document in documents : created_documents . append ( self . get_conn ( ) . CreateItem ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) ) , single_document ) ) return created_documents
def delete_document ( self , document_id , database_name = None , collection_name = None ) : if document_id is None : raise AirflowBadRequest ( "Cannot delete a document without an id" ) self . get_conn ( ) . DeleteItem ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) )
def get_document ( self , document_id , database_name = None , collection_name = None ) : if document_id is None : raise AirflowBadRequest ( "Cannot get a document without an id" ) try : return self . get_conn ( ) . ReadItem ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) ) except HTTPFailure : return None
def get_documents ( self , sql_string , database_name = None , collection_name = None , partition_key = None ) : if sql_string is None : raise AirflowBadRequest ( "SQL query string cannot be None" ) query = { 'query' : sql_string } try : result_iterable = self . get_conn ( ) . QueryItems ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) ) , query , partition_key ) return list ( result_iterable ) except HTTPFailure : return None
def get_code ( dag_id ) : session = settings . Session ( ) DM = models . DagModel dag = session . query ( DM ) . filter ( DM . dag_id == dag_id ) . first ( ) session . close ( ) if dag is None : error_message = "Dag id {} not found" . format ( dag_id ) raise DagNotFound ( error_message ) try : with wwwutils . open_maybe_zipped ( dag . fileloc , 'r' ) as f : code = f . read ( ) return code except IOError as e : error_message = "Error {} while reading Dag id {} Code" . format ( str ( e ) , dag_id ) raise AirflowException ( error_message )
def get_conn ( self ) : conn = self . get_connection ( self . vertica_conn_id ) conn_config = { "user" : conn . login , "password" : conn . password or '' , "database" : conn . schema , "host" : conn . host or 'localhost' } if not conn . port : conn_config [ "port" ] = 5433 else : conn_config [ "port" ] = int ( conn . port ) conn = connect ( * * conn_config ) return conn
def flush ( self ) : if len ( self . _buffer ) > 0 : self . logger . log ( self . level , self . _buffer ) self . _buffer = str ( )
def start ( self ) : self . _process = self . _launch_process ( self . _dag_directory , self . _file_paths , self . _max_runs , self . _processor_factory , self . _child_signal_conn , self . _stat_queue , self . _result_queue , self . _async_mode ) self . log . info ( "Launched DagFileProcessorManager with pid: %s" , self . _process . pid )
def _exit_gracefully ( self , signum , frame ) : self . log . info ( "Exiting gracefully upon receiving signal %s" , signum ) self . terminate ( ) self . end ( ) self . log . debug ( "Finished terminating DAG processors." ) sys . exit ( os . EX_OK )
def start_in_async ( self ) : while True : loop_start_time = time . time ( ) if self . _signal_conn . poll ( ) : agent_signal = self . _signal_conn . recv ( ) if agent_signal == DagParsingSignal . TERMINATE_MANAGER : self . terminate ( ) break elif agent_signal == DagParsingSignal . END_MANAGER : self . end ( ) sys . exit ( os . EX_OK ) self . _refresh_dag_dir ( ) simple_dags = self . heartbeat ( ) for simple_dag in simple_dags : self . _result_queue . put ( simple_dag ) self . _print_stat ( ) all_files_processed = all ( self . get_last_finish_time ( x ) is not None for x in self . file_paths ) max_runs_reached = self . max_runs_reached ( ) dag_parsing_stat = DagParsingStat ( self . _file_paths , self . get_all_pids ( ) , max_runs_reached , all_files_processed , len ( simple_dags ) ) self . _stat_queue . put ( dag_parsing_stat ) if max_runs_reached : self . log . info ( "Exiting dag parsing loop as all files " "have been processed %s times" , self . _max_runs ) break loop_duration = time . time ( ) - loop_start_time if loop_duration < 1 : sleep_length = 1 - loop_duration self . log . debug ( "Sleeping for %.2f seconds to prevent excessive logging" , sleep_length ) time . sleep ( sleep_length )
def _refresh_dag_dir ( self ) : elapsed_time_since_refresh = ( timezone . utcnow ( ) - self . last_dag_dir_refresh_time ) . total_seconds ( ) if elapsed_time_since_refresh > self . dag_dir_list_interval : self . log . info ( "Searching for files in %s" , self . _dag_directory ) self . _file_paths = list_py_file_paths ( self . _dag_directory ) self . last_dag_dir_refresh_time = timezone . utcnow ( ) self . log . info ( "There are %s files in %s" , len ( self . _file_paths ) , self . _dag_directory ) self . set_file_paths ( self . _file_paths ) try : self . log . debug ( "Removing old import errors" ) self . clear_nonexistent_import_errors ( ) except Exception : self . log . exception ( "Error removing old import errors" )
def _print_stat ( self ) : if ( ( timezone . utcnow ( ) - self . last_stat_print_time ) . total_seconds ( ) > self . print_stats_interval ) : if len ( self . _file_paths ) > 0 : self . _log_file_processing_stats ( self . _file_paths ) self . last_stat_print_time = timezone . utcnow ( )
def wait_until_finished ( self ) : for file_path , processor in self . _processors . items ( ) : while not processor . done : time . sleep ( 0.1 )
def open_slots ( self , session ) : from airflow . models . taskinstance import TaskInstance as TI used_slots = session . query ( func . count ( ) ) . filter ( TI . pool == self . pool ) . filter ( TI . state . in_ ( [ State . RUNNING , State . QUEUED ] ) ) . scalar ( ) return self . slots - used_slots
def run_command ( command ) : process = subprocess . Popen ( shlex . split ( command ) , stdout = subprocess . PIPE , stderr = subprocess . PIPE , close_fds = True ) output , stderr = [ stream . decode ( sys . getdefaultencoding ( ) , 'ignore' ) for stream in process . communicate ( ) ] if process . returncode != 0 : raise AirflowConfigException ( "Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}" . format ( command , process . returncode , output , stderr ) ) return output
def get_task ( dag_id , task_id ) : dagbag = DagBag ( ) if dag_id not in dagbag . dags : error_message = "Dag id {} not found" . format ( dag_id ) raise DagNotFound ( error_message ) dag = dagbag . get_dag ( dag_id ) if not dag . has_task ( task_id ) : error_message = 'Task {} not found in dag {}' . format ( task_id , dag_id ) raise TaskNotFound ( error_message ) return dag . get_task ( task_id )
def dispose_orm ( ) : log . debug ( "Disposing DB connection pool (PID %s)" , os . getpid ( ) ) global engine global Session if Session : Session . remove ( ) Session = None if engine : engine . dispose ( ) engine = None
def prepare_classpath ( ) : if DAGS_FOLDER not in sys . path : sys . path . append ( DAGS_FOLDER ) config_path = os . path . join ( AIRFLOW_HOME , 'config' ) if config_path not in sys . path : sys . path . append ( config_path ) if PLUGINS_FOLDER not in sys . path : sys . path . append ( PLUGINS_FOLDER )
def alchemy_to_dict ( obj ) : if not obj : return None d = { } for c in obj . __table__ . columns : value = getattr ( obj , c . name ) if type ( value ) == datetime : value = value . isoformat ( ) d [ c . name ] = value return d
def chunks ( items , chunk_size ) : if chunk_size <= 0 : raise ValueError ( 'Chunk size must be a positive integer' ) for i in range ( 0 , len ( items ) , chunk_size ) : yield items [ i : i + chunk_size ]
def get_task_instance ( dag_id , task_id , execution_date ) : dagbag = DagBag ( ) if dag_id not in dagbag . dags : error_message = "Dag id {} not found" . format ( dag_id ) raise DagNotFound ( error_message ) dag = dagbag . get_dag ( dag_id ) if not dag . has_task ( task_id ) : error_message = 'Task {} not found in dag {}' . format ( task_id , dag_id ) raise TaskNotFound ( error_message ) dagrun = dag . get_dagrun ( execution_date = execution_date ) if not dagrun : error_message = ( 'Dag Run for date {} not found in dag {}' . format ( execution_date , dag_id ) ) raise DagRunNotFound ( error_message ) task_instance = dagrun . get_task_instance ( task_id ) if not task_instance : error_message = ( 'Task {} instance for date {} not found' . format ( task_id , execution_date ) ) raise TaskInstanceNotFound ( error_message ) return task_instance
def _integrate_plugins ( ) : import sys from airflow . plugins_manager import operators_modules for operators_module in operators_modules : sys . modules [ operators_module . __name__ ] = operators_module globals ( ) [ operators_module . _name ] = operators_module
def get_conn ( self ) : http_authorized = self . _authorize ( ) return build ( 'dataproc' , self . api_version , http = http_authorized , cache_discovery = False )
def wait ( self , operation ) : submitted = _DataProcOperation ( self . get_conn ( ) , operation , self . num_retries ) submitted . wait_for_done ( )
def get_conn ( self ) : authed_http = self . _authorize ( ) return build ( 'ml' , 'v1' , http = authed_http , cache_discovery = False )
def set_default_version ( self , project_id , model_name , version_name ) : full_version_name = 'projects/{}/models/{}/versions/{}' . format ( project_id , model_name , version_name ) request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . setDefault ( name = full_version_name , body = { } ) try : response = request . execute ( ) self . log . info ( 'Successfully set version: %s to default' , response ) return response except HttpError as e : self . log . error ( 'Something went wrong: %s' , e ) raise
def list_versions ( self , project_id , model_name ) : result = [ ] full_parent_name = 'projects/{}/models/{}' . format ( project_id , model_name ) request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full_parent_name , pageSize = 100 ) response = request . execute ( ) next_page_token = response . get ( 'nextPageToken' , None ) result . extend ( response . get ( 'versions' , [ ] ) ) while next_page_token is not None : next_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full_parent_name , pageToken = next_page_token , pageSize = 100 ) response = next_request . execute ( ) next_page_token = response . get ( 'nextPageToken' , None ) result . extend ( response . get ( 'versions' , [ ] ) ) time . sleep ( 5 ) return result
def delete_version ( self , project_id , model_name , version_name ) : full_name = 'projects/{}/models/{}/versions/{}' . format ( project_id , model_name , version_name ) delete_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . delete ( name = full_name ) response = delete_request . execute ( ) get_request = self . _mlengine . projects ( ) . operations ( ) . get ( name = response [ 'name' ] ) return _poll_with_exponential_delay ( request = get_request , max_n = 9 , is_done_func = lambda resp : resp . get ( 'done' , False ) , is_error_func = lambda resp : resp . get ( 'error' , None ) is not None )
def create_model ( self , project_id , model ) : if not model [ 'name' ] : raise ValueError ( "Model name must be provided and " "could not be an empty string" ) project = 'projects/{}' . format ( project_id ) request = self . _mlengine . projects ( ) . models ( ) . create ( parent = project , body = model ) return request . execute ( )
def get_model ( self , project_id , model_name ) : if not model_name : raise ValueError ( "Model name must be provided and " "it could not be an empty string" ) full_model_name = 'projects/{}/models/{}' . format ( project_id , model_name ) request = self . _mlengine . projects ( ) . models ( ) . get ( name = full_model_name ) try : return request . execute ( ) except HttpError as e : if e . resp . status == 404 : self . log . error ( 'Model was not found: %s' , e ) return None raise
def write_batch_data ( self , items ) : dynamodb_conn = self . get_conn ( ) try : table = dynamodb_conn . Table ( self . table_name ) with table . batch_writer ( overwrite_by_pkeys = self . table_keys ) as batch : for item in items : batch . put_item ( Item = item ) return True except Exception as general_error : raise AirflowException ( 'Failed to insert items in dynamodb, error: {error}' . format ( error = str ( general_error ) ) )
def _integrate_plugins ( ) : from airflow . plugins_manager import executors_modules for executors_module in executors_modules : sys . modules [ executors_module . __name__ ] = executors_module globals ( ) [ executors_module . _name ] = executors_module
def get_default_executor ( ) : global DEFAULT_EXECUTOR if DEFAULT_EXECUTOR is not None : return DEFAULT_EXECUTOR executor_name = configuration . conf . get ( 'core' , 'EXECUTOR' ) DEFAULT_EXECUTOR = _get_executor ( executor_name ) log = LoggingMixin ( ) . log log . info ( "Using executor %s" , executor_name ) return DEFAULT_EXECUTOR
def on_error ( self , error , items ) : self . log . error ( 'Encountered Segment error: {segment_error} with ' 'items: {with_items}' . format ( segment_error = error , with_items = items ) ) raise AirflowException ( 'Segment error: {}' . format ( error ) )
def get_conn ( self ) : conn = self . get_connection ( self . mssql_conn_id ) conn = pymssql . connect ( server = conn . host , user = conn . login , password = conn . password , database = self . schema or conn . schema , port = conn . port ) return conn
def execute ( self , context ) : self . _hook = SparkSubmitHook ( conf = self . _conf , conn_id = self . _conn_id , files = self . _files , py_files = self . _py_files , archives = self . _archives , driver_class_path = self . _driver_class_path , jars = self . _jars , java_class = self . _java_class , packages = self . _packages , exclude_packages = self . _exclude_packages , repositories = self . _repositories , total_executor_cores = self . _total_executor_cores , executor_cores = self . _executor_cores , executor_memory = self . _executor_memory , driver_memory = self . _driver_memory , keytab = self . _keytab , principal = self . _principal , name = self . _name , num_executors = self . _num_executors , application_args = self . _application_args , env_vars = self . _env_vars , verbose = self . _verbose , spark_binary = self . _spark_binary ) self . _hook . submit ( self . _application )
def delete_dag ( dag_id ) : try : count = delete . delete_dag ( dag_id ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = "{}" . format ( err ) ) response . status_code = err . status_code return response return jsonify ( message = "Removed {} record(s)" . format ( count ) , count = count )
def get_dag_code ( dag_id ) : try : return get_code ( dag_id ) except AirflowException as err : _log . info ( err ) response = jsonify ( error = "{}" . format ( err ) ) response . status_code = err . status_code return response
def task_info ( dag_id , task_id ) : try : info = get_task ( dag_id , task_id ) except AirflowException as err : _log . info ( err ) response = jsonify ( error = "{}" . format ( err ) ) response . status_code = err . status_code return response fields = { k : str ( v ) for k , v in vars ( info ) . items ( ) if not k . startswith ( '_' ) } return jsonify ( fields )
def get_pools ( ) : try : pools = pool_api . get_pools ( ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = "{}" . format ( err ) ) response . status_code = err . status_code return response else : return jsonify ( [ p . to_json ( ) for p in pools ] )
def create_pool ( ) : params = request . get_json ( force = True ) try : pool = pool_api . create_pool ( * * params ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = "{}" . format ( err ) ) response . status_code = err . status_code return response else : return jsonify ( pool . to_json ( ) )
def get_task_instances ( self , state = None , session = None ) : from airflow . models . taskinstance import TaskInstance tis = session . query ( TaskInstance ) . filter ( TaskInstance . dag_id == self . dag_id , TaskInstance . execution_date == self . execution_date , ) if state : if isinstance ( state , six . string_types ) : tis = tis . filter ( TaskInstance . state == state ) else : if None in state : tis = tis . filter ( or_ ( TaskInstance . state . in_ ( state ) , TaskInstance . state . is_ ( None ) ) ) else : tis = tis . filter ( TaskInstance . state . in_ ( state ) ) if self . dag and self . dag . partial : tis = tis . filter ( TaskInstance . task_id . in_ ( self . dag . task_ids ) ) return tis . all ( )
def get_previous_dagrun ( self , session = None ) : return session . query ( DagRun ) . filter ( DagRun . dag_id == self . dag_id , DagRun . execution_date < self . execution_date ) . order_by ( DagRun . execution_date . desc ( ) ) . first ( )
def get_previous_scheduled_dagrun ( self , session = None ) : dag = self . get_dag ( ) return session . query ( DagRun ) . filter ( DagRun . dag_id == self . dag_id , DagRun . execution_date == dag . previous_schedule ( self . execution_date ) ) . first ( )
def conditionally_trigger ( context , dag_run_obj ) : c_p = context [ 'params' ] [ 'condition_param' ] print ( "Controller DAG : conditionally_trigger = {}" . format ( c_p ) ) if context [ 'params' ] [ 'condition_param' ] : dag_run_obj . payload = { 'message' : context [ 'params' ] [ 'message' ] } pp . pprint ( dag_run_obj . payload ) return dag_run_obj
def get_dag ( self , dag_id ) : from airflow . models . dag import DagModel root_dag_id = dag_id if dag_id in self . dags : dag = self . dags [ dag_id ] if dag . is_subdag : root_dag_id = dag . parent_dag . dag_id orm_dag = DagModel . get_current ( root_dag_id ) if orm_dag and ( root_dag_id not in self . dags or ( orm_dag . last_expired and dag . last_loaded < orm_dag . last_expired ) ) : found_dags = self . process_file ( filepath = orm_dag . fileloc , only_if_updated = False ) if found_dags and dag_id in [ found_dag . dag_id for found_dag in found_dags ] : return self . dags [ dag_id ] elif dag_id in self . dags : del self . dags [ dag_id ] return self . dags . get ( dag_id )
def dagbag_report ( self ) : report = textwrap . dedent ( ) stats = self . dagbag_stats return report . format ( dag_folder = self . dag_folder , duration = sum ( [ o . duration for o in stats ] ) , dag_num = sum ( [ o . dag_num for o in stats ] ) , task_num = sum ( [ o . task_num for o in stats ] ) , table = pprinttable ( stats ) , )
def execute ( self , context ) : self . _hook = SparkJDBCHook ( spark_app_name = self . _spark_app_name , spark_conn_id = self . _spark_conn_id , spark_conf = self . _spark_conf , spark_py_files = self . _spark_py_files , spark_files = self . _spark_files , spark_jars = self . _spark_jars , num_executors = self . _num_executors , executor_cores = self . _executor_cores , executor_memory = self . _executor_memory , driver_memory = self . _driver_memory , verbose = self . _verbose , keytab = self . _keytab , principal = self . _principal , cmd_type = self . _cmd_type , jdbc_table = self . _jdbc_table , jdbc_conn_id = self . _jdbc_conn_id , jdbc_driver = self . _jdbc_driver , metastore_table = self . _metastore_table , jdbc_truncate = self . _jdbc_truncate , save_mode = self . _save_mode , save_format = self . _save_format , batch_size = self . _batch_size , fetch_size = self . _fetch_size , num_partitions = self . _num_partitions , partition_column = self . _partition_column , lower_bound = self . _lower_bound , upper_bound = self . _upper_bound , create_table_column_types = self . _create_table_column_types ) self . _hook . submit_jdbc_job ( )
def _integrate_plugins ( ) : import sys from airflow . plugins_manager import macros_modules for macros_module in macros_modules : sys . modules [ macros_module . __name__ ] = macros_module globals ( ) [ macros_module . _name ] = macros_module
def error ( self , session = None ) : self . log . error ( "Recording the task instance as FAILED" ) self . state = State . FAILED session . merge ( self ) session . commit ( )
def clear_xcom_data ( self , session = None ) : session . query ( XCom ) . filter ( XCom . dag_id == self . dag_id , XCom . task_id == self . task_id , XCom . execution_date == self . execution_date ) . delete ( ) session . commit ( )
def key ( self ) : return self . dag_id , self . task_id , self . execution_date , self . try_number
def init_run_context ( self , raw = False ) : self . raw = raw self . _set_context ( self )
def close ( self ) : if self . closed : return super ( ) . close ( ) if not self . upload_on_close : return local_loc = os . path . join ( self . local_base , self . log_relative_path ) remote_loc = os . path . join ( self . remote_base , self . log_relative_path ) if os . path . exists ( local_loc ) : with open ( local_loc , 'r' ) as logfile : log = logfile . read ( ) self . wasb_write ( log , remote_loc , append = True ) if self . delete_local_copy : shutil . rmtree ( os . path . dirname ( local_loc ) ) self . closed = True
def _query_cassandra ( self ) : self . hook = CassandraHook ( cassandra_conn_id = self . cassandra_conn_id ) session = self . hook . get_conn ( ) cursor = session . execute ( self . cql ) return cursor
def execute ( self , context ) : self . _hook = SparkSqlHook ( sql = self . _sql , conf = self . _conf , conn_id = self . _conn_id , total_executor_cores = self . _total_executor_cores , executor_cores = self . _executor_cores , executor_memory = self . _executor_memory , keytab = self . _keytab , principal = self . _principal , name = self . _name , num_executors = self . _num_executors , master = self . _master , yarn_queue = self . _yarn_queue ) self . _hook . run_query ( )
def get_conn ( self ) : conn = self . get_connection ( self . conn_id ) service_options = conn . extra_dejson self . account_name = service_options . get ( 'account_name' ) adlCreds = lib . auth ( tenant_id = service_options . get ( 'tenant' ) , client_secret = conn . password , client_id = conn . login ) adlsFileSystemClient = core . AzureDLFileSystem ( adlCreds , store_name = self . account_name ) adlsFileSystemClient . connect ( ) return adlsFileSystemClient
def execute ( self , context ) : self . hook = self . get_hook ( ) self . hook . get_conn ( ) self . query_execution_context [ 'Database' ] = self . database self . result_configuration [ 'OutputLocation' ] = self . output_location self . query_execution_id = self . hook . run_query ( self . query , self . query_execution_context , self . result_configuration , self . client_request_token ) query_status = self . hook . poll_query_status ( self . query_execution_id , self . max_tries ) if query_status in AWSAthenaHook . FAILURE_STATES : raise Exception ( 'Final state of Athena job is {}, query_execution_id is {}.' . format ( query_status , self . query_execution_id ) ) elif not query_status or query_status in AWSAthenaHook . INTERMEDIATE_STATES : raise Exception ( 'Final state of Athena job is {}. ' 'Max tries of poll status exceeded, query_execution_id is {}.' . format ( query_status , self . query_execution_id ) )
def on_kill ( self ) : if self . query_execution_id : self . log . info ( '⚰️⚰️⚰️ Received a kill Signal. Time to Die')  self . log . info ( 'Stopping Query with executionId - %s' , self . query_execution_id ) response = self . hook . stop_query ( self . query_execution_id ) http_status_code = None try : http_status_code = response [ 'ResponseMetadata' ] [ 'HTTPStatusCode' ] except Exception as ex : self . log . error ( 'Exception while cancelling query' , ex ) finally : if http_status_code is None or http_status_code != 200 : self . log . error ( 'Unable to request query cancel on athena. Exiting' ) else : self . log . info ( 'Polling Athena for query with id %s to reach final state' , self . query_execution_id ) self . hook . poll_query_status ( self . query_execution_id )
def uncompress_file ( input_file_name , file_extension , dest_dir ) : if file_extension . lower ( ) not in ( '.gz' , '.bz2' ) : raise NotImplementedError ( "Received {} format. Only gz and bz2 " "files can currently be uncompressed." . format ( file_extension ) ) if file_extension . lower ( ) == '.gz' : fmodule = gzip . GzipFile elif file_extension . lower ( ) == '.bz2' : fmodule = bz2 . BZ2File with fmodule ( input_file_name , mode = 'rb' ) as f_compressed , NamedTemporaryFile ( dir = dest_dir , mode = 'wb' , delete = False ) as f_uncompressed : shutil . copyfileobj ( f_compressed , f_uncompressed ) return f_uncompressed . name
def get_conn ( self ) : if not self . conn : connection = self . get_connection ( self . conn_id ) extras = connection . extra_dejson self . conn = Salesforce ( username = connection . login , password = connection . password , security_token = extras [ 'security_token' ] , instance_url = connection . host , sandbox = extras . get ( 'sandbox' , False ) ) return self . conn
def put_records ( self , records ) : firehose_conn = self . get_conn ( ) response = firehose_conn . put_record_batch ( DeliveryStreamName = self . delivery_stream , Records = records ) return response
def send_email ( to , subject , html_content , files = None , dryrun = False , cc = None , bcc = None , mime_subtype = 'mixed' , mime_charset = 'utf-8' , * * kwargs ) : path , attr = configuration . conf . get ( 'email' , 'EMAIL_BACKEND' ) . rsplit ( '.' , 1 ) module = importlib . import_module ( path ) backend = getattr ( module , attr ) to = get_email_address_list ( to ) to = ", " . join ( to ) return backend ( to , subject , html_content , files = files , dryrun = dryrun , cc = cc , bcc = bcc , mime_subtype = mime_subtype , mime_charset = mime_charset , * * kwargs )
def get_conn ( self ) : if self . conn is None : params = self . get_connection ( self . ftp_conn_id ) pasv = params . extra_dejson . get ( "passive" , True ) self . conn = ftplib . FTP ( params . host , params . login , params . password ) self . conn . set_pasv ( pasv ) return self . conn
def execute ( self , context ) : self . hook = DiscordWebhookHook ( self . http_conn_id , self . webhook_endpoint , self . message , self . username , self . avatar_url , self . tts , self . proxy ) self . hook . execute ( )
def get_conn ( self ) : conn = self . get_connection ( self . conn_id ) service_options = conn . extra_dejson return FileService ( account_name = conn . login , account_key = conn . password , * * service_options )
def get_conn ( self ) : if not self . _conn : self . _conn = storage . Client ( credentials = self . _get_credentials ( ) ) return self . _conn
def describe_training_job_with_log ( self , job_name , positions , stream_names , instance_count , state , last_description , last_describe_job_call ) : log_group = '/aws/sagemaker/TrainingJobs' if len ( stream_names ) < instance_count : logs_conn = self . get_log_conn ( ) try : streams = logs_conn . describe_log_streams ( logGroupName = log_group , logStreamNamePrefix = job_name + '/' , orderBy = 'LogStreamName' , limit = instance_count ) stream_names = [ s [ 'logStreamName' ] for s in streams [ 'logStreams' ] ] positions . update ( [ ( s , Position ( timestamp = 0 , skip = 0 ) ) for s in stream_names if s not in positions ] ) except logs_conn . exceptions . ResourceNotFoundException : pass if len ( stream_names ) > 0 : for idx , event in self . multi_stream_iter ( log_group , stream_names , positions ) : self . log . info ( event [ 'message' ] ) ts , count = positions [ stream_names [ idx ] ] if event [ 'timestamp' ] == ts : positions [ stream_names [ idx ] ] = Position ( timestamp = ts , skip = count + 1 ) else : positions [ stream_names [ idx ] ] = Position ( timestamp = event [ 'timestamp' ] , skip = 1 ) if state == LogState . COMPLETE : return state , last_description , last_describe_job_call if state == LogState . JOB_COMPLETE : state = LogState . COMPLETE elif time . time ( ) - last_describe_job_call >= 30 : description = self . describe_training_job ( job_name ) last_describe_job_call = time . time ( ) if secondary_training_status_changed ( description , last_description ) : self . log . info ( secondary_training_status_message ( description , last_description ) ) last_description = description status = description [ 'TrainingJobStatus' ] if status not in self . non_terminal_states : state = LogState . JOB_COMPLETE return state , last_description , last_describe_job_call
def execute ( self , context ) : bucket_helper = GoogleCloudBucketHelper ( self . gcp_conn_id , self . delegate_to ) self . py_file = bucket_helper . google_cloud_to_local ( self . py_file ) hook = DataFlowHook ( gcp_conn_id = self . gcp_conn_id , delegate_to = self . delegate_to , poll_sleep = self . poll_sleep ) dataflow_options = self . dataflow_default_options . copy ( ) dataflow_options . update ( self . options ) camel_to_snake = lambda name : re . sub ( r'[A-Z]' , lambda x : '_' + x . group ( 0 ) . lower ( ) , name ) formatted_options = { camel_to_snake ( key ) : dataflow_options [ key ] for key in dataflow_options } hook . start_python_dataflow ( self . job_name , formatted_options , self . py_file , self . py_options )
def _prepare_cli_cmd ( self ) : conn = self . conn hive_bin = 'hive' cmd_extra = [ ] if self . use_beeline : hive_bin = 'beeline' jdbc_url = "jdbc:hive2://{host}:{port}/{schema}" . format ( host = conn . host , port = conn . port , schema = conn . schema ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : template = conn . extra_dejson . get ( 'principal' , "hive/_HOST@EXAMPLE.COM" ) if "_HOST" in template : template = utils . replace_hostname_pattern ( utils . get_components ( template ) ) proxy_user = "" if conn . extra_dejson . get ( 'proxy_user' ) == "login" and conn . login : proxy_user = "hive.server2.proxy.user={0}" . format ( conn . login ) elif conn . extra_dejson . get ( 'proxy_user' ) == "owner" and self . run_as : proxy_user = "hive.server2.proxy.user={0}" . format ( self . run_as ) jdbc_url += ";principal={template};{proxy_user}" . format ( template = template , proxy_user = proxy_user ) elif self . auth : jdbc_url += ";auth=" + self . auth jdbc_url = '"{}"' . format ( jdbc_url ) cmd_extra += [ '-u' , jdbc_url ] if conn . login : cmd_extra += [ '-n' , conn . login ] if conn . password : cmd_extra += [ '-p' , conn . password ] hive_params_list = self . hive_cli_params . split ( ) return [ hive_bin ] + cmd_extra + hive_params_list
def get_metastore_client ( self ) : import hmsclient from thrift . transport import TSocket , TTransport from thrift . protocol import TBinaryProtocol ms = self . metastore_conn auth_mechanism = ms . extra_dejson . get ( 'authMechanism' , 'NOSASL' ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : auth_mechanism = ms . extra_dejson . get ( 'authMechanism' , 'GSSAPI' ) kerberos_service_name = ms . extra_dejson . get ( 'kerberos_service_name' , 'hive' ) socket = TSocket . TSocket ( ms . host , ms . port ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' and auth_mechanism == 'GSSAPI' : try : import saslwrapper as sasl except ImportError : import sasl def sasl_factory ( ) : sasl_client = sasl . Client ( ) sasl_client . setAttr ( "host" , ms . host ) sasl_client . setAttr ( "service" , kerberos_service_name ) sasl_client . init ( ) return sasl_client from thrift_sasl import TSaslClientTransport transport = TSaslClientTransport ( sasl_factory , "GSSAPI" , socket ) else : transport = TTransport . TBufferedTransport ( socket ) protocol = TBinaryProtocol . TBinaryProtocol ( transport ) return hmsclient . HMSClient ( iprot = protocol )
def get_tables ( self , db , pattern = '*' ) : with self . metastore as client : tables = client . get_tables ( db_name = db , pattern = pattern ) return client . get_table_objects_by_name ( db , tables )
def get_conn ( self , schema = None ) : db = self . get_connection ( self . hiveserver2_conn_id ) auth_mechanism = db . extra_dejson . get ( 'authMechanism' , 'NONE' ) if auth_mechanism == 'NONE' and db . login is None : username = 'airflow' kerberos_service_name = None if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : auth_mechanism = db . extra_dejson . get ( 'authMechanism' , 'KERBEROS' ) kerberos_service_name = db . extra_dejson . get ( 'kerberos_service_name' , 'hive' ) if auth_mechanism == 'GSSAPI' : self . log . warning ( "Detected deprecated 'GSSAPI' for authMechanism " "for %s. Please use 'KERBEROS' instead" , self . hiveserver2_conn_id ) auth_mechanism = 'KERBEROS' from pyhive . hive import connect return connect ( host = db . host , port = db . port , auth = auth_mechanism , kerberos_service_name = kerberos_service_name , username = db . login or username , password = db . password , database = schema or db . schema or 'default' )
def _get_endpoint ( self ) : conn = self . get_connection ( self . http_conn_id ) token = conn . password if not token : raise AirflowException ( 'Dingding token is requests but get nothing, ' 'check you conn_id configuration.' ) return 'robot/send?access_token={}' . format ( token )
def _bind_parameters ( operation , parameters ) : string_parameters = { } for ( name , value ) in iteritems ( parameters ) : if value is None : string_parameters [ name ] = 'NULL' elif isinstance ( value , basestring ) : string_parameters [ name ] = "'" + _escape ( value ) + "'" else : string_parameters [ name ] = str ( value ) return operation % string_parameters
def _escape ( s ) : e = s e = e . replace ( '\\' , '\\\\' ) e = e . replace ( '\n' , '\\n' ) e = e . replace ( '\r' , '\\r' ) e = e . replace ( "'" , "\\'" ) e = e . replace ( '"' , '\\"' ) return e
def get_conn ( self ) : service = self . get_service ( ) project = self . _get_field ( 'project' ) return BigQueryConnection ( service = service , project_id = project , use_legacy_sql = self . use_legacy_sql , location = self . location , num_retries = self . num_retries )
def get_service ( self ) : http_authorized = self . _authorize ( ) return build ( 'bigquery' , 'v2' , http = http_authorized , cache_discovery = False )
def cancel_query ( self ) : jobs = self . service . jobs ( ) if ( self . running_job_id and not self . poll_job_complete ( self . running_job_id ) ) : self . log . info ( 'Attempting to cancel job : %s, %s' , self . project_id , self . running_job_id ) if self . location : jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id , location = self . location ) . execute ( num_retries = self . num_retries ) else : jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id ) . execute ( num_retries = self . num_retries ) else : self . log . info ( 'No running BigQuery jobs to cancel.' ) return max_polling_attempts = 12 polling_attempts = 0 job_complete = False while polling_attempts < max_polling_attempts and not job_complete : polling_attempts = polling_attempts + 1 job_complete = self . poll_job_complete ( self . running_job_id ) if job_complete : self . log . info ( 'Job successfully canceled: %s, %s' , self . project_id , self . running_job_id ) elif polling_attempts == max_polling_attempts : self . log . info ( "Stopping polling due to timeout. Job with id %s " "has not completed cancel and may or may not finish." , self . running_job_id ) else : self . log . info ( 'Waiting for canceled job with id %s to finish.' , self . running_job_id ) time . sleep ( 5 )
def _query_postgres ( self ) : postgres = PostgresHook ( postgres_conn_id = self . postgres_conn_id ) conn = postgres . get_conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql , self . parameters ) return cursor
def _integrate_plugins ( ) : from airflow . plugins_manager import hooks_modules for hooks_module in hooks_modules : sys . modules [ hooks_module . __name__ ] = hooks_module globals ( ) [ hooks_module . _name ] = hooks_module
def on_finish ( self ) : if self . _cfg_path and os . path . isfile ( self . _cfg_path ) : if self . run_as_user : subprocess . call ( [ 'sudo' , 'rm' , self . _cfg_path ] , close_fds = True ) else : os . remove ( self . _cfg_path )
def _main ( ) : usage = "usage: nvd3.py [options]" parser = OptionParser ( usage = usage , version = ( "python-nvd3 - Charts generator with " "nvd3.js and d3.js" ) ) parser . add_option ( "-q" , "--quiet" , action = "store_false" , dest = "verbose" , default = True , help = "don't print messages to stdout" ) ( options , args ) = parser . parse_args ( )
def buildhtmlheader ( self ) : self . htmlheader = '' global _js_initialized if '_js_initialized' not in globals ( ) or not _js_initialized : for css in self . header_css : self . htmlheader += css for js in self . header_js : self . htmlheader += js
def buildjschart ( self ) : self . jschart = '' if self . tooltip_condition_string == '' : self . tooltip_condition_string = 'var y = String(graph.point.y);\n' self . series_js = json . dumps ( self . series )
def create_x_axis ( self , name , label = None , format = None , date = False , custom_format = False ) : axis = { } if custom_format and format : axis [ 'tickFormat' ] = format elif format : if format == 'AM_PM' : axis [ 'tickFormat' ] = "function(d) { return get_am_pm(parseInt(d)); }" else : axis [ 'tickFormat' ] = "d3.format(',%s')" % format if label : axis [ 'axisLabel' ] = "'" + label + "'" if date : self . dateformat = format axis [ 'tickFormat' ] = ( "function(d) { return d3.time.format('%s')" "(new Date(parseInt(d))) }\n" "" % self . dateformat ) if name [ 0 ] == 'x' : self . x_axis_date = True self . axislist [ name ] = axis if name == "xAxis" and self . focus_enable : self . axislist [ 'x2Axis' ] = axis
def create_y_axis ( self , name , label = None , format = None , custom_format = False ) : axis = { } if custom_format and format : axis [ 'tickFormat' ] = format elif format : axis [ 'tickFormat' ] = "d3.format(',%s')" % format if label : axis [ 'axisLabel' ] = "'" + label + "'" self . axislist [ name ] = axis
def get_conn ( self ) : conn = self . get_connection ( self . sqlite_conn_id ) conn = sqlite3 . connect ( conn . host ) return conn
def action_logging ( f ) : @ functools . wraps ( f ) def wrapper ( * args , * * kwargs ) : with create_session ( ) as session : if g . user . is_anonymous : user = 'anonymous' else : user = g . user . username log = Log ( event = f . __name__ , task_instance = None , owner = user , extra = str ( list ( request . args . items ( ) ) ) , task_id = request . args . get ( 'task_id' ) , dag_id = request . args . get ( 'dag_id' ) ) if 'execution_date' in request . args : log . execution_date = pendulum . parse ( request . args . get ( 'execution_date' ) ) session . add ( log ) return f ( * args , * * kwargs ) return wrapper
def gzipped ( f ) : @ functools . wraps ( f ) def view_func ( * args , * * kwargs ) : @ after_this_request def zipper ( response ) : accept_encoding = request . headers . get ( 'Accept-Encoding' , '' ) if 'gzip' not in accept_encoding . lower ( ) : return response response . direct_passthrough = False if ( response . status_code < 200 or response . status_code >= 300 or 'Content-Encoding' in response . headers ) : return response gzip_buffer = IO ( ) gzip_file = gzip . GzipFile ( mode = 'wb' , fileobj = gzip_buffer ) gzip_file . write ( response . data ) gzip_file . close ( ) response . data = gzip_buffer . getvalue ( ) response . headers [ 'Content-Encoding' ] = 'gzip' response . headers [ 'Vary' ] = 'Accept-Encoding' response . headers [ 'Content-Length' ] = len ( response . data ) return response return f ( * args , * * kwargs ) return view_func
def json_response ( obj ) : return Response ( response = json . dumps ( obj , indent = 4 , cls = AirflowJsonEncoder ) , status = 200 , mimetype = "application/json" )
def make_cache_key ( * args , * * kwargs ) : path = request . path args = str ( hash ( frozenset ( request . args . items ( ) ) ) ) return ( path + args ) . encode ( 'ascii' , 'ignore' )
def _get_api_key ( self ) : conn = self . get_connection ( self . http_conn_id ) api_key = conn . password if not api_key : raise AirflowException ( 'Opsgenie API Key is required for this hook, ' 'please check your conn_id configuration.' ) return api_key
def execute ( self , context ) : self . hook = OpsgenieAlertHook ( self . opsgenie_conn_id ) self . hook . execute ( self . _build_opsgenie_payload ( ) )
def get_conn ( self ) : if self . conn is None : cnopts = pysftp . CnOpts ( ) if self . no_host_key_check : cnopts . hostkeys = None cnopts . compression = self . compress conn_params = { 'host' : self . remote_host , 'port' : self . port , 'username' : self . username , 'cnopts' : cnopts } if self . password and self . password . strip ( ) : conn_params [ 'password' ] = self . password if self . key_file : conn_params [ 'private_key' ] = self . key_file if self . private_key_pass : conn_params [ 'private_key_pass' ] = self . private_key_pass self . conn = pysftp . Connection ( * * conn_params ) return self . conn
def execute ( self , context ) : s3_conn = S3Hook ( self . s3_conn_id ) if self . is_pipeline : results = MongoHook ( self . mongo_conn_id ) . aggregate ( mongo_collection = self . mongo_collection , aggregate_query = self . mongo_query , mongo_db = self . mongo_db ) else : results = MongoHook ( self . mongo_conn_id ) . find ( mongo_collection = self . mongo_collection , query = self . mongo_query , mongo_db = self . mongo_db ) docs_str = self . _stringify ( self . transform ( results ) ) s3_conn . load_string ( string_data = docs_str , key = self . s3_key , bucket_name = self . s3_bucket , replace = self . replace ) return True
def get_pool ( name , session = None ) : if not ( name and name . strip ( ) ) : raise AirflowBadRequest ( "Pool name shouldn't be empty" ) pool = session . query ( Pool ) . filter_by ( pool = name ) . first ( ) if pool is None : raise PoolNotFound ( "Pool '%s' doesn't exist" % name ) return pool
def create_pool ( name , slots , description , session = None ) : if not ( name and name . strip ( ) ) : raise AirflowBadRequest ( "Pool name shouldn't be empty" ) try : slots = int ( slots ) except ValueError : raise AirflowBadRequest ( "Bad value for `slots`: %s" % slots ) session . expire_on_commit = False pool = session . query ( Pool ) . filter_by ( pool = name ) . first ( ) if pool is None : pool = Pool ( pool = name , slots = slots , description = description ) session . add ( pool ) else : pool . slots = slots pool . description = description session . commit ( ) return pool
def delete_pool ( name , session = None ) : if not ( name and name . strip ( ) ) : raise AirflowBadRequest ( "Pool name shouldn't be empty" ) pool = session . query ( Pool ) . filter_by ( pool = name ) . first ( ) if pool is None : raise PoolNotFound ( "Pool '%s' doesn't exist" % name ) session . delete ( pool ) session . commit ( ) return pool
def execute ( self ) : proxies = { } if self . proxy : proxies = { 'https' : self . proxy } discord_payload = self . _build_discord_payload ( ) self . run ( endpoint = self . webhook_endpoint , data = discord_payload , headers = { 'Content-type' : 'application/json' } , extra_options = { 'proxies' : proxies } )
def close ( self ) : if self . closed : return super ( ) . close ( ) if not self . upload_on_close : return local_loc = os . path . join ( self . local_base , self . log_relative_path ) remote_loc = os . path . join ( self . remote_base , self . log_relative_path ) if os . path . exists ( local_loc ) : with open ( local_loc , 'r' ) as logfile : log = logfile . read ( ) self . s3_write ( log , remote_loc ) self . closed = True
def _get_init_containers ( self ) : if self . kube_config . dags_volume_claim or self . kube_config . dags_volume_host or self . kube_config . dags_in_image : return [ ] init_environment = [ { 'name' : 'GIT_SYNC_REPO' , 'value' : self . kube_config . git_repo } , { 'name' : 'GIT_SYNC_BRANCH' , 'value' : self . kube_config . git_branch } , { 'name' : 'GIT_SYNC_ROOT' , 'value' : self . kube_config . git_sync_root } , { 'name' : 'GIT_SYNC_DEST' , 'value' : self . kube_config . git_sync_dest } , { 'name' : 'GIT_SYNC_DEPTH' , 'value' : '1' } , { 'name' : 'GIT_SYNC_ONE_TIME' , 'value' : 'true' } ] if self . kube_config . git_user : init_environment . append ( { 'name' : 'GIT_SYNC_USERNAME' , 'value' : self . kube_config . git_user } ) if self . kube_config . git_password : init_environment . append ( { 'name' : 'GIT_SYNC_PASSWORD' , 'value' : self . kube_config . git_password } ) volume_mounts = [ { 'mountPath' : self . kube_config . git_sync_root , 'name' : self . dags_volume_name , 'readOnly' : False } ] if self . kube_config . git_ssh_key_secret_name : volume_mounts . append ( { 'name' : self . git_sync_ssh_secret_volume_name , 'mountPath' : '/etc/git-secret/ssh' , 'subPath' : 'ssh' } ) init_environment . extend ( [ { 'name' : 'GIT_SSH_KEY_FILE' , 'value' : '/etc/git-secret/ssh' } , { 'name' : 'GIT_SYNC_SSH' , 'value' : 'true' } ] ) if self . kube_config . git_ssh_known_hosts_configmap_name : volume_mounts . append ( { 'name' : self . git_sync_ssh_known_hosts_volume_name , 'mountPath' : '/etc/git-secret/known_hosts' , 'subPath' : 'known_hosts' } ) init_environment . extend ( [ { 'name' : 'GIT_KNOWN_HOSTS' , 'value' : 'true' } , { 'name' : 'GIT_SSH_KNOWN_HOSTS_FILE' , 'value' : '/etc/git-secret/known_hosts' } ] ) else : init_environment . append ( { 'name' : 'GIT_KNOWN_HOSTS' , 'value' : 'false' } ) return [ { 'name' : self . kube_config . git_sync_init_container_name , 'image' : self . kube_config . git_sync_container , 'securityContext' : { 'runAsUser' : 65533 } , 'env' : init_environment , 'volumeMounts' : volume_mounts } ]
def _get_environment ( self ) : env = { } for env_var_name , env_var_val in six . iteritems ( self . kube_config . kube_env_vars ) : env [ env_var_name ] = env_var_val env [ "AIRFLOW__CORE__EXECUTOR" ] = "LocalExecutor" if self . kube_config . airflow_configmap : env [ 'AIRFLOW_HOME' ] = self . worker_airflow_home env [ 'AIRFLOW__CORE__DAGS_FOLDER' ] = self . worker_airflow_dags if ( not self . kube_config . airflow_configmap and 'AIRFLOW__CORE__SQL_ALCHEMY_CONN' not in self . kube_config . kube_secrets ) : env [ 'AIRFLOW__CORE__SQL_ALCHEMY_CONN' ] = conf . get ( "core" , "SQL_ALCHEMY_CONN" ) if self . kube_config . git_dags_folder_mount_point : dag_volume_mount_path = os . path . join ( self . kube_config . git_dags_folder_mount_point , self . kube_config . git_sync_dest , self . kube_config . git_subpath ) env [ 'AIRFLOW__CORE__DAGS_FOLDER' ] = dag_volume_mount_path return env
def _get_secrets ( self ) : worker_secrets = [ ] for env_var_name , obj_key_pair in six . iteritems ( self . kube_config . kube_secrets ) : k8s_secret_obj , k8s_secret_key = obj_key_pair . split ( '=' ) worker_secrets . append ( Secret ( 'env' , env_var_name , k8s_secret_obj , k8s_secret_key ) ) if self . kube_config . env_from_secret_ref : for secret_ref in self . kube_config . env_from_secret_ref . split ( ',' ) : worker_secrets . append ( Secret ( 'env' , None , secret_ref ) ) return worker_secrets
def _get_security_context ( self ) : security_context = { } if self . kube_config . worker_run_as_user : security_context [ 'runAsUser' ] = self . kube_config . worker_run_as_user if self . kube_config . worker_fs_group : security_context [ 'fsGroup' ] = self . kube_config . worker_fs_group if self . kube_config . git_ssh_key_secret_name and security_context . get ( 'fsGroup' ) is None : security_context [ 'fsGroup' ] = 65533 return security_context
def start ( self ) : self . _process = DagFileProcessor . _launch_process ( self . _result_queue , self . file_path , self . _pickle_dags , self . _dag_id_white_list , "DagFileProcessor{}" . format ( self . _instance_id ) , self . _zombies ) self . _start_time = timezone . utcnow ( )
def _exit_gracefully ( self , signum , frame ) : self . log . info ( "Exiting gracefully upon receiving signal %s" , signum ) if self . processor_agent : self . processor_agent . end ( ) sys . exit ( os . EX_OK )
def _process_executor_events ( self , simple_dag_bag , session = None ) : TI = models . TaskInstance for key , state in list ( self . executor . get_event_buffer ( simple_dag_bag . dag_ids ) . items ( ) ) : dag_id , task_id , execution_date , try_number = key self . log . info ( "Executor reports execution of %s.%s execution_date=%s " "exited with status %s for try_number %s" , dag_id , task_id , execution_date , state , try_number ) if state == State . FAILED or state == State . SUCCESS : qry = session . query ( TI ) . filter ( TI . dag_id == dag_id , TI . task_id == task_id , TI . execution_date == execution_date ) ti = qry . first ( ) if not ti : self . log . warning ( "TaskInstance %s went missing from the database" , ti ) continue if ti . try_number == try_number and ti . state == State . QUEUED : msg = ( "Executor reports task instance {} finished ({}) " "although the task says its {}. Was the task " "killed externally?" . format ( ti , state , ti . state ) ) self . log . error ( msg ) try : simple_dag = simple_dag_bag . get_dag ( dag_id ) dagbag = models . DagBag ( simple_dag . full_filepath ) dag = dagbag . get_dag ( dag_id ) ti . task = dag . get_task ( task_id ) ti . handle_failure ( msg ) except Exception : self . log . error ( "Cannot load the dag bag to handle failure for %s" ". Setting task to FAILED without callbacks or " "retries. Do you have enough resources?" , ti ) ti . state = State . FAILED session . merge ( ti ) session . commit ( )
def heartbeat_callback ( self , session = None ) : if self . terminating : self . task_runner . terminate ( ) return self . task_instance . refresh_from_db ( ) ti = self . task_instance fqdn = get_hostname ( ) same_hostname = fqdn == ti . hostname same_process = ti . pid == os . getpid ( ) if ti . state == State . RUNNING : if not same_hostname : self . log . warning ( "The recorded hostname %s " "does not match this instance's hostname " "%s" , ti . hostname , fqdn ) raise AirflowException ( "Hostname of job runner does not match" ) elif not same_process : current_pid = os . getpid ( ) self . log . warning ( "Recorded pid %s does not match " "the current pid %s" , ti . pid , current_pid ) raise AirflowException ( "PID of job runner does not match" ) elif ( self . task_runner . return_code ( ) is None and hasattr ( self . task_runner , 'process' ) ) : self . log . warning ( "State of this instance has been externally set to %s. " "Taking the poison pill." , ti . state ) self . task_runner . terminate ( ) self . terminating = True
def get_conn ( self ) : if self . session and not self . session . is_shutdown : return self . session self . session = self . cluster . connect ( self . keyspace ) return self . session
def _query_mysql ( self ) : mysql = MySqlHook ( mysql_conn_id = self . mysql_conn_id ) conn = mysql . get_conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql ) return cursor
def _get_col_type_dict ( self ) : schema = [ ] if isinstance ( self . schema , string_types ) : schema = json . loads ( self . schema ) elif isinstance ( self . schema , list ) : schema = self . schema elif self . schema is not None : self . log . warn ( 'Using default schema due to unexpected type.' 'Should be a string or list.' ) col_type_dict = { } try : col_type_dict = { col [ 'name' ] : col [ 'type' ] for col in schema } except KeyError : self . log . warn ( 'Using default schema due to missing name or type. Please ' 'refer to: https://cloud.google.com/bigquery/docs/schemas' '#specifying_a_json_schema_file' ) return col_type_dict
def extra_dejson ( self ) : obj = { } if self . extra : try : obj = json . loads ( self . extra ) except Exception as e : self . log . exception ( e ) self . log . error ( "Failed parsing the json for conn_id %s" , self . conn_id ) return obj
def scale_time_units ( time_seconds_arr , unit ) : if unit == 'minutes' : return list ( map ( lambda x : x * 1.0 / 60 , time_seconds_arr ) ) elif unit == 'hours' : return list ( map ( lambda x : x * 1.0 / ( 60 * 60 ) , time_seconds_arr ) ) elif unit == 'days' : return list ( map ( lambda x : x * 1.0 / ( 24 * 60 * 60 ) , time_seconds_arr ) ) return time_seconds_arr
def get_all_permissions_views ( self ) : perms_views = set ( ) for role in self . get_user_roles ( ) : perms_views . update ( { ( perm_view . permission . name , perm_view . view_menu . name ) for perm_view in role . permissions } ) return perms_views
def _has_role ( self , role_name_or_list ) : if not isinstance ( role_name_or_list , list ) : role_name_or_list = [ role_name_or_list ] return any ( [ r . name in role_name_or_list for r in self . get_user_roles ( ) ] )
def _has_perm ( self , permission_name , view_menu_name ) : if hasattr ( self , 'perms' ) : if ( permission_name , view_menu_name ) in self . perms : return True self . _get_and_cache_perms ( ) return ( permission_name , view_menu_name ) in self . perms
def clean_perms ( self ) : self . log . debug ( 'Cleaning faulty perms' ) sesh = self . get_session pvms = ( sesh . query ( sqla_models . PermissionView ) . filter ( or_ ( sqla_models . PermissionView . permission == None , sqla_models . PermissionView . view_menu == None , ) ) ) deleted_count = pvms . delete ( ) sesh . commit ( ) if deleted_count : self . log . info ( 'Deleted %s faulty permissions' , deleted_count )
def create_perm_vm_for_all_dag ( self ) : for dag_vm in self . DAG_VMS : for perm in self . DAG_PERMS : self . _merge_perm ( permission_name = perm , view_menu_name = dag_vm )
def poke ( self , context ) : if '.' in self . table_name : self . database_name , self . table_name = self . table_name . split ( '.' ) self . log . info ( 'Poking for table %s. %s, expression %s' , self . database_name , self . table_name , self . expression ) return self . get_hook ( ) . check_for_partition ( self . database_name , self . table_name , self . expression )
def get_conn ( self ) : effective_user = self . proxy_user autoconfig = self . autoconfig use_sasl = configuration . conf . get ( 'core' , 'security' ) == 'kerberos' try : connections = self . get_connections ( self . hdfs_conn_id ) if not effective_user : effective_user = connections [ 0 ] . login if not autoconfig : autoconfig = connections [ 0 ] . extra_dejson . get ( 'autoconfig' , False ) hdfs_namenode_principal = connections [ 0 ] . extra_dejson . get ( 'hdfs_namenode_principal' ) except AirflowException : if not autoconfig : raise if autoconfig : client = AutoConfigClient ( effective_user = effective_user , use_sasl = use_sasl ) elif len ( connections ) == 1 : client = Client ( connections [ 0 ] . host , connections [ 0 ] . port , effective_user = effective_user , use_sasl = use_sasl , hdfs_namenode_principal = hdfs_namenode_principal ) elif len ( connections ) > 1 : nn = [ Namenode ( conn . host , conn . port ) for conn in connections ] client = HAClient ( nn , effective_user = effective_user , use_sasl = use_sasl , hdfs_namenode_principal = hdfs_namenode_principal ) else : raise HDFSHookException ( "conn_id doesn't exist in the repository " "and autoconfig is not specified" ) return client
def get_conn ( self ) : conn = self . get_connection ( self . pinot_broker_conn_id ) pinot_broker_conn = connect ( host = conn . host , port = conn . port , path = conn . extra_dejson . get ( 'endpoint' , '/pql' ) , scheme = conn . extra_dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to pinot ' 'broker on {host}' . format ( host = conn . host ) ) return pinot_broker_conn
def _convert_date_to_dict ( field_date ) : return { DAY : field_date . day , MONTH : field_date . month , YEAR : field_date . year }
def _convert_time_to_dict ( time ) : return { HOURS : time . hour , MINUTES : time . minute , SECONDS : time . second }
def get_conn ( self ) : conn = self . get_connection ( self . redis_conn_id ) self . host = conn . host self . port = conn . port self . password = None if str ( conn . password ) . lower ( ) in [ 'none' , 'false' , '' ] else conn . password self . db = conn . extra_dejson . get ( 'db' , None ) if not self . redis : self . log . debug ( 'Initializing redis object for conn_id "%s" on %s:%s:%s' , self . redis_conn_id , self . host , self . port , self . db ) self . redis = Redis ( host = self . host , port = self . port , password = self . password , db = self . db ) return self . redis
def get_conn ( self ) : db = self . get_connection ( getattr ( self , self . conn_name_attr ) ) return self . connector . connect ( host = db . host , port = db . port , username = db . login , schema = db . schema )
def set_autocommit ( self , conn , autocommit ) : if not self . supports_autocommit and autocommit : self . log . warn ( ( "%s connection doesn't support " "autocommit but autocommit activated." ) , getattr ( self , self . conn_name_attr ) ) conn . autocommit = autocommit
def get_query ( self ) : return ( super ( ) . get_query ( ) . filter ( or_ ( models . DagModel . is_active , models . DagModel . is_paused ) ) . filter ( ~ models . DagModel . is_subdag ) )
def get_count_query ( self ) : return ( super ( ) . get_count_query ( ) . filter ( models . DagModel . is_active ) . filter ( ~ models . DagModel . is_subdag ) )
def execute ( self , context ) : self . hook = SlackWebhookHook ( self . http_conn_id , self . webhook_token , self . message , self . attachments , self . channel , self . username , self . icon_emoji , self . link_names , self . proxy ) self . hook . execute ( )
def _get_credentials ( self ) : key_path = self . _get_field ( 'key_path' , False ) keyfile_dict = self . _get_field ( 'keyfile_dict' , False ) scope = self . _get_field ( 'scope' , None ) if scope : scopes = [ s . strip ( ) for s in scope . split ( ',' ) ] else : scopes = _DEFAULT_SCOPES if not key_path and not keyfile_dict : self . log . info ( 'Getting connection using `google.auth.default()` ' 'since no key file is defined for hook.' ) credentials , _ = google . auth . default ( scopes = scopes ) elif key_path : if key_path . endswith ( '.json' ) : self . log . debug ( 'Getting connection using JSON key file %s' % key_path ) credentials = ( google . oauth2 . service_account . Credentials . from_service_account_file ( key_path , scopes = scopes ) ) elif key_path . endswith ( '.p12' ) : raise AirflowException ( 'Legacy P12 key file are not supported, ' 'use a JSON key file.' ) else : raise AirflowException ( 'Unrecognised extension for key file.' ) else : try : keyfile_dict = json . loads ( keyfile_dict ) keyfile_dict [ 'private_key' ] = keyfile_dict [ 'private_key' ] . replace ( '\\n' , '\n' ) credentials = ( google . oauth2 . service_account . Credentials . from_service_account_info ( keyfile_dict , scopes = scopes ) ) except json . decoder . JSONDecodeError : raise AirflowException ( 'Invalid key JSON.' ) return credentials . with_subject ( self . delegate_to ) if self . delegate_to else credentials
def read_image_file ( data_dir , image_ext , n ) : def PIL2array ( _img ) : return np . array ( _img . getdata ( ) , dtype = np . uint8 ) . reshape ( 64 , 64 ) def find_files ( _data_dir , _image_ext ) : files = [ ] for file_dir in os . listdir ( _data_dir ) : if file_dir . endswith ( _image_ext ) : files . append ( os . path . join ( _data_dir , file_dir ) ) return sorted ( files ) patches = [ ] list_files = find_files ( data_dir , image_ext ) for fpath in list_files : img = Image . open ( fpath ) for y in range ( 0 , 1024 , 64 ) : for x in range ( 0 , 1024 , 64 ) : patch = img . crop ( ( x , y , x + 64 , y + 64 ) ) patches . append ( PIL2array ( patch ) ) return torch . ByteTensor ( np . array ( patches [ : n ] ) )
def accuracy ( output , target , topk = ( 1 , ) ) : with torch . no_grad ( ) : maxk = max ( topk ) batch_size = target . size ( 0 ) _ , pred = output . topk ( maxk , 1 , True , True ) pred = pred . t ( ) correct = pred . eq ( target [ None ] ) res = [ ] for k in topk : correct_k = correct [ : k ] . flatten ( ) . sum ( dtype = torch . float32 ) res . append ( correct_k * ( 100.0 / batch_size ) ) return res
def setup_for_distributed ( is_master ) : import builtins as __builtin__ builtin_print = __builtin__ . print def print ( * args , * * kwargs ) : force = kwargs . pop ( 'force' , False ) if is_master or force : builtin_print ( * args , * * kwargs ) __builtin__ . print = print
def download ( self ) : import tarfile if self . _check_integrity ( ) : print ( 'Files already downloaded and verified' ) return download_url ( self . url , self . root , self . filename , self . md5_checksum ) with tarfile . open ( os . path . join ( self . root , self . filename ) , 'r:gz' ) as tar : tar . extractall ( path = self . root ) with open ( os . path . join ( self . root , 'dataset' , 'SBU_captioned_photo_dataset_urls.txt' ) ) as fh : for line in fh : url = line . rstrip ( ) try : download_url ( url , os . path . join ( self . root , 'dataset' ) ) except OSError : pass
def download ( self ) : if self . _check_exists ( ) : return makedir_exist_ok ( self . raw_folder ) makedir_exist_ok ( self . processed_folder ) for url in self . urls : filename = url . rpartition ( '/' ) [ 2 ] file_path = os . path . join ( self . raw_folder , filename ) download_url ( url , root = self . raw_folder , filename = filename , md5 = None ) self . extract_gzip ( gzip_path = file_path , remove_finished = True ) print ( 'Processing...' ) training_set = ( read_image_file ( os . path . join ( self . raw_folder , 'train-images-idx3-ubyte' ) ) , read_label_file ( os . path . join ( self . raw_folder , 'train-labels-idx1-ubyte' ) ) ) test_set = ( read_image_file ( os . path . join ( self . raw_folder , 't10k-images-idx3-ubyte' ) ) , read_label_file ( os . path . join ( self . raw_folder , 't10k-labels-idx1-ubyte' ) ) ) with open ( os . path . join ( self . processed_folder , self . training_file ) , 'wb' ) as f : torch . save ( training_set , f ) with open ( os . path . join ( self . processed_folder , self . test_file ) , 'wb' ) as f : torch . save ( test_set , f ) print ( 'Done!' )
def download ( self ) : import shutil import zipfile if self . _check_exists ( ) : return makedir_exist_ok ( self . raw_folder ) makedir_exist_ok ( self . processed_folder ) filename = self . url . rpartition ( '/' ) [ 2 ] file_path = os . path . join ( self . raw_folder , filename ) download_url ( self . url , root = self . raw_folder , filename = filename , md5 = None ) print ( 'Extracting zip archive' ) with zipfile . ZipFile ( file_path ) as zip_f : zip_f . extractall ( self . raw_folder ) os . unlink ( file_path ) gzip_folder = os . path . join ( self . raw_folder , 'gzip' ) for gzip_file in os . listdir ( gzip_folder ) : if gzip_file . endswith ( '.gz' ) : self . extract_gzip ( gzip_path = os . path . join ( gzip_folder , gzip_file ) ) for split in self . splits : print ( 'Processing ' + split ) training_set = ( read_image_file ( os . path . join ( gzip_folder , 'emnist-{}-train-images-idx3-ubyte' . format ( split ) ) ) , read_label_file ( os . path . join ( gzip_folder , 'emnist-{}-train-labels-idx1-ubyte' . format ( split ) ) ) ) test_set = ( read_image_file ( os . path . join ( gzip_folder , 'emnist-{}-test-images-idx3-ubyte' . format ( split ) ) ) , read_label_file ( os . path . join ( gzip_folder , 'emnist-{}-test-labels-idx1-ubyte' . format ( split ) ) ) ) with open ( os . path . join ( self . processed_folder , self . _training_file ( split ) ) , 'wb' ) as f : torch . save ( training_set , f ) with open ( os . path . join ( self . processed_folder , self . _test_file ( split ) ) , 'wb' ) as f : torch . save ( test_set , f ) shutil . rmtree ( gzip_folder ) print ( 'Done!' )
def preferences ( ) : if request . method == 'POST' : resp = make_response ( redirect ( urljoin ( settings [ 'server' ] [ 'base_url' ] , url_for ( 'index' ) ) ) ) try : request . preferences . parse_form ( request . form ) except ValidationException : request . errors . append ( gettext ( 'Invalid settings, please edit your preferences' ) ) return resp return request . preferences . save ( resp ) image_proxy = request . preferences . get_value ( 'image_proxy' ) lang = request . preferences . get_value ( 'language' ) disabled_engines = request . preferences . engines . get_disabled ( ) allowed_plugins = request . preferences . plugins . get_enabled ( ) stats = { } for c in categories : for e in categories [ c ] : stats [ e . name ] = { 'time' : None , 'warn_timeout' : False , 'warn_time' : False } if e . timeout > settings [ 'outgoing' ] [ 'request_timeout' ] : stats [ e . name ] [ 'warn_timeout' ] = True stats [ e . name ] [ 'supports_selected_language' ] = _is_selected_language_supported ( e , request . preferences ) for engine_stat in get_engines_stats ( ) [ 0 ] [ 1 ] : stats [ engine_stat . get ( 'name' ) ] [ 'time' ] = round ( engine_stat . get ( 'avg' ) , 3 ) if engine_stat . get ( 'avg' ) > settings [ 'outgoing' ] [ 'request_timeout' ] : stats [ engine_stat . get ( 'name' ) ] [ 'warn_time' ] = True return render ( 'preferences.html' , locales = settings [ 'locales' ] , current_locale = get_locale ( ) , image_proxy = image_proxy , engines_by_category = categories , stats = stats , answerers = [ { 'info' : a . self_info ( ) , 'keywords' : a . keywords } for a in answerers ] , disabled_engines = disabled_engines , autocomplete_backends = autocomplete_backends , shortcuts = { y : x for x , y in engine_shortcuts . items ( ) } , themes = themes , plugins = plugins , doi_resolvers = settings [ 'doi_resolvers' ] , current_doi_resolver = get_doi_resolver ( request . args , request . preferences . get_value ( 'doi_resolver' ) ) , allowed_plugins = allowed_plugins , theme = get_current_theme_name ( ) , preferences_url_params = request . preferences . get_as_url_params ( ) , base_url = get_base_url ( ) , preferences = True )
def get_themes ( templates_path ) : themes = os . listdir ( templates_path ) if '__common__' in themes : themes . remove ( '__common__' ) return themes
def searx_bang ( full_query ) : if len ( full_query . getSearchQuery ( ) ) == 0 : return [ ] results = [ ] first_char = full_query . getSearchQuery ( ) [ 0 ] if first_char == '!' or first_char == '?' : if len ( full_query . getSearchQuery ( ) ) == 1 : results . append ( first_char + "images" ) results . append ( first_char + "wikipedia" ) results . append ( first_char + "osm" ) else : engine_query = full_query . getSearchQuery ( ) [ 1 : ] for categorie in categories : if categorie . startswith ( engine_query ) : results . append ( first_char + '{categorie}' . format ( categorie = categorie ) ) for engine in engines : if engine . startswith ( engine_query . replace ( '_' , ' ' ) ) : results . append ( first_char + '{engine}' . format ( engine = engine . replace ( ' ' , '_' ) ) ) for engine_shortcut in engine_shortcuts : if engine_shortcut . startswith ( engine_query ) : results . append ( first_char + '{engine_shortcut}' . format ( engine_shortcut = engine_shortcut ) ) elif first_char == ':' : if len ( full_query . getSearchQuery ( ) ) == 1 : results . append ( ":en" ) results . append ( ":en_us" ) results . append ( ":english" ) results . append ( ":united_kingdom" ) else : engine_query = full_query . getSearchQuery ( ) [ 1 : ] for lc in language_codes : lang_id , lang_name , country , english_name = map ( unicode . lower , lc ) if lang_id . startswith ( engine_query ) : if len ( engine_query ) <= 2 : results . append ( u':{lang_id}' . format ( lang_id = lang_id . split ( '-' ) [ 0 ] ) ) else : results . append ( u':{lang_id}' . format ( lang_id = lang_id ) ) if lang_name . startswith ( engine_query ) or english_name . startswith ( engine_query ) : results . append ( u':{lang_name}' . format ( lang_name = lang_name ) ) if country . startswith ( engine_query . replace ( '_' , ' ' ) ) : results . append ( u':{country}' . format ( country = country . replace ( ' ' , '_' ) ) ) result_set = set ( results ) for query_part in full_query . query_parts : if query_part in result_set : result_set . remove ( query_part ) return list ( result_set )
def response ( resp ) : json_resp = resp . text [ resp . text . find ( '\n' ) + 1 : resp . text . rfind ( '\n' ) - 2 ] results = [ ] try : conversion_rate = float ( json . loads ( json_resp ) [ 'conversion' ] [ 'converted-amount' ] ) except : return results answer = '{0} {1} = {2} {3}, 1 {1} ({5}) = {4} {3} ({6})' . format ( resp . search_params [ 'amount' ] , resp . search_params [ 'from' ] , resp . search_params [ 'amount' ] * conversion_rate , resp . search_params [ 'to' ] , conversion_rate , resp . search_params [ 'from_name' ] , resp . search_params [ 'to_name' ] , ) url = 'https://duckduckgo.com/js/spice/currency/1/{0}/{1}' . format ( resp . search_params [ 'from' ] . upper ( ) , resp . search_params [ 'to' ] ) results . append ( { 'answer' : answer , 'url' : url } ) return results
def mvn ( * args , * * kwargs ) : return tfd . Independent ( tfd . Normal ( * args , * * kwargs ) , reinterpreted_batch_ndims = 1 )
def eight_schools_joint_log_prob ( treatment_effects , treatment_stddevs , avg_effect , avg_stddev , school_effects_standard ) : rv_avg_effect = tfd . Normal ( loc = 0. , scale = 10. ) rv_avg_stddev = tfd . Normal ( loc = 5. , scale = 1. ) rv_school_effects_standard = mvn ( loc = tf . zeros_like ( school_effects_standard ) , scale = tf . ones_like ( school_effects_standard ) ) rv_treatment_effects = mvn ( loc = ( avg_effect + tf . exp ( avg_stddev ) * school_effects_standard ) , scale = treatment_stddevs ) return ( rv_avg_effect . log_prob ( avg_effect ) + rv_avg_stddev . log_prob ( avg_stddev ) + rv_school_effects_standard . log_prob ( school_effects_standard ) + rv_treatment_effects . log_prob ( treatment_effects ) )
def benchmark_eight_schools_hmc ( num_results = int ( 5e3 ) , num_burnin_steps = int ( 3e3 ) , num_leapfrog_steps = 3 , step_size = 0.4 ) : num_schools = 8 treatment_effects = tf . constant ( [ 28 , 8 , - 3 , 7 , - 1 , 1 , 18 , 12 ] , dtype = np . float32 , name = 'treatment_effects' ) treatment_stddevs = tf . constant ( [ 15 , 10 , 16 , 11 , 9 , 11 , 10 , 18 ] , dtype = np . float32 , name = 'treatment_stddevs' ) def unnormalized_posterior_log_prob ( avg_effect , avg_stddev , school_effects_standard ) : """Eight-schools unnormalized log posterior.""" return eight_schools_joint_log_prob ( treatment_effects , treatment_stddevs , avg_effect , avg_stddev , school_effects_standard ) if tf . executing_eagerly ( ) : sample_chain = tf . function ( tfp . mcmc . sample_chain ) else : sample_chain = tfp . mcmc . sample_chain def computation ( ) : """The benchmark computation.""" _ , kernel_results = sample_chain ( num_results = num_results , num_burnin_steps = num_burnin_steps , current_state = ( tf . zeros ( [ ] , name = 'init_avg_effect' ) , tf . zeros ( [ ] , name = 'init_avg_stddev' ) , tf . ones ( [ num_schools ] , name = 'init_school_effects_standard' ) , ) , kernel = tfp . mcmc . HamiltonianMonteCarlo ( target_log_prob_fn = unnormalized_posterior_log_prob , step_size = step_size , num_leapfrog_steps = num_leapfrog_steps ) ) return kernel_results . is_accepted is_accepted_tensor = computation ( ) if not tf . executing_eagerly ( ) : session = tf . compat . v1 . Session ( ) session . run ( is_accepted_tensor ) start_time = time . time ( ) if tf . executing_eagerly ( ) : is_accepted = computation ( ) else : is_accepted = session . run ( is_accepted_tensor ) wall_time = time . time ( ) - start_time num_accepted = np . sum ( is_accepted ) acceptance_rate = np . float32 ( num_accepted ) / np . float32 ( num_results ) return dict ( iters = ( num_results + num_burnin_steps ) * num_leapfrog_steps , extras = { 'acceptance_rate' : acceptance_rate } , wall_time = wall_time )
def _build_custom_rv ( distribution , sample_shape , value , name ) : del name return RandomVariable ( distribution = distribution , sample_shape = sample_shape , value = value )
def _make_random_variable ( distribution_cls ) : @ interceptable @ functools . wraps ( distribution_cls , assigned = ( '__module__' , '__name__' ) ) @ docstring_util . expand_docstring ( cls = distribution_cls . __name__ , doc = inspect . cleandoc ( distribution_cls . __init__ . __doc__ or '' ) ) def func ( * args , * * kwargs ) : sample_shape = kwargs . pop ( 'sample_shape' , ( ) ) value = kwargs . pop ( 'value' , None ) return RandomVariable ( distribution = distribution_cls ( * args , * * kwargs ) , sample_shape = sample_shape , value = value ) return func
def _max_mask_non_finite ( x , axis = - 1 , keepdims = False , mask = 0 ) : m = np . max ( x , axis = _astuple ( axis ) , keepdims = keepdims ) needs_masking = ~ np . isfinite ( m ) if needs_masking . ndim > 0 : m [ needs_masking ] = mask elif needs_masking : m = mask return m
def _eval_all_one_hot ( fn , dist , name = None ) : with tf . compat . v1 . name_scope ( name , 'eval_all_one_hot' ) : event_size = dist . event_shape_tensor ( ) [ - 1 ] batch_ndims = tf . size ( input = dist . batch_shape_tensor ( ) ) x = tf . reshape ( tf . eye ( event_size , dtype = dist . dtype ) , shape = tf . pad ( tensor = tf . ones ( batch_ndims , tf . int32 ) , paddings = [ [ 1 , 1 ] ] , constant_values = event_size ) ) perm = tf . pad ( tensor = tf . range ( 1 , batch_ndims + 1 ) , paddings = [ [ 0 , 1 ] ] ) return tf . transpose ( a = fn ( dist , x ) , perm = perm )
def _get_convert_to_tensor_fn ( identifier ) : if identifier is None : return None if isinstance ( identifier , six . string_types ) : identifier = str ( identifier ) return _deserialize ( identifier ) if isinstance ( identifier , dict ) : return _deserialize ( identifier ) if isinstance ( identifier , property ) : identifier = identifier . fget if callable ( identifier ) : return identifier raise ValueError ( 'Could not interpret ' 'convert-to-tensor function identifier:' , identifier )
def new ( params , event_size , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'MultivariateNormalTriL' , [ params , event_size ] ) : params = tf . convert_to_tensor ( value = params , name = 'params' ) scale_tril = tfb . ScaleTriL ( diag_shift = np . array ( 1e-5 , params . dtype . as_numpy_dtype ( ) ) , validate_args = validate_args ) return tfd . MultivariateNormalTriL ( loc = params [ ... , : event_size ] , scale_tril = scale_tril ( params [ ... , event_size : ] ) , validate_args = validate_args )
def params_size ( event_size , name = None ) : with tf . compat . v1 . name_scope ( name , 'MultivariateNormalTriL_params_size' , [ event_size ] ) : return event_size + event_size * ( event_size + 1 ) // 2
def new ( params , event_size , dtype = None , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'OneHotCategorical' , [ params , event_size ] ) : return tfd . OneHotCategorical ( logits = params , dtype = dtype or params . dtype . base_dtype , validate_args = validate_args )
def new ( params , event_size , num_components , dtype = None , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'CategoricalMixtureOfOneHotCategorical' , [ params , event_size , num_components ] ) : dist = MixtureSameFamily . new ( params , num_components , OneHotCategorical ( event_size , validate_args = False , name = name ) , validate_args = validate_args , name = name ) dist . _mean = functools . partial ( _eval_all_one_hot , tfd . Distribution . prob , dist ) dist . log_mean = functools . partial ( _eval_all_one_hot , tfd . Distribution . log_prob , dist ) return dist
def params_size ( event_size , num_components , name = None ) : with tf . compat . v1 . name_scope ( name , 'CategoricalMixtureOfOneHotCategorical_params_size' , [ event_size , num_components ] ) : return MixtureSameFamily . params_size ( num_components , OneHotCategorical . params_size ( event_size , name = name ) , name = name )
def new ( params , event_shape = ( ) , dtype = None , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'IndependentBernoulli' , [ params , event_shape ] ) : params = tf . convert_to_tensor ( value = params , name = 'params' ) event_shape = dist_util . expand_to_vector ( tf . convert_to_tensor ( value = event_shape , name = 'event_shape' , dtype_hint = tf . int32 ) , tensor_name = 'event_shape' ) new_shape = tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , event_shape , ] , axis = 0 ) dist = tfd . Independent ( tfd . Bernoulli ( logits = tf . reshape ( params , new_shape ) , dtype = dtype or params . dtype . base_dtype , validate_args = validate_args ) , reinterpreted_batch_ndims = tf . size ( input = event_shape ) , validate_args = validate_args ) dist . _logits = dist . distribution . _logits dist . _probs = dist . distribution . _probs dist . logits = tfd . Bernoulli . logits dist . probs = tfd . Bernoulli . probs return dist
def new ( params , event_shape = ( ) , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'IndependentLogistic' , [ params , event_shape ] ) : params = tf . convert_to_tensor ( value = params , name = 'params' ) event_shape = dist_util . expand_to_vector ( tf . convert_to_tensor ( value = event_shape , name = 'event_shape' , dtype_hint = tf . int32 ) , tensor_name = 'event_shape' ) output_shape = tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , event_shape , ] , axis = 0 ) loc_params , scale_params = tf . split ( params , 2 , axis = - 1 ) return tfd . Independent ( tfd . Logistic ( loc = tf . reshape ( loc_params , output_shape ) , scale = tf . math . softplus ( tf . reshape ( scale_params , output_shape ) ) , validate_args = validate_args ) , reinterpreted_batch_ndims = tf . size ( input = event_shape ) , validate_args = validate_args )
def params_size ( event_shape = ( ) , name = None ) : with tf . compat . v1 . name_scope ( name , 'IndependentNormal_params_size' , [ event_shape ] ) : event_shape = tf . convert_to_tensor ( value = event_shape , name = 'event_shape' , dtype_hint = tf . int32 ) return 2 * _event_size ( event_shape , name = name or 'IndependentNormal_params_size' )
def new ( params , event_shape = ( ) , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'IndependentPoisson' , [ params , event_shape ] ) : params = tf . convert_to_tensor ( value = params , name = 'params' ) event_shape = dist_util . expand_to_vector ( tf . convert_to_tensor ( value = event_shape , name = 'event_shape' , dtype_hint = tf . int32 ) , tensor_name = 'event_shape' ) output_shape = tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , event_shape , ] , axis = 0 ) return tfd . Independent ( tfd . Poisson ( log_rate = tf . reshape ( params , output_shape ) , validate_args = validate_args ) , reinterpreted_batch_ndims = tf . size ( input = event_shape ) , validate_args = validate_args )
def new ( params , num_components , component_layer , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'MixtureSameFamily' , [ params , num_components , component_layer ] ) : params = tf . convert_to_tensor ( value = params , name = 'params' ) num_components = tf . convert_to_tensor ( value = num_components , name = 'num_components' , dtype_hint = tf . int32 ) components_dist = component_layer ( tf . reshape ( params [ ... , num_components : ] , tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , [ num_components , - 1 ] ] , axis = 0 ) ) ) mixture_dist = tfd . Categorical ( logits = params [ ... , : num_components ] ) return tfd . MixtureSameFamily ( mixture_dist , components_dist , validate_args = False )
def params_size ( num_components , event_shape = ( ) , name = None ) : return MixtureSameFamily . params_size ( num_components , IndependentNormal . params_size ( event_shape , name = name ) , name = name )
def new ( params , num_components , event_shape = ( ) , validate_args = False , name = None ) : return MixtureSameFamily . new ( params , num_components , IndependentLogistic ( event_shape , validate_args = validate_args , name = name ) , validate_args = validate_args , name = name )
def params_size ( num_components , event_shape = ( ) , name = None ) : return MixtureSameFamily . params_size ( num_components , IndependentLogistic . params_size ( event_shape , name = name ) , name = name )
def _maybe_check_valid_map_values ( map_values , validate_args ) : assertions = [ ] message = 'Rank of map_values must be 1.' if tensorshape_util . rank ( map_values . shape ) is not None : if tensorshape_util . rank ( map_values . shape ) != 1 : raise ValueError ( message ) elif validate_args : assertions . append ( assert_util . assert_rank ( map_values , 1 , message = message ) ) message = 'Size of map_values must be greater than 0.' if tensorshape_util . num_elements ( map_values . shape ) is not None : if tensorshape_util . num_elements ( map_values . shape ) == 0 : raise ValueError ( message ) elif validate_args : assertions . append ( assert_util . assert_greater ( tf . size ( input = map_values ) , 0 , message = message ) ) if validate_args : assertions . append ( assert_util . assert_equal ( tf . math . is_strictly_increasing ( map_values ) , True , message = 'map_values is not strictly increasing.' ) ) return assertions
def _as_tensor ( x , name , dtype ) : return None if x is None else tf . convert_to_tensor ( value = x , name = name , dtype = dtype )
def _get_default_reinterpreted_batch_ndims ( self , distribution ) : ndims = prefer_static . rank_from_shape ( distribution . batch_shape_tensor , distribution . batch_shape ) return prefer_static . maximum ( 0 , ndims - 1 )
def _cat_probs ( self , log_probs ) : which_softmax = tf . nn . log_softmax if log_probs else tf . nn . softmax cat_probs = which_softmax ( self . cat . logits ) cat_probs = tf . unstack ( cat_probs , num = self . num_components , axis = - 1 ) return cat_probs
def _maybe_validate_args ( outcomes , logits , probs , validate_args ) : assertions = [ ] def validate_equal_last_dim ( tensor_a , tensor_b , message ) : if tensor_a . shape . is_fully_defined ( ) and tensor_b . shape . is_fully_defined ( ) : if tensor_a . shape [ - 1 ] != tensor_b . shape [ - 1 ] : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_equal ( tf . shape ( input = tensor_a ) [ - 1 ] , tf . shape ( input = tensor_b ) [ - 1 ] , message = message ) ) if logits is not None : validate_equal_last_dim ( outcomes , logits , message = 'Last dimension of outcomes and logits must be equal size.' ) if probs is not None : validate_equal_last_dim ( outcomes , probs , message = 'Last dimension of outcomes and probs must be equal size.' ) message = 'Rank of outcomes must be 1.' if outcomes . shape . ndims is not None : if outcomes . shape . ndims != 1 : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_rank ( outcomes , 1 , message = message ) ) message = 'Size of outcomes must be greater than 0.' if outcomes . shape . num_elements ( ) is not None : if outcomes . shape . num_elements ( ) == 0 : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_greater ( tf . size ( input = outcomes ) , 0 , message = message ) ) if validate_args : assertions . append ( tf . compat . v1 . assert_equal ( tf . math . is_strictly_increasing ( outcomes ) , True , message = 'outcomes is not strictly increasing.' ) ) return assertions
def logistic_regression ( features ) : coeffs = ed . MultivariateNormalDiag ( loc = tf . zeros ( features . shape [ 1 ] ) , name = "coeffs" ) labels = ed . Bernoulli ( logits = tf . tensordot ( features , coeffs , [ [ 1 ] , [ 0 ] ] ) , name = "labels" ) return labels
def covertype ( ) : import sklearn . datasets data = sklearn . datasets . covtype . fetch_covtype ( ) features = data . data labels = data . target features -= features . mean ( 0 ) features /= features . std ( 0 ) features = np . hstack ( [ features , np . ones ( [ features . shape [ 0 ] , 1 ] ) ] ) features = tf . cast ( features , dtype = tf . float32 ) _ , counts = np . unique ( labels , return_counts = True ) specific_category = np . argmax ( counts ) labels = ( labels == specific_category ) labels = tf . cast ( labels , dtype = tf . int32 ) return features , labels
def _maybe_assert_valid_concentration ( self , concentration , validate_args ) : if not validate_args : return concentration return distribution_util . with_dependencies ( [ assert_util . assert_positive ( concentration , message = "Concentration parameter must be positive." ) , assert_util . assert_rank_at_least ( concentration , 1 , message = "Concentration parameter must have >=1 dimensions." ) , assert_util . assert_less ( 1 , tf . shape ( input = concentration ) [ - 1 ] , message = "Concentration parameter must have event_size >= 2." ) , ] , concentration )
def _maybe_assert_valid_sample ( self , x ) : if not self . validate_args : return x return distribution_util . with_dependencies ( [ assert_util . assert_positive ( x , message = "samples must be positive" ) , assert_util . assert_near ( tf . ones ( [ ] , dtype = self . dtype ) , tf . reduce_sum ( input_tensor = x , axis = - 1 ) , message = "sample last-dimension must sum to `1`" ) , ] , x )
def _make_positive_axis ( axis , ndims ) : axis = _make_list_or_1d_tensor ( axis ) ndims = tf . convert_to_tensor ( value = ndims , name = 'ndims' , dtype = tf . int32 ) ndims_ = tf . get_static_value ( ndims ) if _is_list_like ( axis ) and ndims_ is not None : positive_axis = [ ] for a in axis : if a < 0 : a = ndims_ + a positive_axis . append ( a ) else : axis = tf . convert_to_tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) positive_axis = tf . where ( axis >= 0 , axis , axis + ndims ) return positive_axis
def _squeeze ( x , axis ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) if axis is None : return tf . squeeze ( x , axis = None ) axis = tf . convert_to_tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) axis += tf . zeros ( [ 1 ] , dtype = axis . dtype ) keep_axis , _ = tf . compat . v1 . setdiff1d ( tf . range ( 0 , tf . rank ( x ) ) , axis ) return tf . reshape ( x , tf . gather ( tf . shape ( input = x ) , keep_axis ) )
def _z ( self , x ) : with tf . name_scope ( "standardize" ) : return ( x - self . loc ) / self . scale
def _inv_z ( self , z ) : with tf . name_scope ( "reconstruct" ) : return z * self . scale + self . loc
def semilocal_linear_trend_transition_matrix ( autoregressive_coef ) : fixed_entries = tf . constant ( [ [ 1. , 1. ] , [ 0. , 0. ] ] , dtype = autoregressive_coef . dtype ) autoregressive_coef_mask = tf . constant ( [ [ 0. , 0. ] , [ 0. , 1. ] ] , dtype = autoregressive_coef . dtype ) bottom_right_entry = ( autoregressive_coef [ ... , tf . newaxis , tf . newaxis ] * autoregressive_coef_mask ) return tf . linalg . LinearOperatorFullMatrix ( fixed_entries + bottom_right_entry )
def semilocal_linear_trend_transition_noise ( level_scale , slope_mean , slope_scale , autoregressive_coef ) : broadcast_batch_shape = dist_util . get_broadcast_shape ( level_scale , slope_mean , slope_scale , autoregressive_coef ) broadcast_ones = tf . ones ( broadcast_batch_shape , dtype = level_scale . dtype ) scale_diag = tf . stack ( [ level_scale * broadcast_ones , slope_scale * broadcast_ones ] , axis = - 1 ) bias = tf . stack ( [ tf . zeros_like ( broadcast_ones ) , slope_mean * ( 1 - autoregressive_coef ) * broadcast_ones ] , axis = - 1 ) return tfd . MultivariateNormalDiag ( loc = bias , scale_diag = scale_diag )
def _machine_eps ( dtype ) : if isinstance ( dtype , tf . DType ) : dtype = dtype . as_numpy_dtype ( ) return np . finfo ( dtype ) . eps
def _fix_step_size ( value_and_gradients_function , val_c_input , active , step_size_shrink_param ) : iter_max = np . ceil ( - np . log2 ( _machine_eps ( val_c_input . x . dtype ) ) ) def _cond ( i , val_c , to_fix ) : del val_c return ( i < iter_max ) & tf . reduce_any ( input_tensor = to_fix ) def _body ( i , val_c , to_fix ) : next_c = tf . where ( to_fix , val_c . x * step_size_shrink_param , val_c . x ) next_val_c = value_and_gradients_function ( next_c ) still_to_fix = to_fix & ~ hzl . is_finite ( next_val_c ) return ( i + 1 , next_val_c , still_to_fix ) to_fix = active & ~ hzl . is_finite ( val_c_input ) return tf . while_loop ( cond = _cond , body = _body , loop_vars = ( 0 , val_c_input , to_fix ) )
def _line_search_inner_bisection ( value_and_gradients_function , search_interval , active , f_lim ) : midpoint = ( search_interval . left . x + search_interval . right . x ) / 2 val_mid = value_and_gradients_function ( midpoint ) is_valid_mid = hzl . is_finite ( val_mid ) still_active = active & is_valid_mid new_failed = active & ~ is_valid_mid next_inteval = search_interval . _replace ( failed = search_interval . failed | new_failed , func_evals = search_interval . func_evals + 1 ) def _apply_update ( ) : update_result = hzl . update ( value_and_gradients_function , next_inteval . left , next_inteval . right , val_mid , f_lim , active = still_active ) return HagerZhangLineSearchResult ( converged = next_inteval . converged , failed = next_inteval . failed | update_result . failed , iterations = next_inteval . iterations + update_result . iteration , func_evals = next_inteval . func_evals + update_result . num_evals , left = update_result . left , right = update_result . right ) return prefer_static . cond ( tf . reduce_any ( input_tensor = still_active ) , _apply_update , lambda : next_inteval )
def _print ( pass_through_tensor , values ) : flat_values = [ ] for value in values : if hasattr ( value , '_fields' ) : for field in value . _fields : flat_values . extend ( [ field , _to_str ( getattr ( value , field ) ) ] ) continue if isinstance ( value , ( list , tuple ) ) : for v in value : flat_values . append ( _to_str ( v ) ) continue flat_values . append ( _to_str ( value ) ) return tf . compat . v1 . Print ( pass_through_tensor , flat_values )
def maybe_check_quadrature_param ( param , name , validate_args ) : with tf . name_scope ( "check_" + name ) : assertions = [ ] if tensorshape_util . rank ( param . shape ) is not None : if tensorshape_util . rank ( param . shape ) == 0 : raise ValueError ( "Mixing params must be a (batch of) vector; " "{}.rank={} is not at least one." . format ( name , tensorshape_util . rank ( param . shape ) ) ) elif validate_args : assertions . append ( assert_util . assert_rank_at_least ( param , 1 , message = ( "Mixing params must be a (batch of) vector; " "{}.rank is not at least one." . format ( name ) ) ) ) if tensorshape_util . with_rank_at_least ( param . shape , 1 ) [ - 1 ] is not None : if tf . compat . dimension_value ( param . shape [ - 1 ] ) != 1 : raise NotImplementedError ( "Currently only bimixtures are supported; " "{}.shape[-1]={} is not 1." . format ( name , tf . compat . dimension_value ( param . shape [ - 1 ] ) ) ) elif validate_args : assertions . append ( assert_util . assert_equal ( tf . shape ( input = param ) [ - 1 ] , 1 , message = ( "Currently only bimixtures are supported; " "{}.shape[-1] is not 1." . format ( name ) ) ) ) if assertions : return distribution_util . with_dependencies ( assertions , param ) return param
def determine_batch_event_shapes ( grid , endpoint_affine ) : with tf . name_scope ( "determine_batch_event_shapes" ) : batch_shape = grid . shape [ : - 2 ] batch_shape_tensor = tf . shape ( input = grid ) [ : - 2 ] event_shape = None event_shape_tensor = None def _set_event_shape ( shape , shape_tensor ) : if event_shape is None : return shape , shape_tensor return ( tf . broadcast_static_shape ( event_shape , shape ) , tf . broadcast_dynamic_shape ( event_shape_tensor , shape_tensor ) ) for aff in endpoint_affine : if aff . shift is not None : batch_shape = tf . broadcast_static_shape ( batch_shape , aff . shift . shape [ : - 1 ] ) batch_shape_tensor = tf . broadcast_dynamic_shape ( batch_shape_tensor , tf . shape ( input = aff . shift ) [ : - 1 ] ) event_shape , event_shape_tensor = _set_event_shape ( aff . shift . shape [ - 1 : ] , tf . shape ( input = aff . shift ) [ - 1 : ] ) if aff . scale is not None : batch_shape = tf . broadcast_static_shape ( batch_shape , aff . scale . batch_shape ) batch_shape_tensor = tf . broadcast_dynamic_shape ( batch_shape_tensor , aff . scale . batch_shape_tensor ( ) ) event_shape , event_shape_tensor = _set_event_shape ( tf . TensorShape ( [ aff . scale . range_dimension ] ) , aff . scale . range_dimension_tensor ( ) [ tf . newaxis ] ) return batch_shape , batch_shape_tensor , event_shape , event_shape_tensor
def interpolate_loc ( grid , loc ) : if len ( loc ) != 2 : raise NotImplementedError ( "Currently only bimixtures are supported; " "len(scale)={} is not 2." . format ( len ( loc ) ) ) deg = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( grid . shape , 1 ) [ - 1 ] ) if deg is None : raise ValueError ( "Num quadrature grid points must be known prior " "to graph execution." ) with tf . name_scope ( "interpolate_loc" ) : if loc is None or loc [ 0 ] is None and loc [ 1 ] is None : return [ None ] * deg w = grid [ ... , tf . newaxis , : , : ] loc = [ x [ ... , tf . newaxis ] if x is not None else None for x in loc ] if loc [ 0 ] is None : x = w [ ... , 1 , : ] * loc [ 1 ] elif loc [ 1 ] is None : x = w [ ... , 0 , : ] * loc [ 0 ] else : delta = loc [ 0 ] - loc [ 1 ] x = w [ ... , 0 , : ] * delta + loc [ 1 ] return [ x [ ... , k ] for k in range ( deg ) ]
def interpolate_scale ( grid , scale ) : if len ( scale ) != 2 : raise NotImplementedError ( "Currently only bimixtures are supported; " "len(scale)={} is not 2." . format ( len ( scale ) ) ) deg = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( grid . shape , 1 ) [ - 1 ] ) if deg is None : raise ValueError ( "Num quadrature grid points must be known prior " "to graph execution." ) with tf . name_scope ( "interpolate_scale" ) : return [ linop_add_lib . add_operators ( [ linop_scale ( grid [ ... , k , q ] , s ) for k , s in enumerate ( scale ) ] ) [ 0 ] for q in range ( deg ) ]
def linop_scale ( w , op ) : with tf . name_scope ( "linop_scale" ) : def scaled_identity ( w ) : return tf . linalg . LinearOperatorScaledIdentity ( num_rows = op . range_dimension_tensor ( ) , multiplier = w , is_non_singular = op . is_non_singular , is_self_adjoint = op . is_self_adjoint , is_positive_definite = op . is_positive_definite ) if isinstance ( op , tf . linalg . LinearOperatorIdentity ) : return scaled_identity ( w ) if isinstance ( op , tf . linalg . LinearOperatorScaledIdentity ) : return scaled_identity ( w * op . multiplier ) if isinstance ( op , tf . linalg . LinearOperatorDiag ) : return tf . linalg . LinearOperatorDiag ( diag = w [ ... , tf . newaxis ] * op . diag_part ( ) , is_non_singular = op . is_non_singular , is_self_adjoint = op . is_self_adjoint , is_positive_definite = op . is_positive_definite ) if isinstance ( op , tf . linalg . LinearOperatorLowerTriangular ) : return tf . linalg . LinearOperatorLowerTriangular ( tril = w [ ... , tf . newaxis , tf . newaxis ] * op . to_dense ( ) , is_non_singular = op . is_non_singular , is_self_adjoint = op . is_self_adjoint , is_positive_definite = op . is_positive_definite ) raise NotImplementedError ( "Unsupported Linop type ({})" . format ( type ( op ) . __name__ ) )
def concat_vectors ( * args ) : args_ = [ tf . get_static_value ( x ) for x in args ] if any ( vec is None for vec in args_ ) : return tf . concat ( args , axis = 0 ) return [ val for vec in args_ for val in vec ]
def _log_vector_matrix ( vs , ms ) : return tf . reduce_logsumexp ( input_tensor = vs [ ... , tf . newaxis ] + ms , axis = - 2 )
def _log_matrix_vector ( ms , vs ) : return tf . reduce_logsumexp ( input_tensor = ms + vs [ ... , tf . newaxis , : ] , axis = - 1 )
def _vector_matrix ( vs , ms ) : return tf . reduce_sum ( input_tensor = vs [ ... , tf . newaxis ] * ms , axis = - 2 )
def _extract_log_probs ( num_states , dist ) : states = tf . reshape ( tf . range ( num_states ) , tf . concat ( [ [ num_states ] , tf . ones_like ( dist . batch_shape_tensor ( ) ) ] , axis = 0 ) ) return distribution_util . move_dimension ( dist . log_prob ( states ) , 0 , - 1 )
def _marginal_hidden_probs ( self ) : initial_log_probs = tf . broadcast_to ( self . _log_init , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . _num_states ] ] , axis = 0 ) ) if self . _num_steps > 1 : transition_log_probs = self . _log_trans def forward_step ( log_probs , _ ) : return _log_vector_matrix ( log_probs , transition_log_probs ) dummy_index = tf . zeros ( self . _num_steps - 1 , dtype = tf . float32 ) forward_log_probs = tf . scan ( forward_step , dummy_index , initializer = initial_log_probs , name = "forward_log_probs" ) forward_log_probs = tf . concat ( [ [ initial_log_probs ] , forward_log_probs ] , axis = 0 ) else : forward_log_probs = initial_log_probs [ tf . newaxis , ... ] return tf . exp ( forward_log_probs )
def _choose_random_direction ( current_state_parts , batch_rank , seed = None ) : seed_gen = distributions . SeedStream ( seed , salt = '_choose_random_direction' ) rnd_direction_parts = [ tf . random . normal ( tf . shape ( input = current_state_part ) , dtype = tf . float32 , seed = seed_gen ( ) ) for current_state_part in current_state_parts ] sum_squares = sum ( tf . reduce_sum ( input_tensor = rnd_direction ** 2. , axis = tf . range ( batch_rank , tf . rank ( rnd_direction ) ) , keepdims = True ) for rnd_direction in rnd_direction_parts ) rnd_direction_parts = [ rnd_direction / tf . sqrt ( sum_squares ) for rnd_direction in rnd_direction_parts ] return rnd_direction_parts
def _maybe_call_fn ( fn , fn_arg_list , fn_result = None , description = 'target_log_prob' ) : fn_arg_list = ( list ( fn_arg_list ) if mcmc_util . is_list_like ( fn_arg_list ) else [ fn_arg_list ] ) if fn_result is None : fn_result = fn ( * fn_arg_list ) if not fn_result . dtype . is_floating : raise TypeError ( '`{}` must be a `Tensor` with `float` `dtype`.' . format ( description ) ) return fn_result
def _prepare_args ( target_log_prob_fn , state , step_size , target_log_prob = None , maybe_expand = False , description = 'target_log_prob' ) : state_parts = list ( state ) if mcmc_util . is_list_like ( state ) else [ state ] state_parts = [ tf . convert_to_tensor ( value = s , name = 'current_state' ) for s in state_parts ] target_log_prob = _maybe_call_fn ( target_log_prob_fn , state_parts , target_log_prob , description ) step_sizes = ( list ( step_size ) if mcmc_util . is_list_like ( step_size ) else [ step_size ] ) step_sizes = [ tf . convert_to_tensor ( value = s , name = 'step_size' , dtype = target_log_prob . dtype ) for s in step_sizes ] if len ( step_sizes ) == 1 : step_sizes *= len ( state_parts ) if len ( state_parts ) != len ( step_sizes ) : raise ValueError ( 'There should be exactly one `step_size` or it should ' 'have same length as `current_state`.' ) def maybe_flatten ( x ) : return x if maybe_expand or mcmc_util . is_list_like ( state ) else x [ 0 ] return [ maybe_flatten ( state_parts ) , maybe_flatten ( step_sizes ) , target_log_prob ]
def _build_trainable_posterior ( param , initial_loc_fn ) : loc = tf . compat . v1 . get_variable ( param . name + '_loc' , initializer = lambda : initial_loc_fn ( param ) , dtype = param . prior . dtype , use_resource = True ) scale = tf . nn . softplus ( tf . compat . v1 . get_variable ( param . name + '_scale' , initializer = lambda : - 4 * tf . ones_like ( initial_loc_fn ( param ) ) , dtype = param . prior . dtype , use_resource = True ) ) q = tfd . Normal ( loc = loc , scale = scale ) if ( param . prior . event_shape . ndims is None or param . prior . event_shape . ndims > 0 ) : q = tfd . Independent ( q , reinterpreted_batch_ndims = param . prior . event_shape . ndims ) return tfd . TransformedDistribution ( q , param . bijector )
def _minimize_in_graph ( build_loss_fn , num_steps = 200 , optimizer = None ) : optimizer = tf . compat . v1 . train . AdamOptimizer ( 0.1 ) if optimizer is None else optimizer def train_loop_body ( step ) : train_op = optimizer . minimize ( build_loss_fn if tf . executing_eagerly ( ) else build_loss_fn ( ) ) return tf . tuple ( tensors = [ tf . add ( step , 1 ) ] , control_inputs = [ train_op ] ) minimize_op = tf . compat . v1 . while_loop ( cond = lambda step : step < num_steps , body = train_loop_body , loop_vars = [ tf . constant ( 0 ) ] , return_same_structure = True ) [ 0 ] return minimize_op
def broadcast_batch_shape ( distributions ) : batch_shape = distributions [ 0 ] . batch_shape for distribution in distributions : batch_shape = tf . broadcast_static_shape ( batch_shape , distribution . batch_shape ) if batch_shape . is_fully_defined ( ) : return batch_shape . as_list ( ) batch_shape = distributions [ 0 ] . batch_shape_tensor ( ) for distribution in distributions : batch_shape = tf . broadcast_dynamic_shape ( batch_shape , distribution . batch_shape_tensor ( ) ) return tf . convert_to_tensor ( value = batch_shape )
def range ( self , name = "range" ) : with self . _name_scope ( name ) : return self . high - self . low
def _make_summary_statistic ( attr ) : def _fn ( self ) : if any ( self . _dist_fn_args ) : raise ValueError ( 'Can only compute ' + attr + ' when all distributions are ' 'independent; {}' . format ( self . model ) ) return self . _unflatten ( getattr ( d ( ) , attr ) ( ) for d in self . _dist_fn_wrapped ) return _fn
def _resolve_distribution_names ( dist_fn_args , dist_names , leaf_name ) : if dist_names is None : dist_names = [ ] else : dist_names = dist_names . copy ( ) n = len ( dist_fn_args ) dist_names . extend ( [ None ] * ( n - len ( dist_names ) ) ) for i_ , args in enumerate ( reversed ( dist_fn_args ) ) : if not args : continue i = n - i_ - 1 for j , arg_name in enumerate ( args ) : dist_names [ i - j - 1 ] = arg_name j = 0 for i_ in range ( len ( dist_names ) ) : i = n - i_ - 1 if dist_names [ i ] is None : dist_names [ i ] = leaf_name if j == 0 else leaf_name + str ( j ) j += 1 return tuple ( dist_names )
def _get_required_args ( fn ) : argspec = tf_inspect . getfullargspec ( fn ) args = argspec . args if tf_inspect . isclass ( fn ) : args = args [ 1 : ] if argspec . defaults : args = args [ : - len ( argspec . defaults ) ] return tuple ( args )
def _build ( self , model ) : if not isinstance ( model , collections . Sequence ) : raise TypeError ( '`model` must be `list`-like (saw: {}).' . format ( type ( model ) . __name__ ) ) self . _dist_fn = model self . _dist_fn_wrapped , self . _dist_fn_args = zip ( * [ _unify_call_signature ( i , dist_fn ) for i , dist_fn in enumerate ( model ) ] )
def _entropy ( self ) : if any ( self . _dist_fn_args ) : raise ValueError ( 'Can only compute entropy when all distributions are independent.' ) return sum ( joint_distribution_lib . maybe_check_wont_broadcast ( ( d ( ) . entropy ( ) for d in self . _dist_fn_wrapped ) , self . validate_args ) )
def _prepare_args ( log_likelihood_fn , state , log_likelihood = None , description = 'log_likelihood' ) : state_parts = list ( state ) if mcmc_util . is_list_like ( state ) else [ state ] state_parts = [ tf . convert_to_tensor ( s , name = 'current_state' ) for s in state_parts ] log_likelihood = _maybe_call_fn ( log_likelihood_fn , state_parts , log_likelihood , description ) return [ state_parts , log_likelihood ]
def vector_size_to_square_matrix_size ( d , validate_args , name = None ) : if isinstance ( d , ( float , int , np . generic , np . ndarray ) ) : n = ( - 1 + np . sqrt ( 1 + 8 * d ) ) / 2. if float ( int ( n ) ) != n : raise ValueError ( "Vector length is not a triangular number." ) return int ( n ) else : with tf . name_scope ( name or "vector_size_to_square_matrix_size" ) as name : n = ( - 1. + tf . sqrt ( 1 + 8. * tf . cast ( d , dtype = tf . float32 ) ) ) / 2. if validate_args : with tf . control_dependencies ( [ assert_util . assert_equal ( tf . cast ( tf . cast ( n , dtype = tf . int32 ) , dtype = tf . float32 ) , n , message = "Vector length is not a triangular number" ) ] ) : n = tf . identity ( n ) return tf . cast ( n , d . dtype )
def _argsort ( values , axis = - 1 , direction = 'ASCENDING' , stable = False , name = None ) : if direction == 'ASCENDING' : pass elif direction == 'DESCENDING' : values = np . negative ( values ) else : raise ValueError ( 'Unrecognized direction: {}.' . format ( direction ) ) return np . argsort ( values , axis , kind = 'stable' if stable else 'quicksort' )
def _sort ( values , axis = - 1 , direction = 'ASCENDING' , stable = False , name = None ) : if direction == 'ASCENDING' : pass elif direction == 'DESCENDING' : values = np . negative ( values ) else : raise ValueError ( 'Unrecognized direction: {}.' . format ( direction ) ) result = np . sort ( values , axis , kind = 'stable' if stable else 'quicksort' ) if direction == 'DESCENDING' : return np . negative ( result ) return result
def _ndtr ( x ) : half_sqrt_2 = tf . constant ( 0.5 * np . sqrt ( 2. ) , dtype = x . dtype , name = "half_sqrt_2" ) w = x * half_sqrt_2 z = tf . abs ( w ) y = tf . where ( tf . less ( z , half_sqrt_2 ) , 1. + tf . math . erf ( w ) , tf . where ( tf . greater ( w , 0. ) , 2. - tf . math . erfc ( z ) , tf . math . erfc ( z ) ) ) return 0.5 * y
def _log_ndtr_asymptotic_series ( x , series_order ) : npdt = dtype_util . as_numpy_dtype ( x . dtype ) if series_order <= 0 : return npdt ( 1 ) x_2 = tf . square ( x ) even_sum = tf . zeros_like ( x ) odd_sum = tf . zeros_like ( x ) x_2n = x_2 for n in range ( 1 , series_order + 1 ) : y = npdt ( _double_factorial ( 2 * n - 1 ) ) / x_2n if n % 2 : odd_sum += y else : even_sum += y x_2n *= x_2 return 1. + even_sum - odd_sum
def text_messages_joint_log_prob ( count_data , lambda_1 , lambda_2 , tau ) : alpha = ( 1. / tf . reduce_mean ( input_tensor = count_data ) ) rv_lambda = tfd . Exponential ( rate = alpha ) rv_tau = tfd . Uniform ( ) lambda_ = tf . gather ( [ lambda_1 , lambda_2 ] , indices = tf . cast ( tau * tf . cast ( tf . size ( input = count_data ) , dtype = tf . float32 ) <= tf . cast ( tf . range ( tf . size ( input = count_data ) ) , dtype = tf . float32 ) , dtype = tf . int32 ) ) rv_observation = tfd . Poisson ( rate = lambda_ ) return ( rv_lambda . log_prob ( lambda_1 ) + rv_lambda . log_prob ( lambda_2 ) + rv_tau . log_prob ( tau ) + tf . reduce_sum ( input_tensor = rv_observation . log_prob ( count_data ) ) )
def _outer_squared_difference ( x , y ) : z = x - y return z [ ... , tf . newaxis , : ] * z [ ... , tf . newaxis ]
def _split_covariance_into_marginals ( covariance , block_sizes ) : start_dim = 0 marginals = [ ] for size in block_sizes : end_dim = start_dim + size marginals . append ( covariance [ ... , start_dim : end_dim , start_dim : end_dim ] ) start_dim = end_dim return marginals
def _numpy_text ( tensor , is_repr = False ) : if tensor . dtype . is_numpy_compatible : text = repr ( tensor . numpy ( ) ) if is_repr else str ( tensor . numpy ( ) ) else : text = "<unprintable>" if "\n" in text : text = "\n" + text return text
def sample_shape ( self ) : if isinstance ( self . _sample_shape , tf . Tensor ) : return tf . TensorShape ( tf . get_static_value ( self . _sample_shape ) ) return tf . TensorShape ( self . _sample_shape )
def value ( self ) : if self . _value is None : try : self . _value = self . distribution . sample ( self . sample_shape_tensor ( ) ) except NotImplementedError : raise NotImplementedError ( "sample is not implemented for {0}. You must either pass in the " "value argument or implement sample for {0}." . format ( self . distribution . __class__ . __name__ ) ) return self . _value
def numpy ( self ) : if not isinstance ( self . value , ops . EagerTensor ) : raise NotImplementedError ( "value argument must be a EagerTensor." ) return self . value . numpy ( )
def _uniform_unit_norm ( dimension , shape , dtype , seed ) : raw = normal . Normal ( loc = dtype_util . as_numpy_dtype ( dtype ) ( 0 ) , scale = dtype_util . as_numpy_dtype ( dtype ) ( 1 ) ) . sample ( tf . concat ( [ shape , [ dimension ] ] , axis = 0 ) , seed = seed ( ) ) unit_norm = raw / tf . norm ( tensor = raw , ord = 2 , axis = - 1 ) [ ... , tf . newaxis ] return unit_norm
def common_dtype ( args_list , preferred_dtype = None ) : dtype = None preferred_dtype = ( None if preferred_dtype is None else tf . as_dtype ( preferred_dtype ) ) for a in tf . nest . flatten ( args_list ) : if hasattr ( a , 'dtype' ) : dt = tf . as_dtype ( a . dtype ) else : continue if dtype is None : dtype = dt elif dtype != dt : raise TypeError ( 'Found incompatible dtypes, {} and {}.' . format ( dtype , dt ) ) if dtype is None and preferred_dtype is None : return None return ( preferred_dtype if dtype is None else dtype ) . as_numpy_dtype
def _make_summary_statistic ( attr ) : def _fn ( self , * * kwargs ) : """Implements summary statistic, eg, mean, stddev, mode.""" x = getattr ( self . distribution , attr ) ( * * kwargs ) shape = prefer_static . concat ( [ self . distribution . batch_shape_tensor ( ) , prefer_static . ones ( prefer_static . rank_from_shape ( self . sample_shape ) , dtype = self . sample_shape . dtype ) , self . distribution . event_shape_tensor ( ) , ] , axis = 0 ) x = tf . reshape ( x , shape = shape ) shape = prefer_static . concat ( [ self . distribution . batch_shape_tensor ( ) , self . sample_shape , self . distribution . event_shape_tensor ( ) , ] , axis = 0 ) return tf . broadcast_to ( x , shape ) return _fn
def _broadcast_to ( tensor_to_broadcast , target_tensors ) : output = tensor_to_broadcast for tensor in target_tensors : output += tf . zeros_like ( tensor ) return output
def _pdf_at_peak ( self ) : return ( self . peak - self . low ) / ( self . high - self . low )
def _effective_sample_size_single_state ( states , filter_beyond_lag , filter_threshold ) : with tf . compat . v1 . name_scope ( 'effective_sample_size_single_state' , values = [ states , filter_beyond_lag , filter_threshold ] ) : states = tf . convert_to_tensor ( value = states , name = 'states' ) dt = states . dtype auto_corr = stats . auto_correlation ( states , axis = 0 , max_lags = filter_beyond_lag ) if filter_threshold is not None : filter_threshold = tf . convert_to_tensor ( value = filter_threshold , dtype = dt , name = 'filter_threshold' ) mask = auto_corr < filter_threshold mask = tf . cast ( mask , dtype = dt ) mask = tf . cumsum ( mask , axis = 0 ) mask = tf . maximum ( 1. - mask , 0. ) auto_corr *= mask n = _axis_size ( states , axis = 0 ) k = tf . range ( 0. , _axis_size ( auto_corr , axis = 0 ) ) nk_factor = ( n - k ) / n if auto_corr . shape . ndims is not None : new_shape = [ - 1 ] + [ 1 ] * ( auto_corr . shape . ndims - 1 ) else : new_shape = tf . concat ( ( [ - 1 ] , tf . ones ( [ tf . rank ( auto_corr ) - 1 ] , dtype = tf . int32 ) ) , axis = 0 ) nk_factor = tf . reshape ( nk_factor , new_shape ) return n / ( - 1 + 2 * tf . reduce_sum ( input_tensor = nk_factor * auto_corr , axis = 0 ) )
def _potential_scale_reduction_single_state ( state , independent_chain_ndims ) : with tf . compat . v1 . name_scope ( 'potential_scale_reduction_single_state' , values = [ state , independent_chain_ndims ] ) : state = tf . convert_to_tensor ( value = state , name = 'state' ) sample_ndims = 1 sample_axis = tf . range ( 0 , sample_ndims ) chain_axis = tf . range ( sample_ndims , sample_ndims + independent_chain_ndims ) sample_and_chain_axis = tf . range ( 0 , sample_ndims + independent_chain_ndims ) n = _axis_size ( state , sample_axis ) m = _axis_size ( state , chain_axis ) b_div_n = _reduce_variance ( tf . reduce_mean ( input_tensor = state , axis = sample_axis , keepdims = True ) , sample_and_chain_axis , biased = False ) w = tf . reduce_mean ( input_tensor = _reduce_variance ( state , sample_axis , keepdims = True , biased = True ) , axis = sample_and_chain_axis ) sigma_2_plus = w + b_div_n return ( ( m + 1. ) / m ) * sigma_2_plus / w - ( n - 1. ) / ( m * n )
def _axis_size ( x , axis = None ) : if axis is None : return tf . cast ( tf . size ( input = x ) , x . dtype ) return tf . cast ( tf . reduce_prod ( input_tensor = tf . gather ( tf . shape ( input = x ) , axis ) ) , x . dtype )
def _broadcast_maybelist_arg ( states , secondary_arg , name ) : if _is_list_like ( secondary_arg ) : if len ( secondary_arg ) != len ( states ) : raise ValueError ( 'Argument `%s` was a list of different length ({}) than ' '`states` ({})' . format ( name , len ( states ) ) ) else : secondary_arg = [ secondary_arg ] * len ( states ) return secondary_arg
def remove ( self , field ) : return _Mapping ( x = None if field == "x" else self . x , y = None if field == "y" else self . y , ildj = self . ildj , kwargs = self . kwargs )
def _merge ( self , old , new , use_equals = False ) : if old is None : return new if new is None : return old if ( old == new ) if use_equals else ( old is new ) : return old raise ValueError ( "Incompatible values: %s != %s" % ( old , new ) )
def _deep_tuple ( self , x ) : if isinstance ( x , dict ) : return self . _deep_tuple ( tuple ( sorted ( x . items ( ) ) ) ) elif isinstance ( x , ( list , tuple ) ) : return tuple ( map ( self . _deep_tuple , x ) ) return x
def _vggconv_block ( x , filters , kernel , stride , kernel_posterior_fn ) : out = tfp . layers . Convolution2DFlipout ( filters , kernel , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( x ) out = tf . keras . layers . BatchNormalization ( ) ( out ) out = tf . keras . layers . Activation ( 'relu' ) ( out ) out = tfp . layers . Convolution2DFlipout ( filters , kernel , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( out ) out = tf . keras . layers . BatchNormalization ( ) ( out ) out = tf . keras . layers . Activation ( 'relu' ) ( out ) out = tf . keras . layers . MaxPooling2D ( pool_size = ( 2 , 2 ) , strides = stride ) ( out ) return out
def _embed_no_none_gradient_check ( value_and_gradients_fn ) : @ functools . wraps ( value_and_gradients_fn ) def func_wrapped ( * args , * * kwargs ) : """Wrapped function which checks for None gradients.""" value , grads = value_and_gradients_fn ( * args , * * kwargs ) if any ( grad is None for grad in grads ) : raise ValueError ( "Gradient is None for a state." ) return value , grads return func_wrapped
def _has_no_u_turn ( state_one , state_two , momentum ) : dot_product = sum ( [ tf . reduce_sum ( input_tensor = ( s1 - s2 ) * m ) for s1 , s2 , m in zip ( state_one , state_two , momentum ) ] ) return dot_product > 0
def _leapfrog ( value_and_gradients_fn , current_state , current_grads_target_log_prob , current_momentum , step_size ) : mid_momentum = [ m + 0.5 * step * g for m , step , g in zip ( current_momentum , step_size , current_grads_target_log_prob ) ] next_state = [ s + step * m for s , step , m in zip ( current_state , step_size , mid_momentum ) ] next_target_log_prob , next_grads_target_log_prob = value_and_gradients_fn ( * next_state ) next_momentum = [ m + 0.5 * step * g for m , step , g in zip ( mid_momentum , step_size , next_grads_target_log_prob ) ] return [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , ]
def _log_joint ( current_target_log_prob , current_momentum ) : momentum_log_prob = - sum ( [ tf . reduce_sum ( input_tensor = 0.5 * ( m ** 2. ) ) for m in current_momentum ] ) return current_target_log_prob + momentum_log_prob
def _random_bernoulli ( shape , probs , dtype = tf . int32 , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , "random_bernoulli" , [ shape , probs ] ) : probs = tf . convert_to_tensor ( value = probs ) random_uniform = tf . random . uniform ( shape , dtype = probs . dtype , seed = seed ) return tf . cast ( tf . less ( random_uniform , probs ) , dtype )
def expand_as_args ( args ) : return ( isinstance ( args , collections . Sequence ) and not _is_namedtuple ( args ) and not _force_leaf ( args ) )
def _nested_convert_to_tensor ( struct , dtype = None , name = None ) : if dtype is not None or not tf . nest . is_nested ( struct ) : return tf . convert_to_tensor ( struct , dtype = dtype ) if _maybe_convertible_to_tensor ( struct ) : try : return tf . convert_to_tensor ( value = struct , name = name ) except ( ValueError , TypeError ) : pass shallow_struct = _get_shallow_structure ( struct ) return nest . map_structure_up_to ( shallow_struct , lambda s : _nested_convert_to_tensor ( s , name = name ) , struct )
def _get_tensor_like_attributes ( ) : attrs = dict ( ) attrs . update ( ( attr , _wrap_method ( tf . Tensor , attr ) ) for attr in tf . Tensor . OVERLOADABLE_OPERATORS . union ( { '__iter__' } ) ) attrs . update ( ( attr , getattr ( tf . Tensor , attr ) ) for attr in { '__nonzero__' , '__bool__' , '__array_priority__' } ) return attrs
def pack_images ( images , rows , cols ) : shape = tf . shape ( input = images ) width = shape [ - 3 ] height = shape [ - 2 ] depth = shape [ - 1 ] images = tf . reshape ( images , ( - 1 , width , height , depth ) ) batch = tf . shape ( input = images ) [ 0 ] rows = tf . minimum ( rows , batch ) cols = tf . minimum ( batch // rows , cols ) images = images [ : rows * cols ] images = tf . reshape ( images , ( rows , cols , width , height , depth ) ) images = tf . transpose ( a = images , perm = [ 0 , 2 , 1 , 3 , 4 ] ) images = tf . reshape ( images , [ 1 , rows * width , cols * height , depth ] ) return images
def download ( directory , filename ) : filepath = os . path . join ( directory , filename ) if tf . io . gfile . exists ( filepath ) : return filepath if not tf . io . gfile . exists ( directory ) : tf . io . gfile . makedirs ( directory ) url = os . path . join ( ROOT_PATH , filename ) print ( "Downloading %s to %s" % ( url , filepath ) ) urllib . request . urlretrieve ( url , filepath ) return filepath
def build_fake_input_fns ( batch_size ) : random_sample = np . random . rand ( batch_size , * IMAGE_SHAPE ) . astype ( "float32" ) def train_input_fn ( ) : dataset = tf . data . Dataset . from_tensor_slices ( random_sample ) . map ( lambda row : ( row , 0 ) ) . batch ( batch_size ) . repeat ( ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) def eval_input_fn ( ) : dataset = tf . data . Dataset . from_tensor_slices ( random_sample ) . map ( lambda row : ( row , 0 ) ) . batch ( batch_size ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) return train_input_fn , eval_input_fn
def build_input_fns ( data_dir , batch_size ) : def train_input_fn ( ) : dataset = static_mnist_dataset ( data_dir , "train" ) dataset = dataset . shuffle ( 50000 ) . repeat ( ) . batch ( batch_size ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) def eval_input_fn ( ) : eval_dataset = static_mnist_dataset ( data_dir , "valid" ) eval_dataset = eval_dataset . batch ( batch_size ) return tf . compat . v1 . data . make_one_shot_iterator ( eval_dataset ) . get_next ( ) return train_input_fn , eval_input_fn
def _validate_block_sizes ( block_sizes , bijectors , validate_args ) : block_sizes_shape = block_sizes . shape if tensorshape_util . is_fully_defined ( block_sizes_shape ) : if ( tensorshape_util . rank ( block_sizes_shape ) != 1 or ( tensorshape_util . num_elements ( block_sizes_shape ) != len ( bijectors ) ) ) : raise ValueError ( '`block_sizes` must be `None`, or a vector of the same length as ' '`bijectors`. Got a `Tensor` with shape {} and `bijectors` of ' 'length {}' . format ( block_sizes_shape , len ( bijectors ) ) ) return block_sizes elif validate_args : message = ( '`block_sizes` must be `None`, or a vector of the same length ' 'as `bijectors`.' ) with tf . control_dependencies ( [ assert_util . assert_equal ( tf . size ( input = block_sizes ) , len ( bijectors ) , message = message ) , assert_util . assert_equal ( tf . rank ( block_sizes ) , 1 ) ] ) : return tf . identity ( block_sizes ) else : return block_sizes
def maybe_check_wont_broadcast ( flat_xs , validate_args ) : flat_xs = tuple ( flat_xs ) if not validate_args : return flat_xs msg = 'Broadcasting probably indicates an error in model specification.' s = tuple ( x . shape for x in flat_xs ) if all ( tensorshape_util . is_fully_defined ( s_ ) for s_ in s ) : if not all ( a == b for a , b in zip ( s [ 1 : ] , s [ : - 1 ] ) ) : raise ValueError ( msg ) return flat_xs assertions = [ assert_util . assert_equal ( a , b , message = msg ) for a , b in zip ( s [ 1 : ] , s [ : - 1 ] ) ] with tf . control_dependencies ( assertions ) : return tuple ( tf . identity ( x ) for x in flat_xs )
def _maybe_call_volatility_fn_and_grads ( volatility_fn , state , volatility_fn_results = None , grads_volatility_fn = None , sample_shape = None , parallel_iterations = 10 ) : state_parts = list ( state ) if mcmc_util . is_list_like ( state ) else [ state ] needs_volatility_fn_gradients = grads_volatility_fn is None if volatility_fn_results is None : volatility_fn_results = volatility_fn ( * state_parts ) volatility_fn_results = ( list ( volatility_fn_results ) if mcmc_util . is_list_like ( volatility_fn_results ) else [ volatility_fn_results ] ) if len ( volatility_fn_results ) == 1 : volatility_fn_results *= len ( state_parts ) if len ( state_parts ) != len ( volatility_fn_results ) : raise ValueError ( '`volatility_fn` should return a tensor or a list ' 'of the same length as `current_state`.' ) volatility_fn_results = _maybe_broadcast_volatility ( volatility_fn_results , state_parts ) if grads_volatility_fn is None : [ _ , grads_volatility_fn , ] = diag_jacobian ( xs = state_parts , ys = volatility_fn_results , sample_shape = sample_shape , parallel_iterations = parallel_iterations , fn = volatility_fn ) if needs_volatility_fn_gradients : grads_volatility_fn = [ 2. * g * volatility if g is not None else tf . zeros_like ( fn_arg , dtype = fn_arg . dtype . base_dtype ) for g , volatility , fn_arg in zip ( grads_volatility_fn , volatility_fn_results , state_parts ) ] return volatility_fn_results , grads_volatility_fn
def _maybe_broadcast_volatility ( volatility_parts , state_parts ) : return [ v + tf . zeros_like ( sp , dtype = sp . dtype . base_dtype ) for v , sp in zip ( volatility_parts , state_parts ) ]
def _prepare_args ( target_log_prob_fn , volatility_fn , state , step_size , target_log_prob = None , grads_target_log_prob = None , volatility = None , grads_volatility_fn = None , diffusion_drift = None , parallel_iterations = 10 ) : state_parts = list ( state ) if mcmc_util . is_list_like ( state ) else [ state ] [ target_log_prob , grads_target_log_prob , ] = mcmc_util . maybe_call_fn_and_grads ( target_log_prob_fn , state_parts , target_log_prob , grads_target_log_prob ) [ volatility_parts , grads_volatility , ] = _maybe_call_volatility_fn_and_grads ( volatility_fn , state_parts , volatility , grads_volatility_fn , distribution_util . prefer_static_shape ( target_log_prob ) , parallel_iterations ) step_sizes = ( list ( step_size ) if mcmc_util . is_list_like ( step_size ) else [ step_size ] ) step_sizes = [ tf . convert_to_tensor ( value = s , name = 'step_size' , dtype = target_log_prob . dtype ) for s in step_sizes ] if len ( step_sizes ) == 1 : step_sizes *= len ( state_parts ) if len ( state_parts ) != len ( step_sizes ) : raise ValueError ( 'There should be exactly one `step_size` or it should ' 'have same length as `current_state`.' ) if diffusion_drift is None : diffusion_drift_parts = _get_drift ( step_sizes , volatility_parts , grads_volatility , grads_target_log_prob ) else : diffusion_drift_parts = ( list ( diffusion_drift ) if mcmc_util . is_list_like ( diffusion_drift ) else [ diffusion_drift ] ) if len ( state_parts ) != len ( diffusion_drift ) : raise ValueError ( 'There should be exactly one `diffusion_drift` or it ' 'should have same length as list-like `current_state`.' ) return [ state_parts , step_sizes , target_log_prob , grads_target_log_prob , volatility_parts , grads_volatility , diffusion_drift_parts , ]
def validate_init_args_statically ( distribution , batch_shape ) : if tensorshape_util . rank ( batch_shape . shape ) is not None : if tensorshape_util . rank ( batch_shape . shape ) != 1 : raise ValueError ( "`batch_shape` must be a vector " "(saw rank: {})." . format ( tensorshape_util . rank ( batch_shape . shape ) ) ) batch_shape_static = tensorshape_util . constant_value_as_shape ( batch_shape ) batch_size_static = tensorshape_util . num_elements ( batch_shape_static ) dist_batch_size_static = tensorshape_util . num_elements ( distribution . batch_shape ) if batch_size_static is not None and dist_batch_size_static is not None : if batch_size_static != dist_batch_size_static : raise ValueError ( "`batch_shape` size ({}) must match " "`distribution.batch_shape` size ({})." . format ( batch_size_static , dist_batch_size_static ) ) if tensorshape_util . dims ( batch_shape_static ) is not None : if any ( tf . compat . dimension_value ( dim ) is not None and tf . compat . dimension_value ( dim ) < 1 for dim in batch_shape_static ) : raise ValueError ( "`batch_shape` elements must be >=-1." )
def _sample_shape ( self , x ) : x_ndims = ( tf . rank ( x ) if tensorshape_util . rank ( x . shape ) is None else tensorshape_util . rank ( x . shape ) ) event_ndims = ( tf . size ( input = self . event_shape_tensor ( ) ) if tensorshape_util . rank ( self . event_shape ) is None else tensorshape_util . rank ( self . event_shape ) ) batch_ndims = ( tf . size ( input = self . _batch_shape_unexpanded ) if tensorshape_util . rank ( self . batch_shape ) is None else tensorshape_util . rank ( self . batch_shape ) ) sample_ndims = x_ndims - batch_ndims - event_ndims if isinstance ( sample_ndims , int ) : static_sample_shape = x . shape [ : sample_ndims ] else : static_sample_shape = tf . TensorShape ( None ) if tensorshape_util . is_fully_defined ( static_sample_shape ) : sample_shape = np . int32 ( static_sample_shape ) else : sample_shape = tf . shape ( input = x ) [ : sample_ndims ] return sample_shape , static_sample_shape
def _call_reshape_input_output ( self , fn , x , extra_kwargs = None ) : with tf . control_dependencies ( self . _runtime_assertions + self . _validate_sample_arg ( x ) ) : sample_shape , static_sample_shape = self . _sample_shape ( x ) old_shape = tf . concat ( [ sample_shape , self . distribution . batch_shape_tensor ( ) , self . event_shape_tensor ( ) , ] , axis = 0 ) x_reshape = tf . reshape ( x , old_shape ) result = fn ( x_reshape , * * extra_kwargs ) if extra_kwargs else fn ( x_reshape ) new_shape = tf . concat ( [ sample_shape , self . _batch_shape_unexpanded , ] , axis = 0 ) result = tf . reshape ( result , new_shape ) if ( tensorshape_util . rank ( static_sample_shape ) is not None and tensorshape_util . rank ( self . batch_shape ) is not None ) : new_shape = tensorshape_util . concatenate ( static_sample_shape , self . batch_shape ) tensorshape_util . set_shape ( result , new_shape ) return result
def _call_and_reshape_output ( self , fn , event_shape_list = None , static_event_shape_list = None , extra_kwargs = None ) : with tf . control_dependencies ( self . _runtime_assertions ) : if event_shape_list is None : event_shape_list = [ self . _event_shape_tensor ( ) ] if static_event_shape_list is None : static_event_shape_list = [ self . event_shape ] new_shape = tf . concat ( [ self . _batch_shape_unexpanded ] + event_shape_list , axis = 0 ) result = tf . reshape ( fn ( * * extra_kwargs ) if extra_kwargs else fn ( ) , new_shape ) if ( tensorshape_util . rank ( self . batch_shape ) is not None and tensorshape_util . rank ( self . event_shape ) is not None ) : event_shape = tf . TensorShape ( [ ] ) for rss in static_event_shape_list : event_shape = tensorshape_util . concatenate ( event_shape , rss ) static_shape = tensorshape_util . concatenate ( self . batch_shape , event_shape ) tensorshape_util . set_shape ( result , static_shape ) return result
def _validate_sample_arg ( self , x ) : with tf . name_scope ( "validate_sample_arg" ) : x_ndims = ( tf . rank ( x ) if tensorshape_util . rank ( x . shape ) is None else tensorshape_util . rank ( x . shape ) ) event_ndims = ( tf . size ( input = self . event_shape_tensor ( ) ) if tensorshape_util . rank ( self . event_shape ) is None else tensorshape_util . rank ( self . event_shape ) ) batch_ndims = ( tf . size ( input = self . _batch_shape_unexpanded ) if tensorshape_util . rank ( self . batch_shape ) is None else tensorshape_util . rank ( self . batch_shape ) ) expected_batch_event_ndims = batch_ndims + event_ndims if ( isinstance ( x_ndims , int ) and isinstance ( expected_batch_event_ndims , int ) ) : if x_ndims < expected_batch_event_ndims : raise NotImplementedError ( "Broadcasting is not supported; too few batch and event dims " "(expected at least {}, saw {})." . format ( expected_batch_event_ndims , x_ndims ) ) ndims_assertion = [ ] elif self . validate_args : ndims_assertion = [ assert_util . assert_greater_equal ( x_ndims , expected_batch_event_ndims , message = ( "Broadcasting is not supported; too few " "batch and event dims." ) , name = "assert_batch_and_event_ndims_large_enough" ) , ] if ( tensorshape_util . is_fully_defined ( self . batch_shape ) and tensorshape_util . is_fully_defined ( self . event_shape ) ) : expected_batch_event_shape = np . int32 ( tensorshape_util . concatenate ( self . batch_shape , self . event_shape ) ) else : expected_batch_event_shape = tf . concat ( [ self . batch_shape_tensor ( ) , self . event_shape_tensor ( ) , ] , axis = 0 ) sample_ndims = x_ndims - expected_batch_event_ndims if isinstance ( sample_ndims , int ) : sample_ndims = max ( sample_ndims , 0 ) if ( isinstance ( sample_ndims , int ) and tensorshape_util . is_fully_defined ( x . shape [ sample_ndims : ] ) ) : actual_batch_event_shape = np . int32 ( x . shape [ sample_ndims : ] ) else : sample_ndims = tf . maximum ( sample_ndims , 0 ) actual_batch_event_shape = tf . shape ( input = x ) [ sample_ndims : ] if ( isinstance ( expected_batch_event_shape , np . ndarray ) and isinstance ( actual_batch_event_shape , np . ndarray ) ) : if any ( expected_batch_event_shape != actual_batch_event_shape ) : raise NotImplementedError ( "Broadcasting is not supported; " "unexpected batch and event shape " "(expected {}, saw {})." . format ( expected_batch_event_shape , actual_batch_event_shape ) ) runtime_assertions = ndims_assertion elif self . validate_args : with tf . control_dependencies ( ndims_assertion ) : shape_assertion = assert_util . assert_equal ( expected_batch_event_shape , actual_batch_event_shape , message = ( "Broadcasting is not supported; " "unexpected batch and event shape." ) , name = "assert_batch_and_event_shape_same" ) runtime_assertions = [ shape_assertion ] else : runtime_assertions = [ ] return runtime_assertions
def _maybe_assert_valid_sample ( self , counts ) : if not self . validate_args : return counts counts = distribution_util . embed_check_nonnegative_integer_form ( counts ) return distribution_util . with_dependencies ( [ assert_util . assert_less_equal ( counts , self . total_count , message = "counts are not less than or equal to n." ) , ] , counts )
def _flat_sample_distributions ( self , sample_shape = ( ) , seed = None , value = None ) : ds = [ ] values_out = [ ] seed = seed_stream . SeedStream ( 'JointDistributionCoroutine' , seed ) gen = self . _model ( ) index = 0 d = next ( gen ) try : while True : actual_distribution = d . distribution if isinstance ( d , self . Root ) else d ds . append ( actual_distribution ) if ( value is not None and len ( value ) > index and value [ index ] is not None ) : seed ( ) next_value = value [ index ] else : next_value = actual_distribution . sample ( sample_shape = sample_shape if isinstance ( d , self . Root ) else ( ) , seed = seed ( ) ) values_out . append ( next_value ) index += 1 d = gen . send ( next_value ) except StopIteration : pass return ds , values_out
def newsgroups_dataset ( directory , split_name , num_words , shuffle_and_repeat ) : data = np . load ( download ( directory , FILE_TEMPLATE . format ( split = split_name ) ) ) data = data [ : - 1 ] num_documents = data . shape [ 0 ] indices = np . array ( [ ( row_idx , column_idx ) for row_idx , row in enumerate ( data ) for column_idx in row ] ) sparse_matrix = scipy . sparse . coo_matrix ( ( np . ones ( indices . shape [ 0 ] ) , ( indices [ : , 0 ] , indices [ : , 1 ] ) ) , shape = ( num_documents , num_words ) , dtype = np . float32 ) sparse_matrix = sparse_matrix . tocsr ( ) dataset = tf . data . Dataset . range ( num_documents ) if shuffle_and_repeat : dataset = dataset . shuffle ( num_documents ) . repeat ( ) def get_row_py_func ( idx ) : def get_row_python ( idx_py ) : return np . squeeze ( np . array ( sparse_matrix [ idx_py ] . todense ( ) ) , axis = 0 ) py_func = tf . compat . v1 . py_func ( get_row_python , [ idx ] , tf . float32 , stateful = False ) py_func . set_shape ( ( num_words , ) ) return py_func dataset = dataset . map ( get_row_py_func ) return dataset
def build_fake_input_fns ( batch_size ) : num_words = 1000 vocabulary = [ str ( i ) for i in range ( num_words ) ] random_sample = np . random . randint ( 10 , size = ( batch_size , num_words ) ) . astype ( np . float32 ) def train_input_fn ( ) : dataset = tf . data . Dataset . from_tensor_slices ( random_sample ) dataset = dataset . batch ( batch_size ) . repeat ( ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) def eval_input_fn ( ) : dataset = tf . data . Dataset . from_tensor_slices ( random_sample ) dataset = dataset . batch ( batch_size ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) return train_input_fn , eval_input_fn , vocabulary
def load_bernoulli_mnist_dataset ( directory , split_name ) : amat_file = download ( directory , FILE_TEMPLATE . format ( split = split_name ) ) dataset = tf . data . TextLineDataset ( amat_file ) str_to_arr = lambda string : np . array ( [ c == b"1" for c in string . split ( ) ] ) def _parser ( s ) : booltensor = tf . compat . v1 . py_func ( str_to_arr , [ s ] , tf . bool ) reshaped = tf . reshape ( booltensor , [ 28 , 28 , 1 ] ) return tf . cast ( reshaped , dtype = tf . float32 ) , tf . constant ( 0 , tf . int32 ) return dataset . map ( _parser )
def build_input_pipeline ( data_dir , batch_size , heldout_size , mnist_type ) : if mnist_type in [ MnistType . FAKE_DATA , MnistType . THRESHOLD ] : if mnist_type == MnistType . FAKE_DATA : mnist_data = build_fake_data ( ) else : mnist_data = mnist . read_data_sets ( data_dir ) training_dataset = tf . data . Dataset . from_tensor_slices ( ( mnist_data . train . images , np . int32 ( mnist_data . train . labels ) ) ) heldout_dataset = tf . data . Dataset . from_tensor_slices ( ( mnist_data . validation . images , np . int32 ( mnist_data . validation . labels ) ) ) elif mnist_type == MnistType . BERNOULLI : training_dataset = load_bernoulli_mnist_dataset ( data_dir , "train" ) heldout_dataset = load_bernoulli_mnist_dataset ( data_dir , "valid" ) else : raise ValueError ( "Unknown MNIST type." ) training_batches = training_dataset . repeat ( ) . batch ( batch_size ) training_iterator = tf . compat . v1 . data . make_one_shot_iterator ( training_batches ) heldout_frozen = ( heldout_dataset . take ( heldout_size ) . repeat ( ) . batch ( heldout_size ) ) heldout_iterator = tf . compat . v1 . data . make_one_shot_iterator ( heldout_frozen ) handle = tf . compat . v1 . placeholder ( tf . string , shape = [ ] ) feedable_iterator = tf . compat . v1 . data . Iterator . from_string_handle ( handle , training_batches . output_types , training_batches . output_shapes ) images , labels = feedable_iterator . get_next ( ) images = tf . reshape ( images , shape = [ - 1 ] + IMAGE_SHAPE ) if mnist_type in [ MnistType . FAKE_DATA , MnistType . THRESHOLD ] : images = tf . cast ( images > 0.5 , dtype = tf . int32 ) return images , labels , handle , training_iterator , heldout_iterator
def as_numpy_dtype ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'as_numpy_dtype' ) : return dtype . as_numpy_dtype return dtype
def base_dtype ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'base_dtype' ) : return dtype . base_dtype return dtype
def is_bool ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'is_bool' ) : return dtype . is_bool return np . dtype ( dtype ) . kind == 'b'
def is_complex ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'is_complex' ) : return dtype . is_complex return np . issubdtype ( np . dtype ( dtype ) , np . complex )
def max ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'max' ) : return dtype . max use_finfo = is_floating ( dtype ) or is_complex ( dtype ) return np . finfo ( dtype ) . max if use_finfo else np . iinfo ( dtype ) . max
def name ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'name' ) : return dtype . name if hasattr ( dtype , '__name__' ) : return dtype . __name__ return str ( dtype )
def size ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'size' ) : return dtype . size return np . dtype ( dtype ) . itemsize
def nelder_mead_one_step ( current_simplex , current_objective_values , objective_function = None , dim = None , func_tolerance = None , position_tolerance = None , batch_evaluate_objective = False , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'nelder_mead_one_step' ) : domain_dtype = current_simplex . dtype . base_dtype order = tf . argsort ( current_objective_values , direction = 'ASCENDING' , stable = True ) ( best_index , worst_index , second_worst_index ) = order [ 0 ] , order [ - 1 ] , order [ - 2 ] worst_vertex = current_simplex [ worst_index ] ( best_objective_value , worst_objective_value , second_worst_objective_value ) = ( current_objective_values [ best_index ] , current_objective_values [ worst_index ] , current_objective_values [ second_worst_index ] ) face_centroid = tf . reduce_sum ( input_tensor = current_simplex , axis = 0 ) - worst_vertex face_centroid /= tf . cast ( dim , domain_dtype ) reflected = face_centroid + reflection * ( face_centroid - worst_vertex ) objective_at_reflected = objective_function ( reflected ) num_evaluations = 1 has_converged = _check_convergence ( current_simplex , current_simplex [ best_index ] , best_objective_value , worst_objective_value , func_tolerance , position_tolerance ) def _converged_fn ( ) : return ( True , current_simplex , current_objective_values , 0 ) case0 = has_converged , _converged_fn accept_reflected = ( ( objective_at_reflected < second_worst_objective_value ) & ( objective_at_reflected >= best_objective_value ) ) accept_reflected_fn = _accept_reflected_fn ( current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected ) case1 = accept_reflected , accept_reflected_fn do_expansion = objective_at_reflected < best_objective_value expansion_fn = _expansion_fn ( objective_function , current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) case2 = do_expansion , expansion_fn do_outside_contraction = ( ( objective_at_reflected < worst_objective_value ) & ( objective_at_reflected >= second_worst_objective_value ) ) outside_contraction_fn = _outside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) case3 = do_outside_contraction , outside_contraction_fn default_fn = _inside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , worst_objective_value , contraction , shrinkage , batch_evaluate_objective ) ( converged , next_simplex , next_objective_at_simplex , case_evals ) = prefer_static . case ( [ case0 , case1 , case2 , case3 ] , default = default_fn , exclusive = False ) next_simplex . set_shape ( current_simplex . shape ) next_objective_at_simplex . set_shape ( current_objective_values . shape ) return ( converged , next_simplex , next_objective_at_simplex , num_evaluations + case_evals )
def _accept_reflected_fn ( simplex , objective_values , worst_index , reflected , objective_at_reflected ) : def _replace_worst_with_reflected ( ) : next_simplex = _replace_at_index ( simplex , worst_index , reflected ) next_objective_values = _replace_at_index ( objective_values , worst_index , objective_at_reflected ) return False , next_simplex , next_objective_values , 0 return _replace_worst_with_reflected
def _expansion_fn ( objective_function , simplex , objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) : def _expand_and_maybe_replace ( ) : """Performs the expansion step.""" expanded = face_centroid + expansion * ( reflected - face_centroid ) expanded_objective_value = objective_function ( expanded ) expanded_is_better = ( expanded_objective_value < objective_at_reflected ) accept_expanded_fn = lambda : ( expanded , expanded_objective_value ) accept_reflected_fn = lambda : ( reflected , objective_at_reflected ) next_pt , next_objective_value = prefer_static . cond ( expanded_is_better , accept_expanded_fn , accept_reflected_fn ) next_simplex = _replace_at_index ( simplex , worst_index , next_pt ) next_objective_at_simplex = _replace_at_index ( objective_values , worst_index , next_objective_value ) return False , next_simplex , next_objective_at_simplex , 1 return _expand_and_maybe_replace
def _outside_contraction_fn ( objective_function , simplex , objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) : def _contraction ( ) : """Performs a contraction.""" contracted = face_centroid + contraction * ( reflected - face_centroid ) objective_at_contracted = objective_function ( contracted ) is_contracted_acceptable = objective_at_contracted <= objective_at_reflected def _accept_contraction ( ) : next_simplex = _replace_at_index ( simplex , worst_index , contracted ) objective_at_next_simplex = _replace_at_index ( objective_values , worst_index , objective_at_contracted ) return ( False , next_simplex , objective_at_next_simplex , 1 ) def _reject_contraction ( ) : return _shrink_towards_best ( objective_function , simplex , best_index , shrinkage , batch_evaluate_objective ) return prefer_static . cond ( is_contracted_acceptable , _accept_contraction , _reject_contraction ) return _contraction
def _shrink_towards_best ( objective_function , simplex , best_index , shrinkage , batch_evaluate_objective ) : best_vertex = simplex [ best_index ] shrunk_simplex = best_vertex + shrinkage * ( simplex - best_vertex ) objective_at_shrunk_simplex , evals = _evaluate_objective_multiple ( objective_function , shrunk_simplex , batch_evaluate_objective ) return ( False , shrunk_simplex , objective_at_shrunk_simplex , evals )
def _replace_at_index ( x , index , replacement ) : x_new = tf . concat ( [ x [ : index ] , tf . expand_dims ( replacement , axis = 0 ) , x [ ( index + 1 ) : ] ] , axis = 0 ) return x_new
def _prepare_args_with_initial_simplex ( objective_function , initial_simplex , objective_at_initial_simplex , batch_evaluate_objective ) : initial_simplex = tf . convert_to_tensor ( value = initial_simplex ) num_vertices = tf . shape ( input = initial_simplex ) [ 0 ] dim = num_vertices - 1 num_evaluations = 0 if objective_at_initial_simplex is None : objective_at_initial_simplex , n_evals = _evaluate_objective_multiple ( objective_function , initial_simplex , batch_evaluate_objective ) num_evaluations += n_evals objective_at_initial_simplex = tf . convert_to_tensor ( value = objective_at_initial_simplex ) return ( dim , num_vertices , initial_simplex , objective_at_initial_simplex , num_evaluations )
def _prepare_args_with_initial_vertex ( objective_function , initial_vertex , step_sizes , objective_at_initial_vertex , batch_evaluate_objective ) : dim = tf . size ( input = initial_vertex ) num_vertices = dim + 1 unit_vectors_along_axes = tf . reshape ( tf . eye ( dim , dim , dtype = initial_vertex . dtype . base_dtype ) , tf . concat ( [ [ dim ] , tf . shape ( input = initial_vertex ) ] , axis = 0 ) ) simplex_face = initial_vertex + step_sizes * unit_vectors_along_axes simplex = tf . concat ( [ tf . expand_dims ( initial_vertex , axis = 0 ) , simplex_face ] , axis = 0 ) num_evaluations = 0 if objective_at_initial_vertex is None : objective_at_initial_vertex = objective_function ( initial_vertex ) num_evaluations += 1 objective_at_simplex_face , num_evals = _evaluate_objective_multiple ( objective_function , simplex_face , batch_evaluate_objective ) num_evaluations += num_evals objective_at_simplex = tf . concat ( [ tf . expand_dims ( objective_at_initial_vertex , axis = 0 ) , objective_at_simplex_face ] , axis = 0 ) return ( dim , num_vertices , simplex , objective_at_simplex , num_evaluations )
def build_input_pipeline ( mnist_data , batch_size , heldout_size ) : training_dataset = tf . data . Dataset . from_tensor_slices ( ( mnist_data . train . images , np . int32 ( mnist_data . train . labels ) ) ) training_batches = training_dataset . shuffle ( 50000 , reshuffle_each_iteration = True ) . repeat ( ) . batch ( batch_size ) training_iterator = tf . compat . v1 . data . make_one_shot_iterator ( training_batches ) heldout_dataset = tf . data . Dataset . from_tensor_slices ( ( mnist_data . validation . images , np . int32 ( mnist_data . validation . labels ) ) ) heldout_frozen = ( heldout_dataset . take ( heldout_size ) . repeat ( ) . batch ( heldout_size ) ) heldout_iterator = tf . compat . v1 . data . make_one_shot_iterator ( heldout_frozen ) handle = tf . compat . v1 . placeholder ( tf . string , shape = [ ] ) feedable_iterator = tf . compat . v1 . data . Iterator . from_string_handle ( handle , training_batches . output_types , training_batches . output_shapes ) images , labels = feedable_iterator . get_next ( ) return images , labels , handle , training_iterator , heldout_iterator
def build_fake_data ( num_examples = 10 ) : class Dummy ( object ) : pass num_examples = 10 mnist_data = Dummy ( ) mnist_data . train = Dummy ( ) mnist_data . train . images = np . float32 ( np . random . randn ( num_examples , * IMAGE_SHAPE ) ) mnist_data . train . labels = np . int32 ( np . random . permutation ( np . arange ( num_examples ) ) ) mnist_data . train . num_examples = num_examples mnist_data . validation = Dummy ( ) mnist_data . validation . images = np . float32 ( np . random . randn ( num_examples , * IMAGE_SHAPE ) ) mnist_data . validation . labels = np . int32 ( np . random . permutation ( np . arange ( num_examples ) ) ) mnist_data . validation . num_examples = num_examples return mnist_data
def get_config ( self ) : return { 'initializers' : [ tf . compat . v2 . initializers . serialize ( tf . keras . initializers . get ( init ) ) for init in self . initializers ] , 'sizes' : self . sizes , 'validate_args' : self . validate_args , }
def from_config ( cls , config ) : return cls ( * * { 'initializers' : [ tf . compat . v2 . initializers . deserialize ( init ) for init in config . get ( 'initializers' , [ ] ) ] , 'sizes' : config . get ( 'sizes' , [ ] ) , 'validate_args' : config . get ( 'validate_args' , False ) , } )
def _matmul ( a , b , transpose_a = False , transpose_b = False , adjoint_a = False , adjoint_b = False , a_is_sparse = False , b_is_sparse = False , name = None ) : if a_is_sparse or b_is_sparse : raise NotImplementedError ( 'Numpy backend does not support sparse matmul.' ) if transpose_a or adjoint_a : a = _matrix_transpose ( a , conjugate = adjoint_a ) if transpose_b or adjoint_b : b = _matrix_transpose ( b , conjugate = adjoint_b ) return np . matmul ( a , b )
def _std_var_helper ( self , statistic , statistic_name , statistic_ndims , df_factor_fn ) : df = tf . reshape ( self . df , tf . concat ( [ tf . shape ( input = self . df ) , tf . ones ( [ statistic_ndims ] , dtype = tf . int32 ) ] , - 1 ) ) df = _broadcast_to_shape ( df , tf . shape ( input = statistic ) ) denom = tf . where ( df > 2. , df - 2. , tf . ones_like ( df ) ) statistic = statistic * df_factor_fn ( df / denom ) inf = dtype_util . as_numpy_dtype ( self . dtype ) ( np . inf ) result_where_defined = tf . where ( df > 2. , statistic , tf . fill ( tf . shape ( input = statistic ) , inf , name = "inf" ) ) if self . allow_nan_stats : nan = dtype_util . as_numpy_dtype ( self . dtype ) ( np . nan ) return tf . where ( df > 1. , result_where_defined , tf . fill ( tf . shape ( input = statistic ) , nan , name = "nan" ) ) else : with tf . control_dependencies ( [ assert_util . assert_less ( tf . cast ( 1. , self . dtype ) , df , message = statistic_name + " not defined for components of df <= 1" ) , ] ) : return tf . identity ( result_where_defined )
def _pick_scalar_condition ( pred , cond_true , cond_false ) : pred_ = tf . get_static_value ( tf . convert_to_tensor ( value = pred ) ) if pred_ is None : return tf . where ( pred , cond_true , cond_false ) return cond_true if pred_ else cond_false
def _finish_log_prob_for_one_fiber ( self , y , x , ildj , event_ndims , * * distribution_kwargs ) : x = self . _maybe_rotate_dims ( x , rotate_right = True ) log_prob = self . distribution . log_prob ( x , * * distribution_kwargs ) if self . _is_maybe_event_override : log_prob = tf . reduce_sum ( input_tensor = log_prob , axis = self . _reduce_event_indices ) log_prob += tf . cast ( ildj , log_prob . dtype ) if self . _is_maybe_event_override and isinstance ( event_ndims , int ) : tensorshape_util . set_shape ( log_prob , tf . broadcast_static_shape ( tensorshape_util . with_rank_at_least ( y . shape , 1 ) [ : - event_ndims ] , self . batch_shape ) ) return log_prob
def _finish_prob_for_one_fiber ( self , y , x , ildj , event_ndims , * * distribution_kwargs ) : x = self . _maybe_rotate_dims ( x , rotate_right = True ) prob = self . distribution . prob ( x , * * distribution_kwargs ) if self . _is_maybe_event_override : prob = tf . reduce_prod ( input_tensor = prob , axis = self . _reduce_event_indices ) prob *= tf . exp ( tf . cast ( ildj , prob . dtype ) ) if self . _is_maybe_event_override and isinstance ( event_ndims , int ) : tensorshape_util . set_shape ( prob , tf . broadcast_static_shape ( tensorshape_util . with_rank_at_least ( y . shape , 1 ) [ : - event_ndims ] , self . batch_shape ) ) return prob
def _maybe_rotate_dims ( self , x , rotate_right = False ) : needs_rotation_const = tf . get_static_value ( self . _needs_rotation ) if needs_rotation_const is not None and not needs_rotation_const : return x ndims = prefer_static . rank ( x ) n = ( ndims - self . _rotate_ndims ) if rotate_right else self . _rotate_ndims perm = prefer_static . concat ( [ prefer_static . range ( n , ndims ) , prefer_static . range ( 0 , n ) ] , axis = 0 ) return tf . transpose ( a = x , perm = perm )
def _apply_single_step ( dist , params_event_ndims , slices , params_overrides ) : if len ( slices ) == 1 and slices [ 0 ] == Ellipsis : override_dict = { } else : override_dict = _slice_params_to_dict ( dist , params_event_ndims , slices ) override_dict . update ( params_overrides ) parameters = dict ( dist . parameters , * * override_dict ) new_dist = type ( dist ) ( * * parameters ) return new_dist
def _apply_slice_sequence ( dist , params_event_ndims , slice_overrides_seq ) : for slices , overrides in slice_overrides_seq : dist = _apply_single_step ( dist , params_event_ndims , slices , overrides ) return dist
def num_cols ( x ) : if tf . compat . dimension_value ( x . shape [ - 1 ] ) is not None : return tf . compat . dimension_value ( x . shape [ - 1 ] ) return tf . shape ( input = x ) [ - 1 ]
def _prefer_static ( original_fn , static_fn ) : original_spec = tf_inspect . getfullargspec ( original_fn ) static_spec = tf_inspect . getfullargspec ( static_fn ) if original_spec != static_spec : raise ValueError ( 'Arg specs do not match: original={}, static={}, fn={}' . format ( original_spec , static_spec , original_fn ) ) @ decorator . decorator def wrap ( wrapped_fn , * args , * * kwargs ) : del wrapped_fn [ args_ , kwargs_ ] , all_static = _maybe_get_static_args ( [ args , kwargs ] ) if all_static : return static_fn ( * args_ , * * kwargs_ ) return original_fn ( * args , * * kwargs ) return wrap ( original_fn )
def _copy_docstring ( original_fn , new_fn ) : original_spec = tf_inspect . getfullargspec ( original_fn ) new_spec = tf_inspect . getfullargspec ( new_fn ) if original_spec != new_spec : raise ValueError ( 'Arg specs do not match: original={}, new={}, fn={}' . format ( original_spec , new_spec , original_fn ) ) @ decorator . decorator def wrap ( wrapped_fn , * args , * * kwargs ) : del wrapped_fn return new_fn ( * args , * * kwargs ) return wrap ( original_fn )
def _get_static_predicate ( pred ) : if pred in { 0 , 1 } : pred_value = bool ( pred ) elif isinstance ( pred , bool ) : pred_value = pred elif isinstance ( pred , tf . Tensor ) : pred_value = tf . get_static_value ( pred ) if pred_value is None : pred_value = c_api . TF_TryEvaluateConstant_wrapper ( pred . graph . _c_graph , pred . _as_tf_output ( ) ) else : raise TypeError ( '`pred` must be a Tensor, or a Python bool, or 1 or 0. ' 'Found instead: {}' . format ( pred ) ) return pred_value
def rank_from_shape ( shape_tensor_fn , tensorshape = None ) : if tensorshape is None : shape_tensor = ( shape_tensor_fn ( ) if callable ( shape_tensor_fn ) else shape_tensor_fn ) if ( hasattr ( shape_tensor , 'shape' ) and hasattr ( shape_tensor . shape , 'num_elements' ) ) : ndims_ = tensorshape_util . num_elements ( shape_tensor . shape ) else : ndims_ = len ( shape_tensor ) ndims_fn = lambda : tf . size ( input = shape_tensor ) else : ndims_ = tensorshape_util . rank ( tensorshape ) ndims_fn = lambda : tf . size ( input = shape_tensor_fn ( ) if callable ( shape_tensor_fn ) else shape_tensor_fn ) return ndims_fn ( ) if ndims_ is None else ndims_
def _name_scope ( self , name = None , default_name = None , values = None ) : with tf . compat . v1 . name_scope ( self . name ) : with tf . compat . v1 . name_scope ( name , default_name , values = values or [ ] ) as scope : yield scope
def embed_check_nonnegative_integer_form ( x , name = "embed_check_nonnegative_integer_form" ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = "x" ) assertions = [ assert_util . assert_non_negative ( x , message = "'{}' must be non-negative." . format ( x ) ) , ] if not dtype_util . is_integer ( x . dtype ) : assertions += [ assert_integer_form ( x , message = "'{}' cannot contain fractional components." . format ( x ) ) , ] return with_dependencies ( assertions , x )
def _is_known_unsigned_by_dtype ( dt ) : return { tf . bool : True , tf . uint8 : True , tf . uint16 : True , } . get ( dt . base_dtype , False )
def _is_known_signed_by_dtype ( dt ) : return { tf . float16 : True , tf . float32 : True , tf . float64 : True , tf . int8 : True , tf . int16 : True , tf . int32 : True , tf . int64 : True , } . get ( dt . base_dtype , False )
def _largest_integer_by_dtype ( dt ) : if not _is_known_dtype ( dt ) : raise TypeError ( "Unrecognized dtype: {}" . format ( dt . name ) ) if dt . is_floating : return int ( 2 ** ( np . finfo ( dt . as_numpy_dtype ) . nmant + 1 ) ) if dt . is_integer : return np . iinfo ( dt . as_numpy_dtype ) . max if dt . base_dtype == tf . bool : return int ( 1 ) raise TypeError ( "Unrecognized dtype: {}" . format ( dt . name ) )
def _smallest_integer_by_dtype ( dt ) : if not _is_known_dtype ( dt ) : raise TypeError ( "Unrecognized dtype: {}" . format ( dt . name ) ) if _is_known_unsigned_by_dtype ( dt ) : return 0 return - 1 * _largest_integer_by_dtype ( dt )
def _is_integer_like_by_dtype ( dt ) : if not _is_known_dtype ( dt ) : raise TypeError ( "Unrecognized dtype: {}" . format ( dt . name ) ) return dt . is_integer or dt . base_dtype == tf . bool
def gen_new_seed ( seed , salt ) : if seed is None : return None string = ( str ( seed ) + salt ) . encode ( "utf-8" ) return int ( hashlib . md5 ( string ) . hexdigest ( ) [ : 8 ] , 16 ) & 0x7FFFFFFF
def dimension_size ( x , axis ) : s = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( x . shape , np . abs ( axis ) ) [ axis ] ) if s is not None : return s return tf . shape ( input = x ) [ axis ]
def _maybe_validate_rightmost_transposed_ndims ( rightmost_transposed_ndims , validate_args , name = None ) : with tf . name_scope ( name or 'maybe_validate_rightmost_transposed_ndims' ) : assertions = [ ] if not dtype_util . is_integer ( rightmost_transposed_ndims . dtype ) : raise TypeError ( '`rightmost_transposed_ndims` must be integer type.' ) if tensorshape_util . rank ( rightmost_transposed_ndims . shape ) is not None : if tensorshape_util . rank ( rightmost_transposed_ndims . shape ) != 0 : raise ValueError ( '`rightmost_transposed_ndims` must be a scalar, ' 'saw rank: {}.' . format ( tensorshape_util . rank ( rightmost_transposed_ndims . shape ) ) ) elif validate_args : assertions += [ assert_util . assert_rank ( rightmost_transposed_ndims , 0 ) ] rightmost_transposed_ndims_ = tf . get_static_value ( rightmost_transposed_ndims ) msg = '`rightmost_transposed_ndims` must be non-negative.' if rightmost_transposed_ndims_ is not None : if rightmost_transposed_ndims_ < 0 : raise ValueError ( msg [ : - 1 ] + ', saw: {}.' . format ( rightmost_transposed_ndims_ ) ) elif validate_args : assertions += [ assert_util . assert_non_negative ( rightmost_transposed_ndims , message = msg ) ] return assertions
def _maybe_validate_perm ( perm , validate_args , name = None ) : with tf . name_scope ( name or 'maybe_validate_perm' ) : assertions = [ ] if not dtype_util . is_integer ( perm . dtype ) : raise TypeError ( '`perm` must be integer type' ) msg = '`perm` must be a vector.' if tensorshape_util . rank ( perm . shape ) is not None : if tensorshape_util . rank ( perm . shape ) != 1 : raise ValueError ( msg [ : - 1 ] + ', saw rank: {}.' . format ( tensorshape_util . rank ( perm . shape ) ) ) elif validate_args : assertions += [ assert_util . assert_rank ( perm , 1 , message = msg ) ] perm_ = tf . get_static_value ( perm ) msg = '`perm` must be a valid permutation vector.' if perm_ is not None : if not np . all ( np . arange ( np . size ( perm_ ) ) == np . sort ( perm_ ) ) : raise ValueError ( msg [ : - 1 ] + ', saw: {}.' . format ( perm_ ) ) elif validate_args : assertions += [ assert_util . assert_equal ( tf . sort ( perm ) , tf . range ( tf . size ( input = perm ) ) , message = msg ) ] return assertions
def _event_shape ( self , shape , static_perm_to_shape ) : rightmost_ = tf . get_static_value ( self . rightmost_transposed_ndims ) if tensorshape_util . rank ( shape ) is None or rightmost_ is None : return tf . TensorShape ( None ) if tensorshape_util . rank ( shape ) < rightmost_ : raise ValueError ( 'Invalid shape: min event ndims={} but got {}' . format ( rightmost_ , shape ) ) perm_ = tf . get_static_value ( self . perm , partial = True ) if perm_ is None : return shape [ : tensorshape_util . rank ( shape ) - rightmost_ ] . concatenate ( [ None ] * int ( rightmost_ ) ) if sum ( p is None for p in perm_ ) == 1 : present = np . argsort ( [ - 1 if p is None else p for p in perm_ ] ) for i , p in enumerate ( present [ 1 : ] ) : if i != p : perm_ = [ i if p is None else p for p in perm_ ] break return shape [ : tensorshape_util . rank ( shape ) - rightmost_ ] . concatenate ( static_perm_to_shape ( shape [ tensorshape_util . rank ( shape ) - rightmost_ : ] , perm_ ) )
def _check_equal_shape ( name , static_shape , dynamic_shape , static_target_shape , dynamic_target_shape = None ) : static_target_shape = tf . TensorShape ( static_target_shape ) if tensorshape_util . is_fully_defined ( static_shape ) and tensorshape_util . is_fully_defined ( static_target_shape ) : if static_shape != static_target_shape : raise ValueError ( "{}: required shape {} but found {}" . format ( name , static_target_shape , static_shape ) ) return None else : if dynamic_target_shape is None : if tensorshape_util . is_fully_defined ( static_target_shape ) : dynamic_target_shape = tensorshape_util . as_list ( static_target_shape ) else : raise ValueError ( "{}: cannot infer target shape: no dynamic shape " "specified and static shape {} is not fully defined" . format ( name , static_target_shape ) ) return assert_util . assert_equal ( dynamic_shape , dynamic_target_shape , message = ( "{}: required shape {}" . format ( name , static_target_shape ) ) )
def kalman_transition ( filtered_mean , filtered_cov , transition_matrix , transition_noise ) : predicted_mean = _propagate_mean ( filtered_mean , transition_matrix , transition_noise ) predicted_cov = _propagate_cov ( filtered_cov , transition_matrix , transition_noise ) return predicted_mean , predicted_cov
def _propagate_mean ( mean , linop , dist ) : return linop . matmul ( mean ) + dist . mean ( ) [ ... , tf . newaxis ]
def _propagate_cov ( cov , linop , dist ) : return linop . matmul ( linop . matmul ( cov ) , adjoint_arg = True ) + dist . covariance ( )
def _joint_sample_n ( self , n , seed = None ) : with tf . name_scope ( "sample_n_joint" ) : stream = seed_stream . SeedStream ( seed , salt = "LinearGaussianStateSpaceModel_sample_n_joint" ) sample_and_batch_shape = distribution_util . prefer_static_value ( tf . concat ( [ [ n ] , self . batch_shape_tensor ( ) ] , axis = 0 ) ) with tf . control_dependencies ( self . runtime_assertions ) : initial_latent = self . initial_state_prior . sample ( sample_shape = _augment_sample_shape ( self . initial_state_prior , sample_and_batch_shape , self . validate_args ) , seed = stream ( ) ) initial_latent = initial_latent [ ... , tf . newaxis ] initial_observation_matrix = ( self . get_observation_matrix_for_timestep ( self . initial_step ) ) initial_observation_noise = ( self . get_observation_noise_for_timestep ( self . initial_step ) ) initial_observation_pred = initial_observation_matrix . matmul ( initial_latent ) initial_observation = ( initial_observation_pred + initial_observation_noise . sample ( sample_shape = _augment_sample_shape ( initial_observation_noise , sample_and_batch_shape , self . validate_args ) , seed = stream ( ) ) [ ... , tf . newaxis ] ) sample_step = build_kalman_sample_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep , full_sample_and_batch_shape = sample_and_batch_shape , stream = stream , validate_args = self . validate_args ) ( latents , observations ) = tf . scan ( sample_step , elems = tf . range ( self . initial_step + 1 , self . final_step ) , initializer = ( initial_latent , initial_observation ) ) latents = tf . concat ( [ initial_latent [ tf . newaxis , ... ] , latents ] , axis = 0 ) observations = tf . concat ( [ initial_observation [ tf . newaxis , ... ] , observations ] , axis = 0 ) latents = tf . squeeze ( latents , - 1 ) latents = distribution_util . move_dimension ( latents , 0 , - 2 ) observations = tf . squeeze ( observations , - 1 ) observations = distribution_util . move_dimension ( observations , 0 , - 2 ) return latents , observations
def _log_normalization ( self ) : event_dim = tf . compat . dimension_value ( self . event_shape [ 0 ] ) if event_dim is None : raise ValueError ( 'vMF _log_normalizer currently only supports ' 'statically known event shape' ) safe_conc = tf . where ( self . concentration > 0 , self . concentration , tf . ones_like ( self . concentration ) ) safe_lognorm = ( ( event_dim / 2 - 1 ) * tf . math . log ( safe_conc ) - ( event_dim / 2 ) * np . log ( 2 * np . pi ) - tf . math . log ( _bessel_ive ( event_dim / 2 - 1 , safe_conc ) ) - tf . abs ( safe_conc ) ) log_nsphere_surface_area = ( np . log ( 2. ) + ( event_dim / 2 ) * np . log ( np . pi ) - tf . math . lgamma ( tf . cast ( event_dim / 2 , self . dtype ) ) ) return tf . where ( self . concentration > 0 , - safe_lognorm , log_nsphere_surface_area * tf . ones_like ( safe_lognorm ) )
def _maybe_assert_valid_sample ( self , samples ) : if not self . validate_args : return samples with tf . control_dependencies ( [ assert_util . assert_near ( 1. , tf . linalg . norm ( tensor = samples , axis = - 1 ) , message = 'samples must be unit length' ) , assert_util . assert_equal ( tf . shape ( input = samples ) [ - 1 : ] , self . event_shape_tensor ( ) , message = ( 'samples must have innermost dimension matching that of ' '`self.mean_direction`' ) ) , ] ) : return tf . identity ( samples )
def _mode ( self ) : return ( self . mean_direction + tf . zeros_like ( self . concentration ) [ ... , tf . newaxis ] )
def _rotate ( self , samples ) : event_dim = ( tf . compat . dimension_value ( self . event_shape [ 0 ] ) or self . _event_shape_tensor ( ) [ 0 ] ) basis = tf . concat ( [ [ 1. ] , tf . zeros ( [ event_dim - 1 ] , dtype = self . dtype ) ] , axis = 0 ) , u = tf . nn . l2_normalize ( basis - self . mean_direction , axis = - 1 ) return samples - 2 * tf . reduce_sum ( input_tensor = samples * u , axis = - 1 , keepdims = True ) * u
def _sample_3d ( self , n , seed = None ) : seed = seed_stream . SeedStream ( seed , salt = 'von_mises_fisher_3d' ) u_shape = tf . concat ( [ [ n ] , self . _batch_shape_tensor ( ) ] , axis = 0 ) z = tf . random . uniform ( u_shape , seed = seed ( ) , dtype = self . dtype ) safe_conc = tf . where ( self . concentration > 0 , self . concentration , tf . ones_like ( self . concentration ) ) safe_z = tf . where ( z > 0 , z , tf . ones_like ( z ) ) safe_u = 1 + tf . reduce_logsumexp ( input_tensor = [ tf . math . log ( safe_z ) , tf . math . log1p ( - safe_z ) - 2 * safe_conc ] , axis = 0 ) / safe_conc u = tf . where ( self . concentration > tf . zeros_like ( safe_u ) , safe_u , 2 * z - 1 ) u = tf . where ( tf . equal ( z , 0 ) , - tf . ones_like ( u ) , u ) if not self . _allow_nan_stats : u = tf . debugging . check_numerics ( u , 'u in _sample_3d' ) return u [ ... , tf . newaxis ]
def _remove_dict_keys_with_value ( dict_ , val ) : return { k : v for k , v in dict_ . items ( ) if v is not val }
def _recursively_replace_dict_for_pretty_dict ( x ) : if isinstance ( x , dict ) : return _PrettyDict ( { k : _recursively_replace_dict_for_pretty_dict ( v ) for k , v in x . items ( ) } ) if ( isinstance ( x , collections . Sequence ) and not isinstance ( x , six . string_types ) ) : args = ( _recursively_replace_dict_for_pretty_dict ( x_ ) for x_ in x ) is_named_tuple = ( isinstance ( x , tuple ) and hasattr ( x , "_asdict" ) and hasattr ( x , "_fields" ) ) return type ( x ) ( * args ) if is_named_tuple else type ( x ) ( args ) if isinstance ( x , collections . Mapping ) : return type ( x ) ( * * { k : _recursively_replace_dict_for_pretty_dict ( v ) for k , v in x . items ( ) } ) return x
def _get_samples ( dist , z , n , seed ) : with tf . compat . v1 . name_scope ( 'get_samples' , values = [ z , n ] ) : if ( n is None ) == ( z is None ) : raise ValueError ( 'Must specify exactly one of arguments "n" and "z".  Found: ' 'n = %s, z = %s' % ( n , z ) ) if n is not None : return dist . sample ( n , seed = seed ) else : return tf . convert_to_tensor ( value = z , name = 'z' )
def is_namedtuple_like ( x ) : try : for fn in x . _fields : _ = getattr ( x , fn ) return True except AttributeError : return False
def make_name ( super_name , default_super_name , sub_name ) : name = super_name if super_name is not None else default_super_name if sub_name is not None : name += '_' + sub_name return name
def _choose_base_case ( is_accepted , accepted , rejected , name = None ) : def _expand_is_accepted_like ( x ) : """Helper to expand `is_accepted` like the shape of some input arg.""" with tf . compat . v1 . name_scope ( 'expand_is_accepted_like' ) : expand_shape = tf . concat ( [ tf . shape ( input = is_accepted ) , tf . ones ( [ tf . rank ( x ) - tf . rank ( is_accepted ) ] , dtype = tf . int32 ) , ] , axis = 0 ) multiples = tf . concat ( [ tf . ones ( [ tf . rank ( is_accepted ) ] , dtype = tf . int32 ) , tf . shape ( input = x ) [ tf . rank ( is_accepted ) : ] , ] , axis = 0 ) m = tf . tile ( tf . reshape ( is_accepted , expand_shape ) , multiples ) m . set_shape ( m . shape . merge_with ( x . shape ) ) return m def _where ( accepted , rejected ) : if accepted is rejected : return accepted accepted = tf . convert_to_tensor ( value = accepted , name = 'accepted' ) rejected = tf . convert_to_tensor ( value = rejected , name = 'rejected' ) r = tf . where ( _expand_is_accepted_like ( accepted ) , accepted , rejected ) r . set_shape ( r . shape . merge_with ( accepted . shape . merge_with ( rejected . shape ) ) ) return r with tf . compat . v1 . name_scope ( name , 'choose' , values = [ is_accepted , accepted , rejected ] ) : if not is_list_like ( accepted ) : return _where ( accepted , rejected ) return [ ( choose ( is_accepted , a , r , name = name ) if is_namedtuple_like ( a ) else _where ( a , r ) ) for a , r in zip ( accepted , rejected ) ]
def choose ( is_accepted , accepted , rejected , name = None ) : if not is_namedtuple_like ( accepted ) : return _choose_base_case ( is_accepted , accepted , rejected , name = name ) if not isinstance ( accepted , type ( rejected ) ) : raise TypeError ( 'Type of `accepted` ({}) must be identical to ' 'type of `rejected` ({})' . format ( type ( accepted ) . __name__ , type ( rejected ) . __name__ ) ) return type ( accepted ) ( * * dict ( [ ( fn , choose ( is_accepted , getattr ( accepted , fn ) , getattr ( rejected , fn ) , name = name ) ) for fn in accepted . _fields ] ) )
def _value_and_gradients ( fn , fn_arg_list , result = None , grads = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'value_and_gradients' , [ fn_arg_list , result , grads ] ) : def _convert_to_tensor ( x , name ) : ctt = lambda x_ : x_ if x_ is None else tf . convert_to_tensor ( value = x_ , name = name ) return [ ctt ( x_ ) for x_ in x ] if is_list_like ( x ) else ctt ( x ) fn_arg_list = ( list ( fn_arg_list ) if is_list_like ( fn_arg_list ) else [ fn_arg_list ] ) fn_arg_list = _convert_to_tensor ( fn_arg_list , 'fn_arg' ) if result is None : result = fn ( * fn_arg_list ) if grads is None and tf . executing_eagerly ( ) : fn_arg_list = [ 0 + x for x in fn_arg_list ] result = _convert_to_tensor ( result , 'fn_result' ) if grads is not None : grads = _convert_to_tensor ( grads , 'fn_grad' ) return result , grads if is_list_like ( result ) and len ( result ) == len ( fn_arg_list ) : def fn_slice ( i ) : """Needed to prevent `cell-var-from-loop` pylint warning.""" return lambda x : fn ( * ( fn_arg_list [ : i ] + [ x ] + fn_arg_list [ i + 1 : ] ) ) grads = [ tfp_math_value_and_gradients ( fn_slice ( i ) , fn_arg_list [ i ] ) [ 1 ] for i in range ( len ( result ) ) ] else : _ , grads = tfp_math_value_and_gradients ( fn , fn_arg_list ) return result , grads
def maybe_call_fn_and_grads ( fn , fn_arg_list , result = None , grads = None , check_non_none_grads = True , name = None ) : with tf . compat . v1 . name_scope ( name , 'maybe_call_fn_and_grads' , [ fn_arg_list , result , grads ] ) : fn_arg_list = ( list ( fn_arg_list ) if is_list_like ( fn_arg_list ) else [ fn_arg_list ] ) result , grads = _value_and_gradients ( fn , fn_arg_list , result , grads ) if not all ( r . dtype . is_floating for r in ( result if is_list_like ( result ) else [ result ] ) ) : raise TypeError ( 'Function result must be a `Tensor` with `float` ' '`dtype`.' ) if len ( fn_arg_list ) != len ( grads ) : raise ValueError ( 'Function args must be in one-to-one correspondence ' 'with grads.' ) if check_non_none_grads and any ( g is None for g in grads ) : raise ValueError ( 'Encountered `None` gradient.\n' '  fn_arg_list: {}\n' '  grads: {}' . format ( fn_arg_list , grads ) ) return result , grads
def _maybe_check_valid_shape ( shape , validate_args ) : if not dtype_util . is_integer ( shape . dtype ) : raise TypeError ( '{} dtype ({}) should be `int`-like.' . format ( shape , dtype_util . name ( shape . dtype ) ) ) assertions = [ ] message = '`{}` rank should be <= 1.' if tensorshape_util . rank ( shape . shape ) is not None : if tensorshape_util . rank ( shape . shape ) > 1 : raise ValueError ( message . format ( shape ) ) elif validate_args : assertions . append ( assert_util . assert_less ( tf . rank ( shape ) , 2 , message = message . format ( shape ) ) ) shape_ = tf . get_static_value ( shape ) message = '`{}` elements must have at most one `-1`.' if shape_ is not None : if sum ( shape_ == - 1 ) > 1 : raise ValueError ( message . format ( shape ) ) elif validate_args : assertions . append ( assert_util . assert_less ( tf . reduce_sum ( input_tensor = tf . cast ( tf . equal ( shape , - 1 ) , tf . int32 ) ) , 2 , message = message . format ( shape ) ) ) message = '`{}` elements must be either positive integers or `-1`.' if shape_ is not None : if np . any ( shape_ < - 1 ) : raise ValueError ( message . format ( shape ) ) elif validate_args : assertions . append ( assert_util . assert_greater ( shape , - 2 , message = message . format ( shape ) ) ) return assertions
def _maybe_assert_valid_sample ( self , x ) : if not self . validate_args : return x return distribution_util . with_dependencies ( [ assert_util . assert_positive ( x , message = "sample must be positive" ) , assert_util . assert_less ( x , 1. , message = "sample must be less than `1`." ) , ] , x )
def converged_any ( converged , failed ) : return ( tf . reduce_any ( input_tensor = converged ) | tf . reduce_all ( input_tensor = failed ) )
def _update_position ( state , position_delta , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) : failed = state . failed | ~ tf . math . is_finite ( next_objective ) | ~ tf . reduce_all ( input_tensor = tf . math . is_finite ( next_gradient ) , axis = - 1 ) next_position = state . position + position_delta converged = ~ failed & _check_convergence ( state . position , next_position , state . objective_value , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) return update_fields ( state , converged = state . converged | converged , failed = failed , position = next_position , objective_value = next_objective , objective_gradient = next_gradient )
def _check_convergence ( current_position , next_position , current_objective , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) : grad_converged = norm ( next_gradient , dims = 1 ) <= grad_tolerance x_converged = norm ( next_position - current_position , dims = 1 ) <= x_tolerance f_converged = ( norm ( next_objective - current_objective , dims = 0 ) <= f_relative_tolerance * current_objective ) return grad_converged | x_converged | f_converged
def _get_field ( kernel_results , field_name ) : if hasattr ( kernel_results , field_name ) : return getattr ( kernel_results , field_name ) if hasattr ( kernel_results , 'accepted_results' ) : return getattr ( kernel_results . accepted_results , field_name ) raise TypeError ( 'Cannot extract %s from %s' % ( field_name , kernel_results ) )
def _variance_scale_term ( self ) : c0 = self . total_concentration [ ... , tf . newaxis ] return tf . sqrt ( ( 1. + c0 / self . total_count [ ... , tf . newaxis ] ) / ( 1. + c0 ) )
def _maybe_assert_valid_concentration ( self , concentration , validate_args ) : if not validate_args : return concentration concentration = distribution_util . embed_check_categorical_event_shape ( concentration ) return distribution_util . with_dependencies ( [ assert_util . assert_positive ( concentration , message = "Concentration parameter must be positive." ) , ] , concentration )
def _maybe_assert_valid_sample ( self , counts ) : if not self . validate_args : return counts counts = distribution_util . embed_check_nonnegative_integer_form ( counts ) return distribution_util . with_dependencies ( [ assert_util . assert_equal ( self . total_count , tf . reduce_sum ( input_tensor = counts , axis = - 1 ) , message = "counts last-dimension must sum to `self.total_count`" ) , ] , counts )
def forward_log_det_jacobian_fn ( bijector ) : if not mcmc_util . is_list_like ( bijector ) : bijector = [ bijector ] def fn ( transformed_state_parts , event_ndims ) : return sum ( [ b . forward_log_det_jacobian ( sp , event_ndims = e ) for b , e , sp in zip ( bijector , event_ndims , transformed_state_parts ) ] ) return fn
def forward_transform_fn ( bijector ) : if not mcmc_util . is_list_like ( bijector ) : bijector = [ bijector ] def fn ( transformed_state_parts ) : return [ b . forward ( sp ) for b , sp in zip ( bijector , transformed_state_parts ) ] return fn
def inverse_transform_fn ( bijector ) : if not mcmc_util . is_list_like ( bijector ) : bijector = [ bijector ] def fn ( state_parts ) : return [ b . inverse ( sp ) for b , sp in zip ( bijector , state_parts ) ] return fn
def val_where ( cond , tval , fval ) : if isinstance ( tval , tf . Tensor ) : return tf . where ( cond , tval , fval ) elif isinstance ( tval , tuple ) : cls = type ( tval ) return cls ( * ( val_where ( cond , t , f ) for t , f in zip ( tval , fval ) ) ) else : raise Exception ( TypeError )
def _secant2_inner ( value_and_gradients_function , initial_args , val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) : update_result = update ( value_and_gradients_function , initial_args . left , initial_args . right , val_c , f_lim , active = initial_args . active ) active = initial_args . active & ~ update_result . failed failed = initial_args . failed | update_result . failed val_left = val_where ( active , update_result . left , initial_args . left ) val_right = val_where ( active , update_result . right , initial_args . right ) updated_left = active & tf . equal ( val_left . x , val_c . x ) updated_right = active & tf . equal ( val_right . x , val_c . x ) is_new = updated_left | updated_right next_c = tf . where ( updated_left , _secant ( initial_args . left , val_left ) , val_c . x ) next_c = tf . where ( updated_right , _secant ( initial_args . right , val_right ) , next_c ) in_range = ( val_left . x <= next_c ) & ( next_c <= val_right . x ) needs_extra_eval = tf . reduce_any ( input_tensor = in_range & is_new ) num_evals = initial_args . num_evals + update_result . num_evals num_evals = num_evals + tf . cast ( needs_extra_eval , num_evals . dtype ) next_args = _Secant2Result ( active = active & in_range , converged = initial_args . converged , failed = failed , num_evals = num_evals , left = val_left , right = val_right ) def _apply_inner_update ( ) : next_val_c = prefer_static . cond ( needs_extra_eval , ( lambda : value_and_gradients_function ( next_c ) ) , ( lambda : val_c ) ) return _secant2_inner_update ( value_and_gradients_function , next_args , val_0 , next_val_c , f_lim , sufficient_decrease_param , curvature_param ) return prefer_static . cond ( tf . reduce_any ( input_tensor = next_args . active ) , _apply_inner_update , lambda : next_args )
def _secant2_inner_update ( value_and_gradients_function , initial_args , val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) : new_failed = initial_args . active & ~ is_finite ( val_c ) active = initial_args . active & ~ new_failed failed = initial_args . failed | new_failed found_wolfe = active & _satisfies_wolfe ( val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) val_left = val_where ( found_wolfe , val_c , initial_args . left ) val_right = val_where ( found_wolfe , val_c , initial_args . right ) converged = initial_args . converged | found_wolfe active = active & ~ found_wolfe def _apply_update ( ) : update_result = update ( value_and_gradients_function , val_left , val_right , val_c , f_lim , active = active ) return _Secant2Result ( active = tf . zeros_like ( active ) , converged = converged , failed = failed | update_result . failed , num_evals = initial_args . num_evals + update_result . num_evals , left = update_result . left , right = update_result . right ) def _default ( ) : return _Secant2Result ( active = active , converged = converged , failed = failed , num_evals = initial_args . num_evals , left = val_left , right = val_right ) return prefer_static . cond ( tf . reduce_any ( input_tensor = active ) , _apply_update , _default )
def _bisect ( value_and_gradients_function , initial_args , f_lim ) : def _loop_cond ( curr ) : return ~ tf . reduce_all ( input_tensor = curr . stopped ) def _loop_body ( curr ) : """Narrow down interval to satisfy opposite slope conditions.""" mid = value_and_gradients_function ( ( curr . left . x + curr . right . x ) / 2 ) failed = ( curr . failed | ~ is_finite ( mid ) | tf . equal ( mid . x , curr . left . x ) | tf . equal ( mid . x , curr . right . x ) ) to_update = ~ ( curr . stopped | failed ) update_left = ( mid . df < 0 ) & ( mid . f <= f_lim ) left = val_where ( to_update & update_left , mid , curr . left ) right = val_where ( to_update & ~ update_left , mid , curr . right ) stopped = curr . stopped | failed | ( right . df >= 0 ) return [ _IntermediateResult ( iteration = curr . iteration , stopped = stopped , failed = failed , num_evals = curr . num_evals + 1 , left = left , right = right ) ] return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ initial_args ] ) [ 0 ]
def _prepare_args ( target_log_prob_fn , state , step_size , target_log_prob = None , grads_target_log_prob = None , maybe_expand = False , state_gradients_are_stopped = False ) : state_parts = list ( state ) if mcmc_util . is_list_like ( state ) else [ state ] state_parts = [ tf . convert_to_tensor ( value = s , name = 'current_state' ) for s in state_parts ] if state_gradients_are_stopped : state_parts = [ tf . stop_gradient ( x ) for x in state_parts ] target_log_prob , grads_target_log_prob = mcmc_util . maybe_call_fn_and_grads ( target_log_prob_fn , state_parts , target_log_prob , grads_target_log_prob ) step_sizes = ( list ( step_size ) if mcmc_util . is_list_like ( step_size ) else [ step_size ] ) step_sizes = [ tf . convert_to_tensor ( value = s , name = 'step_size' , dtype = target_log_prob . dtype ) for s in step_sizes ] if len ( step_sizes ) == 1 : step_sizes *= len ( state_parts ) if len ( state_parts ) != len ( step_sizes ) : raise ValueError ( 'There should be exactly one `step_size` or it should ' 'have same length as `current_state`.' ) def maybe_flatten ( x ) : return x if maybe_expand or mcmc_util . is_list_like ( state ) else x [ 0 ] return [ maybe_flatten ( state_parts ) , maybe_flatten ( step_sizes ) , target_log_prob , grads_target_log_prob , ]
def bootstrap_results ( self , init_state ) : kernel_results = self . _impl . bootstrap_results ( init_state ) if self . step_size_update_fn is not None : step_size_assign = self . step_size_update_fn ( self . step_size , None ) kernel_results = kernel_results . _replace ( extra = HamiltonianMonteCarloExtraKernelResults ( step_size_assign = step_size_assign ) ) return kernel_results
def _resnet_block ( x , filters , kernel , stride , kernel_posterior_fn ) : x = tf . keras . layers . BatchNormalization ( ) ( x ) x = tf . keras . layers . Activation ( 'relu' ) ( x ) if stride != 1 or filters != x . shape [ 1 ] : shortcut = _projection_shortcut ( x , filters , stride , kernel_posterior_fn ) else : shortcut = x x = tfp . layers . Convolution2DFlipout ( filters , kernel , strides = stride , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( x ) x = tf . keras . layers . BatchNormalization ( ) ( x ) x = tf . keras . layers . Activation ( 'relu' ) ( x ) x = tfp . layers . Convolution2DFlipout ( filters , kernel , strides = 1 , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( x ) x = tf . keras . layers . add ( [ x , shortcut ] ) return x
def deep_exponential_family ( data_size , feature_size , units , shape ) : w2 = ed . Gamma ( 0.1 , 0.3 , sample_shape = [ units [ 2 ] , units [ 1 ] ] , name = "w2" ) w1 = ed . Gamma ( 0.1 , 0.3 , sample_shape = [ units [ 1 ] , units [ 0 ] ] , name = "w1" ) w0 = ed . Gamma ( 0.1 , 0.3 , sample_shape = [ units [ 0 ] , feature_size ] , name = "w0" ) z2 = ed . Gamma ( 0.1 , 0.1 , sample_shape = [ data_size , units [ 2 ] ] , name = "z2" ) z1 = ed . Gamma ( shape , shape / tf . matmul ( z2 , w2 ) , name = "z1" ) z0 = ed . Gamma ( shape , shape / tf . matmul ( z1 , w1 ) , name = "z0" ) x = ed . Poisson ( tf . matmul ( z0 , w0 ) , name = "x" ) return x
def trainable_positive_deterministic ( shape , min_loc = 1e-3 , name = None ) : with tf . compat . v1 . variable_scope ( None , default_name = "trainable_positive_deterministic" ) : unconstrained_loc = tf . compat . v1 . get_variable ( "unconstrained_loc" , shape ) loc = tf . maximum ( tf . nn . softplus ( unconstrained_loc ) , min_loc ) rv = ed . Deterministic ( loc = loc , name = name ) return rv
def trainable_gamma ( shape , min_concentration = 1e-3 , min_scale = 1e-5 , name = None ) : with tf . compat . v1 . variable_scope ( None , default_name = "trainable_gamma" ) : unconstrained_concentration = tf . compat . v1 . get_variable ( "unconstrained_concentration" , shape , initializer = tf . compat . v1 . initializers . random_normal ( mean = 0.5 , stddev = 0.1 ) ) unconstrained_scale = tf . compat . v1 . get_variable ( "unconstrained_scale" , shape , initializer = tf . compat . v1 . initializers . random_normal ( stddev = 0.1 ) ) concentration = tf . maximum ( tf . nn . softplus ( unconstrained_concentration ) , min_concentration ) rate = tf . maximum ( 1. / tf . nn . softplus ( unconstrained_scale ) , 1. / min_scale ) rv = ed . Gamma ( concentration = concentration , rate = rate , name = name ) return rv
def _registered_kl ( type_a , type_b ) : hierarchy_a = tf_inspect . getmro ( type_a ) hierarchy_b = tf_inspect . getmro ( type_b ) dist_to_children = None kl_fn = None for mro_to_a , parent_a in enumerate ( hierarchy_a ) : for mro_to_b , parent_b in enumerate ( hierarchy_b ) : candidate_dist = mro_to_a + mro_to_b candidate_kl_fn = _DIVERGENCES . get ( ( parent_a , parent_b ) , None ) if not kl_fn or ( candidate_kl_fn and candidate_dist < dist_to_children ) : dist_to_children = candidate_dist kl_fn = candidate_kl_fn return kl_fn
def read_image ( filepath ) : im_bytes = tf . io . read_file ( filepath ) im = tf . image . decode_image ( im_bytes , channels = CHANNELS ) im = tf . image . convert_image_dtype ( im , tf . float32 ) return im
def download_sprites ( ) : filepath = os . path . join ( FLAGS . data_dir , DATA_SPRITES_DIR ) if not tf . io . gfile . exists ( filepath ) : if not tf . io . gfile . exists ( FLAGS . data_dir ) : tf . io . gfile . makedirs ( FLAGS . data_dir ) zip_name = "{}.zip" . format ( filepath ) urllib . request . urlretrieve ( DATA_SPRITES_URL , zip_name ) with zipfile . ZipFile ( zip_name , "r" ) as zip_file : zip_file . extractall ( FLAGS . data_dir ) tf . io . gfile . remove ( zip_name ) return filepath
def create_character ( skin , hair , top , pants ) : dtype = skin . dtype hair_mask = tf . cast ( hair [ ... , - 1 : ] <= 0 , dtype ) top_mask = tf . cast ( top [ ... , - 1 : ] <= 0 , dtype ) pants_mask = tf . cast ( pants [ ... , - 1 : ] <= 0 , dtype ) char = ( skin * hair_mask ) + hair char = ( char * top_mask ) + top char = ( char * pants_mask ) + pants return char
def create_random_seq ( character , action_metadata , direction , length = 8 ) : start = tf . random . uniform ( [ ] , maxval = action_metadata [ 1 ] , dtype = tf . int32 ) return create_seq ( character , action_metadata , direction , length , start )
def _maybe_validate_distributions ( distributions , dtype_override , validate_args ) : assertions = [ ] if not _is_iterable ( distributions ) or not distributions : raise ValueError ( '`distributions` must be a list of one or more ' 'distributions.' ) if dtype_override is None : dts = [ dtype_util . base_dtype ( d . dtype ) for d in distributions if d . dtype is not None ] if dts [ 1 : ] != dts [ : - 1 ] : raise TypeError ( 'Distributions must have same dtype; found: {}.' . format ( set ( dtype_util . name ( dt ) for dt in dts ) ) ) for d in distributions : if tensorshape_util . rank ( d . event_shape ) is not None : if tensorshape_util . rank ( d . event_shape ) != 1 : raise ValueError ( '`Distribution` must be vector variate, ' 'found event nimds: {}.' . format ( tensorshape_util . rank ( d . event_shape ) ) ) elif validate_args : assertions . append ( assert_util . assert_equal ( 1 , tf . size ( input = d . event_shape_tensor ( ) ) , message = '`Distribution` must be vector variate.' ) ) batch_shapes = [ d . batch_shape for d in distributions ] if all ( tensorshape_util . is_fully_defined ( b ) for b in batch_shapes ) : if batch_shapes [ 1 : ] != batch_shapes [ : - 1 ] : raise ValueError ( 'Distributions must have the same `batch_shape`; ' 'found: {}.' . format ( batch_shapes ) ) elif validate_args : batch_shapes = [ tensorshape_util . as_list ( d . batch_shape ) if tensorshape_util . is_fully_defined ( d . batch_shape ) else d . batch_shape_tensor ( ) for d in distributions ] assertions . extend ( assert_util . assert_equal ( b1 , b2 , message = 'Distribution `batch_shape`s must be identical.' ) for b1 , b2 in zip ( batch_shapes [ 1 : ] , batch_shapes [ : - 1 ] ) ) return assertions
def build_input_pipeline ( x_train , x_test , y_train , y_test , batch_size , valid_size ) : x_train = x_train . astype ( "float32" ) x_test = x_test . astype ( "float32" ) x_train /= 255 x_test /= 255 y_train = y_train . flatten ( ) y_test = y_test . flatten ( ) if FLAGS . subtract_pixel_mean : x_train_mean = np . mean ( x_train , axis = 0 ) x_train -= x_train_mean x_test -= x_train_mean print ( "x_train shape:" + str ( x_train . shape ) ) print ( str ( x_train . shape [ 0 ] ) + " train samples" ) print ( str ( x_test . shape [ 0 ] ) + " test samples" ) training_dataset = tf . data . Dataset . from_tensor_slices ( ( x_train , np . int32 ( y_train ) ) ) training_batches = training_dataset . shuffle ( 50000 , reshuffle_each_iteration = True ) . repeat ( ) . batch ( batch_size ) training_iterator = tf . compat . v1 . data . make_one_shot_iterator ( training_batches ) heldout_dataset = tf . data . Dataset . from_tensor_slices ( ( x_test , np . int32 ( y_test ) ) ) heldout_batches = heldout_dataset . repeat ( ) . batch ( valid_size ) heldout_iterator = tf . compat . v1 . data . make_one_shot_iterator ( heldout_batches ) handle = tf . compat . v1 . placeholder ( tf . string , shape = [ ] ) feedable_iterator = tf . compat . v1 . data . Iterator . from_string_handle ( handle , training_batches . output_types , training_batches . output_shapes ) images , labels = feedable_iterator . get_next ( ) return images , labels , handle , training_iterator , heldout_iterator
def build_fake_data ( ) : num_examples = 10 x_train = np . random . rand ( num_examples , * IMAGE_SHAPE ) . astype ( np . float32 ) y_train = np . random . permutation ( np . arange ( num_examples ) ) . astype ( np . int32 ) x_test = np . random . rand ( num_examples , * IMAGE_SHAPE ) . astype ( np . float32 ) y_test = np . random . permutation ( np . arange ( num_examples ) ) . astype ( np . int32 ) return ( x_train , y_train ) , ( x_test , y_test )
def _sort_tensor ( tensor ) : sorted_ , _ = tf . nn . top_k ( tensor , k = tf . shape ( input = tensor ) [ - 1 ] ) sorted_ . set_shape ( tensor . shape ) return sorted_
def _assert_ndims_statically ( x , expect_ndims = None , expect_ndims_at_least = None , expect_static = False ) : ndims = x . shape . ndims if ndims is None : if expect_static : raise ValueError ( 'Expected static ndims. Found: {}' . format ( x ) ) return if expect_ndims is not None and ndims != expect_ndims : raise ValueError ( 'ndims must be {}.  Found: {}' . format ( expect_ndims , ndims ) ) if expect_ndims_at_least is not None and ndims < expect_ndims_at_least : raise ValueError ( 'ndims must be at least {}. Found {}' . format ( expect_ndims_at_least , ndims ) )
def _batch_gather_with_broadcast ( params , indices , axis ) : leading_bcast_shape = tf . broadcast_dynamic_shape ( tf . shape ( input = params ) [ : axis ] , tf . shape ( input = indices ) [ : - 1 ] ) params += tf . zeros ( tf . concat ( ( leading_bcast_shape , tf . shape ( input = params ) [ axis : ] ) , axis = 0 ) , dtype = params . dtype ) indices += tf . zeros ( tf . concat ( ( leading_bcast_shape , tf . shape ( input = indices ) [ - 1 : ] ) , axis = 0 ) , dtype = indices . dtype ) return tf . compat . v1 . batch_gather ( params , indices )
def _broadcast_cat_event_and_params ( event , params , base_dtype ) : if dtype_util . is_integer ( event . dtype ) : pass elif dtype_util . is_floating ( event . dtype ) : event = tf . cast ( event , dtype = tf . int32 ) else : raise TypeError ( "`value` should have integer `dtype` or " "`self.dtype` ({})" . format ( base_dtype ) ) shape_known_statically = ( tensorshape_util . rank ( params . shape ) is not None and tensorshape_util . is_fully_defined ( params . shape [ : - 1 ] ) and tensorshape_util . is_fully_defined ( event . shape ) ) if not shape_known_statically or params . shape [ : - 1 ] != event . shape : params *= tf . ones_like ( event [ ... , tf . newaxis ] , dtype = params . dtype ) params_shape = tf . shape ( input = params ) [ : - 1 ] event *= tf . ones ( params_shape , dtype = event . dtype ) if tensorshape_util . rank ( params . shape ) is not None : tensorshape_util . set_shape ( event , params . shape [ : - 1 ] ) return event , params
def _broadcast_event_and_samples ( event , samples , event_ndims ) : samples_shape = tf . concat ( [ tf . shape ( input = samples ) [ : - event_ndims - 1 ] , tf . shape ( input = samples ) [ tf . rank ( samples ) - event_ndims : ] ] , axis = 0 ) event *= tf . ones ( samples_shape , dtype = event . dtype ) event = tf . expand_dims ( event , axis = - event_ndims - 1 ) samples *= tf . ones_like ( event , dtype = samples . dtype ) return event , samples
def _update_inv_hessian ( prev_state , next_state ) : should_update = ~ next_state . converged & ~ next_state . failed gradient_delta = next_state . objective_gradient - prev_state . objective_gradient position_delta = next_state . position - prev_state . position normalization_factor = tf . reduce_sum ( input_tensor = gradient_delta * position_delta , axis = - 1 ) should_update = should_update & ~ tf . equal ( normalization_factor , 0 ) def _do_update_inv_hessian ( ) : next_inv_hessian = _bfgs_inv_hessian_update ( gradient_delta , position_delta , normalization_factor , prev_state . inverse_hessian_estimate ) return bfgs_utils . update_fields ( next_state , inverse_hessian_estimate = tf . where ( should_update , next_inv_hessian , prev_state . inverse_hessian_estimate ) ) return prefer_static . cond ( tf . reduce_any ( input_tensor = should_update ) , _do_update_inv_hessian , lambda : next_state )
def _get_initial_state ( value_and_gradients_function , initial_position , num_correction_pairs , tolerance ) : init_args = bfgs_utils . get_initial_state_args ( value_and_gradients_function , initial_position , tolerance ) empty_queue = _make_empty_queue_for ( num_correction_pairs , initial_position ) init_args . update ( position_deltas = empty_queue , gradient_deltas = empty_queue ) return LBfgsOptimizerResults ( * * init_args )
def _von_mises_cdf_series ( x , concentration , num_terms , dtype ) : num_terms = tf . cast ( num_terms , dtype = dtype ) def loop_body ( n , rn , drn_dconcentration , vn , dvn_dconcentration ) : """One iteration of the series loop.""" denominator = 2. * n / concentration + rn ddenominator_dk = - 2. * n / concentration ** 2 + drn_dconcentration rn = 1. / denominator drn_dconcentration = - ddenominator_dk / denominator ** 2 multiplier = tf . sin ( n * x ) / n + vn vn = rn * multiplier dvn_dconcentration = ( drn_dconcentration * multiplier + rn * dvn_dconcentration ) n -= 1. return n , rn , drn_dconcentration , vn , dvn_dconcentration ( _ , _ , _ , vn , dvn_dconcentration ) = tf . while_loop ( cond = lambda n , * _ : n > 0. , body = loop_body , loop_vars = ( num_terms , tf . zeros_like ( x , name = "rn" ) , tf . zeros_like ( x , name = "drn_dconcentration" ) , tf . zeros_like ( x , name = "vn" ) , tf . zeros_like ( x , name = "dvn_dconcentration" ) , ) , ) cdf = .5 + x / ( 2. * np . pi ) + vn / np . pi dcdf_dconcentration = dvn_dconcentration / np . pi cdf_clipped = tf . clip_by_value ( cdf , 0. , 1. ) dcdf_dconcentration *= tf . cast ( ( cdf >= 0. ) & ( cdf <= 1. ) , dtype ) return cdf_clipped , dcdf_dconcentration
def _von_mises_cdf_normal ( x , concentration , dtype ) : def cdf_func ( concentration ) : """A helper function that is passed to value_and_gradient.""" z = ( ( np . sqrt ( 2. / np . pi ) / tf . math . bessel_i0e ( concentration ) ) * tf . sin ( .5 * x ) ) z2 = z ** 2 z3 = z2 * z z4 = z2 ** 2 c = 24. * concentration c1 = 56. xi = z - z3 / ( ( c - 2. * z2 - 16. ) / 3. - ( z4 + ( 7. / 4. ) * z2 + 167. / 2. ) / ( c - c1 - z2 + 3. ) ) ** 2 distrib = normal . Normal ( tf . cast ( 0. , dtype ) , tf . cast ( 1. , dtype ) ) return distrib . cdf ( xi ) return value_and_gradient ( cdf_func , concentration )
def _get_initial_args ( objective_function , initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob , seed ) : was_iterable = False if initial_position is not None : initial_position , was_iterable = _ensure_list ( initial_position ) if initial_population is not None : initial_population , was_iterable = _ensure_list ( initial_population ) population = _get_starting_population ( initial_population , initial_position , population_size , population_stddev , seed = seed ) differential_weight = tf . convert_to_tensor ( value = differential_weight , dtype = population [ 0 ] . dtype . base_dtype ) crossover_prob = tf . convert_to_tensor ( value = crossover_prob ) population_values = objective_function ( * population ) if max_iterations is not None : max_iterations = tf . convert_to_tensor ( value = max_iterations ) func_tolerance = tf . convert_to_tensor ( value = func_tolerance , dtype = population_values . dtype . base_dtype ) position_tolerance = tf . convert_to_tensor ( value = position_tolerance , dtype = population [ 0 ] . dtype . base_dtype ) return ( was_iterable , population , population_values , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob )
def _find_best_in_population ( population , values ) : best_value = tf . math . reduce_min ( input_tensor = values ) best_index = tf . where ( tf . math . equal ( values , best_value ) ) [ 0 , 0 ] return ( [ population_part [ best_index ] for population_part in population ] , best_value )
def _check_convergence ( population , population_values , func_tolerance , position_tolerance ) : value_range = tf . math . abs ( tf . math . reduce_max ( input_tensor = population_values ) - tf . math . reduce_min ( input_tensor = population_values ) ) value_converged = value_range <= func_tolerance half_tol = position_tolerance / 2 def part_converged ( part ) : return tf . math . reduce_max ( input_tensor = tf . math . abs ( part - part [ 0 ] ) ) <= half_tol x_converged = tf . math . reduce_all ( input_tensor = [ part_converged ( part ) for part in population ] ) return value_converged | x_converged
def _get_tol ( tol , dtype , validate_args ) : if tol is None : return tf . convert_to_tensor ( value = 0 , dtype = dtype ) tol = tf . convert_to_tensor ( value = tol , dtype = dtype ) if validate_args : tol = distribution_util . with_dependencies ( [ assert_util . assert_non_negative ( tol , message = "Argument 'tol' must be non-negative" ) ] , tol ) return tol
def build_input_pipeline ( train_images , batch_size ) : training_dataset = tf . data . Dataset . from_tensor_slices ( train_images ) training_batches = training_dataset . shuffle ( 50000 , reshuffle_each_iteration = True ) . repeat ( ) . batch ( batch_size ) training_iterator = tf . compat . v1 . data . make_one_shot_iterator ( training_batches ) images = training_iterator . get_next ( ) return images
def _hat_integral_inverse ( self , x ) : x = tf . cast ( x , self . power . dtype ) t = self . power - 1. return tf . math . expm1 ( - ( tf . math . log ( t ) + tf . math . log ( x ) ) / t )
def _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) : assertions = [ ] message = 'Input `lower_upper` must have at least 2 dimensions.' if lower_upper . shape . ndims is not None : if lower_upper . shape . ndims < 2 : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_rank_at_least ( lower_upper , rank = 2 , message = message ) ) message = '`rank(lower_upper)` must equal `rank(perm) + 1`' if lower_upper . shape . ndims is not None and perm . shape . ndims is not None : if lower_upper . shape . ndims != perm . shape . ndims + 1 : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_rank ( lower_upper , rank = tf . rank ( perm ) + 1 , message = message ) ) message = '`lower_upper` must be square.' if lower_upper . shape [ : - 2 ] . is_fully_defined ( ) : if lower_upper . shape [ - 2 ] != lower_upper . shape [ - 1 ] : raise ValueError ( message ) elif validate_args : m , n = tf . split ( tf . shape ( input = lower_upper ) [ - 2 : ] , num_or_size_splits = 2 ) assertions . append ( tf . compat . v1 . assert_equal ( m , n , message = message ) ) return assertions
def _lu_solve_assertions ( lower_upper , perm , rhs , validate_args ) : assertions = _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) message = 'Input `rhs` must have at least 2 dimensions.' if rhs . shape . ndims is not None : if rhs . shape . ndims < 2 : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_rank_at_least ( rhs , rank = 2 , message = message ) ) message = '`lower_upper.shape[-1]` must equal `rhs.shape[-1]`.' if ( tf . compat . dimension_value ( lower_upper . shape [ - 1 ] ) is not None and tf . compat . dimension_value ( rhs . shape [ - 2 ] ) is not None ) : if lower_upper . shape [ - 1 ] != rhs . shape [ - 2 ] : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_equal ( tf . shape ( input = lower_upper ) [ - 1 ] , tf . shape ( input = rhs ) [ - 2 ] , message = message ) ) return assertions
def _maybe_validate_matrix ( a , validate_args ) : assertions = [ ] if not a . dtype . is_floating : raise TypeError ( 'Input `a` must have `float`-like `dtype` ' '(saw {}).' . format ( a . dtype . name ) ) if a . shape . ndims is not None : if a . shape . ndims < 2 : raise ValueError ( 'Input `a` must have at least 2 dimensions ' '(saw: {}).' . format ( a . shape . ndims ) ) elif validate_args : assertions . append ( tf . compat . v1 . assert_rank_at_least ( a , rank = 2 , message = 'Input `a` must have at least 2 dimensions.' ) ) return assertions
def _gen_slices ( num_blocks , n_in , n_out , mask_type = MASK_EXCLUSIVE ) : slices = [ ] col = 0 d_in = n_in // num_blocks d_out = n_out // num_blocks row = d_out if mask_type == MASK_EXCLUSIVE else 0 for _ in range ( num_blocks ) : row_slice = slice ( row , None ) col_slice = slice ( col , col + d_in ) slices . append ( [ row_slice , col_slice ] ) col += d_in row += d_out return slices
def _gen_mask ( num_blocks , n_in , n_out , mask_type = MASK_EXCLUSIVE , dtype = tf . float32 ) : mask = np . zeros ( [ n_out , n_in ] , dtype = dtype . as_numpy_dtype ( ) ) slices = _gen_slices ( num_blocks , n_in , n_out , mask_type = mask_type ) for [ row_slice , col_slice ] in slices : mask [ row_slice , col_slice ] = 1 return mask
def _create_input_order ( input_size , input_order = "left-to-right" ) : if isinstance ( input_order , six . string_types ) : if input_order == "left-to-right" : return np . arange ( start = 1 , stop = input_size + 1 ) elif input_order == "right-to-left" : return np . arange ( start = input_size , stop = 0 , step = - 1 ) elif input_order == "random" : ret = np . arange ( start = 1 , stop = input_size + 1 ) np . random . shuffle ( ret ) return ret elif np . all ( np . sort ( input_order ) == np . arange ( 1 , input_size + 1 ) ) : return np . array ( input_order ) raise ValueError ( "Invalid input order: '{}'." . format ( input_order ) )
def _create_masks ( degrees ) : return [ inp [ : , np . newaxis ] <= out for inp , out in zip ( degrees [ : - 1 ] , degrees [ 1 : ] ) ] + [ degrees [ - 1 ] [ : , np . newaxis ] < degrees [ 0 ] ]
def _make_masked_initializer ( mask , initializer ) : initializer = tf . keras . initializers . get ( initializer ) def masked_initializer ( shape , dtype = None , partition_info = None ) : if partition_info is None : x = initializer ( shape , dtype ) else : x = initializer ( shape , dtype , partition_info ) return tf . cast ( mask , x . dtype ) * x return masked_initializer
def build ( self , input_shape ) : if self . _event_shape is None : self . _event_shape = [ tf . compat . dimension_value ( input_shape [ - 1 ] ) ] self . _event_size = self . _event_shape [ - 1 ] self . _event_ndims = len ( self . _event_shape ) if input_shape [ - 1 ] != self . _event_shape [ - 1 ] : raise ValueError ( "Invalid final dimension of `input_shape`. " "Expected `{!r}`, but got `{!r}`" . format ( self . _event_shape [ - 1 ] , input_shape [ - 1 ] ) ) self . _input_order = _create_input_order ( self . _event_size , self . _input_order_param ) self . _masks = _create_masks ( _create_degrees ( input_size = self . _event_size , hidden_units = self . _hidden_units , input_order = self . _input_order , hidden_degrees = self . _hidden_degrees ) ) # self . _masks [ - 1 ] = np . reshape ( np . tile ( self . _masks [ - 1 ] [ ... , tf . newaxis ] , [ 1 , 1 , self . _params ] ) , [ self . _masks [ - 1 ] . shape [ 0 ] , self . _event_size * self . _params ] ) self . _network = tf . keras . Sequential ( [ tf . keras . layers . InputLayer ( ( self . _event_size , ) , dtype = self . dtype ) ] ) layer_output_sizes = self . _hidden_units + [ self . _event_size * self . _params ] for k in range ( len ( self . _masks ) ) : self . _network . add ( tf . keras . layers . Dense ( layer_output_sizes [ k ] , kernel_initializer = _make_masked_initializer ( self . _masks [ k ] , self . _kernel_initializer ) , kernel_constraint = _make_masked_constraint ( self . _masks [ k ] ) , activation = self . _activation if k + 1 < len ( self . _masks ) else None , use_bias = self . _use_bias , * * self . _kwargs ) ) super ( AutoregressiveLayer , self ) . build ( input_shape )
def call ( self , x ) : with tf . compat . v2 . name_scope ( self . name or "AutoregressiveLayer_call" ) : x = tf . convert_to_tensor ( value = x , dtype = self . dtype , name = "x" ) input_shape = tf . shape ( input = x ) if tensorshape_util . rank ( x . shape ) == 1 : x = x [ tf . newaxis , ... ] return tf . reshape ( self . _network ( x ) , tf . concat ( [ input_shape , [ self . _params ] ] , axis = 0 ) )
def _zero_dimensional_mvndiag ( dtype ) : dummy_mvndiag = tfd . MultivariateNormalDiag ( scale_diag = tf . ones ( [ 0 ] , dtype = dtype ) ) dummy_mvndiag . covariance = lambda : dummy_mvndiag . variance ( ) [ ... , tf . newaxis ] return dummy_mvndiag
def _observe_timeseries_fn ( timeseries ) : def observation_noise_fn ( t ) : current_slice = timeseries [ ... , t , : ] return tfd . MultivariateNormalDiag ( loc = current_slice , scale_diag = tf . zeros_like ( current_slice ) ) return observation_noise_fn
def params_to_weights ( self , global_scale_variance , global_scale_noncentered , local_scale_variances , local_scales_noncentered , weights_noncentered ) : global_scale = ( global_scale_noncentered * tf . sqrt ( global_scale_variance ) * self . weights_prior_scale ) local_scales = local_scales_noncentered * tf . sqrt ( local_scale_variances ) return weights_noncentered * local_scales * global_scale [ ... , tf . newaxis ]
def _depth ( g ) : def _explore ( v ) : if v . depth < 0 : v . depth = ( ( 1 + max ( [ - 1 ] + [ _explore ( annotated_graph [ u ] ) for u in v . parents ] ) ) if v . parents else 0 ) return v . depth annotated_graph = { k : _Node ( k , v ) for k , v in g . items ( ) } for v in annotated_graph . values ( ) : _explore ( v ) return annotated_graph
def _best_order ( g ) : def _explore ( u ) : """Recursive function to ascend up through unvisited dependencies.""" if u . depth < 0 : return if not u . parents : result . append ( ( u . name , u . parents ) ) u . depth = - 1 return b = ( u . name , [ ] ) result . append ( b ) u . depth = - 1 d = 0 for v in sorted ( ( g . get ( p ) for p in u . parents ) , key = lambda v : v . depth ) : n0 = len ( result ) _explore ( v ) n1 = len ( result ) b [ 1 ] . extend ( [ '_' ] * d + [ v . name ] ) d = n1 - n0 - 1 g = _depth ( g ) result = [ ] for u in sorted ( g . values ( ) , key = lambda v : v . depth , reverse = True ) : _explore ( u ) return tuple ( reversed ( result ) )
def _prob_chain_rule_flatten ( named_makers ) : def _make ( dist_fn , args ) : if args is None : return lambda * _ : dist_fn if not args : return lambda * _ : dist_fn ( ) def _fn ( * xs ) : kwargs = dict ( zip ( args , reversed ( xs [ - len ( args ) : ] ) ) ) kwargs . pop ( '_' , None ) return dist_fn ( * * kwargs ) return _fn named_makers = _convert_to_dict ( named_makers ) g = { k : ( None if distribution_util . is_distribution_instance ( v ) else joint_distribution_sequential . _get_required_args ( v ) ) for k , v in named_makers . items ( ) } g = _best_order ( g ) dist_fn_name , dist_fn_args = zip ( * g ) dist_fn_args = tuple ( None if a is None else tuple ( a ) for a in dist_fn_args ) dist_fn_wrapped = tuple ( _make ( named_makers [ name ] , parents ) for ( name , parents ) in g ) dist_fn = tuple ( named_makers . get ( n ) for n in dist_fn_name ) return dist_fn , dist_fn_wrapped , dist_fn_args , dist_fn_name
def _build ( self , model ) : if not _is_dict_like ( model ) : raise TypeError ( '`model` must be convertible to `dict` (saw: {}).' . format ( type ( model ) . __name__ ) ) [ self . _dist_fn , self . _dist_fn_wrapped , self . _dist_fn_args , self . _dist_fn_name , ] = _prob_chain_rule_flatten ( model )
def build_is_last_day_of_season ( num_steps_per_season ) : num_steps_per_cycle = np . sum ( num_steps_per_season ) changepoints = np . cumsum ( np . ravel ( num_steps_per_season ) ) - 1 def is_last_day_of_season ( t ) : t_ = dist_util . maybe_get_static_value ( t ) if t_ is not None : step_in_cycle = t_ % num_steps_per_cycle return any ( step_in_cycle == changepoints ) else : step_in_cycle = tf . math . floormod ( t , num_steps_per_cycle ) return tf . reduce_any ( input_tensor = tf . equal ( step_in_cycle , changepoints ) ) return is_last_day_of_season
def build_seasonal_transition_matrix ( num_seasons , is_last_day_of_season , dtype , basis_change_matrix = None , basis_change_matrix_inv = None ) : with tf . compat . v1 . name_scope ( 'build_seasonal_transition_matrix' ) : seasonal_permutation = np . concatenate ( [ np . arange ( 1 , num_seasons ) , [ 0 ] ] , axis = 0 ) seasonal_permutation_matrix = tf . constant ( np . eye ( num_seasons ) [ seasonal_permutation ] , dtype = dtype ) if basis_change_matrix is not None : seasonal_permutation_matrix = tf . matmul ( basis_change_matrix , tf . matmul ( seasonal_permutation_matrix , basis_change_matrix_inv ) ) identity_matrix = tf . eye ( tf . shape ( input = seasonal_permutation_matrix ) [ - 1 ] , dtype = dtype ) def seasonal_transition_matrix ( t ) : return tf . linalg . LinearOperatorFullMatrix ( matrix = dist_util . pick_scalar_condition ( is_last_day_of_season ( t ) , seasonal_permutation_matrix , identity_matrix ) ) return seasonal_transition_matrix
def build_seasonal_transition_noise ( drift_scale , num_seasons , is_last_day_of_season ) : drift_scale_diag = tf . stack ( [ tf . zeros_like ( drift_scale ) ] * ( num_seasons - 1 ) + [ drift_scale ] , axis = - 1 ) def seasonal_transition_noise ( t ) : noise_scale_diag = dist_util . pick_scalar_condition ( is_last_day_of_season ( t ) , drift_scale_diag , tf . zeros_like ( drift_scale_diag ) ) return tfd . MultivariateNormalDiag ( loc = tf . zeros ( num_seasons , dtype = drift_scale . dtype ) , scale_diag = noise_scale_diag ) return seasonal_transition_noise
def build_constrained_seasonal_transition_noise ( drift_scale , num_seasons , is_last_day_of_season ) : # # # drift_scale_tril_nonzeros = tf . concat ( [ tf . ones ( [ num_seasons - 1 , 1 ] , dtype = drift_scale . dtype ) , tf . zeros ( [ num_seasons - 1 , num_seasons - 2 ] , dtype = drift_scale . dtype ) ] , axis = - 1 ) drift_scale_tril = ( drift_scale_tril_nonzeros * drift_scale [ ... , tf . newaxis , tf . newaxis ] / num_seasons ) def seasonal_transition_noise ( t ) : noise_scale_tril = dist_util . pick_scalar_condition ( is_last_day_of_season ( t ) , drift_scale_tril , tf . zeros_like ( drift_scale_tril ) ) return tfd . MultivariateNormalTriL ( loc = tf . zeros ( num_seasons - 1 , dtype = drift_scale . dtype ) , scale_tril = noise_scale_tril ) return seasonal_transition_noise
def optimize ( self ) : jmodel = callJavaFunc ( self . value . optimize ) from bigdl . nn . layer import Layer return Layer . of ( jmodel )
def get_end_trigger ( options ) : if options . endTriggerType . lower ( ) == "epoch" : return MaxEpoch ( options . endTriggerNum ) else : return MaxIteration ( options . endTriggerNum )
def validate_optimizer ( optimizer , test_data , options ) : optimizer . set_validation ( batch_size = options . batchSize , val_rdd = test_data , trigger = EveryEpoch ( ) , val_method = [ Top1Accuracy ( ) ] ) optimizer . set_checkpoint ( EveryEpoch ( ) , options . checkpointPath )
def value ( self ) : if not hasattr ( self , "_value" ) and self . _path is not None : self . _value = self . _load ( self . _path ) return self . _value
def callBigDlFunc ( bigdl_type , name , * args ) : gateway = _get_gateway ( ) error = Exception ( "Cannot find function: %s" % name ) for jinvoker in JavaCreator . instance ( bigdl_type , gateway ) . value : try : api = getattr ( jinvoker , name ) result = callJavaFunc ( api , * args ) except Exception as e : error = e if "does not exist" not in str ( e ) : raise e else : return result raise error
def _py2java ( gateway , obj ) : if isinstance ( obj , RDD ) : obj = _to_java_object_rdd ( obj ) elif isinstance ( obj , DataFrame ) : obj = obj . _jdf elif isinstance ( obj , SparkContext ) : obj = obj . _jsc elif isinstance ( obj , ( list , tuple ) ) : obj = ListConverter ( ) . convert ( [ _py2java ( gateway , x ) for x in obj ] , gateway . _gateway_client ) elif isinstance ( obj , dict ) : result = { } for ( key , value ) in obj . items ( ) : result [ key ] = _py2java ( gateway , value ) obj = MapConverter ( ) . convert ( result , gateway . _gateway_client ) elif isinstance ( obj , JavaValue ) : obj = obj . value elif isinstance ( obj , JavaObject ) : pass elif isinstance ( obj , ( int , long , float , bool , bytes , unicode ) ) : pass else : data = bytearray ( PickleSerializer ( ) . dumps ( obj ) ) obj = gateway . jvm . org . apache . spark . bigdl . api . python . BigDLSerDe . loads ( data ) return obj
def get_label ( self ) : label = callBigDlFunc ( self . bigdl_type , "imageFeatureToLabelTensor" , self . value ) return label . to_ndarray ( )
def read_parquet ( cls , path , sc , bigdl_type = "float" ) : return DistributedImageFrame ( jvalue = callBigDlFunc ( bigdl_type , "readParquet" , path , sc ) )
def write_parquet ( cls , path , output , sc , partition_num = 1 , bigdl_type = "float" ) : return callBigDlFunc ( bigdl_type , "writeParquet" , path , output , sc , partition_num )
def get_image ( self , float_key = "floats" , to_chw = True ) : return self . image_frame . get_image ( float_key , to_chw )
def get_image ( self , float_key = "floats" , to_chw = True ) : tensors = callBigDlFunc ( self . bigdl_type , "localImageFrameToImageTensor" , self . value , float_key , to_chw ) return map ( lambda tensor : tensor . to_ndarray ( ) , tensors )
def get_label ( self ) : tensor_rdd = callBigDlFunc ( self . bigdl_type , "distributedImageFrameToLabelTensorRdd" , self . value ) return tensor_rdd . map ( lambda tensor : tensor . to_ndarray ( ) )
def get_predict ( self , key = "predict" ) : predicts = callBigDlFunc ( self . bigdl_type , "distributedImageFrameToPredict" , self . value , key ) return predicts . map ( lambda predict : ( predict [ 0 ] , predict [ 1 ] . to_ndarray ( ) ) if predict [ 1 ] else ( predict [ 0 ] , None ) )
def save_keras_definition ( keras_model , path ) : model_json = keras_model . to_json ( ) with open ( path , "w" ) as json_file : json_file . write ( model_json )
def build_keras_model ( ) : from keras . models import Sequential from keras . layers import Dense , Dropout , Activation , Flatten from keras . layers import Convolution2D , MaxPooling2D keras_model = Sequential ( ) keras_model . add ( Convolution2D ( 32 , 3 , 3 , border_mode = 'valid' , input_shape = input_shape ) ) keras_model . add ( Activation ( 'relu' ) ) keras_model . add ( Convolution2D ( 32 , 3 , 3 ) ) keras_model . add ( Activation ( 'relu' ) ) keras_model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ) ) ) keras_model . add ( Dropout ( 0.25 ) ) keras_model . add ( Flatten ( ) ) keras_model . add ( Dense ( 128 ) ) keras_model . add ( Activation ( 'relu' ) ) keras_model . add ( Dropout ( 0.5 ) ) keras_model . add ( Dense ( 10 ) ) keras_model . add ( Activation ( 'softmax' ) ) return keras_model
def training ( self , is_training = True ) : if is_training : callJavaFunc ( self . value . training ) else : callJavaFunc ( self . value . evaluate ) return self
def build_keras_model ( ) : from keras . models import Sequential from keras . layers import Dense , Dropout , Activation from keras . layers import Embedding from keras . layers import LSTM from keras . layers import Convolution1D , MaxPooling1D keras_model = Sequential ( ) keras_model . add ( Embedding ( 20000 , 128 , input_length = 100 ) ) keras_model . add ( Dropout ( 0.25 ) ) keras_model . add ( Convolution1D ( nb_filter = 64 , filter_length = 5 , border_mode = 'valid' , activation = 'relu' , subsample_length = 1 ) ) keras_model . add ( MaxPooling1D ( pool_length = 4 ) ) keras_model . add ( LSTM ( 70 ) ) keras_model . add ( Dense ( 1 ) ) keras_model . add ( Activation ( 'sigmoid' ) ) return keras_model
def get_bigdl_classpath ( ) : if os . getenv ( "BIGDL_CLASSPATH" ) : return os . environ [ "BIGDL_CLASSPATH" ] jar_dir = os . path . abspath ( __file__ + "/../../" ) jar_paths = glob . glob ( os . path . join ( jar_dir , "share/lib/*.jar" ) ) if jar_paths : assert len ( jar_paths ) == 1 , "Expecting one jar: %s" % len ( jar_paths ) return jar_paths [ 0 ] return ""
def is_spark_below_2_2 ( ) : import pyspark if ( hasattr ( pyspark , "version" ) ) : full_version = pyspark . version . __version__ parts = full_version . split ( "." ) spark_version = parts [ 0 ] + "." + parts [ 1 ] if ( compare_version ( spark_version , "2.2" ) >= 0 ) : return False return True
def attention ( inputs , state , att_size , mask , scope = "attention" ) : with tf . variable_scope ( scope ) : u = tf . concat ( [ tf . tile ( tf . expand_dims ( state , axis = 1 ) , [ 1 , tf . shape ( inputs ) [ 1 ] , 1 ] ) , inputs ] , axis = 2 ) logits = tf . layers . dense ( tf . layers . dense ( u , att_size , activation = tf . nn . tanh ) , 1 , use_bias = False ) logits = softmax_mask ( tf . squeeze ( logits , [ 2 ] ) , mask ) att_weights = tf . expand_dims ( tf . nn . softmax ( logits ) , axis = 2 ) res = tf . reduce_sum ( att_weights * inputs , axis = 1 ) return res , logits
def summary_gradient_updates ( grads , opt , lr ) : vars_grads = { } for v in tf . trainable_variables ( ) : vars_grads [ v . name ] = [ v , None , None ] for g , v in grads : vars_grads [ v . name ] [ 1 ] = g vars_grads [ v . name ] [ 2 ] = opt . get_slot ( v , 'accumulator' ) ret = [ ] for vname , ( v , g , a ) in vars_grads . items ( ) : if g is None : continue if isinstance ( g , tf . IndexedSlices ) : updates = lr * g . values if a is not None : updates /= tf . sqrt ( tf . gather ( a , g . indices ) ) else : updates = lr * g if a is not None : updates /= tf . sqrt ( a ) values_norm = tf . sqrt ( tf . reduce_sum ( v * v ) ) + 1.0e-7 updates_norm = tf . sqrt ( tf . reduce_sum ( updates * updates ) ) ret . append ( tf . summary . scalar ( 'UPDATE/' + vname . replace ( ":" , "_" ) , updates_norm / values_norm ) ) return ret
def dump_weights ( tf_save_dir , outfile , options ) : def _get_outname ( tf_name ) : outname = re . sub ( ':0$' , '' , tf_name ) outname = outname . lstrip ( 'lm/' ) outname = re . sub ( '/rnn/' , '/RNN/' , outname ) outname = re . sub ( '/multi_rnn_cell/' , '/MultiRNNCell/' , outname ) outname = re . sub ( '/cell_' , '/Cell' , outname ) outname = re . sub ( '/lstm_cell/' , '/LSTMCell/' , outname ) if '/RNN/' in outname : if 'projection' in outname : outname = re . sub ( 'projection/kernel' , 'W_P_0' , outname ) else : outname = re . sub ( '/kernel' , '/W_0' , outname ) outname = re . sub ( '/bias' , '/B' , outname ) return outname ckpt_file = tf . train . latest_checkpoint ( tf_save_dir ) config = tf . ConfigProto ( allow_soft_placement = True ) with tf . Graph ( ) . as_default ( ) : with tf . Session ( config = config ) as sess : with tf . variable_scope ( 'lm' ) : LanguageModel ( options , False ) loader = tf . train . Saver ( ) loader . restore ( sess , ckpt_file ) with h5py . File ( outfile , 'w' ) as fout : for v in tf . trainable_variables ( ) : if v . name . find ( 'softmax' ) >= 0 : continue outname = _get_outname ( v . name ) shape = v . get_shape ( ) . as_list ( ) dset = fout . create_dataset ( outname , shape , dtype = 'float32' ) values = sess . run ( [ v ] ) [ 0 ] dset [ ... ] = values
def read_data_by_config ( config : dict ) : dataset_config = config . get ( 'dataset' , None ) if dataset_config : config . pop ( 'dataset' ) ds_type = dataset_config [ 'type' ] if ds_type == 'classification' : reader = { 'class_name' : 'basic_classification_reader' } iterator = { 'class_name' : 'basic_classification_iterator' } config [ 'dataset_reader' ] = { * * dataset_config , * * reader } config [ 'dataset_iterator' ] = { * * dataset_config , * * iterator } else : raise Exception ( "Unsupported dataset type: {}" . format ( ds_type ) ) try : reader_config = dict ( config [ 'dataset_reader' ] ) except KeyError : raise ConfigError ( "No dataset reader is provided in the JSON config." ) reader = get_model ( reader_config . pop ( 'class_name' ) ) ( ) data_path = reader_config . pop ( 'data_path' , '' ) if isinstance ( data_path , list ) : data_path = [ expand_path ( x ) for x in data_path ] else : data_path = expand_path ( data_path ) return reader . read ( data_path , * * reader_config )
def train_evaluate_model_from_config ( config : Union [ str , Path , dict ] , iterator : Union [ DataLearningIterator , DataFittingIterator ] = None , * , to_train : bool = True , evaluation_targets : Optional [ Iterable [ str ] ] = None , to_validate : Optional [ bool ] = None , download : bool = False , start_epoch_num : Optional [ int ] = None , recursive : bool = False ) -> Dict [ str , Dict [ str , float ] ] : config = parse_config ( config ) if download : deep_download ( config ) if to_train and recursive : for subconfig in get_all_elems_from_json ( config [ 'chainer' ] , 'config_path' ) : log . info ( f'Training "{subconfig}"' ) train_evaluate_model_from_config ( subconfig , download = False , recursive = True ) import_packages ( config . get ( 'metadata' , { } ) . get ( 'imports' , [ ] ) ) if iterator is None : try : data = read_data_by_config ( config ) except ConfigError as e : to_train = False log . warning ( f'Skipping training. {e.message}' ) else : iterator = get_iterator_from_config ( config , data ) if 'train' not in config : log . warning ( 'Train config is missing. Populating with default values' ) train_config = config . get ( 'train' ) if start_epoch_num is not None : train_config [ 'start_epoch_num' ] = start_epoch_num if 'evaluation_targets' not in train_config and ( 'validate_best' in train_config or 'test_best' in train_config ) : log . warning ( '"validate_best" and "test_best" parameters are deprecated.' ' Please, use "evaluation_targets" list instead' ) train_config [ 'evaluation_targets' ] = [ ] if train_config . pop ( 'validate_best' , True ) : train_config [ 'evaluation_targets' ] . append ( 'valid' ) if train_config . pop ( 'test_best' , True ) : train_config [ 'evaluation_targets' ] . append ( 'test' ) trainer_class = get_model ( train_config . pop ( 'class_name' , 'nn_trainer' ) ) trainer = trainer_class ( config [ 'chainer' ] , * * train_config ) if to_train : trainer . train ( iterator ) res = { } if iterator is not None : if to_validate is not None : if evaluation_targets is None : log . warning ( '"to_validate" parameter is deprecated and will be removed in future versions.' ' Please, use "evaluation_targets" list instead' ) evaluation_targets = [ 'test' ] if to_validate : evaluation_targets . append ( 'valid' ) else : log . warn ( 'Both "evaluation_targets" and "to_validate" parameters are specified.' ' "to_validate" is deprecated and will be ignored' ) res = trainer . evaluate ( iterator , evaluation_targets , print_reports = True ) trainer . get_chainer ( ) . destroy ( ) res = { k : v [ 'metrics' ] for k , v in res . items ( ) } return res
def load ( self ) -> None : if self . load_path . exists ( ) : path = str ( self . load_path . resolve ( ) ) log . info ( '[loading model from {}]' . format ( path ) ) self . _net . load ( path )
def build ( self ) : word_inputs = kl . Input ( shape = ( None , MAX_WORD_LENGTH + 2 ) , dtype = "int32" ) inputs = [ word_inputs ] word_outputs = self . _build_word_cnn ( word_inputs ) if len ( self . word_vectorizers ) > 0 : additional_word_inputs = [ kl . Input ( shape = ( None , input_dim ) , dtype = "float32" ) for input_dim , dense_dim in self . word_vectorizers ] inputs . extend ( additional_word_inputs ) additional_word_embeddings = [ kl . Dense ( dense_dim ) ( additional_word_inputs [ i ] ) for i , ( _ , dense_dim ) in enumerate ( self . word_vectorizers ) ] word_outputs = kl . Concatenate ( ) ( [ word_outputs ] + additional_word_embeddings ) outputs , lstm_outputs = self . _build_basic_network ( word_outputs ) compile_args = { "optimizer" : ko . nadam ( lr = 0.002 , clipnorm = 5.0 ) , "loss" : "categorical_crossentropy" , "metrics" : [ "accuracy" ] } self . model_ = Model ( inputs , outputs ) self . model_ . compile ( * * compile_args ) if self . verbose > 0 : self . model_ . summary ( print_fn = log . info ) return self
def _build_word_cnn ( self , inputs ) : inputs = kl . Lambda ( kb . one_hot , arguments = { "num_classes" : self . symbols_number_ } , output_shape = lambda x : tuple ( x ) + ( self . symbols_number_ , ) ) ( inputs ) char_embeddings = kl . Dense ( self . char_embeddings_size , use_bias = False ) ( inputs ) conv_outputs = [ ] self . char_output_dim_ = 0 for window_size , filters_number in zip ( self . char_window_size , self . char_filters ) : curr_output = char_embeddings curr_filters_number = ( min ( self . char_filter_multiple * window_size , 200 ) if filters_number is None else filters_number ) for _ in range ( self . char_conv_layers - 1 ) : curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = "same" , activation = "relu" , data_format = "channels_last" ) ( curr_output ) if self . conv_dropout > 0.0 : curr_output = kl . Dropout ( self . conv_dropout ) ( curr_output ) curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = "same" , activation = "relu" , data_format = "channels_last" ) ( curr_output ) conv_outputs . append ( curr_output ) self . char_output_dim_ += curr_filters_number if len ( conv_outputs ) > 1 : conv_output = kl . Concatenate ( axis = - 1 ) ( conv_outputs ) else : conv_output = conv_outputs [ 0 ] highway_input = kl . Lambda ( kb . max , arguments = { "axis" : - 2 } ) ( conv_output ) if self . intermediate_dropout > 0.0 : highway_input = kl . Dropout ( self . intermediate_dropout ) ( highway_input ) for i in range ( self . char_highway_layers - 1 ) : highway_input = Highway ( activation = "relu" ) ( highway_input ) if self . highway_dropout > 0.0 : highway_input = kl . Dropout ( self . highway_dropout ) ( highway_input ) highway_output = Highway ( activation = "relu" ) ( highway_input ) return highway_output
def main ( ) : args = parser . parse_args ( ) path = get_settings_path ( ) if args . default : if populate_settings_dir ( force = True ) : print ( f'Populated {path} with default settings files' ) else : print ( f'{path} is already a default settings directory' ) else : print ( f'Current DeepPavlov settings path: {path}' )
def _graph_wrap ( func , graph ) : @ wraps ( func ) def _wrapped ( * args , * * kwargs ) : with graph . as_default ( ) : return func ( * args , * * kwargs ) return _wrapped
def _keras_wrap ( func , graph , session ) : import keras . backend as K @ wraps ( func ) def _wrapped ( * args , * * kwargs ) : with graph . as_default ( ) : K . set_session ( session ) return func ( * args , * * kwargs ) return _wrapped
def prettify_metrics ( metrics : List [ Tuple [ str , float ] ] , precision : int = 4 ) -> OrderedDict : prettified_metrics = OrderedDict ( ) for key , value in metrics : value = round ( value , precision ) prettified_metrics [ key ] = value return prettified_metrics
def load ( self , exclude_scopes : tuple = ( 'Optimizer' , ) ) -> None : if not hasattr ( self , 'sess' ) : raise RuntimeError ( 'Your TensorFlow model {} must' ' have sess attribute!' . format ( self . __class__ . __name__ ) ) path = str ( self . load_path . resolve ( ) ) if tf . train . checkpoint_exists ( path ) : log . info ( '[loading model from {}]' . format ( path ) ) var_list = self . _get_saveable_variables ( exclude_scopes ) saver = tf . train . Saver ( var_list ) saver . restore ( self . sess , path )
def save ( self , exclude_scopes : tuple = ( 'Optimizer' , ) ) -> None : if not hasattr ( self , 'sess' ) : raise RuntimeError ( 'Your TensorFlow model {} must' ' have sess attribute!' . format ( self . __class__ . __name__ ) ) path = str ( self . save_path . resolve ( ) ) log . info ( '[saving model to {}]' . format ( path ) ) var_list = self . _get_saveable_variables ( exclude_scopes ) saver = tf . train . Saver ( var_list ) saver . save ( self . sess , path )
def print_number_of_parameters ( ) : log . info ( 'Number of parameters: ' ) variables = tf . trainable_variables ( ) blocks = defaultdict ( int ) for var in variables : block_name = var . name . split ( '/' ) [ 0 ] number_of_parameters = np . prod ( var . get_shape ( ) . as_list ( ) ) blocks [ block_name ] += number_of_parameters for block_name , cnt in blocks . items ( ) : log . info ( "{} - {}." . format ( block_name , cnt ) ) total_num_parameters = np . sum ( list ( blocks . values ( ) ) ) log . info ( 'Total number of parameters equal {}' . format ( total_num_parameters ) )
def search ( self , word , d , allow_spaces = True , return_cost = True ) : if not all ( ( c in self . alphabet or ( c == " " and self . allow_spaces ) ) for c in word ) : return [ ] return self . _trie_search ( word , d , allow_spaces = allow_spaces , return_cost = return_cost )
def _make_default_operation_costs ( self , allow_spaces = False ) : self . operation_costs = dict ( ) self . operation_costs [ "" ] = { c : 1.0 for c in list ( self . alphabet ) + [ ' ' ] } for a in self . alphabet : current_costs = { c : 1.0 for c in self . alphabet } current_costs [ a ] = 0.0 current_costs [ "" ] = 1.0 if allow_spaces : current_costs [ " " ] = 1.0 self . operation_costs [ a ] = current_costs for a , b in itertools . permutations ( self . alphabet , 2 ) : self . operation_costs [ a + b ] = { b + a : 1.0 } if allow_spaces : self . operation_costs [ " " ] = { c : 1.0 for c in self . alphabet } self . operation_costs [ " " ] [ "" ] = 1.0
def _start_timer ( self ) -> None : self . timer = Timer ( self . config [ 'conversation_lifetime' ] , self . self_destruct_callback ) self . timer . start ( )
def build_model ( config : Union [ str , Path , dict ] , mode : str = 'infer' , load_trained : bool = False , download : bool = False , serialized : Optional [ bytes ] = None ) -> Chainer : config = parse_config ( config ) if serialized : serialized : list = pickle . loads ( serialized ) if download : deep_download ( config ) import_packages ( config . get ( 'metadata' , { } ) . get ( 'imports' , [ ] ) ) model_config = config [ 'chainer' ] model = Chainer ( model_config [ 'in' ] , model_config [ 'out' ] , model_config . get ( 'in_y' ) ) for component_config in model_config [ 'pipe' ] : if load_trained and ( 'fit_on' in component_config or 'in_y' in component_config ) : try : component_config [ 'load_path' ] = component_config [ 'save_path' ] except KeyError : log . warning ( 'No "save_path" parameter for the {} component, so "load_path" will not be renewed' . format ( component_config . get ( 'class_name' , component_config . get ( 'ref' , 'UNKNOWN' ) ) ) ) if serialized and 'in' in component_config : component_serialized = serialized . pop ( 0 ) else : component_serialized = None component = from_params ( component_config , mode = mode , serialized = component_serialized ) if 'in' in component_config : c_in = component_config [ 'in' ] c_out = component_config [ 'out' ] in_y = component_config . get ( 'in_y' , None ) main = component_config . get ( 'main' , False ) model . append ( component , c_in , c_out , in_y , main ) return model
def interact_model ( config : Union [ str , Path , dict ] ) -> None : model = build_model ( config ) while True : args = [ ] for in_x in model . in_x : args . append ( ( input ( '{}::' . format ( in_x ) ) , ) ) if args [ - 1 ] [ 0 ] in { 'exit' , 'stop' , 'quit' , 'q' } : return pred = model ( * args ) if len ( model . out_params ) > 1 : pred = zip ( * pred ) print ( '>>' , * pred )
def predict_on_stream ( config : Union [ str , Path , dict ] , batch_size : int = 1 , file_path : Optional [ str ] = None ) -> None : if file_path is None or file_path == '-' : if sys . stdin . isatty ( ) : raise RuntimeError ( 'To process data from terminal please use interact mode' ) f = sys . stdin else : f = open ( file_path , encoding = 'utf8' ) model : Chainer = build_model ( config ) args_count = len ( model . in_x ) while True : batch = list ( ( l . strip ( ) for l in islice ( f , batch_size * args_count ) ) ) if not batch : break args = [ ] for i in range ( args_count ) : args . append ( batch [ i : : args_count ] ) res = model ( * args ) if len ( model . out_params ) == 1 : res = [ res ] for res in zip ( * res ) : res = json . dumps ( res , ensure_ascii = False ) print ( res , flush = True ) if f is not sys . stdin : f . close ( )
def fn_from_str ( name : str ) -> Callable [ ... , Any ] : try : module_name , fn_name = name . split ( ':' ) except ValueError : raise ConfigError ( 'Expected function description in a `module.submodules:function_name` form, but got `{}`' . format ( name ) ) return getattr ( importlib . import_module ( module_name ) , fn_name )
def register_metric ( metric_name : str ) -> Callable [ ... , Any ] : def decorate ( fn ) : fn_name = fn . __module__ + ':' + fn . __name__ if metric_name in _REGISTRY and _REGISTRY [ metric_name ] != fn_name : log . warning ( '"{}" is already registered as a metric name, the old function will be ignored' . format ( metric_name ) ) _REGISTRY [ metric_name ] = fn_name return fn return decorate
def get_metric_by_name ( name : str ) -> Callable [ ... , Any ] : if name not in _REGISTRY : raise ConfigError ( f'"{name}" is not registered as a metric' ) return fn_from_str ( _REGISTRY [ name ] )
def read_requirements ( ) : reqs_path = os . path . join ( __location__ , 'requirements.txt' ) with open ( reqs_path , encoding = 'utf8' ) as f : reqs = [ line . strip ( ) for line in f if not line . strip ( ) . startswith ( '#' ) ] names = [ ] links = [ ] for req in reqs : if '://' in req : links . append ( req ) else : names . append ( req ) return { 'install_requires' : names , 'dependency_links' : links }
def export2hub ( weight_file , hub_dir , options ) : spec = make_module_spec ( options , str ( weight_file ) ) try : with tf . Graph ( ) . as_default ( ) : module = hub . Module ( spec ) with tf . Session ( ) as sess : sess . run ( tf . global_variables_initializer ( ) ) if hub_dir . exists ( ) : shutil . rmtree ( hub_dir ) module . export ( str ( hub_dir ) , sess ) finally : pass
def main ( ) : args = parser . parse_args ( ) run_ms_bot_framework_server ( agent_generator = make_agent , app_id = args . ms_id , app_secret = args . ms_secret , stateful = True )
def check_gpu_existence ( ) : global _gpu_available if _gpu_available is None : sess_config = tf . ConfigProto ( ) sess_config . gpu_options . allow_growth = True try : with tf . Session ( config = sess_config ) : device_list = device_lib . list_local_devices ( ) _gpu_available = any ( device . device_type == 'GPU' for device in device_list ) except AttributeError as e : log . warning ( f'Got an AttributeError `{e}`, assuming documentation building' ) _gpu_available = False return _gpu_available
def _parse_config_property ( item : _T , variables : Dict [ str , Union [ str , Path , float , bool , None ] ] ) -> _T : if isinstance ( item , str ) : return item . format ( * * variables ) elif isinstance ( item , list ) : return [ _parse_config_property ( item , variables ) for item in item ] elif isinstance ( item , dict ) : return { k : _parse_config_property ( v , variables ) for k , v in item . items ( ) } else : return item
def parse_config ( config : Union [ str , Path , dict ] ) -> dict : if isinstance ( config , ( str , Path ) ) : config = read_json ( find_config ( config ) ) variables = { 'DEEPPAVLOV_PATH' : os . getenv ( f'DP_DEEPPAVLOV_PATH' , Path ( __file__ ) . parent . parent . parent ) } for name , value in config . get ( 'metadata' , { } ) . get ( 'variables' , { } ) . items ( ) : env_name = f'DP_{name}' if env_name in os . environ : value = os . getenv ( env_name ) variables [ name ] = value . format ( * * variables ) return _parse_config_property ( config , variables )
def expand_path ( path : Union [ str , Path ] ) -> Path : return Path ( path ) . expanduser ( ) . resolve ( )
def from_params ( params : Dict , mode : str = 'infer' , serialized : Any = None , * * kwargs ) -> Component : config_params = { k : _resolve ( v ) for k , v in params . items ( ) } if 'ref' in config_params : try : component = _refs [ config_params [ 'ref' ] ] if serialized is not None : component . deserialize ( serialized ) return component except KeyError : e = ConfigError ( 'Component with id "{id}" was referenced but not initialized' . format ( id = config_params [ 'ref' ] ) ) log . exception ( e ) raise e elif 'config_path' in config_params : from deeppavlov . core . commands . infer import build_model refs = _refs . copy ( ) _refs . clear ( ) config = parse_config ( expand_path ( config_params [ 'config_path' ] ) ) model = build_model ( config , serialized = serialized ) _refs . clear ( ) _refs . update ( refs ) try : _refs [ config_params [ 'id' ] ] = model except KeyError : pass return model cls_name = config_params . pop ( 'class_name' , None ) if not cls_name : e = ConfigError ( 'Component config has no `class_name` nor `ref` fields' ) log . exception ( e ) raise e cls = get_model ( cls_name ) config_params = { k : _init_param ( v , mode ) for k , v in config_params . items ( ) } try : spec = inspect . getfullargspec ( cls ) if 'mode' in spec . args + spec . kwonlyargs or spec . varkw is not None : kwargs [ 'mode' ] = mode component = cls ( * * dict ( config_params , * * kwargs ) ) try : _refs [ config_params [ 'id' ] ] = component except KeyError : pass except Exception : log . exception ( "Exception in {}" . format ( cls ) ) raise if serialized is not None : component . deserialize ( serialized ) return component
def run ( self ) -> None : while True : request = self . input_queue . get ( ) response = self . _handle_request ( request ) self . output_queue . put ( response )
def _refresh_valid_certs ( self ) -> None : self . timer = Timer ( REFRESH_VALID_CERTS_PERIOD_SECS , self . _refresh_valid_certs ) self . timer . start ( ) expired_certificates = [ ] for valid_cert_url , valid_cert in self . valid_certificates . items ( ) : valid_cert : ValidatedCert = valid_cert cert_expiration_time : datetime = valid_cert . expiration_timestamp if datetime . utcnow ( ) > cert_expiration_time : expired_certificates . append ( valid_cert_url ) for expired_cert_url in expired_certificates : del self . valid_certificates [ expired_cert_url ] log . info ( f'Validation period of {expired_cert_url} certificate expired' )
def cls_from_str ( name : str ) -> type : try : module_name , cls_name = name . split ( ':' ) except ValueError : raise ConfigError ( 'Expected class description in a `module.submodules:ClassName` form, but got `{}`' . format ( name ) ) return getattr ( importlib . import_module ( module_name ) , cls_name )
def get_model ( name : str ) -> type : if name not in _REGISTRY : if ':' not in name : raise ConfigError ( "Model {} is not registered." . format ( name ) ) return cls_from_str ( name ) return cls_from_str ( _REGISTRY [ name ] )
def list_jobs ( self ) : res = h2o . api ( "GET /3/Jobs" ) table = [ [ "type" ] , [ "dest" ] , [ "description" ] , [ "status" ] ] for job in res [ "jobs" ] : job_dest = job [ "dest" ] table [ 0 ] . append ( self . _translate_job_type ( job_dest [ "type" ] ) ) table [ 1 ] . append ( job_dest [ "name" ] ) table [ 2 ] . append ( job [ "description" ] ) table [ 3 ] . append ( job [ "status" ] ) return table
def list_timezones ( self ) : from h2o . expr import ExprNode return h2o . H2OFrame . _expr ( expr = ExprNode ( "listTimeZones" ) ) . _frame ( )
def summary ( self , key , column = "C1" , timeoutSecs = 10 , * * kwargs ) : params_dict = { } h2o_methods . check_params_update_kwargs ( params_dict , kwargs , 'summary' , True ) result = self . do_json_request ( '3/Frames.json/%s/columns/%s/summary' % ( key , column ) , timeout = timeoutSecs , params = params_dict ) h2o_sandbox . check_sandbox_for_errors ( ) return result
def delete_frame ( self , key , ignoreMissingKey = True , timeoutSecs = 60 , * * kwargs ) : assert key is not None , '"key" parameter is null' result = self . do_json_request ( '/3/Frames.json/' + key , cmd = 'delete' , timeout = timeoutSecs ) if not ignoreMissingKey and 'f00b4r' in result : raise ValueError ( 'Frame key not found: ' + key ) return result
def compute_model_metrics ( self , model , frame , timeoutSecs = 60 , * * kwargs ) : assert model is not None , '"model" parameter is null' assert frame is not None , '"frame" parameter is null' models = self . models ( key = model , timeoutSecs = timeoutSecs ) assert models is not None , "/Models REST call failed" assert models [ 'models' ] [ 0 ] [ 'model_id' ] [ 'name' ] == model , "/Models/{0} returned Model {1} rather than Model {2}" . format ( model , models [ 'models' ] [ 0 ] [ 'key' ] [ 'name' ] , model ) frames = self . frames ( key = frame ) assert frames is not None , "/Frames/{0} REST call failed" . format ( frame ) print "frames:" , dump_json ( frames ) result = self . do_json_request ( '/3/ModelMetrics.json/models/' + model + '/frames/' + frame , cmd = 'post' , timeout = timeoutSecs ) mm = result [ 'model_metrics' ] [ 0 ] verboseprint ( "model metrics: " + repr ( mm ) ) h2o_sandbox . check_sandbox_for_errors ( ) return mm
def delete_model ( self , key , ignoreMissingKey = True , timeoutSecs = 60 , * * kwargs ) : assert key is not None , '"key" parameter is null' result = self . do_json_request ( '/3/Models.json/' + key , cmd = 'delete' , timeout = timeoutSecs ) if not ignoreMissingKey and 'f00b4r' in result : raise ValueError ( 'Model key not found: ' + key ) verboseprint ( "delete_model result:" , dump_json ( result ) ) return result
def _tabulate ( self , tablefmt = "simple" , rollups = False , rows = 10 ) : if not self . is_valid ( ) : self . fill ( rows = rows ) d = collections . OrderedDict ( ) if rollups : col = next ( iter ( viewvalues ( self . _data ) ) ) lrows = len ( col [ 'data' ] ) d [ "" ] = [ "type" , "mins" , "mean" , "maxs" , "sigma" , "zeros" , "missing" ] + list ( map ( str , range ( lrows ) ) ) for k , v in viewitems ( self . _data ) : x = v [ 'data' ] t = v [ "type" ] if t == "enum" : domain = v [ 'domain' ] x = [ "" if math . isnan ( idx ) else domain [ int ( idx ) ] for idx in x ] elif t == "time" : x = [ "" if math . isnan ( z ) else time . strftime ( "%Y-%m-%d %H:%M:%S" , time . gmtime ( z / 1000 ) ) for z in x ] if rollups : mins = v [ 'mins' ] [ 0 ] if v [ 'mins' ] and v [ "type" ] != "enum" else None maxs = v [ 'maxs' ] [ 0 ] if v [ 'maxs' ] and v [ "type" ] != "enum" else None #Cross check type with mean and sigma. Set to None if of type enum. if v [ 'type' ] == "enum" : v [ 'mean' ] = v [ 'sigma' ] = v [ 'zero_count' ] = None x = [ v [ 'type' ] , mins , v [ 'mean' ] , maxs , v [ 'sigma' ] , v [ 'zero_count' ] , v [ 'missing_count' ] ] + x d [ k ] = x return tabulate . tabulate ( d , headers = "keys" , tablefmt = tablefmt )
def run_instances ( count , ec2_config , region , waitForSSH = True , tags = None ) : ec2params = inheritparams ( ec2_config , EC2_API_RUN_INSTANCE ) ec2params . setdefault ( 'min_count' , count ) ec2params . setdefault ( 'max_count' , count ) reservation = None conn = ec2_connect ( region ) try : reservation = conn . run_instances ( * * ec2params ) log ( 'Reservation: {0}' . format ( reservation . id ) ) log ( 'Waiting for {0} EC2 instances {1} to come up, this can take 1-2 minutes.' . format ( len ( reservation . instances ) , reservation . instances ) ) start = time . time ( ) time . sleep ( 1 ) for instance in reservation . instances : while instance . update ( ) == 'pending' : time . sleep ( 1 ) h2o_cmd . dot ( ) if not instance . state == 'running' : raise Exception ( '\033[91m[ec2] Error waiting for running state. Instance is in state {0}.\033[0m' . format ( instance . state ) ) log ( 'Instances started in {0} seconds' . format ( time . time ( ) - start ) ) log ( 'Instances: ' ) for inst in reservation . instances : log ( "   {0} ({1}) : public ip: {2}, private ip: {3}" . format ( inst . public_dns_name , inst . id , inst . ip_address , inst . private_ip_address ) ) if waitForSSH : wait_for_ssh ( [ i . private_ip_address for i in reservation . instances ] ) try : if tags : conn . create_tags ( [ i . id for i in reservation . instances ] , tags ) except : warn ( 'Something wrong during tagging instances. Exceptions IGNORED!' ) print sys . exc_info ( ) pass return reservation except : print "\033[91mUnexpected error\033[0m :" , sys . exc_info ( ) if reservation : terminate_reservation ( reservation , region ) raise
def terminate_instances ( instances , region ) : if not instances : return conn = ec2_connect ( region ) log ( "Terminating instances {0}." . format ( instances ) ) conn . terminate_instances ( instances ) log ( "Done" )
def stop_instances ( instances , region ) : if not instances : return conn = ec2_connect ( region ) log ( "Stopping instances {0}." . format ( instances ) ) conn . stop_instances ( instances ) log ( "Done" )
def start_instances ( instances , region ) : if not instances : return conn = ec2_connect ( region ) log ( "Starting instances {0}." . format ( instances ) ) conn . start_instances ( instances ) log ( "Done" )
def reboot_instances ( instances , region ) : if not instances : return conn = ec2_connect ( region ) log ( "Rebooting instances {0}." . format ( instances ) ) conn . reboot_instances ( instances ) log ( "Done" )
def wait_for_ssh ( ips , port = 22 , skipAlive = True , requiredsuccess = 3 ) : log ( 'Waiting for SSH on following hosts: {0}' . format ( ips ) ) for ip in ips : if not skipAlive or not ssh_live ( ip , port ) : log ( 'Waiting for SSH on instance {0}...' . format ( ip ) ) count = 0 while count < requiredsuccess : if ssh_live ( ip , port ) : count += 1 else : count = 0 time . sleep ( 1 ) h2o_cmd . dot ( )
def join ( self ) : self . _future = False self . _job . poll ( ) model_key = self . _job . dest_key self . _job = None model_json = h2o . api ( "GET /%d/Models/%s" % ( self . _rest_version , model_key ) ) [ "models" ] [ 0 ] self . _resolve_model ( model_key , model_json )
def signal_handler ( signum , stackframe ) : global g_runner global g_handling_signal if g_handling_signal : return g_handling_signal = True print ( "" ) print ( "----------------------------------------------------------------------" ) print ( "" ) print ( "SIGNAL CAUGHT (" + str ( signum ) + ").  TEARING DOWN CLOUDS." ) print ( "" ) print ( "----------------------------------------------------------------------" ) g_runner . terminate ( )
def wipe_output_dir ( ) : print ( "Wiping output directory." ) try : if os . path . exists ( g_output_dir ) : shutil . rmtree ( str ( g_output_dir ) ) except OSError as e : print ( "ERROR: Removing output directory %s failed: " % g_output_dir ) print ( "       (errno {0}): {1}" . format ( e . errno , e . strerror ) ) print ( "" ) sys . exit ( 1 )
def get_ip ( self ) : if len ( self . client_nodes ) > 0 : node = self . client_nodes [ 0 ] else : node = self . nodes [ 0 ] return node . get_ip ( )
def get_port ( self ) : if len ( self . client_nodes ) > 0 : node = self . client_nodes [ 0 ] else : node = self . nodes [ 0 ] return node . get_port ( )
def _determine_vec_size ( self ) : first_column = self . pre_trained . types [ self . pre_trained . columns [ 0 ] ] if first_column != 'string' : raise H2OValueError ( "First column of given pre_trained model %s is required to be a String" , self . pre_trained . frame_id ) if list ( self . pre_trained . types . values ( ) ) . count ( 'string' ) > 1 : raise H2OValueError ( "There are multiple columns in given pre_trained model %s with a String type." , self . pre_trained . frame_id ) self . vec_size = self . pre_trained . dim [ 1 ] - 1
def _get_lambda_source_code ( lambda_fn , src ) : def gen_lambdas ( ) : def gen ( ) : yield src + "\n" g = gen ( ) step = 0 tokens = [ ] for tok in tokenize . generate_tokens ( getattr ( g , "next" , getattr ( g , "__next__" , None ) ) ) : if step == 0 : if tok [ 0 ] == tokenize . NAME and tok [ 1 ] == "lambda" : step = 1 tokens = [ tok ] level = 0 elif step == 1 : if tok [ 0 ] == tokenize . NAME : tokens . append ( tok ) step = 2 else : step = 0 elif step == 2 : if tok [ 0 ] == tokenize . OP and tok [ 1 ] == ":" : tokens . append ( tok ) step = 3 else : step = 0 elif step == 3 : if level == 0 and ( tok [ 0 ] == tokenize . OP and tok [ 1 ] in ",)" or tok [ 0 ] == tokenize . ENDMARKER ) : yield tokenize . untokenize ( tokens ) . strip ( ) step = 0 else : tokens . append ( tok ) if tok [ 0 ] == tokenize . OP : if tok [ 1 ] in "[({" : level += 1 if tok [ 1 ] in "])}" : level -= 1 assert not tokens actual_code = lambda_fn . __code__ . co_code for lambda_src in gen_lambdas ( ) : try : fn = eval ( lambda_src , globals ( ) , locals ( ) ) if fn . __code__ . co_code == actual_code : return lambda_src . split ( ":" , 1 ) [ 1 ] . strip ( ) except Exception : pass return "<lambda>"
def name ( self , src = None ) : res = [ _get_type_name ( tt , src ) for tt in self . _types ] if len ( res ) == 2 and "None" in res : res . remove ( "None" ) return "?" + res [ 0 ] else : return " | " . join ( res )
def check ( self , var ) : return all ( _check_type ( var , tt ) for tt in self . _types )
def name ( self , src = None ) : return " & " . join ( _get_type_name ( tt , src ) for tt in self . _types )
def check ( self , var ) : return not any ( _check_type ( var , tt ) for tt in self . _types )
def name ( self , src = None ) : if len ( self . _types ) > 1 : return "!(%s)" % str ( "|" . join ( _get_type_name ( tt , src ) for tt in self . _types ) ) else : return "!" + _get_type_name ( self . _types [ 0 ] , src )
def check ( self , var ) : return isinstance ( var , tuple ) and all ( _check_type ( t , self . _element_type ) for t in var )
def check ( self , var ) : if not isinstance ( var , dict ) : return False if any ( key not in self . _types for key in var ) : return False for key , ktype in viewitems ( self . _types ) : val = var . get ( key , None ) if not _check_type ( val , ktype ) : return False return True
def name ( self , src = None ) : return "{%s}" % ", " . join ( "%s: %s" % ( key , _get_type_name ( ktype , src ) ) for key , ktype in viewitems ( self . _types ) )
def check ( self , var ) : return ( isinstance ( var , _int_type ) and ( self . _lower_bound is None or var >= self . _lower_bound ) and ( self . _upper_bound is None or var <= self . _upper_bound ) )
def name ( self , src = None ) : if self . _upper_bound is None and self . _lower_bound is None : return "int" if self . _upper_bound is None : if self . _lower_bound == 1 : return "int>0" return "int≥%d" % s lf._ l ower_bound if self . _lower_bound is None : return "int≤%d" % s lf._ u pper_bound return "int[%d…%d]" % ( e lf._ l ower_bound,  s lf._ u pper_bound) 
def check ( self , var ) : return ( isinstance ( var , _num_type ) and ( self . _lower_bound is None or var >= self . _lower_bound ) and ( self . _upper_bound is None or var <= self . _upper_bound ) )
def check ( self , var ) : if self . _class is None : self . _init ( ) return self . _class and self . _checker ( var , self . _class )
def check ( self , var ) : if not isinstance ( var , _str_type ) : return False return _enum_mangle ( var ) in self . _consts
def get_config ( ) : self = H2OConfigReader . _get_instance ( ) if not self . _config_loaded : self . _read_config ( ) return self . _config
def _read_config ( self ) : self . _config_loaded = True conf = [ ] for f in self . _candidate_log_files ( ) : if os . path . isfile ( f ) : self . _logger . info ( "Reading config file %s" % f ) section_rx = re . compile ( r"^\[(\w+)\]$" ) keyvalue_rx = re . compile ( r"^(\w+:)?([\w.]+)\s*=(.*)$" ) with io . open ( f , "rt" , encoding = "utf-8" ) as config_file : section_name = None for lineno , line in enumerate ( config_file ) : line = line . strip ( ) if line == "" or line . startswith ( "#" ) : continue m1 = section_rx . match ( line ) if m1 : section_name = m1 . group ( 1 ) continue m2 = keyvalue_rx . match ( line ) if m2 : lng = m2 . group ( 1 ) key = m2 . group ( 2 ) val = m2 . group ( 3 ) . strip ( ) if lng and lng . lower ( ) != "py:" : continue if section_name : key = section_name + "." + key if key in H2OConfigReader . _allowed_config_keys : conf . append ( ( key , val ) ) else : self . _logger . error ( "Key %s is not a valid config key" % key ) continue self . _logger . error ( "Syntax error in config file line %d: %s" % ( lineno , line ) ) self . _config = dict ( conf ) return
def _candidate_log_files ( ) : relpath = ".h2oconfig" prevpath = None while True : abspath = os . path . abspath ( relpath ) if abspath == prevpath : break prevpath = abspath relpath = "../" + relpath yield abspath yield os . path . expanduser ( "~/.h2oconfig" )
def _recalculate_model_parameters ( self , now ) : time_until_end = self . _estimate_progress_completion_time ( now ) - now assert time_until_end >= 0 , "Estimated progress completion cannot be in the past." x_real = self . _get_real_progress ( ) if x_real == 1 : t0 , x0 , v0 , ve = now , 1 , 0 , 0 else : x0 , v0 = self . _compute_progress_at_time ( now ) t0 = now if x0 >= 1 : t0 , x0 , v0 = self . _t0 , self . _x0 , self . _v0 time_until_end += now - t0 z = self . BETA * time_until_end max_speed = ( 1 - x_real ** 2 ) / self . FINISH_DELAY ve = v0 + ( self . BETA * ( 1 - x0 ) - v0 * z ) / ( z - 1 + math . exp ( - z ) ) if ve < 0 : v0 = self . BETA * ( 1 - x0 ) / ( 1 - math . exp ( - z ) ) ve = 0 if ve > max_speed : ve = max_speed self . _t0 , self . _x0 , self . _v0 , self . _ve = t0 , x0 , v0 , ve
def _draw ( self , txt , final = False ) : if not self . _file_mode : sys . stdout . write ( "\r" ) sys . stdout . write ( txt ) if final and not isinstance ( self . _widget , _HiddenWidget ) : sys . stdout . write ( "\n" ) else : if not self . _file_mode : sys . stdout . write ( "\r" ) sys . stdout . flush ( )
def render ( self , progress , width = None , status = None ) : results = [ widget . render ( progress , width = self . _widget_lengths [ i ] , status = status ) for i , widget in enumerate ( self . _widgets ) ] if self . _file_mode : res = "" for i , result in enumerate ( results ) : res += result . rendered if result . length < self . _widget_lengths [ i ] and progress < 1 : break res += " " if i < len ( results ) - 1 else "" rendered_str = res [ len ( self . _rendered ) : ] self . _rendered = res else : rendered_str = " " . join ( r . rendered for r in results ) if self . _to_render : rendered_str = self . _to_render + rendered_str self . _to_render = None next_progress = min ( r . next_progress for r in results ) next_time = min ( r . next_time for r in results ) return RenderResult ( rendered_str , next_progress = next_progress , next_time = next_time )
def _compute_widget_sizes ( self ) : wl = [ 0 ] * len ( self . _widgets ) flex_count = 0 for i , widget in enumerate ( self . _widgets ) : if isinstance ( widget , ProgressBarFlexibleWidget ) : flex_count += 1 else : wl [ i ] = widget . render ( 1 ) . length remaining_width = self . _width - sum ( wl ) remaining_width -= len ( self . _widgets ) - 1 if remaining_width < 10 * flex_count : if self . _file_mode : remaining_width = 10 * flex_count else : widget0 = self . _widgets [ 0 ] if isinstance ( widget0 , PBWString ) and remaining_width + widget0 . render ( 0 ) . length >= 10 * flex_count : remaining_width += widget0 . render ( 0 ) . length + 1 self . _to_render = widget0 . render ( 0 ) . rendered + "\n" self . _widgets = self . _widgets [ 1 : ] if remaining_width < 10 * flex_count : self . _file_mode = True remaining_width = 10 * flex_count remaining_width = max ( remaining_width , 10 * flex_count ) for i , widget in enumerate ( self . _widgets ) : if isinstance ( widget , ProgressBarFlexibleWidget ) : target_length = int ( remaining_width / flex_count ) result = widget . render ( 1 , target_length ) wl [ i ] = result . length remaining_width -= result . length flex_count -= 1 return wl
def _get_terminal_size ( ) : if not sys . stdout . isatty ( ) : return 80 try : import subprocess ret = subprocess . check_output ( [ "stty" , "size" ] ) . strip ( ) . split ( " " ) if len ( ret ) == 2 : return int ( ret [ 1 ] ) except : pass try : from termios import TIOCGWINSZ from fcntl import ioctl from struct import unpack res = unpack ( "hh" , ioctl ( sys . stdout , TIOCGWINSZ , b"1234" ) ) return int ( res [ 1 ] ) except : pass return int ( os . environ . get ( "COLUMNS" , 80 ) )
def render ( self , progress , width = None , status = None ) : if width <= 3 : return RenderResult ( ) bar_width = width - 2 n_chars = int ( progress * bar_width + 0.001 ) endf , endl = self . _bar_ends if self . _file_mode : out = endf out += self . _bar_symbols [ - 1 ] * n_chars out += endl if progress == 1 else "" if status : out += " (%s)" % status next_progress = ( n_chars + 1 ) / bar_width rendered_len = len ( out ) else : frac_chars = int ( ( progress * bar_width - n_chars ) * len ( self . _bar_symbols ) ) out = endf out += self . _bar_symbols [ - 1 ] * n_chars out += self . _bar_symbols [ frac_chars - 1 ] if frac_chars > 0 else "" rendered_len = len ( out ) if status : out += colorama . Fore . RED + " (" + status + ")" + colorama . Style . RESET_ALL rendered_len += 3 + len ( status ) out += " " * ( width - 1 - rendered_len ) out += endl next_progress = ( n_chars + ( frac_chars + 1 ) / len ( self . _bar_symbols ) ) / bar_width rendered_len += max ( 0 , width - 1 - rendered_len ) + 1 return RenderResult ( rendered = out , length = rendered_len , next_progress = next_progress )
def set_encoding ( self , encoding ) : self . _bar_ends = "[]" self . _bar_symbols = "#" if not encoding : return s1 = "\u258F\u258E\u258D\u258C\u258B\u258A\u2589\u2588" s2 = "\u258C\u2588" s3 = "\u2588" if self . _file_mode : s1 = s2 = None assert len ( s3 ) == 1 for s in ( s1 , s2 , s3 ) : if s is None : continue try : s . encode ( encoding ) self . _bar_ends = "||" self . _bar_symbols = s return except UnicodeEncodeError : pass except LookupError : print ( "Warning: unknown encoding %s" % encoding )
def render ( self , progress , width = None , status = None ) : current_pct = int ( progress * 100 + 0.1 ) return RenderResult ( rendered = "%3d%%" % current_pct , next_progress = ( current_pct + 1 ) / 100 )
def refresh ( self ) : self . _ex . _cache . flush ( ) self . _frame ( fill_cache = True )
def structure ( self ) : df = self . as_data_frame ( use_pandas = False ) cn = df . pop ( 0 ) nr = self . nrow nc = self . ncol width = max ( [ len ( c ) for c in cn ] ) isfactor = self . isfactor ( ) numlevels = self . nlevels ( ) lvls = self . levels ( ) print ( "H2OFrame: '{}' \nDimensions: {} obs. of {} variables" . format ( self . frame_id , nr , nc ) ) for i in range ( nc ) : print ( "$ {} {}: " . format ( cn [ i ] , ' ' * ( width - max ( 0 , len ( cn [ i ] ) ) ) ) , end = ' ' ) if isfactor [ i ] : nl = numlevels [ i ] print ( "Factor w/ {} level(s) {} " . format ( nl , '"' + '","' . join ( lvls [ i ] ) + '"' ) , end = '\n' ) else : print ( "num {}" . format ( " " . join ( it [ 0 ] if it else "nan" for it in h2o . as_list ( self [ : 10 , i ] , False ) [ 1 : ] ) ) )
def parse_text ( text ) : assert isinstance ( text , _str_type ) , "`text` parameter should be a string, got %r" % type ( text ) gen = iter ( text . splitlines ( True ) ) readline = gen . next if hasattr ( gen , "next" ) else gen . __next__ return Code ( _tokenize ( readline ) )
def parse_file ( filename ) : assert isinstance ( filename , _str_type ) , "`filename` parameter should be a string, got %r" % type ( filename ) with open ( filename , "rt" , encoding = "utf-8" ) as f : return Code ( _tokenize ( f . readline ) )
def move ( self , drow , dcol = 0 ) : self . _start_row += drow self . _start_col += dcol self . _end_row += drow self . _end_col += dcol
def unparse ( self ) : ut = Untokenizer ( start_row = self . _tokens [ 0 ] . start_row ) self . _unparse ( ut ) return ut . result ( )
def centers ( self ) : o = self . _model_json [ "output" ] cvals = o [ "centers" ] . cell_values centers = [ list ( cval [ 1 : ] ) for cval in cvals ] return centers
def centers_std ( self ) : o = self . _model_json [ "output" ] cvals = o [ "centers_std" ] . cell_values centers_std = [ list ( cval [ 1 : ] ) for cval in cvals ] centers_std = [ list ( x ) for x in zip ( * centers_std ) ] return centers_std
def version_check ( ) : from . __init__ import __version__ as ver_pkg ci = h2oconn . cluster if not ci : raise H2OConnectionError ( "Connection not initialized. Did you run h2o.connect()?" ) ver_h2o = ci . version if ver_pkg == "SUBST_PROJECT_VERSION" : ver_pkg = "UNKNOWN" if str ( ver_h2o ) != str ( ver_pkg ) : branch_name_h2o = ci . branch_name build_number_h2o = ci . build_number if build_number_h2o is None or build_number_h2o == "unknown" : raise H2OConnectionError ( "Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. " "Upgrade H2O and h2o-Python to latest stable version - " "http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html" "" . format ( ver_h2o , ver_pkg ) ) elif build_number_h2o == "99999" : raise H2OConnectionError ( "Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. " "This is a developer build, please contact your developer." "" . format ( ver_h2o , ver_pkg ) ) else : raise H2OConnectionError ( "Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. " "Install the matching h2o-Python version from - " "http://h2o-release.s3.amazonaws.com/h2o/{2}/{3}/index.html." "" . format ( ver_h2o , ver_pkg , branch_name_h2o , build_number_h2o ) ) if ci . build_too_old : print ( "Warning: Your H2O cluster version is too old ({})! Please download and install the latest " "version from http://h2o.ai/download/" . format ( ci . build_age ) )
def load_dataset ( relative_path ) : assert_is_type ( relative_path , str ) h2o_dir = os . path . split ( __file__ ) [ 0 ] for possible_file in [ os . path . join ( h2o_dir , relative_path ) , os . path . join ( h2o_dir , "h2o_data" , relative_path ) , os . path . join ( h2o_dir , "h2o_data" , relative_path + ".csv" ) ] : if os . path . exists ( possible_file ) : return upload_file ( possible_file ) raise H2OValueError ( "Data file %s cannot be found" % relative_path )
def check_frame_id ( frame_id ) : if frame_id is None : return if frame_id . strip ( ) == "" : raise H2OValueError ( "Frame id cannot be an empty string: %r" % frame_id ) for i , ch in enumerate ( frame_id ) : if ch == "$" and i == 0 : continue if ch not in _id_allowed_characters : raise H2OValueError ( "Character '%s' is illegal in frame id: %s" % ( ch , frame_id ) ) if re . match ( r"-?[0-9]" , frame_id ) : raise H2OValueError ( "Frame id cannot start with a number: %s" % frame_id )
def slice_is_normalized ( s ) : return ( s . start is not None and s . stop is not None and s . step is not None and s . start <= s . stop )
def deprecated ( message ) : from traceback import extract_stack assert message , "`message` argument in @deprecated is required." def deprecated_decorator ( fun ) : def decorator_invisible ( * args , * * kwargs ) : stack = extract_stack ( ) assert len ( stack ) >= 2 and stack [ - 1 ] [ 2 ] == "decorator_invisible" , "Got confusing stack... %r" % stack print ( "[WARNING] in %s line %d:" % ( stack [ - 2 ] [ 0 ] , stack [ - 2 ] [ 1 ] ) ) print ( "    >>> %s" % ( stack [ - 2 ] [ 3 ] or "????" ) ) print ( "        ^^^^ %s" % message ) return fun ( * args , * * kwargs ) decorator_invisible . __doc__ = message decorator_invisible . __name__ = fun . __name__ decorator_invisible . __module__ = fun . __module__ decorator_invisible . __deprecated__ = True return decorator_invisible return deprecated_decorator
def join ( self ) : self . _future = False self . _job . poll ( ) self . _job = None
def summary ( self , header = True ) : table = [ ] for model in self . models : model_summary = model . _model_json [ "output" ] [ "model_summary" ] r_values = list ( model_summary . cell_values [ 0 ] ) r_values [ 0 ] = model . model_id table . append ( r_values ) print ( ) if header : print ( 'Grid Summary:' ) print ( ) H2ODisplay ( table , [ 'Model Id' ] + model_summary . col_header [ 1 : ] , numalign = "left" , stralign = "left" )
def show ( self ) : hyper_combos = itertools . product ( * list ( self . hyper_params . values ( ) ) ) if not self . models : c_values = [ [ idx + 1 , list ( val ) ] for idx , val in enumerate ( hyper_combos ) ] print ( H2OTwoDimTable ( col_header = [ 'Model' , 'Hyperparameters: [' + ', ' . join ( list ( self . hyper_params . keys ( ) ) ) + ']' ] , table_header = 'Grid Search of Model ' + self . model . __class__ . __name__ , cell_values = c_values ) ) else : print ( self . sorted_metric_table ( ) )
def parse ( self ) : f = open ( self . parse_log_path , "r" ) self . parse2 ( f ) f . close ( )
def _log_start_transaction ( self , endpoint , data , json , files , params ) : self . _requests_counter += 1 if not self . _is_logging : return msg = "\n---- %d --------------------------------------------------------\n" % self . _requests_counter msg += "[%s] %s\n" % ( time . strftime ( "%H:%M:%S" ) , endpoint ) if params is not None : msg += "     params: {%s}\n" % ", " . join ( "%s:%s" % item for item in viewitems ( params ) ) if data is not None : msg += "     body: {%s}\n" % ", " . join ( "%s:%s" % item for item in viewitems ( data ) ) if json is not None : import json as j msg += "     json: %s\n" % j . dumps ( json ) if files is not None : msg += "     file: %s\n" % ", " . join ( f . name for f in viewvalues ( files ) ) self . _log_message ( msg + "\n" )
def _log_end_transaction ( self , start_time , response ) : if not self . _is_logging : return elapsed_time = int ( ( time . time ( ) - start_time ) * 1000 ) msg = "<<< HTTP %d %s   (%d ms)\n" % ( response . status_code , response . reason , elapsed_time ) if "Content-Type" in response . headers : msg += "    Content-Type: %s\n" % response . headers [ "Content-Type" ] msg += response . text self . _log_message ( msg + "\n\n" )
def _print ( self , msg , flush = False , end = "\n" ) : if self . _verbose : print2 ( msg , end = end , flush = flush )
def normalize_enum_constant ( s ) : if s . islower ( ) : return s if s . isupper ( ) : return s . lower ( ) return "" . join ( ch if ch . islower ( ) else "_" + ch . lower ( ) for ch in s ) . strip ( "_" )
def default_params ( self ) : params = { } for p in self . parms : params [ p ] = self . parms [ p ] [ "default_value" ] return params
def actual_params ( self ) : params_to_select = { "model_id" : "name" , "response_column" : "column_name" , "training_frame" : "name" , "validation_frame" : "name" } params = { } for p in self . parms : if p in params_to_select . keys ( ) : params [ p ] = self . parms [ p ] [ "actual_value" ] . get ( params_to_select [ p ] , None ) else : params [ p ] = self . parms [ p ] [ "actual_value" ] return params
def show ( self ) : if self . _future : self . _job . poll_once ( ) return if self . _model_json is None : print ( "No model trained yet" ) return if self . model_id is None : print ( "This H2OEstimator has been removed." ) return model = self . _model_json [ "output" ] print ( "Model Details" ) print ( "=============" ) print ( self . __class__ . __name__ , ": " , self . _model_json [ "algo_full_name" ] ) print ( "Model Key: " , self . _id ) self . summary ( ) print ( ) tm = model [ "training_metrics" ] if tm : tm . show ( ) vm = model [ "validation_metrics" ] if vm : vm . show ( ) xm = model [ "cross_validation_metrics" ] if xm : xm . show ( ) xms = model [ "cross_validation_metrics_summary" ] if xms : xms . show ( ) if "scoring_history" in model and model [ "scoring_history" ] : model [ "scoring_history" ] . show ( ) if "variable_importances" in model and model [ "variable_importances" ] : model [ "variable_importances" ] . show ( )
def gbm ( interactive = True , echo = True , testing = False ) : def demo_body ( go ) : go ( ) h2o . init ( ) go ( ) prostate = h2o . load_dataset ( "prostate" ) go ( ) prostate . describe ( ) go ( ) train , test = prostate . split_frame ( ratios = [ 0.70 ] ) go ( ) train [ "CAPSULE" ] = train [ "CAPSULE" ] . asfactor ( ) test [ "CAPSULE" ] = test [ "CAPSULE" ] . asfactor ( ) go ( ) from h2o . estimators import H2OGradientBoostingEstimator prostate_gbm = H2OGradientBoostingEstimator ( distribution = "bernoulli" , ntrees = 10 , max_depth = 8 , min_rows = 10 , learn_rate = 0.2 ) prostate_gbm . train ( x = [ "AGE" , "RACE" , "PSA" , "VOL" , "GLEASON" ] , y = "CAPSULE" , training_frame = train ) go ( ) prostate_gbm . show ( ) go ( ) predictions = prostate_gbm . predict ( test ) predictions . show ( ) go ( ) from h2o . tree import H2OTree , H2ONode tree = H2OTree ( prostate_gbm , 0 , "0" ) len ( tree ) tree . left_children tree . right_children tree . root_node . show ( ) go ( ) performance = prostate_gbm . model_performance ( test ) performance . show ( ) _run_demo ( demo_body , interactive , echo , testing )
def deeplearning ( interactive = True , echo = True , testing = False ) : def demo_body ( go ) : go ( ) h2o . init ( ) go ( ) prostate = h2o . load_dataset ( "prostate" ) go ( ) prostate . describe ( ) go ( ) train , test = prostate . split_frame ( ratios = [ 0.70 ] ) go ( ) train [ "CAPSULE" ] = train [ "CAPSULE" ] . asfactor ( ) test [ "CAPSULE" ] = test [ "CAPSULE" ] . asfactor ( ) go ( ) from h2o . estimators import H2ODeepLearningEstimator prostate_dl = H2ODeepLearningEstimator ( activation = "Tanh" , hidden = [ 10 , 10 , 10 ] , epochs = 10000 ) prostate_dl . train ( x = list ( set ( prostate . col_names ) - { "ID" , "CAPSULE" } ) , y = "CAPSULE" , training_frame = train ) go ( ) prostate_dl . show ( ) go ( ) predictions = prostate_dl . predict ( test ) predictions . show ( ) go ( ) performance = prostate_dl . model_performance ( test ) performance . show ( ) _run_demo ( demo_body , interactive , echo , testing )
def glm ( interactive = True , echo = True , testing = False ) : def demo_body ( go ) : go ( ) h2o . init ( ) go ( ) prostate = h2o . load_dataset ( "prostate" ) go ( ) prostate . describe ( ) go ( ) train , test = prostate . split_frame ( ratios = [ 0.70 ] ) go ( ) train [ "CAPSULE" ] = train [ "CAPSULE" ] . asfactor ( ) test [ "CAPSULE" ] = test [ "CAPSULE" ] . asfactor ( ) go ( ) from h2o . estimators import H2OGeneralizedLinearEstimator prostate_glm = H2OGeneralizedLinearEstimator ( family = "binomial" , alpha = [ 0.5 ] ) prostate_glm . train ( x = [ "AGE" , "RACE" , "PSA" , "VOL" , "GLEASON" ] , y = "CAPSULE" , training_frame = train ) go ( ) prostate_glm . show ( ) go ( ) predictions = prostate_glm . predict ( test ) predictions . show ( ) go ( ) performance = prostate_glm . model_performance ( test ) performance . show ( ) _run_demo ( demo_body , interactive , echo , testing )
def as_data_frame ( self ) : if can_use_pandas ( ) : import pandas pandas . options . display . max_colwidth = 70 return pandas . DataFrame ( self . _cell_values , columns = self . _col_header ) return self
def show ( self , header = True ) : if header and self . _table_header : print ( self . _table_header + ":" , end = ' ' ) if self . _table_description : print ( self . _table_description ) print ( ) table = copy . deepcopy ( self . _cell_values ) nr = 0 if _is_list_of_lists ( table ) : nr = len ( table ) if nr > 20 : trunc_table = [ ] trunc_table += [ v for v in table [ : 5 ] ] trunc_table . append ( [ "---" ] * len ( table [ 0 ] ) ) trunc_table += [ v for v in table [ ( nr - 5 ) : ] ] table = trunc_table H2ODisplay ( table , self . _col_header , numalign = "left" , stralign = "left" ) if nr > 20 and can_use_pandas ( ) : print ( '\nSee the whole table with table.as_data_frame()' )
def _jar_paths ( ) : own_jar = os . getenv ( "H2O_JAR_PATH" , "" ) if own_jar != "" : if not os . path . isfile ( own_jar ) : raise H2OStartupError ( "Environment variable H2O_JAR_PATH is set to '%d' but file does not exists, unset environment variable or provide valid path to h2o.jar file." % own_jar ) yield own_jar cwd_chunks = os . path . abspath ( "." ) . split ( os . path . sep ) for i in range ( len ( cwd_chunks ) , 0 , - 1 ) : if cwd_chunks [ i - 1 ] == "h2o-3" : yield os . path . sep . join ( cwd_chunks [ : i ] + [ "build" , "h2o.jar" ] ) backend_dir = os . path . split ( os . path . realpath ( __file__ ) ) [ 0 ] yield os . path . join ( backend_dir , "bin" , "h2o.jar" ) prefix1 = prefix2 = sys . prefix if prefix1 . startswith ( os . path . sep + "Library" ) : prefix2 = os . path . join ( "" , "System" , prefix1 ) elif prefix1 . startswith ( os . path . sep + "System" ) : prefix2 = prefix1 [ len ( os . path . join ( "" , "System" ) ) : ] yield os . path . join ( prefix1 , "h2o_jar" , "h2o.jar" ) yield os . path . join ( os . path . abspath ( os . sep ) , "usr" , "local" , "h2o_jar" , "h2o.jar" ) yield os . path . join ( prefix1 , "local" , "h2o_jar" , "h2o.jar" ) yield os . path . join ( get_config_var ( "userbase" ) , "h2o_jar" , "h2o.jar" ) yield os . path . join ( prefix2 , "h2o_jar" , "h2o.jar" )
def csv_dict_writer ( f , fieldnames , * * kwargs ) : import csv if "delimiter" in kwargs : kwargs [ "delimiter" ] = str ( kwargs [ "delimiter" ] ) return csv . DictWriter ( f , fieldnames , * * kwargs )
def _path2uri ( self , dirpath ) : relpath = dirpath . replace ( self . root_path , self . package_name ) if relpath . startswith ( os . path . sep ) : relpath = relpath [ 1 : ] return relpath . replace ( os . path . sep , '.' )
def _parse_module ( self , uri ) : filename = self . _uri2path ( uri ) if filename is None : return ( [ ] , [ ] ) f = open ( filename , 'rt' ) functions , classes = self . _parse_lines ( f ) f . close ( ) return functions , classes
def _parse_lines ( self , linesource ) : functions = [ ] classes = [ ] for line in linesource : if line . startswith ( 'def ' ) and line . count ( '(' ) : name = self . _get_object_name ( line ) if not name . startswith ( '_' ) : functions . append ( name ) elif line . startswith ( 'class ' ) : name = self . _get_object_name ( line ) if not name . startswith ( '_' ) : classes . append ( name ) else : pass functions . sort ( ) classes . sort ( ) return functions , classes
def to_list ( self ) : return [ [ int ( self . table . cell_values [ 0 ] [ 1 ] ) , int ( self . table . cell_values [ 0 ] [ 2 ] ) ] , [ int ( self . table . cell_values [ 1 ] [ 1 ] ) , int ( self . table . cell_values [ 1 ] [ 2 ] ) ] ]
def locate_files ( root_dir ) : all_files = [ ] root_dir = os . path . abspath ( root_dir ) for dir_name , subdirs , files in os . walk ( root_dir ) : for f in files : if f . endswith ( ".py" ) : all_files . append ( os . path . join ( dir_name , f ) ) return all_files
def main ( ) : for filename in locate_files ( ROOT_DIR ) : print ( "Processing %s" % filename ) with open ( filename , "rt" ) as f : tokens = list ( tokenize . generate_tokens ( f . readline ) ) text1 = tokenize . untokenize ( tokens ) ntokens = normalize_tokens ( tokens ) text2 = tokenize . untokenize ( ntokens ) assert text1 == text2
def generate_schema ( class_name , schema ) : has_map = False for field in schema [ "fields" ] : if field [ "type" ] . startswith ( "Map" ) : has_map = True superclass = schema [ "superclass" ] if superclass == "Iced" : superclass = "Object" yield "/**" yield " * This file is auto-generated by h2o-3/h2o-bindings/bin/gen_csharp.py" yield " * Copyright 2016 H2O.ai;  Apache License Version 2.0 (see LICENSE for details)" yield " */" yield "namespace ai.h2o" yield "{" yield "  using System;" yield "  using System.Collections.Generic;" if has_map else None yield "" yield "  public class {name}: {super} {{" . format ( name = class_name , super = superclass ) for field in schema [ "fields" ] : if field [ "name" ] == "__meta" : continue csharp_type = translate_type ( field [ "type" ] , field [ "schema_name" ] ) yield "    /// <summary>" yield bi . wrap ( field [ "help" ] , "    ///   " ) yield "    /// </summary>" yield "    public {type} {name} {{ get; set; }}" . format ( type = csharp_type , name = field [ "name" ] ) yield "" yield "  }" yield "}"
def available ( ) : builder_json = h2o . api ( "GET /3/ModelBuilders" , data = { "algo" : "deepwater" } ) visibility = builder_json [ "model_builders" ] [ "deepwater" ] [ "visibility" ] if visibility == "Experimental" : print ( "Cannot build a Deep Water model - no backend found." ) return False else : return True
def endpoint_groups ( ) : groups = defaultdict ( list ) for e in endpoints ( ) : groups [ e [ "class_name" ] ] . append ( e ) return groups
def update_site_forward ( apps , schema_editor ) : Site = apps . get_model ( "sites" , "Site" ) Site . objects . update_or_create ( id = settings . SITE_ID , defaults = { "domain" : "{{cookiecutter.domain_name}}" , "name" : "{{cookiecutter.project_name}}" , } , )
def json_data ( self , data = None ) : if data is None : data = { } data . update ( self . default_data ) return json . dumps ( data )
def comment_user ( self , user_id , amount = None ) : if not self . check_user ( user_id , filter_closed_acc = True ) : return False self . logger . info ( "Going to comment user_%s's feed:" % user_id ) user_id = self . convert_to_user_id ( user_id ) medias = self . get_user_medias ( user_id , is_comment = True ) if not medias : self . logger . info ( "None medias received: account is closed or medias have been filtered." ) return False return self . comment_medias ( medias [ : amount ] )
def get_credentials ( username = None ) : while not check_secret ( ) : pass while True : try : with open ( SECRET_FILE , "r" ) as f : lines = [ line . strip ( ) . split ( ":" , 2 ) for line in f . readlines ( ) ] except ValueError : msg = 'Problem with opening `{}`, will remove the file.' raise Exception ( msg . format ( SECRET_FILE ) ) if username is not None : for login , password in lines : if login == username . strip ( ) : return login , password print ( "Which account do you want to use? (Type number)" ) for ind , ( login , password ) in enumerate ( lines ) : print ( "%d: %s" % ( ind + 1 , login ) ) print ( "%d: %s" % ( 0 , "add another account." ) ) print ( "%d: %s" % ( - 1 , "delete all accounts." ) ) try : ind = int ( sys . stdin . readline ( ) ) if ind == 0 : add_credentials ( ) continue elif ind == - 1 : delete_credentials ( ) check_secret ( ) continue elif 0 <= ind - 1 < len ( lines ) : return lines [ ind - 1 ] except Exception : print ( "Wrong input, enter the number of the account to use." )
def like_user ( self , user_id , amount = None , filtration = True ) : if filtration : if not self . check_user ( user_id ) : return False self . logger . info ( "Liking user_%s's feed:" % user_id ) user_id = self . convert_to_user_id ( user_id ) medias = self . get_user_medias ( user_id , filtration = filtration ) if not medias : self . logger . info ( "None medias received: account is closed or medias have been filtered." ) return False return self . like_medias ( medias [ : amount ] )
def like_hashtag ( self , hashtag , amount = None ) : self . logger . info ( "Going to like media with hashtag #%s." % hashtag ) medias = self . get_total_hashtag_medias ( hashtag , amount ) return self . like_medias ( medias )
def check_not_bot ( self , user_id ) : self . small_delay ( ) user_id = self . convert_to_user_id ( user_id ) if not user_id : return False if user_id in self . whitelist : return True if user_id in self . blacklist : return False user_info = self . get_user_info ( user_id ) if not user_info : return True skipped = self . skipped_file if "following_count" in user_info and user_info [ "following_count" ] > self . max_following_to_block : msg = 'following_count > bot.max_following_to_block, skipping!' self . console_print ( msg , 'red' ) skipped . append ( user_id ) return False if search_stop_words_in_user ( self , user_info ) : msg = '`bot.search_stop_words_in_user` found in user, skipping!' skipped . append ( user_id ) return False return True
def get_uri ( self , request ) : protocol = request . protocol_override if request . protocol_override else self . protocol protocol = protocol . lower ( ) port = HTTP_PORT if protocol == 'http' else HTTPS_PORT return protocol + '://' + request . host + ':' + str ( port ) + request . path
def get_connection ( self , request ) : protocol = request . protocol_override if request . protocol_override else self . protocol protocol = protocol . lower ( ) target_host = request . host connection = _RequestsConnection ( target_host , protocol , self . request_session , self . timeout ) proxy_host = self . proxy_host proxy_port = self . proxy_port if self . proxy_host : headers = None if self . proxy_user and self . proxy_password : auth = base64 . b64encode ( "{0}:{1}" . format ( self . proxy_user , self . proxy_password ) . encode ( ) ) headers = { 'Proxy-Authorization' : 'Basic {0}' . format ( auth . decode ( ) ) } connection . set_tunnel ( proxy_host , int ( proxy_port ) , headers ) return connection
def perform_request ( self , request ) : connection = self . get_connection ( request ) try : connection . putrequest ( request . method , request . path ) self . send_request_headers ( connection , request . headers ) self . send_request_body ( connection , request . body ) if DEBUG_REQUESTS and request . body : print ( 'request:' ) try : print ( request . body ) except : pass resp = connection . getresponse ( ) status = int ( resp . status ) message = resp . reason respheaders = resp . getheaders ( ) for i , value in enumerate ( respheaders ) : respheaders [ i ] = ( value [ 0 ] . lower ( ) , value [ 1 ] ) respbody = None if resp . length is None : respbody = resp . read ( ) elif resp . length > 0 : respbody = resp . read ( resp . length ) if DEBUG_RESPONSES and respbody : print ( 'response:' ) try : print ( respbody ) except : pass response = HTTPResponse ( status , resp . reason , respheaders , respbody ) if status == 307 : new_url = urlparse ( dict ( respheaders ) [ 'location' ] ) request . host = new_url . hostname request . path = new_url . path request . path , request . query = self . _update_request_uri_query ( request ) return self . perform_request ( request ) if status >= 300 : raise HTTPError ( status , message , respheaders , respbody ) return response finally : connection . close ( )
def get_authorization_server ( self ) : value = '' for key in [ 'authorization_uri' , 'authorization' ] : value = self . get_value ( key ) or '' if value : break return value
def _validate_request_uri ( self , uri ) : if not uri : raise ValueError ( 'request_uri cannot be empty' ) uri = parse . urlparse ( uri ) if not uri . netloc : raise ValueError ( 'request_uri must be an absolute URI' ) if uri . scheme . lower ( ) not in [ 'http' , 'https' ] : raise ValueError ( 'request_uri must be HTTP or HTTPS' ) return uri . netloc
def set_timeout ( self , timeout_in_seconds ) : timeout_in_ms = int ( timeout_in_seconds * 1000 ) _WinHttpRequest . _SetTimeouts ( self , 0 , timeout_in_ms , timeout_in_ms , timeout_in_ms )
def set_request_header ( self , name , value ) : _name = BSTR ( name ) _value = BSTR ( value ) _WinHttpRequest . _SetRequestHeader ( self , _name , _value )
def get_all_response_headers ( self ) : bstr_headers = c_void_p ( ) _WinHttpRequest . _GetAllResponseHeaders ( self , byref ( bstr_headers ) ) bstr_headers = ctypes . cast ( bstr_headers , c_wchar_p ) headers = bstr_headers . value _SysFreeString ( bstr_headers ) return headers
def send ( self , request = None ) : if request is None : var_empty = VARIANT . create_empty ( ) _WinHttpRequest . _Send ( self , var_empty ) else : _request = VARIANT . create_safearray_from_str ( request ) _WinHttpRequest . _Send ( self , _request )
def status ( self ) : status = c_long ( ) _WinHttpRequest . _Status ( self , byref ( status ) ) return int ( status . value )
def status_text ( self ) : bstr_status_text = c_void_p ( ) _WinHttpRequest . _StatusText ( self , byref ( bstr_status_text ) ) bstr_status_text = ctypes . cast ( bstr_status_text , c_wchar_p ) status_text = bstr_status_text . value _SysFreeString ( bstr_status_text ) return status_text
def response_body ( self ) : var_respbody = VARIANT ( ) _WinHttpRequest . _ResponseBody ( self , byref ( var_respbody ) ) if var_respbody . is_safearray_of_bytes ( ) : respbody = var_respbody . str_from_safearray ( ) return respbody else : return ''
def set_client_certificate ( self , certificate ) : _certificate = BSTR ( certificate ) _WinHttpRequest . _SetClientCertificate ( self , _certificate )
def set_tunnel ( self , host , port ) : url = host if port : url = url + u':' + port var_host = VARIANT . create_bstr_from_str ( url ) var_empty = VARIANT . create_empty ( ) _WinHttpRequest . _SetProxy ( self , HTTPREQUEST_PROXYSETTING_PROXY , var_host , var_empty )
def set_tunnel ( self , host , port = None , headers = None ) : self . _httprequest . set_tunnel ( unicode ( host ) , unicode ( str ( port ) ) )
def putrequest ( self , method , uri ) : protocol = unicode ( self . protocol + '://' ) url = protocol + self . host + unicode ( uri ) self . _httprequest . set_timeout ( self . timeout ) self . _httprequest . open ( unicode ( method ) , url ) if self . cert_file is not None : self . _httprequest . set_client_certificate ( unicode ( self . cert_file ) )
def putheader ( self , name , value ) : if sys . version_info < ( 3 , ) : name = str ( name ) . decode ( 'utf-8' ) value = str ( value ) . decode ( 'utf-8' ) self . _httprequest . set_request_header ( name , value )
def send ( self , request_body ) : if not request_body : self . _httprequest . send ( ) else : self . _httprequest . send ( request_body )
def getresponse ( self ) : status = self . _httprequest . status ( ) status_text = self . _httprequest . status_text ( ) resp_headers = self . _httprequest . get_all_response_headers ( ) fixed_headers = [ ] for resp_header in resp_headers . split ( '\n' ) : if ( resp_header . startswith ( '\t' ) or resp_header . startswith ( ' ' ) ) and fixed_headers : fixed_headers [ - 1 ] += resp_header else : fixed_headers . append ( resp_header ) headers = [ ] for resp_header in fixed_headers : if ':' in resp_header : pos = resp_header . find ( ':' ) headers . append ( ( resp_header [ : pos ] . lower ( ) , resp_header [ pos + 1 : ] . strip ( ) ) ) body = self . _httprequest . response_body ( ) length = len ( body ) return _Response ( status , status_text , length , headers , body )
def _get_readable_id ( id_name , id_prefix_to_skip ) : pos = id_name . find ( '//' ) if pos != - 1 : pos += 2 if id_prefix_to_skip : pos = id_name . find ( id_prefix_to_skip , pos ) if pos != - 1 : pos += len ( id_prefix_to_skip ) pos = id_name . find ( '/' , pos ) if pos != - 1 : return id_name [ pos + 1 : ] return id_name
def _get_serialization_name ( element_name ) : known = _KNOWN_SERIALIZATION_XFORMS . get ( element_name ) if known is not None : return known if element_name . startswith ( 'x_ms_' ) : return element_name . replace ( '_' , '-' ) if element_name . endswith ( '_id' ) : element_name = element_name . replace ( '_id' , 'ID' ) for name in [ 'content_' , 'last_modified' , 'if_' , 'cache_control' ] : if element_name . startswith ( name ) : element_name = element_name . replace ( '_' , '-_' ) return '' . join ( name . capitalize ( ) for name in element_name . split ( '_' ) )
def get_entry_properties_from_node ( entry , include_id , id_prefix_to_skip = None , use_title_as_id = False ) : properties = { } etag = entry . getAttributeNS ( METADATA_NS , 'etag' ) if etag : properties [ 'etag' ] = etag for updated in _MinidomXmlToObject . get_child_nodes ( entry , 'updated' ) : properties [ 'updated' ] = updated . firstChild . nodeValue for name in _MinidomXmlToObject . get_children_from_path ( entry , 'author' , 'name' ) : if name . firstChild is not None : properties [ 'author' ] = name . firstChild . nodeValue if include_id : if use_title_as_id : for title in _MinidomXmlToObject . get_child_nodes ( entry , 'title' ) : properties [ 'name' ] = title . firstChild . nodeValue else : for id in _MinidomXmlToObject . get_child_nodes ( entry , 'id' ) : properties [ 'name' ] = _get_readable_id ( id . firstChild . nodeValue , id_prefix_to_skip ) return properties
def _parse_response_body_from_xml_node ( node , return_type ) : return_obj = return_type ( ) _MinidomXmlToObject . _fill_data_to_return_object ( node , return_obj ) return return_obj
def _fill_instance_child ( xmldoc , element_name , return_type ) : xmlelements = _MinidomXmlToObject . get_child_nodes ( xmldoc , _get_serialization_name ( element_name ) ) if not xmlelements : return None return_obj = return_type ( ) _MinidomXmlToObject . _fill_data_to_return_object ( xmlelements [ 0 ] , return_obj ) return return_obj
def build_package_from_pr_number ( gh_token , sdk_id , pr_number , output_folder , * , with_comment = False ) : con = Github ( gh_token ) repo = con . get_repo ( sdk_id ) sdk_pr = repo . get_pull ( pr_number ) package_names = { f . filename . split ( '/' ) [ 0 ] for f in sdk_pr . get_files ( ) if f . filename . startswith ( "azure" ) } absolute_output_folder = Path ( output_folder ) . resolve ( ) with tempfile . TemporaryDirectory ( ) as temp_dir , manage_git_folder ( gh_token , Path ( temp_dir ) / Path ( "sdk" ) , sdk_id , pr_number = pr_number ) as sdk_folder : for package_name in package_names : _LOGGER . debug ( "Build {}" . format ( package_name ) ) execute_simple_command ( [ "python" , "./build_package.py" , "--dest" , str ( absolute_output_folder ) , package_name ] , cwd = sdk_folder ) _LOGGER . debug ( "Build finished: {}" . format ( package_name ) ) if with_comment : files = [ f . name for f in absolute_output_folder . iterdir ( ) ] comment_message = None dashboard = DashboardCommentableObject ( sdk_pr , "(message created by the CI based on PR content)" ) try : installation_message = build_installation_message ( sdk_pr ) download_message = build_download_message ( sdk_pr , files ) comment_message = installation_message + "\n\n" + download_message dashboard . create_comment ( comment_message ) except Exception : _LOGGER . critical ( "Unable to do PR comment:\n%s" , comment_message )
def extract_api_version_from_code ( function ) : try : srccode = inspect . getsource ( function ) try : ast_tree = ast . parse ( srccode ) except IndentationError : ast_tree = ast . parse ( 'with 0:\n' + srccode ) api_version_visitor = ApiVersionExtractor ( ) api_version_visitor . visit ( ast_tree ) return api_version_visitor . api_version except Exception : raise
def _build_receiver ( self ) : self . _handler . message_handler = self . _handler . receiver_type ( self . _handler . _session , self . _handler . _remote_address , self . _handler . _name , on_message_received = self . _handler . _message_received , name = 'receiver-link-{}' . format ( uuid . uuid4 ( ) ) , debug = self . _handler . _debug_trace , prefetch = self . _handler . _prefetch , max_message_size = self . _handler . _max_message_size , properties = self . _handler . _link_properties , error_policy = self . _handler . _error_policy , encoding = self . _handler . _encoding ) if self . mode != ReceiveSettleMode . PeekLock : self . _handler . message_handler . send_settle_mode = constants . SenderSettleMode . Settled self . _handler . message_handler . receive_settle_mode = constants . ReceiverSettleMode . ReceiveAndDelete self . _handler . message_handler . _settle_mode = constants . ReceiverSettleMode . ReceiveAndDelete self . _handler . message_handler . open ( )
def parse_response_for_async_op ( response ) : if response is None : return None result = AsynchronousOperationResult ( ) if response . headers : for name , value in response . headers : if name . lower ( ) == 'x-ms-request-id' : result . request_id = value return result
def _update_management_header ( self , request , x_ms_version ) : if request . method in [ 'PUT' , 'POST' , 'MERGE' , 'DELETE' ] : request . headers . append ( ( 'Content-Length' , str ( len ( request . body ) ) ) ) request . headers . append ( ( 'x-ms-version' , x_ms_version or self . x_ms_version ) ) if not request . method in [ 'GET' , 'HEAD' ] : for name , _ in request . headers : if 'content-type' == name . lower ( ) : break else : request . headers . append ( ( 'Content-Type' , self . content_type ) ) return request . headers
def get_regions ( self ) : response = self . _perform_get ( self . _get_path ( 'services/serviceBus/Regions/' , None ) , None ) return _MinidomXmlToObject . convert_response_to_feeds ( response , _ServiceBusManagementXmlSerializer . xml_to_region )
def list_namespaces ( self ) : response = self . _perform_get ( self . _get_path ( 'services/serviceBus/Namespaces/' , None ) , None ) return _MinidomXmlToObject . convert_response_to_feeds ( response , _ServiceBusManagementXmlSerializer . xml_to_namespace )
def create ( env_dir , system_site_packages = False , clear = False , symlinks = False , with_pip = False , prompt = None ) : builder = ExtendedEnvBuilder ( system_site_packages = system_site_packages , clear = clear , symlinks = symlinks , with_pip = with_pip , prompt = prompt ) builder . create ( env_dir ) return builder . context
def list_databases ( self , name ) : response = self . _perform_get ( self . _get_list_databases_path ( name ) , None ) return _MinidomXmlToObject . parse_service_resources_response ( response , Database )
def _validate_challenge ( self , challenge ) : bearer_string = 'Bearer ' if not challenge : raise ValueError ( 'Challenge cannot be empty' ) challenge = challenge . strip ( ) if not challenge . startswith ( bearer_string ) : raise ValueError ( 'Challenge is not Bearer' ) return challenge [ len ( bearer_string ) : ]
def list_queues ( self ) : request = HTTPRequest ( ) request . method = 'GET' request . host = self . _get_host ( ) request . path = '/$Resources/Queues' request . path , request . query = self . _httpclient . _update_request_uri_query ( request ) request . headers = self . _update_service_bus_header ( request ) response = self . _perform_request ( request ) return _ETreeXmlToObject . convert_response_to_feeds ( response , _convert_etree_element_to_queue )
def send_event ( self , hub_name , message , device_id = None , broker_properties = None ) : _validate_not_none ( 'hub_name' , hub_name ) request = HTTPRequest ( ) request . method = 'POST' request . host = self . _get_host ( ) if device_id : request . path = '/{0}/publishers/{1}/messages?api-version=2014-01' . format ( hub_name , device_id ) else : request . path = '/{0}/messages?api-version=2014-01' . format ( hub_name ) if broker_properties : request . headers . append ( ( 'BrokerProperties' , str ( broker_properties ) ) ) request . body = _get_request_body ( message ) request . path , request . query = self . _httpclient . _update_request_uri_query ( request ) request . headers = self . _update_service_bus_header ( request ) self . _perform_request ( request )
def _update_service_bus_header ( self , request ) : if request . method in [ 'PUT' , 'POST' , 'MERGE' , 'DELETE' ] : request . headers . append ( ( 'Content-Length' , str ( len ( request . body ) ) ) ) if not request . method in [ 'GET' , 'HEAD' ] : for name , _ in request . headers : if name . lower ( ) == 'content-type' : break else : request . headers . append ( ( 'Content-Type' , 'application/atom+xml;type=entry;charset=utf-8' ) ) self . authentication . sign_request ( request , self . _httpclient ) return request . headers
def _get_authorization ( self , request , httpclient ) : return 'WRAP access_token="' + self . _get_token ( request . host , request . path , httpclient ) + '"'
def _token_is_expired ( self , token ) : time_pos_begin = token . find ( 'ExpiresOn=' ) + len ( 'ExpiresOn=' ) time_pos_end = token . find ( '&' , time_pos_begin ) token_expire_time = int ( token [ time_pos_begin : time_pos_end ] ) time_now = time . mktime ( time . localtime ( ) ) return ( token_expire_time - time_now ) < 30
def add_headers ( self , request ) : if self . custom_properties : for name , value in self . custom_properties . items ( ) : request . headers . append ( ( name , self . _serialize_escaped_properties_value ( value ) ) ) request . headers . append ( ( 'Content-Type' , self . type ) ) if self . broker_properties : if hasattr ( self . broker_properties , 'items' ) : broker_properties = { name : self . _serialize_basic_properties_value ( value ) for name , value in self . broker_properties . items ( ) } broker_properties = json . dumps ( broker_properties ) else : broker_properties = self . broker_properties request . headers . append ( ( 'BrokerProperties' , str ( broker_properties ) ) ) return request . headers
def as_batch_body ( self ) : if sys . version_info >= ( 3 , ) and isinstance ( self . body , bytes ) : body = self . body . decode ( 'utf-8' ) else : body = self . body result = { 'Body' : body } if self . custom_properties : result [ 'UserProperties' ] = { name : self . _serialize_basic_properties_value ( value ) for name , value in self . custom_properties . items ( ) } if self . broker_properties : result [ 'BrokerProperties' ] = { name : self . _serialize_basic_properties_value ( value ) for name , value in self . broker_properties . items ( ) } return result
def _general_error_handler ( http_error ) : message = str ( http_error ) if http_error . respbody is not None : message += '\n' + http_error . respbody . decode ( 'utf-8-sig' ) raise AzureHttpError ( message , http_error . status )
def _handle_redirect ( self , r , * * kwargs ) : if r . is_redirect : self . _thread_local . auth_attempted = False
def use ( self , profile ) : if not isinstance ( profile , ( KnownProfiles , ProfileDefinition ) ) : raise ValueError ( "Can only set as default a ProfileDefinition or a KnownProfiles" ) type ( self ) . profile = profile
def build_config ( config : Dict [ str , Any ] ) -> Dict [ str , str ] : result = config . copy ( ) is_stable = result . pop ( "is_stable" , False ) if is_stable : result [ "classifier" ] = "Development Status :: 5 - Production/Stable" else : result [ "classifier" ] = "Development Status :: 4 - Beta" package_name = result [ "package_name" ] result [ "package_nspkg" ] = result . pop ( "package_nspkg" , package_name [ : package_name . rindex ( '-' ) ] + "-nspkg" ) result [ 'is_arm' ] = result . pop ( "is_arm" , True ) result [ 'need_msrestazure' ] = result . pop ( "need_msrestazure" , True ) package_parts = result [ "package_nspkg" ] [ : - len ( '-nspkg' ) ] . split ( '-' ) result [ 'nspkg_names' ] = [ "." . join ( package_parts [ : i + 1 ] ) for i in range ( len ( package_parts ) ) ] result [ 'init_names' ] = [ "/" . join ( package_parts [ : i + 1 ] ) + "/__init__.py" for i in range ( len ( package_parts ) ) ] return result
def get_entry_properties_from_element ( element , include_id , id_prefix_to_skip = None , use_title_as_id = False ) : properties = { } etag = element . attrib . get ( _make_etree_ns_attr_name ( _etree_entity_feed_namespaces [ 'm' ] , 'etag' ) , None ) if etag is not None : properties [ 'etag' ] = etag updated = element . findtext ( './atom:updated' , '' , _etree_entity_feed_namespaces ) if updated : properties [ 'updated' ] = updated author_name = element . findtext ( './atom:author/atom:name' , '' , _etree_entity_feed_namespaces ) if author_name : properties [ 'author' ] = author_name if include_id : if use_title_as_id : title = element . findtext ( './atom:title' , '' , _etree_entity_feed_namespaces ) if title : properties [ 'name' ] = title else : element_id = element . findtext ( './atom:id' , '' , _etree_entity_feed_namespaces ) if element_id : properties [ 'name' ] = _get_readable_id ( element_id , id_prefix_to_skip ) return properties
def _parse_response_body_from_xml_node ( node , return_type ) : return_obj = return_type ( ) _ETreeXmlToObject . _fill_data_to_return_object ( node , return_obj ) return return_obj
def _fill_instance_child ( xmldoc , element_name , return_type ) : element = xmldoc . find ( _get_serialization_name ( element_name ) ) if element is None : return None return_obj = return_type ( ) _ETreeXmlToObject . _fill_data_to_return_object ( element , return_obj ) return return_obj
def terminal_width ( value ) : if isinstance ( value , bytes ) : value = value . decode ( "utf8" , "ignore" ) return sum ( map ( get_width , map ( ord , value ) ) )
def get_cut_prefix ( value , max_len ) : should_convert = isinstance ( value , bytes ) if should_convert : value = value . decode ( "utf8" , "ignore" ) for i in range ( len ( value ) ) : if terminal_width ( value [ i : ] ) <= max_len : break return value [ i : ] . encode ( "utf8" , "ignore" ) if should_convert else value [ i : ]
def print_inplace ( msg ) : term_width = get_terminal_size ( ) . columns spacing = term_width - terminal_width ( msg ) if is_win32 : spacing -= 1 sys . stderr . write ( "\r{0}" . format ( msg ) ) sys . stderr . write ( " " * max ( 0 , spacing ) ) sys . stderr . flush ( )
def format_filesize ( size ) : for suffix in ( "bytes" , "KB" , "MB" , "GB" , "TB" ) : if size < 1024.0 : if suffix in ( "GB" , "TB" ) : return "{0:3.2f} {1}" . format ( size , suffix ) else : return "{0:3.1f} {1}" . format ( size , suffix ) size /= 1024.0
def format_time ( elapsed ) : hours = int ( elapsed / ( 60 * 60 ) ) minutes = int ( ( elapsed % ( 60 * 60 ) ) / 60 ) seconds = int ( elapsed % 60 ) rval = "" if hours : rval += "{0}h" . format ( hours ) if elapsed > 60 : rval += "{0}m" . format ( minutes ) rval += "{0}s" . format ( seconds ) return rval
def create_status_line ( * * params ) : max_size = get_terminal_size ( ) . columns - 1 for fmt in PROGRESS_FORMATS : status = fmt . format ( * * params ) if len ( status ) <= max_size : break return status
def close ( self ) : if not self . closed : log . debug ( "Closing worker thread" ) self . closed = True if self . _wait : self . _wait . set ( )
def close ( self ) : if not self . closed : log . debug ( "Closing writer thread" ) self . closed = True self . reader . buffer . close ( ) self . executor . shutdown ( wait = False ) if concurrent . futures . thread . _threads_queues : concurrent . futures . thread . _threads_queues . clear ( )
def put ( self , segment ) : if self . closed : return if segment is not None : future = self . executor . submit ( self . fetch , segment , retries = self . retries ) else : future = None self . queue ( self . futures , ( segment , future ) )
def queue ( self , queue_ , value ) : while not self . closed : try : queue_ . put ( value , block = True , timeout = 1 ) return except queue . Full : continue
def pkcs7_decode ( paddedData , keySize = 16 ) : val = ord ( paddedData [ - 1 : ] ) if val > keySize : raise StreamError ( "Input is not padded or padding is corrupt, got padding size of {0}" . format ( val ) ) return paddedData [ : - val ]
def prepend_www ( url ) : parsed = urlparse ( url ) if parsed . netloc . split ( "." ) [ 0 ] != "www" : return parsed . scheme + "://www." + parsed . netloc + parsed . path else : return url
def json ( cls , res , * args , * * kwargs ) : if res . encoding is None : res . encoding = cls . determine_json_encoding ( res . content [ : 4 ] ) return parse_json ( res . text , * args , * * kwargs )
def xml ( cls , res , * args , * * kwargs ) : return parse_xml ( res . text , * args , * * kwargs )
def _get_streams ( self ) : token = self . login ( self . get_option ( "username" ) , self . get_option ( "password" ) ) m = self . _url_re . match ( self . url ) scode = m and m . group ( "scode" ) or self . get_option ( "station_code" ) res = self . session . http . get ( self . _guide_url , params = dict ( token = token ) ) channels = OrderedDict ( ) for t in itertags ( res . text , "a" ) : if t . attributes . get ( 'cs' ) : channels [ t . attributes . get ( 'cs' ) . lower ( ) ] = t . attributes . get ( 'title' ) . replace ( "Watch " , "" ) . strip ( ) if not scode : log . error ( "Station code not provided, use --ustvnow-station-code." ) log . info ( "Available stations are: \n{0} " . format ( '\n' . join ( '    {0} ({1})' . format ( c , n ) for c , n in channels . items ( ) ) ) ) return if scode in channels : log . debug ( "Finding streams for: {0}" , channels . get ( scode ) ) r = self . session . http . get ( self . _stream_url , params = { "scode" : scode , "token" : token , "br_n" : "Firefox" , "br_v" : "52" , "br_d" : "desktop" } , headers = { "User-Agent" : useragents . FIREFOX } ) data = self . session . http . json ( r ) return HLSStream . parse_variant_playlist ( self . session , data [ "stream" ] ) else : log . error ( "Invalid station-code: {0}" , scode )
def login ( self ) : email = self . get_option ( "email" ) password = self . get_option ( "password" ) if email and password : res = self . session . http . get ( self . login_url ) csrf_match = self . csrf_re . search ( res . text ) token = csrf_match and csrf_match . group ( 1 ) self . logger . debug ( "Attempting login as {0} (token={1})" , email , token ) res = self . session . http . post ( self . login_url , data = dict ( login = email , password = password , csrfmiddlewaretoken = token ) , allow_redirects = False , raise_for_status = False , headers = { "Referer" : self . login_url } ) if res . status_code != 302 : self . logger . error ( "Failed to login to LiveEdu account: {0}" , email )
def output_stream_http ( plugin , initial_streams , external = False , port = 0 ) : global output if not external : if not args . player : console . exit ( "The default player (VLC) does not seem to be " "installed. You must specify the path to a player " "executable with --player." ) title = create_title ( plugin ) server = create_http_server ( ) player = output = PlayerOutput ( args . player , args = args . player_args , filename = server . url , quiet = not args . verbose_player , title = title ) try : log . info ( "Starting player: {0}" , args . player ) if player : player . open ( ) except OSError as err : console . exit ( "Failed to start player: {0} ({1})" , args . player , err ) else : server = create_http_server ( host = None , port = port ) player = None log . info ( "Starting server, access with one of:" ) for url in server . urls : log . info ( " " + url ) for req in iter_http_requests ( server , player ) : user_agent = req . headers . get ( "User-Agent" ) or "unknown player" log . info ( "Got HTTP request from {0}" . format ( user_agent ) ) stream_fd = prebuffer = None while not stream_fd and ( not player or player . running ) : try : streams = initial_streams or fetch_streams ( plugin ) initial_streams = None for stream_name in ( resolve_stream_name ( streams , s ) for s in args . stream ) : if stream_name in streams : stream = streams [ stream_name ] break else : log . info ( "Stream not available, will re-fetch " "streams in 10 sec" ) sleep ( 10 ) continue except PluginError as err : log . error ( u"Unable to fetch new streams: {0}" , err ) continue try : log . info ( "Opening stream: {0} ({1})" , stream_name , type ( stream ) . shortname ( ) ) stream_fd , prebuffer = open_stream ( stream ) except StreamError as err : log . error ( "{0}" , err ) if stream_fd and prebuffer : log . debug ( "Writing stream to player" ) read_stream ( stream_fd , server , prebuffer ) server . close ( True ) player . close ( ) server . close ( )
def output_stream_passthrough ( plugin , stream ) : global output title = create_title ( plugin ) filename = '"{0}"' . format ( stream_to_url ( stream ) ) output = PlayerOutput ( args . player , args = args . player_args , filename = filename , call = True , quiet = not args . verbose_player , title = title ) try : log . info ( "Starting player: {0}" , args . player ) output . open ( ) except OSError as err : console . exit ( "Failed to start player: {0} ({1})" , args . player , err ) return False return True
def output_stream ( plugin , stream ) : global output success_open = False for i in range ( args . retry_open ) : try : stream_fd , prebuffer = open_stream ( stream ) success_open = True break except StreamError as err : log . error ( "Try {0}/{1}: Could not open stream {2} ({3})" , i + 1 , args . retry_open , stream , err ) if not success_open : console . exit ( "Could not open stream {0}, tried {1} times, exiting" , stream , args . retry_open ) output = create_output ( plugin ) try : output . open ( ) except ( IOError , OSError ) as err : if isinstance ( output , PlayerOutput ) : console . exit ( "Failed to start player: {0} ({1})" , args . player , err ) else : console . exit ( "Failed to open output: {0} ({1})" , args . output , err ) with closing ( output ) : log . debug ( "Writing stream to output" ) read_stream ( stream_fd , output , prebuffer ) return True
def read_stream ( stream , output , prebuffer , chunk_size = 8192 ) : is_player = isinstance ( output , PlayerOutput ) is_http = isinstance ( output , HTTPServer ) is_fifo = is_player and output . namedpipe show_progress = isinstance ( output , FileOutput ) and output . fd is not stdout and sys . stdout . isatty ( ) show_record_progress = hasattr ( output , "record" ) and isinstance ( output . record , FileOutput ) and output . record . fd is not stdout and sys . stdout . isatty ( ) stream_iterator = chain ( [ prebuffer ] , iter ( partial ( stream . read , chunk_size ) , b"" ) ) if show_progress : stream_iterator = progress ( stream_iterator , prefix = os . path . basename ( args . output ) ) elif show_record_progress : stream_iterator = progress ( stream_iterator , prefix = os . path . basename ( args . record ) ) try : for data in stream_iterator : if is_win32 and is_fifo : output . player . poll ( ) if output . player . returncode is not None : log . info ( "Player closed" ) break try : output . write ( data ) except IOError as err : if is_player and err . errno in ACCEPTABLE_ERRNO : log . info ( "Player closed" ) elif is_http and err . errno in ACCEPTABLE_ERRNO : log . info ( "HTTP connection closed" ) else : console . exit ( "Error when writing to output: {0}, exiting" , err ) break except IOError as err : console . exit ( "Error when reading from stream: {0}, exiting" , err ) finally : stream . close ( ) log . info ( "Stream ended" )
def fetch_streams ( plugin ) : return plugin . streams ( stream_types = args . stream_types , sorting_excludes = args . stream_sorting_excludes )
def resolve_stream_name ( streams , stream_name ) : if stream_name in STREAM_SYNONYMS and stream_name in streams : for name , stream in streams . items ( ) : if stream is streams [ stream_name ] and name not in STREAM_SYNONYMS : return name return stream_name
def print_plugins ( ) : pluginlist = list ( streamlink . get_plugins ( ) . keys ( ) ) pluginlist_formatted = ", " . join ( sorted ( pluginlist ) ) if console . json : console . msg_json ( pluginlist ) else : console . msg ( "Loaded plugins: {0}" , pluginlist_formatted )
def load_plugins ( dirs ) : dirs = [ os . path . expanduser ( d ) for d in dirs ] for directory in dirs : if os . path . isdir ( directory ) : streamlink . load_plugins ( directory ) else : log . warning ( "Plugin path {0} does not exist or is not " "a directory!" , directory )
def setup_http_session ( ) : if args . http_proxy : streamlink . set_option ( "http-proxy" , args . http_proxy ) if args . https_proxy : streamlink . set_option ( "https-proxy" , args . https_proxy ) if args . http_cookie : streamlink . set_option ( "http-cookies" , dict ( args . http_cookie ) ) if args . http_header : streamlink . set_option ( "http-headers" , dict ( args . http_header ) ) if args . http_query_param : streamlink . set_option ( "http-query-params" , dict ( args . http_query_param ) ) if args . http_ignore_env : streamlink . set_option ( "http-trust-env" , False ) if args . http_no_ssl_verify : streamlink . set_option ( "http-ssl-verify" , False ) if args . http_disable_dh : streamlink . set_option ( "http-disable-dh" , True ) if args . http_ssl_cert : streamlink . set_option ( "http-ssl-cert" , args . http_ssl_cert ) if args . http_ssl_cert_crt_key : streamlink . set_option ( "http-ssl-cert" , tuple ( args . http_ssl_cert_crt_key ) ) if args . http_timeout : streamlink . set_option ( "http-timeout" , args . http_timeout ) if args . http_cookies : streamlink . set_option ( "http-cookies" , args . http_cookies ) if args . http_headers : streamlink . set_option ( "http-headers" , args . http_headers ) if args . http_query_params : streamlink . set_option ( "http-query-params" , args . http_query_params )
def setup_plugins ( extra_plugin_dir = None ) : if os . path . isdir ( PLUGINS_DIR ) : load_plugins ( [ PLUGINS_DIR ] ) if extra_plugin_dir : load_plugins ( extra_plugin_dir )
def setup_plugin_args ( session , parser ) : plugin_args = parser . add_argument_group ( "Plugin options" ) for pname , plugin in session . plugins . items ( ) : defaults = { } for parg in plugin . arguments : plugin_args . add_argument ( parg . argument_name ( pname ) , * * parg . options ) defaults [ parg . dest ] = parg . default plugin . options = PluginOptions ( defaults )
def setup_plugin_options ( session , plugin ) : pname = plugin . module required = OrderedDict ( { } ) for parg in plugin . arguments : if parg . options . get ( "help" ) != argparse . SUPPRESS : if parg . required : required [ parg . name ] = parg value = getattr ( args , parg . namespace_dest ( pname ) ) session . set_plugin_option ( pname , parg . dest , value ) if parg . required or value : try : for rparg in plugin . arguments . requires ( parg . name ) : required [ rparg . name ] = rparg except RuntimeError : console . logger . error ( "{0} plugin has a configuration error and the arguments " "cannot be parsed" . format ( pname ) ) break if required : for req in required . values ( ) : if not session . get_plugin_option ( pname , req . dest ) : prompt = req . prompt or "Enter {0} {1}" . format ( pname , req . name ) session . set_plugin_option ( pname , req . dest , console . askpass ( prompt + ": " ) if req . sensitive else console . ask ( prompt + ": " ) )
def log_current_versions ( ) : if logger . root . isEnabledFor ( logging . DEBUG ) : if sys . platform == "darwin" : os_version = "macOS {0}" . format ( platform . mac_ver ( ) [ 0 ] ) elif sys . platform . startswith ( "win" ) : os_version = "{0} {1}" . format ( platform . system ( ) , platform . release ( ) ) else : os_version = platform . platform ( ) log . debug ( "OS:         {0}" . format ( os_version ) ) log . debug ( "Python:     {0}" . format ( platform . python_version ( ) ) ) log . debug ( "Streamlink: {0}" . format ( streamlink_version ) ) log . debug ( "Requests({0}), Socks({1}), Websocket({2})" . format ( requests . __version__ , socks_version , websocket_version ) )
def _get_stream_id ( self , text ) : m = self . _image_re . search ( text ) if m : return m . group ( "stream_id" )
def _get_iframe ( self , text ) : m = self . _iframe_re . search ( text ) if m : return self . session . streams ( m . group ( "url" ) )
def startswith ( string ) : def starts_with ( value ) : validate ( text , value ) if not value . startswith ( string ) : raise ValueError ( "'{0}' does not start with '{1}'" . format ( value , string ) ) return True return starts_with
def endswith ( string ) : def ends_with ( value ) : validate ( text , value ) if not value . endswith ( string ) : raise ValueError ( "'{0}' does not end with '{1}'" . format ( value , string ) ) return True return ends_with
def contains ( string ) : def contains_str ( value ) : validate ( text , value ) if string not in value : raise ValueError ( "'{0}' does not contain '{1}'" . format ( value , string ) ) return True return contains_str
def url ( * * attributes ) : def check_url ( value ) : validate ( text , value ) parsed = urlparse ( value ) if not parsed . netloc : raise ValueError ( "'{0}' is not a valid URL" . format ( value ) ) for name , schema in attributes . items ( ) : if not _hasattr ( parsed , name ) : raise ValueError ( "Invalid URL attribute '{0}'" . format ( name ) ) try : validate ( schema , _getattr ( parsed , name ) ) except ValueError as err : raise ValueError ( "Unable to validate URL attribute '{0}': {1}" . format ( name , err ) ) return True if attributes . get ( "scheme" ) == "http" : attributes [ "scheme" ] = any ( "http" , "https" ) return check_url
def xml_find ( xpath ) : def xpath_find ( value ) : validate ( ET . iselement , value ) value = value . find ( xpath ) if value is None : raise ValueError ( "XPath '{0}' did not return an element" . format ( xpath ) ) return validate ( ET . iselement , value ) return transform ( xpath_find )
def xml_findall ( xpath ) : def xpath_findall ( value ) : validate ( ET . iselement , value ) return value . findall ( xpath ) return transform ( xpath_findall )
def dologin ( self , email , password , emailauth = "" , emailsteamid = "" , captchagid = "-1" , captcha_text = "" , twofactorcode = "" ) : epassword , rsatimestamp = self . encrypt_password ( email , password ) login_data = { 'username' : email , "password" : epassword , "emailauth" : emailauth , "loginfriendlyname" : "Streamlink" , "captchagid" : captchagid , "captcha_text" : captcha_text , "emailsteamid" : emailsteamid , "rsatimestamp" : rsatimestamp , "remember_login" : True , "donotcache" : self . donotcache , "twofactorcode" : twofactorcode } res = self . session . http . post ( self . _dologin_url , data = login_data ) resp = self . session . http . json ( res , schema = self . _dologin_schema ) if not resp [ u"success" ] : if resp . get ( u"captcha_needed" ) : captchagid = resp [ u"captcha_gid" ] log . error ( "Captcha result required, open this URL to see the captcha: {}" . format ( self . _captcha_url . format ( captchagid ) ) ) try : captcha_text = self . input_ask ( "Captcha text" ) except FatalPluginError : captcha_text = None if not captcha_text : return False else : if resp . get ( u"emailauth_needed" ) : if not emailauth : try : emailauth = self . input_ask ( "Email auth code required" ) except FatalPluginError : emailauth = None if not emailauth : return False else : raise SteamLoginFailed ( "Email auth key error" ) if resp . get ( u"requires_twofactor" ) : try : twofactorcode = self . input_ask ( "Two factor auth code required" ) except FatalPluginError : twofactorcode = None if not twofactorcode : return False if resp . get ( u"message" ) : raise SteamLoginFailed ( resp [ u"message" ] ) return self . dologin ( email , password , emailauth = emailauth , emailsteamid = resp . get ( u"emailsteamid" , u"" ) , captcha_text = captcha_text , captchagid = captchagid , twofactorcode = twofactorcode ) elif resp . get ( "login_complete" ) : return True else : log . error ( "Something when wrong when logging in to Steam" ) return False
def get_stream_id ( self , html ) : stream_id = stream_id_pattern . search ( html ) if not stream_id : self . logger . error ( "Failed to extract stream_id." ) return stream_id . group ( "stream_id" )
def _login ( self , username , password ) : self . logger . debug ( 'login ...' ) res = self . session . http . get ( self . login_url ) input_list = self . _input_re . findall ( res . text ) if not input_list : raise PluginError ( 'Missing input data on login website.' ) data = { } for _input_data in input_list : try : _input_name = self . _name_re . search ( _input_data ) . group ( 1 ) except AttributeError : continue try : _input_value = self . _value_re . search ( _input_data ) . group ( 1 ) except AttributeError : _input_value = '' data [ _input_name ] = _input_value login_data = { 'ctl00$Login1$UserName' : username , 'ctl00$Login1$Password' : password , 'ctl00$Login1$LoginButton.x' : '0' , 'ctl00$Login1$LoginButton.y' : '0' } data . update ( login_data ) res = self . session . http . post ( self . login_url , data = data ) for cookie in self . session . http . cookies : self . _session_attributes . set ( cookie . name , cookie . value , expires = 3600 * 24 ) if self . _session_attributes . get ( 'ASP.NET_SessionId' ) and self . _session_attributes . get ( '.abportail1' ) : self . logger . debug ( 'New session data' ) self . set_expires_time_cache ( ) return True else : self . logger . error ( 'Failed to login, check your username/password' ) return False
def outputCharFormatter ( c ) : #TODO 2: allow hex only output if 32 < c < 127 : return chr ( c ) elif c == 10 : return '\\n' elif c == 13 : return '\\r' elif c == 32 : return '" "' else : return '\\x{:02x}' . format ( c )
def outputFormatter ( s ) : result = '' def formatSubString ( s ) : for c in s : if c == 32 : yield ' ' else : yield outputCharFormatter ( c ) if len ( result ) < 200 : return '' . join ( formatSubString ( s ) ) else : return '' . join ( formatSubString ( s [ : 100 ] ) ) + '...' + '' . join ( formatSubString ( s [ - 100 : ] ) )
def readBytes ( self , n ) : if self . pos & 7 : raise ValueError ( 'readBytes: need byte boundary' ) result = self . data [ self . pos >> 3 : ( self . pos >> 3 ) + n ] self . pos += 8 * n return result
def showCode ( self , width = 80 ) : #make table of all symbols with binary strings symbolStrings = [ ( self . bitPattern ( s . index ) , self . mnemonic ( s . index ) ) for s in self ] #determine column widths the way Lisp programmers do it leftColWidth , rightColWidth = map ( max , map ( map , repeat ( len ) , zip ( * symbolStrings ) ) ) colwidth = leftColWidth + rightColWidth columns = 81 // ( colwidth + 2 ) rows = - ( - len ( symbolStrings ) // columns ) def justify ( bs ) : b , s = bs return b . rjust ( leftColWidth ) + ':' + s . ljust ( rightColWidth ) for i in range ( rows ) : print ( ' ' . join ( map ( justify , symbolStrings [ i : : rows ] ) ) . rstrip ( ) )
def readTuple ( self , stream ) : length , symbol = self . decodePeek ( stream . peek ( self . maxLength ) ) stream . pos += length return length , symbol
def value ( self , index , extra ) : lower , upper = self . span ( index ) value = lower + ( extra or 0 ) if value > upper : raise ValueError ( 'value: extra out of range' ) return value
def value ( self , index , extra ) : index = index if index == 0 : return 1 , 0 if index <= self . RLEMAX : return ( 1 << index ) + extra , 0 return 1 , index - self . RLEMAX
def mnemonic ( self , index ) : i , c , d0 = self . splitSymbol ( index ) iLower , _ = i . code . span ( i . index ) iExtra = i . extraBits ( ) cLower , _ = c . code . span ( c . index ) cExtra = c . extraBits ( ) return 'I{}{}{}C{}{}{}{}' . format ( iLower , '+' if iExtra else '' , 'x' * iExtra if iExtra < 6 else '[{}*x]' . format ( iExtra ) , cLower , '+' if cExtra else '' , 'x' * cExtra if cExtra < 6 else '[{}*x]' . format ( cExtra ) , '&D=0' if d0 else '' )
def compileActions ( self ) : import re self . actionList = actions = [ None ] * 121 #Action 73, which is too long, looks like this when expanded: actions [ 73 ] = "b' the '+w+b' of the '" #find out what the columns are actionLines = self . actionTable . splitlines ( ) colonPositions = [ m . start ( ) for m in re . finditer ( ':' , actionLines [ 1 ] ) ] + [ 100 ] columns = [ ( colonPositions [ i ] - 3 , colonPositions [ i + 1 ] - 3 ) for i in range ( len ( colonPositions ) - 1 ) ] for line in self . actionTable . splitlines ( keepends = False ) : for start , end in columns : action = line [ start : end ] #skip empty actions if not action or action . isspace ( ) : continue #chop it up, and check if the colon is properly placed index , colon , action = action [ : 3 ] , action [ 3 ] , action [ 4 : ] assert colon == ':' #remove filler spaces at right action = action . rstrip ( ) #replace space symbols action = action . replace ( '_' , ' ' ) wPos = action . index ( 'w' ) #add quotes around left string when present #translation: any pattern from beginning, up to #(but not including) a + following by a w later on action = re . sub ( r"^(.*)(?=\+[U(]*w)" , r"b'\1'" , action ) #add quotes around right string when present #translation: anything with a w in it, followed by a + #and a pattern up to the end #(there is no variable lookbehind assertion, #so we have to copy the pattern) action = re . sub ( r"(w[[:\-1\]).U]*)\+(.*)$" , r"\1+b'\2'" , action ) #expand shortcut for uppercaseAll action = action . replace ( ".U" , ".upper()" ) #store action actions [ int ( index ) ] = action
def doAction ( self , w , action ) : #set environment for the UpperCaseFirst U = self . upperCase1 return eval ( self . actionList [ action ] , locals ( ) )
def processStream ( self ) : print ( 'addr  hex{:{}s}binary context explanation' . format ( '' , self . width - 10 ) ) print ( 'Stream header' . center ( 60 , '-' ) ) self . windowSize = self . verboseRead ( WindowSizeAlphabet ( ) ) print ( 'Metablock header' . center ( 60 , '=' ) ) self . ISLAST = False self . output = bytearray ( ) while not self . ISLAST : self . ISLAST = self . verboseRead ( BoolCode ( 'LAST' , description = "Last block" ) ) if self . ISLAST : if self . verboseRead ( BoolCode ( 'EMPTY' , description = "Empty block" ) ) : break if self . metablockLength ( ) : continue if not self . ISLAST and self . uncompressed ( ) : continue print ( 'Block type descriptors' . center ( 60 , '-' ) ) self . numberOfBlockTypes = { } self . currentBlockCounts = { } self . blockTypeCodes = { } self . blockCountCodes = { } for blockType in ( L , I , D ) : self . blockType ( blockType ) print ( 'Distance code parameters' . center ( 60 , '-' ) ) self . NPOSTFIX , self . NDIRECT = self . verboseRead ( DistanceParamAlphabet ( ) ) self . readLiteralContextModes ( ) print ( 'Context maps' . center ( 60 , '-' ) ) self . cmaps = { } #keep the number of each kind of prefix tree for the last loop numberOfTrees = { I : self . numberOfBlockTypes [ I ] } for blockType in ( L , D ) : numberOfTrees [ blockType ] = self . contextMap ( blockType ) print ( 'Prefix code lists' . center ( 60 , '-' ) ) self . prefixCodes = { } for blockType in ( L , I , D ) : self . readPrefixArray ( blockType , numberOfTrees [ blockType ] ) self . metablock ( )
def uncompressed ( self ) : ISUNCOMPRESSED = self . verboseRead ( BoolCode ( 'UNCMPR' , description = 'Is uncompressed?' ) ) if ISUNCOMPRESSED : self . verboseRead ( FillerAlphabet ( streamPos = self . stream . pos ) ) print ( 'Uncompressed data:' ) self . output += self . stream . readBytes ( self . MLEN ) print ( outputFormatter ( self . output [ - self . MLEN : ] ) ) return ISUNCOMPRESSED
def blockType ( self , kind ) : NBLTYPES = self . verboseRead ( TypeCountAlphabet ( 'BT#' + kind [ 0 ] . upper ( ) , description = '{} block types' . format ( kind ) , ) ) self . numberOfBlockTypes [ kind ] = NBLTYPES if NBLTYPES >= 2 : self . blockTypeCodes [ kind ] = self . readPrefixCode ( BlockTypeAlphabet ( 'BT' + kind [ 0 ] . upper ( ) , NBLTYPES ) ) self . blockCountCodes [ kind ] = self . readPrefixCode ( BlockCountAlphabet ( 'BC' + kind [ 0 ] . upper ( ) ) ) blockCount = self . verboseRead ( self . blockCountCodes [ kind ] ) else : blockCount = 1 << 24 self . currentBlockCounts [ kind ] = blockCount
def IMTF ( v ) : #mtf is initialized virtually with range(infinity) mtf = [ ] for i , vi in enumerate ( v ) : #get old value from mtf. If never seen, take virtual value try : value = mtf . pop ( vi ) except IndexError : value = vi #put value at front mtf . insert ( 0 , value ) #replace transformed value v [ i ] = value
def readPrefixArray ( self , kind , numberOfTrees ) : prefixes = [ ] for i in range ( numberOfTrees ) : if kind == L : alphabet = LiteralAlphabet ( i ) elif kind == I : alphabet = InsertAndCopyAlphabet ( i ) elif kind == D : alphabet = DistanceAlphabet ( i , NPOSTFIX = self . NPOSTFIX , NDIRECT = self . NDIRECT ) self . readPrefixCode ( alphabet ) prefixes . append ( alphabet ) self . prefixCodes [ kind ] = prefixes
def arrow_table_from_vaex_df ( ds , column_names = None , selection = None , strings = True , virtual = False ) : names = [ ] arrays = [ ] for name , array in ds . to_items ( column_names = column_names , selection = selection , strings = strings , virtual = virtual ) : names . append ( name ) arrays . append ( arrow_array_from_numpy_array ( array ) ) return pyarrow . Table . from_arrays ( arrays , names )
def patch ( f ) : name = f . __name__ Dataset . __hidden__ [ name ] = f return f
def _graphviz ( self , dot = None ) : from graphviz import Graph , Digraph node = self . _graph ( ) dot = dot or Digraph ( comment = self . expression ) def walk ( node ) : if isinstance ( node , six . string_types ) : dot . node ( node , node ) return node , node else : node_repr , fname , fobj , deps = node node_id = node_repr dot . node ( node_id , node_repr ) for dep in deps : dep_id , dep = walk ( dep ) dot . edge ( node_id , dep_id ) return node_id , node walk ( node ) return dot
def from_astropy_table ( table ) : import vaex . file . other return vaex . file . other . DatasetAstropyTable ( table = table )
def zeldovich ( dim = 2 , N = 256 , n = - 2.5 , t = None , scale = 1 , seed = None ) : import vaex . file return vaex . file . other . Zeldovich ( dim = dim , N = N , n = n , t = t , scale = scale )
def vrange ( start , stop , step = 1 , dtype = 'f8' ) : from . column import ColumnVirtualRange return ColumnVirtualRange ( start , stop , step , dtype )
def open ( self , path ) : logger . debug ( "open dataset: %r" , path ) if path . startswith ( "http" ) or path . startswith ( "ws" ) : dataset = vaex . open ( path , thread_mover = self . call_in_main_thread ) else : dataset = vaex . open ( path ) self . add_recently_opened ( path ) self . dataset_selector . add ( dataset ) return dataset
def evaluate ( self , expression , i1 = None , i2 = None , out = None , selection = None , delay = False ) : expression = _ensure_strings_from_expressions ( expression ) result = self . server . _call_dataset ( "evaluate" , self , expression = expression , i1 = i1 , i2 = i2 , selection = selection , delay = delay ) return result
def _depending_columns ( self , ds ) : depending = set ( ) for expression in self . expressions : expression = ds . _expr ( expression ) depending |= expression . variables ( ) if self . previous_selection : depending |= self . previous_selection . _depending_columns ( ds ) return depending
def _task ( self , task , progressbar = False ) : if self . delay : return self . executor . schedule ( task ) else : import vaex . utils callback = None try : if progressbar == True : def update ( fraction ) : bar . update ( fraction ) return True bar = vaex . utils . progressbar ( task . name ) callback = self . executor . signal_progress . connect ( update ) elif progressbar : callback = self . executor . signal_progress . connect ( progressbar ) result = self . executor . run ( task ) if progressbar == True : bar . finish ( ) sys . stdout . write ( '\n' ) return result finally : if callback : self . executor . signal_progress . disconnect ( callback )
def sort ( self , Ncol , order ) : self . emit ( QtCore . SIGNAL ( "layoutAboutToBeChanged()" ) ) if Ncol == 0 : print ( "by name" ) sortlist = list ( zip ( self . pairs , list ( range ( len ( self . pairs ) ) ) ) ) print ( sortlist ) sortlist . sort ( key = operator . itemgetter ( 0 ) ) print ( sortlist ) self . indices = list ( map ( operator . itemgetter ( 1 ) , sortlist ) ) print ( ( self . indices ) ) if Ncol == 1 : if None not in self . ranking : sortlist = list ( zip ( self . ranking , list ( range ( len ( self . pairs ) ) ) ) ) sortlist . sort ( key = operator . itemgetter ( 0 ) ) self . indices = list ( map ( operator . itemgetter ( 1 ) , sortlist ) ) else : self . indices = list ( range ( len ( self . pairs ) ) ) print ( ( self . indices ) ) if order == QtCore . Qt . DescendingOrder : self . indices . reverse ( ) print ( ( self . indices ) ) self . emit ( QtCore . SIGNAL ( "layoutChanged()" ) )
def _wait ( self ) : logger . debug ( "will wait for last plot to finish" ) self . _plot_event = threading . Event ( ) self . queue_update . _wait ( ) self . queue_replot . _wait ( ) self . queue_redraw . _wait ( ) qt_app = QtCore . QCoreApplication . instance ( ) sleep = 10 while not self . _plot_event . is_set ( ) : logger . debug ( "waiting for last plot to finish" ) qt_app . processEvents ( ) QtTest . QTest . qSleep ( sleep ) logger . debug ( "waiting for plot finished" )
def os_open ( document ) : osname = platform . system ( ) . lower ( ) if osname == "darwin" : os . system ( "open \"" + document + "\"" ) if osname == "linux" : cmd = "xdg-open \"" + document + "\"&" os . system ( cmd ) if osname == "windows" : os . system ( "start \"" + document + "\"" )
def write_to ( f , mode ) : if hasattr ( f , 'write' ) : yield f else : f = open ( f , mode ) yield f f . close ( )
def _split_and_combine_mask ( arrays ) : masks = [ np . ma . getmaskarray ( block ) for block in arrays if np . ma . isMaskedArray ( block ) ] arrays = [ block . data if np . ma . isMaskedArray ( block ) else block for block in arrays ] mask = None if masks : mask = masks [ 0 ] . copy ( ) for other in masks [ 1 : ] : mask |= other return arrays , mask
def nop ( self , expression , progress = False , delay = False ) : expression = _ensure_string_from_expression ( expression ) def map ( ar ) : pass def reduce ( a , b ) : pass return self . map_reduce ( map , reduce , [ expression ] , delay = delay , progress = progress , name = 'nop' , to_numpy = False )
def plot3d ( self , x , y , z , vx = None , vy = None , vz = None , vwhat = None , limits = None , grid = None , what = "count(*)" , shape = 128 , selection = [ None , True ] , f = None , vcount_limits = None , smooth_pre = None , smooth_post = None , grid_limits = None , normalize = "normalize" , colormap = "afmhot" , figure_key = None , fig = None , lighting = True , level = [ 0.1 , 0.5 , 0.9 ] , opacity = [ 0.01 , 0.05 , 0.1 ] , level_width = 0.1 , show = True , * * kwargs ) : import vaex . ext . ipyvolume cls = vaex . ext . ipyvolume . PlotDefault plot3d = cls ( df = self , x = x , y = y , z = z , vx = vx , vy = vy , vz = vz , grid = grid , shape = shape , limits = limits , what = what , f = f , figure_key = figure_key , fig = fig , selection = selection , smooth_pre = smooth_pre , smooth_post = smooth_post , grid_limits = grid_limits , vcount_limits = vcount_limits , normalize = normalize , colormap = colormap , * * kwargs ) if show : plot3d . show ( ) return plot3d
def dtype ( self , expression , internal = False ) : expression = _ensure_string_from_expression ( expression ) if expression in self . variables : return np . float64 ( 1 ) . dtype elif expression in self . columns . keys ( ) : column = self . columns [ expression ] data = column [ 0 : 1 ] dtype = data . dtype else : data = self . evaluate ( expression , 0 , 1 , filtered = False ) dtype = data . dtype if not internal : if dtype != str_type : if dtype . kind in 'US' : return str_type if dtype . kind == 'O' : if isinstance ( data [ 0 ] , six . string_types ) : return str_type return dtype
def remove_virtual_meta ( self ) : dir = self . get_private_dir ( create = True ) path = os . path . join ( dir , "virtual_meta.yaml" ) try : if os . path . exists ( path ) : os . remove ( path ) if not os . listdir ( dir ) : os . rmdir ( dir ) except : logger . exception ( "error while trying to remove %s or %s" , path , dir )
def evaluate_variable ( self , name ) : if isinstance ( self . variables [ name ] , six . string_types ) : value = eval ( self . variables [ name ] , expression_namespace , self . variables ) return value else : return self . variables [ name ]
def _evaluate_selection_mask ( self , name = "default" , i1 = None , i2 = None , selection = None , cache = False ) : i1 = i1 or 0 i2 = i2 or len ( self ) scope = scopes . _BlockScopeSelection ( self , i1 , i2 , selection , cache = cache ) return scope . evaluate ( name )
def add_column ( self , name , f_or_array ) : if isinstance ( f_or_array , ( np . ndarray , Column ) ) : data = ar = f_or_array if self . _length_original is None : self . _length_unfiltered = _len ( data ) self . _length_original = _len ( data ) self . _index_end = self . _length_unfiltered if _len ( ar ) != self . length_original ( ) : if self . filtered : if len ( self ) == len ( ar ) : raise ValueError ( "Array is of length %s, while the length of the DataFrame is %s due to the filtering, the (unfiltered) length is %s." % ( len ( ar ) , len ( self ) , self . length_unfiltered ( ) ) ) raise ValueError ( "array is of length %s, while the length of the DataFrame is %s" % ( len ( ar ) , self . length_original ( ) ) ) self . columns [ name ] = f_or_array if name not in self . column_names : self . column_names . append ( name ) else : raise ValueError ( "functions not yet implemented" ) self . _save_assign_expression ( name , Expression ( self , name ) )
def rename_column ( self , name , new_name , unique = False , store_in_state = True ) : new_name = vaex . utils . find_valid_name ( new_name , used = [ ] if not unique else list ( self ) ) data = self . columns . get ( name ) if data is not None : del self . columns [ name ] self . column_names [ self . column_names . index ( name ) ] = new_name self . columns [ new_name ] = data else : expression = self . virtual_columns [ name ] del self . virtual_columns [ name ] self . virtual_columns [ new_name ] = expression if store_in_state : self . _renamed_columns . append ( ( name , new_name ) ) for d in [ self . ucds , self . units , self . descriptions ] : if name in d : d [ new_name ] = d [ name ] del d [ name ] return new_name
def delete_virtual_column ( self , name ) : del self . virtual_columns [ name ] self . signal_column_changed . emit ( self , name , "delete" )
def delete_variable ( self , name ) : del self . variables [ name ] self . signal_variable_changed . emit ( self , name , "delete" )
def tail ( self , n = 10 ) : N = len ( self ) return self [ max ( 0 , N - n ) : min ( len ( self ) , N ) ]
def head_and_tail_print ( self , n = 5 ) : from IPython import display display . display ( display . HTML ( self . _head_and_tail_table ( n ) ) )
def set_current_row ( self , value ) : if ( value is not None ) and ( ( value < 0 ) or ( value >= len ( self ) ) ) : raise IndexError ( "index %d out of range [0,%d]" % ( value , len ( self ) ) ) self . _current_row = value self . signal_pick . emit ( self , value )
def selection_undo ( self , name = "default" , executor = None ) : logger . debug ( "undo" ) executor = executor or self . executor assert self . selection_can_undo ( name = name ) selection_history = self . selection_histories [ name ] index = self . selection_history_indices [ name ] self . selection_history_indices [ name ] -= 1 self . signal_selection_changed . emit ( self ) logger . debug ( "undo: selection history is %r, index is %r" , selection_history , self . selection_history_indices [ name ] )
def selection_redo ( self , name = "default" , executor = None ) : logger . debug ( "redo" ) executor = executor or self . executor assert self . selection_can_redo ( name = name ) selection_history = self . selection_histories [ name ] index = self . selection_history_indices [ name ] next = selection_history [ index + 1 ] self . selection_history_indices [ name ] += 1 self . signal_selection_changed . emit ( self ) logger . debug ( "redo: selection history is %r, index is %r" , selection_history , index )
def selection_can_redo ( self , name = "default" ) : return ( self . selection_history_indices [ name ] + 1 ) < len ( self . selection_histories [ name ] )
def _selection ( self , create_selection , name , executor = None , execute_fully = False ) : selection_history = self . selection_histories [ name ] previous_index = self . selection_history_indices [ name ] current = selection_history [ previous_index ] if selection_history else None selection = create_selection ( current ) executor = executor or self . executor selection_history . append ( selection ) self . selection_history_indices [ name ] += 1 del selection_history [ self . selection_history_indices [ name ] : - 1 ] if 0 : if self . is_local ( ) : if selection : result = vaex . promise . Promise . fulfilled ( None ) self . signal_selection_changed . emit ( self ) else : result = vaex . promise . Promise . fulfilled ( None ) self . signal_selection_changed . emit ( self ) else : self . signal_selection_changed . emit ( self ) result = vaex . promise . Promise . fulfilled ( None ) self . signal_selection_changed . emit ( self ) result = vaex . promise . Promise . fulfilled ( None ) logger . debug ( "select selection history is %r, index is %r" , selection_history , self . selection_history_indices [ name ] ) return result
def _find_valid_name ( self , initial_name ) : return vaex . utils . find_valid_name ( initial_name , used = self . get_column_names ( hidden = True ) )
def _root_nodes ( self ) : root_nodes = [ ] leafes = [ ] def walk ( node ) : if isinstance ( node , six . string_types ) : leafes . append ( node ) if node in root_nodes : root_nodes . remove ( node ) else : node_repr , fname , fobj , deps = node if node_repr in self . virtual_columns : leafes . append ( node_repr ) if node_repr in root_nodes : root_nodes . remove ( node_repr ) for dep in deps : walk ( dep ) for column in self . virtual_columns . keys ( ) : if column not in leafes : root_nodes . append ( column ) node = self [ column ] . _graph ( ) node_repr , fname , fobj , deps = node for dep in deps : walk ( dep ) return root_nodes
def _graphviz ( self , dot = None ) : from graphviz import Digraph dot = dot or Digraph ( comment = 'whole dataframe' ) root_nodes = self . _root_nodes ( ) for column in root_nodes : self [ column ] . _graphviz ( dot = dot ) return dot
def categorize ( self , column , labels = None , check = True ) : column = _ensure_string_from_expression ( column ) if check : vmin , vmax = self . minmax ( column ) if labels is None : N = int ( vmax + 1 ) labels = list ( map ( str , range ( N ) ) ) if ( vmax - vmin ) >= len ( labels ) : raise ValueError ( 'value of {} found, which is larger than number of labels {}' . format ( vmax , len ( labels ) ) ) self . _categories [ column ] = dict ( labels = labels , N = len ( labels ) )
def _hstack ( self , other , prefix = None ) : assert len ( self ) == len ( other ) , "does not make sense to horizontally stack DataFrames with different lengths" for name in other . get_column_names ( ) : if prefix : new_name = prefix + name else : new_name = name self . add_column ( new_name , other . columns [ name ] )
def patch ( f ) : name = f . __name__ setattr ( DataFrame , name , f ) return f
def as_recarray ( self ) : dtype = [ ( k , v . dtype ) for k , v in self . __dict__ . iteritems ( ) ] R = numpy . recarray ( len ( self . __dict__ [ k ] ) , dtype = dtype ) for key in self . __dict__ : R [ key ] = self . __dict__ [ key ] return R
def show_versions ( ) : core_deps = [ 'audioread' , 'numpy' , 'scipy' , 'sklearn' , 'joblib' , 'decorator' , 'six' , 'soundfile' , 'resampy' , 'numba' ] extra_deps = [ 'numpydoc' , 'sphinx' , 'sphinx_rtd_theme' , 'sphinxcontrib.versioning' , 'sphinx-gallery' , 'pytest' , 'pytest-mpl' , 'pytest-cov' , 'matplotlib' ] print ( 'INSTALLED VERSIONS' ) print ( '------------------' ) print ( 'python: {}\n' . format ( sys . version ) ) print ( 'librosa: {}\n' . format ( version ) ) for dep in core_deps : print ( '{}: {}' . format ( dep , __get_mod_version ( dep ) ) ) print ( '' ) for dep in extra_deps : print ( '{}: {}' . format ( dep , __get_mod_version ( dep ) ) ) pass
def adjust_tuning ( input_file , output_file ) : print ( 'Loading ' , input_file ) y , sr = librosa . load ( input_file ) print ( 'Separating harmonic component ... ' ) y_harm = librosa . effects . harmonic ( y ) print ( 'Estimating tuning ... ' ) tuning = librosa . estimate_tuning ( y = y_harm , sr = sr ) print ( '{:+0.2f} cents' . format ( 100 * tuning ) ) print ( 'Applying pitch-correction of {:+0.2f} cents' . format ( - 100 * tuning ) ) y_tuned = librosa . effects . pitch_shift ( y , sr , - tuning ) print ( 'Saving tuned audio to: ' , output_file ) librosa . output . write_wav ( output_file , y_tuned , sr )
def __cqt_filter_fft ( sr , fmin , n_bins , bins_per_octave , tuning , filter_scale , norm , sparsity , hop_length = None , window = 'hann' ) : basis , lengths = filters . constant_q ( sr , fmin = fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , norm = norm , pad_fft = True , window = window ) n_fft = basis . shape [ 1 ] if ( hop_length is not None and n_fft < 2.0 ** ( 1 + np . ceil ( np . log2 ( hop_length ) ) ) ) : n_fft = int ( 2.0 ** ( 1 + np . ceil ( np . log2 ( hop_length ) ) ) ) basis *= lengths [ : , np . newaxis ] / float ( n_fft ) fft = get_fftlib ( ) fft_basis = fft . fft ( basis , n = n_fft , axis = 1 ) [ : , : ( n_fft // 2 ) + 1 ] fft_basis = util . sparsify_rows ( fft_basis , quantile = sparsity ) return fft_basis , n_fft , lengths
def __trim_stack ( cqt_resp , n_bins ) : max_col = min ( x . shape [ 1 ] for x in cqt_resp ) cqt_resp = np . vstack ( [ x [ : , : max_col ] for x in cqt_resp ] [ : : - 1 ] ) return np . ascontiguousarray ( cqt_resp [ - n_bins : ] . T ) . T
def __cqt_response ( y , n_fft , hop_length , fft_basis , mode ) : D = stft ( y , n_fft = n_fft , hop_length = hop_length , window = 'ones' , pad_mode = mode ) return fft_basis . dot ( D )
def __early_downsample_count ( nyquist , filter_cutoff , hop_length , n_octaves ) : downsample_count1 = max ( 0 , int ( np . ceil ( np . log2 ( audio . BW_FASTEST * nyquist / filter_cutoff ) ) - 1 ) - 1 ) num_twos = __num_two_factors ( hop_length ) downsample_count2 = max ( 0 , num_twos - n_octaves + 1 ) return min ( downsample_count1 , downsample_count2 )
def __early_downsample ( y , sr , hop_length , res_type , n_octaves , nyquist , filter_cutoff , scale ) : downsample_count = __early_downsample_count ( nyquist , filter_cutoff , hop_length , n_octaves ) if downsample_count > 0 and res_type == 'kaiser_fast' : downsample_factor = 2 ** ( downsample_count ) hop_length //= downsample_factor if len ( y ) < downsample_factor : raise ParameterError ( 'Input signal length={:d} is too short for ' '{:d}-octave CQT' . format ( len ( y ) , n_octaves ) ) new_sr = sr / float ( downsample_factor ) y = audio . resample ( y , sr , new_sr , res_type = res_type , scale = True ) if not scale : y *= np . sqrt ( downsample_factor ) sr = new_sr return y , sr , hop_length
def __check_axes ( axes ) : if axes is None : import matplotlib . pyplot as plt axes = plt . gca ( ) elif not isinstance ( axes , Axes ) : raise ValueError ( "`axes` must be an instance of matplotlib.axes.Axes. " "Found type(axes)={}" . format ( type ( axes ) ) ) return axes
def __scale_axes ( axes , ax_type , which ) : kwargs = dict ( ) if which == 'x' : thresh = 'linthreshx' base = 'basex' scale = 'linscalex' scaler = axes . set_xscale limit = axes . set_xlim else : thresh = 'linthreshy' base = 'basey' scale = 'linscaley' scaler = axes . set_yscale limit = axes . set_ylim if ax_type == 'mel' : mode = 'symlog' kwargs [ thresh ] = 1000.0 kwargs [ base ] = 2 elif ax_type == 'log' : mode = 'symlog' kwargs [ base ] = 2 kwargs [ thresh ] = core . note_to_hz ( 'C2' ) kwargs [ scale ] = 0.5 elif ax_type in [ 'cqt' , 'cqt_hz' , 'cqt_note' ] : mode = 'log' kwargs [ base ] = 2 elif ax_type == 'tempo' : mode = 'log' kwargs [ base ] = 2 limit ( 16 , 480 ) else : return scaler ( mode , * * kwargs )
def __coord_fft_hz ( n , sr = 22050 , * * _kwargs ) : n_fft = 2 * ( n - 1 ) basis = core . fft_frequencies ( sr = sr , n_fft = n_fft ) fmax = basis [ - 1 ] basis -= 0.5 * ( basis [ 1 ] - basis [ 0 ] ) basis = np . append ( np . maximum ( 0 , basis ) , [ fmax ] ) return basis
def __coord_mel_hz ( n , fmin = 0 , fmax = 11025.0 , * * _kwargs ) : if fmin is None : fmin = 0 if fmax is None : fmax = 11025.0 basis = core . mel_frequencies ( n , fmin = fmin , fmax = fmax ) basis [ 1 : ] -= 0.5 * np . diff ( basis ) basis = np . append ( np . maximum ( 0 , basis ) , [ fmax ] ) return basis
def __coord_cqt_hz ( n , fmin = None , bins_per_octave = 12 , * * _kwargs ) : if fmin is None : fmin = core . note_to_hz ( 'C1' ) return core . cqt_frequencies ( n + 1 , fmin = fmin / 2.0 ** ( 0.5 / bins_per_octave ) , bins_per_octave = bins_per_octave )
def __coord_chroma ( n , bins_per_octave = 12 , * * _kwargs ) : return np . linspace ( 0 , ( 12.0 * n ) / bins_per_octave , num = n + 1 , endpoint = True )
def __coord_time ( n , sr = 22050 , hop_length = 512 , * * _kwargs ) : return core . frames_to_time ( np . arange ( n + 1 ) , sr = sr , hop_length = hop_length )
def __window_ss_fill ( x , win_sq , n_frames , hop_length ) : n = len ( x ) n_fft = len ( win_sq ) for i in range ( n_frames ) : sample = i * hop_length x [ sample : min ( n , sample + n_fft ) ] += win_sq [ : max ( 0 , min ( n_fft , n - sample ) ) ]
def __match_interval_overlaps ( query , intervals_to , candidates ) : best_score = - 1 best_idx = - 1 for idx in candidates : score = __jaccard ( query , intervals_to [ idx ] ) if score > best_score : best_score , best_idx = score , idx return best_idx
def __match_intervals ( intervals_from , intervals_to , strict = True ) : start_index = np . argsort ( intervals_to [ : , 0 ] ) end_index = np . argsort ( intervals_to [ : , 1 ] ) start_sorted = intervals_to [ start_index , 0 ] end_sorted = intervals_to [ end_index , 1 ] search_ends = np . searchsorted ( start_sorted , intervals_from [ : , 1 ] , side = 'right' ) search_starts = np . searchsorted ( end_sorted , intervals_from [ : , 0 ] , side = 'left' ) output = np . empty ( len ( intervals_from ) , dtype = numba . uint32 ) for i in range ( len ( intervals_from ) ) : query = intervals_from [ i ] after_query = search_ends [ i ] before_query = search_starts [ i ] candidates = set ( start_index [ : after_query ] ) & set ( end_index [ before_query : ] ) if len ( candidates ) > 0 : output [ i ] = __match_interval_overlaps ( query , intervals_to , candidates ) elif strict : raise ParameterError else : dist_before = np . inf dist_after = np . inf if search_starts [ i ] > 0 : dist_before = query [ 0 ] - end_sorted [ search_starts [ i ] - 1 ] if search_ends [ i ] + 1 < len ( intervals_to ) : dist_after = start_sorted [ search_ends [ i ] + 1 ] - query [ 1 ] if dist_before < dist_after : output [ i ] = end_index [ search_starts [ i ] - 1 ] else : output [ i ] = start_index [ search_ends [ i ] + 1 ] return output
def __get_files ( dir_name , extensions ) : dir_name = os . path . abspath ( os . path . expanduser ( dir_name ) ) myfiles = set ( ) for sub_ext in extensions : globstr = os . path . join ( dir_name , '*' + os . path . extsep + sub_ext ) myfiles |= set ( glob . glob ( globstr ) ) return myfiles
def process_arguments ( args ) : parser = argparse . ArgumentParser ( description = 'Time stretching example' ) parser . add_argument ( 'input_file' , action = 'store' , help = 'path to the input file (wav, mp3, etc)' ) parser . add_argument ( 'output_file' , action = 'store' , help = 'path to the stretched output (wav)' ) parser . add_argument ( '-s' , '--speed' , action = 'store' , type = float , default = 2.0 , required = False , help = 'speed' ) return vars ( parser . parse_args ( args ) )
def __beat_local_score ( onset_envelope , period ) : window = np . exp ( - 0.5 * ( np . arange ( - period , period + 1 ) * 32.0 / period ) ** 2 ) return scipy . signal . convolve ( __normalize_onsets ( onset_envelope ) , window , 'same' )
def __beat_track_dp ( localscore , period , tightness ) : backlink = np . zeros_like ( localscore , dtype = int ) cumscore = np . zeros_like ( localscore ) window = np . arange ( - 2 * period , - np . round ( period / 2 ) + 1 , dtype = int ) if tightness <= 0 : raise ParameterError ( 'tightness must be strictly positive' ) txwt = - tightness * ( np . log ( - window / period ) ** 2 ) first_beat = True for i , score_i in enumerate ( localscore ) : z_pad = np . maximum ( 0 , min ( - window [ 0 ] , len ( window ) ) ) candidates = txwt . copy ( ) candidates [ z_pad : ] = candidates [ z_pad : ] + cumscore [ window [ z_pad : ] ] beat_location = np . argmax ( candidates ) cumscore [ i ] = score_i + candidates [ beat_location ] if first_beat and score_i < 0.01 * localscore . max ( ) : backlink [ i ] = - 1 else : backlink [ i ] = window [ beat_location ] first_beat = False window = window + 1 return backlink , cumscore
def __last_beat ( cumscore ) : maxes = util . localmax ( cumscore ) med_score = np . median ( cumscore [ np . argwhere ( maxes ) ] ) return np . argwhere ( ( cumscore * maxes * 2 > med_score ) ) . max ( )
def conv3x3 ( in_planes , out_planes , dilation = 1 ) : return nn . Conv2d ( in_planes , out_planes , kernel_size = 3 , padding = dilation , dilation = dilation )
def average ( self , n = 0 ) : assert n >= 0 for key in self . val_history : values = np . array ( self . val_history [ key ] [ - n : ] ) nums = np . array ( self . n_history [ key ] [ - n : ] ) avg = np . sum ( values * nums ) / np . sum ( nums ) self . output [ key ] = avg self . ready = True
def scatter ( input , devices , streams = None ) : if streams is None : streams = [ None ] * len ( devices ) if isinstance ( input , list ) : chunk_size = ( len ( input ) - 1 ) // len ( devices ) + 1 outputs = [ scatter ( input [ i ] , [ devices [ i // chunk_size ] ] , [ streams [ i // chunk_size ] ] ) for i in range ( len ( input ) ) ] return outputs elif isinstance ( input , torch . Tensor ) : output = input . contiguous ( ) stream = streams [ 0 ] if output . numel ( ) > 0 else None with torch . cuda . device ( devices [ 0 ] ) , torch . cuda . stream ( stream ) : output = output . cuda ( devices [ 0 ] , non_blocking = True ) return output else : raise Exception ( 'Unknown type {}.' . format ( type ( input ) ) )
def start ( self ) : if not self . _is_running : self . _t_start = time ( ) self . _is_running = True self . _t_last = time ( )
def scatter_kwargs ( inputs , kwargs , target_gpus , dim = 0 ) : inputs = scatter ( inputs , target_gpus , dim ) if inputs else [ ] kwargs = scatter ( kwargs , target_gpus , dim ) if kwargs else [ ] if len ( inputs ) < len ( kwargs ) : inputs . extend ( [ ( ) for _ in range ( len ( kwargs ) - len ( inputs ) ) ] ) elif len ( kwargs ) < len ( inputs ) : kwargs . extend ( [ { } for _ in range ( len ( inputs ) - len ( kwargs ) ) ] ) inputs = tuple ( inputs ) kwargs = tuple ( kwargs ) return inputs , kwargs
async def json ( self , * , encoding : str = None , loads : JSONDecoder = DEFAULT_JSON_DECODER , content_type : Optional [ str ] = 'application/json' ) -> Any : return await self . _aws_json ( encoding = encoding , loads = loads , content_type = content_type )
async def text ( self , * , encoding : Optional [ str ] = None , errors : str = 'strict' ) -> str : return await self . _aws_text ( encoding = encoding , errors = errors )
async def handle_callback ( self , aws_callback : typing . Coroutine , response ) : callback_result = None try : callback_result = await aws_callback except NothingMatchedError as e : self . logger . error ( f'<Item: {str(e).lower()}>' ) except Exception as e : self . logger . error ( f'<Callback[{aws_callback.__name__}]: {e}' ) return callback_result , response
async def multiple_request ( self , urls , is_gather = False , * * kwargs ) : if is_gather : resp_results = await asyncio . gather ( * [ self . handle_request ( self . request ( url = url , * * kwargs ) ) for url in urls ] , return_exceptions = True ) for index , task_result in enumerate ( resp_results ) : if not isinstance ( task_result , RuntimeError ) and task_result : _ , response = task_result response . index = index yield response else : for index , url in enumerate ( urls ) : _ , response = await self . handle_request ( self . request ( url = url , * * kwargs ) ) response . index = index yield response
def request ( self , url : str , method : str = 'GET' , * , callback = None , encoding : typing . Optional [ str ] = None , headers : dict = None , metadata : dict = None , request_config : dict = None , request_session = None , * * kwargs ) : headers = headers or { } metadata = metadata or { } request_config = request_config or { } request_session = request_session or self . request_session headers . update ( self . headers . copy ( ) ) request_config . update ( self . request_config . copy ( ) ) kwargs . update ( self . kwargs . copy ( ) ) return Request ( url = url , method = method , callback = callback , encoding = encoding , headers = headers , metadata = metadata , request_config = request_config , request_session = request_session , * * kwargs )
async def start_master ( self ) : for url in self . start_urls : request_ins = self . request ( url = url , callback = self . parse , metadata = self . metadata ) self . request_queue . put_nowait ( self . handle_request ( request_ins ) ) workers = [ asyncio . ensure_future ( self . start_worker ( ) ) for i in range ( self . worker_numbers ) ] for worker in workers : self . logger . info ( f"Worker started: {id(worker)}" ) await self . request_queue . join ( ) if not self . is_async_start : await self . stop ( SIGINT ) else : await self . _cancel_tasks ( )
def normalize_task_v2 ( task ) : result = dict ( ) mod_arg_parser = ModuleArgsParser ( task ) try : action , arguments , result [ 'delegate_to' ] = mod_arg_parser . parse ( ) except AnsibleParserError as e : try : task_info = "%s:%s" % ( task [ FILENAME_KEY ] , task [ LINE_NUMBER_KEY ] ) del task [ FILENAME_KEY ] del task [ LINE_NUMBER_KEY ] except KeyError : task_info = "Unknown" try : import pprint pp = pprint . PrettyPrinter ( indent = 2 ) task_pprint = pp . pformat ( task ) except ImportError : task_pprint = task raise SystemExit ( "Couldn't parse task at %s (%s)\n%s" % ( task_info , e . message , task_pprint ) ) if '_uses_shell' in arguments : action = 'shell' del ( arguments [ '_uses_shell' ] ) for ( k , v ) in list ( task . items ( ) ) : if k in ( 'action' , 'local_action' , 'args' , 'delegate_to' ) or k == action : continue else : result [ k ] = v result [ 'action' ] = dict ( __ansible_module__ = action ) if '_raw_params' in arguments : result [ 'action' ] [ '__ansible_arguments__' ] = arguments [ '_raw_params' ] . split ( ' ' ) del ( arguments [ '_raw_params' ] ) else : result [ 'action' ] [ '__ansible_arguments__' ] = list ( ) if 'argv' in arguments and not result [ 'action' ] [ '__ansible_arguments__' ] : result [ 'action' ] [ '__ansible_arguments__' ] = arguments [ 'argv' ] del ( arguments [ 'argv' ] ) result [ 'action' ] . update ( arguments ) return result
def wheel_dist_name ( self ) : return '-' . join ( ( safer_name ( self . distribution . get_name ( ) ) , safer_version ( self . distribution . get_version ( ) ) ) )
def get_archive_basename ( self ) : impl_tag , abi_tag , plat_tag = self . get_tag ( ) archive_basename = "%s-%s-%s-%s" % ( self . wheel_dist_name , impl_tag , abi_tag , plat_tag ) return archive_basename
def add_requirements ( self , metadata_path ) : additional = list ( self . setupcfg_requirements ( ) ) if not additional : return pkg_info = read_pkg_info ( metadata_path ) if 'Provides-Extra' in pkg_info or 'Requires-Dist' in pkg_info : warnings . warn ( 'setup.cfg requirements overwrite values from setup.py' ) del pkg_info [ 'Provides-Extra' ] del pkg_info [ 'Requires-Dist' ] for k , v in additional : pkg_info [ k ] = v write_pkg_info ( metadata_path , pkg_info )
def telemetry_client ( self , value : BotTelemetryClient ) -> None : if value is None : self . _telemetry_client = NullTelemetryClient ( ) else : self . _telemetry_client = value
def __create_db_and_container ( self ) : db_id = self . config . database container_name = self . config . container self . db = self . __get_or_create_database ( self . client , db_id ) self . container = self . __get_or_create_container ( self . client , container_name )
def get_step_name ( self , index : int ) -> str : step_name = self . _steps [ index ] . __qualname__ if not step_name or ">" in step_name : step_name = f"Step{index + 1}of{len(self._steps)}" return step_name
def c_if ( self , classical , val ) : if not isinstance ( classical , ClassicalRegister ) : raise QiskitError ( "c_if must be used with a classical register" ) if val < 0 : raise QiskitError ( "control value should be non-negative" ) self . control = ( classical , val ) return self
def _qasmif ( self , string ) : if self . control is None : return string return "if(%s==%d) " % ( self . control [ 0 ] . name , self . control [ 1 ] ) + string
def u_base ( self , theta , phi , lam , q ) : return self . append ( UBase ( theta , phi , lam ) , [ q ] , [ ] )
def to_matrix ( self ) : theta , phi , lam = self . params return numpy . array ( [ [ numpy . cos ( theta / 2 ) , - numpy . exp ( 1j * lam ) * numpy . sin ( theta / 2 ) ] , [ numpy . exp ( 1j * phi ) * numpy . sin ( theta / 2 ) , numpy . exp ( 1j * ( phi + lam ) ) * numpy . cos ( theta / 2 ) ] ] , dtype = complex )
def exp_fit_fun ( x , a , tau , c ) : return a * np . exp ( - x / tau ) + c
def osc_fit_fun ( x , a , tau , f , phi , c ) : return a * np . exp ( - x / tau ) * np . cos ( 2 * np . pi * f * x + phi ) + c
def _trim ( image ) : background = PIL . Image . new ( image . mode , image . size , image . getpixel ( ( 0 , 0 ) ) ) diff = PIL . ImageChops . difference ( image , background ) diff = PIL . ImageChops . add ( diff , diff , 2.0 , - 100 ) bbox = diff . getbbox ( ) if bbox : image = image . crop ( bbox ) return image
def _get_gate_span ( qregs , instruction ) : min_index = len ( qregs ) max_index = 0 for qreg in instruction . qargs : index = qregs . index ( qreg ) if index < min_index : min_index = index if index > max_index : max_index = index if instruction . cargs : return qregs [ min_index : ] return qregs [ min_index : max_index + 1 ]
def is_cptp ( self , atol = None , rtol = None ) : if self . _data [ 1 ] is not None : return False if atol is None : atol = self . _atol if rtol is None : rtol = self . _rtol accum = 0j for op in self . _data [ 0 ] : accum += np . dot ( np . transpose ( np . conj ( op ) ) , op ) return is_identity_matrix ( accum , rtol = rtol , atol = atol )
def conjugate ( self ) : kraus_l , kraus_r = self . _data kraus_l = [ k . conj ( ) for k in kraus_l ] if kraus_r is not None : kraus_r = [ k . conj ( ) for k in kraus_r ] return Kraus ( ( kraus_l , kraus_r ) , self . input_dims ( ) , self . output_dims ( ) )
def transpose ( self ) : kraus_l , kraus_r = self . _data kraus_l = [ k . T for k in kraus_l ] if kraus_r is not None : kraus_r = [ k . T for k in kraus_r ] return Kraus ( ( kraus_l , kraus_r ) , input_dims = self . output_dims ( ) , output_dims = self . input_dims ( ) )
def real ( self , nested_scope = None ) : operation = self . children [ 0 ] . operation ( ) lhs = self . children [ 1 ] . real ( nested_scope ) rhs = self . children [ 2 ] . real ( nested_scope ) return operation ( lhs , rhs )
def sym ( self , nested_scope = None ) : operation = self . children [ 0 ] . operation ( ) lhs = self . children [ 1 ] . sym ( nested_scope ) rhs = self . children [ 2 ] . sym ( nested_scope ) return operation ( lhs , rhs )
def _process_custom_unitary ( self , node ) : name = node . name if node . arguments is not None : args = self . _process_node ( node . arguments ) else : args = [ ] bits = [ self . _process_bit_id ( node_element ) for node_element in node . bitlist . children ] if name in self . gates : gargs = self . gates [ name ] [ "args" ] gbits = self . gates [ name ] [ "bits" ] maxidx = max ( map ( len , bits ) ) for idx in range ( maxidx ) : self . arg_stack . append ( { gargs [ j ] : args [ j ] for j in range ( len ( gargs ) ) } ) element = [ idx * x for x in [ len ( bits [ j ] ) > 1 for j in range ( len ( bits ) ) ] ] self . bit_stack . append ( { gbits [ j ] : bits [ j ] [ element [ j ] ] for j in range ( len ( gbits ) ) } ) self . _create_dag_op ( name , [ self . arg_stack [ - 1 ] [ s ] . sym ( ) for s in gargs ] , [ self . bit_stack [ - 1 ] [ s ] for s in gbits ] ) self . arg_stack . pop ( ) self . bit_stack . pop ( ) else : raise QiskitError ( "internal error undefined gate:" , "line=%s" % node . line , "file=%s" % node . file )
def _process_cnot ( self , node ) : id0 = self . _process_bit_id ( node . children [ 0 ] ) id1 = self . _process_bit_id ( node . children [ 1 ] ) if not ( len ( id0 ) == len ( id1 ) or len ( id0 ) == 1 or len ( id1 ) == 1 ) : raise QiskitError ( "internal error: qreg size mismatch" , "line=%s" % node . line , "file=%s" % node . file ) maxidx = max ( [ len ( id0 ) , len ( id1 ) ] ) for idx in range ( maxidx ) : if len ( id0 ) > 1 and len ( id1 ) > 1 : self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ idx ] , id1 [ idx ] ] , [ ] , self . condition ) elif len ( id0 ) > 1 : self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ idx ] , id1 [ 0 ] ] , [ ] , self . condition ) else : self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ 0 ] , id1 [ idx ] ] , [ ] , self . condition )
def _process_measure ( self , node ) : id0 = self . _process_bit_id ( node . children [ 0 ] ) id1 = self . _process_bit_id ( node . children [ 1 ] ) if len ( id0 ) != len ( id1 ) : raise QiskitError ( "internal error: reg size mismatch" , "line=%s" % node . line , "file=%s" % node . file ) for idx , idy in zip ( id0 , id1 ) : self . dag . apply_operation_back ( Measure ( ) , [ idx ] , [ idy ] , self . condition )
def _process_if ( self , node ) : creg_name = node . children [ 0 ] . name creg = self . dag . cregs [ creg_name ] cval = node . children [ 1 ] . value self . condition = ( creg , cval ) self . _process_node ( node . children [ 2 ] ) self . condition = None
def qasm ( self , prec = 15 ) : return "measure " + self . children [ 0 ] . qasm ( prec ) + " -> " + self . children [ 1 ] . qasm ( prec ) + ";"
def to_string ( self , indent ) : ind = indent * ' ' print ( ind , 'indexed_id' , self . name , self . index )
def _validate ( instance ) : try : _ = instance . schema . validate ( instance . to_dict ( ) ) except ValidationError as ex : raise ModelValidationError ( ex . messages , ex . field_names , ex . fields , ex . data , * * ex . kwargs )
def _validate_after_init ( init_method ) : @ wraps ( init_method ) def _decorated ( self , * * kwargs ) : try : _ = self . shallow_schema . validate ( kwargs ) except ValidationError as ex : raise ModelValidationError ( ex . messages , ex . field_names , ex . fields , ex . data , * * ex . kwargs ) from None init_method ( self , * * kwargs ) return _decorated
def qft ( circ , q , n ) : for j in range ( n ) : for k in range ( j ) : circ . cu1 ( math . pi / float ( 2 ** ( j - k ) ) , q [ j ] , q [ k ] ) circ . h ( q [ j ] )
def random_unitary_matrix ( dim , seed = None ) : warnings . warn ( 'The random_unitary_matrix() function in qiskit.tools.qi has been ' 'deprecated and will be removed in the future. Instead use ' 'the function in qiskit.quantum_info.random' , DeprecationWarning ) return random . random_unitary ( dim , seed ) . data
def random_density_matrix ( length , rank = None , method = 'Hilbert-Schmidt' , seed = None ) : warnings . warn ( 'The random_density_matrix() function in qiskit.tools.qi has been ' 'deprecated and will be removed in the future. Instead use ' 'the function in qiskit.quantum_info.random' , DeprecationWarning ) return random . random_density_matrix ( length , rank , method , seed )
def u3 ( self , theta , phi , lam , q ) : return self . append ( U3Gate ( theta , phi , lam ) , [ q ] , [ ] )
def check_type ( self , value , attr , data ) : root_value = super ( InstructionParameter , self ) . check_type ( value , attr , data ) if is_collection ( value ) : _ = [ super ( InstructionParameter , self ) . check_type ( item , attr , data ) for item in value ] return root_value
def check_range ( self , j ) : if isinstance ( j , int ) : if j < 0 or j >= self . size : raise QiskitIndexError ( "register index out of range" ) elif isinstance ( j , slice ) : if j . start < 0 or j . stop >= self . size or ( j . step is not None and j . step <= 0 ) : raise QiskitIndexError ( "register index slice out of range" )
def to_string ( self , indent ) : ind = indent * ' ' if self . root : print ( ind , self . type , '---' , self . root ) else : print ( ind , self . type ) indent = indent + 3 ind = indent * ' ' for children in self . children : if children is None : print ( "OOPS! type of parent is" , type ( self ) ) print ( self . children ) if isinstance ( children , str ) : print ( ind , children ) elif isinstance ( children , int ) : print ( ind , str ( children ) ) elif isinstance ( children , float ) : print ( ind , str ( children ) ) else : children . to_string ( indent )
def is_square_matrix ( mat ) : mat = np . array ( mat ) if mat . ndim != 2 : return False shape = mat . shape return shape [ 0 ] == shape [ 1 ]
def is_diagonal_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : if atol is None : atol = ATOL_DEFAULT if rtol is None : rtol = RTOL_DEFAULT mat = np . array ( mat ) if mat . ndim != 2 : return False return np . allclose ( mat , np . diag ( np . diagonal ( mat ) ) , rtol = rtol , atol = atol )
def is_symmetric_matrix ( op , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : if atol is None : atol = ATOL_DEFAULT if rtol is None : rtol = RTOL_DEFAULT mat = np . array ( op ) if mat . ndim != 2 : return False return np . allclose ( mat , mat . T , rtol = rtol , atol = atol )
def is_hermitian_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : if atol is None : atol = ATOL_DEFAULT if rtol is None : rtol = RTOL_DEFAULT mat = np . array ( mat ) if mat . ndim != 2 : return False return np . allclose ( mat , np . conj ( mat . T ) , rtol = rtol , atol = atol )
def is_positive_semidefinite_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : if atol is None : atol = ATOL_DEFAULT if rtol is None : rtol = RTOL_DEFAULT if not is_hermitian_matrix ( mat , rtol = rtol , atol = atol ) : return False vals = np . linalg . eigvalsh ( mat ) for v in vals : if v < - atol : return False return True
def is_identity_matrix ( mat , ignore_phase = False , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : if atol is None : atol = ATOL_DEFAULT if rtol is None : rtol = RTOL_DEFAULT mat = np . array ( mat ) if mat . ndim != 2 : return False if ignore_phase : theta = np . angle ( mat [ 0 , 0 ] ) mat = np . exp ( - 1j * theta ) * mat iden = np . eye ( len ( mat ) ) return np . allclose ( mat , iden , rtol = rtol , atol = atol )
def is_unitary_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : if atol is None : atol = ATOL_DEFAULT if rtol is None : rtol = RTOL_DEFAULT mat = np . array ( mat ) mat = np . conj ( mat . T ) . dot ( mat ) return is_identity_matrix ( mat , ignore_phase = False , rtol = rtol , atol = atol )
def run ( self , dag ) : swaps = dag . op_nodes ( SwapGate ) for swap in swaps : final_successor = [ ] for successor in dag . successors ( swap ) : final_successor . append ( successor . type == 'out' or ( successor . type == 'op' and successor . op . name == 'measure' ) ) if all ( final_successor ) : swap_qargs = swap . qargs measure_layer = DAGCircuit ( ) for qreg in dag . qregs . values ( ) : measure_layer . add_qreg ( qreg ) for creg in dag . cregs . values ( ) : measure_layer . add_creg ( creg ) for successor in dag . successors ( swap ) : if successor . type == 'op' and successor . op . name == 'measure' : dag . remove_op_node ( successor ) old_measure_qarg = successor . qargs [ 0 ] new_measure_qarg = swap_qargs [ swap_qargs . index ( old_measure_qarg ) - 1 ] measure_layer . apply_operation_back ( Measure ( ) , [ new_measure_qarg ] , [ successor . cargs [ 0 ] ] ) dag . extend_back ( measure_layer ) dag . remove_op_node ( swap ) return dag
def _to_choi ( rep , data , input_dim , output_dim ) : if rep == 'Choi' : return data if rep == 'Operator' : return _from_operator ( 'Choi' , data , input_dim , output_dim ) if rep == 'SuperOp' : return _superop_to_choi ( data , input_dim , output_dim ) if rep == 'Kraus' : return _kraus_to_choi ( data , input_dim , output_dim ) if rep == 'Chi' : return _chi_to_choi ( data , input_dim , output_dim ) if rep == 'PTM' : data = _ptm_to_superop ( data , input_dim , output_dim ) return _superop_to_choi ( data , input_dim , output_dim ) if rep == 'Stinespring' : return _stinespring_to_choi ( data , input_dim , output_dim ) raise QiskitError ( 'Invalid QuantumChannel {}' . format ( rep ) )
def _to_superop ( rep , data , input_dim , output_dim ) : if rep == 'SuperOp' : return data if rep == 'Operator' : return _from_operator ( 'SuperOp' , data , input_dim , output_dim ) if rep == 'Choi' : return _choi_to_superop ( data , input_dim , output_dim ) if rep == 'Kraus' : return _kraus_to_superop ( data , input_dim , output_dim ) if rep == 'Chi' : data = _chi_to_choi ( data , input_dim , output_dim ) return _choi_to_superop ( data , input_dim , output_dim ) if rep == 'PTM' : return _ptm_to_superop ( data , input_dim , output_dim ) if rep == 'Stinespring' : return _stinespring_to_superop ( data , input_dim , output_dim ) raise QiskitError ( 'Invalid QuantumChannel {}' . format ( rep ) )
def _to_kraus ( rep , data , input_dim , output_dim ) : if rep == 'Kraus' : return data if rep == 'Stinespring' : return _stinespring_to_kraus ( data , input_dim , output_dim ) if rep == 'Operator' : return _from_operator ( 'Kraus' , data , input_dim , output_dim ) if rep != 'Choi' : data = _to_choi ( rep , data , input_dim , output_dim ) return _choi_to_kraus ( data , input_dim , output_dim )
def _to_chi ( rep , data , input_dim , output_dim ) : if rep == 'Chi' : return data _check_nqubit_dim ( input_dim , output_dim ) if rep == 'Operator' : return _from_operator ( 'Chi' , data , input_dim , output_dim ) if rep != 'Choi' : data = _to_choi ( rep , data , input_dim , output_dim ) return _choi_to_chi ( data , input_dim , output_dim )
def _to_ptm ( rep , data , input_dim , output_dim ) : if rep == 'PTM' : return data _check_nqubit_dim ( input_dim , output_dim ) if rep == 'Operator' : return _from_operator ( 'PTM' , data , input_dim , output_dim ) if rep != 'SuperOp' : data = _to_superop ( rep , data , input_dim , output_dim ) return _superop_to_ptm ( data , input_dim , output_dim )
def _to_stinespring ( rep , data , input_dim , output_dim ) : if rep == 'Stinespring' : return data if rep == 'Operator' : return _from_operator ( 'Stinespring' , data , input_dim , output_dim ) if rep != 'Kraus' : data = _to_kraus ( rep , data , input_dim , output_dim ) return _kraus_to_stinespring ( data , input_dim , output_dim )
def _to_operator ( rep , data , input_dim , output_dim ) : if rep == 'Operator' : return data if rep == 'Stinespring' : return _stinespring_to_operator ( data , input_dim , output_dim ) if rep != 'Kraus' : data = _to_kraus ( rep , data , input_dim , output_dim ) return _kraus_to_operator ( data , input_dim , output_dim )
def _from_operator ( rep , data , input_dim , output_dim ) : if rep == 'Operator' : return data if rep == 'SuperOp' : return np . kron ( np . conj ( data ) , data ) if rep == 'Choi' : vec = np . ravel ( data , order = 'F' ) return np . outer ( vec , np . conj ( vec ) ) if rep == 'Kraus' : return ( [ data ] , None ) if rep == 'Stinespring' : return ( data , None ) if rep == 'Chi' : _check_nqubit_dim ( input_dim , output_dim ) data = _from_operator ( 'Choi' , data , input_dim , output_dim ) return _choi_to_chi ( data , input_dim , output_dim ) if rep == 'PTM' : _check_nqubit_dim ( input_dim , output_dim ) data = _from_operator ( 'SuperOp' , data , input_dim , output_dim ) return _superop_to_ptm ( data , input_dim , output_dim ) raise QiskitError ( 'Invalid QuantumChannel {}' . format ( rep ) )
def _stinespring_to_operator ( data , input_dim , output_dim ) : trace_dim = data [ 0 ] . shape [ 0 ] // output_dim if data [ 1 ] is not None or trace_dim != 1 : raise QiskitError ( 'Channel cannot be converted to Operator representation' ) return data [ 0 ]
def _superop_to_choi ( data , input_dim , output_dim ) : shape = ( output_dim , output_dim , input_dim , input_dim ) return _reshuffle ( data , shape )
def _choi_to_superop ( data , input_dim , output_dim ) : shape = ( input_dim , output_dim , input_dim , output_dim ) return _reshuffle ( data , shape )
def _kraus_to_choi ( data , input_dim , output_dim ) : choi = 0 kraus_l , kraus_r = data if kraus_r is None : for i in kraus_l : vec = i . ravel ( order = 'F' ) choi += np . outer ( vec , vec . conj ( ) ) else : for i , j in zip ( kraus_l , kraus_r ) : choi += np . outer ( i . ravel ( order = 'F' ) , j . ravel ( order = 'F' ) . conj ( ) ) return choi
def _choi_to_kraus ( data , input_dim , output_dim , atol = ATOL_DEFAULT ) : if is_hermitian_matrix ( data , atol = atol ) : w , v = la . eigh ( data ) if len ( w [ w < - atol ] ) == 0 : kraus = [ ] for val , vec in zip ( w , v . T ) : if abs ( val ) > atol : k = np . sqrt ( val ) * vec . reshape ( ( output_dim , input_dim ) , order = 'F' ) kraus . append ( k ) if not kraus : kraus . append ( np . zeros ( ( output_dim , input_dim ) , dtype = complex ) ) return ( kraus , None ) mat_u , svals , mat_vh = la . svd ( data ) kraus_l = [ ] kraus_r = [ ] for val , vec_l , vec_r in zip ( svals , mat_u . T , mat_vh . conj ( ) ) : kraus_l . append ( np . sqrt ( val ) * vec_l . reshape ( ( output_dim , input_dim ) , order = 'F' ) ) kraus_r . append ( np . sqrt ( val ) * vec_r . reshape ( ( output_dim , input_dim ) , order = 'F' ) ) return ( kraus_l , kraus_r )
def _stinespring_to_kraus ( data , input_dim , output_dim ) : kraus_pair = [ ] for stine in data : if stine is None : kraus_pair . append ( None ) else : trace_dim = stine . shape [ 0 ] // output_dim iden = np . eye ( output_dim ) kraus = [ ] for j in range ( trace_dim ) : vec = np . zeros ( trace_dim ) vec [ j ] = 1 kraus . append ( np . kron ( iden , vec [ None , : ] ) . dot ( stine ) ) kraus_pair . append ( kraus ) return tuple ( kraus_pair )
def _stinespring_to_choi ( data , input_dim , output_dim ) : trace_dim = data [ 0 ] . shape [ 0 ] // output_dim stine_l = np . reshape ( data [ 0 ] , ( output_dim , trace_dim , input_dim ) ) if data [ 1 ] is None : stine_r = stine_l else : stine_r = np . reshape ( data [ 1 ] , ( output_dim , trace_dim , input_dim ) ) return np . reshape ( np . einsum ( 'iAj,kAl->jilk' , stine_l , stine_r . conj ( ) ) , 2 * [ input_dim * output_dim ] )
def _kraus_to_stinespring ( data , input_dim , output_dim ) : stine_pair = [ None , None ] for i , kraus in enumerate ( data ) : if kraus is not None : num_kraus = len ( kraus ) stine = np . zeros ( ( output_dim * num_kraus , input_dim ) , dtype = complex ) for j , mat in enumerate ( kraus ) : vec = np . zeros ( num_kraus ) vec [ j ] = 1 stine += np . kron ( mat , vec [ : , None ] ) stine_pair [ i ] = stine return tuple ( stine_pair )
def _kraus_to_superop ( data , input_dim , output_dim ) : kraus_l , kraus_r = data superop = 0 if kraus_r is None : for i in kraus_l : superop += np . kron ( np . conj ( i ) , i ) else : for i , j in zip ( kraus_l , kraus_r ) : superop += np . kron ( np . conj ( j ) , i ) return superop
def _chi_to_choi ( data , input_dim , output_dim ) : num_qubits = int ( np . log2 ( input_dim ) ) return _transform_from_pauli ( data , num_qubits )
def _choi_to_chi ( data , input_dim , output_dim ) : num_qubits = int ( np . log2 ( input_dim ) ) return _transform_to_pauli ( data , num_qubits )
def _reravel ( mat1 , mat2 , shape1 , shape2 ) : left_dims = shape1 [ : 2 ] + shape2 [ : 2 ] right_dims = shape1 [ 2 : ] + shape2 [ 2 : ] tensor_shape = left_dims + right_dims final_shape = ( np . product ( left_dims ) , np . product ( right_dims ) ) data = np . kron ( mat1 , mat2 ) data = np . reshape ( np . transpose ( np . reshape ( data , tensor_shape ) , ( 0 , 2 , 1 , 3 , 4 , 6 , 5 , 7 ) ) , final_shape ) return data
def _transform_from_pauli ( data , num_qubits ) : basis_mat = np . array ( [ [ 1 , 0 , 0 , 1 ] , [ 0 , 1 , 1j , 0 ] , [ 0 , 1 , - 1j , 0 ] , [ 1 , 0j , 0 , - 1 ] ] , dtype = complex ) cob = basis_mat for _ in range ( num_qubits - 1 ) : dim = int ( np . sqrt ( len ( cob ) ) ) cob = np . reshape ( np . transpose ( np . reshape ( np . kron ( basis_mat , cob ) , ( 2 , 2 , dim , dim , 4 , dim * dim ) ) , ( 0 , 2 , 1 , 3 , 4 , 5 ) ) , ( 4 * dim * dim , 4 * dim * dim ) ) return np . dot ( np . dot ( cob , data ) , cob . conj ( ) . T ) / 2 ** num_qubits
def _check_nqubit_dim ( input_dim , output_dim ) : if input_dim != output_dim : raise QiskitError ( 'Not an n-qubit channel: input_dim' + ' ({}) != output_dim ({})' . format ( input_dim , output_dim ) ) num_qubits = int ( np . log2 ( input_dim ) ) if 2 ** num_qubits != input_dim : raise QiskitError ( 'Not an n-qubit channel: input_dim != 2 ** n' )
def _hide_tick_lines_and_labels ( axis ) : for item in axis . get_ticklines ( ) + axis . get_ticklabels ( ) : item . set_visible ( False )
def clear ( self ) : self . points = [ ] self . vectors = [ ] self . point_style = [ ] self . annotations = [ ]
def render ( self , title = '' ) : if self . _rendered : self . axes . clear ( ) self . _rendered = True if not self . _ext_fig : self . fig = plt . figure ( figsize = self . figsize ) if not self . _ext_axes : self . axes = Axes3D ( self . fig , azim = self . view [ 0 ] , elev = self . view [ 1 ] ) if self . background : self . axes . clear ( ) self . axes . set_xlim3d ( - 1.3 , 1.3 ) self . axes . set_ylim3d ( - 1.3 , 1.3 ) self . axes . set_zlim3d ( - 1.3 , 1.3 ) else : self . plot_axes ( ) self . axes . set_axis_off ( ) self . axes . set_xlim3d ( - 0.7 , 0.7 ) self . axes . set_ylim3d ( - 0.7 , 0.7 ) self . axes . set_zlim3d ( - 0.7 , 0.7 ) self . axes . grid ( False ) self . plot_back ( ) self . plot_points ( ) self . plot_vectors ( ) self . plot_front ( ) self . plot_axes_labels ( ) self . plot_annotations ( ) self . axes . set_title ( title , fontsize = self . font_size , y = 1.08 )
def plot_front ( self ) : u_angle = np . linspace ( - np . pi , 0 , 25 ) v_angle = np . linspace ( 0 , np . pi , 25 ) x_dir = np . outer ( np . cos ( u_angle ) , np . sin ( v_angle ) ) y_dir = np . outer ( np . sin ( u_angle ) , np . sin ( v_angle ) ) z_dir = np . outer ( np . ones ( u_angle . shape [ 0 ] ) , np . cos ( v_angle ) ) self . axes . plot_surface ( x_dir , y_dir , z_dir , rstride = 2 , cstride = 2 , color = self . sphere_color , linewidth = 0 , alpha = self . sphere_alpha ) self . axes . plot_wireframe ( x_dir , y_dir , z_dir , rstride = 5 , cstride = 5 , color = self . frame_color , alpha = self . frame_alpha ) self . axes . plot ( 1.0 * np . cos ( u_angle ) , 1.0 * np . sin ( u_angle ) , zs = 0 , zdir = 'z' , lw = self . frame_width , color = self . frame_color ) self . axes . plot ( 1.0 * np . cos ( u_angle ) , 1.0 * np . sin ( u_angle ) , zs = 0 , zdir = 'x' , lw = self . frame_width , color = self . frame_color )
def show ( self , title = '' ) : self . render ( title = title ) if self . fig : plt . show ( self . fig )
def two_qubit_kak ( unitary_matrix , verify_gate_sequence = False ) : warnings . warn ( "two_qubit_kak function is now accessible under " "qiskit.quantum_info.synthesis" , DeprecationWarning ) return synthesis . two_qubit_kak ( unitary_matrix )
def top ( self ) : ret = self . top_format % self . top_connect . center ( self . width , self . top_pad ) if self . right_fill : ret = ret . ljust ( self . right_fill , self . top_pad ) if self . left_fill : ret = ret . rjust ( self . left_fill , self . top_pad ) ret = ret . center ( self . layer_width , self . top_bck ) return ret
def mid ( self ) : ret = self . mid_format % self . mid_content . center ( self . width , self . _mid_padding ) if self . right_fill : ret = ret . ljust ( self . right_fill , self . _mid_padding ) if self . left_fill : ret = ret . rjust ( self . left_fill , self . _mid_padding ) ret = ret . center ( self . layer_width , self . mid_bck ) return ret
def bot ( self ) : ret = self . bot_format % self . bot_connect . center ( self . width , self . bot_pad ) if self . right_fill : ret = ret . ljust ( self . right_fill , self . bot_pad ) if self . left_fill : ret = ret . rjust ( self . left_fill , self . bot_pad ) ret = ret . center ( self . layer_width , self . bot_bck ) return ret
def length ( self ) : return max ( len ( self . top ) , len ( self . mid ) , len ( self . bot ) )
def label_for_box ( instruction ) : label = instruction . name . capitalize ( ) params = TextDrawing . params_for_label ( instruction ) if params : label += "(%s)" % ',' . join ( params ) return label
def latex ( self , prec = 15 , nested_scope = None ) : if not nested_scope : return "\textrm{" + self . name + "}" else : if self . name not in nested_scope [ - 1 ] : raise NodeException ( "Expected local parameter name: " , "name=%s, " % self . name , "line=%s, " % self . line , "file=%s" % self . file ) else : return nested_scope [ - 1 ] [ self . name ] . latex ( prec , nested_scope [ 0 : - 1 ] )
def sym ( self , nested_scope = None ) : if not nested_scope or self . name not in nested_scope [ - 1 ] : raise NodeException ( "Expected local parameter name: " , "name=%s, line=%s, file=%s" % ( self . name , self . line , self . file ) ) else : return nested_scope [ - 1 ] [ self . name ] . sym ( nested_scope [ 0 : - 1 ] )
def real ( self , nested_scope = None ) : if not nested_scope or self . name not in nested_scope [ - 1 ] : raise NodeException ( "Expected local parameter name: " , "name=%s, line=%s, file=%s" % ( self . name , self . line , self . file ) ) else : return nested_scope [ - 1 ] [ self . name ] . real ( nested_scope [ 0 : - 1 ] )
def _attach ( self , instruction , qargs , cargs ) : self . append ( instruction , qargs , cargs )
def _check_dups ( self , qubits ) : squbits = set ( qubits ) if len ( squbits ) != len ( qubits ) : raise QiskitError ( "duplicate qubit arguments" )
def _check_qargs ( self , qargs ) : if not all ( isinstance ( i , tuple ) and isinstance ( i [ 0 ] , QuantumRegister ) and isinstance ( i [ 1 ] , int ) for i in qargs ) : raise QiskitError ( "qarg not (QuantumRegister, int) tuple" ) if not all ( self . has_register ( i [ 0 ] ) for i in qargs ) : raise QiskitError ( "register not in this circuit" ) for qubit in qargs : qubit [ 0 ] . check_range ( qubit [ 1 ] )
def _check_cargs ( self , cargs ) : if not all ( isinstance ( i , tuple ) and isinstance ( i [ 0 ] , ClassicalRegister ) and isinstance ( i [ 1 ] , int ) for i in cargs ) : raise QiskitError ( "carg not (ClassicalRegister, int) tuple" ) if not all ( self . has_register ( i [ 0 ] ) for i in cargs ) : raise QiskitError ( "register not in this circuit" ) for clbit in cargs : clbit [ 0 ] . check_range ( clbit [ 1 ] )
def _check_compatible_regs ( self , rhs ) : list1 = self . qregs + self . cregs list2 = rhs . qregs + rhs . cregs for element1 in list1 : for element2 in list2 : if element2 . name == element1 . name : if element1 != element2 : raise QiskitError ( "circuits are not compatible" )
def qasm ( self ) : string_temp = self . header + "\n" string_temp += self . extension_lib + "\n" for register in self . qregs : string_temp += register . qasm ( ) + "\n" for register in self . cregs : string_temp += register . qasm ( ) + "\n" for instruction , qargs , cargs in self . data : if instruction . name == 'measure' : qubit = qargs [ 0 ] clbit = cargs [ 0 ] string_temp += "%s %s[%d] -> %s[%d];\n" % ( instruction . qasm ( ) , qubit [ 0 ] . name , qubit [ 1 ] , clbit [ 0 ] . name , clbit [ 1 ] ) else : string_temp += "%s %s;\n" % ( instruction . qasm ( ) , "," . join ( [ "%s[%d]" % ( j [ 0 ] . name , j [ 1 ] ) for j in qargs + cargs ] ) ) return string_temp
def _bind_parameter ( self , parameter , value ) : for ( instr , param_index ) in self . _parameter_table [ parameter ] : instr . params [ param_index ] = value
def _score_step ( step ) : return len ( [ g for g in step [ 'gates_mapped' ] if len ( g . qargs ) == 2 ] ) - 3 * step [ 'swaps_added' ]
def _transform_gate_for_layout ( gate , layout ) : mapped_op_node = deepcopy ( [ n for n in gate [ 'graph' ] . nodes ( ) if n . type == 'op' ] [ 0 ] ) device_qreg = QuantumRegister ( len ( layout . get_physical_bits ( ) ) , 'q' ) mapped_qargs = [ ( device_qreg , layout [ a ] ) for a in mapped_op_node . qargs ] mapped_op_node . qargs = mapped_op_node . op . qargs = mapped_qargs mapped_op_node . pop ( 'name' ) return mapped_op_node
def _swap_ops_from_edge ( edge , layout ) : device_qreg = QuantumRegister ( len ( layout . get_physical_bits ( ) ) , 'q' ) qreg_edge = [ ( device_qreg , i ) for i in edge ] return [ DAGNode ( { 'op' : SwapGate ( ) , 'qargs' : qreg_edge , 'cargs' : [ ] , 'type' : 'op' } ) ]
def physical_qubits ( self ) : if self . _qubit_list is None : self . _qubit_list = sorted ( [ pqubit for pqubit in self . graph . nodes ] ) return self . _qubit_list
def cu1 ( self , theta , ctl , tgt ) : return self . append ( Cu1Gate ( theta ) , [ ctl , tgt ] , [ ] )
def inverse ( self ) : for index , instruction in enumerate ( self . instructions ) : self . instructions [ index ] = instruction . inverse ( ) return self
def q_if ( self , * qregs ) : for gate in self . instructions : gate . q_if ( * qregs ) return self
def c_if ( self , classical , val ) : for gate in self . instructions : gate . c_if ( classical , val ) return self
def initialize ( self , params , qubits ) : if isinstance ( qubits , QuantumRegister ) : qubits = qubits [ : ] else : qubits = _convert_to_bits ( [ qubits ] , [ qbit for qreg in self . qregs for qbit in qreg ] ) [ 0 ] return self . append ( Initialize ( params ) , qubits )
def is_virtual ( value ) : return value is None or isinstance ( value , tuple ) and len ( value ) == 2 and isinstance ( value [ 0 ] , Register ) and isinstance ( value [ 1 ] , int )
def copy ( self ) : layout_copy = type ( self ) ( ) layout_copy . _p2v = self . _p2v . copy ( ) layout_copy . _v2p = self . _v2p . copy ( ) return layout_copy
def ccx ( self , ctl1 , ctl2 , tgt ) : return self . append ( ToffoliGate ( ) , [ ctl1 , ctl2 , tgt ] , [ ] )
def u2 ( self , phi , lam , q ) : return self . append ( U2Gate ( phi , lam ) , [ q ] , [ ] )
def to_matrix ( self ) : isqrt2 = 1 / numpy . sqrt ( 2 ) phi , lam = self . params phi , lam = float ( phi ) , float ( lam ) return numpy . array ( [ [ isqrt2 , - numpy . exp ( 1j * lam ) * isqrt2 ] , [ numpy . exp ( 1j * phi ) * isqrt2 , numpy . exp ( 1j * ( phi + lam ) ) * isqrt2 ] ] , dtype = complex )
def is_cptp ( self , atol = None , rtol = None ) : if atol is None : atol = self . _atol if rtol is None : rtol = self . _rtol if self . _data [ 1 ] is not None : return False check = np . dot ( np . transpose ( np . conj ( self . _data [ 0 ] ) ) , self . _data [ 0 ] ) return is_identity_matrix ( check , rtol = self . _rtol , atol = self . _atol )
def conjugate ( self ) : stine_l = np . conjugate ( self . _data [ 0 ] ) stine_r = None if self . _data [ 1 ] is not None : stine_r = np . conjugate ( self . _data [ 1 ] ) return Stinespring ( ( stine_l , stine_r ) , self . input_dims ( ) , self . output_dims ( ) )
def transpose ( self ) : din , dout = self . dim dtr = self . _data [ 0 ] . shape [ 0 ] // dout stine = [ None , None ] for i , mat in enumerate ( self . _data ) : if mat is not None : stine [ i ] = np . reshape ( np . transpose ( np . reshape ( mat , ( dout , dtr , din ) ) , ( 2 , 1 , 0 ) ) , ( din * dtr , dout ) ) return Stinespring ( tuple ( stine ) , input_dims = self . output_dims ( ) , output_dims = self . input_dims ( ) )
def to_operator ( self ) : from qiskit . quantum_info . operators . operator import Operator return Operator ( self . to_matrix ( ) )
def to_instruction ( self ) : from qiskit . circuit import QuantumCircuit , QuantumRegister from qiskit . extensions . standard import IdGate , XGate , YGate , ZGate gates = { 'I' : IdGate ( ) , 'X' : XGate ( ) , 'Y' : YGate ( ) , 'Z' : ZGate ( ) } label = self . to_label ( ) n_qubits = self . numberofqubits qreg = QuantumRegister ( n_qubits ) circuit = QuantumCircuit ( qreg , name = 'Pauli:{}' . format ( label ) ) for i , pauli in enumerate ( reversed ( label ) ) : circuit . append ( gates [ pauli ] , [ qreg [ i ] ] ) return circuit . to_instruction ( )
def _validate_initial_statevector ( self ) : if self . _initial_statevector is None : return length = len ( self . _initial_statevector ) required_dim = 2 ** self . _number_of_qubits if length != required_dim : raise BasicAerError ( 'initial statevector is incorrect length: ' + '{} != {}' . format ( length , required_dim ) )
def _set_options ( self , qobj_config = None , backend_options = None ) : self . _initial_statevector = self . DEFAULT_OPTIONS [ "initial_statevector" ] self . _chop_threshold = self . DEFAULT_OPTIONS [ "chop_threshold" ] if backend_options is None : backend_options = { } if 'initial_statevector' in backend_options : self . _initial_statevector = np . array ( backend_options [ 'initial_statevector' ] , dtype = complex ) elif hasattr ( qobj_config , 'initial_statevector' ) : self . _initial_statevector = np . array ( qobj_config . initial_statevector , dtype = complex ) if self . _initial_statevector is not None : norm = np . linalg . norm ( self . _initial_statevector ) if round ( norm , 12 ) != 1 : raise BasicAerError ( 'initial statevector is not normalized: ' + 'norm {} != 1' . format ( norm ) ) if 'chop_threshold' in backend_options : self . _chop_threshold = backend_options [ 'chop_threshold' ] elif hasattr ( qobj_config , 'chop_threshold' ) : self . _chop_threshold = qobj_config . chop_threshold
def _initialize_statevector ( self ) : if self . _initial_statevector is None : self . _statevector = np . zeros ( 2 ** self . _number_of_qubits , dtype = complex ) self . _statevector [ 0 ] = 1 else : self . _statevector = self . _initial_statevector . copy ( ) self . _statevector = np . reshape ( self . _statevector , self . _number_of_qubits * [ 2 ] )
def _get_statevector ( self ) : vec = np . reshape ( self . _statevector , 2 ** self . _number_of_qubits ) vec = np . stack ( [ vec . real , vec . imag ] , axis = 1 ) vec [ abs ( vec ) < self . _chop_threshold ] = 0.0 return vec
def _validate ( self , qobj ) : n_qubits = qobj . config . n_qubits max_qubits = self . configuration ( ) . n_qubits if n_qubits > max_qubits : raise BasicAerError ( 'Number of qubits {} ' . format ( n_qubits ) + 'is greater than maximum ({}) ' . format ( max_qubits ) + 'for "{}".' . format ( self . name ( ) ) ) for experiment in qobj . experiments : name = experiment . header . name if experiment . config . memory_slots == 0 : logger . warning ( 'No classical registers in circuit "%s", ' 'counts will be empty.' , name ) elif 'measure' not in [ op . name for op in experiment . instructions ] : logger . warning ( 'No measurements in circuit "%s", ' 'classical register will remain all zeros.' , name )
def _validate_initial_unitary ( self ) : if self . _initial_unitary is None : return shape = np . shape ( self . _initial_unitary ) required_shape = ( 2 ** self . _number_of_qubits , 2 ** self . _number_of_qubits ) if shape != required_shape : raise BasicAerError ( 'initial unitary is incorrect shape: ' + '{} != 2 ** {}' . format ( shape , required_shape ) )
def _set_options ( self , qobj_config = None , backend_options = None ) : self . _initial_unitary = self . DEFAULT_OPTIONS [ "initial_unitary" ] self . _chop_threshold = self . DEFAULT_OPTIONS [ "chop_threshold" ] if backend_options is None : backend_options = { } if 'initial_unitary' in backend_options : self . _initial_unitary = np . array ( backend_options [ 'initial_unitary' ] , dtype = complex ) elif hasattr ( qobj_config , 'initial_unitary' ) : self . _initial_unitary = np . array ( qobj_config . initial_unitary , dtype = complex ) if self . _initial_unitary is not None : shape = np . shape ( self . _initial_unitary ) if len ( shape ) != 2 or shape [ 0 ] != shape [ 1 ] : raise BasicAerError ( "initial unitary is not a square matrix" ) iden = np . eye ( len ( self . _initial_unitary ) ) u_dagger_u = np . dot ( self . _initial_unitary . T . conj ( ) , self . _initial_unitary ) norm = np . linalg . norm ( u_dagger_u - iden ) if round ( norm , 10 ) != 0 : raise BasicAerError ( "initial unitary is not unitary" ) if 'chop_threshold' in backend_options : self . _chop_threshold = backend_options [ 'chop_threshold' ] elif hasattr ( qobj_config , 'chop_threshold' ) : self . _chop_threshold = qobj_config . chop_threshold
def _initialize_unitary ( self ) : self . _validate_initial_unitary ( ) if self . _initial_unitary is None : self . _unitary = np . eye ( 2 ** self . _number_of_qubits , dtype = complex ) else : self . _unitary = self . _initial_unitary . copy ( ) self . _unitary = np . reshape ( self . _unitary , self . _number_of_qubits * [ 2 , 2 ] )
def _get_unitary ( self ) : unitary = np . reshape ( self . _unitary , 2 * [ 2 ** self . _number_of_qubits ] ) unitary = np . stack ( ( unitary . real , unitary . imag ) , axis = - 1 ) unitary [ abs ( unitary ) < self . _chop_threshold ] = 0.0 return unitary
def _is_bit ( obj ) : if isinstance ( obj , tuple ) and len ( obj ) == 2 : if isinstance ( obj [ 0 ] , Register ) and isinstance ( obj [ 1 ] , int ) and obj [ 1 ] < len ( obj [ 0 ] ) : return True return False
def to_matrix ( self ) : lam = self . params [ 0 ] lam = float ( lam ) return numpy . array ( [ [ 1 , 0 ] , [ 0 , numpy . exp ( 1j * lam ) ] ] , dtype = complex )
def real ( self , nested_scope = None ) : op = self . children [ 0 ] . name expr = self . children [ 1 ] dispatch = { 'sin' : sympy . sin , 'cos' : sympy . cos , 'tan' : sympy . tan , 'asin' : sympy . asin , 'acos' : sympy . acos , 'atan' : sympy . atan , 'exp' : sympy . exp , 'ln' : sympy . log , 'sqrt' : sympy . sqrt } if op in dispatch : arg = expr . real ( nested_scope ) return dispatch [ op ] ( arg ) else : raise NodeException ( "internal error: undefined external" )
def rzz ( self , theta , qubit1 , qubit2 ) : return self . append ( RZZGate ( theta ) , [ qubit1 , qubit2 ] , [ ] )
def cswap ( self , ctl , tgt1 , tgt2 ) : return self . append ( FredkinGate ( ) , [ ctl , tgt1 , tgt2 ] , [ ] )
def _select_best_remaining_cx ( self ) : candidates = [ ] for gate in self . gate_list : chk1 = gate [ 0 ] in self . available_hw_qubits chk2 = gate [ 1 ] in self . available_hw_qubits if chk1 and chk2 : candidates . append ( gate ) best_reliab = 0 best_item = None for item in candidates : if self . gate_cost [ item ] > best_reliab : best_reliab = self . gate_cost [ item ] best_item = item return best_item
def _select_best_remaining_qubit ( self , prog_qubit ) : reliab_store = { } for hw_qubit in self . available_hw_qubits : reliab = 1 for n in self . prog_graph . neighbors ( prog_qubit ) : if n in self . prog2hw : reliab *= self . swap_costs [ self . prog2hw [ n ] ] [ hw_qubit ] reliab *= self . readout_errors [ hw_qubit ] reliab_store [ hw_qubit ] = reliab max_reliab = 0 best_hw_qubit = None for hw_qubit in reliab_store : if reliab_store [ hw_qubit ] > max_reliab : max_reliab = reliab_store [ hw_qubit ] best_hw_qubit = hw_qubit return best_hw_qubit
def run ( self , dag ) : self . _initialize_backend_prop ( ) num_qubits = self . _create_program_graph ( dag ) if num_qubits > len ( self . swap_graph ) : raise TranspilerError ( 'Number of qubits greater than device.' ) for end1 , end2 , _ in sorted ( self . prog_graph . edges ( data = True ) , key = lambda x : x [ 2 ] [ 'weight' ] , reverse = True ) : self . pending_program_edges . append ( ( end1 , end2 ) ) while self . pending_program_edges : edge = self . _select_next_edge ( ) q1_mapped = edge [ 0 ] in self . prog2hw q2_mapped = edge [ 1 ] in self . prog2hw if ( not q1_mapped ) and ( not q2_mapped ) : best_hw_edge = self . _select_best_remaining_cx ( ) self . prog2hw [ edge [ 0 ] ] = best_hw_edge [ 0 ] self . prog2hw [ edge [ 1 ] ] = best_hw_edge [ 1 ] self . available_hw_qubits . remove ( best_hw_edge [ 0 ] ) self . available_hw_qubits . remove ( best_hw_edge [ 1 ] ) elif not q1_mapped : best_hw_qubit = self . _select_best_remaining_qubit ( edge [ 0 ] ) self . prog2hw [ edge [ 0 ] ] = best_hw_qubit self . available_hw_qubits . remove ( best_hw_qubit ) else : best_hw_qubit = self . _select_best_remaining_qubit ( edge [ 1 ] ) self . prog2hw [ edge [ 1 ] ] = best_hw_qubit self . available_hw_qubits . remove ( best_hw_qubit ) new_edges = [ x for x in self . pending_program_edges if not ( x [ 0 ] in self . prog2hw and x [ 1 ] in self . prog2hw ) ] self . pending_program_edges = new_edges for qid in self . qarg_to_id . values ( ) : if qid not in self . prog2hw : self . prog2hw [ qid ] = self . available_hw_qubits [ 0 ] self . available_hw_qubits . remove ( self . prog2hw [ qid ] ) layout = Layout ( ) for q in dag . qubits ( ) : pid = self . _qarg_to_id ( q ) hwid = self . prog2hw [ pid ] layout [ ( q [ 0 ] , q [ 1 ] ) ] = hwid self . property_set [ 'layout' ] = layout
def inverse ( self ) : self . data = [ gate . inverse ( ) for gate in reversed ( self . data ) ] self . inverse_flag = not self . inverse_flag return self
def q_if ( self , * qregs ) : self . data = [ gate . q_if ( qregs ) for gate in self . data ] return self
def c_if ( self , classical , val ) : self . data = [ gate . c_if ( classical , val ) for gate in self . data ] return self
def is_unitary ( self , atol = None , rtol = None ) : if atol is None : atol = self . _atol if rtol is None : rtol = self . _rtol return is_unitary_matrix ( self . _data , rtol = rtol , atol = atol )
def conjugate ( self ) : return Operator ( np . conj ( self . data ) , self . input_dims ( ) , self . output_dims ( ) )
def transpose ( self ) : return Operator ( np . transpose ( self . data ) , self . input_dims ( ) , self . output_dims ( ) )
def _shape ( self ) : return tuple ( reversed ( self . output_dims ( ) ) ) + tuple ( reversed ( self . input_dims ( ) ) )
def _format_state ( self , state ) : state = np . array ( state ) shape = state . shape ndim = state . ndim if ndim > 2 : raise QiskitError ( 'Input state is not a vector or matrix.' ) if ndim == 2 : if shape [ 1 ] != 1 and shape [ 1 ] != shape [ 0 ] : raise QiskitError ( 'Input state is not a vector or matrix.' ) if shape [ 1 ] == 1 : state = np . reshape ( state , shape [ 0 ] ) return state
def _instruction_to_operator ( cls , instruction ) : if isinstance ( instruction , QuantumCircuit ) : instruction = instruction . to_instruction ( ) op = Operator ( np . eye ( 2 ** instruction . num_qubits ) ) op . _append_instruction ( instruction ) return op
def _append_instruction ( self , obj , qargs = None ) : if isinstance ( obj , Instruction ) : mat = None if hasattr ( obj , 'to_matrix' ) : try : mat = obj . to_matrix ( ) except QiskitError : pass if mat is not None : op = self . compose ( mat , qargs = qargs ) self . _data = op . data else : if obj . definition is None : raise QiskitError ( 'Cannot apply Instruction: {}' . format ( obj . name ) ) for instr , qregs , cregs in obj . definition : if cregs : raise QiskitError ( 'Cannot apply instruction with classical registers: {}' . format ( instr . name ) ) new_qargs = [ tup [ 1 ] for tup in qregs ] self . _append_instruction ( instr , qargs = new_qargs ) else : raise QiskitError ( 'Input is not an instruction.' )
def real ( self , nested_scope = None ) : operation = self . children [ 0 ] . operation ( ) expr = self . children [ 1 ] . real ( nested_scope ) return operation ( expr )
def sym ( self , nested_scope = None ) : operation = self . children [ 0 ] . operation ( ) expr = self . children [ 1 ] . sym ( nested_scope ) return operation ( expr )
def _separate_bitstring ( bitstring , creg_sizes ) : substrings = [ ] running_index = 0 for _ , size in reversed ( creg_sizes ) : substrings . append ( bitstring [ running_index : running_index + size ] ) running_index += size return ' ' . join ( substrings )
def bit_string_index ( text ) : n = len ( text ) k = text . count ( "1" ) if text . count ( "0" ) != n - k : raise VisualizationError ( "s must be a string of 0 and 1" ) ones = [ pos for pos , char in enumerate ( text ) if char == "1" ] return lex_index ( n , k , ones )
def bit_string_index ( s ) : n = len ( s ) k = s . count ( "1" ) if s . count ( "0" ) != n - k : raise VisualizationError ( "s must be a string of 0 and 1" ) ones = [ pos for pos , char in enumerate ( s ) if char == "1" ] return lex_index ( n , k , ones )
def op ( self ) : if 'type' not in self . data_dict or self . data_dict [ 'type' ] != 'op' : raise QiskitError ( "The node %s is not an op node" % ( str ( self ) ) ) return self . data_dict . get ( 'op' )
def run ( self , dag ) : diagonal_1q_gates = ( RZGate , ZGate , TGate , SGate , TdgGate , SdgGate , U1Gate ) diagonal_2q_gates = ( CzGate , CrzGate , Cu1Gate , RZZGate ) nodes_to_remove = set ( ) for measure in dag . op_nodes ( Measure ) : predecessor = dag . quantum_predecessors ( measure ) [ 0 ] if predecessor . type == 'op' and isinstance ( predecessor . op , diagonal_1q_gates ) : nodes_to_remove . add ( predecessor ) if predecessor . type == 'op' and isinstance ( predecessor . op , diagonal_2q_gates ) : successors = dag . quantum_successors ( predecessor ) if all ( [ s . type == 'op' and isinstance ( s . op , Measure ) for s in successors ] ) : nodes_to_remove . add ( predecessor ) for node_to_remove in nodes_to_remove : dag . remove_op_node ( node_to_remove ) return dag
def to_string ( self , indent ) : ind = indent * ' ' print ( ind , 'qreg' ) self . children [ 0 ] . to_string ( indent + 3 )
def remove_all_ops_named ( self , opname ) : for n in self . named_nodes ( opname ) : self . remove_op_node ( n )
def add_qreg ( self , qreg ) : if not isinstance ( qreg , QuantumRegister ) : raise DAGCircuitError ( "not a QuantumRegister instance." ) if qreg . name in self . qregs : raise DAGCircuitError ( "duplicate register %s" % qreg . name ) self . qregs [ qreg . name ] = qreg for j in range ( qreg . size ) : self . _add_wire ( ( qreg , j ) )
def add_creg ( self , creg ) : if not isinstance ( creg , ClassicalRegister ) : raise DAGCircuitError ( "not a ClassicalRegister instance." ) if creg . name in self . cregs : raise DAGCircuitError ( "duplicate register %s" % creg . name ) self . cregs [ creg . name ] = creg for j in range ( creg . size ) : self . _add_wire ( ( creg , j ) )
def extend_back ( self , dag , edge_map = None ) : edge_map = edge_map or { } for qreg in dag . qregs . values ( ) : if qreg . name not in self . qregs : self . add_qreg ( QuantumRegister ( qreg . size , qreg . name ) ) edge_map . update ( [ ( qbit , qbit ) for qbit in qreg if qbit not in edge_map ] ) for creg in dag . cregs . values ( ) : if creg . name not in self . cregs : self . add_creg ( ClassicalRegister ( creg . size , creg . name ) ) edge_map . update ( [ ( cbit , cbit ) for cbit in creg if cbit not in edge_map ] ) self . compose_back ( dag , edge_map )
def named_nodes ( self , * names ) : named_nodes = [ ] for node in self . _multi_graph . nodes ( ) : if node . type == 'op' and node . op . name in names : named_nodes . append ( node ) return named_nodes
def twoQ_gates ( self ) : two_q_gates = [ ] for node in self . gate_nodes ( ) : if len ( node . qargs ) == 2 : two_q_gates . append ( node ) return two_q_gates
def predecessors ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling predecessors() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2 ) node = self . _id_to_node [ node ] return self . _multi_graph . predecessors ( node )
def ancestors ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling ancestors() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2 ) node = self . _id_to_node [ node ] return nx . ancestors ( self . _multi_graph , node )
def remove_ancestors_of ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling remove_ancestors_of() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2 ) node = self . _id_to_node [ node ] anc = nx . ancestors ( self . _multi_graph , node ) for anc_node in anc : if anc_node . type == "op" : self . remove_op_node ( anc_node )
def remove_descendants_of ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling remove_descendants_of() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2 ) node = self . _id_to_node [ node ] desc = nx . descendants ( self . _multi_graph , node ) for desc_node in desc : if desc_node . type == "op" : self . remove_op_node ( desc_node )
def remove_nonancestors_of ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling remove_nonancestors_of() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2 ) node = self . _id_to_node [ node ] anc = nx . ancestors ( self . _multi_graph , node ) comp = list ( set ( self . _multi_graph . nodes ( ) ) - set ( anc ) ) for n in comp : if n . type == "op" : self . remove_op_node ( n )
def remove_nondescendants_of ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling remove_nondescendants_of() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2 ) node = self . _id_to_node [ node ] dec = nx . descendants ( self . _multi_graph , node ) comp = list ( set ( self . _multi_graph . nodes ( ) ) - set ( dec ) ) for n in comp : if n . type == "op" : self . remove_op_node ( n )
def multigraph_layers ( self ) : predecessor_count = dict ( ) cur_layer = [ node for node in self . input_map . values ( ) ] yield cur_layer next_layer = [ ] while cur_layer : for node in cur_layer : for successor in self . _multi_graph . successors ( node ) : multiplicity = self . _multi_graph . number_of_edges ( node , successor ) if successor in predecessor_count : predecessor_count [ successor ] -= multiplicity else : predecessor_count [ successor ] = self . _multi_graph . in_degree ( successor ) - multiplicity if predecessor_count [ successor ] == 0 : next_layer . append ( successor ) del predecessor_count [ successor ] yield next_layer cur_layer = next_layer next_layer = [ ]
def properties ( self ) : summary = { "size" : self . size ( ) , "depth" : self . depth ( ) , "width" : self . width ( ) , "bits" : self . num_cbits ( ) , "factors" : self . num_tensor_factors ( ) , "operations" : self . count_ops ( ) } return summary
def __pauli_prep_gates ( circuit , qreg , op ) : bas , proj = op if bas not in [ 'X' , 'Y' , 'Z' ] : raise QiskitError ( "There's no X, Y or Z basis for this Pauli " "preparation" ) if bas == "X" : if proj == 1 : circuit . u2 ( np . pi , np . pi , qreg ) else : circuit . u2 ( 0. , np . pi , qreg ) elif bas == "Y" : if proj == 1 : circuit . u2 ( - 0.5 * np . pi , np . pi , qreg ) else : circuit . u2 ( 0.5 * np . pi , np . pi , qreg ) elif bas == "Z" and proj == 1 : circuit . u3 ( np . pi , 0. , np . pi , qreg )
def __pauli_meas_gates ( circuit , qreg , op ) : if op not in [ 'X' , 'Y' , 'Z' ] : raise QiskitError ( "There's no X, Y or Z basis for this Pauli " "measurement" ) if op == "X" : circuit . u2 ( 0. , np . pi , qreg ) elif op == "Y" : circuit . u2 ( 0. , 0.5 * np . pi , qreg )
def __sic_prep_gates ( circuit , qreg , op ) : bas , proj = op if bas != 'S' : raise QiskitError ( 'Not in SIC basis!' ) theta = - 2 * np . arctan ( np . sqrt ( 2 ) ) if proj == 1 : circuit . u3 ( theta , np . pi , 0.0 , qreg ) elif proj == 2 : circuit . u3 ( theta , np . pi / 3 , 0.0 , qreg ) elif proj == 3 : circuit . u3 ( theta , - np . pi / 3 , 0.0 , qreg )
def __projector ( op_list , basis ) : ret = 1 for op in op_list : label , eigenstate = op ret = np . kron ( basis [ label ] [ eigenstate ] , ret ) return ret
def run ( self , dag ) : resets = dag . op_nodes ( Reset ) for reset in resets : predecessor = next ( dag . predecessors ( reset ) ) if predecessor . type == 'in' : dag . remove_op_node ( reset ) return dag
def cu3 ( self , theta , phi , lam , ctl , tgt ) : return self . append ( Cu3Gate ( theta , phi , lam ) , [ ctl , tgt ] , [ ] )
def build_bell_circuit ( ) : q = QuantumRegister ( 2 ) c = ClassicalRegister ( 2 ) qc = QuantumCircuit ( q , c ) qc . h ( q [ 0 ] ) qc . cx ( q [ 0 ] , q [ 1 ] ) qc . measure ( q , c ) return qc
def drive ( self ) -> DriveChannel : if self . _drives : return self . _drives [ 0 ] else : raise PulseError ( "No drive channels in q[%d]" % self . _index )
def control ( self ) -> ControlChannel : if self . _controls : return self . _controls [ 0 ] else : raise PulseError ( "No control channels in q[%d]" % self . _index )
def measure ( self ) -> MeasureChannel : if self . _measures : return self . _measures [ 0 ] else : raise PulseError ( "No measurement channels in q[%d]" % self . _index )
def acquire ( self ) -> AcquireChannel : if self . _acquires : return self . _acquires [ 0 ] else : raise PulseError ( "No acquire channels in q[%d]" % self . _index )
def input_state ( circ , q , n ) : for j in range ( n ) : circ . h ( q [ j ] ) circ . u1 ( math . pi / float ( 2 ** ( j ) ) , q [ j ] ) . inverse ( )
def unset_qiskit_logger ( ) : qiskit_logger = logging . getLogger ( 'qiskit' ) for handler in qiskit_logger . handlers : qiskit_logger . removeHandler ( handler )
def input ( self , data ) : self . data = data self . lexer . input ( data )
def pop ( self ) : self . lexer = self . stack . pop ( ) self . filename = self . lexer . qasm_file self . lineno = self . lexer . qasm_line
def push ( self , filename ) : self . lexer . qasm_file = self . filename self . lexer . qasm_line = self . lineno self . stack . append ( self . lexer ) self . __mklexer__ ( filename )
def get_bound_method ( self , instruction ) : try : return self . _bound_instructions [ type ( instruction ) ] except KeyError : raise PulseError ( 'Qobj conversion method for %s is not found.' % instruction )
def verify_declared_bit ( self , obj ) : if obj . name not in self . current_symtab : raise QasmError ( "Cannot find symbol '" + obj . name + "' in argument list for gate, line" , str ( obj . line ) , 'file' , obj . file ) sym = self . current_symtab [ obj . name ] if not ( sym . type == 'id' and sym . is_bit ) : raise QasmError ( "Bit" , obj . name , 'is not declared as a bit in the gate.' )
def verify_exp_list ( self , obj ) : # if obj . children is not None : for children in obj . children : if isinstance ( children , node . Id ) : if children . name in self . external_functions : continue if children . name not in self . current_symtab : raise QasmError ( "Argument '" + children . name + "' in expression cannot be " + "found, line" , str ( children . line ) , "file" , children . file ) else : if hasattr ( children , "children" ) : self . verify_exp_list ( children )
def verify_as_gate ( self , obj , bitlist , arglist = None ) : if obj . name not in self . global_symtab : raise QasmError ( "Cannot find gate definition for '" + obj . name + "', line" , str ( obj . line ) , 'file' , obj . file ) g_sym = self . global_symtab [ obj . name ] if not ( g_sym . type == 'gate' or g_sym . type == 'opaque' ) : raise QasmError ( "'" + obj . name + "' is used as a gate " + "or opaque call but the symbol is neither;" + " it is a '" + g_sym . type + "' line" , str ( obj . line ) , 'file' , obj . file ) if g_sym . n_bits ( ) != bitlist . size ( ) : raise QasmError ( "Gate or opaque call to '" + obj . name + "' uses" , str ( bitlist . size ( ) ) , "qubits but is declared for" , str ( g_sym . n_bits ( ) ) , "qubits" , "line" , str ( obj . line ) , 'file' , obj . file ) if arglist : if g_sym . n_args ( ) != arglist . size ( ) : raise QasmError ( "Gate or opaque call to '" + obj . name + "' uses" , str ( arglist . size ( ) ) , "qubits but is declared for" , str ( g_sym . n_args ( ) ) , "qubits" , "line" , str ( obj . line ) , 'file' , obj . file ) else : if g_sym . n_args ( ) > 0 : raise QasmError ( "Gate or opaque call to '" + obj . name + "' has no arguments but is declared for" , str ( g_sym . n_args ( ) ) , "qubits" , "line" , str ( obj . line ) , 'file' , obj . file )
def verify_reg ( self , obj , object_type ) : if obj . name not in self . global_symtab : raise QasmError ( 'Cannot find definition for' , object_type , "'" + obj . name + "'" , 'at line' , str ( obj . line ) , 'file' , obj . file ) g_sym = self . global_symtab [ obj . name ] if g_sym . type != object_type : raise QasmError ( "Type for '" + g_sym . name + "' should be '" + object_type + "' but was found to be '" + g_sym . type + "'" , "line" , str ( obj . line ) , "file" , obj . file ) if obj . type == 'indexed_id' : bound = g_sym . index ndx = obj . index if ndx < 0 or ndx >= bound : raise QasmError ( "Register index for '" + g_sym . name + "' out of bounds. Index is" , str ( ndx ) , "bound is 0 <= index <" , str ( bound ) , "at line" , str ( obj . line ) , "file" , obj . file )
def verify_reg_list ( self , obj , object_type ) : for children in obj . children : self . verify_reg ( children , object_type )
def get_tokens ( self ) : try : while True : token = self . lexer . token ( ) if not token : break yield token except QasmError as e : print ( 'Exception tokenizing qasm file:' , e . msg )
def parse_debug ( self , val ) : if val is True : self . parse_deb = True elif val is False : self . parse_deb = False else : raise QasmError ( "Illegal debug value '" + str ( val ) + "' must be True or False." )
def parse ( self , data ) : self . parser . parse ( data , lexer = self . lexer , debug = self . parse_deb ) if self . qasm is None : raise QasmError ( "Uncaught exception in parser; " + "see previous messages for details." ) return self . qasm
def get_tokens ( self ) : if self . _filename : with open ( self . _filename ) as ifile : self . _data = ifile . read ( ) with QasmParser ( self . _filename ) as qasm_p : return qasm_p . get_tokens ( )
def parse ( self ) : if self . _filename : with open ( self . _filename ) as ifile : self . _data = ifile . read ( ) with QasmParser ( self . _filename ) as qasm_p : qasm_p . parse_debug ( False ) return qasm_p . parse ( self . _data )
def crz ( self , theta , ctl , tgt ) : return self . append ( CrzGate ( theta ) , [ ctl , tgt ] , [ ] )
def qasm ( self , prec = 15 ) : string = "gate " + self . name if self . arguments is not None : string += "(" + self . arguments . qasm ( prec ) + ")" string += " " + self . bitlist . qasm ( prec ) + "\n" string += "{\n" + self . body . qasm ( prec ) + "}" return string
def backend_widget ( backend ) : config = backend . configuration ( ) . to_dict ( ) props = backend . properties ( ) . to_dict ( ) name = widgets . HTML ( value = "<h4>{name}</h4>" . format ( name = backend . name ( ) ) , layout = widgets . Layout ( ) ) n_qubits = config [ 'n_qubits' ] qubit_count = widgets . HTML ( value = "<h5><b>{qubits}</b></h5>" . format ( qubits = n_qubits ) , layout = widgets . Layout ( justify_content = 'center' ) ) cmap = widgets . Output ( layout = widgets . Layout ( min_width = '250px' , max_width = '250px' , max_height = '250px' , min_height = '250px' , justify_content = 'center' , align_items = 'center' , margin = '0px 0px 0px 0px' ) ) with cmap : _cmap_fig = plot_gate_map ( backend , plot_directed = False , label_qubits = False ) if _cmap_fig is not None : display ( _cmap_fig ) plt . close ( _cmap_fig ) pending = generate_jobs_pending_widget ( ) is_oper = widgets . HTML ( value = "<h5></h5>" , layout = widgets . Layout ( justify_content = 'center' ) ) least_busy = widgets . HTML ( value = "<h5></h5>" , layout = widgets . Layout ( justify_content = 'center' ) ) t1_units = props [ 'qubits' ] [ 0 ] [ 0 ] [ 'unit' ] avg_t1 = round ( sum ( [ q [ 0 ] [ 'value' ] for q in props [ 'qubits' ] ] ) / n_qubits , 1 ) t1_widget = widgets . HTML ( value = "<h5>{t1} {units}</h5>" . format ( t1 = avg_t1 , units = t1_units ) , layout = widgets . Layout ( ) ) t2_units = props [ 'qubits' ] [ 0 ] [ 1 ] [ 'unit' ] avg_t2 = round ( sum ( [ q [ 1 ] [ 'value' ] for q in props [ 'qubits' ] ] ) / n_qubits , 1 ) t2_widget = widgets . HTML ( value = "<h5>{t2} {units}</h5>" . format ( t2 = avg_t2 , units = t2_units ) , layout = widgets . Layout ( ) ) out = widgets . VBox ( [ name , cmap , qubit_count , pending , least_busy , is_oper , t1_widget , t2_widget ] , layout = widgets . Layout ( display = 'inline-flex' , flex_flow = 'column' , align_items = 'center' ) ) out . _is_alive = True return out
def generate_jobs_pending_widget ( ) : pbar = widgets . IntProgress ( value = 0 , min = 0 , max = 50 , description = '' , orientation = 'horizontal' , layout = widgets . Layout ( max_width = '180px' ) ) pbar . style . bar_color = '#71cddd' pbar_current = widgets . Label ( value = str ( pbar . value ) , layout = widgets . Layout ( min_width = 'auto' ) ) pbar_max = widgets . Label ( value = str ( pbar . max ) , layout = widgets . Layout ( min_width = 'auto' ) ) def _on_max_change ( change ) : pbar_max . value = str ( change [ 'new' ] ) def _on_val_change ( change ) : pbar_current . value = str ( change [ 'new' ] ) pbar . observe ( _on_max_change , names = 'max' ) pbar . observe ( _on_val_change , names = 'value' ) jobs_widget = widgets . HBox ( [ pbar_current , pbar , pbar_max ] , layout = widgets . Layout ( max_width = '250px' , min_width = '250px' , justify_content = 'center' ) ) return jobs_widget
def _bipartite_shape ( self ) : return ( self . _input_dim , self . _output_dim , self . _input_dim , self . _output_dim )
def conjugate ( self ) : return Choi ( np . conj ( self . _data ) , self . input_dims ( ) , self . output_dims ( ) )
def transpose ( self ) : d_in , d_out = self . dim data = np . reshape ( self . _data , ( d_in , d_out , d_in , d_out ) ) data = np . transpose ( data , ( 1 , 0 , 3 , 2 ) ) data = np . reshape ( data , ( d_in * d_out , d_in * d_out ) ) return Choi ( data , input_dims = self . output_dims ( ) , output_dims = self . input_dims ( ) )
def _load_schemas_and_validators ( ) : schema_base_path = os . path . join ( os . path . dirname ( __file__ ) , '../..' ) for name , path in _DEFAULT_SCHEMA_PATHS . items ( ) : _load_schema ( os . path . join ( schema_base_path , path ) , name ) _get_validator ( name )
def qasm ( self , prec = 15 ) : return "," . join ( [ self . children [ j ] . qasm ( prec ) for j in range ( self . size ( ) ) ] )
def qasm ( self , prec = 15 ) : string = "" for children in self . children : string += "  " + children . qasm ( prec ) + "\n" return string
def calls ( self ) : lst = [ ] for children in self . children : if children . type == "custom_unitary" : lst . append ( children . name ) return lst
def qasm ( self , prec = 15 ) : if self . value == pi : return "pi" return ccode ( self . value , precision = prec )
def conjugate ( self ) : return SuperOp ( np . conj ( self . _data ) , self . input_dims ( ) , self . output_dims ( ) )
def transpose ( self ) : return SuperOp ( np . transpose ( self . _data ) , input_dims = self . output_dims ( ) , output_dims = self . input_dims ( ) )
def _compose_subsystem ( self , other , qargs , front = False ) : input_dims = list ( self . input_dims ( ) ) output_dims = list ( self . output_dims ( ) ) if front : num_indices = len ( self . input_dims ( ) ) shift = 2 * len ( self . output_dims ( ) ) right_mul = True for pos , qubit in enumerate ( qargs ) : input_dims [ qubit ] = other . _input_dims [ pos ] else : num_indices = len ( self . output_dims ( ) ) shift = 0 right_mul = False for pos , qubit in enumerate ( qargs ) : output_dims [ qubit ] = other . _output_dims [ pos ] tensor = np . reshape ( self . data , self . _shape ) mat = np . reshape ( other . data , other . _shape ) indices = [ 2 * num_indices - 1 - qubit for qubit in qargs ] + [ num_indices - 1 - qubit for qubit in qargs ] final_shape = [ np . product ( output_dims ) ** 2 , np . product ( input_dims ) ** 2 ] data = np . reshape ( self . _einsum_matmul ( tensor , mat , indices , shift , right_mul ) , final_shape ) return SuperOp ( data , input_dims , output_dims )
def _instruction_to_superop ( cls , instruction ) : if isinstance ( instruction , QuantumCircuit ) : instruction = instruction . to_instruction ( ) op = SuperOp ( np . eye ( 4 ** instruction . num_qubits ) ) op . _append_instruction ( instruction ) return op
def _append_instruction ( self , obj , qargs = None ) : if isinstance ( obj , Instruction ) : chan = None if obj . name == 'reset' : chan = SuperOp ( np . array ( [ [ 1 , 0 , 0 , 1 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] ] ) ) if obj . name == 'kraus' : kraus = obj . params dim = len ( kraus [ 0 ] ) chan = SuperOp ( _to_superop ( 'Kraus' , ( kraus , None ) , dim , dim ) ) elif hasattr ( obj , 'to_matrix' ) : try : kraus = [ obj . to_matrix ( ) ] dim = len ( kraus [ 0 ] ) chan = SuperOp ( _to_superop ( 'Kraus' , ( kraus , None ) , dim , dim ) ) except QiskitError : pass if chan is not None : op = self . compose ( chan , qargs = qargs ) self . _data = op . data else : if obj . definition is None : raise QiskitError ( 'Cannot apply Instruction: {}' . format ( obj . name ) ) for instr , qregs , cregs in obj . definition : if cregs : raise QiskitError ( 'Cannot apply instruction with classical registers: {}' . format ( instr . name ) ) new_qargs = [ tup [ 1 ] for tup in qregs ] self . _append_instruction ( instr , qargs = new_qargs ) else : raise QiskitError ( 'Input is not an instruction.' )
def run ( self , dag ) : final_op_types = [ 'measure' , 'barrier' ] final_ops = [ ] for candidate_node in dag . named_nodes ( * final_op_types ) : is_final_op = True for _ , child_successors in dag . bfs_successors ( candidate_node ) : if any ( suc . type == 'op' and suc . name not in final_op_types for suc in child_successors ) : is_final_op = False break if is_final_op : final_ops . append ( candidate_node ) if not final_ops : return dag barrier_layer = DAGCircuit ( ) for qreg in dag . qregs . values ( ) : barrier_layer . add_qreg ( qreg ) for creg in dag . cregs . values ( ) : barrier_layer . add_creg ( creg ) final_qubits = set ( final_op . qargs [ 0 ] for final_op in final_ops ) barrier_layer . apply_operation_back ( Barrier ( len ( final_qubits ) ) , list ( final_qubits ) , [ ] ) ordered_final_nodes = [ node for node in dag . topological_op_nodes ( ) if node in set ( final_ops ) ] for final_node in ordered_final_nodes : barrier_layer . apply_operation_back ( final_node . op , final_node . qargs , final_node . cargs ) for final_op in final_ops : dag . remove_op_node ( final_op ) dag . extend_back ( barrier_layer ) adjacent_pass = MergeAdjacentBarriers ( ) return adjacent_pass . run ( dag )
def unitary ( self , obj , qubits , label = None ) : if isinstance ( qubits , QuantumRegister ) : qubits = qubits [ : ] return self . append ( UnitaryGate ( obj , label = label ) , qubits , [ ] )
def _define ( self ) : if self . num_qubits == 1 : q = QuantumRegister ( 1 , "q" ) angles = euler_angles_1q ( self . to_matrix ( ) ) self . definition = [ ( U3Gate ( * angles ) , [ q [ 0 ] ] , [ ] ) ] if self . num_qubits == 2 : self . definition = two_qubit_kak ( self . to_matrix ( ) )
def _atol ( self , atol ) : max_tol = self . __class__ . MAX_TOL if atol < 0 : raise QiskitError ( "Invalid atol: must be non-negative." ) if atol > max_tol : raise QiskitError ( "Invalid atol: must be less than {}." . format ( max_tol ) ) self . __class__ . ATOL = atol
def _rtol ( self , rtol ) : max_tol = self . __class__ . MAX_TOL if rtol < 0 : raise QiskitError ( "Invalid rtol: must be non-negative." ) if rtol > max_tol : raise QiskitError ( "Invalid rtol: must be less than {}." . format ( max_tol ) ) self . __class__ . RTOL = rtol
def input_dims ( self , qargs = None ) : if qargs is None : return self . _input_dims return tuple ( self . _input_dims [ i ] for i in qargs )
def output_dims ( self , qargs = None ) : if qargs is None : return self . _output_dims return tuple ( self . _output_dims [ i ] for i in qargs )
def copy ( self ) : return self . __class__ ( self . data , self . input_dims ( ) , self . output_dims ( ) )
def _automatic_dims ( cls , dims , size ) : if dims is None : dims = size elif np . product ( dims ) != size : raise QiskitError ( "dimensions do not match size." ) if isinstance ( dims , ( int , np . integer ) ) : num_qubits = int ( np . log2 ( dims ) ) if 2 ** num_qubits == size : return num_qubits * ( 2 , ) return ( dims , ) return tuple ( dims )
def _deserialize ( self , value , attr , data ) : try : return super ( ) . _deserialize ( value , attr , data ) except ValidationError as ex : if 'deserialization_schema_selector' in ex . messages [ 0 ] : ex . messages [ 0 ] = 'Cannot find a valid schema among the choices' raise
def _serialize ( self , value , key , obj ) : try : return super ( ) . _serialize ( value , key , obj ) except TypeError as ex : if 'serialization_schema_selector' in str ( ex ) : raise ValidationError ( 'Data from an invalid schema' ) raise
def inverse ( self ) : return Snapshot ( self . num_qubits , self . num_clbits , self . params [ 0 ] , self . params [ 1 ] )
def is_unitary ( self , atol = None , rtol = None ) : try : op = self . to_operator ( ) return op . is_unitary ( atol = atol , rtol = rtol ) except QiskitError : return False
def to_operator ( self ) : mat = _to_operator ( self . rep , self . _data , * self . dim ) return Operator ( mat , self . input_dims ( ) , self . output_dims ( ) )
def _format_state ( self , state , density_matrix = False ) : state = np . array ( state ) shape = state . shape ndim = state . ndim if ndim > 2 : raise QiskitError ( 'Input state is not a vector or matrix.' ) if ndim == 2 : if shape [ 1 ] != 1 and shape [ 1 ] != shape [ 0 ] : raise QiskitError ( 'Input state is not a vector or matrix.' ) if shape [ 1 ] == 1 : state = np . reshape ( state , shape [ 0 ] ) if density_matrix and ndim == 1 : state = np . outer ( state , np . transpose ( np . conj ( state ) ) ) return state
def _init_transformer ( cls , data ) : if isinstance ( data , QuantumChannel ) : return data if hasattr ( data , 'to_quantumchannel' ) : return data . to_channel ( ) if hasattr ( data , 'to_channel' ) : return data . to_channel ( ) return Operator ( data )
def _parse_time ( self , date_string , settings ) : date_string = PATTERN . sub ( '' , date_string ) date_string = re . sub ( r'\b(?:ago|in)\b' , '' , date_string ) try : return time_parser ( date_string ) except : pass
def read_config ( self ) : self . threads = self . cfg [ "threads" ] or str ( int ( multiprocessing . cpu_count ( ) / 2 ) + 1 ) self . phantom_modules_path = self . cfg [ "phantom_modules_path" ] self . additional_libs = ' ' . join ( self . cfg [ "additional_libs" ] ) self . answ_log_level = self . cfg [ "writelog" ] if self . answ_log_level . lower ( ) in [ '0' , 'false' ] : self . answ_log_level = 'none' elif self . answ_log_level . lower ( ) in [ '1' , 'true' ] : self . answ_log_level = 'all' self . timeout = parse_duration ( self . cfg [ "timeout" ] ) if self . timeout > 120000 : logger . warning ( "You've set timeout over 2 minutes." " Are you a functional tester?" ) self . answ_log = self . core . mkstemp ( ".log" , "answ_" ) self . core . add_artifact_file ( self . answ_log ) self . core . add_artifact_file ( self . phout_file ) self . core . add_artifact_file ( self . stat_log ) self . phantom_log = self . core . mkstemp ( ".log" , "phantom_" ) self . core . add_artifact_file ( self . phantom_log ) main_stream = StreamConfig ( self . core , len ( self . streams ) , self . phout_file , self . answ_log , self . answ_log_level , self . timeout , self . cfg , True ) self . streams . append ( main_stream ) for section in self . multi ( ) : self . streams . append ( StreamConfig ( self . core , len ( self . streams ) , self . phout_file , self . answ_log , self . answ_log_level , self . timeout , section ) ) for stream in self . streams : stream . read_config ( ) if any ( stream . ssl for stream in self . streams ) : self . additional_libs += ' ssl io_benchmark_method_stream_transport_ssl'
def compose_config ( self ) : streams_config = '' stat_benchmarks = '' for stream in self . streams : streams_config += stream . compose_config ( ) if not stream . is_main : stat_benchmarks += " " + "benchmark_io%s" % stream . sequence_no kwargs = { } kwargs [ 'threads' ] = self . threads kwargs [ 'phantom_log' ] = self . phantom_log kwargs [ 'stat_log' ] = self . stat_log kwargs [ 'benchmarks_block' ] = streams_config kwargs [ 'stat_benchmarks' ] = stat_benchmarks kwargs [ 'additional_libs' ] = self . additional_libs kwargs [ 'phantom_modules_path' ] = self . phantom_modules_path filename = self . core . mkstemp ( ".conf" , "phantom_" ) self . core . add_artifact_file ( filename ) logger . debug ( "Generating phantom config: %s" , filename ) template_str = resource_string ( __name__ , "config/phantom.conf.tpl" ) tpl = string . Template ( template_str ) config = tpl . substitute ( kwargs ) with open ( filename , 'w' ) as conffile : conffile . write ( config ) return filename
def get_info ( self ) : result = copy . copy ( self . streams [ 0 ] ) result . stat_log = self . stat_log result . steps = [ ] result . ammo_file = '' result . rps_schedule = None result . ammo_count = 0 result . duration = 0 result . instances = 0 result . loadscheme = [ ] result . loop_count = 0 for stream in self . streams : sec_no = 0 logger . debug ( "Steps: %s" , stream . stepper_wrapper . steps ) for item in stream . stepper_wrapper . steps : for x in range ( 0 , item [ 1 ] ) : if len ( result . steps ) > sec_no : result . steps [ sec_no ] [ 0 ] += item [ 0 ] else : result . steps . append ( [ item [ 0 ] , 1 ] ) sec_no += 1 if result . rps_schedule : result . rps_schedule = [ ] else : result . rps_schedule = stream . stepper_wrapper . loadscheme if result . loadscheme : result . loadscheme = '' else : result . loadscheme = '' if result . loop_count : result . loop_count = u'0' else : result . loop_count = stream . stepper_wrapper . loop_count result . ammo_file += '{} ' . format ( stream . stepper_wrapper . ammo_file ) result . ammo_count += stream . stepper_wrapper . ammo_count result . duration = max ( result . duration , stream . stepper_wrapper . duration ) result . instances += stream . instances if not result . ammo_count : raise ValueError ( "Total ammo count cannot be zero" ) return result
def expand_time ( str_time , default_unit = 's' , multiplier = 1 ) : parser = re . compile ( r'(\d+)([a-zA-Z]*)' ) parts = parser . findall ( str_time ) result = 0.0 for value , unit in parts : value = int ( value ) unit = unit . lower ( ) if unit == '' : unit = default_unit if unit == 'ms' : result += value * 0.001 continue elif unit == 's' : result += value continue elif unit == 'm' : result += value * 60 continue elif unit == 'h' : result += value * 60 * 60 continue elif unit == 'd' : result += value * 60 * 60 * 24 continue elif unit == 'w' : result += value * 60 * 60 * 24 * 7 continue else : raise ValueError ( "String contains unsupported unit %s: %s" % ( unit , str_time ) ) return int ( result * multiplier )
def pid_exists ( pid ) : if pid < 0 : return False try : os . kill ( pid , 0 ) except OSError as exc : logging . debug ( "No process[%s]: %s" , exc . errno , exc ) return exc . errno == errno . EPERM else : p = psutil . Process ( pid ) return p . status != psutil . STATUS_ZOMBIE
def read_config ( self ) : self . log . info ( "Configuring StepperWrapper..." ) self . ammo_file = self . get_option ( self . OPTION_AMMOFILE ) self . ammo_type = self . get_option ( 'ammo_type' ) if self . ammo_file : self . ammo_file = os . path . expanduser ( self . ammo_file ) self . loop_limit = self . get_option ( self . OPTION_LOOP ) self . ammo_limit = self . get_option ( "ammo_limit" ) self . load_profile = LoadProfile ( * * self . get_option ( 'load_profile' ) ) self . instances = int ( self . get_option ( self . OPTION_INSTANCES_LIMIT , '1000' ) ) self . uris = self . get_option ( "uris" , [ ] ) while '' in self . uris : self . uris . remove ( '' ) self . headers = self . get_option ( "headers" ) self . http_ver = self . get_option ( "header_http" ) self . autocases = self . get_option ( "autocases" ) self . enum_ammo = self . get_option ( "enum_ammo" ) self . use_caching = self . get_option ( "use_caching" ) self . file_cache = self . get_option ( 'file_cache' ) cache_dir = self . get_option ( "cache_dir" ) or self . core . artifacts_base_dir self . cache_dir = os . path . expanduser ( cache_dir ) self . force_stepping = self . get_option ( "force_stepping" ) if self . get_option ( self . OPTION_LOAD ) [ self . OPTION_LOAD_TYPE ] == 'stpd_file' : self . stpd = self . get_option ( self . OPTION_LOAD ) [ self . OPTION_SCHEDULE ] self . chosen_cases = self . get_option ( "chosen_cases" ) . split ( ) if self . chosen_cases : self . log . info ( "chosen_cases LIMITS: %s" , self . chosen_cases )
def prepare_stepper ( self ) : def publish_info ( stepper_info ) : info . status . publish ( 'loadscheme' , stepper_info . loadscheme ) info . status . publish ( 'loop_count' , stepper_info . loop_count ) info . status . publish ( 'steps' , stepper_info . steps ) info . status . publish ( 'duration' , stepper_info . duration ) info . status . ammo_count = stepper_info . ammo_count info . status . publish ( 'instances' , stepper_info . instances ) self . core . publish ( 'stepper' , 'loadscheme' , stepper_info . loadscheme ) self . core . publish ( 'stepper' , 'loop_count' , stepper_info . loop_count ) self . core . publish ( 'stepper' , 'steps' , stepper_info . steps ) self . core . publish ( 'stepper' , 'duration' , stepper_info . duration ) self . core . publish ( 'stepper' , 'ammo_count' , stepper_info . ammo_count ) self . core . publish ( 'stepper' , 'instances' , stepper_info . instances ) return stepper_info if not self . stpd : self . stpd = self . __get_stpd_filename ( ) if self . use_caching and not self . force_stepping and os . path . exists ( self . stpd ) and os . path . exists ( self . __si_filename ( ) ) : self . log . info ( "Using cached stpd-file: %s" , self . stpd ) stepper_info = self . __read_cached_options ( ) if self . instances and self . load_profile . is_rps ( ) : self . log . info ( "rps_schedule is set. Overriding cached instances param from config: %s" , self . instances ) stepper_info = stepper_info . _replace ( instances = self . instances ) publish_info ( stepper_info ) else : if ( self . force_stepping and os . path . exists ( self . __si_filename ( ) ) ) : os . remove ( self . __si_filename ( ) ) self . __make_stpd_file ( ) stepper_info = info . status . get_info ( ) self . __write_cached_options ( stepper_info ) else : self . log . info ( "Using specified stpd-file: %s" , self . stpd ) stepper_info = publish_info ( self . __read_cached_options ( ) ) self . ammo_count = stepper_info . ammo_count self . duration = stepper_info . duration self . loop_count = stepper_info . loop_count self . loadscheme = stepper_info . loadscheme self . steps = stepper_info . steps if stepper_info . instances : self . instances = stepper_info . instances
def __get_stpd_filename ( self ) : if self . use_caching : sep = "|" hasher = hashlib . md5 ( ) hashed_str = "cache version 6" + sep + ';' . join ( self . load_profile . schedule ) + sep + str ( self . loop_limit ) hashed_str += sep + str ( self . ammo_limit ) + sep + ';' . join ( self . load_profile . schedule ) + sep + str ( self . autocases ) hashed_str += sep + ";" . join ( self . uris ) + sep + ";" . join ( self . headers ) + sep + self . http_ver + sep + ";" . join ( self . chosen_cases ) hashed_str += sep + str ( self . enum_ammo ) + sep + str ( self . ammo_type ) if self . load_profile . is_instances ( ) : hashed_str += sep + str ( self . instances ) if self . ammo_file : opener = resource . get_opener ( self . ammo_file ) hashed_str += sep + opener . hash else : if not self . uris : raise RuntimeError ( "Neither ammofile nor uris specified" ) hashed_str += sep + ';' . join ( self . uris ) + sep + ';' . join ( self . headers ) self . log . debug ( "stpd-hash source: %s" , hashed_str ) hasher . update ( hashed_str . encode ( 'utf8' ) ) if not os . path . exists ( self . cache_dir ) : os . makedirs ( self . cache_dir ) stpd = self . cache_dir + '/' + os . path . basename ( self . ammo_file ) + "_" + hasher . hexdigest ( ) + ".stpd" else : stpd = os . path . realpath ( "ammo.stpd" ) self . log . debug ( "Generated cache file name: %s" , stpd ) return stpd
def __read_cached_options ( self ) : self . log . debug ( "Reading cached stepper info: %s" , self . __si_filename ( ) ) with open ( self . __si_filename ( ) , 'r' ) as si_file : si = info . StepperInfo ( * * json . load ( si_file ) ) return si
def __write_cached_options ( self , si ) : self . log . debug ( "Saving stepper info: %s" , self . __si_filename ( ) ) with open ( self . __si_filename ( ) , 'w' ) as si_file : json . dump ( si . _asdict ( ) , si_file , indent = 4 )
def __make_stpd_file ( self ) : self . log . info ( "Making stpd-file: %s" , self . stpd ) stepper = Stepper ( self . core , rps_schedule = self . load_profile . schedule if self . load_profile . is_rps ( ) else None , http_ver = self . http_ver , ammo_file = self . ammo_file , instances_schedule = self . load_profile . schedule if self . load_profile . is_instances ( ) else None , instances = self . instances , loop_limit = self . loop_limit , ammo_limit = self . ammo_limit , uris = self . uris , headers = [ header . strip ( '[]' ) for header in self . headers ] , autocases = self . autocases , enum_ammo = self . enum_ammo , ammo_type = self . ammo_type , chosen_cases = self . chosen_cases , use_cache = self . use_caching ) with open ( self . stpd , 'w' , self . file_cache ) as os : stepper . write ( os )
def create ( rps_schedule ) : if len ( rps_schedule ) > 1 : lp = Composite ( [ StepFactory . produce ( step_config ) for step_config in rps_schedule ] ) else : lp = StepFactory . produce ( rps_schedule [ 0 ] ) info . status . publish ( 'duration' , lp . get_duration ( ) / 1000 ) info . status . publish ( 'steps' , lp . get_rps_list ( ) ) info . status . lp_len = len ( lp ) return lp
def rps_at ( self , t ) : if 0 <= t <= self . duration : return self . minrps + float ( self . maxrps - self . minrps ) * t / self . duration else : return 0
def execute ( self , cmd ) : self . log . info ( "Executing: %s" , cmd ) retcode = execute ( cmd , shell = True , poll_period = 0.1 , catch_out = self . catch_out ) [ 0 ] if retcode : raise RuntimeError ( "Subprocess returned %s" % retcode ) return retcode
def publish ( self , key , value ) : self . log . debug ( "Publishing status: %s/%s: %s" , self . __class__ . __name__ , key , value ) self . core . publish ( self . __class__ . __name__ , key , value )
def count_matched_codes ( codes_regex , codes_dict ) : total = 0 for code , count in codes_dict . items ( ) : if codes_regex . match ( str ( code ) ) : total += count return total
def stop ( self ) : self . quit . set ( ) while sorted ( [ self . pool [ i ] . is_alive ( ) for i in xrange ( len ( self . pool ) ) ] ) [ - 1 ] : time . sleep ( 1 ) try : while not self . task_queue . empty ( ) : self . task_queue . get ( timeout = 0.1 ) self . task_queue . close ( ) self . feeder . join ( ) except Exception as ex : logger . info ( ex )
def _feed ( self ) : self . plan = StpdReader ( self . stpd_filename ) if self . cached_stpd : self . plan = list ( self . plan ) for task in self . plan : if self . quit . is_set ( ) : logger . info ( "Stop feeding: gonna quit" ) return while True : try : self . task_queue . put ( task , timeout = 1 ) break except Full : if self . quit . is_set ( ) or self . workers_finished : return else : continue workers_count = self . instances logger . info ( "Feeded all data. Publishing %d killer tasks" % ( workers_count ) ) retry_delay = 1 for _ in range ( 5 ) : try : [ self . task_queue . put ( None , timeout = 1 ) for _ in xrange ( 0 , workers_count ) ] break except Full : logger . debug ( "Couldn't post killer tasks" " because queue is full. Retrying in %ss" , retry_delay ) time . sleep ( retry_delay ) retry_delay *= 2 try : logger . info ( "Waiting for workers" ) map ( lambda x : x . join ( ) , self . pool ) logger . info ( "All workers exited." ) self . workers_finished = True except ( KeyboardInterrupt , SystemExit ) : self . task_queue . close ( ) self . results . close ( ) self . quit . set ( ) logger . info ( "Going to quit. Waiting for workers" ) map ( lambda x : x . join ( ) , self . pool ) self . workers_finished = True
def _worker ( self ) : logger . debug ( "Init shooter process" ) try : self . gun . setup ( ) except Exception : logger . exception ( "Couldn't initialize gun. Exit shooter process" ) return while not self . quit . is_set ( ) : try : task = self . task_queue . get ( timeout = 1 ) if not task : logger . debug ( "Got killer task." ) break timestamp , missile , marker = task planned_time = self . start_time + ( timestamp / 1000.0 ) delay = planned_time - time . time ( ) if delay > 0 : time . sleep ( delay ) try : with self . instance_counter . get_lock ( ) : self . instance_counter . value += 1 self . gun . shoot ( missile , marker ) finally : with self . instance_counter . get_lock ( ) : self . instance_counter . value -= 1 except ( KeyboardInterrupt , SystemExit ) : break except Empty : if self . quit . is_set ( ) : logger . debug ( "Empty queue. Exiting process" ) return except Full : logger . warning ( "Couldn't put to result queue because it's full" ) except Exception : logger . exception ( "Bfg shoot exception" ) try : self . gun . teardown ( ) except Exception : logger . exception ( "Couldn't finalize gun. Exit shooter process" ) return logger . debug ( "Exit shooter process" )
def _green_worker ( self ) : while not self . quit . is_set ( ) : try : task = self . green_queue . get ( timeout = 1 ) timestamp , missile , marker = task planned_time = self . start_time + ( timestamp / 1000.0 ) delay = planned_time - time . time ( ) if delay > 0 : time . sleep ( delay ) try : with self . instance_counter . get_lock ( ) : self . instance_counter . value += 1 self . gun . shoot ( missile , marker ) finally : with self . instance_counter . get_lock ( ) : self . instance_counter . value -= 1 self . _free_threads_count += 1 except ( KeyboardInterrupt , SystemExit ) : break except Empty : continue except Full : logger . warning ( "Couldn't put to result queue because it's full" ) except Exception : logger . exception ( "Bfg shoot exception" )
def __add_user_options ( self ) : if self . options . get ( 'user_options' , None ) : self . core . apply_shorthand_options ( self . options [ 'user_options' ] )
def configure ( self , options ) : self . options = options if self . options . get ( 'lock_dir' , None ) : self . core . set_option ( self . core . SECTION , "lock_dir" , self . options [ 'lock_dir' ] ) if self . options . get ( 'ignore_lock' , None ) : self . core . set_option ( self . core . SECTION , 'ignore_lock' , self . options [ 'ignore_lock' ] ) while True : try : self . core . get_lock ( ) break except Exception as exc : if self . options . get ( 'lock_fail' , None ) : raise RuntimeError ( "Lock file present, cannot continue" ) self . log . info ( "Couldn't get lock. Will retry in 5 seconds... (%s)" , str ( exc ) ) time . sleep ( 5 ) configs = self . get_default_configs ( ) if self . options . get ( 'config' , None ) : configs . append ( self . options [ 'config' ] ) self . core . load_configs ( configs ) self . __add_user_options ( ) self . core . load_plugins ( ) if self . options . get ( 'ignore_lock' , None ) : self . core . set_option ( self . core . SECTION , self . IGNORE_LOCKS , "1" )
def _collect_data ( self , end = False ) : data = get_nowait_from_queue ( self . results ) stats = get_nowait_from_queue ( self . stats_results ) logger . debug ( "Data timestamps: %s" % [ d . get ( 'ts' ) for d in data ] ) logger . debug ( "Stats timestamps: %s" % [ d . get ( 'ts' ) for d in stats ] ) for item in data : ts = item [ 'ts' ] if ts in self . stat_cache : data_item = item stat_item = self . stat_cache . pop ( ts ) self . __notify_listeners ( data_item , stat_item ) else : self . data_cache [ ts ] = item for item in stats : ts = item [ 'ts' ] if ts in self . data_cache : data_item = self . data_cache . pop ( ts ) stat_item = item self . __notify_listeners ( data_item , stat_item ) else : self . stat_cache [ ts ] = item if end and len ( self . data_cache ) > 0 : logger . info ( 'Timestamps without stats:' ) for ts , data_item in sorted ( self . data_cache . items ( ) , key = lambda i : i [ 0 ] ) : logger . info ( ts ) self . __notify_listeners ( data_item , StatsReader . stats_item ( ts , 0 , 0 ) )
def __notify_listeners ( self , data , stats ) : for listener in self . listeners : listener . on_aggregated_data ( data , stats )
def clean_markup ( self , orig_str ) : for val in [ self . YELLOW , self . RED , self . RESET , self . CYAN , self . BG_MAGENTA , self . WHITE , self . BG_GREEN , self . GREEN , self . BG_BROWN , self . RED_DARK , self . MAGENTA , self . BG_CYAN ] : orig_str = orig_str . replace ( val , '' ) return orig_str
def uninstall ( self ) : if self . session : logger . info ( 'Waiting monitoring data...' ) self . session . terminate ( ) self . session . wait ( ) self . session = None log_filename = "agent_{host}.log" . format ( host = "localhost" ) data_filename = "agent_{host}.rawdata" . format ( host = "localhost" ) try : logger . info ( 'Saving monitoring artefacts from localhost' ) copyfile ( self . workdir + "/_agent.log" , log_filename ) copyfile ( self . workdir + "/monitoring.rawdata" , data_filename ) logger . info ( 'Deleting temp directory: %s' , self . workdir ) rmtree ( self . workdir ) except Exception : logger . error ( "Exception while uninstalling agent" , exc_info = True ) logger . info ( "Removing agent from: localhost" ) return log_filename , data_filename
def uninstall ( self ) : log_filename = "agent_{host}.log" . format ( host = self . host ) data_filename = "agent_{host}.rawdata" . format ( host = self . host ) try : if self . session : self . session . send ( "stop\n" ) self . session . close ( ) self . session = None except BaseException : logger . warning ( 'Unable to correctly stop monitoring agent - session is broken. Pay attention to agent log (%s).' , log_filename , exc_info = True ) else : try : self . ssh . get_file ( os . path . join ( self . path [ 'AGENT_REMOTE_FOLDER' ] , "_agent.log" ) , log_filename ) self . ssh . get_file ( os . path . join ( self . path [ 'AGENT_REMOTE_FOLDER' ] , "monitoring.rawdata" ) , data_filename ) self . ssh . rm_r ( self . path [ 'AGENT_REMOTE_FOLDER' ] ) except Exception : logger . error ( "Unable to get agent artefacts" , exc_info = True ) self . _kill_agent ( ) return log_filename , data_filename
def __add_jmeter_components ( self , jmx , jtl , variables ) : logger . debug ( "Original JMX: %s" , os . path . realpath ( jmx ) ) with open ( jmx , 'r' ) as src_jmx : source_lines = src_jmx . readlines ( ) try : closing = source_lines . pop ( - 1 ) if "WorkBenchGui" in source_lines [ - 5 ] : logger . info ( "WorkBench checkbox enabled...bypassing" ) last_string_count = 6 else : last_string_count = 2 while last_string_count > 0 : closing = source_lines . pop ( - 1 ) + closing last_string_count -= 1 logger . debug ( "Closing statement: %s" , closing ) except Exception as exc : raise RuntimeError ( "Failed to find the end of JMX XML: %s" % exc ) udv_tpl = resource_string ( __name__ , 'config/jmeter_var_template.xml' ) udv_set = [ ] for var_name , var_value in variables . iteritems ( ) : udv_set . append ( udv_tpl % ( var_name , var_name , var_value ) ) udv = "\n" . join ( udv_set ) if self . jmeter_ver >= 2.13 : save_connect = '<connectTime>true</connectTime>' else : save_connect = '' if self . ext_log in [ 'errors' , 'all' ] : level_map = { 'errors' : 'true' , 'all' : 'false' } tpl_resource = 'jmeter_writer_ext.xml' tpl_args = { 'jtl' : self . jtl_file , 'udv' : udv , 'ext_log' : self . ext_log_file , 'ext_level' : level_map [ self . ext_log ] , 'save_connect' : save_connect } else : tpl_resource = 'jmeter_writer.xml' tpl_args = { 'jtl' : self . jtl_file , 'udv' : udv , 'save_connect' : save_connect } tpl = resource_string ( __name__ , 'config/' + tpl_resource ) try : new_jmx = self . core . mkstemp ( '.jmx' , 'modified_' , os . path . dirname ( os . path . realpath ( jmx ) ) ) except OSError as exc : logger . debug ( "Can't create modified jmx near original: %s" , exc ) new_jmx = self . core . mkstemp ( '.jmx' , 'modified_' ) logger . debug ( "Modified JMX: %s" , new_jmx ) with open ( new_jmx , "wb" ) as fh : fh . write ( '' . join ( source_lines ) ) fh . write ( tpl % tpl_args ) fh . write ( closing ) return new_jmx
def __terminate ( self ) : if self . __stderr_file : self . __stderr_file . close ( ) if not self . __process : return waitfor = time . time ( ) + _PROCESS_KILL_TIMEOUT while time . time ( ) < waitfor : try : self . __process . terminate ( ) except EnvironmentError as e : if e . errno != errno . ESRCH : _LOGGER . warning ( "Failed to terminate process '{}': {}" . format ( self . __cmd , e ) ) return time . sleep ( 0.1 ) try : self . __process . kill ( ) except EnvironmentError as e : if e . errno != errno . ESRCH : _LOGGER . warning ( "Failed to kill process '{}': {}" . format ( self . __cmd , e ) ) return
def _read_data ( self , lines ) : results = [ ] for line in lines : timestamp , rps , instances = line . split ( "\t" ) curr_ts = int ( float ( timestamp ) ) if self . __last_ts < curr_ts : self . __last_ts = curr_ts results . append ( self . stats_item ( self . __last_ts , float ( rps ) , float ( instances ) ) ) return results
def __create_criterion ( self , criterion_str ) : parsed = criterion_str . split ( "(" ) type_str = parsed [ 0 ] . strip ( ) . lower ( ) parsed [ 1 ] = parsed [ 1 ] . split ( ")" ) [ 0 ] . strip ( ) for criterion_class in self . custom_criterions : if criterion_class . get_type_string ( ) == type_str : return criterion_class ( self , parsed [ 1 ] ) raise ValueError ( "Unsupported autostop criterion type: %s" % criterion_str )
def getconfig ( self , filename , target_hint ) : try : tree = self . parse_xml ( filename ) except IOError as exc : logger . error ( "Error loading config: %s" , exc ) raise RuntimeError ( "Can't read monitoring config %s" % filename ) hosts = tree . findall ( 'Host' ) config = [ ] for host in hosts : host_config = self . get_host_config ( host , target_hint ) config . append ( host_config ) return config
def __check_disk ( self ) : cmd = "sh -c \"df --no-sync -m -P -l -x fuse -x tmpfs -x devtmpfs -x davfs -x nfs " cmd += self . core . artifacts_base_dir cmd += " | tail -n 1 | awk '{print \$4}' \"" res = execute ( cmd , True , 0.1 , True ) logging . debug ( "Result: %s" , res ) if not len ( res [ 1 ] ) : self . log . debug ( "No disk usage info: %s" , res [ 2 ] ) return disk_free = res [ 1 ] self . log . debug ( "Disk free space: %s/%s" , disk_free . strip ( ) , self . disk_limit ) if int ( disk_free . strip ( ) ) < self . disk_limit : raise RuntimeError ( "Not enough local resources: disk space less than %sMB in %s: %sMB" % ( self . disk_limit , self . core . artifacts_base_dir , int ( disk_free . strip ( ) ) ) )
def __check_mem ( self ) : mem_free = psutil . virtual_memory ( ) . available / 2 ** 20 self . log . debug ( "Memory free: %s/%s" , mem_free , self . mem_limit ) if mem_free < self . mem_limit : raise RuntimeError ( "Not enough resources: free memory less " "than %sMB: %sMB" % ( self . mem_limit , mem_free ) )
def get_terminal_size ( ) : default_size = ( 30 , 120 ) env = os . environ def ioctl_gwinsz ( file_d ) : try : sizes = struct . unpack ( 'hh' , fcntl . ioctl ( file_d , termios . TIOCGWINSZ , '1234' ) ) except Exception : sizes = default_size return sizes sizes = ioctl_gwinsz ( 0 ) or ioctl_gwinsz ( 1 ) or ioctl_gwinsz ( 2 ) if not sizes : try : file_d = os . open ( os . ctermid ( ) , os . O_RDONLY ) sizes = ioctl_gwinsz ( file_d ) os . close ( file_d . fileno ( ) ) except Exception : pass if not sizes : try : sizes = ( env [ 'LINES' ] , env [ 'COLUMNS' ] ) except Exception : sizes = default_size return int ( sizes [ 1 ] ) , int ( sizes [ 0 ] )
def __get_right_line ( self , widget_output ) : right_line = '' if widget_output : right_line = widget_output . pop ( 0 ) if len ( right_line ) > self . right_panel_width : right_line_plain = self . markup . clean_markup ( right_line ) if len ( right_line_plain ) > self . right_panel_width : right_line = right_line [ : self . right_panel_width ] + self . markup . RESET return right_line
def __truncate ( self , line_arr , max_width ) : def is_space ( chunk ) : return all ( [ True if i == ' ' else False for i in chunk ] ) def is_empty ( chunks , markups ) : result = [ ] for chunk in chunks : if chunk in markups : result . append ( True ) elif is_space ( chunk ) : result . append ( True ) else : result . append ( False ) return all ( result ) left = max_width result = '' markups = self . markup . get_markup_vars ( ) for num , chunk in enumerate ( line_arr ) : if chunk in markups : result += chunk else : if left > 0 : if len ( chunk ) <= left : result += chunk left -= len ( chunk ) else : leftover = ( chunk [ left : ] , ) + line_arr [ num + 1 : ] was_cut = not is_empty ( leftover , markups ) if was_cut : result += chunk [ : left - 1 ] + self . markup . RESET + u'\u2026' else : result += chunk [ : left ] left = 0 return result
def render_screen ( self ) : self . term_width , self . term_height = get_terminal_size ( ) self . log . debug ( "Terminal size: %sx%s" , self . term_width , self . term_height ) self . right_panel_width = int ( ( self . term_width - len ( self . RIGHT_PANEL_SEPARATOR ) ) * ( float ( self . info_panel_percent ) / 100 ) ) - 1 if self . right_panel_width > 0 : self . left_panel_width = self . term_width - self . right_panel_width - len ( self . RIGHT_PANEL_SEPARATOR ) - 2 else : self . right_panel_width = 0 self . left_panel_width = self . term_width - 1 self . log . debug ( "Left/right panels width: %s/%s" , self . left_panel_width , self . right_panel_width ) widget_output = [ ] if self . right_panel_width : widget_output = [ ] self . log . debug ( "There are %d info widgets" % len ( self . info_widgets ) ) for index , widget in sorted ( self . info_widgets . iteritems ( ) , key = lambda item : ( item [ 1 ] . get_index ( ) , item [ 0 ] ) ) : self . log . debug ( "Rendering info widget #%s: %s" , index , widget ) widget_out = widget . render ( self ) . strip ( ) if widget_out : widget_output += widget_out . split ( "\n" ) widget_output += [ "" ] left_lines = self . __render_left_panel ( ) self . log . debug ( "Composing final screen output" ) output = [ ] for line_no in range ( 1 , self . term_height ) : line = " " if line_no > 1 and left_lines : left_line = left_lines . pop ( 0 ) left_line_plain = self . markup . clean_markup ( left_line ) left_line += ( ' ' * ( self . left_panel_width - len ( left_line_plain ) ) ) line += left_line else : line += ' ' * self . left_panel_width if self . right_panel_width : line += self . markup . RESET line += self . markup . WHITE line += self . RIGHT_PANEL_SEPARATOR line += self . markup . RESET right_line = self . __get_right_line ( widget_output ) line += right_line output . append ( line ) return self . markup . new_line . join ( output ) + self . markup . new_line
def add_info_widget ( self , widget ) : index = widget . get_index ( ) while index in self . info_widgets . keys ( ) : index += 1 self . info_widgets [ widget . get_index ( ) ] = widget
def fill_rectangle ( self , prepared ) : result = [ ] width = max ( [ self . clean_len ( line ) for line in prepared ] ) for line in prepared : spacer = ' ' * ( width - self . clean_len ( line ) ) result . append ( line + ( self . screen . markup . RESET , spacer ) ) return ( width , result )
def clean_len ( self , line ) : if isinstance ( line , basestring ) : return len ( self . screen . markup . clean_markup ( line ) ) elif isinstance ( line , tuple ) or isinstance ( line , list ) : markups = self . screen . markup . get_markup_vars ( ) length = 0 for i in line : if i not in markups : length += len ( i ) return length
def add_info_widget ( self , widget ) : if not self . screen : self . log . debug ( "No screen instance to add widget" ) else : self . screen . add_info_widget ( widget )
def clean_markup ( self , orig_str ) : for val in self . get_markup_vars ( ) : orig_str = orig_str . replace ( val , '' ) return orig_str
def __make_writer_request ( self , params = None , json = None , http_method = "POST" , trace = False ) : request = requests . Request ( http_method , self . writer_url , params = params , json = json , headers = { 'User-Agent' : self . user_agent } ) ids = id_gen ( str ( uuid . uuid4 ( ) ) ) network_timeouts = self . network_timeouts ( ) maintenance_timeouts = self . maintenance_timeouts ( ) while True : try : response = self . __send_single_request ( request , ids . next ( ) , trace = trace ) return response except ( Timeout , ConnectionError , ProtocolError ) : logger . warn ( traceback . format_exc ( ) ) try : timeout = next ( network_timeouts ) logger . warn ( "Network error, will retry in %ss..." % timeout ) time . sleep ( timeout ) continue except StopIteration : raise self . NetworkError ( ) except self . UnderMaintenance as e : try : timeout = next ( maintenance_timeouts ) logger . warn ( "Writer is under maintenance, will retry in %ss..." % timeout ) time . sleep ( timeout ) continue except StopIteration : raise e
def load_plugins ( self ) : logger . info ( "Loading plugins..." ) for ( plugin_name , plugin_path , plugin_cfg ) in self . config . plugins : logger . debug ( "Loading plugin %s from %s" , plugin_name , plugin_path ) if plugin_path == "yandextank.plugins.Overload" : logger . warning ( "Deprecated plugin name: 'yandextank.plugins.Overload'\n" "There is a new generic plugin now.\n" "Correcting to 'yandextank.plugins.DataUploader overload'" ) plugin_path = "yandextank.plugins.DataUploader overload" try : plugin = il . import_module ( plugin_path ) except ImportError : logger . warning ( 'Plugin name %s path %s import error' , plugin_name , plugin_path ) logger . debug ( 'Plugin name %s path %s import error' , plugin_name , plugin_path , exc_info = True ) raise try : instance = getattr ( plugin , 'Plugin' ) ( self , cfg = plugin_cfg , name = plugin_name ) except AttributeError : logger . warning ( 'Plugin %s classname should be `Plugin`' , plugin_name ) raise else : self . register_plugin ( self . PLUGIN_PREFIX + plugin_name , instance ) logger . debug ( "Plugin instances: %s" , self . _plugins )
def get_plugin_of_type ( self , plugin_class ) : logger . debug ( "Searching for plugin: %s" , plugin_class ) matches = [ plugin for plugin in self . plugins . values ( ) if isinstance ( plugin , plugin_class ) ] if matches : if len ( matches ) > 1 : logger . debug ( "More then one plugin of type %s found. Using first one." , plugin_class ) return matches [ - 1 ] else : raise KeyError ( "Requested plugin type not found: %s" % plugin_class )
def get_plugins_of_type ( self , plugin_class ) : logger . debug ( "Searching for plugins: %s" , plugin_class ) matches = [ plugin for plugin in self . plugins . values ( ) if isinstance ( plugin , plugin_class ) ] if matches : return matches else : raise KeyError ( "Requested plugin type not found: %s" % plugin_class )
def __collect_file ( self , filename , keep_original = False ) : dest = self . artifacts_dir + '/' + os . path . basename ( filename ) logger . debug ( "Collecting file: %s to %s" , filename , dest ) if not filename or not os . path . exists ( filename ) : logger . warning ( "File not found to collect: %s" , filename ) return if os . path . exists ( dest ) : logger . warning ( "File already exists: %s" , dest ) return if keep_original : shutil . copy ( filename , self . artifacts_dir ) else : shutil . move ( filename , self . artifacts_dir ) os . chmod ( dest , 0o644 )
def add_artifact_file ( self , filename , keep_original = False ) : if filename : logger . debug ( "Adding artifact file to collect (keep=%s): %s" , keep_original , filename ) self . artifact_files [ filename ] = keep_original
def load_files ( self , configs ) : logger . debug ( "Reading configs: %s" , configs ) config_filenames = [ resource . resource_filename ( config ) for config in configs ] try : self . config . read ( config_filenames ) except Exception as ex : logger . error ( "Can't load configs: %s" , ex ) raise ex
def flush ( self , filename = None ) : if not filename : filename = self . file if filename : with open ( filename , 'w' ) as handle : self . config . write ( handle )
def get_options ( self , section , prefix = '' ) : res = [ ] try : for option in self . config . options ( section ) : if not prefix or option . find ( prefix ) == 0 : res += [ ( option [ len ( prefix ) : ] , self . config . get ( section , option ) ) ] except ConfigParser . NoSectionError as ex : logger . warning ( "No section: %s" , ex ) logger . debug ( "Section: [%s] prefix: '%s' options:\n%s" , section , prefix , res ) return res
def find_sections ( self , prefix ) : res = [ ] for section in self . config . sections ( ) : if section . startswith ( prefix ) : res . append ( section ) return res
def _decode_stat_data ( self , chunk ) : for date_str , statistics in chunk . iteritems ( ) : date_obj = datetime . datetime . strptime ( date_str . split ( "." ) [ 0 ] , '%Y-%m-%d %H:%M:%S' ) chunk_date = int ( time . mktime ( date_obj . timetuple ( ) ) ) instances = 0 for benchmark_name , benchmark in statistics . iteritems ( ) : if not benchmark_name . startswith ( "benchmark_io" ) : continue for method , meth_obj in benchmark . iteritems ( ) : if "mmtasks" in meth_obj : instances += meth_obj [ "mmtasks" ] [ 2 ] offset = chunk_date - 1 - self . start_time reqps = 0 if 0 <= offset < len ( self . phantom_info . steps ) : reqps = self . phantom_info . steps [ offset ] [ 0 ] yield self . stats_item ( chunk_date - 1 , instances , reqps )
def prepare ( self ) : agent_configs = [ ] if self . config : agent_configs = self . config_manager . getconfig ( self . config , self . default_target ) for config in agent_configs : if config [ 'host' ] in [ 'localhost' , '127.0.0.1' , '::1' ] : client = self . clients [ 'localhost' ] ( config , self . old_style_configs , kill_old = self . kill_old ) else : client = self . clients [ 'ssh' ] ( config , self . old_style_configs , timeout = 5 , kill_old = self . kill_old ) logger . debug ( 'Installing monitoring agent. Host: %s' , client . host ) agent_config , startup_config , customs_script = client . install ( ) if agent_config : self . agents . append ( client ) self . artifact_files . append ( agent_config ) if startup_config : self . artifact_files . append ( startup_config ) if customs_script : self . artifact_files . append ( customs_script )
def poll ( self ) : start_time = time . time ( ) for agent in self . agents : for collect in agent . reader : if not collect : return 0 for chunk in collect : ts , prepared_results = chunk if self . load_start_time and int ( ts ) >= self . load_start_time : ready_to_send = { "timestamp" : int ( ts ) , "data" : { self . hash_hostname ( agent . host ) : { "comment" : agent . config . comment , "metrics" : prepared_results } } } self . __collected_data . append ( ready_to_send ) logger . debug ( 'Polling/decoding agents data took: %.2fms' , ( time . time ( ) - start_time ) * 1000 ) collected_data_length = len ( self . __collected_data ) if not self . first_data_received and self . __collected_data : self . first_data_received = True logger . info ( "Monitoring received first data." ) else : self . send_collected_data ( ) return collected_data_length
def send_collected_data ( self ) : data = self . __collected_data self . __collected_data = [ ] for listener in self . listeners : listener . monitoring_data ( copy . deepcopy ( data ) )
def _decode_agents_data ( self , block ) : collect = [ ] if block : for chunk in block . split ( '\n' ) : try : if chunk : prepared_results = { } jsn = json . loads ( chunk ) for ts , values in jsn . iteritems ( ) : for key , value in values . iteritems ( ) : try : key_group , key_name = key . split ( '_' ) [ 0 ] . split ( '-' ) [ 0 ] , '_' . join ( key . split ( '_' ) [ 1 : ] ) except : key_group , key_name = key . split ( '_' ) [ 0 ] , '_' . join ( key . split ( '_' ) [ 1 : ] ) if key_group in decoder . diff_metrics . keys ( ) : if key_name in decoder . diff_metrics [ key_group ] : decoded_key = decoder . find_common_names ( key ) if self . prev_check : try : value = jsn [ ts ] [ key ] - self . prev_check [ key ] except KeyError : logger . debug ( 'There is no diff value for metric %s.\n' 'Timestamp: %s. Is it initial data?' , key , ts , exc_info = True ) value = 0 prepared_results [ decoded_key ] = value else : decoded_key = decoder . find_common_names ( key ) prepared_results [ decoded_key ] = value else : decoded_key = decoder . find_common_names ( key ) prepared_results [ decoded_key ] = value self . prev_check = jsn [ ts ] collect . append ( ( ts , prepared_results ) ) except ValueError : logger . error ( 'Telegraf agent send trash to output: %s' , chunk ) logger . debug ( 'Telegraf agent data block w/ trash: %s' , exc_info = True ) return [ ] except BaseException : logger . error ( 'Exception trying to parse agent data: %s' , chunk , exc_info = True ) return [ ] if collect : return collect
async def close ( self ) : if self . _ws is not None : await self . _ws . close ( ) if self . polygon is not None : await self . polygon . close ( )
def submit_order ( self , symbol , qty , side , type , time_in_force , limit_price = None , stop_price = None , client_order_id = None ) : params = { 'symbol' : symbol , 'qty' : qty , 'side' : side , 'type' : type , 'time_in_force' : time_in_force , } if limit_price is not None : params [ 'limit_price' ] = limit_price if stop_price is not None : params [ 'stop_price' ] = stop_price if client_order_id is not None : params [ 'client_order_id' ] = client_order_id resp = self . post ( '/orders' , params ) return Order ( resp )
def get_position ( self , symbol ) : resp = self . get ( '/positions/{}' . format ( symbol ) ) return Position ( resp )
def list_assets ( self , status = None , asset_class = None ) : params = { 'status' : status , 'assert_class' : asset_class , } resp = self . get ( '/assets' , params ) return [ Asset ( o ) for o in resp ]
def construct_event_logger ( event_record_callback ) : check . callable_param ( event_record_callback , 'event_record_callback' ) return construct_single_handler_logger ( 'event-logger' , DEBUG , StructuredLoggerHandler ( lambda logger_message : event_record_callback ( construct_event_record ( logger_message ) ) ) , )
def construct_json_event_logger ( json_path ) : check . str_param ( json_path , 'json_path' ) return construct_single_handler_logger ( "json-event-record-logger" , DEBUG , JsonEventLoggerHandler ( json_path , lambda record : construct_event_record ( StructuredLoggerMessage ( name = record . name , message = record . msg , level = record . levelno , meta = record . dagster_meta , record = record , ) ) , ) , )
def format_config_for_graphql ( config ) : def _format_config_subdict ( config , current_indent = 0 ) : check . dict_param ( config , 'config' , key_type = str ) printer = IndentingStringIoPrinter ( indent_level = 2 , current_indent = current_indent ) printer . line ( '{' ) n_elements = len ( config ) for i , key in enumerate ( sorted ( config , key = lambda x : x [ 0 ] ) ) : value = config [ key ] with printer . with_indent ( ) : formatted_value = ( _format_config_item ( value , current_indent = printer . current_indent ) . lstrip ( ' ' ) . rstrip ( '\n' ) ) printer . line ( '{key}: {formatted_value}{comma}' . format ( key = key , formatted_value = formatted_value , comma = ',' if i != n_elements - 1 else '' , ) ) printer . line ( '}' ) return printer . read ( ) def _format_config_sublist ( config , current_indent = 0 ) : printer = IndentingStringIoPrinter ( indent_level = 2 , current_indent = current_indent ) printer . line ( '[' ) n_elements = len ( config ) for i , value in enumerate ( config ) : with printer . with_indent ( ) : formatted_value = ( _format_config_item ( value , current_indent = printer . current_indent ) . lstrip ( ' ' ) . rstrip ( '\n' ) ) printer . line ( '{formatted_value}{comma}' . format ( formatted_value = formatted_value , comma = ',' if i != n_elements - 1 else '' ) ) printer . line ( ']' ) return printer . read ( ) def _format_config_item ( config , current_indent = 0 ) : printer = IndentingStringIoPrinter ( indent_level = 2 , current_indent = current_indent ) if isinstance ( config , dict ) : return _format_config_subdict ( config , printer . current_indent ) elif isinstance ( config , list ) : return _format_config_sublist ( config , printer . current_indent ) elif isinstance ( config , bool ) : return repr ( config ) . lower ( ) else : return repr ( config ) . replace ( '\'' , '"' ) check . dict_param ( config , 'config' , key_type = str ) if not isinstance ( config , dict ) : check . failed ( 'Expected a dict to format as config, got: {item}' . format ( item = repr ( config ) ) ) return _format_config_subdict ( config )
def execute_pipeline_through_queue ( repository_info , pipeline_name , solid_subset , environment_dict , run_id , message_queue , reexecution_config , step_keys_to_execute , ) : message_queue . put ( ProcessStartedSentinel ( os . getpid ( ) ) ) run_config = RunConfig ( run_id , event_callback = message_queue . put , executor_config = InProcessExecutorConfig ( raise_on_error = False ) , reexecution_config = reexecution_config , step_keys_to_execute = step_keys_to_execute , ) repository_container = RepositoryContainer ( repository_info ) if repository_container . repo_error : message_queue . put ( MultiprocessingError ( serializable_error_info_from_exc_info ( repository_container . repo_error ) ) ) return try : result = execute_pipeline ( repository_container . repository . get_pipeline ( pipeline_name ) . build_sub_pipeline ( solid_subset ) , environment_dict , run_config = run_config , ) return result except : error_info = serializable_error_info_from_exc_info ( sys . exc_info ( ) ) message_queue . put ( MultiprocessingError ( error_info ) ) finally : message_queue . put ( MultiprocessingDone ( ) ) message_queue . close ( )
def join ( self ) : while True : with self . _processes_lock : if not self . _processes and self . _processing_semaphore . locked ( ) : return True gevent . sleep ( 0.1 )
def build ( self , pipeline_def , artifacts_persisted ) : deps = { step . key : set ( ) for step in self . steps } for step in self . steps : for step_input in step . step_inputs : deps [ step . key ] . add ( step_input . prev_output_handle . step_key ) step_dict = { step . key : step for step in self . steps } return ExecutionPlan ( pipeline_def , step_dict , deps , artifacts_persisted )
def construct_publish_comands ( additional_steps = None , nightly = False ) : publish_commands = ( [ 'rm -rf dist' ] + ( additional_steps if additional_steps else [ ] ) + [ 'python setup.py sdist bdist_wheel{nightly}' . format ( nightly = ' --nightly' if nightly else '' ) , 'twine upload dist/*' , ] ) return publish_commands
def block ( self , text , prefix = '' ) : wrapper = TextWrapper ( width = self . line_length - len ( self . current_indent_str ) , initial_indent = prefix , subsequent_indent = prefix , break_long_words = False , break_on_hyphens = False , ) for line in wrapper . wrap ( text ) : self . line ( line )
def _define_shared_fields ( ) : clustering_fields = Field ( List ( String ) , description = , is_optional = True , ) create_disposition = Field ( BQCreateDisposition , description = , is_optional = True , ) destination_encryption_configuration = Field ( String , description = , is_optional = True , ) schema_update_options = Field ( List ( BQSchemaUpdateOption ) , description = , is_optional = True , ) time_partitioning = Field ( Dict ( fields = { 'expiration_ms' : Field ( Int , description = , is_optional = True , ) , 'field' : Field ( String , description = , is_optional = True , ) , 'require_partition_filter' : Field ( Bool , description = , is_optional = True , ) , } ) , description = 'Specifies time-based partitioning for the destination table.' , is_optional = True , ) write_disposition = Field ( BQWriteDisposition , description = , is_optional = True , ) return { 'clustering_fields' : clustering_fields , 'create_disposition' : create_disposition , 'destination_encryption_configuration' : destination_encryption_configuration , 'schema_update_options' : schema_update_options , 'time_partitioning' : time_partitioning , 'write_disposition' : write_disposition , }
def mkdir_p ( newdir , mode = 0o777 ) : try : os . makedirs ( newdir , mode ) except OSError as err : if err . errno != errno . EEXIST or not os . path . isdir ( newdir ) : raise
def success ( self ) : any_success = False for step_event in itertools . chain ( self . input_expectations , self . output_expectations , self . transforms ) : if step_event . event_type == DagsterEventType . STEP_FAILURE : return False if step_event . event_type == DagsterEventType . STEP_SUCCESS : any_success = True return any_success
def skipped ( self ) : return all ( [ step_event . event_type == DagsterEventType . STEP_SKIPPED for step_event in itertools . chain ( self . input_expectations , self . output_expectations , self . transforms ) ] )
def failure_data ( self ) : for result in itertools . chain ( self . input_expectations , self . output_expectations , self . transforms ) : if result . event_type == DagsterEventType . STEP_FAILURE : return result . step_failure_data
def _is_valid_dataset ( config_value ) : return re . match ( r'^' + RE_PROJECT + r'\.' + RE_DS_TABLE + r'$|^' + RE_DS_TABLE + r'$' , config_value , )
def _is_valid_table ( config_value ) : return re . match ( r'^' + RE_PROJECT + r'\.' + RE_DS_TABLE + r'\.' + RE_DS_TABLE + r'$|^' + RE_DS_TABLE + r'\.' + RE_DS_TABLE + r'$' , config_value , )
def coalesce_execution_steps ( execution_plan ) : solid_order = _coalesce_solid_order ( execution_plan ) steps = defaultdict ( list ) for solid_name , solid_steps in itertools . groupby ( execution_plan . topological_steps ( ) , lambda x : x . solid_name ) : steps [ solid_name ] += list ( solid_steps ) return OrderedDict ( [ ( solid_name , steps [ solid_name ] ) for solid_name in solid_order ] )
def create_cursor ( self , name = None ) : return Cursor ( self . client_connection , self . connection , self . djongo_connection )
def _close ( self ) : if self . connection : with self . wrap_database_errors : self . connection . client . close ( )
def make_mdl ( model , model_dict ) : for field_name in model_dict : field = model . _meta . get_field ( field_name ) model_dict [ field_name ] = field . to_python ( model_dict [ field_name ] ) return model ( * * model_dict )
def formfield ( self , * * kwargs ) : defaults = { 'form_class' : ArrayFormField , 'model_container' : self . model_container , 'model_form_class' : self . model_form_class , 'name' : self . attname , 'mdl_form_kw_l' : self . model_form_kwargs_l } defaults . update ( kwargs ) return super ( ) . formfield ( * * defaults )
def _apply_rel_filters ( self , queryset ) : queryset . _add_hints ( instance = self . instance ) if self . _db : queryset = queryset . using ( self . _db ) queryset = queryset . filter ( * * self . core_filters ) return queryset
def _calc_c ( self , a1 , a2 , r1 , r2 ) : if r1 == 0.0 and r2 == 0.0 : return a1 , a2 div = 1 / ( r1 + r2 ) c1 = ( a1 * r2 + a2 * r1 ) * div c2 = ( a1 * r1 + a2 * r2 ) * div return c1 , c2
def clear ( self ) : self . reg = np . zeros ( ( self . m , ) , dtype = np . int8 )
def index ( self ) : for i , hashtable in enumerate ( self . hashtables ) : self . sorted_hashtables [ i ] = [ H for H in hashtable . keys ( ) ] self . sorted_hashtables [ i ] . sort ( )
async def close ( self ) : async with self . _lock : for t in self . hashtables : await t . close ( ) if self . keys is not None : await self . keys . close ( ) self . _initialized = False
def parse_scoped_selector ( scoped_selector ) : if scoped_selector [ 0 ] == '%' : if scoped_selector . endswith ( '.value' ) : err_str = '{} is invalid cannot use % and end with .value' raise ValueError ( err_str . format ( scoped_selector ) ) scoped_selector = scoped_selector [ 1 : ] + '/macro.value' scope_selector_list = scoped_selector . rsplit ( '/' , 1 ) scope = '' . join ( scope_selector_list [ : - 1 ] ) selector = scope_selector_list [ - 1 ] return scope , selector
def advance_one_line ( self ) : current_line = self . _current_token . line_number while current_line == self . _current_token . line_number : self . _current_token = ConfigParser . Token ( * next ( self . _token_generator ) )
def augment_exception_message_and_reraise ( exception , message ) : class ExceptionProxy ( type ( exception ) ) : """Acts as a proxy for an exception with an augmented message.""" __module__ = type ( exception ) . __module__ def __init__ ( self ) : pass def __getattr__ ( self , attr_name ) : return getattr ( exception , attr_name ) def __str__ ( self ) : return str ( exception ) + message ExceptionProxy . __name__ = type ( exception ) . __name__ proxy = ExceptionProxy ( ) if six . PY3 : ExceptionProxy . __qualname__ = type ( exception ) . __qualname__ six . raise_from ( proxy . with_traceback ( exception . __traceback__ ) , None ) else : six . reraise ( proxy , None , sys . exc_info ( ) [ 2 ] )
def _markdownify_operative_config_str ( self , string ) : def process ( line ) : """Convert a single line to markdown format.""" if not line . startswith ( '#' ) : return '    ' + line line = line [ 2 : ] if line . startswith ( '====' ) : return '' if line . startswith ( 'None' ) : return if line . endswith ( ':' ) : return + line return line output_lines = [ ] for line in string . splitlines ( ) : procd_line = process ( line ) if procd_line is not None : output_lines . append ( procd_line ) return '\n' . join ( output_lines )
def after_create_session ( self , session = None , coord = None ) : config_str = config . operative_config_str ( ) if not tf . gfile . IsDirectory ( self . _output_dir ) : tf . gfile . MakeDirs ( self . _output_dir ) global_step_val = 0 if session is not None : global_step = tf . train . get_global_step ( ) if global_step is not None : global_step_val = session . run ( global_step ) filename = '%s-%s.gin' % ( self . _base_name , global_step_val ) config_path = os . path . join ( self . _output_dir , filename ) with tf . gfile . GFile ( config_path , 'w' ) as f : f . write ( config_str ) if self . _summarize_config : md_config_str = self . _markdownify_operative_config_str ( config_str ) summary_metadata = summary_pb2 . SummaryMetadata ( ) summary_metadata . plugin_data . plugin_name = 'text' summary_metadata . plugin_data . content = b'{}' text_tensor = tf . make_tensor_proto ( md_config_str ) summary = summary_pb2 . Summary ( ) summary . value . add ( tag = 'gin/' + self . _base_name , tensor = text_tensor , metadata = summary_metadata ) if not self . _summary_writer : self . _summary_writer = tf . summary . FileWriterCache . get ( self . _output_dir ) self . _summary_writer . add_summary ( summary , global_step_val ) self . _summary_writer . flush ( )
def _find_class_construction_fn ( cls ) : for base in type . mro ( cls ) : if '__init__' in base . __dict__ : return base . __init__ if '__new__' in base . __dict__ : return base . __new__
def _ensure_wrappability ( fn ) : if isinstance ( fn , ( type ( object . __init__ ) , type ( object . __call__ ) ) ) : wrappable_fn = lambda * args , * * kwargs : fn ( * args , * * kwargs ) wrappable_fn . __name__ = fn . __name__ wrappable_fn . __doc__ = fn . __doc__ wrappable_fn . __module__ = '' wrappable_fn . __wrapped__ = fn return wrappable_fn return fn
def _get_cached_arg_spec ( fn ) : arg_spec = _ARG_SPEC_CACHE . get ( fn ) if arg_spec is None : arg_spec_fn = inspect . getfullargspec if six . PY3 else inspect . getargspec try : arg_spec = arg_spec_fn ( fn ) except TypeError : arg_spec = arg_spec_fn ( fn . __call__ ) _ARG_SPEC_CACHE [ fn ] = arg_spec return arg_spec
def _get_supplied_positional_parameter_names ( fn , args ) : arg_spec = _get_cached_arg_spec ( fn ) return arg_spec . args [ : len ( args ) ]
def _get_all_positional_parameter_names ( fn ) : arg_spec = _get_cached_arg_spec ( fn ) args = arg_spec . args if arg_spec . defaults : args = args [ : - len ( arg_spec . defaults ) ] return args
def parse_value ( value ) : if not isinstance ( value , six . string_types ) : raise ValueError ( 'value ({}) should be a string type.' . format ( value ) ) return config_parser . ConfigParser ( value , ParserDelegate ( ) ) . parse_value ( )
def _iterate_flattened_values ( value ) : if isinstance ( value , six . string_types ) : yield value return if isinstance ( value , collections . Mapping ) : value = collections . ValuesView ( value ) if isinstance ( value , collections . Iterable ) : for nested_value in value : for nested_nested_value in _iterate_flattened_values ( nested_value ) : yield nested_nested_value yield value
def get_all_matches ( self , partial_selector ) : matching_selectors = self . matching_selectors ( partial_selector ) return [ self . _selector_map [ selector ] for selector in matching_selectors ]
def sp_search_query ( query ) : result = [ ] for ( field , values ) in query . items ( ) : field = SEARCH_FIELD_MAP . get ( field , field ) if field is None : continue for value in values : if field == 'year' : value = _transform_year ( value ) if value is not None : result . append ( '%s:%d' % ( field , value ) ) elif field == 'any' : result . append ( '"%s"' % value ) else : result . append ( '%s:"%s"' % ( field , value ) ) return ' ' . join ( result )
def _parse_retry_after ( self , response ) : value = response . headers . get ( 'Retry-After' ) if not value : seconds = 0 elif re . match ( r'^\s*[0-9]+\s*$' , value ) : seconds = int ( value ) else : date_tuple = email . utils . parsedate ( value ) if date_tuple is None : seconds = 0 else : seconds = time . mktime ( date_tuple ) - time . time ( ) return max ( 0 , seconds )
def set_default_headers ( self , * args , * * kwargs ) : self . set_header ( 'Access-Control-Allow-Origin' , '*' ) self . set_header ( 'Access-Control-Allow-Headers' , 'Origin, X-Requested-With, Content-Type, Accept' ) self . set_header ( 'Access-Control-Allow-Methods' , 'GET, HEAD, PUT, POST, DELETE' )
def prepare ( self ) : host = self . request . headers . get ( 'Host' , None ) if host is not None and host in self . hosts : return raise tornado . web . HTTPError ( 403 )
def start ( self ) : self . service_info = ServiceInfo ( '_webthing._tcp.local.' , '{}._webthing._tcp.local.' . format ( self . name ) , address = socket . inet_aton ( get_ip ( ) ) , port = self . port , properties = { 'path' : '/' , } , server = '{}.local.' . format ( socket . gethostname ( ) ) ) self . zeroconf = Zeroconf ( ) self . zeroconf . register_service ( self . service_info ) self . server . listen ( self . port ) tornado . ioloop . IOLoop . current ( ) . start ( )
def start ( self ) : self . status = 'pending' self . thing . action_notify ( self ) self . perform_action ( ) self . finish ( )
def finish ( self ) : self . status = 'completed' self . time_completed = timestamp ( ) self . thing . action_notify ( self )
def update ( self , * * fields ) : self . _for_write = True if django . VERSION >= ( 2 , 0 ) : query = self . query . chain ( UpdateQuery ) else : query = self . query . clone ( UpdateQuery ) query . _annotations = None query . add_update_values ( fields ) connection = django . db . connections [ self . db ] compiler = PostgresReturningUpdateCompiler ( query , connection , self . db ) with transaction . atomic ( using = self . db , savepoint = False ) : rows = compiler . execute_sql ( CURSOR ) self . _result_cache = None for row in rows : signals . update . send ( self . model , pk = row [ 0 ] ) return len ( rows )
def _on_model_save ( sender , * * kwargs ) : created , instance = kwargs [ 'created' ] , kwargs [ 'instance' ] if created : signals . create . send ( sender , pk = instance . pk ) else : signals . update . send ( sender , pk = instance . pk )
def _on_model_delete ( sender , * * kwargs ) : instance = kwargs [ 'instance' ] signals . delete . send ( sender , pk = instance . pk )
def resolve_expression ( self , * args , * * kwargs ) : result = dict ( ) for key , value in self . value . items ( ) : if hasattr ( value , 'resolve_expression' ) : result [ key ] = value . resolve_expression ( * args , * * kwargs ) else : result [ key ] = value return HStoreValue ( result )
def as_sql ( self , compiler , connection ) : qn = compiler . quote_name_unless_alias return "%s.%s->'%s'" % ( qn ( self . alias ) , qn ( self . target . column ) , self . hstore_key ) , [ ]
def relabeled_clone ( self , relabels ) : return self . __class__ ( relabels . get ( self . alias , self . alias ) , self . target , self . hstore_key , self . output_field )
def as_sql ( self , compiler , connection ) : sql , params = super ( ) . as_sql ( compiler , connection ) return 'EXTRACT(epoch FROM {})' . format ( sql ) , params
def create_model ( self , model ) : for field in model . _meta . local_fields : if not isinstance ( field , HStoreField ) : continue self . add_field ( model , field )
def delete_model ( self , model ) : for field in model . _meta . local_fields : if not isinstance ( field , HStoreField ) : continue self . remove_field ( model , field )
def alter_db_table ( self , model , old_db_table , new_db_table ) : for field in model . _meta . local_fields : if not isinstance ( field , HStoreField ) : continue for key in self . _iterate_required_keys ( field ) : self . _rename_hstore_required ( old_db_table , new_db_table , field , field , key )
def add_field ( self , model , field ) : for key in self . _iterate_required_keys ( field ) : self . _create_hstore_required ( model . _meta . db_table , field , key )
def remove_field ( self , model , field ) : for key in self . _iterate_required_keys ( field ) : self . _drop_hstore_required ( model . _meta . db_table , field , key )
def alter_field ( self , model , old_field , new_field , strict = False ) : is_old_field_hstore = isinstance ( old_field , HStoreField ) is_new_field_hstore = isinstance ( new_field , HStoreField ) if not is_old_field_hstore and not is_new_field_hstore : return old_required = getattr ( old_field , 'required' , [ ] ) or [ ] new_required = getattr ( new_field , 'required' , [ ] ) or [ ] if str ( old_field . column ) != str ( new_field . column ) : for key in self . _iterate_required_keys ( old_field ) : self . _rename_hstore_required ( model . _meta . db_table , model . _meta . db_table , old_field , new_field , key ) for key in old_required : if key not in new_required : self . _drop_hstore_required ( model . _meta . db_table , old_field , key ) for key in new_required : if key not in old_required : self . _create_hstore_required ( model . _meta . db_table , new_field , key )
def _create_hstore_required ( self , table_name , field , key ) : name = self . _required_constraint_name ( table_name , field , key ) sql = self . sql_hstore_required_create . format ( name = self . quote_name ( name ) , table = self . quote_name ( table_name ) , field = self . quote_name ( field . column ) , key = key ) self . execute ( sql )
def _drop_hstore_required ( self , table_name , field , key ) : name = self . _required_constraint_name ( table_name , field , key ) sql = self . sql_hstore_required_drop . format ( table = self . quote_name ( table_name ) , name = self . quote_name ( name ) ) self . execute ( sql )
def create_sql ( self , model , schema_editor , using = '' ) : if django . VERSION >= ( 2 , 0 ) : statement = super ( ) . create_sql ( model , schema_editor , using ) statement . template = self . sql_create_index statement . parts [ 'condition' ] = self . condition return statement else : sql_create_index = self . sql_create_index sql_parameters = { * * Index . get_sql_create_template_values ( self , model , schema_editor , using ) , 'condition' : self . condition } return sql_create_index % sql_parameters
def create_command ( text , commands ) : class CustomCommand ( BaseCommand ) : description = text def run ( self ) : for cmd in commands : subprocess . check_call ( cmd ) return CustomCommand
def create_model ( self , model ) : super ( ) . create_model ( model ) for mixin in self . post_processing_mixins : mixin . create_model ( model )
def delete_model ( self , model ) : for mixin in self . post_processing_mixins : mixin . delete_model ( model ) super ( ) . delete_model ( model )
def alter_db_table ( self , model , old_db_table , new_db_table ) : super ( SchemaEditor , self ) . alter_db_table ( model , old_db_table , new_db_table ) for mixin in self . post_processing_mixins : mixin . alter_db_table ( model , old_db_table , new_db_table )
def add_field ( self , model , field ) : super ( SchemaEditor , self ) . add_field ( model , field ) for mixin in self . post_processing_mixins : mixin . add_field ( model , field )
def remove_field ( self , model , field ) : for mixin in self . post_processing_mixins : mixin . remove_field ( model , field ) super ( SchemaEditor , self ) . remove_field ( model , field )
def alter_field ( self , model , old_field , new_field , strict = False ) : super ( SchemaEditor , self ) . alter_field ( model , old_field , new_field , strict ) for mixin in self . post_processing_mixins : mixin . alter_field ( model , old_field , new_field , strict )
def _form_returning ( self ) : qn = self . connection . ops . quote_name return ' RETURNING %s' % qn ( self . query . model . _meta . pk . attname )
def as_sql ( self , return_id = False ) : queries = [ self . _rewrite_insert ( sql , params , return_id ) for sql , params in super ( ) . as_sql ( ) ] return queries
def alter_db_table ( self , model , old_db_table , new_db_table ) : for field in model . _meta . local_fields : if not isinstance ( field , HStoreField ) : continue for keys in self . _iterate_uniqueness_keys ( field ) : self . _rename_hstore_unique ( old_db_table , new_db_table , field , field , keys )
def add_field ( self , model , field ) : for keys in self . _iterate_uniqueness_keys ( field ) : self . _create_hstore_unique ( model , field , keys )
def remove_field ( self , model , field ) : for keys in self . _iterate_uniqueness_keys ( field ) : self . _drop_hstore_unique ( model , field , keys )
def alter_field ( self , model , old_field , new_field , strict = False ) : is_old_field_hstore = isinstance ( old_field , HStoreField ) is_new_field_hstore = isinstance ( new_field , HStoreField ) if not is_old_field_hstore and not is_new_field_hstore : return old_uniqueness = getattr ( old_field , 'uniqueness' , [ ] ) or [ ] new_uniqueness = getattr ( new_field , 'uniqueness' , [ ] ) or [ ] if str ( old_field . column ) != str ( new_field . column ) : for keys in self . _iterate_uniqueness_keys ( old_field ) : self . _rename_hstore_unique ( model . _meta . db_table , model . _meta . db_table , old_field , new_field , keys ) for keys in old_uniqueness : if keys not in new_uniqueness : self . _drop_hstore_unique ( model , old_field , self . _compose_keys ( keys ) ) for keys in new_uniqueness : if keys not in old_uniqueness : self . _create_hstore_unique ( model , new_field , self . _compose_keys ( keys ) )
def _create_hstore_unique ( self , model , field , keys ) : name = self . _unique_constraint_name ( model . _meta . db_table , field , keys ) columns = [ '(%s->\'%s\')' % ( field . column , key ) for key in keys ] sql = self . sql_hstore_unique_create . format ( name = self . quote_name ( name ) , table = self . quote_name ( model . _meta . db_table ) , columns = ',' . join ( columns ) ) self . execute ( sql )
def _drop_hstore_unique ( self , model , field , keys ) : name = self . _unique_constraint_name ( model . _meta . db_table , field , keys ) sql = self . sql_hstore_unique_drop . format ( name = self . quote_name ( name ) ) self . execute ( sql )
def as_sql ( self , compiler , connection ) -> Tuple [ str , List [ Any ] ] : sql , params = super ( ) . as_sql ( compiler , connection ) qn = compiler . quote_name_unless_alias extra_conditions = ' AND ' . join ( [ '{}.{} = %s' . format ( qn ( self . table_name ) , qn ( field . column ) ) for field , value in self . extra_conditions ] ) for _ , value in self . extra_conditions : params . append ( value ) rewritten_sql = sql . replace ( ')' , ' AND {})' . format ( extra_conditions ) ) return rewritten_sql , params
def select ( self , board ) : if self . unexplored : i = random . randrange ( len ( self . unexplored ) ) pos = self . unexplored [ i ] self . unexplored [ i ] = self . unexplored [ len ( self . unexplored ) - 1 ] self . unexplored . pop ( ) return pos elif self . bestchild : return self . bestchild . pos else : return PASS
def random_playout ( self , board ) : for x in range ( MAXMOVES ) : if board . finished : break board . move ( board . random_move ( ) )
def GetDomain ( self ) : return ( self . knots [ self . degree - 1 ] , self . knots [ len ( self . knots ) - self . degree ] )
def _parse_posts ( self , raw_posts ) : parsed_posts = self . parse_json ( raw_posts ) for post_id in parsed_posts [ 'order' ] : yield parsed_posts [ 'posts' ] [ post_id ]
def posts ( self , channel , page = None ) : entrypoint = self . RCHANNELS + '/' + channel + '/' + self . RPOSTS params = { self . PPER_PAGE : self . max_items } if page is not None : params [ self . PPAGE ] = page response = self . _fetch ( entrypoint , params ) return response
def user ( self , user ) : entrypoint = self . RUSERS + '/' + user response = self . _fetch ( entrypoint , None ) return response
def _pre_init ( self ) : if not self . parsed_args . mboxes_path : base_path = os . path . expanduser ( '~/.perceval/mailinglists/' ) dirpath = os . path . join ( base_path , self . parsed_args . url ) else : dirpath = self . parsed_args . mboxes_path setattr ( self . parsed_args , 'dirpath' , dirpath )
def setup_cmd_parser ( cls ) : parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , archive = True ) parser . parser . add_argument ( 'url' , help = "URL of the RSS feed" ) return parser
def __fetch_merge_requests ( self , from_date ) : merges_groups = self . client . merges ( from_date = from_date ) for raw_merges in merges_groups : merges = json . loads ( raw_merges ) for merge in merges : merge_id = merge [ 'iid' ] if self . blacklist_ids and merge_id in self . blacklist_ids : logger . warning ( "Skipping blacklisted merge request %s" , merge_id ) continue merge_full_raw = self . client . merge ( merge_id ) merge_full = json . loads ( merge_full_raw ) self . __init_merge_extra_fields ( merge_full ) merge_full [ 'notes_data' ] = self . __get_merge_notes ( merge_id ) merge_full [ 'award_emoji_data' ] = self . __get_award_emoji ( GitLabClient . MERGES , merge_id ) merge_full [ 'versions_data' ] = self . __get_merge_versions ( merge_id ) yield merge_full
def issues ( self , from_date = None ) : payload = { 'state' : 'all' , 'order_by' : 'updated_at' , 'sort' : 'asc' , 'per_page' : PER_PAGE } if from_date : payload [ 'updated_after' ] = from_date . isoformat ( ) return self . fetch_items ( GitLabClient . ISSUES , payload )
def merges ( self , from_date = None ) : payload = { 'state' : 'all' , 'order_by' : 'updated_at' , 'sort' : 'asc' , 'view' : 'simple' , 'per_page' : PER_PAGE } if from_date : payload [ 'updated_after' ] = from_date . isoformat ( ) return self . fetch_items ( GitLabClient . MERGES , payload )
def merge ( self , merge_id ) : path = urijoin ( self . base_url , GitLabClient . PROJECTS , self . owner + '%2F' + self . repository , GitLabClient . MERGES , merge_id ) response = self . fetch ( path ) return response . text
def merge_versions ( self , merge_id ) : payload = { 'order_by' : 'updated_at' , 'sort' : 'asc' , 'per_page' : PER_PAGE } path = urijoin ( GitLabClient . MERGES , str ( merge_id ) , GitLabClient . VERSIONS ) return self . fetch_items ( path , payload )
def merge_version ( self , merge_id , version_id ) : path = urijoin ( self . base_url , GitLabClient . PROJECTS , self . owner + '%2F' + self . repository , GitLabClient . MERGES , merge_id , GitLabClient . VERSIONS , version_id ) response = self . fetch ( path ) return response . text
def notes ( self , item_type , item_id ) : payload = { 'order_by' : 'updated_at' , 'sort' : 'asc' , 'per_page' : PER_PAGE } path = urijoin ( item_type , str ( item_id ) , GitLabClient . NOTES ) return self . fetch_items ( path , payload )
def emojis ( self , item_type , item_id ) : payload = { 'order_by' : 'updated_at' , 'sort' : 'asc' , 'per_page' : PER_PAGE } path = urijoin ( item_type , str ( item_id ) , GitLabClient . EMOJI ) return self . fetch_items ( path , payload )
def note_emojis ( self , item_type , item_id , note_id ) : payload = { 'order_by' : 'updated_at' , 'sort' : 'asc' , 'per_page' : PER_PAGE } path = urijoin ( item_type , str ( item_id ) , GitLabClient . NOTES , str ( note_id ) , GitLabClient . EMOJI ) return self . fetch_items ( path , payload )
def fetch_items ( self , path , payload ) : page = 0 last_page = None url_next = urijoin ( self . base_url , GitLabClient . PROJECTS , self . owner + '%2F' + self . repository , path ) logger . debug ( "Get GitLab paginated items from " + url_next ) response = self . fetch ( url_next , payload = payload ) items = response . text page += 1 if 'last' in response . links : last_url = response . links [ 'last' ] [ 'url' ] last_page = last_url . split ( '&page=' ) [ 1 ] . split ( '&' ) [ 0 ] last_page = int ( last_page ) logger . debug ( "Page: %i/%i" % ( page , last_page ) ) while items : yield items items = None if 'next' in response . links : url_next = response . links [ 'next' ] [ 'url' ] response = self . fetch ( url_next , payload = payload ) page += 1 items = response . text logger . debug ( "Page: %i/%i" % ( page , last_page ) )
def _init_rate_limit ( self ) : url = urijoin ( self . base_url , 'projects' , self . owner + '%2F' + self . repository ) try : response = super ( ) . fetch ( url ) self . update_rate_limit ( response ) except requests . exceptions . HTTPError as error : if error . response . status_code == 401 : raise error else : logger . warning ( "Rate limit not initialized: %s" , error )
def setup_cmd_parser ( cls ) : parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = True , token_auth = True , archive = True ) group = parser . parser . add_argument_group ( 'GitLab arguments' ) group . add_argument ( '--enterprise-url' , dest = 'base_url' , help = "Base URL for GitLab Enterprise instance" ) group . add_argument ( '--sleep-for-rate' , dest = 'sleep_for_rate' , action = 'store_true' , help = "sleep for getting more rate" ) group . add_argument ( '--min-rate-to-sleep' , dest = 'min_rate_to_sleep' , default = MIN_RATE_LIMIT , type = int , help = ) group . add_argument ( '--blacklist-ids' , dest = 'blacklist_ids' , nargs = '*' , type = int , help = "Ids of items that must not be retrieved." ) group . add_argument ( '--max-retries' , dest = 'max_retries' , default = MAX_RETRIES , type = int , help = "number of API call retries" ) group . add_argument ( '--sleep-time' , dest = 'sleep_time' , default = DEFAULT_SLEEP_TIME , type = int , help = "sleeping time between API call retries" ) parser . parser . add_argument ( 'owner' , help = "GitLab owner" ) parser . parser . add_argument ( 'repository' , help = "GitLab repository" ) return parser
def channel_info ( self , channel ) : resource = self . RCHANNEL_INFO params = { self . PCHANNEL : channel , } response = self . _fetch ( resource , params ) return response
def history ( self , channel , oldest = None , latest = None ) : resource = self . RCHANNEL_HISTORY params = { self . PCHANNEL : channel , self . PCOUNT : self . max_items } if oldest is not None : params [ self . POLDEST ] = oldest if latest is not None : params [ self . PLATEST ] = latest response = self . _fetch ( resource , params ) return response
def user ( self , user_id ) : resource = self . RUSER_INFO params = { self . PUSER : user_id } response = self . _fetch ( resource , params ) return response
def setup_cmd_parser ( cls ) : parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = True , token_auth = True , archive = True ) action = parser . parser . _option_string_actions [ '--api-token' ] action . required = True group = parser . parser . add_argument_group ( 'Slack arguments' ) group . add_argument ( '--max-items' , dest = 'max_items' , type = int , default = MAX_ITEMS , help = "Maximum number of items requested on the same query" ) parser . parser . add_argument ( 'channel' , help = "Slack channel identifier" ) return parser
def logout ( self ) : params = { self . PLOGOUT : '1' } self . call ( self . CGI_LOGIN , params ) self . _close_http_session ( ) logger . debug ( "Bugzilla user logged out from %s" , self . base_url )
def metadata ( self ) : params = { self . PCTYPE : self . CTYPE_XML } response = self . call ( self . CGI_BUG , params ) return response
def events ( self , group , from_date = DEFAULT_DATETIME ) : date = datetime_to_utc ( from_date ) date = date . strftime ( "since:%Y-%m-%dT%H:%M:%S.000Z" ) resource = urijoin ( group , self . REVENTS ) fixed_params = '?' + self . PFIELDS + '=' + ',' . join ( self . VEVENT_FIELDS ) fixed_params += '&' + self . PSTATUS + '=' + ',' . join ( self . VSTATUS ) resource += fixed_params params = { self . PORDER : self . VUPDATED , self . PSCROLL : date , self . PPAGE : self . max_items } try : for page in self . _fetch ( resource , params ) : yield page except requests . exceptions . HTTPError as error : if error . response . status_code == 410 : msg = "Group is no longer accessible: {}" . format ( error ) raise RepositoryError ( cause = msg ) else : raise error
def comments ( self , group , event_id ) : resource = urijoin ( group , self . REVENTS , event_id , self . RCOMMENTS ) params = { self . PPAGE : self . max_items } for page in self . _fetch ( resource , params ) : yield page
def rsvps ( self , group , event_id ) : resource = urijoin ( group , self . REVENTS , event_id , self . RRSVPS ) fixed_params = '?' + self . PFIELDS + '=' + ',' . join ( self . VRSVP_FIELDS ) fixed_params += '&' + self . PRESPONSE + '=' + ',' . join ( self . VRESPONSE ) resource += fixed_params params = { self . PPAGE : self . max_items } for page in self . _fetch ( resource , params ) : yield page
def parse_reviews ( raw_data ) : items_raw = "[" + raw_data . replace ( "\n" , "," ) + "]" items_raw = items_raw . replace ( ",]" , "]" ) items = json . loads ( items_raw ) reviews = [ ] for item in items : if 'project' in item . keys ( ) : reviews . append ( item ) return reviews
def version ( self ) : if self . _version : return self . _version cmd = self . gerrit_cmd + " %s " % ( GerritClient . CMD_VERSION ) logger . debug ( "Getting version: %s" % ( cmd ) ) raw_data = self . __execute ( cmd ) raw_data = str ( raw_data , "UTF-8" ) logger . debug ( "Gerrit version: %s" % ( raw_data ) ) m = re . match ( GerritClient . VERSION_REGEX , raw_data ) if not m : cause = "Invalid gerrit version %s" % raw_data raise BackendError ( cause = cause ) try : mayor = int ( m . group ( 1 ) ) minor = int ( m . group ( 2 ) ) except Exception : cause = "Gerrit client could not determine the server version." raise BackendError ( cause = cause ) self . _version = [ mayor , minor ] return self . _version
def reviews ( self , last_item , filter_ = None ) : cmd = self . _get_gerrit_cmd ( last_item , filter_ ) logger . debug ( "Getting reviews with command: %s" , cmd ) raw_data = self . __execute ( cmd ) raw_data = str ( raw_data , "UTF-8" ) return raw_data
def next_retrieve_group_item ( self , last_item = None , entry = None ) : next_item = None gerrit_version = self . version if gerrit_version [ 0 ] == 2 and gerrit_version [ 1 ] > 9 : if last_item is None : next_item = 0 else : next_item = last_item elif gerrit_version [ 0 ] == 2 and gerrit_version [ 1 ] == 9 : cause = "Gerrit 2.9.0 does not support pagination" raise BackendError ( cause = cause ) else : if entry is not None : next_item = entry [ 'sortKey' ] return next_item
def __execute_from_archive ( self , cmd ) : cmd = self . sanitize_for_archive ( cmd ) response = self . archive . retrieve ( cmd , None , None ) if isinstance ( response , RuntimeError ) : raise response return response
def __execute_from_remote ( self , cmd ) : result = None retries = 0 while retries < self . MAX_RETRIES : try : result = subprocess . check_output ( cmd , shell = True ) break except subprocess . CalledProcessError as ex : logger . error ( "gerrit cmd %s failed: %s" , cmd , ex ) time . sleep ( self . RETRY_WAIT * retries ) retries += 1 if result is None : result = RuntimeError ( cmd + " failed " + str ( self . MAX_RETRIES ) + " times. Giving up!" ) if self . archive : cmd = self . sanitize_for_archive ( cmd ) self . archive . store ( cmd , None , None , result ) if isinstance ( result , RuntimeError ) : raise result return result
def setup_cmd_parser ( cls ) : parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = True , archive = True ) group = parser . parser . add_argument_group ( 'Gerrit arguments' ) group . add_argument ( '--user' , dest = 'user' , help = "Gerrit ssh user" ) group . add_argument ( '--max-reviews' , dest = 'max_reviews' , type = int , default = MAX_REVIEWS , help = "Max number of reviews per ssh query." ) group . add_argument ( '--blacklist-reviews' , dest = 'blacklist_reviews' , nargs = '*' , help = "Wrong reviews that must not be retrieved." ) group . add_argument ( '--disable-host-key-check' , dest = 'disable_host_key_check' , action = 'store_true' , help = "Don't check remote host identity" ) group . add_argument ( '--ssh-port' , dest = 'port' , default = PORT , type = int , help = "Set SSH port of the Gerrit server" ) parser . parser . add_argument ( 'hostname' , help = "Hostname of the Gerrit server" ) return parser
def __fetch_issue_data ( self , issue_id ) : raw_issue = self . client . issue ( issue_id ) issue = json . loads ( raw_issue ) return issue
def __fetch_issue_attachments ( self , issue_id ) : for attachments_raw in self . client . issue_collection ( issue_id , "attachments" ) : attachments = json . loads ( attachments_raw ) for attachment in attachments [ 'entries' ] : yield attachment
def __fetch_issue_messages ( self , issue_id ) : for messages_raw in self . client . issue_collection ( issue_id , "messages" ) : messages = json . loads ( messages_raw ) for msg in messages [ 'entries' ] : msg [ 'owner_data' ] = self . __fetch_user_data ( '{OWNER}' , msg [ 'owner_link' ] ) yield msg
def __fetch_issue_activities ( self , issue_id ) : for activities_raw in self . client . issue_collection ( issue_id , "activity" ) : activities = json . loads ( activities_raw ) for act in activities [ 'entries' ] : act [ 'person_data' ] = self . __fetch_user_data ( '{PERSON}' , act [ 'person_link' ] ) yield act
def __fetch_user_data ( self , tag_type , user_link ) : user_name = self . client . user_name ( user_link ) user = { } if not user_name : return user user_raw = self . client . user ( user_name ) user = json . loads ( user_raw ) return user
def issues ( self , start = None ) : payload = self . __build_payload ( size = self . items_per_page , operation = True , startdate = start ) path = self . __get_url_project ( ) return self . __fetch_items ( path = path , payload = payload )
def user ( self , user_name ) : user = None if user_name in self . _users : return self . _users [ user_name ] url_user = self . __get_url ( "~" + user_name ) logger . info ( "Getting info for %s" % ( url_user ) ) try : raw_user = self . __send_request ( url_user ) user = raw_user except requests . exceptions . HTTPError as e : if e . response . status_code in [ 404 , 410 ] : logger . warning ( "Data is not available - %s" , url_user ) user = '{}' else : raise e self . _users [ user_name ] = user return user
def issue ( self , issue_id ) : path = urijoin ( "bugs" , str ( issue_id ) ) url_issue = self . __get_url ( path ) raw_text = self . __send_request ( url_issue ) return raw_text
def issue_collection ( self , issue_id , collection_name ) : path = urijoin ( "bugs" , str ( issue_id ) , collection_name ) url_collection = self . __get_url ( path ) payload = { 'ws.size' : self . items_per_page , 'ws.start' : 0 , 'order_by' : 'date_last_updated' } raw_items = self . __fetch_items ( path = url_collection , payload = payload ) return raw_items
def __fetch_items ( self , path , payload ) : page = 0 url_next = path fetch_data = True while fetch_data : logger . debug ( "Fetching page: %i" , page ) try : raw_content = self . __send_request ( url_next , payload ) content = json . loads ( raw_content ) except requests . exceptions . HTTPError as e : if e . response . status_code in [ 410 ] : logger . warning ( "Data is not available - %s" , url_next ) raw_content = '{"total_size": 0, "start": 0, "entries": []}' content = json . loads ( raw_content ) else : raise e if 'next_collection_link' in content : url_next = content [ 'next_collection_link' ] payload = None else : fetch_data = False yield raw_content page += 1
def __find_group_id ( self ) : group_subscriptions = self . subscriptions ( self . auth ) for subscriptions in group_subscriptions : for sub in subscriptions : if sub [ 'group_name' ] == self . group_name : return sub [ 'group_id' ] msg = "Group id not found for group name %s" % self . group_name raise BackendError ( cause = msg )
def __fetch ( self , url , payload ) : r = requests . get ( url , params = payload , auth = self . auth , verify = self . verify ) try : r . raise_for_status ( ) except requests . exceptions . HTTPError as e : raise e return r
def _pre_init ( self ) : if not self . parsed_args . mboxes_path : base_path = os . path . expanduser ( '~/.perceval/mailinglists/' ) dirpath = os . path . join ( base_path , GROUPSIO_URL , 'g' , self . parsed_args . group_name ) else : dirpath = self . parsed_args . mboxes_path setattr ( self . parsed_args , 'dirpath' , dirpath )
def setup_cmd_parser ( cls ) : parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = True , token_auth = True ) action = parser . parser . _option_string_actions [ '--api-token' ] action . required = True group = parser . parser . add_argument_group ( 'Groupsio arguments' ) group . add_argument ( '--mboxes-path' , dest = 'mboxes_path' , help = "Path where mbox files will be stored" ) group . add_argument ( '--no-verify' , dest = 'verify' , action = 'store_false' , help = "Value 'True' enable SSL verification" ) parser . parser . add_argument ( 'group_name' , help = "Name of the group on Groups.io" ) return parser
def _set_auth_arguments ( self , basic_auth = True , token_auth = False ) : group = self . parser . add_argument_group ( 'authentication arguments' ) if basic_auth : group . add_argument ( '-u' , '--backend-user' , dest = 'user' , help = "backend user" ) group . add_argument ( '-p' , '--backend-password' , dest = 'password' , help = "backend password" ) if token_auth : group . add_argument ( '-t' , '--api-token' , dest = 'api_token' , help = "backend authentication token / API key" )
def _set_archive_arguments ( self ) : group = self . parser . add_argument_group ( 'archive arguments' ) group . add_argument ( '--archive-path' , dest = 'archive_path' , default = None , help = "directory path to the archives" ) group . add_argument ( '--no-archive' , dest = 'no_archive' , action = 'store_true' , help = "do not archive data" ) group . add_argument ( '--fetch-archive' , dest = 'fetch_archive' , action = 'store_true' , help = "fetch data from the archives" ) group . add_argument ( '--archived-since' , dest = 'archived_since' , default = '1970-01-01' , help = "retrieve items archived since the given date" )
def _set_output_arguments ( self ) : group = self . parser . add_argument_group ( 'output arguments' ) group . add_argument ( '-o' , '--output' , type = argparse . FileType ( 'w' ) , dest = 'outfile' , default = sys . stdout , help = "output file" ) group . add_argument ( '--json-line' , dest = 'json_line' , action = 'store_true' , help = "produce a JSON line for each output item" )
def _initialize_archive ( self ) : if 'archive_path' not in self . parsed_args : manager = None elif self . parsed_args . no_archive : manager = None else : if not self . parsed_args . archive_path : archive_path = os . path . expanduser ( ARCHIVES_DEFAULT_PATH ) else : archive_path = self . parsed_args . archive_path manager = ArchiveManager ( archive_path ) self . archive_manager = manager
def _fetch_and_parse_messages ( self , mailing_list , from_date ) : from_date = datetime_to_utc ( from_date ) nmsgs , imsgs , tmsgs = ( 0 , 0 , 0 ) for mbox in mailing_list . mboxes : tmp_path = None try : tmp_path = self . _copy_mbox ( mbox ) for message in self . parse_mbox ( tmp_path ) : tmsgs += 1 if not self . _validate_message ( message ) : imsgs += 1 continue dt = str_to_datetime ( message [ MBox . DATE_FIELD ] ) if dt < from_date : logger . debug ( "Message %s sent before %s; skipped" , message [ 'unixfrom' ] , str ( from_date ) ) tmsgs -= 1 continue message = self . _casedict_to_dict ( message ) nmsgs += 1 logger . debug ( "Message %s parsed" , message [ 'unixfrom' ] ) yield message except ( OSError , EOFError ) as e : logger . warning ( "Ignoring %s mbox due to: %s" , mbox . filepath , str ( e ) ) except Exception as e : if tmp_path and os . path . exists ( tmp_path ) : os . remove ( tmp_path ) raise e finally : if tmp_path and os . path . exists ( tmp_path ) : os . remove ( tmp_path ) logger . info ( "Done. %s/%s messages fetched; %s ignored" , nmsgs , tmsgs , imsgs )
def _copy_mbox ( self , mbox ) : tmp_path = tempfile . mktemp ( prefix = 'perceval_' ) with mbox . container as f_in : with open ( tmp_path , mode = 'wb' ) as f_out : for l in f_in : f_out . write ( l ) return tmp_path
def _validate_message ( self , message ) : if self . MESSAGE_ID_FIELD not in message : logger . warning ( "Field 'Message-ID' not found in message %s; ignoring" , message [ 'unixfrom' ] ) return False if not message [ self . MESSAGE_ID_FIELD ] : logger . warning ( "Field 'Message-ID' is empty in message %s; ignoring" , message [ 'unixfrom' ] ) return False if self . DATE_FIELD not in message : logger . warning ( "Field 'Date' not found in message %s; ignoring" , message [ 'unixfrom' ] ) return False if not message [ self . DATE_FIELD ] : logger . warning ( "Field 'Date' is empty in message %s; ignoring" , message [ 'unixfrom' ] ) return False try : str_to_datetime ( message [ self . DATE_FIELD ] ) except InvalidDateError : logger . warning ( "Invalid date %s in message %s; ignoring" , message [ self . DATE_FIELD ] , message [ 'unixfrom' ] ) return False return True
def get_message ( self , key ) : start , stop = self . _lookup ( key ) self . _file . seek ( start ) from_line = self . _file . readline ( ) . replace ( mailbox . linesep , b'' ) string = self . _file . read ( stop - self . _file . tell ( ) ) msg = self . _message_factory ( string . replace ( mailbox . linesep , b'\n' ) ) try : msg . set_from ( from_line [ 5 : ] . decode ( 'ascii' ) ) return msg except UnicodeDecodeError : pass try : msg . set_from ( from_line [ 5 : ] . decode ( 'utf-8' ) ) except UnicodeDecodeError : msg . set_from ( from_line [ 5 : ] . decode ( 'iso-8859-1' ) ) return msg
def _pre_init ( self ) : if self . parsed_args . git_log : git_path = self . parsed_args . git_log elif not self . parsed_args . git_path : base_path = os . path . expanduser ( '~/.perceval/repositories/' ) processed_uri = self . parsed_args . uri . lstrip ( '/' ) git_path = os . path . join ( base_path , processed_uri ) + '-git' else : git_path = self . parsed_args . git_path setattr ( self . parsed_args , 'gitpath' , git_path )
def setup_cmd_parser ( cls ) : parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = True , to_date = True ) group = parser . parser . add_argument_group ( 'Git arguments' ) group . add_argument ( '--branches' , dest = 'branches' , nargs = '+' , type = str , default = None , help = "Fetch commits only from these branches" ) exgroup = group . add_mutually_exclusive_group ( ) exgroup . add_argument ( '--git-path' , dest = 'git_path' , help = "Path where the Git repository will be cloned" ) exgroup . add_argument ( '--git-log' , dest = 'git_log' , help = "Path to the Git log file" ) exgroup_fetch = group . add_mutually_exclusive_group ( ) exgroup_fetch . add_argument ( '--latest-items' , dest = 'latest_items' , action = 'store_true' , help = "Fetch latest commits added to the repository" ) exgroup_fetch . add_argument ( '--no-update' , dest = 'no_update' , action = 'store_true' , help = "Fetch all commits without updating the repository" ) parser . parser . add_argument ( 'uri' , help = "URI of the Git log repository" ) return parser
def parse ( self ) : for line in self . stream : line = line . rstrip ( '\n' ) parsed = False self . nline += 1 while not parsed : parsed = self . handlers [ self . state ] ( line ) if self . state == self . COMMIT and self . commit : commit = self . _build_commit ( ) logger . debug ( "Commit %s parsed" , commit [ 'commit' ] ) yield commit if self . commit : commit = self . _build_commit ( ) logger . debug ( "Commit %s parsed" , commit [ 'commit' ] ) yield commit
def _fetch_pack ( self ) : def prepare_refs ( refs ) : return [ ref . hash . encode ( 'utf-8' ) for ref in refs if not ref . refname . endswith ( '^{}' ) ] def determine_wants ( refs ) : remote_refs = prepare_refs ( self . _discover_refs ( remote = True ) ) local_refs = prepare_refs ( self . _discover_refs ( ) ) wants = [ ref for ref in remote_refs if ref not in local_refs ] return wants client , repo_path = dulwich . client . get_transport_and_path ( self . uri ) repo = dulwich . repo . Repo ( self . dirpath ) fd = io . BytesIO ( ) local_refs = self . _discover_refs ( ) graph_walker = _GraphWalker ( local_refs ) result = client . fetch_pack ( repo_path , determine_wants , graph_walker , fd . write ) refs = [ GitRef ( ref_hash . decode ( 'utf-8' ) , ref_name . decode ( 'utf-8' ) ) for ref_name , ref_hash in result . refs . items ( ) ] if len ( fd . getvalue ( ) ) > 0 : fd . seek ( 0 ) pack = repo . object_store . add_thin_pack ( fd . read , None ) pack_name = pack . name ( ) . decode ( 'utf-8' ) else : pack_name = None return ( pack_name , refs )
def _read_commits_from_pack ( self , packet_name ) : filepath = 'objects/pack/pack-' + packet_name cmd_verify_pack = [ 'git' , 'verify-pack' , '-v' , filepath ] outs = self . _exec ( cmd_verify_pack , cwd = self . dirpath , env = self . gitenv ) outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) lines = [ line . split ( ' ' ) for line in outs . split ( '\n' ) ] commits = [ parts [ 0 ] for parts in lines if parts [ 1 ] == 'commit' ] commits . reverse ( ) return commits
def _update_references ( self , refs ) : new_refs = [ ref . refname for ref in refs ] for old_ref in self . _discover_refs ( ) : if not old_ref . refname . startswith ( 'refs/heads/' ) : continue if old_ref . refname in new_refs : continue self . _update_ref ( old_ref , delete = True ) for new_ref in refs : refname = new_ref . refname if refname . endswith ( '^{}' ) : logger . debug ( "Annotated tag %s ignored for updating in sync process" , refname ) continue elif not refname . startswith ( 'refs/heads/' ) and not refname . startswith ( 'refs/tags/' ) : logger . debug ( "Reference %s not needed; ignored for updating in sync process" , refname ) continue else : self . _update_ref ( new_ref ) cmd = [ 'git' , 'remote' , 'prune' , 'origin' ] self . _exec ( cmd , cwd = self . dirpath , env = self . gitenv )
def _discover_refs ( self , remote = False ) : if remote : cmd_refs = [ 'git' , 'ls-remote' , '-h' , '-t' , '--exit-code' , 'origin' ] sep = '\t' ignored_error_codes = [ 2 ] else : if self . is_empty ( ) : raise EmptyRepositoryError ( repository = self . uri ) cmd_refs = [ 'git' , 'show-ref' , '--heads' , '--tags' ] sep = ' ' ignored_error_codes = [ 1 ] outs = self . _exec ( cmd_refs , cwd = self . dirpath , env = self . gitenv , ignored_error_codes = ignored_error_codes ) outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) outs = outs . split ( '\n' ) if outs else [ ] refs = [ ] for line in outs : data = line . split ( sep ) ref = GitRef ( data [ 0 ] , data [ 1 ] ) refs . append ( ref ) return refs
def _update_ref ( self , ref , delete = False ) : cmd = [ 'git' , 'update-ref' ] if delete : cmd . extend ( [ '-d' , ref . refname ] ) action = 'deleted' else : cmd . extend ( [ ref . refname , ref . hash ] ) action = 'updated to %s' % ref . hash try : self . _exec ( cmd , cwd = self . dirpath , env = self . gitenv ) except RepositoryError as e : logger . warning ( "Git %s ref could not be %s during sync process in %s (%s); skipped" , ref . refname , action , self . uri , self . dirpath ) else : logger . debug ( "Git %s ref %s in %s (%s)" , ref . refname , action , self . uri , self . dirpath )
def setup_cmd_parser ( cls ) : parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , token_auth = True , archive = True ) action = parser . parser . _option_string_actions [ '--api-token' ] action . required = True group = parser . parser . add_argument_group ( 'Twitter arguments' ) group . add_argument ( '--max-items' , dest = 'max_items' , type = int , default = MAX_ITEMS , help = "Maximum number of items requested on the same query" ) group . add_argument ( '--no-entities' , dest = 'include_entities' , action = 'store_false' , help = " Exclude entities node" ) group . add_argument ( '--geo-code' , dest = 'geocode' , help = "Select tweets by users located at latitude,longitude,radius" ) group . add_argument ( '--lang' , dest = 'lang' , help = "Select tweets to the given language in ISO 639-1 code" ) group . add_argument ( '--tweets-type' , dest = 'tweets_type' , default = TWEET_TYPE_MIXED , help = "Type of tweets returned. Default is 'mixed', others are 'recent' and 'popular'" ) group . add_argument ( '--sleep-for-rate' , dest = 'sleep_for_rate' , action = 'store_true' , help = "sleep for getting more rate" ) group . add_argument ( '--min-rate-to-sleep' , dest = 'min_rate_to_sleep' , default = MIN_RATE_LIMIT , type = int , help = "sleep until reset when the rate limit reaches this value" ) group . add_argument ( '--sleep-time' , dest = 'sleep_time' , default = SLEEP_TIME , type = int , help = "minimun sleeping time to avoid too many request exception" ) parser . parser . add_argument ( 'query' , help = "Search query including operators, max 500 chars" ) return parser
def __parse_hits ( self , hit_raw ) : bs_result = bs4 . BeautifulSoup ( hit_raw , 'html.parser' ) hit_string = bs_result . find ( "div" , id = "resultStats" ) . text hit_string = hit_string . replace ( ',' , u'' ) hit_string = hit_string . replace ( '.' , u'' ) fetched_on = datetime_utcnow ( ) . timestamp ( ) id_args = self . keywords [ : ] id_args . append ( str ( fetched_on ) ) hits_json = { 'fetched_on' : fetched_on , 'id' : uuid ( * id_args ) , 'keywords' : self . keywords , 'type' : 'googleSearchHits' } if not hit_string : logger . warning ( "No hits for %s" , self . keywords ) hits_json [ 'hits' ] = 0 return hits_json str_hits = re . search ( r'\d+' , hit_string ) . group ( 0 ) hits = int ( str_hits ) hits_json [ 'hits' ] = hits return hits_json
def hits ( self , keywords ) : if len ( keywords ) == 1 : query_str = keywords [ 0 ] else : query_str = ' ' . join ( [ k for k in keywords ] ) logger . info ( "Fetching hits for '%s'" , query_str ) params = { 'q' : query_str } req = self . fetch ( GOOGLE_SEARCH_URL , payload = params ) return req . text
def __fetch_pull_requests ( self , from_date , to_date ) : raw_pulls = self . client . pulls ( from_date = from_date ) for raw_pull in raw_pulls : pull = json . loads ( raw_pull ) if str_to_datetime ( pull [ 'updated_at' ] ) > to_date : return self . __init_extra_pull_fields ( pull ) for field in TARGET_PULL_FIELDS : if not pull [ field ] : continue if field == 'user' : pull [ field + '_data' ] = self . __get_user ( pull [ field ] [ 'login' ] ) elif field == 'merged_by' : pull [ field + '_data' ] = self . __get_user ( pull [ field ] [ 'login' ] ) elif field == 'review_comments' : pull [ field + '_data' ] = self . __get_pull_review_comments ( pull [ 'number' ] ) elif field == 'requested_reviewers' : pull [ field + '_data' ] = self . __get_pull_requested_reviewers ( pull [ 'number' ] ) elif field == 'commits' : pull [ field + '_data' ] = self . __get_pull_commits ( pull [ 'number' ] ) yield pull
def __fetch_repo_info ( self ) : raw_repo = self . client . repo ( ) repo = json . loads ( raw_repo ) fetched_on = datetime_utcnow ( ) repo [ 'fetched_on' ] = fetched_on . timestamp ( ) yield repo
def __get_issue_comment_reactions ( self , comment_id , total_count ) : reactions = [ ] if total_count == 0 : return reactions group_reactions = self . client . issue_comment_reactions ( comment_id ) for raw_reactions in group_reactions : for reaction in json . loads ( raw_reactions ) : reaction [ 'user_data' ] = self . __get_user ( reaction [ 'user' ] [ 'login' ] ) reactions . append ( reaction ) return reactions
def __get_pull_requested_reviewers ( self , pr_number ) : requested_reviewers = [ ] group_requested_reviewers = self . client . pull_requested_reviewers ( pr_number ) for raw_requested_reviewers in group_requested_reviewers : group_requested_reviewers = json . loads ( raw_requested_reviewers ) for requested_reviewer in group_requested_reviewers [ 'users' ] : user_data = self . __get_user ( requested_reviewer [ 'login' ] ) requested_reviewers . append ( user_data ) return requested_reviewers
def __get_pull_commits ( self , pr_number ) : hashes = [ ] group_pull_commits = self . client . pull_commits ( pr_number ) for raw_pull_commits in group_pull_commits : for commit in json . loads ( raw_pull_commits ) : commit_hash = commit [ 'sha' ] hashes . append ( commit_hash ) return hashes
def __get_pull_review_comments ( self , pr_number ) : comments = [ ] group_comments = self . client . pull_review_comments ( pr_number ) for raw_comments in group_comments : for comment in json . loads ( raw_comments ) : comment_id = comment . get ( 'id' ) user = comment . get ( 'user' , None ) if not user : logger . warning ( "Missing user info for %s" , comment [ 'url' ] ) comment [ 'user_data' ] = None else : comment [ 'user_data' ] = self . __get_user ( user [ 'login' ] ) comment [ 'reactions_data' ] = self . __get_pull_review_comment_reactions ( comment_id , comment [ 'reactions' ] [ 'total_count' ] ) comments . append ( comment ) return comments
def __get_pull_review_comment_reactions ( self , comment_id , total_count ) : reactions = [ ] if total_count == 0 : return reactions group_reactions = self . client . pull_review_comment_reactions ( comment_id ) for raw_reactions in group_reactions : for reaction in json . loads ( raw_reactions ) : reaction [ 'user_data' ] = self . __get_user ( reaction [ 'user' ] [ 'login' ] ) reactions . append ( reaction ) return reactions
def __get_user ( self , login ) : user = { } if not login : return user user_raw = self . client . user ( login ) user = json . loads ( user_raw ) user_orgs_raw = self . client . user_orgs ( login ) user [ 'organizations' ] = json . loads ( user_orgs_raw ) return user
def issue_reactions ( self , issue_number ) : payload = { 'per_page' : PER_PAGE , 'direction' : 'asc' , 'sort' : 'updated' } path = urijoin ( "issues" , str ( issue_number ) , "reactions" ) return self . fetch_items ( path , payload )
def pull_requested_reviewers ( self , pr_number ) : requested_reviewers_url = urijoin ( "pulls" , str ( pr_number ) , "requested_reviewers" ) return self . fetch_items ( requested_reviewers_url , { } )
def pull_commits ( self , pr_number ) : payload = { 'per_page' : PER_PAGE , } commit_url = urijoin ( "pulls" , str ( pr_number ) , "commits" ) return self . fetch_items ( commit_url , payload )
def pull_review_comments ( self , pr_number ) : payload = { 'per_page' : PER_PAGE , 'direction' : 'asc' , 'sort' : 'updated' } comments_url = urijoin ( "pulls" , str ( pr_number ) , "comments" ) return self . fetch_items ( comments_url , payload )
def pull_review_comment_reactions ( self , comment_id ) : payload = { 'per_page' : PER_PAGE , 'direction' : 'asc' , 'sort' : 'updated' } path = urijoin ( "pulls" , "comments" , str ( comment_id ) , "reactions" ) return self . fetch_items ( path , payload )
def user ( self , login ) : user = None if login in self . _users : return self . _users [ login ] url_user = urijoin ( self . base_url , 'users' , login ) logging . info ( "Getting info for %s" % ( url_user ) ) r = self . fetch ( url_user ) user = r . text self . _users [ login ] = user return user
def user_orgs ( self , login ) : if login in self . _users_orgs : return self . _users_orgs [ login ] url = urijoin ( self . base_url , 'users' , login , 'orgs' ) try : r = self . fetch ( url ) orgs = r . text except requests . exceptions . HTTPError as error : if error . response . status_code == 404 : logger . error ( "Can't get github login orgs: %s" , error ) orgs = '[]' else : raise error self . _users_orgs [ login ] = orgs return orgs
def _get_token_rate_limit ( self , token ) : rate_url = urijoin ( self . base_url , "rate_limit" ) self . session . headers . update ( { 'Authorization' : 'token ' + token } ) remaining = 0 try : headers = super ( ) . fetch ( rate_url ) . headers if self . rate_limit_header in headers : remaining = int ( headers [ self . rate_limit_header ] ) except requests . exceptions . HTTPError as error : logger . warning ( "Rate limit not initialized: %s" , error ) return remaining
def _get_tokens_rate_limits ( self ) : remainings = [ 0 ] * self . n_tokens arch = self . archive self . archive = None for idx , token in enumerate ( self . tokens ) : remainings [ idx ] = self . _get_token_rate_limit ( token ) self . archive = arch logger . debug ( "Remaining API points: {}" . format ( remainings ) ) return remainings
def _choose_best_api_token ( self ) : if self . n_tokens == 0 : return token_idx = 0 if self . n_tokens > 1 : remainings = self . _get_tokens_rate_limits ( ) token_idx = remainings . index ( max ( remainings ) ) logger . debug ( "Remaining API points: {}, choosen index: {}" . format ( remainings , token_idx ) ) self . current_token = self . tokens [ token_idx ] self . session . headers . update ( { 'Authorization' : 'token ' + self . current_token } ) self . _update_current_rate_limit ( )
def _need_check_tokens ( self ) : if self . n_tokens <= 1 or self . rate_limit is None : return False elif self . last_rate_limit_checked is None : self . last_rate_limit_checked = self . rate_limit return True approaching_limit = float ( self . min_rate_to_sleep ) * ( 1.0 + TOKEN_USAGE_BEFORE_SWITCH ) + 1 if self . rate_limit <= approaching_limit : self . last_rate_limit_checked = self . rate_limit return True ratio = float ( self . rate_limit ) / float ( self . last_rate_limit_checked ) if ratio < 1.0 - TOKEN_USAGE_BEFORE_SWITCH : self . last_rate_limit_checked = self . rate_limit return True elif ratio > 1.0 : self . last_rate_limit_checked = self . rate_limit return False else : return False
def _update_current_rate_limit ( self ) : url = urijoin ( self . base_url , "rate_limit" ) try : arch = self . archive self . archive = None response = super ( ) . fetch ( url ) self . archive = arch self . update_rate_limit ( response ) self . last_rate_limit_checked = self . rate_limit except requests . exceptions . HTTPError as error : if error . response . status_code == 404 : logger . warning ( "Rate limit not initialized: %s" , error ) else : raise error
def _load_metadata ( self ) : logger . debug ( "Loading metadata infomation of archive %s" , self . archive_path ) cursor = self . _db . cursor ( ) select_stmt = "SELECT origin, backend_name, backend_version, " "category, backend_params, created_on " "FROM " + self . METADATA_TABLE + " " "LIMIT 1" cursor . execute ( select_stmt ) row = cursor . fetchone ( ) cursor . close ( ) if row : self . origin = row [ 0 ] self . backend_name = row [ 1 ] self . backend_version = row [ 2 ] self . category = row [ 3 ] self . backend_params = pickle . loads ( row [ 4 ] ) self . created_on = str_to_datetime ( row [ 5 ] ) else : logger . debug ( "Metadata of archive %s was empty" , self . archive_path ) logger . debug ( "Metadata of archive %s loaded" , self . archive_path )
def _count_table_rows ( self , table_name ) : cursor = self . _db . cursor ( ) select_stmt = "SELECT COUNT(*) FROM " + table_name try : cursor . execute ( select_stmt ) row = cursor . fetchone ( ) except sqlite3 . DatabaseError as e : msg = "invalid archive file; cause: %s" % str ( e ) raise ArchiveError ( cause = msg ) finally : cursor . close ( ) return row [ 0 ]
def _search_archives ( self , origin , backend_name , category , archived_after ) : for archive_path in self . _search_files ( ) : try : archive = Archive ( archive_path ) except ArchiveError : continue match = archive . origin == origin and archive . backend_name == backend_name and archive . category == category and archive . created_on >= archived_after if not match : continue yield archive_path , archive . created_on
def _search_files ( self ) : for root , _ , files in os . walk ( self . dirpath ) : for filename in files : location = os . path . join ( root , filename ) yield location
def repository ( self , owner , repository ) : url = urijoin ( self . base_url , self . RREPOSITORY , owner , repository ) logger . debug ( "DockerHub client requests: %s" , url ) response = self . fetch ( url ) return response . text
def get_fields ( self ) : url = urijoin ( self . base_url , self . RESOURCE , self . VERSION_API , 'field' ) req = self . fetch ( url ) return req . text
def get_builds ( self , job_name ) : if self . blacklist_jobs and job_name in self . blacklist_jobs : logger . warning ( "Not getting blacklisted job: %s" , job_name ) return payload = { 'depth' : self . detail_depth } url_build = urijoin ( self . base_url , "job" , job_name , "api" , "json" ) response = self . fetch ( url_build , payload = payload ) return response . text
def setup_cmd_parser ( cls ) : parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = True , token_auth = True , archive = True ) group = parser . parser . add_argument_group ( 'StackExchange arguments' ) group . add_argument ( '--site' , dest = 'site' , required = True , help = "StackExchange site" ) group . add_argument ( '--tagged' , dest = 'tagged' , help = "filter items by question Tag" ) group . add_argument ( '--max-questions' , dest = 'max_questions' , type = int , default = MAX_QUESTIONS , help = "Maximum number of questions requested in the same query" ) return parser
def __get_max_date ( self , reviews ) : max_ts = 0 for review in reviews : ts = str_to_datetime ( review [ 'timestamp' ] ) ts = datetime_to_utc ( ts ) if ts . timestamp ( ) > max_ts : max_ts = ts . timestamp ( ) return max_ts
def get_pages ( self , namespace , apcontinue = '' ) : params = { "action" : "query" , "list" : "allpages" , "aplimit" : self . limit , "apnamespace" : namespace , "format" : "json" } if apcontinue : params [ 'apcontinue' ] = apcontinue return self . call ( params )
def get_recent_pages ( self , namespaces , rccontinue = '' ) : namespaces . sort ( ) params = { "action" : "query" , "list" : "recentchanges" , "rclimit" : self . limit , "rcnamespace" : "|" . join ( namespaces ) , "rcprop" : "title|timestamp|ids" , "format" : "json" } if rccontinue : params [ 'rccontinue' ] = rccontinue return self . call ( params )
def __retrieve_archives ( self , from_date ) : archives = [ ] candidates = self . __list_supybot_archives ( ) for candidate in candidates : dt = self . __parse_date_from_filepath ( candidate ) if dt . date ( ) >= from_date . date ( ) : archives . append ( ( dt , candidate ) ) else : logger . debug ( "Archive %s stored before %s; skipped" , candidate , str ( from_date ) ) archives . sort ( key = lambda x : x [ 0 ] ) return [ archive [ 1 ] for archive in archives ]
def __list_supybot_archives ( self ) : archives = [ ] for root , _ , files in os . walk ( self . dirpath ) : for filename in files : location = os . path . join ( root , filename ) archives . append ( location ) return archives
def capabilities_url ( self , service_url ) : qs = [ ] if service_url . find ( '?' ) != - 1 : qs = cgi . parse_qsl ( service_url . split ( '?' ) [ 1 ] ) params = [ x [ 0 ] for x in qs ] if 'service' not in params : qs . append ( ( 'service' , 'WFS' ) ) if 'request' not in params : qs . append ( ( 'request' , 'GetCapabilities' ) ) if 'version' not in params : qs . append ( ( 'version' , self . version ) ) urlqs = urlencode ( tuple ( qs ) ) return service_url . split ( '?' ) [ 0 ] + '?' + urlqs
def _parse_result ( self ) : if self . result is not None : result = self . result . find ( nspv ( "wml2:MeasurementTimeseries" ) ) self . result = MeasurementTimeseries ( result )
def complex_input_with_reference ( ) : print ( "\ncomplex_input_with_reference ..." ) wps = WebProcessingService ( 'http://localhost:8094/wps' , verbose = verbose ) processid = 'wordcount' textdoc = ComplexDataInput ( "http://www.gutenberg.org/files/28885/28885-h/28885-h.htm" ) inputs = [ ( "text" , textdoc ) ] outputs = [ ( "output" , True , 'some/mime-type' ) ] execution = wps . execute ( processid , inputs , output = outputs ) monitorExecution ( execution ) print ( 'percent complete' , execution . percentCompleted ) print ( 'status message' , execution . statusMessage ) for output in execution . processOutputs : print ( 'identifier=%s, dataType=%s, data=%s, reference=%s' % ( output . identifier , output . dataType , output . data , output . reference ) )
def normalize ( s ) : if ( nonorm ) : return s . split ( ) try : s . split ( ) except : s = " " . join ( s ) for ( pattern , replace ) in normalize1 : s = re . sub ( pattern , replace , s ) s = xml . sax . saxutils . unescape ( s , { '&quot;' : '"' } ) s = " %s " % s if not preserve_case : s = s . lower ( ) return [ tok for tok in normalize3 . split ( s ) if tok and tok != ' ' ]
def erfcc ( x ) : z = abs ( x ) t = 1 / ( 1 + 0.5 * z ) r = t * math . exp ( - z * z - 1.26551223 + t * ( 1.00002368 + t * ( .37409196 + t * ( .09678418 + t * ( - .18628806 + t * ( .27886807 + t * ( - 1.13520398 + t * ( 1.48851587 + t * ( - .82215223 + t * .17087277 ) ) ) ) ) ) ) ) ) if ( x >= 0. ) : return r else : return 2. - r
def log_calls ( func ) : def wrapper ( * args , * * kargs ) : callStr = "%s(%s)" % ( func . __name__ , ", " . join ( [ repr ( p ) for p in args ] + [ "%s=%s" % ( k , repr ( v ) ) for ( k , v ) in list ( kargs . items ( ) ) ] ) ) debug ( ">> %s" , callStr ) ret = func ( * args , * * kargs ) debug ( "<< %s: %s" , callStr , repr ( ret ) ) return ret return wrapper
def synchronized ( func ) : func . __lock__ = threading . Lock ( ) def synced_func ( * args , * * kargs ) : with func . __lock__ : return func ( * args , * * kargs ) return synced_func
def message ( msg , * args ) : clear_progress ( ) text = ( msg % args ) sys . stdout . write ( text + '\n' )
def tempfile_get ( target ) : fn = '%s-%s.tmp' % ( target , '' . join ( random . Random ( ) . sample ( "0123456789abcdefghijklmnopqrstuvwxyz" , 15 ) ) ) TEMP_FILES . add ( fn ) return fn
def tempfile_set ( tempfile , target ) : if target : os . rename ( tempfile , target ) else : os . unlink ( tempfile ) if target in TEMP_FILES : TEMP_FILES . remove ( tempfile )
def clean_tempfiles ( ) : for fn in TEMP_FILES : if os . path . exists ( fn ) : os . unlink ( fn )
def get_fixed_path ( self ) : pi = self . path . split ( PATH_SEP ) fi = [ ] for p in pi : if '*' in p or '?' in p : break fi . append ( p ) return PATH_SEP . join ( fi )
def get_legal_params ( self , method ) : if method not in self . client . meta . method_to_api_mapping : return [ ] api = self . client . meta . method_to_api_mapping [ method ] shape = self . client . meta . service_model . operation_model ( api ) . input_shape if shape is None : return [ ] return shape . members . keys ( )
def add_options ( parser ) : for param , param_type , param_doc in BotoClient . EXTRA_CLIENT_PARAMS : parser . add_option ( '--API-' + param , help = param_doc , type = param_type , dest = param )
def add_task ( self , func_name , * args , * * kargs ) : self . tasks . put ( ( func_name , 0 , args , kargs ) )
def join ( self ) : self . tasks . join ( ) for worker in self . workers : self . tasks . put ( None ) for worker in self . workers : worker . join ( ) worker . s3 = None
def processed ( self ) : self . processed_tasks += 1 qsize = self . tasks . qsize ( ) if qsize > 0 : progress ( '[%d task(s) completed, %d remaining, %d thread(s)]' , self . processed_tasks , qsize , len ( self . workers ) ) else : progress ( '[%d task(s) completed, %d thread(s)]' , self . processed_tasks , len ( self . workers ) )
def s3_keys_from_env ( ) : env = os . environ if S3_ACCESS_KEY_NAME in env and S3_SECRET_KEY_NAME in env : keys = ( env [ S3_ACCESS_KEY_NAME ] , env [ S3_SECRET_KEY_NAME ] ) debug ( "read S3 keys from environment" ) return keys else : return None
def s3_keys_from_cmdline ( opt ) : if opt . access_key != None and opt . secret_key != None : keys = ( opt . access_key , opt . secret_key ) debug ( "read S3 keys from commandline" ) return keys else : return None
def s3_keys_from_s3cfg ( opt ) : try : if opt . s3cfg != None : s3cfg_path = "%s" % opt . s3cfg else : s3cfg_path = "%s/.s3cfg" % os . environ [ "HOME" ] if not os . path . exists ( s3cfg_path ) : return None config = ConfigParser . ConfigParser ( ) config . read ( s3cfg_path ) keys = config . get ( "default" , "access_key" ) , config . get ( "default" , "secret_key" ) debug ( "read S3 keys from %s file" , s3cfg_path ) return keys except Exception as e : info ( "could not read S3 keys from %s file; skipping (%s)" , s3cfg_path , e ) return None
def init_s3_keys ( opt ) : S3Handler . S3_KEYS = S3Handler . s3_keys_from_cmdline ( opt ) or S3Handler . s3_keys_from_env ( ) or S3Handler . s3_keys_from_s3cfg ( opt )
def connect ( self ) : try : if S3Handler . S3_KEYS : self . s3 = BotoClient ( self . opt , S3Handler . S3_KEYS [ 0 ] , S3Handler . S3_KEYS [ 1 ] ) else : self . s3 = BotoClient ( self . opt ) except Exception as e : raise RetryFailure ( 'Unable to connect to s3: %s' % e )
def local_walk ( self , basedir ) : result = [ ] for root , dirs , files in os . walk ( basedir ) : for f in files : result . append ( os . path . join ( root , f ) ) return result
def put_single_file ( self , pool , source , target ) : if os . path . isdir ( source ) : if self . opt . recursive : for f in ( f for f in self . local_walk ( source ) if not os . path . isdir ( f ) ) : target_url = S3URL ( target ) joined_path = os . path . normpath ( os . path . join ( target_url . path , os . path . relpath ( f , source ) ) ) pool . upload ( f , S3URL . combine ( 's3' , target_url . bucket , joined_path ) ) else : message ( 'omitting directory "%s".' % source ) else : pool . upload ( source , target )
def create_bucket ( self , source ) : s3url = S3URL ( source ) message ( 'Creating %s' , source ) if not self . opt . dry_run : resp = self . s3 . create_bucket ( Bucket = s3url . bucket ) if resp [ 'ResponseMetadata' ] [ "HTTPStatusCode" ] == 200 : message ( 'Done.' ) else : raise Failure ( 'Unable to create bucket %s' % source )
def update_privilege ( self , obj , target ) : if 'privilege' in obj [ 'Metadata' ] : os . chmod ( target , int ( obj [ 'Metadata' ] [ 'privilege' ] , 8 ) )
def print_files ( self , source ) : sources = self . source_expand ( source ) for source in sources : s3url = S3URL ( source ) response = self . s3 . get_object ( Bucket = s3url . bucket , Key = s3url . path ) message ( '%s' , response [ 'Body' ] . read ( ) )
def get_single_file ( self , pool , source , target ) : if source [ - 1 ] == PATH_SEP : if self . opt . recursive : basepath = S3URL ( source ) . path for f in ( f for f in self . s3walk ( source ) if not f [ 'is_dir' ] ) : pool . download ( f [ 'name' ] , os . path . join ( target , os . path . relpath ( S3URL ( f [ 'name' ] ) . path , basepath ) ) ) else : message ( 'omitting directory "%s".' % source ) else : pool . download ( source , target )
def cp_single_file ( self , pool , source , target , delete_source ) : if source [ - 1 ] == PATH_SEP : if self . opt . recursive : basepath = S3URL ( source ) . path for f in ( f for f in self . s3walk ( source ) if not f [ 'is_dir' ] ) : pool . copy ( f [ 'name' ] , os . path . join ( target , os . path . relpath ( S3URL ( f [ 'name' ] ) . path , basepath ) ) , delete_source = delete_source ) else : message ( 'omitting directory "%s".' % source ) else : pool . copy ( source , target , delete_source = delete_source )
def del_files ( self , source ) : src_files = [ ] for obj in self . s3walk ( source ) : if not obj [ 'is_dir' ] : src_files . append ( obj [ 'name' ] ) pool = ThreadPool ( ThreadUtil , self . opt ) pool . batch_delete ( src_files ) pool . join ( )
def dsync_files ( self , source , target ) : src_s3_url = S3URL . is_valid ( source ) dst_s3_url = S3URL . is_valid ( target ) source_list = self . relative_dir_walk ( source ) if len ( source_list ) == 0 or '.' in source_list : raise Failure ( 'Sync command need to sync directory to directory.' ) sync_list = [ ( os . path . join ( source , f ) , os . path . join ( target , f ) ) for f in source_list ] pool = ThreadPool ( ThreadUtil , self . opt ) if src_s3_url and not dst_s3_url : for src , dest in sync_list : pool . download ( src , dest ) elif not src_s3_url and dst_s3_url : for src , dest in sync_list : pool . upload ( src , dest ) elif src_s3_url and dst_s3_url : for src , dest in sync_list : pool . copy ( src , dest ) else : raise InvalidArgument ( 'Cannot sync two local directories.' ) pool . join ( ) if self . opt . delete_removed : target_list = self . relative_dir_walk ( target ) remove_list = [ os . path . join ( target , f ) for f in ( set ( target_list ) - set ( source_list ) ) ] if S3URL . is_valid ( target ) : pool = ThreadPool ( ThreadUtil , self . opt ) pool . batch_delete ( remove_list ) pool . join ( ) else : for f in remove_list : try : os . unlink ( f ) message ( 'Delete %s' , f ) except : pass
def file_hash ( self , filename , block_size = 2 ** 20 ) : m = hashlib . md5 ( ) with open ( filename , 'rb' ) as f : while True : data = f . read ( block_size ) if not data : break m . update ( data ) return m . hexdigest ( )
def get_md5 ( self ) : if self . md5 is None : self . md5 = self . file_hash ( self . filename ) return self . md5
def mkdirs ( self , target ) : path = os . path . dirname ( target ) if path and path != PATH_SEP and not os . path . isdir ( path ) : try : os . makedirs ( path ) except OSError as ose : if ose . errno != errno . EEXIST : raise Failure ( 'Unable to create directory (%s)' % ( path , ) )
def conditional ( self , result , obj ) : fileonly = ( self . opt . last_modified_before is not None ) or ( self . opt . last_modified_after is not None ) if obj [ 'is_dir' ] : if not fileonly : result . append ( obj ) return if ( self . opt . last_modified_before is not None ) and obj [ 'last_modified' ] >= self . opt . last_modified_before : return if ( self . opt . last_modified_after is not None ) and obj [ 'last_modified' ] <= self . opt . last_modified_after : return result . append ( obj )
def get_file_privilege ( self , source ) : try : return str ( oct ( os . stat ( source ) . st_mode ) [ - 3 : ] ) except Exception as e : raise Failure ( 'Could not get stat for %s, error_message = %s' , source , e )
def lookup ( self , s3url ) : try : return self . s3 . head_object ( Bucket = s3url . bucket , Key = s3url . path ) except BotoClient . ClientError as e : if e . response [ 'ResponseMetadata' ] [ 'HTTPStatusCode' ] == 404 : return None else : raise e
def read_file_chunk ( self , source , pos , chunk ) : if chunk == 0 : return StringIO ( ) data = None with open ( source , 'rb' ) as f : f . seek ( pos ) data = f . read ( chunk ) if not data : raise Failure ( 'Unable to read data from source: %s' % source ) return StringIO ( data )
def upload ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 ) : s3url = S3URL ( target ) obj = self . lookup ( s3url ) if not mpi : fsize = os . path . getsize ( source ) md5cache = LocalMD5Cache ( source ) if self . opt . dry_run : message ( '%s => %s' , source , target ) return elif self . opt . sync_check and self . sync_check ( md5cache , obj ) : message ( '%s => %s (synced)' , source , target ) return elif not self . opt . force and obj : raise Failure ( 'File already exists: %s' % target ) if fsize < self . opt . max_singlepart_upload_size : data = self . read_file_chunk ( source , 0 , fsize ) self . s3 . put_object ( Bucket = s3url . bucket , Key = s3url . path , Body = data , Metadata = { 'md5' : md5cache . get_md5 ( ) , 'privilege' : self . get_file_privilege ( source ) } ) message ( '%s => %s' , source , target ) return response = self . s3 . create_multipart_upload ( Bucket = s3url . bucket , Key = s3url . path , Metadata = { 'md5' : md5cache . get_md5 ( ) , 'privilege' : self . get_file_privilege ( source ) } ) upload_id = response [ 'UploadId' ] for args in self . get_file_splits ( upload_id , source , target , fsize , self . opt . multipart_split_size ) : self . pool . upload ( * args ) return data = self . read_file_chunk ( source , pos , chunk ) response = self . s3 . upload_part ( Bucket = s3url . bucket , Key = s3url . path , UploadId = mpi . id , Body = data , PartNumber = part ) if mpi . complete ( { 'ETag' : response [ 'ETag' ] , 'PartNumber' : part } ) : try : self . s3 . complete_multipart_upload ( Bucket = s3url . bucket , Key = s3url . path , UploadId = mpi . id , MultipartUpload = { 'Parts' : mpi . sorted_parts ( ) } ) message ( '%s => %s' , source , target ) except Exception as e : message ( 'Unable to complete upload: %s' , str ( e ) ) self . s3 . abort_multipart_upload ( Bucket = s3url . bucket , Key = s3url . path , UploadId = mpi . id ) raise RetryFailure ( 'Upload failed: Unable to complete upload %s.' % source )
def _verify_file_size ( self , obj , downloaded_file ) : file_size = os . path . getsize ( downloaded_file ) if int ( obj [ 'ContentLength' ] ) != file_size : raise RetryFailure ( 'Downloaded file size inconsistent: %s' % ( repr ( obj ) ) )
def write_file_chunk ( self , target , pos , chunk , body ) : fd = os . open ( target , os . O_CREAT | os . O_WRONLY ) try : os . lseek ( fd , pos , os . SEEK_SET ) data = body . read ( chunk ) num_bytes_written = os . write ( fd , data ) if ( num_bytes_written != len ( data ) ) : raise RetryFailure ( 'Number of bytes written inconsistent: %s != %s' % ( num_bytes_written , sys . getsizeof ( data ) ) ) finally : os . close ( fd )
def download ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 ) : s3url = S3URL ( source ) obj = self . lookup ( s3url ) if obj is None : raise Failure ( 'The obj "%s" does not exists.' % ( s3url . path , ) ) if not mpi : if self . opt . dry_run : message ( '%s => %s' , source , target ) return elif self . opt . sync_check and self . sync_check ( LocalMD5Cache ( target ) , obj ) : message ( '%s => %s (synced)' , source , target ) return elif not self . opt . force and os . path . exists ( target ) : raise Failure ( 'File already exists: %s' % target ) fsize = int ( obj [ 'ContentLength' ] ) if fsize < self . opt . max_singlepart_download_size : mpi = ThreadUtil . MultipartItem ( tempfile_get ( target ) ) mpi . total = 1 pos = 0 chunk = fsize else : for args in self . get_file_splits ( tempfile_get ( target ) , source , target , fsize , self . opt . multipart_split_size ) : self . pool . download ( * args ) return tempfile = mpi . id if self . opt . recursive : self . mkdirs ( tempfile ) response = self . s3 . get_object ( Bucket = s3url . bucket , Key = s3url . path , Range = 'bytes=%d-%d' % ( pos , pos + chunk - 1 ) ) self . write_file_chunk ( tempfile , pos , chunk , response [ 'Body' ] ) if mpi . complete ( { 'PartNumber' : part } ) : try : self . update_privilege ( obj , tempfile ) self . _verify_file_size ( obj , tempfile ) tempfile_set ( tempfile , target ) message ( '%s => %s' , source , target ) except Exception as e : tempfile_set ( tempfile , None ) raise Failure ( 'Download Failure: %s, Source: %s.' % ( e . message , source ) )
def copy ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 , delete_source = False ) : if self . opt . dry_run : message ( '%s => %s' % ( source , target ) ) return source_url = S3URL ( source ) target_url = S3URL ( target ) if not mpi : obj = self . lookup ( source_url ) fsize = int ( obj [ 'ContentLength' ] ) if fsize < self . opt . max_singlepart_copy_size : self . s3 . copy_object ( Bucket = target_url . bucket , Key = target_url . path , CopySource = { 'Bucket' : source_url . bucket , 'Key' : source_url . path } ) message ( '%s => %s' % ( source , target ) ) if delete_source : self . delete ( source ) return response = self . s3 . create_multipart_upload ( Bucket = target_url . bucket , Key = target_url . path , Metadata = obj [ 'Metadata' ] ) upload_id = response [ 'UploadId' ] for args in self . get_file_splits ( upload_id , source , target , fsize , self . opt . multipart_split_size ) : self . pool . copy ( * args , delete_source = delete_source ) return response = self . s3 . upload_part_copy ( Bucket = target_url . bucket , Key = target_url . path , CopySource = { 'Bucket' : source_url . bucket , 'Key' : source_url . path } , CopySourceRange = 'bytes=%d-%d' % ( pos , pos + chunk - 1 ) , UploadId = mpi . id , PartNumber = part ) if mpi . complete ( { 'ETag' : response [ 'CopyPartResult' ] [ 'ETag' ] , 'PartNumber' : part } ) : try : self . s3 . complete_multipart_upload ( Bucket = target_url . bucket , Key = target_url . path , UploadId = mpi . id , MultipartUpload = { 'Parts' : mpi . sorted_parts ( ) } ) if delete_source : self . delete ( source ) message ( '%s => %s' % ( source , target ) ) except Exception as e : message ( 'Unable to complete upload: %s' , str ( e ) ) self . s3 . abort_multipart_upload ( Bucket = source_url . bucket , Key = source_url . path , UploadId = mpi . id ) raise RetryFailure ( 'Copy failed: Unable to complete copy %s.' % source )
def delete ( self , source ) : s3url = S3URL ( source ) message ( 'Delete %s' , source ) if not self . opt . dry_run : self . s3 . delete_object ( Bucket = s3url . bucket , Key = s3url . path )
def run ( self , args ) : if len ( args ) == 0 : raise InvalidArgument ( 'No command provided' ) cmd = args [ 0 ] if cmd + '_handler' in CommandHandler . __dict__ : CommandHandler . __dict__ [ cmd + '_handler' ] ( self , args ) else : raise InvalidArgument ( 'Unknown command %s' % cmd )
def ls_handler ( self , args ) : if len ( args ) == 1 : self . pretty_print ( self . s3handler ( ) . list_buckets ( ) ) return self . validate ( 'cmd|s3' , args ) self . pretty_print ( self . s3handler ( ) . s3walk ( args [ 1 ] ) )
def mb_handler ( self , args ) : if len ( args ) == 1 : raise InvalidArgument ( 'No s3 bucketname provided' ) self . validate ( 'cmd|s3' , args ) self . s3handler ( ) . create_bucket ( args [ 1 ] )
def put_handler ( self , args ) : if len ( args ) < 3 : raise InvalidArgument ( 'Invalid number of parameters' ) self . validate ( '|' . join ( [ 'cmd' ] + [ 'local' ] * ( len ( args ) - 2 ) + [ 's3' ] ) , args ) source = args [ 1 : - 1 ] target = args [ - 1 ] self . s3handler ( ) . put_files ( source , target )
def get_handler ( self , args ) : if len ( args ) == 2 : args += [ '.' ] self . validate ( 'cmd|s3|local' , args ) source = args [ 1 ] target = args [ 2 ] self . s3handler ( ) . get_files ( source , target )
def cat_handler ( self , args ) : self . validate ( 'cmd|s3' , args ) source = args [ 1 ] self . s3handler ( ) . print_files ( source )
def dsync_handler ( self , args ) : self . opt . recursive = True self . opt . sync_check = True self . opt . force = True self . validate ( 'cmd|s3,local|s3,local' , args ) source = args [ 1 ] target = args [ 2 ] self . s3handler ( ) . dsync_files ( source , target )
def cp_handler ( self , args ) : self . validate ( 'cmd|s3|s3' , args ) source = args [ 1 ] target = args [ 2 ] self . s3handler ( ) . cp_files ( source , target )
def mv_handler ( self , args ) : self . validate ( 'cmd|s3|s3' , args ) source = args [ 1 ] target = args [ 2 ] self . s3handler ( ) . cp_files ( source , target , delete_source = True )
def del_handler ( self , args ) : self . validate ( 'cmd|s3' , args ) source = args [ 1 ] self . s3handler ( ) . del_files ( source )
def du_handler ( self , args ) : for src , size in self . s3handler ( ) . size ( args [ 1 : ] ) : message ( '%s\t%s' % ( size , src ) )
def _totalsize_handler ( self , args ) : total_size = 0 for src , size in self . s3handler ( ) . size ( args [ 1 : ] ) : total_size += size message ( str ( total_size ) )
def match_date ( self , value ) : m = self . REGEX_DATE . search ( value ) date = datetime . datetime . utcnow ( ) . date ( ) if m : date = datetime . date ( int ( m . group ( 1 ) ) , int ( m . group ( 2 ) ) , int ( m . group ( 3 ) ) ) value = self . REGEX_DATE . sub ( '' , value ) return ( date , value )
def match_time ( self , value ) : m = self . REGEX_TIME . search ( value ) time = datetime . datetime . utcnow ( ) . time ( ) if m : time = datetime . time ( int ( m . group ( 1 ) ) , int ( m . group ( 2 ) ) ) value = self . REGEX_TIME . sub ( '' , value ) return ( time , value )
def match_delta ( self , value ) : m = self . REGEX_DELTA . search ( value ) delta = datetime . timedelta ( days = 0 ) if m : d = int ( m . group ( 1 ) ) if m . group ( 3 ) == 'ago' or m . group ( 3 ) == 'before' : d = - d if m . group ( 2 ) == 'minute' : delta = datetime . timedelta ( minutes = d ) elif m . group ( 2 ) == 'hour' : delta = datetime . timedelta ( hours = d ) elif m . group ( 2 ) == 'day' : delta = datetime . timedelta ( days = d ) elif m . group ( 2 ) == 'week' : delta = datetime . timedelta ( weeks = d ) value = self . REGEX_DELTA . sub ( '' , value ) return ( delta , value )
def check_dict ( self , opt , value ) : try : return json . loads ( value ) except : raise optparse . OptionValueError ( "Option %s: invalid dict value: %r" % ( opt , value ) )
def get_from_hub ( self , sid ) : cmd = '{ "cmd":"read","sid":"' + sid + '"}' resp = self . _send_cmd ( cmd , "read_ack" ) if int ( self . proto [ 0 : 1 ] ) == 1 else self . _send_cmd ( cmd , "read_rsp" ) _LOGGER . debug ( "read_ack << %s" , resp ) return self . push_data ( resp )
def push_data ( self , data ) : if not _validate_data ( data ) : return False jdata = json . loads ( data [ 'data' ] ) if int ( self . proto [ 0 : 1 ] ) == 1 else _list2map ( data [ 'params' ] ) if jdata is None : return False sid = data [ 'sid' ] for func in self . callbacks [ sid ] : func ( jdata , data ) return True
def _get_key ( self ) : init_vector = bytes ( bytearray . fromhex ( '17996d093d28ddb3ba695a2e6f58562e' ) ) encryptor = Cipher ( algorithms . AES ( self . key . encode ( ) ) , modes . CBC ( init_vector ) , backend = default_backend ( ) ) . encryptor ( ) ciphertext = encryptor . update ( self . token . encode ( ) ) + encryptor . finalize ( ) if isinstance ( ciphertext , str ) : return '' . join ( '{:02x}' . format ( ord ( x ) ) for x in ciphertext ) return '' . join ( '{:02x}' . format ( x ) for x in ciphertext )
def _ensure_log_handler ( self ) : if log . handlers : return handler = logging . StreamHandler ( ) formatter = logging . Formatter ( '%(asctime)s %(levelname)-5.5s [%(name)s][%(threadName)s] %(message)s' ) handler . setFormatter ( formatter ) log . addHandler ( handler )
def lambda_function ( f ) : @ functools . wraps ( f ) def wrapper ( event , context ) : global _CURRENT_LAMBDA_CONTEXT _CURRENT_LAMBDA_CONTEXT = context try : result = f ( event , context ) return wait ( lambda : result ) except : cls , exc , trace = sys . exc_info ( ) report_exc_info ( ( cls , exc , trace . tb_next ) ) wait ( ) raise return wrapper
def _create_agent_log ( ) : log_file = SETTINGS [ 'agent.log_file' ] if not log_file . endswith ( '.rollbar' ) : log . error ( "Provided agent log file does not end with .rollbar, which it must. " "Using default instead." ) log_file = DEFAULTS [ 'agent.log_file' ] retval = logging . getLogger ( 'rollbar_agent' ) handler = logging . FileHandler ( log_file , 'a' , 'utf-8' ) formatter = logging . Formatter ( '%(message)s' ) handler . setFormatter ( formatter ) retval . addHandler ( handler ) retval . setLevel ( logging . WARNING ) return retval
def _add_lambda_context_data ( data ) : global _CURRENT_LAMBDA_CONTEXT context = _CURRENT_LAMBDA_CONTEXT if context is None : return try : lambda_data = { 'lambda' : { 'remaining_time_in_millis' : context . get_remaining_time_in_millis ( ) , 'function_name' : context . function_name , 'function_version' : context . function_version , 'arn' : context . invoked_function_arn , 'request_id' : context . aws_request_id , } } if 'custom' in data : data [ 'custom' ] = dict_merge ( data [ 'custom' ] , lambda_data ) else : data [ 'custom' ] = lambda_data except Exception as e : log . exception ( "Exception while adding lambda context data: %r" , e ) finally : _CURRENT_LAMBDA_CONTEXT = None
def _add_request_data ( data , request ) : try : request_data = _build_request_data ( request ) except Exception as e : log . exception ( "Exception while building request_data for Rollbar payload: %r" , e ) else : if request_data : _filter_ip ( request_data , SETTINGS [ 'capture_ip' ] ) data [ 'request' ] = request_data
def _check_add_locals ( frame , frame_num , total_frames ) : return any ( ( ( frame_num == total_frames - 1 ) , ( 'root' in SETTINGS and ( frame . get ( 'filename' ) or '' ) . lower ( ) . startswith ( ( SETTINGS [ 'root' ] or '' ) . lower ( ) ) ) ) )
def _build_server_data ( ) : server_data = { 'host' : socket . gethostname ( ) , 'pid' : os . getpid ( ) } argv = getattr ( sys , 'argv' , None ) if argv : server_data [ 'argv' ] = argv for key in [ 'branch' , 'root' ] : if SETTINGS . get ( key ) : server_data [ key ] = SETTINGS [ key ] return server_data
def _build_payload ( data ) : for k , v in iteritems ( data ) : data [ k ] = _transform ( v , key = ( k , ) ) payload = { 'access_token' : SETTINGS [ 'access_token' ] , 'data' : data } return payload
def main ( ) : rollbar . init ( 'ACCESS_TOKEN' , environment = 'test' , handler = 'twisted' ) factory = protocol . ServerFactory ( ) factory . protocol = Echo reactor . listenTCP ( 8000 , factory ) reactor . run ( )
def decompose ( hangul_letter ) : from . import checker if len ( hangul_letter ) < 1 : raise NotLetterException ( '' ) elif not checker . is_hangul ( hangul_letter ) : raise NotHangulException ( '' ) if hangul_letter in CHO : return hangul_letter , '' , '' if hangul_letter in JOONG : return '' , hangul_letter , '' if hangul_letter in JONG : return '' , '' , hangul_letter code = hangul_index ( hangul_letter ) cho , joong , jong = decompose_index ( code ) if cho < 0 : cho = 0 try : return CHO [ cho ] , JOONG [ joong ] , JONG [ jong ] except : print ( "%d / %d  / %d" % ( cho , joong , jong ) ) print ( "%s / %s " % ( JOONG [ joong ] . encode ( "utf8" ) , JONG [ jong ] . encode ( 'utf8' ) ) ) raise Exception ( )
def has_jongsung ( letter ) : if len ( letter ) != 1 : raise Exception ( 'The target string must be one letter.' ) if not is_hangul ( letter ) : raise NotHangulException ( 'The target string must be Hangul' ) code = lt . hangul_index ( letter ) return code % NUM_JONG > 0
def attach ( word , josa = EUN_NEUN ) : last_letter = word . strip ( ) [ - 1 ] try : _ , _ , letter_jong = letter . decompose ( last_letter ) except NotHangulException : letter_jong = letter . get_substituent_of ( last_letter ) if letter_jong in ( '' , josa [ 'except' ] ) : return word + josa [ 'has' ] return word + josa [ 'not' ]
def is_inside_except ( node ) : current = node while current and not isinstance ( current . parent , astroid . ExceptHandler ) : current = current . parent return current and current is current . parent . name
def is_inside_lambda ( node : astroid . node_classes . NodeNG ) -> bool : parent = node . parent while parent is not None : if isinstance ( parent , astroid . Lambda ) : return True parent = parent . parent return False
def get_all_elements ( node : astroid . node_classes . NodeNG ) -> Iterable [ astroid . node_classes . NodeNG ] : if isinstance ( node , ( astroid . Tuple , astroid . List ) ) : for child in node . elts : for e in get_all_elements ( child ) : yield e else : yield node
def is_super ( node : astroid . node_classes . NodeNG ) -> bool : if getattr ( node , "name" , None ) == "super" and node . root ( ) . name == BUILTINS_NAME : return True return False
def is_error ( node : astroid . node_classes . NodeNG ) -> bool : for child_node in node . get_children ( ) : if isinstance ( child_node , astroid . Raise ) : return True return False
def is_builtin_object ( node : astroid . node_classes . NodeNG ) -> bool : return node and node . root ( ) . name == BUILTINS_NAME
def is_func_decorator ( node : astroid . node_classes . NodeNG ) -> bool : parent = node . parent while parent is not None : if isinstance ( parent , astroid . Decorators ) : return True if parent . is_statement or isinstance ( parent , ( astroid . Lambda , scoped_nodes . ComprehensionScope , scoped_nodes . ListComp ) , ) : break parent = parent . parent return False
def assign_parent ( node : astroid . node_classes . NodeNG ) -> astroid . node_classes . NodeNG : while node and isinstance ( node , ( astroid . AssignName , astroid . Tuple , astroid . List ) ) : node = node . parent return node
def check_messages ( * messages : str ) -> Callable : def store_messages ( func ) : func . checks_msgs = messages return func return store_messages
def decorated_with_property ( node : astroid . FunctionDef ) -> bool : if not node . decorators : return False for decorator in node . decorators . nodes : if not isinstance ( decorator , astroid . Name ) : continue try : if _is_property_decorator ( decorator ) : return True except astroid . InferenceError : pass return False
def decorated_with ( func : astroid . FunctionDef , qnames : Iterable [ str ] ) -> bool : decorators = func . decorators . nodes if func . decorators else [ ] for decorator_node in decorators : try : if any ( i is not None and i . qname ( ) in qnames for i in decorator_node . infer ( ) ) : return True except astroid . InferenceError : continue return False
def find_try_except_wrapper_node ( node : astroid . node_classes . NodeNG ) -> Union [ astroid . ExceptHandler , astroid . TryExcept ] : current = node ignores = ( astroid . ExceptHandler , astroid . TryExcept ) while current and not isinstance ( current . parent , ignores ) : current = current . parent if current and isinstance ( current . parent , ignores ) : return current . parent return None
def is_from_fallback_block ( node : astroid . node_classes . NodeNG ) -> bool : context = find_try_except_wrapper_node ( node ) if not context : return False if isinstance ( context , astroid . ExceptHandler ) : other_body = context . parent . body handlers = context . parent . handlers else : other_body = itertools . chain . from_iterable ( handler . body for handler in context . handlers ) handlers = context . handlers has_fallback_imports = any ( isinstance ( import_node , ( astroid . ImportFrom , astroid . Import ) ) for import_node in other_body ) ignores_import_error = _except_handlers_ignores_exception ( handlers , ImportError ) return ignores_import_error or has_fallback_imports
def is_registered_in_singledispatch_function ( node : astroid . FunctionDef ) -> bool : singledispatch_qnames = ( "functools.singledispatch" , "singledispatch.singledispatch" , ) if not isinstance ( node , astroid . FunctionDef ) : return False decorators = node . decorators . nodes if node . decorators else [ ] for decorator in decorators : if not isinstance ( decorator , astroid . Call ) : continue func = decorator . func if not isinstance ( func , astroid . Attribute ) or func . attrname != "register" : continue try : func_def = next ( func . expr . infer ( ) ) except astroid . InferenceError : continue if isinstance ( func_def , astroid . FunctionDef ) : return decorated_with ( func_def , singledispatch_qnames ) return False
def is_postponed_evaluation_enabled ( node : astroid . node_classes . NodeNG ) -> bool : name = "annotations" module = node . root ( ) stmt = module . locals . get ( name ) return ( stmt and isinstance ( stmt [ 0 ] , astroid . ImportFrom ) and stmt [ 0 ] . modname == "__future__" )
def _repr_tree_defs ( data , indent_str = None ) : lines = [ ] nodes = data . items ( ) for i , ( mod , ( sub , files ) ) in enumerate ( sorted ( nodes , key = lambda x : x [ 0 ] ) ) : if not files : files = "" else : files = "(%s)" % "," . join ( sorted ( files ) ) if indent_str is None : lines . append ( "%s %s" % ( mod , files ) ) sub_indent_str = "  " else : lines . append ( r"%s\-%s %s" % ( indent_str , mod , files ) ) if i == len ( nodes ) - 1 : sub_indent_str = "%s  " % indent_str else : sub_indent_str = "%s| " % indent_str if sub : lines . append ( _repr_tree_defs ( sub , sub_indent_str ) ) return "\n" . join ( lines )
def visit_import ( self , node ) : self . _check_reimport ( node ) self . _check_import_as_rename ( node ) modnode = node . root ( ) names = [ name for name , _ in node . names ] if len ( names ) >= 2 : self . add_message ( "multiple-imports" , args = ", " . join ( names ) , node = node ) for name in names : self . _check_deprecated_module ( node , name ) self . _check_preferred_module ( node , name ) imported_module = self . _get_imported_module ( node , name ) if isinstance ( node . parent , astroid . Module ) : self . _check_position ( node ) if isinstance ( node . scope ( ) , astroid . Module ) : self . _record_import ( node , imported_module ) if imported_module is None : continue self . _check_relative_import ( modnode , node , imported_module , name ) self . _add_imported_module ( node , imported_module . name )
def visit_importfrom ( self , node ) : basename = node . modname imported_module = self . _get_imported_module ( node , basename ) self . _check_import_as_rename ( node ) self . _check_misplaced_future ( node ) self . _check_deprecated_module ( node , basename ) self . _check_preferred_module ( node , basename ) self . _check_wildcard_imports ( node , imported_module ) self . _check_same_line_imports ( node ) self . _check_reimport ( node , basename = basename , level = node . level ) if isinstance ( node . parent , astroid . Module ) : self . _check_position ( node ) if isinstance ( node . scope ( ) , astroid . Module ) : self . _record_import ( node , imported_module ) if imported_module is None : return modnode = node . root ( ) self . _check_relative_import ( modnode , node , imported_module , basename ) for name , _ in node . names : if name != "*" : self . _add_imported_module ( node , "%s.%s" % ( imported_module . name , name ) ) else : self . _add_imported_module ( node , imported_module . name )
def _record_import ( self , node , importedmodnode ) : if isinstance ( node , astroid . ImportFrom ) : importedname = node . modname else : importedname = importedmodnode . name if importedmodnode else None if not importedname : importedname = node . names [ 0 ] [ 0 ] . split ( "." ) [ 0 ] if isinstance ( node , astroid . ImportFrom ) and ( node . level or 0 ) >= 1 : importedname = "." + importedname self . _imports_stack . append ( ( node , importedname ) )
def _add_imported_module ( self , node , importedmodname ) : module_file = node . root ( ) . file context_name = node . root ( ) . name base = os . path . splitext ( os . path . basename ( module_file ) ) [ 0 ] try : importedmodname = astroid . modutils . get_module_part ( importedmodname , module_file ) except ImportError : pass if context_name == importedmodname : self . add_message ( "import-self" , node = node ) elif not astroid . modutils . is_standard_module ( importedmodname ) : if base != "__init__" and context_name not in self . _module_pkg : self . _module_pkg [ context_name ] = context_name . rsplit ( "." , 1 ) [ 0 ] importedmodnames = self . stats [ "dependencies" ] . setdefault ( importedmodname , set ( ) ) if context_name not in importedmodnames : importedmodnames . add ( context_name ) self . import_graph [ context_name ] . add ( importedmodname ) if not self . linter . is_message_enabled ( "cyclic-import" , line = node . lineno ) : self . _excluded_edges [ context_name ] . add ( importedmodname )
def _check_deprecated_module ( self , node , mod_path ) : for mod_name in self . config . deprecated_modules : if mod_path == mod_name or mod_path . startswith ( mod_name + "." ) : self . add_message ( "deprecated-module" , node = node , args = mod_path )
def _check_preferred_module ( self , node , mod_path ) : if mod_path in self . preferred_modules : self . add_message ( "preferred-module" , node = node , args = ( self . preferred_modules [ mod_path ] , mod_path ) , )
def _report_external_dependencies ( self , sect , _ , _dummy ) : dep_info = _make_tree_defs ( self . _external_dependencies_info ( ) . items ( ) ) if not dep_info : raise EmptyReportError ( ) tree_str = _repr_tree_defs ( dep_info ) sect . append ( VerbatimText ( tree_str ) )
def _filter_dependencies_graph ( self , internal ) : graph = collections . defaultdict ( set ) for importee , importers in self . stats [ "dependencies" ] . items ( ) : for importer in importers : package = self . _module_pkg . get ( importer , importer ) is_inside = importee . startswith ( package ) if is_inside and internal or not is_inside and not internal : graph [ importee ] . add ( importer ) return graph
def get_default_options ( ) : options = [ ] home = os . environ . get ( "HOME" , "" ) if home : rcfile = os . path . join ( home , RCFILE ) try : options = open ( rcfile ) . read ( ) . split ( ) except IOError : pass return options
def insert_default_options ( ) : options = get_default_options ( ) options . reverse ( ) for arg in options : sys . argv . insert ( 1 , arg )
def show_attr ( self , node ) : visibility = get_visibility ( getattr ( node , "name" , node ) ) return not self . __mode & VIS_MOD [ visibility ]
def get_callbacks ( self , node ) : klass = node . __class__ methods = self . _cache . get ( klass ) if methods is None : handler = self . handler kid = klass . __name__ . lower ( ) e_method = getattr ( handler , "visit_%s" % kid , getattr ( handler , "visit_default" , None ) ) l_method = getattr ( handler , "leave_%s" % kid , getattr ( handler , "leave_default" , None ) ) self . _cache [ klass ] = ( e_method , l_method ) else : e_method , l_method = methods return e_method , l_method
def visit ( self , node ) : if node in self . _visited : return None self . _visited [ node ] = 1 methods = self . get_callbacks ( node ) if methods [ 0 ] is not None : methods [ 0 ] ( node ) if hasattr ( node , "locals" ) : for local_node in node . values ( ) : self . visit ( local_node ) if methods [ 1 ] is not None : return methods [ 1 ] ( node ) return None
def visit_call ( self , node ) : try : for inferred in node . func . infer ( ) : if inferred is astroid . Uninferable : continue elif inferred . root ( ) . name == OPEN_MODULE : if getattr ( node . func , "name" , None ) in OPEN_FILES : self . _check_open_mode ( node ) elif inferred . root ( ) . name == UNITTEST_CASE : self . _check_redundant_assert ( node , inferred ) elif isinstance ( inferred , astroid . ClassDef ) : if inferred . qname ( ) == THREADING_THREAD : self . _check_bad_thread_instantiation ( node ) elif inferred . qname ( ) == SUBPROCESS_POPEN : self . _check_for_preexec_fn_in_popen ( node ) elif isinstance ( inferred , astroid . FunctionDef ) : name = inferred . qname ( ) if name == COPY_COPY : self . _check_shallow_copy_environ ( node ) elif name in ENV_GETTERS : self . _check_env_function ( node , inferred ) elif name == SUBPROCESS_RUN and PY35 : self . _check_for_check_kw_in_run ( node ) self . _check_deprecated_method ( node , inferred ) except astroid . InferenceError : return
def _check_open_mode ( self , node ) : try : mode_arg = utils . get_argument_from_call ( node , position = 1 , keyword = "mode" ) except utils . NoSuchArgumentError : return if mode_arg : mode_arg = utils . safe_infer ( mode_arg ) if isinstance ( mode_arg , astroid . Const ) and not _check_mode_str ( mode_arg . value ) : self . add_message ( "bad-open-mode" , node = node , args = mode_arg . value )
def handle_message ( self , msg ) : self . messages . append ( { "type" : msg . category , "module" : msg . module , "obj" : msg . obj , "line" : msg . line , "column" : msg . column , "path" : msg . path , "symbol" : msg . symbol , "message" : html . escape ( msg . msg or "" , quote = False ) , "message-id" : msg . msg_id , } )
def get_title ( self , node ) : title = node . name if self . module_names : title = "%s.%s" % ( node . root ( ) . name , title ) return title
def _set_default_options ( self ) : self . module_names = self . _set_option ( self . config . module_names ) all_ancestors = self . _set_option ( self . config . all_ancestors ) all_associated = self . _set_option ( self . config . all_associated ) anc_level , association_level = ( 0 , 0 ) if all_ancestors : anc_level = - 1 if all_associated : association_level = - 1 if self . config . show_ancestors is not None : anc_level = self . config . show_ancestors if self . config . show_associated is not None : association_level = self . config . show_associated self . anc_level , self . association_level = anc_level , association_level
def show_node ( self , node ) : if self . config . show_builtin : return True return node . root ( ) . name != BUILTINS_NAME
def add_class ( self , node ) : self . linker . visit ( node ) self . classdiagram . add_object ( self . get_title ( node ) , node )
def get_ancestors ( self , node , level ) : if level == 0 : return for ancestor in node . ancestors ( recurs = False ) : if not self . show_node ( ancestor ) : continue yield ancestor
def get_associated ( self , klass_node , level ) : if level == 0 : return for association_nodes in list ( klass_node . instance_attrs_type . values ( ) ) + list ( klass_node . locals_type . values ( ) ) : for node in association_nodes : if isinstance ( node , astroid . Instance ) : node = node . _proxied if not ( isinstance ( node , astroid . ClassDef ) and self . show_node ( node ) ) : continue yield node
def extract_classes ( self , klass_node , anc_level , association_level ) : if self . classdiagram . has_node ( klass_node ) or not self . show_node ( klass_node ) : return self . add_class ( klass_node ) for ancestor in self . get_ancestors ( klass_node , anc_level ) : self . extract_classes ( ancestor , anc_level - 1 , association_level ) for node in self . get_associated ( klass_node , association_level ) : self . extract_classes ( node , anc_level , association_level - 1 )
def visit_importfrom ( self , node ) : if self . pkgdiagram : self . pkgdiagram . add_from_depend ( node , node . modname )
def _has_parent_of_type ( node , node_type , statement ) : parent = node . parent while not isinstance ( parent , node_type ) and statement . parent_of ( parent ) : parent = parent . parent return isinstance ( parent , node_type )
def _is_name_used_as_variadic ( name , variadics ) : return any ( variadic . value == name or variadic . value . parent_of ( name ) for variadic in variadics )
def register ( linter ) : linter . register_checker ( TypeChecker ( linter ) ) linter . register_checker ( IterableChecker ( linter ) )
def visit_unaryop ( self , node ) : for error in node . type_errors ( ) : self . add_message ( "invalid-unary-operand-type" , args = str ( error ) , node = node )
def interfaces ( node , herited = True , handler_func = _iface_hdlr ) : try : implements = bases . Instance ( node ) . getattr ( "__implements__" ) [ 0 ] except exceptions . NotFoundError : return if not herited and implements . frame ( ) is not node : return found = set ( ) missing = False for iface in node_classes . unpack_infer ( implements ) : if iface is astroid . Uninferable : missing = True continue if iface not in found and handler_func ( iface ) : found . add ( iface ) yield iface if missing : raise exceptions . InferenceError ( )
def project_from_files ( files , func_wrapper = _astroid_wrapper , project_name = "no name" , black_list = ( "CVS" , ) ) : astroid_manager = manager . AstroidManager ( ) project = Project ( project_name ) for something in files : if not os . path . exists ( something ) : fpath = modutils . file_from_modpath ( something . split ( "." ) ) elif os . path . isdir ( something ) : fpath = os . path . join ( something , "__init__.py" ) else : fpath = something ast = func_wrapper ( astroid_manager . ast_from_file , fpath ) if ast is None : continue project . path = project . path or ast . file project . add_module ( ast ) base_name = ast . name if ast . package and something . find ( "__init__" ) == - 1 : for fpath in modutils . get_module_files ( os . path . dirname ( ast . file ) , black_list ) : ast = func_wrapper ( astroid_manager . ast_from_file , fpath ) if ast is None or ast . name == base_name : continue project . add_module ( ast ) return project
def compute_module ( self , context_name , mod_path ) : package_dir = os . path . dirname ( self . project . path ) if context_name == mod_path : return 0 if modutils . is_standard_module ( mod_path , ( package_dir , ) ) : return 1 return 0
def _imported_module ( self , node , mod_path , relative ) : module = node . root ( ) context_name = module . name if relative : mod_path = "%s.%s" % ( "." . join ( context_name . split ( "." ) [ : - 1 ] ) , mod_path ) if self . compute_module ( context_name , mod_path ) : if not hasattr ( module , "depends" ) : module . depends = [ ] mod_paths = module . depends if mod_path not in mod_paths : mod_paths . append ( mod_path )
def register ( linter ) : linter . register_reporter ( TextReporter ) linter . register_reporter ( ParseableTextReporter ) linter . register_reporter ( VSTextReporter ) linter . register_reporter ( ColorizedTextReporter )
def handle_message ( self , msg ) : if msg . module not in self . _modules : if msg . module : self . writeln ( "************* Module %s" % msg . module ) self . _modules . add ( msg . module ) else : self . writeln ( "************* " ) self . write_message ( msg )
def open_graph ( self , * * args ) : self . _stream . write ( "%sgraph:{\n" % self . _indent ) self . _inc_indent ( ) self . _write_attributes ( GRAPH_ATTRS , * * args )
def edge ( self , from_node , to_node , edge_type = "" , * * args ) : self . _stream . write ( '%s%sedge: {sourcename:"%s" targetname:"%s"' % ( self . _indent , edge_type , from_node , to_node ) ) self . _write_attributes ( EDGE_ATTRS , * * args ) self . _stream . write ( "}\n" )
def _write_attributes ( self , attributes_dict , * * args ) : for key , value in args . items ( ) : try : _type = attributes_dict [ key ] except KeyError : raise Exception ( % ( key , attributes_dict . keys ( ) ) ) if not _type : self . _stream . write ( '%s%s:"%s"\n' % ( self . _indent , key , value ) ) elif _type == 1 : self . _stream . write ( "%s%s:%s\n" % ( self . _indent , key , int ( value ) ) ) elif value in _type : self . _stream . write ( "%s%s:%s\n" % ( self . _indent , key , value ) ) else : raise Exception ( % ( value , key , _type ) )
def register ( linter ) : linter . register_checker ( StringFormatChecker ( linter ) ) linter . register_checker ( StringConstantChecker ( linter ) )
def _check_new_format ( self , node , func ) : # # if isinstance ( node . func , astroid . Attribute ) and not isinstance ( node . func . expr , astroid . Const ) : return if node . starargs or node . kwargs : return try : strnode = next ( func . bound . infer ( ) ) except astroid . InferenceError : return if not ( isinstance ( strnode , astroid . Const ) and isinstance ( strnode . value , str ) ) : return try : call_site = CallSite . from_call ( node ) except astroid . InferenceError : return try : fields , num_args , manual_pos = utils . parse_format_method_string ( strnode . value ) except utils . IncompleteFormatString : self . add_message ( "bad-format-string" , node = node ) return positional_arguments = call_site . positional_arguments named_arguments = call_site . keyword_arguments named_fields = { field [ 0 ] for field in fields if isinstance ( field [ 0 ] , str ) } if num_args and manual_pos : self . add_message ( "format-combined-specification" , node = node ) return check_args = False num_args += sum ( 1 for field in named_fields if field == "" ) if named_fields : for field in named_fields : if field and field not in named_arguments : self . add_message ( "missing-format-argument-key" , node = node , args = ( field , ) ) for field in named_arguments : if field not in named_fields : self . add_message ( "unused-format-string-argument" , node = node , args = ( field , ) ) num_args = num_args or manual_pos if positional_arguments or num_args : empty = any ( True for field in named_fields if field == "" ) if named_arguments or empty : check_args = True else : check_args = True if check_args : num_args = num_args or manual_pos if len ( positional_arguments ) > num_args : self . add_message ( "too-many-format-args" , node = node ) elif len ( positional_arguments ) < num_args : self . add_message ( "too-few-format-args" , node = node ) self . _detect_vacuous_formatting ( node , positional_arguments ) self . _check_new_format_specifiers ( node , fields , named_arguments )
def visit_section ( self , layout ) : self . section += 1 self . writeln ( ) self . format_children ( layout ) self . section -= 1 self . writeln ( )
def visit_evaluationsection ( self , layout ) : self . section += 1 self . format_children ( layout ) self . section -= 1 self . writeln ( )
def visit_table ( self , layout ) : table_content = self . get_table_content ( layout ) cols_width = [ 0 ] * len ( table_content [ 0 ] ) for row in table_content : for index , col in enumerate ( row ) : cols_width [ index ] = max ( cols_width [ index ] , len ( col ) ) self . default_table ( layout , table_content , cols_width ) self . writeln ( )
def _check_symbol ( self , msgid , symbol ) : other_message = self . _messages_definitions . get ( symbol ) if other_message : self . _raise_duplicate_msg_id ( symbol , msgid , other_message . msgid ) else : alternative_msgid = None alternative_message = self . _alternative_names . get ( symbol ) if alternative_message : if alternative_message . symbol == symbol : alternative_msgid = alternative_message . msgid else : for old_msgid , old_symbol in alternative_message . old_names : if old_symbol == symbol : alternative_msgid = old_msgid break if msgid != alternative_msgid : self . _raise_duplicate_msg_id ( symbol , msgid , alternative_msgid )
def help_message ( self , msgids ) : for msgid in msgids : try : for message_definition in self . get_message_definitions ( msgid ) : print ( message_definition . format_help ( checkerref = True ) ) print ( "" ) except UnknownMessageError as ex : print ( ex ) print ( "" ) continue
def list_messages ( self ) : messages = sorted ( self . _messages_definitions . values ( ) , key = lambda m : m . msgid ) for message in messages : if not message . may_be_emitted ( ) : continue print ( message . format_help ( checkerref = False ) ) print ( "" )
def builder_inited ( app ) : base_path = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) ext_path = os . path . join ( base_path , "pylint" , "extensions" ) modules = [ ] doc_files = { } for filename in os . listdir ( ext_path ) : name , ext = os . path . splitext ( filename ) if name [ 0 ] == "_" or name in DEPRECATED_MODULES : continue if ext == ".py" : modules . append ( "pylint.extensions.%s" % name ) elif ext == ".rst" : doc_files [ "pylint.extensions." + name ] = os . path . join ( ext_path , filename ) modules . sort ( ) if not modules : sys . exit ( "No Pylint extensions found?" ) linter = PyLinter ( ) linter . load_plugin_modules ( modules ) extensions_doc = os . path . join ( base_path , "doc" , "technical_reference" , "extensions.rst" ) with open ( extensions_doc , "w" ) as stream : stream . write ( "Optional Pylint checkers in the extensions module\n" ) stream . write ( "=================================================\n\n" ) stream . write ( "Pylint provides the following optional plugins:\n\n" ) for module in modules : stream . write ( "- :ref:`{}`\n" . format ( module ) ) stream . write ( "\n" ) stream . write ( "You can activate any or all of these extensions " "by adding a ``load-plugins`` line to the ``MASTER`` " "section of your ``.pylintrc``, for example::\n" ) stream . write ( "\n    load-plugins=pylint.extensions.docparams," "pylint.extensions.docstyle\n\n" ) by_module = get_plugins_info ( linter , doc_files ) for module , info in sorted ( by_module . items ( ) ) : linter . _print_checker_doc ( info [ "name" ] , info , stream = stream )
def _cpu_count ( ) -> int : sched_getaffinity = getattr ( os , "sched_getaffinity" , None ) if sched_getaffinity : return len ( sched_getaffinity ( 0 ) ) if multiprocessing : return multiprocessing . cpu_count ( ) return 1
def report_messages_stats ( sect , stats , _ ) : if not stats [ "by_msg" ] : raise exceptions . EmptyReportError ( ) in_order = sorted ( [ ( value , msg_id ) for msg_id , value in stats [ "by_msg" ] . items ( ) if not msg_id . startswith ( "I" ) ] ) in_order . reverse ( ) lines = ( "message id" , "occurrences" ) for value , msg_id in in_order : lines += ( msg_id , str ( value ) ) sect . append ( report_nodes . Table ( children = lines , cols = 2 , rheaders = 1 ) )
def python3_porting_mode ( self ) : self . disable ( "all" ) self . enable ( "python3" ) if self . _error_mode : for msg_id in self . _checker_messages ( "python3" ) : if msg_id . startswith ( "E" ) : self . enable ( msg_id ) else : self . disable ( msg_id ) config_parser = self . cfgfile_parser if config_parser . has_option ( "MESSAGES CONTROL" , "disable" ) : value = config_parser . get ( "MESSAGES CONTROL" , "disable" ) self . global_set_option ( "disable" , value ) self . _python3_porting_mode = True
def get_checkers ( self ) : return [ self ] + [ c for _checkers in self . _checkers . values ( ) for c in _checkers if c is not self ]
def get_checker_names ( self ) : current_checkers = self . get_checkers ( ) return sorted ( { check . name for check in current_checkers if check . name != "master" } )
def prepare_checkers ( self ) : if not self . config . reports : self . disable_reporters ( ) neededcheckers = [ self ] for checker in self . get_checkers ( ) [ 1 : ] : messages = { msg for msg in checker . msgs if self . is_message_enabled ( msg ) } if messages or any ( self . report_is_enabled ( r [ 0 ] ) for r in checker . reports ) : neededcheckers . append ( checker ) neededcheckers = sorted ( neededcheckers , key = operator . attrgetter ( "priority" ) , reverse = True ) return neededcheckers
def expand_files ( self , modules ) : result , errors = utils . expand_modules ( modules , self . config . black_list , self . config . black_list_re ) for error in errors : message = modname = error [ "mod" ] key = error [ "key" ] self . set_current_module ( modname ) if key == "fatal" : message = str ( error [ "ex" ] ) . replace ( os . getcwd ( ) + os . sep , "" ) self . add_message ( key , args = message ) return result
def check_astroid_module ( self , ast_node , walker , rawcheckers , tokencheckers ) : try : tokens = utils . tokenize_module ( ast_node ) except tokenize . TokenError as ex : self . add_message ( "syntax-error" , line = ex . args [ 1 ] [ 0 ] , args = ex . args [ 0 ] ) return None if not ast_node . pure_python : self . add_message ( "raw-checker-failed" , args = ast_node . name ) else : self . process_tokens ( tokens ) if self . _ignore_file : return False self . file_state . collect_block_lines ( self . msgs_store , ast_node ) for checker in rawcheckers : checker . process_module ( ast_node ) for checker in tokencheckers : checker . process_tokens ( tokens ) walker . walk ( ast_node ) return True
def _report_evaluation ( self ) : previous_stats = config . load_results ( self . file_state . base_name ) if self . stats [ "statement" ] == 0 : return evaluation = self . config . evaluation try : note = eval ( evaluation , { } , self . stats ) except Exception as ex : msg = "An exception occurred while rating: %s" % ex else : self . stats [ "global_note" ] = note msg = "Your code has been rated at %.2f/10" % note pnote = previous_stats . get ( "global_note" ) if pnote is not None : msg += " (previous run: %.2f/10, %+.2f)" % ( pnote , note - pnote ) if self . config . score : sect = report_nodes . EvaluationSection ( msg ) self . reporter . display_reports ( sect )
def cb_generate_config ( self , * args , * * kwargs ) : self . linter . generate_config ( skipsections = ( "COMMANDS" , ) ) sys . exit ( 0 )
def cb_generate_manpage ( self , * args , * * kwargs ) : from pylint import __pkginfo__ self . linter . generate_manpage ( __pkginfo__ ) sys . exit ( 0 )
def cb_help_message ( self , option , optname , value , parser ) : self . linter . msgs_store . help_message ( utils . _splitstrip ( value ) ) sys . exit ( 0 )
def cb_full_documentation ( self , option , optname , value , parser ) : self . linter . print_full_documentation ( ) sys . exit ( 0 )
def cb_list_messages ( self , option , optname , value , parser ) : self . linter . msgs_store . list_messages ( ) sys . exit ( 0 )
def normalize_text ( text , line_len = 80 , indent = "" ) : return "\n" . join ( textwrap . wrap ( text , width = line_len , initial_indent = indent , subsequent_indent = indent ) )
def get_module_and_frameid ( node ) : frame = node . frame ( ) module , obj = "" , [ ] while frame : if isinstance ( frame , Module ) : module = frame . name else : obj . append ( getattr ( frame , "name" , "<lambda>" ) ) try : frame = frame . parent . frame ( ) except AttributeError : frame = None obj . reverse ( ) return module , "." . join ( obj )
def safe_decode ( line , encoding , * args , * * kwargs ) : try : return line . decode ( encoding or sys . getdefaultencoding ( ) , * args , * * kwargs ) except LookupError : return line . decode ( sys . getdefaultencoding ( ) , * args , * * kwargs )
def _comment ( string ) : lines = [ line . strip ( ) for line in string . splitlines ( ) ] return + ( % linesep ) . join ( lines )
def _format_option_value ( optdict , value ) : if isinstance ( value , ( list , tuple ) ) : value = "," . join ( _format_option_value ( optdict , item ) for item in value ) elif isinstance ( value , dict ) : value = "," . join ( "%s:%s" % ( k , v ) for k , v in value . items ( ) ) elif hasattr ( value , "match" ) : value = value . pattern elif optdict . get ( "type" ) == "yn" : value = "yes" if value else "no" elif isinstance ( value , str ) and value . isspace ( ) : value = "'%s'" % value return value
def format_section ( stream , section , options , doc = None ) : if doc : print ( _comment ( doc ) , file = stream ) print ( "[%s]" % section , file = stream ) _ini_format ( stream , options )
def _ini_format ( stream , options ) : for optname , optdict , value in options : value = _format_option_value ( optdict , value ) help_opt = optdict . get ( "help" ) if help_opt : help_opt = normalize_text ( help_opt , line_len = 79 , indent = ) print ( file = stream ) print ( help_opt , file = stream ) else : print ( file = stream ) if value is None : print ( "#%s=" % optname , file = stream ) else : value = str ( value ) . strip ( ) if re . match ( r"^([\w-]+,)+[\w-]+$" , str ( value ) ) : separator = "\n " + " " * len ( optname ) value = separator . join ( x + "," for x in str ( value ) . split ( "," ) ) value = value [ : - 1 ] print ( "%s=%s" % ( optname , value ) , file = stream )
def insert ( self , index , child ) : self . children . insert ( index , child ) child . parent = self
def append ( self , child ) : assert child not in self . parents ( ) VNode . append ( self , child )
def parents ( self ) : assert self . parent is not self if self . parent is None : return [ ] return [ self . parent ] + self . parent . parents ( )
def collect_block_lines ( self , msgs_store , module_node ) : for msg , lines in self . _module_msgs_state . items ( ) : self . _raw_module_msgs_state [ msg ] = lines . copy ( ) orig_state = self . _module_msgs_state . copy ( ) self . _module_msgs_state = { } self . _suppression_mapping = { } self . _effective_max_line_number = module_node . tolineno self . _collect_block_lines ( msgs_store , module_node , orig_state )
def enable_report ( self , reportid ) : reportid = reportid . upper ( ) self . _reports_state [ reportid ] = True
def disable_report ( self , reportid ) : reportid = reportid . upper ( ) self . _reports_state [ reportid ] = False
def register ( linter ) : linter . register_checker ( EncodingChecker ( linter ) ) linter . register_checker ( ByIdManagedMessagesChecker ( linter ) )
def process_module ( self , module ) : managed_msgs = MessagesHandlerMixIn . get_by_id_managed_msgs ( ) for ( mod_name , msg_id , msg_symbol , lineno , is_disabled ) in managed_msgs : if mod_name == module . name : if is_disabled : txt = "Id '{ident}' is used to disable '{symbol}' message emission" . format ( ident = msg_id , symbol = msg_symbol ) else : txt = "Id '{ident}' is used to enable '{symbol}' message emission" . format ( ident = msg_id , symbol = msg_symbol ) self . add_message ( "use-symbolic-message-instead" , line = lineno , args = txt ) MessagesHandlerMixIn . clear_by_id_managed_msgs ( )
def process_module ( self , module ) : if module . file_encoding : encoding = module . file_encoding else : encoding = "ascii" with module . stream ( ) as stream : for lineno , line in enumerate ( stream ) : self . _check_encoding ( lineno + 1 , line , encoding )
def process_tokens ( self , tokens ) : if not self . config . notes : return comments = ( token_info for token_info in tokens if token_info . type == tokenize . COMMENT ) for comment in comments : comment_text = comment . string [ 1 : ] . lstrip ( ) disable_option_match = OPTION_RGX . search ( comment_text ) if disable_option_match : try : _ , value = disable_option_match . group ( 1 ) . split ( "=" , 1 ) values = [ _val . strip ( ) . upper ( ) for _val in value . split ( "," ) ] if set ( values ) & set ( self . config . notes ) : continue except ValueError : self . add_message ( "bad-inline-option" , args = disable_option_match . group ( 1 ) . strip ( ) , line = comment . string , ) continue match = self . _fixme_pattern . search ( "#" + comment_text . lower ( ) ) if match : note = match . group ( 1 ) self . add_message ( "fixme" , col_offset = comment . string . lower ( ) . index ( note . lower ( ) ) , args = comment_text , line = comment . start [ 0 ] , )
def _is_from_future_import ( stmt , name ) : try : module = stmt . do_import_module ( stmt . modname ) except astroid . AstroidBuildingException : return None for local_node in module . locals . get ( name , [ ] ) : if isinstance ( local_node , astroid . ImportFrom ) and local_node . modname == FUTURE : return True return None
def in_for_else_branch ( parent , stmt ) : return isinstance ( parent , astroid . For ) and any ( else_stmt . parent_of ( stmt ) or else_stmt == stmt for else_stmt in parent . orelse )
def overridden_method ( klass , name ) : try : parent = next ( klass . local_attr_ancestors ( name ) ) except ( StopIteration , KeyError ) : return None try : meth_node = parent [ name ] except KeyError : return None if isinstance ( meth_node , astroid . FunctionDef ) : return meth_node return None
def _assigned_locally ( name_node ) : assign_stmts = name_node . scope ( ) . nodes_of_class ( astroid . AssignName ) return any ( a . name == name_node . name for a in assign_stmts )
def visit_global ( self , node ) : frame = node . frame ( ) if isinstance ( frame , astroid . Module ) : self . add_message ( "global-at-module-level" , node = node ) return module = frame . root ( ) default_message = True locals_ = node . scope ( ) . locals for name in node . names : try : assign_nodes = module . getattr ( name ) except astroid . NotFoundError : assign_nodes = [ ] not_defined_locally_by_import = not any ( isinstance ( local , astroid . node_classes . Import ) for local in locals_ . get ( name , ( ) ) ) if not assign_nodes and not_defined_locally_by_import : self . add_message ( "global-variable-not-assigned" , args = name , node = node ) default_message = False continue for anode in assign_nodes : if ( isinstance ( anode , astroid . AssignName ) and anode . name in module . special_attributes ) : self . add_message ( "redefined-builtin" , args = name , node = node ) break if anode . frame ( ) is module : break else : if not_defined_locally_by_import : self . add_message ( "global-variable-undefined" , args = name , node = node ) default_message = False if default_message : self . add_message ( "global-statement" , node = node )
def visit_import ( self , node ) : if not self . _analyse_fallback_blocks and utils . is_from_fallback_block ( node ) : return for name , _ in node . names : parts = name . split ( "." ) try : module = next ( _infer_name_module ( node , parts [ 0 ] ) ) except astroid . ResolveError : continue self . _check_module_attrs ( node , module , parts [ 1 : ] )
def visit_importfrom ( self , node ) : if not self . _analyse_fallback_blocks and utils . is_from_fallback_block ( node ) : return name_parts = node . modname . split ( "." ) try : module = node . do_import_module ( name_parts [ 0 ] ) except astroid . AstroidBuildingException : return module = self . _check_module_attrs ( node , module , name_parts [ 1 : ] ) if not module : return for name , _ in node . names : if name == "*" : continue self . _check_module_attrs ( node , module , name . split ( "." ) )
def _check_metaclasses ( self , node ) : consumed = [ ] for child_node in node . get_children ( ) : if isinstance ( child_node , astroid . ClassDef ) : consumed . extend ( self . _check_classdef_metaclasses ( child_node , node ) ) for scope_locals , name in consumed : scope_locals . pop ( name , None )
def get_packages ( directory , prefix ) : result = [ ] for package in os . listdir ( directory ) : absfile = join ( directory , package ) if isdir ( absfile ) : if exists ( join ( absfile , "__init__.py" ) ) : if prefix : result . append ( "%s.%s" % ( prefix , package ) ) else : result . append ( package ) result += get_packages ( absfile , result [ - 1 ] ) return result
def run ( self ) : install_lib . install_lib . run ( self ) if include_dirs : for directory in include_dirs : dest = join ( self . install_dir , directory ) if sys . version_info >= ( 3 , 0 ) : exclude = { "invalid_encoded_data*" , "unknown_encoding*" } else : exclude = set ( ) shutil . rmtree ( dest , ignore_errors = True ) shutil . copytree ( directory , dest , ignore = shutil . ignore_patterns ( * exclude ) )
def report_similarities ( sect , stats , old_stats ) : lines = [ "" , "now" , "previous" , "difference" ] lines += table_lines_from_stats ( stats , old_stats , ( "nb_duplicated_lines" , "percent_duplicated_lines" ) ) sect . append ( Table ( children = lines , cols = 4 , rheaders = 1 , cheaders = 1 ) )
def Run ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] from getopt import getopt s_opts = "hdi" l_opts = ( "help" , "duplicates=" , "ignore-comments" , "ignore-imports" , "ignore-docstrings" , ) min_lines = 4 ignore_comments = False ignore_docstrings = False ignore_imports = False opts , args = getopt ( argv , s_opts , l_opts ) for opt , val in opts : if opt in ( "-d" , "--duplicates" ) : min_lines = int ( val ) elif opt in ( "-h" , "--help" ) : usage ( ) elif opt in ( "-i" , "--ignore-comments" ) : ignore_comments = True elif opt in ( "--ignore-docstrings" , ) : ignore_docstrings = True elif opt in ( "--ignore-imports" , ) : ignore_imports = True if not args : usage ( 1 ) sim = Similar ( min_lines , ignore_comments , ignore_docstrings , ignore_imports ) for filename in args : with open ( filename ) as stream : sim . append_stream ( filename , stream ) sim . run ( ) sys . exit ( 0 )
def append_stream ( self , streamid , stream , encoding = None ) : if encoding is None : readlines = stream . readlines else : readlines = decoding_stream ( stream , encoding ) . readlines try : self . linesets . append ( LineSet ( streamid , readlines ( ) , self . ignore_comments , self . ignore_docstrings , self . ignore_imports , ) ) except UnicodeDecodeError : pass
def _compute_sims ( self ) : no_duplicates = defaultdict ( list ) for num , lineset1 , idx1 , lineset2 , idx2 in self . _iter_sims ( ) : duplicate = no_duplicates [ num ] for couples in duplicate : if ( lineset1 , idx1 ) in couples or ( lineset2 , idx2 ) in couples : couples . add ( ( lineset1 , idx1 ) ) couples . add ( ( lineset2 , idx2 ) ) break else : duplicate . append ( { ( lineset1 , idx1 ) , ( lineset2 , idx2 ) } ) sims = [ ] for num , ensembles in no_duplicates . items ( ) : for couples in ensembles : sims . append ( ( num , couples ) ) sims . sort ( ) sims . reverse ( ) return sims
def _display_sims ( self , sims ) : nb_lignes_dupliquees = 0 for num , couples in sims : print ( ) print ( num , "similar lines in" , len ( couples ) , "files" ) couples = sorted ( couples ) for lineset , idx in couples : print ( "==%s:%s" % ( lineset . name , idx ) ) for line in lineset . _real_lines [ idx : idx + num ] : print ( "  " , line . rstrip ( ) ) nb_lignes_dupliquees += num * ( len ( couples ) - 1 ) nb_total_lignes = sum ( [ len ( lineset ) for lineset in self . linesets ] ) print ( "TOTAL lines=%s duplicates=%s percent=%.2f" % ( nb_total_lignes , nb_lignes_dupliquees , nb_lignes_dupliquees * 100.0 / nb_total_lignes , ) )
def _find_common ( self , lineset1 , lineset2 ) : lines1 = lineset1 . enumerate_stripped lines2 = lineset2 . enumerate_stripped find = lineset2 . find index1 = 0 min_lines = self . min_lines while index1 < len ( lineset1 ) : skip = 1 num = 0 for index2 in find ( lineset1 [ index1 ] ) : non_blank = 0 for num , ( ( _ , line1 ) , ( _ , line2 ) ) in enumerate ( zip ( lines1 ( index1 ) , lines2 ( index2 ) ) ) : if line1 != line2 : if non_blank > min_lines : yield num , lineset1 , index1 , lineset2 , index2 skip = max ( skip , num ) break if line1 : non_blank += 1 else : num += 1 if non_blank > min_lines : yield num , lineset1 , index1 , lineset2 , index2 skip = max ( skip , num ) index1 += skip
def _mk_index ( self ) : index = defaultdict ( list ) for line_no , line in enumerate ( self . _stripped_lines ) : if line : index [ line ] . append ( line_no ) return index
def _definition_equivalent_to_call ( definition , call ) : if definition . kwargs : same_kw_variadics = definition . kwargs in call . starred_kws else : same_kw_variadics = not call . starred_kws if definition . varargs : same_args_variadics = definition . varargs in call . starred_args else : same_args_variadics = not call . starred_args same_kwonlyargs = all ( kw in call . kws for kw in definition . kwonlyargs ) same_args = definition . args == call . args no_additional_kwarg_arguments = True if call . kws : for keyword in call . kws : is_arg = keyword in call . args is_kwonly = keyword in definition . kwonlyargs if not is_arg and not is_kwonly : no_additional_kwarg_arguments = False break return all ( ( same_args , same_kwonlyargs , same_args_variadics , same_kw_variadics , no_additional_kwarg_arguments , ) )
def register ( linter ) : linter . register_checker ( ClassChecker ( linter ) ) linter . register_checker ( SpecialMethodsChecker ( linter ) )
def set_accessed ( self , node ) : frame = node_frame_class ( node ) if frame is None : return self . _scopes [ frame ] [ node . attrname ] . append ( node )
def visit_classdef ( self , node ) : self . _check_bases_classes ( node ) if node . type == "class" and has_known_bases ( node ) : try : node . local_attr ( "__init__" ) except astroid . NotFoundError : self . add_message ( "no-init" , args = node , node = node ) self . _check_slots ( node ) self . _check_proper_bases ( node ) self . _check_consistent_mro ( node )
def _check_consistent_mro ( self , node ) : try : node . mro ( ) except InconsistentMroError : self . add_message ( "inconsistent-mro" , args = node . name , node = node ) except DuplicateBasesError : self . add_message ( "duplicate-bases" , args = node . name , node = node ) except NotImplementedError : pass
def visit_functiondef ( self , node ) : if not node . is_method ( ) : return self . _check_useless_super_delegation ( node ) klass = node . parent . frame ( ) self . _meth_could_be_func = True self . _check_first_arg_for_type ( node , klass . type == "metaclass" ) if node . name == "__init__" : self . _check_init ( node ) return for overridden in klass . local_attr_ancestors ( node . name ) : try : meth_node = overridden [ node . name ] except KeyError : continue if not isinstance ( meth_node , astroid . FunctionDef ) : continue self . _check_signature ( node , meth_node , "overridden" , klass ) break if node . decorators : for decorator in node . decorators . nodes : if isinstance ( decorator , astroid . Attribute ) and decorator . attrname in ( "getter" , "setter" , "deleter" , ) : return if isinstance ( decorator , astroid . Name ) : if decorator . name == "property" : return inferred = safe_infer ( decorator ) if not inferred : return if isinstance ( inferred , astroid . FunctionDef ) : try : inferred = next ( inferred . infer_call_result ( inferred ) ) except astroid . InferenceError : return try : if ( isinstance ( inferred , ( astroid . Instance , astroid . ClassDef ) ) and inferred . getattr ( "__get__" ) and inferred . getattr ( "__set__" ) ) : return except astroid . AttributeInferenceError : pass try : overridden = klass . instance_attr ( node . name ) [ 0 ] overridden_frame = overridden . frame ( ) if ( isinstance ( overridden_frame , astroid . FunctionDef ) and overridden_frame . type == "method" ) : overridden_frame = overridden_frame . parent . frame ( ) if isinstance ( overridden_frame , astroid . ClassDef ) and klass . is_subtype_of ( overridden_frame . qname ( ) ) : args = ( overridden . root ( ) . name , overridden . fromlineno ) self . add_message ( "method-hidden" , args = args , node = node ) except astroid . NotFoundError : pass
def _check_accessed_members ( self , node , accessed ) : excs = ( "AttributeError" , "Exception" , "BaseException" ) for attr , nodes in accessed . items ( ) : try : node . local_attr ( attr ) continue except astroid . NotFoundError : pass try : next ( node . instance_attr_ancestors ( attr ) ) continue except StopIteration : pass try : defstmts = node . instance_attr ( attr ) except astroid . NotFoundError : pass else : defstmts = [ stmt for stmt in defstmts if stmt not in nodes ] if not defstmts : continue scope = defstmts [ 0 ] . scope ( ) defstmts = [ stmt for i , stmt in enumerate ( defstmts ) if i == 0 or stmt . scope ( ) is not scope ] if len ( defstmts ) == 1 : defstmt = defstmts [ 0 ] frame = defstmt . frame ( ) lno = defstmt . fromlineno for _node in nodes : if ( _node . frame ( ) is frame and _node . fromlineno < lno and not astroid . are_exclusive ( _node . statement ( ) , defstmt , excs ) ) : self . add_message ( "access-member-before-definition" , node = _node , args = ( attr , lno ) , )
def _check_signature ( self , method1 , refmethod , class_type , cls ) : if not ( isinstance ( method1 , astroid . FunctionDef ) and isinstance ( refmethod , astroid . FunctionDef ) ) : self . add_message ( "method-check-failed" , args = ( method1 , refmethod ) , node = method1 ) return instance = cls . instantiate_class ( ) method1 = function_to_method ( method1 , instance ) refmethod = function_to_method ( refmethod , instance ) if method1 . args . args is None or refmethod . args . args is None : return if is_attr_private ( method1 . name ) : return if method1 . decorators : for decorator in method1 . decorators . nodes : if ( isinstance ( decorator , astroid . Attribute ) and decorator . attrname == "setter" ) : return if _different_parameters ( refmethod , method1 , dummy_parameter_regex = self . _dummy_rgx ) : self . add_message ( "arguments-differ" , args = ( class_type , method1 . name ) , node = method1 ) elif len ( method1 . args . defaults ) < len ( refmethod . args . defaults ) : self . add_message ( "signature-differs" , args = ( class_type , method1 . name ) , node = method1 )
def _is_raising ( body : typing . List ) -> bool : for node in body : if isinstance ( node , astroid . Raise ) : return True return False
def visit_tryexcept ( self , node ) : self . _check_try_except_raise ( node ) exceptions_classes = [ ] nb_handlers = len ( node . handlers ) for index , handler in enumerate ( node . handlers ) : if handler . type is None : if not _is_raising ( handler . body ) : self . add_message ( "bare-except" , node = handler ) if index < ( nb_handlers - 1 ) : msg = "empty except clause should always appear last" self . add_message ( "bad-except-order" , node = node , args = msg ) elif isinstance ( handler . type , astroid . BoolOp ) : self . add_message ( "binary-op-exception" , node = handler , args = handler . type . op ) else : try : excs = list ( _annotated_unpack_infer ( handler . type ) ) except astroid . InferenceError : continue for part , exc in excs : if exc is astroid . Uninferable : continue if isinstance ( exc , astroid . Instance ) and utils . inherit_from_std_ex ( exc ) : exc = exc . _proxied self . _check_catching_non_exception ( handler , exc , part ) if not isinstance ( exc , astroid . ClassDef ) : continue exc_ancestors = [ anc for anc in exc . ancestors ( ) if isinstance ( anc , astroid . ClassDef ) ] for previous_exc in exceptions_classes : if previous_exc in exc_ancestors : msg = "%s is an ancestor class of %s" % ( previous_exc . name , exc . name , ) self . add_message ( "bad-except-order" , node = handler . type , args = msg ) if ( exc . name in self . config . overgeneral_exceptions and exc . root ( ) . name == utils . EXCEPTIONS_MODULE and not _is_raising ( handler . body ) ) : self . add_message ( "broad-except" , args = exc . name , node = handler . type ) if exc in exceptions_classes : self . add_message ( "duplicate-except" , args = exc . name , node = handler . type ) exceptions_classes += [ exc for _ , exc in excs ]
def visit_functiondef ( self , node ) : if not node . is_method ( ) : return klass = node . parent . frame ( ) for stmt in node . nodes_of_class ( astroid . Call ) : if node_frame_class ( stmt ) != node_frame_class ( node ) : continue expr = stmt . func if not isinstance ( expr , astroid . Attribute ) : continue call = expr . expr if not ( isinstance ( call , astroid . Call ) and isinstance ( call . func , astroid . Name ) and call . func . name == "super" ) : continue if not klass . newstyle and has_known_bases ( klass ) : continue else : if not call . args : if sys . version_info [ 0 ] == 3 : continue else : self . add_message ( "missing-super-argument" , node = call ) continue arg0 = call . args [ 0 ] if ( isinstance ( arg0 , astroid . Call ) and isinstance ( arg0 . func , astroid . Name ) and arg0 . func . name == "type" ) : self . add_message ( "bad-super-call" , node = call , args = ( "type" , ) ) continue if ( len ( call . args ) >= 2 and isinstance ( call . args [ 1 ] , astroid . Name ) and call . args [ 1 ] . name == "self" and isinstance ( arg0 , astroid . Attribute ) and arg0 . attrname == "__class__" ) : self . add_message ( "bad-super-call" , node = call , args = ( "self.__class__" , ) ) continue try : supcls = call . args and next ( call . args [ 0 ] . infer ( ) , None ) except astroid . InferenceError : continue if klass is not supcls : name = None if supcls : name = supcls . name elif call . args and hasattr ( call . args [ 0 ] , "name" ) : name = call . args [ 0 ] . name if name : self . add_message ( "bad-super-call" , node = call , args = ( name , ) )
def display_reports ( self , layout ) : self . section = 0 if hasattr ( layout , "report_id" ) : layout . children [ 0 ] . children [ 0 ] . data += " (%s)" % layout . report_id self . _display ( layout )
def _is_typing_namedtuple ( node : astroid . ClassDef ) -> bool : for base in node . ancestors ( ) : if base . qname ( ) == TYPING_NAMEDTUPLE : return True return False
def visit_classdef ( self , node ) : nb_parents = len ( list ( node . ancestors ( ) ) ) if nb_parents > self . config . max_parents : self . add_message ( "too-many-ancestors" , node = node , args = ( nb_parents , self . config . max_parents ) , ) if len ( node . instance_attrs ) > self . config . max_attributes : self . add_message ( "too-many-instance-attributes" , node = node , args = ( len ( node . instance_attrs ) , self . config . max_attributes ) , )
def leave_classdef ( self , node ) : my_methods = sum ( 1 for method in node . mymethods ( ) if not method . name . startswith ( "_" ) ) if my_methods > self . config . max_public_methods : self . add_message ( "too-many-public-methods" , node = node , args = ( my_methods , self . config . max_public_methods ) , ) if ( node . type != "class" or _is_enum_class ( node ) or _is_dataclass ( node ) or _is_typing_namedtuple ( node ) ) : return all_methods = _count_methods_in_class ( node ) if all_methods < self . config . min_public_methods : self . add_message ( "too-few-public-methods" , node = node , args = ( all_methods , self . config . min_public_methods ) , )
def visit_tryexcept ( self , node ) : branches = len ( node . handlers ) if node . orelse : branches += 1 self . _inc_branch ( node , branches ) self . _inc_all_stmts ( branches )
def visit_if ( self , node ) : self . _check_boolean_expressions ( node ) branches = 1 if node . orelse and ( len ( node . orelse ) > 1 or not isinstance ( node . orelse [ 0 ] , If ) ) : branches += 1 self . _inc_branch ( node , branches ) self . _inc_all_stmts ( branches )
def visit_while ( self , node ) : branches = 1 if node . orelse : branches += 1 self . _inc_branch ( node , branches )
def _check_docstring ( self , node ) : docstring = node . doc if not docstring : return start_line = node . lineno + 1 for idx , line in enumerate ( docstring . splitlines ( ) ) : self . _check_spelling ( "wrong-spelling-in-docstring" , line , start_line + idx )
def register ( linter ) : linter . register_checker ( RefactoringChecker ( linter ) ) linter . register_checker ( NotChecker ( linter ) ) linter . register_checker ( RecommandationChecker ( linter ) ) linter . register_checker ( LenChecker ( linter ) )
def _check_stop_iteration_inside_generator ( self , node ) : frame = node . frame ( ) if not isinstance ( frame , astroid . FunctionDef ) or not frame . is_generator ( ) : return if utils . node_ignores_exception ( node , StopIteration ) : return if not node . exc : return exc = utils . safe_infer ( node . exc ) if exc is None or exc is astroid . Uninferable : return if self . _check_exception_inherit_from_stopiteration ( exc ) : self . add_message ( "stop-iteration-return" , node = node )
def _check_exception_inherit_from_stopiteration ( exc ) : stopiteration_qname = "{}.StopIteration" . format ( utils . EXCEPTIONS_MODULE ) return any ( _class . qname ( ) == stopiteration_qname for _class in exc . mro ( ) )
def _check_nested_blocks ( self , node ) : if not isinstance ( node . scope ( ) , astroid . FunctionDef ) : return nested_blocks = self . _nested_blocks [ : ] if node . parent == node . scope ( ) : self . _nested_blocks = [ node ] else : for ancestor_node in reversed ( self . _nested_blocks ) : if ancestor_node == node . parent : break self . _nested_blocks . pop ( ) if isinstance ( node , astroid . If ) and self . _is_actual_elif ( node ) : if self . _nested_blocks : self . _nested_blocks . pop ( ) self . _nested_blocks . append ( node ) if len ( nested_blocks ) > len ( self . _nested_blocks ) : self . _emit_nested_blocks_message_if_needed ( nested_blocks )
def _check_consider_merging_isinstance ( self , node ) : if node . op != "or" : return first_args = self . _duplicated_isinstance_types ( node ) for duplicated_name , class_names in first_args . items ( ) : names = sorted ( name for name in class_names ) self . add_message ( "consider-merging-isinstance" , node = node , args = ( duplicated_name , ", " . join ( names ) ) , )
def visit_for ( self , node ) : if not isinstance ( node . iter , astroid . Call ) : return if not self . _is_builtin ( node . iter . func , "range" ) : return if len ( node . iter . args ) == 2 and not _is_constant_zero ( node . iter . args [ 0 ] ) : return if len ( node . iter . args ) > 2 : return if not isinstance ( node . iter . args [ - 1 ] , astroid . Call ) : return second_func = node . iter . args [ - 1 ] . func if not self . _is_builtin ( second_func , "len" ) : return len_args = node . iter . args [ - 1 ] . args if not len_args or len ( len_args ) != 1 : return iterating_object = len_args [ 0 ] if not isinstance ( iterating_object , astroid . Name ) : return scope = node . scope ( ) if iterating_object . name == "self" and scope . name == "__iter__" : return for child in node . body : for subscript in child . nodes_of_class ( astroid . Subscript ) : if not isinstance ( subscript . value , astroid . Name ) : continue if not isinstance ( subscript . slice , astroid . Index ) : continue if not isinstance ( subscript . slice . value , astroid . Name ) : continue if subscript . slice . value . name != node . target . name : continue if iterating_object . name != subscript . value . name : continue if subscript . value . scope ( ) != node . scope ( ) : continue self . add_message ( "consider-using-enumerate" , node = node ) return
def _check_graphviz_available ( output_format ) : try : subprocess . call ( [ "dot" , "-V" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) except OSError : print ( "The output format '%s' is currently not available.\n" "Please install 'Graphviz' to have other output formats " "than 'dot' or 'vcg'." % output_format ) sys . exit ( 32 )
def run ( self , args ) : if not args : print ( self . help ( ) ) return 1 sys . path . insert ( 0 , os . getcwd ( ) ) try : project = project_from_files ( args , project_name = self . config . project , black_list = self . config . black_list , ) linker = Linker ( project , tag = True ) handler = DiadefsHandler ( self . config ) diadefs = handler . get_diadefs ( project , linker ) finally : sys . path . pop ( 0 ) if self . config . output_format == "vcg" : writer . VCGWriter ( self . config ) . write ( diadefs ) else : writer . DotWriter ( self . config ) . write ( diadefs ) return 0
def visit_tryexcept ( self , node ) : for handler in node . handlers : if handler . type is None : continue if isinstance ( handler . type , astroid . BoolOp ) : continue try : excs = list ( _annotated_unpack_infer ( handler . type ) ) except astroid . InferenceError : continue handled_in_clause = [ ] for part , exc in excs : if exc is astroid . Uninferable : continue if isinstance ( exc , astroid . Instance ) and utils . inherit_from_std_ex ( exc ) : exc = exc . _proxied if not isinstance ( exc , astroid . ClassDef ) : continue exc_ancestors = [ anc for anc in exc . ancestors ( ) if isinstance ( anc , astroid . ClassDef ) ] for prev_part , prev_exc in handled_in_clause : prev_exc_ancestors = [ anc for anc in prev_exc . ancestors ( ) if isinstance ( anc , astroid . ClassDef ) ] if exc == prev_exc : self . add_message ( "overlapping-except" , node = handler . type , args = "%s and %s are the same" % ( prev_part . as_string ( ) , part . as_string ( ) ) , ) elif prev_exc in exc_ancestors or exc in prev_exc_ancestors : ancestor = part if exc in prev_exc_ancestors else prev_part descendant = part if prev_exc in exc_ancestors else prev_part self . add_message ( "overlapping-except" , node = handler . type , args = "%s is an ancestor class of %s" % ( ancestor . as_string ( ) , descendant . as_string ( ) ) , ) handled_in_clause += [ ( part , exc ) ]
def write_packages ( self , diagram ) : for i , obj in enumerate ( sorted ( diagram . modules ( ) , key = lambda x : x . title ) ) : self . printer . emit_node ( i , label = self . get_title ( obj ) , shape = "box" ) obj . fig_id = i for rel in diagram . get_relationships ( "depends" ) : self . printer . emit_edge ( rel . from_object . fig_id , rel . to_object . fig_id , * * self . pkg_edges )
def write_classes ( self , diagram ) : for i , obj in enumerate ( sorted ( diagram . objects , key = lambda x : x . title ) ) : self . printer . emit_node ( i , * * self . get_values ( obj ) ) obj . fig_id = i for rel in diagram . get_relationships ( "specialization" ) : self . printer . emit_edge ( rel . from_object . fig_id , rel . to_object . fig_id , * * self . inh_edges ) for rel in diagram . get_relationships ( "implements" ) : self . printer . emit_edge ( rel . from_object . fig_id , rel . to_object . fig_id , * * self . imp_edges ) for rel in diagram . get_relationships ( "association" ) : self . printer . emit_edge ( rel . from_object . fig_id , rel . to_object . fig_id , label = rel . name , * * self . association_edges )
def set_printer ( self , file_name , basename ) : layout = dict ( rankdir = "BT" ) self . printer = DotBackend ( basename , additional_param = layout ) self . file_name = file_name
def set_printer ( self , file_name , basename ) : self . graph_file = open ( file_name , "w+" ) self . printer = VCGPrinter ( self . graph_file ) self . printer . open_graph ( title = basename , layoutalgorithm = "dfs" , late_edge_labels = "yes" , port_sharing = "no" , manhattan_edges = "yes" , ) self . printer . emit_node = self . printer . node self . printer . emit_edge = self . printer . edge
def may_be_emitted ( self ) : if self . minversion is not None and self . minversion > sys . version_info : return False if self . maxversion is not None and self . maxversion <= sys . version_info : return False return True
def format_help ( self , checkerref = False ) : desc = self . descr if checkerref : desc += " This message belongs to the %s checker." % self . checker . name title = self . msg if self . symbol : msgid = "%s (%s)" % ( self . symbol , self . msgid ) else : msgid = self . msgid if self . minversion or self . maxversion : restr = [ ] if self . minversion : restr . append ( "< %s" % "." . join ( [ str ( n ) for n in self . minversion ] ) ) if self . maxversion : restr . append ( ">= %s" % "." . join ( [ str ( n ) for n in self . maxversion ] ) ) restr = " or " . join ( restr ) if checkerref : desc += " It can't be emitted when using Python %s." % restr else : desc += " This message can't be emitted when using Python %s." % restr desc = normalize_text ( " " . join ( desc . split ( ) ) , indent = "  " ) if title != "%s" : title = title . splitlines ( ) [ 0 ] return ":%s: *%s*\n%s" % ( msgid , title . rstrip ( " " ) , desc ) return ":%s:\n%s" % ( msgid , desc )
def _get_cycles ( graph_dict , path , visited , result , vertice ) : if vertice in path : cycle = [ vertice ] for node in path [ : : - 1 ] : if node == vertice : break cycle . insert ( 0 , node ) start_from = min ( cycle ) index = cycle . index ( start_from ) cycle = cycle [ index : ] + cycle [ 0 : index ] if cycle not in result : result . append ( cycle ) return path . append ( vertice ) try : for node in graph_dict [ vertice ] : if node not in visited : _get_cycles ( graph_dict , path , visited , result , node ) visited . add ( node ) except KeyError : pass path . pop ( )
def get_source ( self ) : if self . _source is None : self . emit ( "}\n" ) self . _source = "\n" . join ( self . lines ) del self . lines return self . _source
def _rest_format_section ( stream , section , options , doc = None ) : if section : print ( "%s\n%s" % ( section , "'" * len ( section ) ) , file = stream ) if doc : print ( normalize_text ( doc , line_len = 79 , indent = "" ) , file = stream ) print ( file = stream ) for optname , optdict , value in options : help_opt = optdict . get ( "help" ) print ( ":%s:" % optname , file = stream ) if help_opt : help_opt = normalize_text ( help_opt , line_len = 79 , indent = "  " ) print ( help_opt , file = stream ) if value : value = str ( _format_option_value ( optdict , value ) ) print ( file = stream ) print ( "  Default: ``%s``" % value . replace ( "`` " , "```` ``" ) , file = stream )
def disable ( self , msgid , scope = "package" , line = None , ignore_unknown = False ) : self . _set_msg_status ( msgid , enable = False , scope = scope , line = line , ignore_unknown = ignore_unknown ) self . _register_by_id_managed_msg ( msgid , line )
def enable ( self , msgid , scope = "package" , line = None , ignore_unknown = False ) : self . _set_msg_status ( msgid , enable = True , scope = scope , line = line , ignore_unknown = ignore_unknown ) self . _register_by_id_managed_msg ( msgid , line , is_disabled = False )
def print_full_documentation ( self , stream = None ) : if not stream : stream = sys . stdout print ( "Pylint global options and switches" , file = stream ) print ( "----------------------------------" , file = stream ) print ( "" , file = stream ) print ( "Pylint provides global options and switches." , file = stream ) print ( "" , file = stream ) by_checker = { } for checker in self . get_checkers ( ) : if checker . name == "master" : if checker . options : for section , options in checker . options_by_section ( ) : if section is None : title = "General options" else : title = "%s options" % section . capitalize ( ) print ( title , file = stream ) print ( "~" * len ( title ) , file = stream ) _rest_format_section ( stream , None , options ) print ( "" , file = stream ) else : name = checker . name try : by_checker [ name ] [ "options" ] += checker . options_and_values ( ) by_checker [ name ] [ "msgs" ] . update ( checker . msgs ) by_checker [ name ] [ "reports" ] += checker . reports except KeyError : by_checker [ name ] = { "options" : list ( checker . options_and_values ( ) ) , "msgs" : dict ( checker . msgs ) , "reports" : list ( checker . reports ) , } print ( "Pylint checkers' options and switches" , file = stream ) print ( "-------------------------------------" , file = stream ) print ( "" , file = stream ) print ( "Pylint checkers can provide three set of features:" , file = stream ) print ( "" , file = stream ) print ( "* options that control their execution," , file = stream ) print ( "* messages that they can raise," , file = stream ) print ( "* reports that they can generate." , file = stream ) print ( "" , file = stream ) print ( "Below is a list of all checkers and their features." , file = stream ) print ( "" , file = stream ) for checker , info in sorted ( by_checker . items ( ) ) : self . _print_checker_doc ( checker , info , stream = stream )
def _get_indent_length ( line ) : result = 0 for char in line : if char == " " : result += 1 elif char == "\t" : result += _TAB_LENGTH else : break return result
def _get_indent_hint_line ( bar_positions , bad_position ) : if not bar_positions : return ( "" , "" ) bar_positions = [ _get_indent_length ( indent ) for indent in bar_positions ] bad_position = _get_indent_length ( bad_position ) delta_message = "" markers = [ ( pos , "|" ) for pos in bar_positions ] if len ( markers ) == 1 : expected_position = markers [ 0 ] [ 0 ] delta = abs ( expected_position - bad_position ) direction = "add" if expected_position > bad_position else "remove" delta_message = _CONTINUATION_HINT_MESSAGE % ( direction , delta , "s" if delta > 1 else "" , ) markers . append ( ( bad_position , "^" ) ) markers . sort ( ) line = [ " " ] * ( markers [ - 1 ] [ 0 ] + 1 ) for position , marker in markers : line [ position ] = marker return ( "" . join ( line ) , delta_message )
def handle_line_start ( self , pos ) : if self . _line_start > - 1 : return check_token_position = pos if self . _tokens . token ( pos ) == _ASYNC_TOKEN : check_token_position += 1 self . _is_block_opener = ( self . _tokens . token ( check_token_position ) in _CONTINUATION_BLOCK_OPENERS ) self . _line_start = pos
def get_valid_indentations ( self , idx ) : stack_top = - 1 if ( self . _tokens . token ( idx ) in ( "}" , "for" ) and self . _cont_stack [ - 1 ] . token == ":" ) : stack_top = - 2 indent = self . _cont_stack [ stack_top ] if self . _tokens . token ( idx ) in _CLOSING_BRACKETS : valid_indentations = indent . valid_outdent_strings else : valid_indentations = indent . valid_continuation_strings return indent , valid_indentations . copy ( )
def _continuation_inside_bracket ( self , bracket , position ) : indentation = self . _tokens . line_indent ( position ) token_indent = self . _tokens . token_indent ( position ) next_token_indent = self . _tokens . token_indent ( position + 1 ) if ( self . _is_block_opener and next_token_indent == indentation + self . _block_indent_string ) : return _ContinuedIndent ( CONTINUED_BLOCK , bracket , position , _Indentations ( token_indent ) , _BeforeBlockIndentations ( next_token_indent , next_token_indent + self . _continuation_string ) , ) return _ContinuedIndent ( CONTINUED , bracket , position , _Indentations ( token_indent , next_token_indent ) , _Indentations ( next_token_indent ) , )
def new_line ( self , tokens , line_end , line_start ) : if _last_token_on_line_is ( tokens , line_end , ";" ) : self . add_message ( "unnecessary-semicolon" , line = tokens . start_line ( line_end ) ) line_num = tokens . start_line ( line_start ) line = tokens . line ( line_start ) if tokens . type ( line_start ) not in _JUNK_TOKENS : self . _lines [ line_num ] = line . split ( "\n" ) [ 0 ] self . check_lines ( line , line_num )
def _has_valid_type_annotation ( self , tokens , i ) : if not self . _inside_brackets ( "(" ) : return False bracket_level = 0 for token in tokens [ i - 1 : : - 1 ] : if token [ 1 ] == ":" : return True if token [ 1 ] == "(" : return False if token [ 1 ] == "]" : bracket_level += 1 elif token [ 1 ] == "[" : bracket_level -= 1 elif token [ 1 ] == "," : if not bracket_level : return False elif token [ 1 ] in ( "." , "..." ) : continue elif token [ 0 ] not in ( tokenize . NAME , tokenize . STRING , tokenize . NL ) : return False return False
def _check_equals_spacing ( self , tokens , i ) : if self . _has_valid_type_annotation ( tokens , i ) : self . _check_space ( tokens , i , ( _MUST , _MUST ) ) elif self . _inside_brackets ( "(" ) or self . _inside_brackets ( "lambda" ) : self . _check_space ( tokens , i , ( _MUST_NOT , _MUST_NOT ) ) else : self . _check_space ( tokens , i , ( _MUST , _MUST ) )
def _check_surrounded_by_space ( self , tokens , i ) : self . _check_space ( tokens , i , ( _MUST , _MUST ) )
def visit_default ( self , node ) : if not node . is_statement : return if not node . root ( ) . pure_python : return prev_sibl = node . previous_sibling ( ) if prev_sibl is not None : prev_line = prev_sibl . fromlineno else : if ( isinstance ( node . parent , nodes . TryFinally ) and node in node . parent . finalbody ) : prev_line = node . parent . body [ 0 ] . tolineno + 1 else : prev_line = node . parent . statement ( ) . fromlineno line = node . fromlineno assert line , node if prev_line == line and self . _visited_lines . get ( line ) != 2 : self . _check_multi_statement_line ( node , line ) return if line in self . _visited_lines : return try : tolineno = node . blockstart_tolineno except AttributeError : tolineno = node . tolineno assert tolineno , node lines = [ ] for line in range ( line , tolineno + 1 ) : self . _visited_lines [ line ] = 1 try : lines . append ( self . _lines [ line ] . rstrip ( ) ) except KeyError : lines . append ( "" )
def _check_multi_statement_line ( self , node , line ) : if isinstance ( node , nodes . With ) : return if isinstance ( node , nodes . TryExcept ) and isinstance ( node . parent , nodes . TryFinally ) : return if ( isinstance ( node . parent , nodes . If ) and not node . parent . orelse and self . config . single_line_if_stmt ) : return if ( isinstance ( node . parent , nodes . ClassDef ) and len ( node . parent . body ) == 1 and self . config . single_line_class_stmt ) : return self . add_message ( "multiple-statements" , node = node ) self . _visited_lines [ line ] = 2
def check_lines ( self , lines , i ) : max_chars = self . config . max_line_length ignore_long_line = self . config . ignore_long_lines def check_line ( line , i ) : if not line . endswith ( "\n" ) : self . add_message ( "missing-final-newline" , line = i ) else : stripped_line = line . rstrip ( "\t\n\r\v " ) if not stripped_line and _EMPTY_LINE in self . config . no_space_check : pass elif line [ len ( stripped_line ) : ] not in ( "\n" , "\r\n" ) : self . add_message ( "trailing-whitespace" , line = i , col_offset = len ( stripped_line ) ) line = stripped_line mobj = OPTION_RGX . search ( line ) if mobj and "=" in line : front_of_equal , _ , back_of_equal = mobj . group ( 1 ) . partition ( "=" ) if front_of_equal . strip ( ) == "disable" : if "line-too-long" in { _msg_id . strip ( ) for _msg_id in back_of_equal . split ( "," ) } : return None line = line . rsplit ( "#" , 1 ) [ 0 ] . rstrip ( ) if len ( line ) > max_chars and not ignore_long_line . search ( line ) : self . add_message ( "line-too-long" , line = i , args = ( len ( line ) , max_chars ) ) return i + 1 unsplit_ends = { "\v" , "\x0b" , "\f" , "\x0c" , "\x1c" , "\x1d" , "\x1e" , "\x85" , "\u2028" , "\u2029" , } unsplit = [ ] for line in lines . splitlines ( True ) : if line [ - 1 ] in unsplit_ends : unsplit . append ( line ) continue if unsplit : unsplit . append ( line ) line = "" . join ( unsplit ) unsplit = [ ] i = check_line ( line , i ) if i is None : break if unsplit : check_line ( "" . join ( unsplit ) , i )
def check_indent_level ( self , string , expected , line_num ) : indent = self . config . indent_string if indent == "\\t" : indent = "\t" level = 0 unit_size = len ( indent ) while string [ : unit_size ] == indent : string = string [ unit_size : ] level += 1 suppl = "" while string and string [ 0 ] in " \t" : if string [ 0 ] != indent [ 0 ] : if string [ 0 ] == "\t" : args = ( "tab" , "space" ) else : args = ( "space" , "tab" ) self . add_message ( "mixed-indentation" , args = args , line = line_num ) return level suppl += string [ 0 ] string = string [ 1 : ] if level != expected or suppl : i_type = "spaces" if indent [ 0 ] == "\t" : i_type = "tabs" self . add_message ( "bad-indentation" , line = line_num , args = ( level * unit_size + len ( suppl ) , i_type , expected * unit_size ) , ) return None
def _is_conditional_import ( node ) : parent = node . parent return isinstance ( parent , ( astroid . TryExcept , astroid . ExceptHandler , astroid . If , astroid . IfExp ) )
def visit_name ( self , node ) : found_node , _ = node . lookup ( node . name ) if not _is_builtin ( found_node ) : return if node . name not in self . _bad_builtins : return if node_ignores_exception ( node ) or isinstance ( find_try_except_wrapper_node ( node ) , astroid . ExceptHandler ) : return message = node . name . lower ( ) + "-builtin" self . add_message ( message , node = node )
def visit_subscript ( self , node ) : try : for inferred in node . value . infer ( ) : if not isinstance ( inferred , astroid . Instance ) : continue if utils . inherit_from_std_ex ( inferred ) : self . add_message ( "indexing-exception" , node = node ) except astroid . InferenceError : return
def visit_attribute ( self , node ) : if node . attrname == "xreadlines" : self . add_message ( "xreadlines-attribute" , node = node ) return exception_message = "message" try : for inferred in node . expr . infer ( ) : if isinstance ( inferred , astroid . Instance ) and utils . inherit_from_std_ex ( inferred ) : if node . attrname == exception_message : if exception_message in inferred . instance_attrs : continue self . add_message ( "exception-message-attribute" , node = node ) if isinstance ( inferred , astroid . Module ) : self . _warn_if_deprecated ( node , inferred . name , { node . attrname } , report_on_modules = False ) except astroid . InferenceError : return
def visit_excepthandler ( self , node ) : def _is_used_in_except_block ( node ) : scope = node . scope ( ) current = node while ( current and current != scope and not isinstance ( current , astroid . ExceptHandler ) ) : current = current . parent return isinstance ( current , astroid . ExceptHandler ) and current . type != node if isinstance ( node . name , ( astroid . Tuple , astroid . List ) ) : self . add_message ( "unpacking-in-except" , node = node ) return if not node . name : return scope = node . parent . scope ( ) scope_names = scope . nodes_of_class ( astroid . Name , skip_klass = astroid . FunctionDef ) scope_names = list ( scope_names ) potential_leaked_names = [ scope_name for scope_name in scope_names if scope_name . name == node . name . name and scope_name . lineno > node . lineno and not _is_used_in_except_block ( scope_name ) ] reassignments_for_same_name = { assign_name . lineno for assign_name in scope . nodes_of_class ( astroid . AssignName , skip_klass = astroid . FunctionDef ) if assign_name . name == node . name . name } for leaked_name in potential_leaked_names : if any ( node . lineno < elem < leaked_name . lineno for elem in reassignments_for_same_name ) : continue self . add_message ( "exception-escape" , node = leaked_name )
def find_pylintrc ( ) : if os . path . exists ( "pylintrc" ) : return os . path . abspath ( "pylintrc" ) if os . path . exists ( ".pylintrc" ) : return os . path . abspath ( ".pylintrc" ) if os . path . isfile ( "__init__.py" ) : curdir = os . path . abspath ( os . getcwd ( ) ) while os . path . isfile ( os . path . join ( curdir , "__init__.py" ) ) : curdir = os . path . abspath ( os . path . join ( curdir , ".." ) ) if os . path . isfile ( os . path . join ( curdir , "pylintrc" ) ) : return os . path . join ( curdir , "pylintrc" ) if os . path . isfile ( os . path . join ( curdir , ".pylintrc" ) ) : return os . path . join ( curdir , ".pylintrc" ) if "PYLINTRC" in os . environ and os . path . exists ( os . environ [ "PYLINTRC" ] ) : pylintrc = os . environ [ "PYLINTRC" ] else : user_home = os . path . expanduser ( "~" ) if user_home in ( "~" , "/root" ) : pylintrc = ".pylintrc" else : pylintrc = os . path . join ( user_home , ".pylintrc" ) if not os . path . isfile ( pylintrc ) : pylintrc = os . path . join ( user_home , ".config" , "pylintrc" ) if not os . path . isfile ( pylintrc ) : if os . path . isfile ( "/etc/pylintrc" ) : pylintrc = "/etc/pylintrc" else : pylintrc = None return pylintrc
def register_options_provider ( self , provider , own_group = True ) : assert provider . priority <= 0 , "provider's priority can't be >= 0" for i in range ( len ( self . options_providers ) ) : if provider . priority > self . options_providers [ i ] . priority : self . options_providers . insert ( i , provider ) break else : self . options_providers . append ( provider ) non_group_spec_options = [ option for option in provider . options if "group" not in option [ 1 ] ] groups = getattr ( provider , "option_groups" , ( ) ) if own_group and non_group_spec_options : self . add_option_group ( provider . name . upper ( ) , provider . __doc__ , non_group_spec_options , provider , ) else : for opt , optdict in non_group_spec_options : self . add_optik_option ( provider , self . cmdline_parser , opt , optdict ) for gname , gdoc in groups : gname = gname . upper ( ) goptions = [ option for option in provider . options if option [ 1 ] . get ( "group" , "" ) . upper ( ) == gname ] self . add_option_group ( gname , gdoc , goptions , provider )
def cb_set_provider_option ( self , option , opt , value , parser ) : if opt . startswith ( "--" ) : opt = opt [ 2 : ] else : opt = self . _short_options [ opt [ 1 : ] ] if value is None : value = 1 self . global_set_option ( opt , value )
def global_set_option ( self , opt , value ) : self . _all_options [ opt ] . set_option ( opt , value )
def add_help_section ( self , title , description , level = 0 ) : group = optparse . OptionGroup ( self . cmdline_parser , title = title . capitalize ( ) , description = description ) group . level = level self . _maxlevel = max ( self . _maxlevel , level ) self . cmdline_parser . add_option_group ( group )
def help ( self , level = 0 ) : self . cmdline_parser . formatter . output_level = level with _patch_optparse ( ) : return self . cmdline_parser . format_help ( )
def load_defaults ( self ) : for opt , optdict in self . options : action = optdict . get ( "action" ) if action != "callback" : if optdict is None : optdict = self . get_option_def ( opt ) default = optdict . get ( "default" ) self . set_option ( opt , default , action , optdict )
def option_attrname ( self , opt , optdict = None ) : if optdict is None : optdict = self . get_option_def ( opt ) return optdict . get ( "dest" , opt . replace ( "-" , "_" ) )
def get_option_def ( self , opt ) : assert self . options for option in self . options : if option [ 0 ] == opt : return option [ 1 ] raise optparse . OptionError ( "no such option %s in section %r" % ( opt , self . name ) , opt )
def visit_module ( self , node ) : self . _logging_names = set ( ) logging_mods = self . config . logging_modules self . _format_style = self . config . logging_format_style self . _logging_modules = set ( logging_mods ) self . _from_imports = { } for logging_mod in logging_mods : parts = logging_mod . rsplit ( "." , 1 ) if len ( parts ) > 1 : self . _from_imports [ parts [ 0 ] ] = parts [ 1 ]
def visit_importfrom ( self , node ) : try : logging_name = self . _from_imports [ node . modname ] for module , as_name in node . names : if module == logging_name : self . _logging_names . add ( as_name or module ) except KeyError : pass
def visit_import ( self , node ) : for module , as_name in node . names : if module in self . _logging_modules : self . _logging_names . add ( as_name or module )
def visit_call ( self , node ) : def is_logging_name ( ) : return ( isinstance ( node . func , astroid . Attribute ) and isinstance ( node . func . expr , astroid . Name ) and node . func . expr . name in self . _logging_names ) def is_logger_class ( ) : try : for inferred in node . func . infer ( ) : if isinstance ( inferred , astroid . BoundMethod ) : parent = inferred . _proxied . parent if isinstance ( parent , astroid . ClassDef ) and ( parent . qname ( ) == "logging.Logger" or any ( ancestor . qname ( ) == "logging.Logger" for ancestor in parent . ancestors ( ) ) ) : return True , inferred . _proxied . name except astroid . exceptions . InferenceError : pass return False , None if is_logging_name ( ) : name = node . func . attrname else : result , name = is_logger_class ( ) if not result : return self . _check_log_method ( node , name )
def in_loop ( node ) : parent = node . parent while parent is not None : if isinstance ( parent , ( astroid . For , astroid . ListComp , astroid . SetComp , astroid . DictComp , astroid . GeneratorExp , ) , ) : return True parent = parent . parent return False
def register ( linter ) : linter . register_checker ( BasicErrorChecker ( linter ) ) linter . register_checker ( BasicChecker ( linter ) ) linter . register_checker ( NameChecker ( linter ) ) linter . register_checker ( DocStringChecker ( linter ) ) linter . register_checker ( PassChecker ( linter ) ) linter . register_checker ( ComparisonChecker ( linter ) )
def visit_starred ( self , node ) : if isinstance ( node . parent , astroid . Call ) : return if PY35 and isinstance ( node . parent , ( astroid . List , astroid . Tuple , astroid . Set , astroid . Dict ) ) : return stmt = node . statement ( ) if not isinstance ( stmt , astroid . Assign ) : return if stmt . value is node or stmt . value . parent_of ( node ) : self . add_message ( "star-needs-assignment-target" , node = node )
def _check_nonlocal_and_global ( self , node ) : def same_scope ( current ) : return current . scope ( ) is node from_iter = itertools . chain . from_iterable nonlocals = set ( from_iter ( child . names for child in node . nodes_of_class ( astroid . Nonlocal ) if same_scope ( child ) ) ) if not nonlocals : return global_vars = set ( from_iter ( child . names for child in node . nodes_of_class ( astroid . Global ) if same_scope ( child ) ) ) for name in nonlocals . intersection ( global_vars ) : self . add_message ( "nonlocal-and-global" , args = ( name , ) , node = node )
def visit_unaryop ( self , node ) : if ( ( node . op in "+-" ) and isinstance ( node . operand , astroid . UnaryOp ) and ( node . operand . op == node . op ) ) : self . add_message ( "nonexistent-operator" , node = node , args = node . op * 2 )
def _check_else_on_loop ( self , node ) : if node . orelse and not _loop_exits_early ( node ) : self . add_message ( "useless-else-on-loop" , node = node , line = node . orelse [ 0 ] . lineno - 1 , )
def _check_in_loop ( self , node , node_name ) : _node = node . parent while _node : if isinstance ( _node , ( astroid . For , astroid . While ) ) : if node not in _node . orelse : return if isinstance ( _node , ( astroid . ClassDef , astroid . FunctionDef ) ) : break if ( isinstance ( _node , astroid . TryFinally ) and node in _node . finalbody and isinstance ( node , astroid . Continue ) ) : self . add_message ( "continue-in-finally" , node = node ) _node = _node . parent self . add_message ( "not-in-loop" , node = node , args = node_name )
def open ( self ) : self . _tryfinallys = [ ] self . stats = self . linter . add_stats ( module = 0 , function = 0 , method = 0 , class_ = 0 )
def visit_expr ( self , node ) : expr = node . value if isinstance ( expr , astroid . Const ) and isinstance ( expr . value , str ) : scope = expr . scope ( ) if isinstance ( scope , ( astroid . ClassDef , astroid . Module , astroid . FunctionDef ) ) : if isinstance ( scope , astroid . FunctionDef ) and scope . name != "__init__" : pass else : sibling = expr . previous_sibling ( ) if ( sibling is not None and sibling . scope ( ) is scope and isinstance ( sibling , ( astroid . Assign , astroid . AnnAssign ) ) ) : return self . add_message ( "pointless-string-statement" , node = node ) return if isinstance ( expr , ( astroid . Yield , astroid . Await , astroid . Ellipsis , astroid . Call ) ) or ( isinstance ( node . parent , astroid . TryExcept ) and node . parent . body == [ node ] ) : return if any ( expr . nodes_of_class ( astroid . Call ) ) : self . add_message ( "expression-not-assigned" , node = node , args = expr . as_string ( ) ) else : self . add_message ( "pointless-statement" , node = node )
def visit_lambda ( self , node ) : if node . args . defaults : return call = node . body if not isinstance ( call , astroid . Call ) : return if isinstance ( node . body . func , astroid . Attribute ) and isinstance ( node . body . func . expr , astroid . Call ) : return call_site = CallSite . from_call ( call ) ordinary_args = list ( node . args . args ) new_call_args = list ( self . _filter_vararg ( node , call . args ) ) if node . args . kwarg : if self . _has_variadic_argument ( call . kwargs , node . args . kwarg ) : return if node . args . vararg : if self . _has_variadic_argument ( call . starargs , node . args . vararg ) : return elif call . starargs : return if call . keywords : lambda_kwargs = { keyword . name for keyword in node . args . defaults } if len ( lambda_kwargs ) != len ( call_site . keyword_arguments ) : return if set ( call_site . keyword_arguments ) . difference ( lambda_kwargs ) : return if len ( ordinary_args ) != len ( new_call_args ) : return for arg , passed_arg in zip ( ordinary_args , new_call_args ) : if not isinstance ( passed_arg , astroid . Name ) : return if arg . name != passed_arg . name : return self . add_message ( "unnecessary-lambda" , line = node . fromlineno , node = node )
def visit_assert ( self , node ) : if ( node . fail is None and isinstance ( node . test , astroid . Tuple ) and len ( node . test . elts ) == 2 ) : self . add_message ( "assert-on-tuple" , node = node )
def visit_dict ( self , node ) : keys = set ( ) for k , _ in node . items : if isinstance ( k , astroid . Const ) : key = k . value if key in keys : self . add_message ( "duplicate-key" , node = node , args = key ) keys . add ( key )
def _check_reversed ( self , node ) : try : argument = utils . safe_infer ( utils . get_argument_from_call ( node , position = 0 ) ) except utils . NoSuchArgumentError : pass else : if argument is astroid . Uninferable : return if argument is None : if isinstance ( node . args [ 0 ] , astroid . Call ) : try : func = next ( node . args [ 0 ] . func . infer ( ) ) except astroid . InferenceError : return if getattr ( func , "name" , None ) == "iter" and utils . is_builtin_object ( func ) : self . add_message ( "bad-reversed-sequence" , node = node ) return if isinstance ( argument , ( astroid . List , astroid . Tuple ) ) : return if isinstance ( argument , astroid . Instance ) : if argument . _proxied . name == "dict" and utils . is_builtin_object ( argument . _proxied ) : self . add_message ( "bad-reversed-sequence" , node = node ) return if any ( ancestor . name == "dict" and utils . is_builtin_object ( ancestor ) for ancestor in argument . _proxied . ancestors ( ) ) : try : argument . locals [ REVERSED_PROTOCOL_METHOD ] except KeyError : self . add_message ( "bad-reversed-sequence" , node = node ) return if hasattr ( argument , "getattr" ) : for methods in REVERSED_METHODS : for meth in methods : try : argument . getattr ( meth ) except astroid . NotFoundError : break else : break else : self . add_message ( "bad-reversed-sequence" , node = node ) else : self . add_message ( "bad-reversed-sequence" , node = node )
def visit_assignname ( self , node ) : self . _check_assign_to_new_keyword_violation ( node . name , node ) frame = node . frame ( ) assign_type = node . assign_type ( ) if isinstance ( assign_type , astroid . Comprehension ) : self . _check_name ( "inlinevar" , node . name , node ) elif isinstance ( frame , astroid . Module ) : if isinstance ( assign_type , astroid . Assign ) and not in_loop ( assign_type ) : if isinstance ( utils . safe_infer ( assign_type . value ) , astroid . ClassDef ) : self . _check_name ( "class" , node . name , node ) else : if not _redefines_import ( node ) : self . _check_name ( "const" , node . name , node ) elif isinstance ( assign_type , astroid . ExceptHandler ) : self . _check_name ( "variable" , node . name , node ) elif isinstance ( frame , astroid . FunctionDef ) : if node . name in frame and node . name not in frame . argnames ( ) : if not _redefines_import ( node ) : self . _check_name ( "variable" , node . name , node ) elif isinstance ( frame , astroid . ClassDef ) : if not list ( frame . local_attr_ancestors ( node . name ) ) : self . _check_name ( "class_attribute" , node . name , node )
def _check_name ( self , node_type , name , node , confidence = interfaces . HIGH ) : def _should_exempt_from_invalid_name ( node ) : if node_type == "variable" : inferred = utils . safe_infer ( node ) if isinstance ( inferred , astroid . ClassDef ) : return True return False if utils . is_inside_except ( node ) : clobbering , _ = utils . clobber_in_except ( node ) if clobbering : return if name in self . config . good_names : return if name in self . config . bad_names : self . stats [ "badname_" + node_type ] += 1 self . add_message ( "blacklisted-name" , node = node , args = name ) return regexp = self . _name_regexps [ node_type ] match = regexp . match ( name ) if _is_multi_naming_match ( match , node_type , confidence ) : name_group = self . _find_name_group ( node_type ) bad_name_group = self . _bad_names . setdefault ( name_group , { } ) warnings = bad_name_group . setdefault ( match . lastgroup , [ ] ) warnings . append ( ( node , node_type , name , confidence ) ) if match is None and not _should_exempt_from_invalid_name ( node ) : self . _raise_name_warning ( node , node_type , name , confidence )
def _check_docstring ( self , node_type , node , report_missing = True , confidence = interfaces . HIGH ) : docstring = node . doc if docstring is None : if not report_missing : return lines = utils . get_node_last_lineno ( node ) - node . lineno if node_type == "module" and not lines : return max_lines = self . config . docstring_min_length if node_type != "module" and max_lines > - 1 and lines < max_lines : return self . stats [ "undocumented_" + node_type ] += 1 if ( node . body and isinstance ( node . body [ 0 ] , astroid . Expr ) and isinstance ( node . body [ 0 ] . value , astroid . Call ) ) : func = utils . safe_infer ( node . body [ 0 ] . value . func ) if isinstance ( func , astroid . BoundMethod ) and isinstance ( func . bound , astroid . Instance ) : if PY3K and func . bound . name == "str" : return if func . bound . name in ( "str" , "unicode" , "bytes" ) : return self . add_message ( "missing-docstring" , node = node , args = ( node_type , ) , confidence = confidence ) elif not docstring . strip ( ) : self . stats [ "undocumented_" + node_type ] += 1 self . add_message ( "empty-docstring" , node = node , args = ( node_type , ) , confidence = confidence )
def _check_literal_comparison ( self , literal , node ) : nodes = ( astroid . List , astroid . Tuple , astroid . Dict , astroid . Set ) is_other_literal = isinstance ( literal , nodes ) is_const = False if isinstance ( literal , astroid . Const ) : if isinstance ( literal . value , bool ) or literal . value is None : return is_const = isinstance ( literal . value , ( bytes , str , int , float ) ) if is_const or is_other_literal : self . add_message ( "literal-comparison" , node = node )
def _subgraph ( self , node , name , extra_blocks = ( ) ) : if self . graph is None : self . graph = PathGraph ( node ) self . _subgraph_parse ( node , node , extra_blocks ) self . graphs [ "%s%s" % ( self . classname , name ) ] = self . graph self . reset ( ) else : self . _append_node ( node ) self . _subgraph_parse ( node , node , extra_blocks )
def _subgraph_parse ( self , node , pathnode , extra_blocks ) : loose_ends = [ ] self . tail = node self . dispatch_list ( node . body ) loose_ends . append ( self . tail ) for extra in extra_blocks : self . tail = node self . dispatch_list ( extra . body ) loose_ends . append ( self . tail ) if node . orelse : self . tail = node self . dispatch_list ( node . orelse ) loose_ends . append ( self . tail ) else : loose_ends . append ( node ) if node : bottom = "%s" % self . _bottom_counter self . _bottom_counter += 1 for le in loose_ends : self . graph . connect ( le , bottom ) self . tail = bottom
def add_checker ( self , checker ) : vcids = set ( ) lcids = set ( ) visits = self . visit_events leaves = self . leave_events for member in dir ( checker ) : cid = member [ 6 : ] if cid == "default" : continue if member . startswith ( "visit_" ) : v_meth = getattr ( checker , member ) if self . _is_method_enabled ( v_meth ) : visits [ cid ] . append ( v_meth ) vcids . add ( cid ) elif member . startswith ( "leave_" ) : l_meth = getattr ( checker , member ) if self . _is_method_enabled ( l_meth ) : leaves [ cid ] . append ( l_meth ) lcids . add ( cid ) visit_default = getattr ( checker , "visit_default" , None ) if visit_default : for cls in nodes . ALL_NODE_CLASSES : cid = cls . __name__ . lower ( ) if cid not in vcids : visits [ cid ] . append ( visit_default )
def add_relationship ( self , from_object , to_object , relation_type , name = None ) : rel = Relationship ( from_object , to_object , relation_type , name ) self . relationships . setdefault ( relation_type , [ ] ) . append ( rel )
def get_relationship ( self , from_object , relation_type ) : for rel in self . relationships . get ( relation_type , ( ) ) : if rel . from_object is from_object : return rel raise KeyError ( relation_type )
def get_attrs ( self , node ) : attrs = [ ] properties = [ ( n , m ) for n , m in node . items ( ) if isinstance ( m , astroid . FunctionDef ) and decorated_with_property ( m ) ] for node_name , associated_nodes in ( list ( node . instance_attrs_type . items ( ) ) + list ( node . locals_type . items ( ) ) + properties ) : if not self . show_attr ( node_name ) : continue names = self . class_names ( associated_nodes ) if names : node_name = "%s : %s" % ( node_name , ", " . join ( names ) ) attrs . append ( node_name ) return sorted ( attrs )
def add_object ( self , title , node ) : assert node not in self . _nodes ent = DiagramEntity ( title , node ) self . _nodes [ node ] = ent self . objects . append ( ent )
def class_names ( self , nodes ) : names = [ ] for node in nodes : if isinstance ( node , astroid . Instance ) : node = node . _proxied if ( isinstance ( node , astroid . ClassDef ) and hasattr ( node , "name" ) and not self . has_node ( node ) ) : if node . name not in names : node_name = node . name names . append ( node_name ) return names
def classes ( self ) : return [ o for o in self . objects if isinstance ( o . node , astroid . ClassDef ) ]
def classe ( self , name ) : for klass in self . classes ( ) : if klass . node . name == name : return klass raise KeyError ( name )
def extract_relationships ( self ) : for obj in self . classes ( ) : node = obj . node obj . attrs = self . get_attrs ( node ) obj . methods = self . get_methods ( node ) if is_interface ( node ) : obj . shape = "interface" else : obj . shape = "class" for par_node in node . ancestors ( recurs = False ) : try : par_obj = self . object_from_node ( par_node ) self . add_relationship ( obj , par_obj , "specialization" ) except KeyError : continue for impl_node in node . implements : try : impl_obj = self . object_from_node ( impl_node ) self . add_relationship ( obj , impl_obj , "implements" ) except KeyError : continue for name , values in list ( node . instance_attrs_type . items ( ) ) + list ( node . locals_type . items ( ) ) : for value in values : if value is astroid . Uninferable : continue if isinstance ( value , astroid . Instance ) : value = value . _proxied try : associated_obj = self . object_from_node ( value ) self . add_relationship ( associated_obj , obj , "association" , name ) except KeyError : continue
def modules ( self ) : return [ o for o in self . objects if isinstance ( o . node , astroid . Module ) ]
def module ( self , name ) : for mod in self . modules ( ) : if mod . node . name == name : return mod raise KeyError ( name )
def add_from_depend ( self , node , from_module ) : mod_name = node . root ( ) . name obj = self . module ( mod_name ) if from_module not in obj . node . depends : obj . node . depends . append ( from_module )
def extract_relationships ( self ) : ClassDiagram . extract_relationships ( self ) for obj in self . classes ( ) : try : mod = self . object_from_node ( obj . node . root ( ) ) self . add_relationship ( obj , mod , "ownership" ) except KeyError : continue for obj in self . modules ( ) : obj . shape = "package" for dep_name in obj . node . depends : try : dep = self . get_module ( dep_name , obj . node ) except KeyError : continue self . add_relationship ( obj , dep , "depends" )
def query ( self ) : if hasattr ( self . model , 'query' ) : return self . model . query else : return self . session . query ( self . model )
def prepare_request ( uri , headers = None , data = None , method = None ) : if headers is None : headers = { } if data and not method : method = 'POST' elif not method : method = 'GET' if method == 'GET' and data : uri = add_params_to_uri ( uri , data ) data = None return uri , headers , data , method
def handle_oauth1_response ( self , args ) : client = self . make_client ( ) client . verifier = args . get ( 'oauth_verifier' ) tup = session . get ( '%s_oauthtok' % self . name ) if not tup : raise OAuthException ( 'Token not found, maybe you disabled cookie' , type = 'token_not_found' ) client . resource_owner_key = tup [ 0 ] client . resource_owner_secret = tup [ 1 ] uri , headers , data = client . sign ( self . expand_url ( self . access_token_url ) , _encode ( self . access_token_method ) ) headers . update ( self . _access_token_headers ) resp , content = self . http_request ( uri , headers , to_bytes ( data , self . encoding ) , method = self . access_token_method ) data = parse_response ( resp , content ) if resp . code not in ( 200 , 201 ) : raise OAuthException ( 'Invalid response from %s' % self . name , type = 'invalid_response' , data = data ) return data
def handle_oauth2_response ( self , args ) : client = self . make_client ( ) remote_args = { 'code' : args . get ( 'code' ) , 'client_secret' : self . consumer_secret , 'redirect_uri' : session . get ( '%s_oauthredir' % self . name ) } log . debug ( 'Prepare oauth2 remote args %r' , remote_args ) remote_args . update ( self . access_token_params ) headers = copy ( self . _access_token_headers ) if self . access_token_method == 'POST' : headers . update ( { 'Content-Type' : 'application/x-www-form-urlencoded' } ) body = client . prepare_request_body ( * * remote_args ) resp , content = self . http_request ( self . expand_url ( self . access_token_url ) , headers = headers , data = to_bytes ( body , self . encoding ) , method = self . access_token_method , ) elif self . access_token_method == 'GET' : qs = client . prepare_request_body ( * * remote_args ) url = self . expand_url ( self . access_token_url ) url += ( '?' in url and '&' or '?' ) + qs resp , content = self . http_request ( url , headers = headers , method = self . access_token_method , ) else : raise OAuthException ( 'Unsupported access_token_method: %s' % self . access_token_method ) data = parse_response ( resp , content , content_type = self . content_type ) if resp . code not in ( 200 , 201 ) : raise OAuthException ( 'Invalid response from %s' % self . name , type = 'invalid_response' , data = data ) return data
def authorized_response ( self , args = None ) : if args is None : args = request . args if 'oauth_verifier' in args : data = self . handle_oauth1_response ( args ) elif 'code' in args : data = self . handle_oauth2_response ( args ) else : data = self . handle_unknown_response ( ) session . pop ( '%s_oauthtok' % self . name , None ) session . pop ( '%s_oauthredir' % self . name , None ) return data
def _make_client_with_token ( self , token ) : cached_clients = getattr ( self , 'clients' , None ) hashed_token = _hash_token ( self , token ) if cached_clients and hashed_token in cached_clients : return cached_clients [ hashed_token ] client = self . make_client ( token ) if cached_clients : cached_clients [ hashed_token ] = client return client
def confirm_authorization_request ( self ) : server = self . server uri , http_method , body , headers = extract_params ( ) try : realms , credentials = server . get_realms_and_credentials ( uri , http_method = http_method , body = body , headers = headers ) ret = server . create_authorization_response ( uri , http_method , body , headers , realms , credentials ) log . debug ( 'Authorization successful.' ) return create_response ( * ret ) except errors . OAuth1Error as e : return redirect ( e . in_uri ( self . error_uri ) ) except errors . InvalidClientError as e : return redirect ( e . in_uri ( self . error_uri ) )
def require_oauth ( self , * realms , * * kwargs ) : def wrapper ( f ) : @ wraps ( f ) def decorated ( * args , * * kwargs ) : for func in self . _before_request_funcs : func ( ) if hasattr ( request , 'oauth' ) and request . oauth : return f ( * args , * * kwargs ) server = self . server uri , http_method , body , headers = extract_params ( ) try : valid , req = server . validate_protected_resource_request ( uri , http_method , body , headers , realms ) except Exception as e : log . warn ( 'Exception: %r' , e ) e . urlencoded = urlencode ( [ ( 'error' , 'unknown' ) ] ) e . status_code = 400 return _error_response ( e ) for func in self . _after_request_funcs : valid , req = func ( valid , req ) if not valid : return abort ( 401 ) req . user = req . access_token . user request . oauth = req return f ( * args , * * kwargs ) return decorated return wrapper
def get_default_realms ( self , client_key , request ) : log . debug ( 'Get realms for %r' , client_key ) if not request . client : request . client = self . _clientgetter ( client_key = client_key ) client = request . client if hasattr ( client , 'default_realms' ) : return client . default_realms return [ ]
def get_realms ( self , token , request ) : log . debug ( 'Get realms of %r' , token ) tok = request . request_token or self . _grantgetter ( token = token ) if not tok : return [ ] request . request_token = tok if hasattr ( tok , 'realms' ) : return tok . realms or [ ] return [ ]
def get_redirect_uri ( self , token , request ) : log . debug ( 'Get redirect uri of %r' , token ) tok = request . request_token or self . _grantgetter ( token = token ) return tok . redirect_uri
def get_rsa_key ( self , client_key , request ) : if not request . client : request . client = self . _clientgetter ( client_key = client_key ) if hasattr ( request . client , 'rsa_key' ) : return request . client . rsa_key return None
def validate_client_key ( self , client_key , request ) : log . debug ( 'Validate client key for %r' , client_key ) if not request . client : request . client = self . _clientgetter ( client_key = client_key ) if request . client : return True return False
def validate_request_token ( self , client_key , token , request ) : log . debug ( 'Validate request token %r for %r' , token , client_key ) tok = request . request_token or self . _grantgetter ( token = token ) if tok and tok . client_key == client_key : request . request_token = tok return True return False
def validate_access_token ( self , client_key , token , request ) : log . debug ( 'Validate access token %r for %r' , token , client_key ) tok = request . access_token or self . _tokengetter ( client_key = client_key , token = token , ) if tok : request . access_token = tok return True return False
def validate_timestamp_and_nonce ( self , client_key , timestamp , nonce , request , request_token = None , access_token = None ) : log . debug ( 'Validate timestamp and nonce %r' , client_key ) nonce_exists = self . _noncegetter ( client_key = client_key , timestamp = timestamp , nonce = nonce , request_token = request_token , access_token = access_token ) if nonce_exists : return False self . _noncesetter ( client_key = client_key , timestamp = timestamp , nonce = nonce , request_token = request_token , access_token = access_token ) return True
def validate_redirect_uri ( self , client_key , redirect_uri , request ) : log . debug ( 'Validate redirect_uri %r for %r' , redirect_uri , client_key ) if not request . client : request . client = self . _clientgetter ( client_key = client_key ) if not request . client : return False if not request . client . redirect_uris and redirect_uri is None : return True request . redirect_uri = redirect_uri return redirect_uri in request . client . redirect_uris
def validate_realms ( self , client_key , token , request , uri = None , realms = None ) : log . debug ( 'Validate realms %r for %r' , realms , client_key ) if request . access_token : tok = request . access_token else : tok = self . _tokengetter ( client_key = client_key , token = token ) request . access_token = tok if not tok : return False return set ( tok . realms ) . issuperset ( set ( realms ) )
def validate_verifier ( self , client_key , token , verifier , request ) : log . debug ( 'Validate verifier %r for %r' , verifier , client_key ) data = self . _verifiergetter ( verifier = verifier , token = token ) if not data : return False if not hasattr ( data , 'user' ) : log . debug ( 'Verifier should has user attribute' ) return False request . user = data . user if hasattr ( data , 'client_key' ) : return data . client_key == client_key return True
def verify_request_token ( self , token , request ) : log . debug ( 'Verify request token %r' , token ) tok = request . request_token or self . _grantgetter ( token = token ) if tok : request . request_token = tok return True return False
def verify_realms ( self , token , realms , request ) : log . debug ( 'Verify realms %r' , realms ) tok = request . request_token or self . _grantgetter ( token = token ) if not tok : return False request . request_token = tok if not hasattr ( tok , 'realms' ) : return True return set ( tok . realms ) == set ( realms )
def confirm_authorization_request ( self ) : server = self . server scope = request . values . get ( 'scope' ) or '' scopes = scope . split ( ) credentials = dict ( client_id = request . values . get ( 'client_id' ) , redirect_uri = request . values . get ( 'redirect_uri' , None ) , response_type = request . values . get ( 'response_type' , None ) , state = request . values . get ( 'state' , None ) ) log . debug ( 'Fetched credentials from request %r.' , credentials ) redirect_uri = credentials . get ( 'redirect_uri' ) log . debug ( 'Found redirect_uri %s.' , redirect_uri ) uri , http_method , body , headers = extract_params ( ) try : ret = server . create_authorization_response ( uri , http_method , body , headers , scopes , credentials ) log . debug ( 'Authorization successful.' ) return create_response ( * ret ) except oauth2 . FatalClientError as e : log . debug ( 'Fatal client error %r' , e , exc_info = True ) return self . _on_exception ( e , e . in_uri ( self . error_uri ) ) except oauth2 . OAuth2Error as e : log . debug ( 'OAuth2Error: %r' , e , exc_info = True ) state = request . values . get ( 'state' ) if state and not e . state : e . state = state return self . _on_exception ( e , e . in_uri ( redirect_uri or self . error_uri ) ) except Exception as e : log . exception ( e ) return self . _on_exception ( e , add_params_to_uri ( self . error_uri , { 'error' : str ( e ) } ) )
def require_oauth ( self , * scopes ) : def wrapper ( f ) : @ wraps ( f ) def decorated ( * args , * * kwargs ) : for func in self . _before_request_funcs : func ( ) if hasattr ( request , 'oauth' ) and request . oauth : return f ( * args , * * kwargs ) valid , req = self . verify_request ( scopes ) for func in self . _after_request_funcs : valid , req = func ( valid , req ) if not valid : if self . _invalid_response : return self . _invalid_response ( req ) return abort ( 401 ) request . oauth = req return f ( * args , * * kwargs ) return decorated return wrapper
def get_default_redirect_uri ( self , client_id , request , * args , * * kwargs ) : request . client = request . client or self . _clientgetter ( client_id ) redirect_uri = request . client . default_redirect_uri log . debug ( 'Found default redirect uri %r' , redirect_uri ) return redirect_uri
def get_default_scopes ( self , client_id , request , * args , * * kwargs ) : request . client = request . client or self . _clientgetter ( client_id ) scopes = request . client . default_scopes log . debug ( 'Found default scopes %r' , scopes ) return scopes
def save_authorization_code ( self , client_id , code , request , * args , * * kwargs ) : log . debug ( 'Persist authorization code %r for client %r' , code , client_id ) request . client = request . client or self . _clientgetter ( client_id ) self . _grantsetter ( client_id , code , request , * args , * * kwargs ) return request . client . default_redirect_uri
def save_bearer_token ( self , token , request , * args , * * kwargs ) : log . debug ( 'Save bearer token %r' , token ) self . _tokensetter ( token , request , * args , * * kwargs ) return request . client . default_redirect_uri
def validate_client_id ( self , client_id , request , * args , * * kwargs ) : log . debug ( 'Validate client %r' , client_id ) client = request . client or self . _clientgetter ( client_id ) if client : request . client = client return True return False
def validate_code ( self , client_id , code , client , request , * args , * * kwargs ) : client = client or self . _clientgetter ( client_id ) log . debug ( 'Validate code for client %r and code %r' , client . client_id , code ) grant = self . _grantgetter ( client_id = client . client_id , code = code ) if not grant : log . debug ( 'Grant not found.' ) return False if hasattr ( grant , 'expires' ) and datetime . datetime . utcnow ( ) > grant . expires : log . debug ( 'Grant is expired.' ) return False request . state = kwargs . get ( 'state' ) request . user = grant . user request . scopes = grant . scopes return True
def validate_scopes ( self , client_id , scopes , client , request , * args , * * kwargs ) : if hasattr ( client , 'validate_scopes' ) : return client . validate_scopes ( scopes ) return set ( client . default_scopes ) . issuperset ( set ( scopes ) )
def revoke_token ( self , token , token_type_hint , request , * args , * * kwargs ) : if token_type_hint : tok = self . _tokengetter ( * * { token_type_hint : token } ) else : tok = self . _tokengetter ( access_token = token ) if not tok : tok = self . _tokengetter ( refresh_token = token ) if tok : request . client_id = tok . client_id request . user = tok . user tok . delete ( ) return True msg = 'Invalid token supplied.' log . debug ( msg ) request . error_message = msg return False
def update_qq_api_request_data ( data = { } ) : defaults = { 'openid' : session . get ( 'qq_openid' ) , 'access_token' : session . get ( 'qq_token' ) [ 0 ] , 'oauth_consumer_key' : QQ_APP_ID , } defaults . update ( data ) return defaults
def convert_keys_to_string ( dictionary ) : if not isinstance ( dictionary , dict ) : return dictionary return dict ( ( str ( k ) , convert_keys_to_string ( v ) ) for k , v in dictionary . items ( ) )
def register_to ( self , oauth , name = None , * * kwargs ) : kwargs = self . _process_kwargs ( name = ( name or self . default_name ) , * * kwargs ) return oauth . remote_app ( * * kwargs )
def create ( self , oauth , * * kwargs ) : kwargs = self . _process_kwargs ( name = self . default_name , register = False , * * kwargs ) return oauth . remote_app ( * * kwargs )
def extract_params ( ) : uri = _get_uri_from_request ( request ) http_method = request . method headers = dict ( request . headers ) if 'wsgi.input' in headers : del headers [ 'wsgi.input' ] if 'wsgi.errors' in headers : del headers [ 'wsgi.errors' ] if request . authorization : headers [ 'Authorization' ] = request . authorization body = request . form . to_dict ( ) return uri , http_method , body , headers
def to_bytes ( text , encoding = 'utf-8' ) : if not text : return text if not isinstance ( text , bytes_type ) : text = text . encode ( encoding ) return text
def decode_base64 ( text , encoding = 'utf-8' ) : text = to_bytes ( text , encoding ) return to_unicode ( base64 . b64decode ( text ) , encoding )
def create_response ( headers , body , status ) : response = Response ( body or '' ) for k , v in headers . items ( ) : response . headers [ str ( k ) ] = v response . status_code = status return response
def get_cached_clients ( ) : if OAuth . state_key not in current_app . extensions : raise RuntimeError ( '%r is not initialized.' % current_app ) state = current_app . extensions [ OAuth . state_key ] return state . cached_clients
def check_exception ( self ) : for i in xrange ( self . iterations ) : cert = X509 ( ) try : cert . get_pubkey ( ) except Error : pass
def check_success ( self ) : small = xrange ( 3 ) for i in xrange ( self . iterations ) : key = PKey ( ) key . generate_key ( TYPE_DSA , 256 ) for i in small : cert = X509 ( ) cert . set_pubkey ( key ) for i in small : cert . get_pubkey ( )
def check_load_privatekey_callback ( self ) : for i in xrange ( self . iterations * 10 ) : load_privatekey ( FILETYPE_PEM , self . ENCRYPTED_PEM , lambda * args : "hello, secret" )
def _bio_to_string ( bio ) : result_buffer = _ffi . new ( 'char**' ) buffer_length = _lib . BIO_get_mem_data ( bio , result_buffer ) return _ffi . buffer ( result_buffer [ 0 ] , buffer_length ) [ : ]
def _print_token_factory ( col ) : def _helper ( msg ) : style = style_from_dict ( { Token . Color : col , } ) tokens = [ ( Token . Color , msg ) ] print_tokens ( tokens , style = style ) def _helper_no_terminal ( msg ) : print ( msg ) if sys . stdout . isatty ( ) : return _helper else : return _helper_no_terminal
def get_service_metadata ( self ) : return { 'import_labels_as_tags' : self . config . get ( 'import_labels_as_tags' , False , asbool ) , 'label_template' : self . config . get ( 'label_template' , DEFAULT_LABEL_TEMPLATE ) , }
def issues ( self ) : for board in self . get_boards ( ) : for lst in self . get_lists ( board [ 'id' ] ) : listextra = dict ( boardname = board [ 'name' ] , listname = lst [ 'name' ] ) for card in self . get_cards ( lst [ 'id' ] ) : issue = self . get_issue_for_record ( card , extra = listextra ) issue . update_extra ( { "annotations" : self . annotations ( card ) } ) yield issue
def get_comments ( self , card_id ) : params = { 'filter' : 'commentCard' , 'memberCreator_fields' : 'username' } comments = self . api_request ( "/1/cards/{card_id}/actions" . format ( card_id = card_id ) , * * params ) for comment in comments : assert comment [ 'type' ] == 'commentCard' yield comment
def get_issues ( self , repo , keys ) : key1 , key2 = keys key3 = key1 [ : - 1 ] url = self . base_url + "/api/0/" + repo + "/" + key1 response = self . session . get ( url , params = dict ( status = 'Open' ) ) if not bool ( response ) : error = response . json ( ) code = error [ 'error_code' ] if code == 'ETRACKERDISABLED' : return [ ] else : raise IOError ( 'Failed to talk to %r %r' % ( url , error ) ) issues = [ ] for result in response . json ( ) [ key2 ] : idx = six . text_type ( result [ 'id' ] ) result [ 'html_url' ] = "/" . join ( [ self . base_url , repo , key3 , idx ] ) issues . append ( ( repo , result ) ) return issues
def _api_url ( self , path , * * context ) : if self . host == 'github.com' : baseurl = "https://api.github.com" else : baseurl = "https://{}/api/v3" . format ( self . host ) return baseurl + path . format ( * * context )
def _getter ( self , url , subkey = None ) : kwargs = { } if 'basic' in self . auth : kwargs [ 'auth' ] = self . auth [ 'basic' ] results = [ ] link = dict ( next = url ) while 'next' in link : response = self . session . get ( link [ 'next' ] , * * kwargs ) if response . status_code == 404 and 'token' in self . auth : log . warn ( "A '404' from github may indicate an auth " "failure. Make sure both that your token is correct " "and that it has 'public_repo' and not 'public " "access' rights." ) json_res = self . json_response ( response ) if subkey is not None : json_res = json_res [ subkey ] results += json_res link = self . _link_field_to_dict ( response . headers . get ( 'link' , None ) ) return results
def get_owned_repo_issues ( self , tag ) : issues = { } for issue in self . client . get_issues ( * tag . split ( '/' ) ) : issues [ issue [ 'url' ] ] = ( tag , issue ) return issues
def get_query ( self , query ) : issues = { } for issue in self . client . get_query ( query ) : url = issue [ 'html_url' ] try : repo = self . get_repository_from_issue ( issue ) except ValueError as e : log . critical ( e ) else : issues [ url ] = ( repo , issue ) return issues
def _reqs ( self , tag ) : return [ ( tag , i ) for i in self . client . get_pulls ( * tag . split ( '/' ) ) ]
def aggregate_issues ( conf , main_section , debug ) : log . info ( "Starting to aggregate remote issues." ) targets = aslist ( conf . get ( main_section , 'targets' ) ) queue = multiprocessing . Queue ( ) log . info ( "Spawning %i workers." % len ( targets ) ) processes = [ ] if debug : for target in targets : _aggregate_issues ( conf , main_section , target , queue , conf . get ( target , 'service' ) ) else : for target in targets : proc = multiprocessing . Process ( target = _aggregate_issues , args = ( conf , main_section , target , queue , conf . get ( target , 'service' ) ) ) proc . start ( ) processes . append ( proc ) time . sleep ( 1 ) currently_running = len ( targets ) while currently_running > 0 : issue = queue . get ( True ) if isinstance ( issue , tuple ) : completion_type , args = issue if completion_type == SERVICE_FINISHED_ERROR : target , e = args log . info ( "Terminating workers" ) for process in processes : process . terminate ( ) raise RuntimeError ( "critical error in target '{}'" . format ( target ) ) currently_running -= 1 continue yield issue log . info ( "Done aggregating remote issues." )
def _get_config_or_default ( self , key , default , as_type = lambda x : x ) : if self . main_config . has_option ( self . main_section , key ) : return as_type ( self . main_config . get ( self . main_section , key ) ) return default
def validate_config ( cls , service_config , target ) : if service_config . has_option ( target , 'only_if_assigned' ) : die ( "[%s] has an 'only_if_assigned' option.  Should be " "'%s.only_if_assigned'." % ( target , cls . CONFIG_PREFIX ) ) if service_config . has_option ( target , 'also_unassigned' ) : die ( "[%s] has an 'also_unassigned' option.  Should be " "'%s.also_unassigned'." % ( target , cls . CONFIG_PREFIX ) ) if service_config . has_option ( target , 'default_priority' ) : die ( "[%s] has a 'default_priority' option.  Should be " "'%s.default_priority'." % ( target , cls . CONFIG_PREFIX ) ) if service_config . has_option ( target , 'add_tags' ) : die ( "[%s] has an 'add_tags' option.  Should be " "'%s.add_tags'." % ( target , cls . CONFIG_PREFIX ) )
def include ( self , issue ) : only_if_assigned = self . config . get ( 'only_if_assigned' , None ) if only_if_assigned : owner = self . get_owner ( issue ) include_owners = [ only_if_assigned ] if self . config . get ( 'also_unassigned' , None , asbool ) : include_owners . append ( None ) return owner in include_owners only_if_author = self . config . get ( 'only_if_author' , None ) if only_if_author : return self . get_author ( issue ) == only_if_author return True
def oracle_eval ( command ) : p = subprocess . Popen ( command , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) p . wait ( ) if p . returncode == 0 : return p . stdout . readline ( ) . strip ( ) . decode ( 'utf-8' ) else : die ( "Error retrieving password: `{command}` returned '{error}'" . format ( command = command , error = p . stderr . read ( ) . strip ( ) ) )
def getint ( self , section , option ) : try : return super ( BugwarriorConfigParser , self ) . getint ( section , option ) except ValueError : if self . get ( section , option ) == u'' : return None else : raise ValueError ( "{section}.{option} must be an integer or empty." . format ( section = section , option = option ) )
def get_data ( self , url ) : return self . json_response ( requests . get ( url , * * self . requests_kwargs ) )
def hamdist ( str1 , str2 ) : diffs = 0 for ch1 , ch2 in zip ( str1 , str2 ) : if ch1 != ch2 : diffs += 1 return diffs
def fdrcorrection ( pvals , alpha = 0.05 ) : pvals = np . asarray ( pvals ) pvals_sortind = np . argsort ( pvals ) pvals_sorted = np . take ( pvals , pvals_sortind ) ecdffactor = _ecdf ( pvals_sorted ) reject = pvals_sorted <= ecdffactor * alpha if reject . any ( ) : rejectmax = max ( np . nonzero ( reject ) [ 0 ] ) reject [ : rejectmax ] = True pvals_corrected_raw = pvals_sorted / ecdffactor pvals_corrected = np . minimum . accumulate ( pvals_corrected_raw [ : : - 1 ] ) [ : : - 1 ] del pvals_corrected_raw pvals_corrected [ pvals_corrected > 1 ] = 1 pvals_corrected_ = np . empty_like ( pvals_corrected ) pvals_corrected_ [ pvals_sortind ] = pvals_corrected del pvals_corrected reject_ = np . empty_like ( reject ) reject_ [ pvals_sortind ] = reject return reject_ , pvals_corrected_
def prepare_argparser ( ) : description = "%(prog)s -- Gene Set Enrichment Analysis in Python" epilog = "For command line options of each command, type: %(prog)s COMMAND -h" argparser = ap . ArgumentParser ( description = description , epilog = epilog ) argparser . add_argument ( "--version" , action = "version" , version = "%(prog)s " + __version__ ) subparsers = argparser . add_subparsers ( dest = 'subcommand_name' ) #help="sub-command help") add_gsea_parser ( subparsers ) add_prerank_parser ( subparsers ) add_singlesample_parser ( subparsers ) add_plot_parser ( subparsers ) add_enrichr_parser ( subparsers ) add_biomart_parser ( subparsers ) return argparser
def add_gsea_parser ( subparsers ) : argparser_gsea = subparsers . add_parser ( "gsea" , help = "Main GSEApy Function: run GSEApy instead of GSEA." ) group_input = argparser_gsea . add_argument_group ( "Input files arguments" ) group_input . add_argument ( "-d" , "--data" , dest = "data" , action = "store" , type = str , required = True , help = "Input gene expression dataset file in txt format.Same with GSEA." ) group_input . add_argument ( "-c" , "--cls" , dest = "cls" , action = "store" , type = str , required = True , help = "Input class vector (phenotype) file in CLS format. Same with GSEA." ) group_input . add_argument ( "-g" , "--gmt" , dest = "gmt" , action = "store" , type = str , required = True , help = "Gene set database in GMT format. Same with GSEA." ) group_input . add_argument ( "-t" , "--permu-type" , action = "store" , dest = "type" , type = str , metavar = 'perType' , choices = ( "gene_set" , "phenotype" ) , default = "gene_set" , help = "Permutation type. Same with GSEA, choose from {'gene_set', 'phenotype'}" ) group_output = argparser_gsea . add_argument_group ( "Output arguments" ) add_output_option ( group_output ) group_opt = argparser_gsea . add_argument_group ( "GSEA advanced arguments" ) group_opt . add_argument ( "-n" , "--permu-num" , dest = "n" , action = "store" , type = int , default = 1000 , metavar = 'nperm' , help = "Number of random permutations. For calculating esnulls. Default: 1000" ) group_opt . add_argument ( "--min-size" , dest = "mins" , action = "store" , type = int , default = 15 , metavar = 'int' , help = "Min size of input genes presented in Gene Sets. Default: 15" ) group_opt . add_argument ( "--max-size" , dest = "maxs" , action = "store" , type = int , default = 500 , metavar = 'int' , help = "Max size of input genes presented in Gene Sets. Default: 500" ) group_opt . add_argument ( "-w" , "--weight" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , help = 'Weighted_score of rank_metrics. For weighting input genes. Choose from {0, 1, 1.5, 2}. Default: 1' , ) group_opt . add_argument ( "-m" , "--method" , action = "store" , dest = "method" , type = str , metavar = '' , choices = ( "signal_to_noise" , "t_test" , "ratio_of_classes" , "diff_of_classes" , "log2_ratio_of_classes" ) , default = "log2_ratio_of_classes" , help = ) group_opt . add_argument ( "-a" , "--ascending" , action = 'store_true' , dest = 'ascending' , default = False , help = 'Rank metric sorting order. If the -a flag was chosen, then ascending equals to True. Default: False.' ) group_opt . add_argument ( "-s" , "--seed" , dest = "seed" , action = "store" , type = int , default = None , metavar = '' , help = "Number of random seed. Default: None" ) group_opt . add_argument ( "-p" , "--threads" , dest = "threads" , action = "store" , type = int , default = 1 , metavar = 'procs' , help = "Number of Processes you are going to use. Default: 1" ) return
def add_prerank_parser ( subparsers ) : argparser_prerank = subparsers . add_parser ( "prerank" , help = "Run GSEApy Prerank tool on preranked gene list." ) prerank_input = argparser_prerank . add_argument_group ( "Input files arguments" ) prerank_input . add_argument ( "-r" , "--rnk" , dest = "rnk" , action = "store" , type = str , required = True , help = "Ranking metric file in .rnk format. Same with GSEA." ) prerank_input . add_argument ( "-g" , "--gmt" , dest = "gmt" , action = "store" , type = str , required = True , help = "Gene set database in GMT format. Same with GSEA." ) prerank_input . add_argument ( "-l" , "--label" , action = 'store' , nargs = 2 , dest = 'label' , metavar = ( 'pos' , 'neg' ) , type = str , default = ( 'Pos' , 'Neg' ) , help = "The phenotype label argument need two parameters to define. Default: ('Pos','Neg')" ) prerank_output = argparser_prerank . add_argument_group ( "Output arguments" ) add_output_option ( prerank_output ) prerank_opt = argparser_prerank . add_argument_group ( "GSEA advanced arguments" ) prerank_opt . add_argument ( "-n" , "--permu-num" , dest = "n" , action = "store" , type = int , default = 1000 , metavar = 'nperm' , help = "Number of random permutations. For calculating esnulls. Default: 1000" ) prerank_opt . add_argument ( "--min-size" , dest = "mins" , action = "store" , type = int , default = 15 , metavar = 'int' , help = "Min size of input genes presented in Gene Sets. Default: 15" ) prerank_opt . add_argument ( "--max-size" , dest = "maxs" , action = "store" , type = int , default = 500 , metavar = 'int' , help = "Max size of input genes presented in Gene Sets. Default: 500" ) prerank_opt . add_argument ( "-w" , "--weight" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , help = 'Weighted_score of rank_metrics. For weighting input genes. Choose from {0, 1, 1.5, 2}. Default: 1' , ) prerank_opt . add_argument ( "-a" , "--ascending" , action = 'store_true' , dest = 'ascending' , default = False , help = 'Rank metric sorting order. If the -a flag was chosen, then ascending equals to True. Default: False.' ) prerank_opt . add_argument ( "-s" , "--seed" , dest = "seed" , action = "store" , type = int , default = None , metavar = '' , help = "Number of random seed. Default: None" ) prerank_opt . add_argument ( "-p" , "--threads" , dest = "threads" , action = "store" , type = int , default = 1 , metavar = 'procs' , help = "Number of Processes you are going to use. Default: 1" ) return
def add_plot_parser ( subparsers ) : argparser_replot = subparsers . add_parser ( "replot" , help = "Reproduce GSEA desktop output figures." ) group_replot = argparser_replot . add_argument_group ( "Input arguments" ) group_replot . add_argument ( "-i" , "--indir" , action = "store" , dest = "indir" , required = True , metavar = 'GSEA_dir' , help = "The GSEA desktop results directroy that you want to reproduce the figure " ) add_output_option ( group_replot ) #add_output_group( argparser_plot ) group_replot . add_argument ( "-w" , "--weight" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , help = 'Weighted_score of rank_metrics. Please Use the same value in GSEA. Choose from (0, 1, 1.5, 2),default: 1' , ) return
def add_enrichr_parser ( subparsers ) : argparser_enrichr = subparsers . add_parser ( "enrichr" , help = "Using Enrichr API to perform GO analysis." ) enrichr_opt = argparser_enrichr . add_argument_group ( "Input arguments" ) enrichr_opt . add_argument ( "-i" , "--input-list" , action = "store" , dest = "gene_list" , type = str , required = True , metavar = 'IDs' , help = "Enrichr uses a list of gene names as input." ) enrichr_opt . add_argument ( "-g" , "--gene-sets" , action = "store" , dest = "library" , type = str , required = True , metavar = 'GMT' , help = "Enrichr library name(s) required. Separate each name by comma." ) enrichr_opt . add_argument ( "--org" , "--organism" , action = "store" , dest = "organism" , type = str , default = '' , help = "Enrichr supported organism name. Default: human. See here: https://amp.pharm.mssm.edu/modEnrichr." ) enrichr_opt . add_argument ( "--ds" , "--description" , action = "store" , dest = "descrip" , type = str , default = 'enrichr' , metavar = 'STRING' , help = ) enrichr_opt . add_argument ( "--cut" , "--cut-off" , action = "store" , dest = "thresh" , metavar = 'float' , type = float , default = 0.05 , help = "Adjust-Pval cutoff, used for generating plots. Default: 0.05." ) enrichr_opt . add_argument ( "--bg" , "--background" , action = "store" , dest = "bg" , default = 'hsapiens_gene_ensembl' , metavar = 'BGNUM' , help = "BioMart Dataset name or Background total genes number. Default: None" ) enrichr_opt . add_argument ( "-t" , "--top-term" , dest = "term" , action = "store" , type = int , default = 10 , metavar = 'int' , help = "Numbers of top terms shown in the plot. Default: 10" ) enrichr_output = argparser_enrichr . add_argument_group ( "Output figure arguments" ) add_output_option ( enrichr_output ) return
def add_biomart_parser ( subparsers ) : argparser_biomart = subparsers . add_parser ( "biomart" , help = "Using BioMart API to convert gene ids." ) biomart_opt = argparser_biomart . add_argument_group ( "Input arguments" ) biomart_opt . add_argument ( "-f" , "--filter" , action = 'store' , nargs = 2 , dest = 'filter' , required = True , metavar = ( 'NAME' , 'VALUE' ) , help = ) biomart_opt . add_argument ( "-a" , "--attributes" , action = "store" , dest = "attrs" , type = str , required = True , metavar = 'ATTR' , help = "Which attribute(s) to retrieve. Separate each attr by comma." ) biomart_opt . add_argument ( "-o" , "--ofile" , dest = "ofile" , type = str , required = True , help = "Output file name" ) biomart_opt . add_argument ( "-d" , "--dataset" , action = "store" , dest = "bg" , type = str , default = 'hsapiens_gene_ensembl' , metavar = 'DATA' , help = "Which dataset to use. Default: hsapiens_gene_ensembl" ) biomart_opt . add_argument ( "--host" , action = "store" , dest = "host" , type = str , default = 'www.ensembl.org' , metavar = 'HOST' , help = "Which host to use. Select from {'www.ensembl.org', 'asia.ensembl.org', 'useast.ensembl.org'}." ) biomart_opt . add_argument ( "-m" , "--mart" , action = "store" , dest = "mart" , type = str , metavar = 'MART' , default = "ENSEMBL_MART_ENSEMBL" , help = "Which mart to use. Default: ENSEMBL_MART_ENSEMBL." ) biomart_opt . add_argument ( "-v" , "--verbose" , action = "store_true" , default = False , dest = 'verbose' , help = "Increase output verbosity, print out progress of your job" , )
def get_marts ( self ) : mart_names = pd . Series ( self . names , name = "Name" ) mart_descriptions = pd . Series ( self . displayNames , name = "Description" ) return pd . concat ( [ mart_names , mart_descriptions ] , axis = 1 )
def get_datasets ( self , mart = 'ENSEMBL_MART_ENSEMBL' ) : datasets = self . datasets ( mart , raw = True ) return pd . read_csv ( StringIO ( datasets ) , header = None , usecols = [ 1 , 2 ] , names = [ "Name" , "Description" ] , sep = "\t" )
def get_attributes ( self , dataset ) : attributes = self . attributes ( dataset ) attr_ = [ ( k , v [ 0 ] ) for k , v in attributes . items ( ) ] return pd . DataFrame ( attr_ , columns = [ "Attribute" , "Description" ] )
def get_filters ( self , dataset ) : filters = self . filters ( dataset ) filt_ = [ ( k , v [ 0 ] ) for k , v in filters . items ( ) ] return pd . DataFrame ( filt_ , columns = [ "Filter" , "Description" ] )
def prepare_outdir ( self ) : self . _outdir = self . outdir if self . _outdir is None : self . _tmpdir = TemporaryDirectory ( ) self . outdir = self . _tmpdir . name elif isinstance ( self . outdir , str ) : mkdirs ( self . outdir ) else : raise Exception ( "Error parsing outdir: %s" % type ( self . outdir ) ) if isinstance ( self . gene_sets , str ) : _gset = os . path . split ( self . gene_sets ) [ - 1 ] . lower ( ) . rstrip ( ".gmt" ) elif isinstance ( self . gene_sets , dict ) : _gset = "blank_name" else : raise Exception ( "Error parsing gene_sets parameter for gene sets" ) logfile = os . path . join ( self . outdir , "gseapy.%s.%s.log" % ( self . module , _gset ) ) return logfile
def _set_cores ( self ) : cpu_num = cpu_count ( ) - 1 if self . _processes > cpu_num : cores = cpu_num elif self . _processes < 1 : cores = 1 else : cores = self . _processes self . _processes = int ( cores )
def load_gmt ( self , gene_list , gmt ) : if isinstance ( gmt , dict ) : genesets_dict = gmt elif isinstance ( gmt , str ) : genesets_dict = self . parse_gmt ( gmt ) else : raise Exception ( "Error parsing gmt parameter for gene sets" ) subsets = list ( genesets_dict . keys ( ) ) self . n_genesets = len ( subsets ) for subset in subsets : subset_list = genesets_dict . get ( subset ) if isinstance ( subset_list , set ) : subset_list = list ( subset_list ) genesets_dict [ subset ] = subset_list tag_indicator = np . in1d ( gene_list , subset_list , assume_unique = True ) tag_len = tag_indicator . sum ( ) if self . min_size <= tag_len <= self . max_size : continue del genesets_dict [ subset ] filsets_num = len ( subsets ) - len ( genesets_dict ) self . _logger . info ( "%04d gene_sets have been filtered out when max_size=%s and min_size=%s" % ( filsets_num , self . max_size , self . min_size ) ) if filsets_num == len ( subsets ) : self . _logger . error ( "No gene sets passed through filtering condition!!!, try new parameters again!\n" + "Note: check gene name, gmt file format, or filtering size." ) sys . exit ( 0 ) self . _gmtdct = genesets_dict return genesets_dict
def get_libraries ( self , database = '' ) : lib_url = 'http://amp.pharm.mssm.edu/%sEnrichr/datasetStatistics' % database libs_json = json . loads ( requests . get ( lib_url ) . text ) libs = [ lib [ 'libraryName' ] for lib in libs_json [ 'statistics' ] ] return sorted ( libs )
def _download_libraries ( self , libname ) : self . _logger . info ( "Downloading and generating Enrichr library gene sets......" ) s = retry ( 5 ) ENRICHR_URL = 'http://amp.pharm.mssm.edu/Enrichr/geneSetLibrary' query_string = '?mode=text&libraryName=%s' response = s . get ( ENRICHR_URL + query_string % libname , timeout = None ) if not response . ok : raise Exception ( 'Error fetching enrichment results, check internet connection first.' ) mkdirs ( DEFAULT_CACHE_PATH ) genesets_dict = { } outname = "enrichr.%s.gmt" % libname gmtout = open ( os . path . join ( DEFAULT_CACHE_PATH , outname ) , "w" ) for line in response . iter_lines ( chunk_size = 1024 , decode_unicode = 'utf-8' ) : line = line . strip ( ) k = line . split ( "\t" ) [ 0 ] v = list ( map ( lambda x : x . split ( "," ) [ 0 ] , line . split ( "\t" ) [ 2 : ] ) ) genesets_dict . update ( { k : v } ) outline = "%s\t\t%s\n" % ( k , "\t" . join ( v ) ) gmtout . write ( outline ) gmtout . close ( ) return genesets_dict
def _heatmat ( self , df , classes , pheno_pos , pheno_neg ) : width = len ( classes ) if len ( classes ) >= 6 else 5 cls_booA = list ( map ( lambda x : True if x == pheno_pos else False , classes ) ) cls_booB = list ( map ( lambda x : True if x == pheno_neg else False , classes ) ) datA = df . loc [ : , cls_booA ] datB = df . loc [ : , cls_booB ] datAB = pd . concat ( [ datA , datB ] , axis = 1 ) self . _width = width self . heatmat = datAB return
def _save_results ( self , zipdata , outdir , module , gmt , rank_metric , permutation_type ) : res = OrderedDict ( ) for gs , gseale , ind , RES in zipdata : rdict = OrderedDict ( ) rdict [ 'es' ] = gseale [ 0 ] rdict [ 'nes' ] = gseale [ 1 ] rdict [ 'pval' ] = gseale [ 2 ] rdict [ 'fdr' ] = gseale [ 3 ] rdict [ 'geneset_size' ] = len ( gmt [ gs ] ) rdict [ 'matched_size' ] = len ( ind ) #reformat gene list. _genes = rank_metric . index . values [ ind ] rdict [ 'genes' ] = ";" . join ( [ str ( g ) . strip ( ) for g in _genes ] ) if self . module != 'ssgsea' : if rdict [ 'es' ] > 0 : idx = RES . argmax ( ) ldg_pos = list ( filter ( lambda x : x <= idx , ind ) ) elif rdict [ 'es' ] < 0 : idx = RES . argmin ( ) ldg_pos = list ( filter ( lambda x : x >= idx , ind ) ) else : ldg_pos = ind rdict [ 'ledge_genes' ] = ';' . join ( list ( map ( str , rank_metric . iloc [ ldg_pos ] . index ) ) ) rdict [ 'RES' ] = RES rdict [ 'hits_indices' ] = ind res [ gs ] = rdict self . results = res res_df = pd . DataFrame . from_dict ( res , orient = 'index' ) res_df . index . name = 'Term' res_df . drop ( [ 'RES' , 'hits_indices' ] , axis = 1 , inplace = True ) res_df . sort_values ( by = [ 'fdr' , 'pval' ] , inplace = True ) self . res2d = res_df if self . _outdir is None : return out = os . path . join ( outdir , 'gseapy.{b}.{c}.report.csv' . format ( b = module , c = permutation_type ) ) if self . module == 'ssgsea' : out = out . replace ( ".csv" , ".txt" ) with open ( out , 'a' ) as f : f . write ( ) f . write ( ) res_df . to_csv ( f , sep = '\t' ) else : res_df . to_csv ( out ) return
def load_data ( self , cls_vec ) : if isinstance ( self . data , pd . DataFrame ) : exprs = self . data . copy ( ) if exprs . index . dtype == 'O' : exprs = exprs . reset_index ( ) elif os . path . isfile ( self . data ) : if self . data . endswith ( "gct" ) : exprs = pd . read_csv ( self . data , skiprows = 1 , comment = '#' , sep = "\t" ) else : exprs = pd . read_csv ( self . data , comment = '#' , sep = "\t" ) else : raise Exception ( 'Error parsing gene expression DataFrame!' ) #drop duplicated gene names if exprs . iloc [ : , 0 ] . duplicated ( ) . sum ( ) > 0 : self . _logger . warning ( "Warning: dropping duplicated gene names, only keep the first values" ) exprs . drop_duplicates ( subset = exprs . columns [ 0 ] , inplace = True ) #drop duplicate gene_names. if exprs . isnull ( ) . any ( ) . sum ( ) > 0 : self . _logger . warning ( "Warning: Input data contains NA, filled NA with 0" ) exprs . dropna ( how = 'all' , inplace = True ) #drop rows with all NAs exprs = exprs . fillna ( 0 ) exprs . set_index ( keys = exprs . columns [ 0 ] , inplace = True ) df = exprs . select_dtypes ( include = [ np . number ] ) df_std = df . groupby ( by = cls_vec , axis = 1 ) . std ( ) df = df [ ~ df_std . isin ( [ 0 ] ) . any ( axis = 1 ) ] df = df + 0.00001 return df
def runSamplesPermu ( self , df , gmt = None ) : assert self . min_size <= self . max_size mkdirs ( self . outdir ) self . resultsOnSamples = OrderedDict ( ) outdir = self . outdir for name , ser in df . iteritems ( ) : self . outdir = os . path . join ( outdir , str ( name ) ) self . _logger . info ( "Run Sample: %s " % name ) mkdirs ( self . outdir ) dat2 = ser . sort_values ( ascending = self . ascending ) gsea_results , hit_ind , rank_ES , subsets = gsea_compute ( data = dat2 , n = self . permutation_num , gmt = gmt , weighted_score_type = self . weighted_score_type , permutation_type = 'gene_set' , method = None , pheno_pos = '' , pheno_neg = '' , classes = None , ascending = self . ascending , processes = self . _processes , seed = self . seed , single = True , scale = self . scale ) res_zip = zip ( subsets , list ( gsea_results ) , hit_ind , rank_ES ) self . _save_results ( zipdata = res_zip , outdir = self . outdir , module = self . module , gmt = gmt , rank_metric = dat2 , permutation_type = "gene_sets" ) self . resultsOnSamples [ name ] = self . res2d . es if self . _noplot : continue self . _logger . info ( "Plotting Sample: %s \n" % name ) self . _plotting ( rank_metric = dat2 , results = self . results , graph_num = self . graph_num , outdir = self . outdir , figsize = self . figsize , format = self . format ) self . _save ( outdir ) return
def _save ( self , outdir ) : samplesRawES = pd . DataFrame ( self . resultsOnSamples ) samplesRawES . index . name = 'Term|ES' samplesNES = samplesRawES / ( samplesRawES . values . max ( ) - samplesRawES . values . min ( ) ) samplesNES = samplesNES . copy ( ) samplesNES . index . rename ( 'Term|NES' , inplace = True ) self . res2d = samplesNES self . _logger . info ( "Congratulations. GSEApy runs successfully................\n" ) if self . _outdir is None : return outESfile = os . path . join ( outdir , "gseapy.samples.raw.es.txt" ) with open ( outESfile , 'a' ) as f : if self . scale : f . write ( ) f . write ( + 'as indicated by Barbie et al., 2009, online methods, pg. 2\n' ) else : f . write ( ) f . write ( ) samplesRawES . to_csv ( f , sep = '\t' ) outNESfile = os . path . join ( outdir , "gseapy.samples.normalized.es.txt" ) with open ( outNESfile , 'a' ) as f : f . write ( ) f . write ( ) samplesNES . to_csv ( f , sep = '\t' ) return
def prepare_outdir ( self ) : self . _outdir = self . outdir if self . _outdir is None : self . _tmpdir = TemporaryDirectory ( ) self . outdir = self . _tmpdir . name elif isinstance ( self . outdir , str ) : mkdirs ( self . outdir ) else : raise Exception ( "Error parsing outdir: %s" % type ( self . outdir ) ) logfile = os . path . join ( self . outdir , "gseapy.%s.%s.log" % ( self . module , self . descriptions ) ) return logfile
def parse_genesets ( self ) : enrichr_library = self . get_libraries ( ) if isinstance ( self . gene_sets , list ) : gss = self . gene_sets elif isinstance ( self . gene_sets , str ) : gss = [ g . strip ( ) for g in self . gene_sets . strip ( ) . split ( "," ) ] elif isinstance ( self . gene_sets , dict ) : gss = [ self . gene_sets ] else : raise Exception ( "Error parsing enrichr libraries, please provided corrected one" ) gss_exist = [ ] for g in gss : if isinstance ( g , dict ) : gss_exist . append ( g ) continue if isinstance ( g , str ) : if g in enrichr_library : gss_exist . append ( g ) continue if g . lower ( ) . endswith ( ".gmt" ) and os . path . exists ( g ) : self . _logger . info ( "User Defined gene sets is given: %s" % g ) with open ( g ) as genesets : g_dict = { line . strip ( ) . split ( "\t" ) [ 0 ] : line . strip ( ) . split ( "\t" ) [ 2 : ] for line in genesets . readlines ( ) } gss_exist . append ( g_dict ) return gss_exist
def send_genes ( self , gene_list , url ) : payload = { 'list' : ( None , gene_list ) , 'description' : ( None , self . descriptions ) } response = requests . post ( url , files = payload ) if not response . ok : raise Exception ( 'Error analyzing gene list' ) sleep ( 1 ) job_id = json . loads ( response . text ) return job_id
def check_genes ( self , gene_list , usr_list_id ) : response = requests . get ( 'http://amp.pharm.mssm.edu/Enrichr/view?userListId=%s' % usr_list_id ) if not response . ok : raise Exception ( 'Error getting gene list back' ) returnedL = json . loads ( response . text ) [ "genes" ] returnedN = sum ( [ 1 for gene in gene_list if gene in returnedL ] ) self . _logger . info ( '{} genes successfully recognized by Enrichr' . format ( returnedN ) )
def run ( self ) : self . get_organism ( ) genes_list = self . parse_genelists ( ) gss = self . parse_genesets ( ) self . _logger . info ( "Connecting to Enrichr Server to get latest library names" ) if len ( gss ) < 1 : sys . stderr . write ( "Not validated Enrichr library name provided\n" ) sys . stdout . write ( "Hint: use get_library_name() to view full list of supported names" ) sys . exit ( 1 ) self . results = pd . DataFrame ( ) for g in gss : if isinstance ( g , dict ) : res = self . enrich ( g ) shortID , self . _gs = str ( id ( g ) ) , "CUSTOM%s" % id ( g ) if res is None : self . _logger . info ( "No hits return, for gene set: Custom%s" % shortID ) continue else : self . _gs = str ( g ) self . _logger . debug ( "Start Enrichr using library: %s" % ( self . _gs ) ) self . _logger . info ( 'Analysis name: %s, Enrichr Library: %s' % ( self . descriptions , self . _gs ) ) shortID , res = self . get_results ( genes_list ) res . insert ( 0 , "Gene_set" , self . _gs ) self . results = self . results . append ( res , ignore_index = True , sort = True ) self . res2d = res if self . _outdir is None : continue self . _logger . info ( 'Save file of enrichment results: Job Id:' + str ( shortID ) ) outfile = "%s/%s.%s.%s.reports.txt" % ( self . outdir , self . _gs , self . descriptions , self . module ) self . res2d . to_csv ( outfile , index = False , encoding = 'utf-8' , sep = "\t" ) if not self . __no_plot : msg = barplot ( df = res , cutoff = self . cutoff , figsize = self . figsize , top_term = self . __top_term , color = 'salmon' , title = self . _gs , ofname = outfile . replace ( "txt" , self . format ) ) if msg is not None : self . _logger . warning ( msg ) self . _logger . info ( 'Done.\n' ) if self . _outdir is None : self . _tmpdir . cleanup ( ) return
def annulus_hires ( script , radius = None , radius1 = None , radius2 = None , diameter = None , diameter1 = None , diameter2 = None , cir_segments = 48 , rad_segments = 1 , color = None ) : if radius is not None and diameter is None : if radius1 is None and diameter1 is None : radius1 = radius if radius2 is None and diameter2 is None : radius2 = 0 if diameter is not None : if radius1 is None and diameter1 is None : radius1 = diameter / 2 if radius2 is None and diameter2 is None : radius2 = 0 if diameter1 is not None : radius1 = diameter1 / 2 if diameter2 is not None : radius2 = diameter2 / 2 if radius1 is None : radius1 = 1 if radius2 is None : radius2 = 0 ring = ( radius1 - radius2 ) / rad_segments for i in range ( 0 , rad_segments ) : annulus ( script , radius1 = radius1 - i * ring , radius2 = radius1 - ( i + 1 ) * ring , cir_segments = cir_segments ) layers . join ( script , merge_vert = True ) if color is not None : vert_color . function ( script , color = color ) return None
def tube_hires ( script , height = 1.0 , radius = None , radius1 = None , radius2 = None , diameter = None , diameter1 = None , diameter2 = None , cir_segments = 32 , rad_segments = 1 , height_segments = 1 , center = False , simple_bottom = False , color = None ) : if radius is not None and diameter is None : if radius1 is None and diameter1 is None : radius1 = radius if radius2 is None and diameter2 is None : radius2 = 0 if diameter is not None : if radius1 is None and diameter1 is None : radius1 = diameter / 2 if radius2 is None and diameter2 is None : radius2 = 0 if diameter1 is not None : radius1 = diameter1 / 2 if diameter2 is not None : radius2 = diameter2 / 2 if radius1 is None : radius1 = 1 if radius2 is None : radius2 = 0 annulus_hires ( script , radius1 = radius1 , radius2 = radius2 , cir_segments = cir_segments , rad_segments = rad_segments ) transform . translate ( script , [ 0 , 0 , height ] ) if simple_bottom : annulus ( script , radius1 = radius1 , radius2 = radius2 , cir_segments = cir_segments ) else : layers . duplicate ( script ) transform . translate ( script , [ 0 , 0 , - height ] ) transform . rotate ( script , 'x' , 180 ) cylinder_open_hires ( script , height , radius1 , cir_segments = cir_segments , height_segments = height_segments ) if radius2 != 0 : cylinder_open_hires ( script , height , radius2 , cir_segments = cir_segments , height_segments = height_segments , invert_normals = True ) layers . join ( script ) clean . merge_vert ( script , threshold = 0.00002 ) if center : transform . translate ( script , [ 0 , 0 , - height / 2 ] ) if color is not None : vert_color . function ( script , color = color ) return None
def save_to_file ( self , script_file ) : if not self . filters : print ( 'WARNING: no filters to save to file!' ) script_file_descriptor = open ( script_file , 'w' ) script_file_descriptor . write ( '' . join ( self . opening + self . filters + self . closing ) ) script_file_descriptor . close ( )
def per_triangle ( script , sidedim = 0 , textdim = 1024 , border = 2 , method = 1 ) : filter_xml = '' . join ( [ '  <filter name="Parametrization: Trivial Per-Triangle ">\n' , '    <Param name="sidedim"' , 'value="%d"' % sidedim , 'description="Quads per line"' , 'type="RichInt"' , 'tooltip="Indicates how many triangles have to be put on each line (every quad contains two triangles). Leave 0 for automatic calculation"' , '/>\n' , '    <Param name="textdim"' , 'value="%d"' % textdim , 'description="Texture Dimension (px)"' , 'type="RichInt"' , 'tooltip="Gives an indication on how big the texture is"' , '/>\n' , '    <Param name="border"' , 'value="%d"' % border , 'description="Inter-Triangle border (px)"' , 'type="RichInt"' , 'tooltip="Specifies how many pixels to be left between triangles in parametrization domain"' , '/>\n' , '    <Param name="method"' , 'value="%d"' % method , 'description="Method"' , 'enum_val0="Basic"' , 'enum_val1="Space-optimizing"' , 'enum_cardinality="2"' , 'type="RichEnum"' , 'tooltip="Choose space optimizing to map smaller faces into smaller triangles in parametrizazion domain"' '/>\n' , '  </filter>\n' ] ) util . write_filter ( script , filter_xml ) return None
def v_multiply ( scalar , v1 ) : vector = [ ] for i , x in enumerate ( v1 ) : vector . append ( '(({})*({}))' . format ( scalar , v1 [ i ] ) ) return vector
def measure_all ( fbasename = None , log = None , ml_version = ml_version ) : ml_script1_file = 'TEMP3D_measure_gAndT.mlx' if ml_version == '1.3.4BETA' : file_out = 'TEMP3D_aabb.xyz' else : file_out = None ml_script1 = mlx . FilterScript ( file_in = fbasename , file_out = file_out , ml_version = ml_version ) compute . measure_geometry ( ml_script1 ) compute . measure_topology ( ml_script1 ) ml_script1 . save_to_file ( ml_script1_file ) ml_script1 . run_script ( log = log , script_file = ml_script1_file ) geometry = ml_script1 . geometry topology = ml_script1 . topology if ml_version == '1.3.4BETA' : if log is not None : log_file = open ( log , 'a' ) log_file . write ( '***Axis Aligned Bounding Results for file "%s":\n' % fbasename ) log_file . close ( ) aabb = measure_aabb ( file_out , log ) else : aabb = geometry [ 'aabb' ] return aabb , geometry , topology
def measure_dimension ( fbasename = None , log = None , axis1 = None , offset1 = 0.0 , axis2 = None , offset2 = 0.0 , ml_version = ml_version ) : axis1 = axis1 . lower ( ) axis2 = axis2 . lower ( ) ml_script1_file = 'TEMP3D_measure_dimension.mlx' file_out = 'TEMP3D_measure_dimension.xyz' ml_script1 = mlx . FilterScript ( file_in = fbasename , file_out = file_out , ml_version = ml_version ) compute . section ( ml_script1 , axis1 , offset1 , surface = True ) compute . section ( ml_script1 , axis2 , offset2 , surface = False ) layers . delete_lower ( ml_script1 ) ml_script1 . save_to_file ( ml_script1_file ) ml_script1 . run_script ( log = log , script_file = ml_script1_file ) for val in ( 'x' , 'y' , 'z' ) : if val not in ( axis1 , axis2 ) : axis = val axis_num = ord ( axis ) - ord ( 'x' ) aabb = measure_aabb ( file_out , log ) dimension = { 'min' : aabb [ 'min' ] [ axis_num ] , 'max' : aabb [ 'max' ] [ axis_num ] , 'length' : aabb [ 'size' ] [ axis_num ] , 'axis' : axis } if log is None : print ( '\nFor file "%s"' % fbasename ) print ( 'Dimension parallel to %s with %s=%s & %s=%s:' % ( axis , axis1 , offset1 , axis2 , offset2 ) ) print ( '  Min = %s, Max = %s, Total length = %s' % ( dimension [ 'min' ] , dimension [ 'max' ] , dimension [ 'length' ] ) ) else : log_file = open ( log , 'a' ) log_file . write ( '\nFor file "%s"\n' % fbasename ) log_file . write ( 'Dimension parallel to %s with %s=%s & %s=%s:\n' % ( axis , axis1 , offset1 , axis2 , offset2 ) ) log_file . write ( 'min = %s\n' % dimension [ 'min' ] ) log_file . write ( 'max = %s\n' % dimension [ 'max' ] ) log_file . write ( 'Total length = %s\n' % dimension [ 'length' ] ) log_file . close ( ) return dimension
def get_vprof_version ( filename ) : with open ( filename ) as src_file : version_match = re . search ( r"^__version__ = ['\"]([^'\"]*)['\"]" , src_file . read ( ) , re . M ) if version_match : return version_match . group ( 1 ) raise RuntimeError ( 'Unable to find version info.' )
def _get_obj_count_difference ( objs1 , objs2 ) : clean_obj_list1 = _process_in_memory_objects ( objs1 ) clean_obj_list2 = _process_in_memory_objects ( objs2 ) obj_count_1 = _get_object_count_by_type ( clean_obj_list1 ) obj_count_2 = _get_object_count_by_type ( clean_obj_list2 ) return obj_count_1 - obj_count_2
def _format_obj_count ( objects ) : result = [ ] regex = re . compile ( r'<(?P<type>\w+) \'(?P<name>\S+)\'>' ) for obj_type , obj_count in objects . items ( ) : if obj_count != 0 : match = re . findall ( regex , repr ( obj_type ) ) if match : obj_type , obj_name = match [ 0 ] result . append ( ( "%s %s" % ( obj_type , obj_name ) , obj_count ) ) return sorted ( result , key = operator . itemgetter ( 1 ) , reverse = True )
def _trace_memory_usage ( self , frame , event , arg ) : #pylint: disable=unused-argument if event == 'line' and frame . f_code . co_filename in self . target_modules : self . _events_list . append ( ( frame . f_lineno , self . _process . memory_info ( ) . rss , frame . f_code . co_name , frame . f_code . co_filename ) ) return self . _trace_memory_usage
def code_events ( self ) : if self . _resulting_events : return self . _resulting_events for i , ( lineno , mem , func , fname ) in enumerate ( self . _events_list ) : mem_in_mb = float ( mem - self . mem_overhead ) / _BYTES_IN_MB if ( self . _resulting_events and self . _resulting_events [ - 1 ] [ 0 ] == lineno and self . _resulting_events [ - 1 ] [ 2 ] == func and self . _resulting_events [ - 1 ] [ 3 ] == fname and self . _resulting_events [ - 1 ] [ 1 ] < mem_in_mb ) : self . _resulting_events [ - 1 ] [ 1 ] = mem_in_mb else : self . _resulting_events . append ( [ i + 1 , lineno , mem_in_mb , func , fname ] ) return self . _resulting_events
def compute_mem_overhead ( self ) : self . mem_overhead = ( self . _process . memory_info ( ) . rss - builtins . initial_rss_size )
def profile_package ( self ) : target_modules = base_profiler . get_pkg_module_names ( self . _run_object ) try : with _CodeEventsTracker ( target_modules ) as prof : prof . compute_mem_overhead ( ) runpy . run_path ( self . _run_object , run_name = '__main__' ) except SystemExit : pass return prof , None
def profile_module ( self ) : target_modules = { self . _run_object } try : with open ( self . _run_object , 'rb' ) as srcfile , _CodeEventsTracker ( target_modules ) as prof : code = compile ( srcfile . read ( ) , self . _run_object , 'exec' ) prof . compute_mem_overhead ( ) exec ( code , self . _globs , None ) except SystemExit : pass return prof , None
def profile_function ( self ) : target_modules = { self . _run_object . __code__ . co_filename } with _CodeEventsTracker ( target_modules ) as prof : prof . compute_mem_overhead ( ) result = self . _run_object ( * self . _run_args , * * self . _run_kwargs ) return prof , result
def run ( self ) : existing_objects = _get_in_memory_objects ( ) prof , result = self . profile ( ) new_objects = _get_in_memory_objects ( ) new_obj_count = _get_obj_count_difference ( new_objects , existing_objects ) result_obj_count = new_obj_count - prof . obj_overhead result_obj_count [ list ] -= 1 pretty_obj_count = _format_obj_count ( result_obj_count ) return { 'objectName' : self . _object_name , 'codeEvents' : prof . code_events , 'totalEvents' : len ( prof . code_events ) , 'objectsCount' : pretty_obj_count , 'result' : result , 'timestamp' : int ( time . time ( ) ) }
def get_run_object_type ( run_object ) : if isinstance ( run_object , tuple ) : return 'function' run_object , _ , _ = run_object . partition ( ' ' ) if os . path . isdir ( run_object ) : return 'package' return 'module'
def init_module ( self , run_object ) : self . profile = self . profile_module self . _run_object , _ , self . _run_args = run_object . partition ( ' ' ) self . _object_name = '%s (module)' % self . _run_object self . _globs = { '__file__' : self . _run_object , '__name__' : '__main__' , '__package__' : None , } program_path = os . path . dirname ( self . _run_object ) if sys . path [ 0 ] != program_path : sys . path . insert ( 0 , program_path ) self . _replace_sysargs ( )
def init_package ( self , run_object ) : self . profile = self . profile_package self . _run_object , _ , self . _run_args = run_object . partition ( ' ' ) self . _object_name = '%s (package)' % self . _run_object self . _replace_sysargs ( )
def init_function ( self , run_object ) : self . profile = self . profile_function self . _run_object , self . _run_args , self . _run_kwargs = run_object filename = inspect . getsourcefile ( self . _run_object ) self . _object_name = '%s @ %s (function)' % ( self . _run_object . __name__ , filename )
def _replace_sysargs ( self ) : sys . argv [ : ] = [ self . _run_object ] if self . _run_args : sys . argv += self . _run_args . split ( )
def _fill_sample_count ( self , node ) : node [ 'sampleCount' ] += sum ( self . _fill_sample_count ( child ) for child in node [ 'children' ] ) return node [ 'sampleCount' ]
def _format_tree ( self , node , total_samples ) : funcname , filename , _ = node [ 'stack' ] sample_percent = self . _get_percentage ( node [ 'sampleCount' ] , total_samples ) color_hash = base_profiler . hash_name ( '%s @ %s' % ( funcname , filename ) ) return { 'stack' : node [ 'stack' ] , 'children' : [ self . _format_tree ( child , total_samples ) for child in node [ 'children' ] ] , 'sampleCount' : node [ 'sampleCount' ] , 'samplePercentage' : sample_percent , 'colorHash' : color_hash }
def call_tree ( self ) : call_tree = { 'stack' : 'base' , 'sampleCount' : 0 , 'children' : [ ] } for stack , sample_count in self . _stats . items ( ) : self . _insert_stack ( reversed ( stack ) , sample_count , call_tree ) self . _fill_sample_count ( call_tree ) if not call_tree [ 'children' ] : return { } return self . _format_tree ( call_tree [ 'children' ] [ 0 ] , call_tree [ 'sampleCount' ] )
def _profile_package ( self ) : with _StatProfiler ( ) as prof : prof . base_frame = inspect . currentframe ( ) try : runpy . run_path ( self . _run_object , run_name = '__main__' ) except SystemExit : pass call_tree = prof . call_tree return { 'objectName' : self . _object_name , 'sampleInterval' : _SAMPLE_INTERVAL , 'runTime' : prof . run_time , 'callStats' : call_tree , 'totalSamples' : call_tree . get ( 'sampleCount' , 0 ) , 'timestamp' : int ( time . time ( ) ) }
def _profile_module ( self ) : with open ( self . _run_object , 'rb' ) as srcfile , _StatProfiler ( ) as prof : code = compile ( srcfile . read ( ) , self . _run_object , 'exec' ) prof . base_frame = inspect . currentframe ( ) try : exec ( code , self . _globs , None ) except SystemExit : pass call_tree = prof . call_tree return { 'objectName' : self . _object_name , 'sampleInterval' : _SAMPLE_INTERVAL , 'runTime' : prof . run_time , 'callStats' : call_tree , 'totalSamples' : call_tree . get ( 'sampleCount' , 0 ) , 'timestamp' : int ( time . time ( ) ) }
def profile_function ( self ) : with _StatProfiler ( ) as prof : result = self . _run_object ( * self . _run_args , * * self . _run_kwargs ) call_tree = prof . call_tree return { 'objectName' : self . _object_name , 'sampleInterval' : _SAMPLE_INTERVAL , 'runTime' : prof . run_time , 'callStats' : call_tree , 'totalSamples' : call_tree . get ( 'sampleCount' , 0 ) , 'result' : result , 'timestamp' : int ( time . time ( ) ) }
def _transform_stats ( prof ) : records = [ ] for info , params in prof . stats . items ( ) : filename , lineno , funcname = info cum_calls , num_calls , time_per_call , cum_time , _ = params if prof . total_tt == 0 : percentage = 0 else : percentage = round ( 100 * ( cum_time / prof . total_tt ) , 4 ) cum_time = round ( cum_time , 4 ) func_name = '%s @ %s' % ( funcname , filename ) color_hash = base_profiler . hash_name ( func_name ) records . append ( ( filename , lineno , funcname , cum_time , percentage , num_calls , cum_calls , time_per_call , filename , color_hash ) ) return sorted ( records , key = operator . itemgetter ( 4 ) , reverse = True )
def _profile_package ( self ) : prof = cProfile . Profile ( ) prof . enable ( ) try : runpy . run_path ( self . _run_object , run_name = '__main__' ) except SystemExit : pass prof . disable ( ) prof_stats = pstats . Stats ( prof ) prof_stats . calc_callees ( ) return { 'objectName' : self . _object_name , 'callStats' : self . _transform_stats ( prof_stats ) , 'totalTime' : prof_stats . total_tt , 'primitiveCalls' : prof_stats . prim_calls , 'totalCalls' : prof_stats . total_calls , 'timestamp' : int ( time . time ( ) ) }
def _profile_module ( self ) : prof = cProfile . Profile ( ) try : with open ( self . _run_object , 'rb' ) as srcfile : code = compile ( srcfile . read ( ) , self . _run_object , 'exec' ) prof . runctx ( code , self . _globs , None ) except SystemExit : pass prof_stats = pstats . Stats ( prof ) prof_stats . calc_callees ( ) return { 'objectName' : self . _object_name , 'callStats' : self . _transform_stats ( prof_stats ) , 'totalTime' : prof_stats . total_tt , 'primitiveCalls' : prof_stats . prim_calls , 'totalCalls' : prof_stats . total_calls , 'timestamp' : int ( time . time ( ) ) }
def profile_function ( self ) : prof = cProfile . Profile ( ) prof . enable ( ) result = self . _run_object ( * self . _run_args , * * self . _run_kwargs ) prof . disable ( ) prof_stats = pstats . Stats ( prof ) prof_stats . calc_callees ( ) return { 'objectName' : self . _object_name , 'callStats' : self . _transform_stats ( prof_stats ) , 'totalTime' : prof_stats . total_tt , 'primitiveCalls' : prof_stats . prim_calls , 'totalCalls' : prof_stats . total_calls , 'result' : result , 'timestamp' : int ( time . time ( ) ) }
def show_guestbook ( ) : cursor = flask . g . db . execute ( 'SELECT name, message FROM entry ORDER BY id DESC;' ) entries = [ { 'name' : row [ 0 ] , 'message' : row [ 1 ] } for row in cursor . fetchall ( ) ] return jinja2 . Template ( LAYOUT ) . render ( entries = entries )
def add_entry ( ) : name , msg = flask . request . form [ 'name' ] , flask . request . form [ 'message' ] flask . g . db . execute ( 'INSERT INTO entry (name, message) VALUES (?, ?)' , ( name , msg ) ) flask . g . db . commit ( ) return flask . redirect ( '/' )
def _handle_root ( ) : res_filename = os . path . join ( os . path . dirname ( __file__ ) , _PROFILE_HTML ) with io . open ( res_filename , 'rb' ) as res_file : content = res_file . read ( ) return content , 'text/html'
def _handle_other ( self ) : res_filename = os . path . join ( os . path . dirname ( __file__ ) , _STATIC_DIR , self . path [ 1 : ] ) with io . open ( res_filename , 'rb' ) as res_file : content = res_file . read ( ) _ , extension = os . path . splitext ( self . path ) return content , 'text/%s' % extension [ 1 : ]
def do_GET ( self ) : handler = self . uri_map . get ( self . path ) or self . _handle_other content , content_type = handler ( ) compressed_content = gzip . compress ( content ) self . _send_response ( 200 , headers = ( ( 'Content-type' , '%s; charset=utf-8' % content_type ) , ( 'Content-Encoding' , 'gzip' ) , ( 'Content-Length' , len ( compressed_content ) ) ) ) self . wfile . write ( compressed_content )
def do_POST ( self ) : post_data = self . rfile . read ( int ( self . headers [ 'Content-Length' ] ) ) json_data = gzip . decompress ( post_data ) self . _profile_json . update ( json . loads ( json_data . decode ( 'utf-8' ) ) ) self . _send_response ( 200 , headers = ( ( 'Content-type' , '%s; charset=utf-8' % 'text/json' ) , ( 'Content-Encoding' , 'gzip' ) , ( 'Content-Length' , len ( post_data ) ) ) )
def _send_response ( self , http_code , message = None , headers = None ) : self . send_response ( http_code , message ) if headers : for header in headers : self . send_header ( * header ) self . end_headers ( )
def check_standard_dir ( module_path ) : if 'site-packages' in module_path : return True for stdlib_path in _STDLIB_PATHS : if fnmatch . fnmatchcase ( module_path , stdlib_path + '*' ) : return True return False
def record_line ( self , frame , event , arg ) : if event == 'line' : if self . prev_timestamp : runtime = time . time ( ) - self . prev_timestamp self . lines . append ( [ self . prev_path , self . prev_lineno , runtime ] ) self . prev_lineno = frame . f_lineno self . prev_path = frame . f_code . co_filename self . prev_timestamp = time . time ( ) return self . record_line
def lines_without_stdlib ( self ) : prev_line = None current_module_path = inspect . getabsfile ( inspect . currentframe ( ) ) for module_path , lineno , runtime in self . lines : module_abspath = os . path . abspath ( module_path ) if not prev_line : prev_line = [ module_abspath , lineno , runtime ] else : if ( not check_standard_dir ( module_path ) and module_abspath != current_module_path ) : yield prev_line prev_line = [ module_abspath , lineno , runtime ] else : prev_line [ 2 ] += runtime yield prev_line
def fill_heatmap ( self ) : for module_path , lineno , runtime in self . lines_without_stdlib : self . _execution_count [ module_path ] [ lineno ] += 1 self . _heatmap [ module_path ] [ lineno ] += runtime
def _skip_lines ( src_code , skip_map ) : if not skip_map : return [ [ 'line' , j + 1 , l ] for j , l in enumerate ( src_code ) ] code_with_skips , i = [ ] , 0 for line , length in skip_map : code_with_skips . extend ( [ 'line' , i + j + 1 , l ] for j , l in enumerate ( src_code [ i : line ] ) ) if ( code_with_skips and code_with_skips [ - 1 ] [ 0 ] == 'skip' ) : code_with_skips [ - 1 ] [ 1 ] += length else : code_with_skips . append ( [ 'skip' , length ] ) i = line + length code_with_skips . extend ( [ 'line' , i + j + 1 , l ] for j , l in enumerate ( src_code [ i : ] ) ) return code_with_skips
def _profile_package ( self ) : with _CodeHeatmapCalculator ( ) as prof : try : runpy . run_path ( self . _run_object , run_name = '__main__' ) except SystemExit : pass heatmaps = [ ] for filename , heatmap in prof . heatmap . items ( ) : if os . path . isfile ( filename ) : heatmaps . append ( self . _format_heatmap ( filename , heatmap , prof . execution_count [ filename ] ) ) run_time = sum ( heatmap [ 'runTime' ] for heatmap in heatmaps ) return { 'objectName' : self . _run_object , 'runTime' : run_time , 'heatmaps' : heatmaps }
def _format_heatmap ( self , filename , heatmap , execution_count ) : with open ( filename ) as src_file : file_source = src_file . read ( ) . split ( '\n' ) skip_map = self . _calc_skips ( heatmap , len ( file_source ) ) run_time = sum ( time for time in heatmap . values ( ) ) return { 'name' : filename , 'heatmap' : heatmap , 'executionCount' : execution_count , 'srcCode' : self . _skip_lines ( file_source , skip_map ) , 'runTime' : run_time }
def _profile_module ( self ) : with open ( self . _run_object , 'r' ) as srcfile : src_code = srcfile . read ( ) code = compile ( src_code , self . _run_object , 'exec' ) try : with _CodeHeatmapCalculator ( ) as prof : exec ( code , self . _globs , None ) except SystemExit : pass heatmaps = [ ] for filename , heatmap in prof . heatmap . items ( ) : if os . path . isfile ( filename ) : heatmaps . append ( self . _format_heatmap ( filename , heatmap , prof . execution_count [ filename ] ) ) run_time = sum ( heatmap [ 'runTime' ] for heatmap in heatmaps ) return { 'objectName' : self . _run_object , 'runTime' : run_time , 'heatmaps' : heatmaps }
def profile_function ( self ) : with _CodeHeatmapCalculator ( ) as prof : result = self . _run_object ( * self . _run_args , * * self . _run_kwargs ) code_lines , start_line = inspect . getsourcelines ( self . _run_object ) source_lines = [ ] for line in code_lines : source_lines . append ( ( 'line' , start_line , line ) ) start_line += 1 filename = os . path . abspath ( inspect . getsourcefile ( self . _run_object ) ) heatmap = prof . heatmap [ filename ] run_time = sum ( time for time in heatmap . values ( ) ) return { 'objectName' : self . _object_name , 'runTime' : run_time , 'result' : result , 'timestamp' : int ( time . time ( ) ) , 'heatmaps' : [ { 'name' : self . _object_name , 'heatmap' : heatmap , 'executionCount' : prof . execution_count [ filename ] , 'srcCode' : source_lines , 'runTime' : run_time } ] }
def _count_vocab ( self , analyzed_docs ) : vocabulary = self . vocabulary_ j_indices = _make_int_array ( ) indptr = _make_int_array ( ) indptr . append ( 0 ) for doc in analyzed_docs : for feature in doc : try : j_indices . append ( vocabulary [ feature ] ) except KeyError : continue indptr . append ( len ( j_indices ) ) j_indices = frombuffer_empty ( j_indices , dtype = np . intc ) indptr = np . frombuffer ( indptr , dtype = np . intc ) values = np . ones ( len ( j_indices ) ) X = sp . csr_matrix ( ( values , j_indices , indptr ) , shape = ( len ( indptr ) - 1 , len ( vocabulary ) ) , dtype = self . dtype ) X . sum_duplicates ( ) if self . binary : X . data . fill ( 1 ) return X
def to_scikit ( self ) : scaler = StandardScaler ( with_mean = self . with_mean , with_std = self . with_std , copy = self . copy ) scaler . __dict__ = self . __dict__ return scaler
def _fit ( self , Z , parameter_iterable ) : self . scorer_ = check_scoring ( self . estimator , scoring = self . scoring ) cv = self . cv cv = _check_cv ( cv , Z ) if self . verbose > 0 : if isinstance ( parameter_iterable , Sized ) : n_candidates = len ( parameter_iterable ) print ( "Fitting {0} folds for each of {1} candidates, totalling" " {2} fits" . format ( len ( cv ) , n_candidates , n_candidates * len ( cv ) ) ) base_estimator = clone ( self . estimator ) pre_dispatch = self . pre_dispatch out = Parallel ( n_jobs = self . n_jobs , verbose = self . verbose , pre_dispatch = pre_dispatch , backend = "threading" ) ( delayed ( _fit_and_score ) ( clone ( base_estimator ) , Z , self . scorer_ , train , test , self . verbose , parameters , self . fit_params , return_parameters = True , error_score = self . error_score ) for parameters in parameter_iterable for train , test in cv ) n_fits = len ( out ) n_folds = len ( cv ) scores = list ( ) grid_scores = list ( ) for grid_start in range ( 0 , n_fits , n_folds ) : n_test_samples = 0 score = 0 all_scores = [ ] for this_score , this_n_test_samples , _ , parameters in out [ grid_start : grid_start + n_folds ] : all_scores . append ( this_score ) if self . iid : this_score *= this_n_test_samples n_test_samples += this_n_test_samples score += this_score if self . iid : score /= float ( n_test_samples ) else : score /= float ( n_folds ) scores . append ( ( score , parameters ) ) grid_scores . append ( _CVScoreTuple ( parameters , score , np . array ( all_scores ) ) ) self . grid_scores_ = grid_scores best = sorted ( grid_scores , key = lambda x : x . mean_validation_score , reverse = True ) [ 0 ] self . best_params_ = best . parameters self . best_score_ = best . mean_validation_score if self . refit : best_estimator = clone ( base_estimator ) . set_params ( * * best . parameters ) best_estimator . fit ( Z , * * self . fit_params ) self . best_estimator_ = best_estimator return self
def _score ( estimator , Z_test , scorer ) : score = scorer ( estimator , Z_test ) if not isinstance ( score , numbers . Number ) : raise ValueError ( "scoring must return a number, got %s (%s) instead." % ( str ( score ) , type ( score ) ) ) return score
def _block_collection ( iterator , dtype , bsize = - 1 ) : i = 0 accumulated = [ ] for a in iterator : if ( bsize > 0 ) and ( i >= bsize ) : yield _pack_accumulated ( accumulated , dtype ) accumulated = [ ] i = 0 accumulated . append ( a ) i += 1 if i > 0 : yield _pack_accumulated ( accumulated , dtype )
def _block_tuple ( iterator , dtypes , bsize = - 1 ) : i = 0 blocked_tuple = None for tuple_i in iterator : if blocked_tuple is None : blocked_tuple = tuple ( [ ] for _ in range ( len ( tuple_i ) ) ) if ( bsize > 0 ) and ( i >= bsize ) : yield tuple ( _pack_accumulated ( x , dtype ) for x , dtype in zip ( blocked_tuple , dtypes ) ) blocked_tuple = tuple ( [ ] for _ in range ( len ( tuple_i ) ) ) i = 0 for x_j , x in zip ( tuple_i , blocked_tuple ) : x . append ( x_j ) i += 1 if i > 0 : yield tuple ( _pack_accumulated ( x , dtype ) for x , dtype in zip ( blocked_tuple , dtypes ) )
def shape ( self ) : first = self . first ( ) . shape shape = self . _rdd . map ( lambda x : x . shape [ 0 ] ) . sum ( ) return ( shape , ) + first [ 1 : ]
def toarray ( self ) : rdd = self . _rdd . map ( lambda x : x . toarray ( ) ) return np . concatenate ( rdd . collect ( ) )
def convert ( self , txn ) : ofxid = self . mk_ofxid ( txn . id ) metadata = { } posting_metadata = { "ofxid" : ofxid } if isinstance ( txn , OfxTransaction ) : posting = Posting ( self . name , Amount ( txn . amount , self . currency ) , metadata = posting_metadata ) return Transaction ( date = txn . date , payee = self . format_payee ( txn ) , postings = [ posting , posting . clone_inverted ( self . mk_dynamic_account ( self . format_payee ( txn ) , exclude = self . name ) ) ] ) elif isinstance ( txn , InvestmentTransaction ) : acct1 = self . name acct2 = self . name posting1 = None posting2 = None security = self . maybe_get_ticker ( txn . security ) if isinstance ( txn . type , str ) : if re . match ( '^(buy|sell)' , txn . type ) : acct2 = self . unknownaccount or 'Assets:Unknown' elif txn . type == 'transfer' : acct2 = 'Transfer' elif txn . type == 'reinvest' : acct2 = 'Income:Interest' elif txn . type == 'income' and txn . income_type == 'DIV' : metadata [ 'dividend_from' ] = security acct2 = 'Income:Dividends' posting1 = Posting ( acct1 , Amount ( txn . total , self . currency ) , metadata = posting_metadata ) posting2 = posting1 . clone_inverted ( acct2 ) else : pass else : if ( txn . type in [ 0 , 1 , 3 , 4 ] ) : acct2 = self . unknownaccount or 'Assets:Unknown' elif ( txn . type == 2 ) : acct2 = 'Income:Interest' else : pass aux_date = None if txn . settleDate is not None and txn . settleDate != txn . tradeDate : aux_date = txn . settleDate if posting1 is None and posting2 is None : posting1 = Posting ( acct1 , Amount ( txn . units , security , unlimited = True ) , unit_price = Amount ( txn . unit_price , self . currency , unlimited = True ) , metadata = posting_metadata ) posting2 = Posting ( acct2 , Amount ( txn . units * txn . unit_price , self . currency , reverse = True ) ) else : pass return Transaction ( date = txn . tradeDate , aux_date = aux_date , payee = self . format_payee ( txn ) , metadata = metadata , postings = [ posting1 , posting2 ] )
def compatibility ( session , install ) : session . install ( '-e' , '.[dev]' ) session . install ( install ) _run_tests ( session )
def text_width ( self , text : str ) -> float : width , _ = self . _font . getsize ( text ) return width
def text_width ( self , text : str ) -> float : width = 0 for index , c in enumerate ( text ) : width += self . _char_to_width . get ( c , self . _default_character_width ) width -= self . _pair_to_kern . get ( text [ index : index + 2 ] , 0 ) return width
def default ( cls ) -> 'PrecalculatedTextMeasurer' : if cls . _default_cache is not None : return cls . _default_cache if pkg_resources . resource_exists ( __name__ , 'default-widths.json.xz' ) : import lzma with pkg_resources . resource_stream ( __name__ , 'default-widths.json.xz' ) as f : with lzma . open ( f , "rt" ) as g : cls . _default_cache = PrecalculatedTextMeasurer . from_json ( cast ( TextIO , g ) ) return cls . _default_cache elif pkg_resources . resource_exists ( __name__ , 'default-widths.json' ) : with pkg_resources . resource_stream ( __name__ , 'default-widths.json' ) as f : cls . _default_cache = PrecalculatedTextMeasurer . from_json ( io . TextIOWrapper ( f , encoding = 'utf-8' ) ) return cls . _default_cache else : raise ValueError ( 'could not load default-widths.json' )
def generate_supported_characters ( deja_vu_sans_path : str ) -> Iterable [ str ] : font = ttLib . TTFont ( deja_vu_sans_path ) for cmap in font [ 'cmap' ] . tables : if cmap . isUnicode ( ) : for code in cmap . cmap : yield chr ( code )
def write_json ( f : TextIO , deja_vu_sans_path : str , measurer : text_measurer . TextMeasurer , encodings : Iterable [ str ] ) -> None : supported_characters = list ( generate_supported_characters ( deja_vu_sans_path ) ) kerning_characters = '' . join ( generate_encodeable_characters ( supported_characters , encodings ) ) char_to_length = calculate_character_to_length_mapping ( measurer , supported_characters ) pair_to_kerning = calculate_pair_to_kern_mapping ( measurer , char_to_length , kerning_characters ) json . dump ( { 'mean-character-length' : statistics . mean ( char_to_length . values ( ) ) , 'character-lengths' : char_to_length , 'kerning-characters' : kerning_characters , 'kerning-pairs' : pair_to_kerning } , f , sort_keys = True , indent = 1 )
def convolve_gaussian_2d ( image , gaussian_kernel_1d ) : result = scipy . ndimage . filters . correlate1d ( image , gaussian_kernel_1d , axis = 0 ) result = scipy . ndimage . filters . correlate1d ( result , gaussian_kernel_1d , axis = 1 ) return result
def get_gaussian_kernel ( gaussian_kernel_width = 11 , gaussian_kernel_sigma = 1.5 ) : gaussian_kernel_1d = numpy . ndarray ( ( gaussian_kernel_width ) ) norm_mu = int ( gaussian_kernel_width / 2 ) for i in range ( gaussian_kernel_width ) : gaussian_kernel_1d [ i ] = ( exp ( - ( ( ( i - norm_mu ) ** 2 ) ) / ( 2 * ( gaussian_kernel_sigma ** 2 ) ) ) ) return gaussian_kernel_1d / numpy . sum ( gaussian_kernel_1d )
def main ( ) : description = '\n' . join ( [ 'Compares an image with a list of images using the SSIM metric.' , '  Example:' , '    pyssim test-images/test1-1.png "test-images/*"' ] ) parser = argparse . ArgumentParser ( prog = 'pyssim' , formatter_class = argparse . RawTextHelpFormatter , description = description ) parser . add_argument ( '--cw' , help = 'compute the complex wavelet SSIM' , action = 'store_true' ) parser . add_argument ( 'base_image' , metavar = 'image1.png' , type = argparse . FileType ( 'r' ) ) parser . add_argument ( 'comparison_images' , metavar = 'image path with* or image2.png' ) parser . add_argument ( '--width' , type = int , default = None , help = 'scales the image before computing SSIM' ) parser . add_argument ( '--height' , type = int , default = None , help = 'scales the image before computing SSIM' ) args = parser . parse_args ( ) if args . width and args . height : size = ( args . width , args . height ) else : size = None if not args . cw : gaussian_kernel_sigma = 1.5 gaussian_kernel_width = 11 gaussian_kernel_1d = get_gaussian_kernel ( gaussian_kernel_width , gaussian_kernel_sigma ) comparison_images = glob . glob ( args . comparison_images ) is_a_single_image = len ( comparison_images ) == 1 for comparison_image in comparison_images : if args . cw : ssim = SSIM ( args . base_image . name , size = size ) ssim_value = ssim . cw_ssim_value ( comparison_image ) else : ssim = SSIM ( args . base_image . name , gaussian_kernel_1d , size = size ) ssim_value = ssim . ssim_value ( comparison_image ) if is_a_single_image : sys . stdout . write ( '%.7g' % ssim_value ) else : sys . stdout . write ( '%s - %s: %.7g' % ( args . base_image . name , comparison_image , ssim_value ) ) sys . stdout . write ( '\n' )
def destroy ( self ) : if self . __conf . autoTick : self . __destroying = True else : self . _doDestroy ( )
def getStatus ( self ) : status = { } status [ 'version' ] = VERSION status [ 'revision' ] = REVISION status [ 'self' ] = self . __selfNode status [ 'state' ] = self . __raftState status [ 'leader' ] = self . __raftLeader status [ 'partner_nodes_count' ] = len ( self . __otherNodes ) for node in self . __otherNodes : status [ 'partner_node_status_server_' + node . id ] = 2 if node in self . __connectedNodes else 0 status [ 'readonly_nodes_count' ] = len ( self . __readonlyNodes ) for node in self . __readonlyNodes : status [ 'readonly_node_status_server_' + node . id ] = 2 if node in self . __connectedNodes else 0 status [ 'log_len' ] = len ( self . __raftLog ) status [ 'last_applied' ] = self . __raftLastApplied status [ 'commit_idx' ] = self . __raftCommitIndex status [ 'raft_term' ] = self . __raftCurrentTerm status [ 'next_node_idx_count' ] = len ( self . __raftNextIndex ) for node , idx in iteritems ( self . __raftNextIndex ) : status [ 'next_node_idx_server_' + node . id ] = idx status [ 'match_idx_count' ] = len ( self . __raftMatchIndex ) for node , idx in iteritems ( self . __raftMatchIndex ) : status [ 'match_idx_server_' + node . id ] = idx status [ 'leader_commit_idx' ] = self . __leaderCommitIndex status [ 'uptime' ] = int ( time . time ( ) - self . __startTime ) status [ 'self_code_version' ] = self . __selfCodeVersion status [ 'enabled_code_version' ] = self . __enabledCodeVersion return status
def printStatus ( self ) : status = self . getStatus ( ) for k , v in iteritems ( status ) : logging . info ( '%s: %s' % ( str ( k ) , str ( v ) ) )
def check ( func ) : def wrapped ( * args , * * kwargs ) : check_name = func . __name__ arg_name = None if args : arg_name = args [ 0 ] try : if arg_name : logger . debug ( "Checking '%s' for '%s'" , check_name , arg_name ) else : logger . debug ( "Checking '%s'" , check_name ) response = func ( * args , * * kwargs ) except Exception as e : message = str ( e ) response = { "ok" : False , "error" : message , "stacktrace" : traceback . format_exc ( ) , } if arg_name : response = { arg_name : response } logger . exception ( "Error calling '%s' for '%s': %s" , check_name , arg_name , message ) else : logger . exception ( "Error calling '%s': %s" , check_name , message ) return response return wrapped
def _str_to_list ( s ) : _list = s . split ( "," ) return list ( map ( lambda i : i . lstrip ( ) , _list ) )
def cli_parse ( file_path , sa , nameservers , dns_timeout , parallel = False ) : try : file_results = parse_report_file ( file_path , nameservers = nameservers , dns_timeout = dns_timeout , strip_attachment_payloads = sa , parallel = parallel ) except ParserError as error : return error , file_path finally : global counter with counter . get_lock ( ) : counter . value += 1 return file_results , file_path
def _publish ( self , subject , reply , payload , payload_size ) : if subject == "" : raise ErrBadSubject payload_size_bytes = ( "%d" % payload_size ) . encode ( ) pub_cmd = b'' . join ( [ PUB_OP , _SPC_ , subject . encode ( ) , _SPC_ , reply , _SPC_ , payload_size_bytes , _CRLF_ , payload , _CRLF_ ] ) self . stats [ 'out_msgs' ] += 1 self . stats [ 'out_bytes' ] += payload_size yield from self . _send_command ( pub_cmd ) if self . _flush_queue . empty ( ) : yield from self . _flush_pending ( )
def _process_pong ( self ) : if len ( self . _pongs ) > 0 : future = self . _pongs . pop ( 0 ) future . set_result ( True ) self . _pongs_received += 1 self . _pings_outstanding -= 1
def _process_msg ( self , sid , subject , reply , data ) : payload_size = len ( data ) self . stats [ 'in_msgs' ] += 1 self . stats [ 'in_bytes' ] += payload_size sub = self . _subs . get ( sid ) if sub is None : return sub . received += 1 if sub . max_msgs > 0 and sub . received >= sub . max_msgs : self . _subs . pop ( sid , None ) msg = self . _build_message ( subject , reply , data ) if sub . future is not None : if sub . future . cancelled ( ) : return sub . future . set_result ( msg ) return try : sub . pending_size += payload_size if sub . pending_size >= sub . pending_bytes_limit : sub . pending_size -= payload_size if self . _error_cb is not None : yield from self . _error_cb ( ErrSlowConsumer ( subject = subject , sid = sid ) ) return sub . pending_queue . put_nowait ( msg ) except asyncio . QueueFull : if self . _error_cb is not None : yield from self . _error_cb ( ErrSlowConsumer ( subject = subject , sid = sid ) )
def _load_features_from_array ( self , features ) : self . feature_images = np . load ( features ) self . feature_names = range ( self . feature_images . shape [ 1 ] )
def _dot_product ( self , imgs_to_decode ) : return np . dot ( imgs_to_decode . T , self . feature_images ) . T
def feature_selection ( feat_select , X , y ) : if re . match ( '.*-best' , feat_select ) is not None : n = int ( feat_select . split ( '-' ) [ 0 ] ) selector = SelectKBest ( k = n ) import warnings with warnings . catch_warnings ( ) : warnings . simplefilter ( 'ignore' , category = UserWarning ) features_selected = np . where ( selector . fit ( X , y ) . get_support ( ) is True ) [ 0 ] elif re . match ( '.*-randombest' , feat_select ) is not None : n = int ( feat_select . split ( '-' ) [ 0 ] ) from random import shuffle features = range ( 0 , X . shape [ 1 ] ) shuffle ( features ) features_selected = features [ : n ] return features_selected
def fit ( self , X , y , cv = None , class_weight = 'auto' ) : self . X = X self . y = y self . set_class_weight ( class_weight = class_weight , y = y ) self . clf = self . clf . fit ( X , y ) return self . clf
def set_class_weight ( self , class_weight = 'auto' , y = None ) : if class_weight is None : cw = None try : self . clf . set_params ( class_weight = cw ) except ValueError : pass elif class_weight == 'auto' : c = np . bincount ( y ) ii = np . nonzero ( c ) [ 0 ] c = c / float ( c . sum ( ) ) cw = dict ( zip ( ii [ : : - 1 ] , c [ ii ] ) ) try : self . clf . set_params ( class_weight = cw ) except ValueError : import warnings warnings . warn ( "Tried to set class_weight, but failed. The classifier " "probably doesn't support it" )
def cross_val_fit ( self , X , y , cross_val = '4-Fold' , scoring = 'accuracy' , feat_select = None , class_weight = 'auto' ) : from sklearn import cross_validation self . X = X self . y = y self . set_class_weight ( class_weight = class_weight , y = y ) if isinstance ( cross_val , string_types ) : if re . match ( '.*-Fold' , cross_val ) is not None : n = int ( cross_val . split ( '-' ) [ 0 ] ) self . cver = cross_validation . StratifiedKFold ( self . y , n ) else : raise Exception ( 'Unrecognized cross validation method' ) else : self . cver = cross_val if feat_select is not None : self . features_selected = [ ] from sklearn . grid_search import GridSearchCV if isinstance ( self . clf , GridSearchCV ) : import warnings if feat_select is not None : warnings . warn ( "Cross-validated feature selection not supported with " "GridSearchCV" ) self . clf . set_params ( cv = self . cver , scoring = scoring ) with warnings . catch_warnings ( ) : warnings . simplefilter ( 'ignore' , category = UserWarning ) self . clf = self . clf . fit ( X , y ) self . cvs = self . clf . best_score_ else : self . cvs = self . feat_select_cvs ( feat_select = feat_select , scoring = scoring ) if feat_select is not None : fs = feature_selection ( feat_select , X , y ) self . features_selected . append ( fs ) X = X [ : , fs ] self . clf . fit ( X , y ) return self . cvs . mean ( )
def fit_dataset ( self , dataset , y , features = None , feature_type = 'features' ) : if feature_type == 'features' : X = np . rot90 ( dataset . feature_table . data . toarray ( ) ) elif feature_type == 'voxels' : X = np . rot90 ( dataset . image_table . data . toarray ( ) ) self . sk_classifier . fit ( X , y )
def _get_top_words ( model , feature_names , n_top_words = 40 ) : topic_words = [ ] for topic in model . components_ : top_words = [ feature_names [ i ] for i in topic . argsort ( ) [ : - n_top_words - 1 : - 1 ] ] topic_words += [ top_words ] return topic_words
def pearson ( x , y ) : data = np . vstack ( ( x , y ) ) ms = data . mean ( axis = 1 ) [ ( slice ( None , None , None ) , None ) ] datam = data - ms datass = np . sqrt ( np . sum ( datam ** 2 , axis = 1 ) ) temp = np . dot ( datam [ 1 : ] , datam [ 0 ] . T ) rs = temp / ( datass [ 1 : ] * datass [ 0 ] ) return rs
def load ( cls , filename ) : try : dataset = pickle . load ( open ( filename , 'rb' ) ) except UnicodeDecodeError : dataset = pickle . load ( open ( filename , 'rb' ) , encoding = 'latin' ) if hasattr ( dataset , 'feature_table' ) : dataset . feature_table . _csr_to_sdf ( ) return dataset
def save ( self , filename ) : if hasattr ( self , 'feature_table' ) : self . feature_table . _sdf_to_csr ( ) pickle . dump ( self , open ( filename , 'wb' ) , - 1 ) if hasattr ( self , 'feature_table' ) : self . feature_table . _csr_to_sdf ( )
def get_ids_by_expression ( self , expression , threshold = 0.001 , func = np . sum ) : lexer = lp . Lexer ( ) lexer . build ( ) parser = lp . Parser ( lexer , self . dataset , threshold = threshold , func = func ) parser . build ( ) return parser . parse ( expression ) . keys ( ) . values
def _sdf_to_csr ( self ) : data = self . data . to_dense ( ) self . data = { 'columns' : list ( data . columns ) , 'index' : list ( data . index ) , 'values' : sparse . csr_matrix ( data . values ) }
def xyz_to_mat ( foci , xyz_dims = None , mat_dims = None ) : foci = np . hstack ( ( foci , np . ones ( ( foci . shape [ 0 ] , 1 ) ) ) ) mat = np . array ( [ [ - 0.5 , 0 , 0 , 45 ] , [ 0 , 0.5 , 0 , 63 ] , [ 0 , 0 , 0.5 , 36 ] ] ) . T result = np . dot ( foci , mat ) [ : , : : - 1 ] return np . round_ ( result ) . astype ( int )
def save_img ( data , filename , masker , header = None ) : if not header : header = masker . get_header ( ) header . set_data_dtype ( data . dtype ) header [ 'cal_max' ] = data . max ( ) header [ 'cal_min' ] = data . min ( ) img = nifti1 . Nifti1Image ( masker . unmask ( data ) , None , header ) img . to_filename ( filename )
def dict_to_object ( item , object_name ) : fields = item . keys ( ) values = item . values ( ) return json . loads ( json . dumps ( item ) , object_hook = lambda d : namedtuple ( object_name , fields ) ( * values ) )
async def get_bearer_info ( self ) : if self . client_id is None : raise SpotifyException ( _GET_BEARER_ERR % 'client_id' ) elif self . client_secret is None : raise SpotifyException ( _GET_BEARER_ERR % 'client_secret' ) token = b64encode ( ':' . join ( ( self . client_id , self . client_secret ) ) . encode ( ) ) kwargs = { 'url' : 'https://accounts.spotify.com/api/token' , 'data' : { 'grant_type' : 'client_credentials' } , 'headers' : { 'Authorization' : 'Basic ' + token . decode ( ) } } async with self . _session . post ( * * kwargs ) as resp : return json . loads ( await resp . text ( encoding = 'utf-8' ) )
def assert_hasattr ( attr : str , msg : str , tp : BaseException = SpotifyException ) -> Callable : def decorator ( func : Callable ) -> Callable : @ functools . wraps ( func ) def decorated ( self , * args , * * kwargs ) : if not hasattr ( self , attr ) : raise tp ( msg ) return func ( self , * args , * * kwargs ) if inspect . iscoroutinefunction ( func ) : @ functools . wraps ( func ) async def decorated ( * args , * * kwargs ) : return await decorated ( * args , * * kwargs ) return decorated return decorator
def from_client ( cls , client , * args , * * kwargs ) : return cls ( client . http . client_id , * args , * * kwargs )
def url_ ( client_id : str , redirect_uri : str , * , scope : str = None , state : str = None , secure : bool = True ) -> str : attrs = { 'client_id' : client_id , 'redirect_uri' : quote ( redirect_uri ) } if scope is not None : attrs [ 'scope' ] = quote ( scope ) if state is not None : attrs [ 'state' ] = state parameters = '&' . join ( '{0}={1}' . format ( * item ) for item in attrs . items ( ) ) return OAuth2 . _BASE . format ( parameters = parameters )
def attrs ( self ) : data = { 'client_id' : self . client_id , 'redirect_uri' : quote ( self . redirect_uri ) , } if self . scope is not None : data [ 'scope' ] = quote ( self . scope ) if self . state is not None : data [ 'state' ] = self . state return data
def parameters ( self ) -> str : return '&' . join ( '{0}={1}' . format ( * item ) for item in self . attrs . items ( ) )
async def from_href ( self ) : if not hasattr ( self , 'href' ) : raise TypeError ( 'Spotify object has no `href` attribute, therefore cannot be retrived' ) elif hasattr ( self , 'http' ) : return await self . http . request ( ( 'GET' , self . href ) ) else : cls = type ( self ) try : client = getattr ( self , '_{0}__client' . format ( cls . __name__ ) ) except AttributeError : raise TypeError ( 'Spotify object has no way to access a HTTPClient.' ) else : http = client . http data = await http . request ( ( 'GET' , self . href ) ) return cls ( client , data )
def _update_code_urls ( self ) : to_ignore = [ ".gitignore" , ".keep" ] for root , _ , files in PyFunceble . walk ( PyFunceble . CURRENT_DIRECTORY + PyFunceble . directory_separator + "PyFunceble" + PyFunceble . directory_separator ) : for file in files : if file not in to_ignore and "__pycache__" not in root : if root . endswith ( PyFunceble . directory_separator ) : self . _update_docs ( root + file ) else : self . _update_docs ( root + PyFunceble . directory_separator + file ) for root , _ , files in PyFunceble . walk ( PyFunceble . CURRENT_DIRECTORY + PyFunceble . directory_separator + "tests" + PyFunceble . directory_separator ) : for file in files : if file not in to_ignore and "__pycache__" not in root : if root . endswith ( PyFunceble . directory_separator ) : self . _update_docs ( root + file ) else : self . _update_docs ( root + PyFunceble . directory_separator + file )
def _is_version_greater ( self ) : checked = Version ( True ) . check_versions ( self . current_version [ 0 ] , self . version_yaml ) if checked is not None and not checked : return True return False
def is_dev_version ( cls ) : command = "git branch" command_result = Command ( command ) . execute ( ) for branch in command_result . split ( "\n" ) : if branch . startswith ( "*" ) and "dev" in branch : return True return False
def _does_require_deprecation ( self ) : for index , version_number in enumerate ( self . current_version [ 0 ] [ : 2 ] ) : if version_number > self . version_yaml [ index ] : return True return False
def backup ( self ) : if PyFunceble . CONFIGURATION [ "auto_continue" ] : data_to_backup = { } configuration_counter = PyFunceble . INTERN [ "counter" ] [ "number" ] data_to_backup [ PyFunceble . INTERN [ "file_to_test" ] ] = { "tested" : configuration_counter [ "tested" ] , "up" : configuration_counter [ "up" ] , "down" : configuration_counter [ "down" ] , "invalid" : configuration_counter [ "invalid" ] , } to_save = { } to_save . update ( self . backup_content ) to_save . update ( data_to_backup ) Dict ( to_save ) . to_json ( self . autocontinue_log_file )
def restore ( self ) : if PyFunceble . CONFIGURATION [ "auto_continue" ] and self . backup_content : file_to_restore = PyFunceble . INTERN [ "file_to_test" ] if file_to_restore in self . backup_content : to_initiate = [ "up" , "down" , "invalid" , "tested" ] alternatives = { "up" : "number_of_up" , "down" : "number_of_down" , "invalid" : "number_of_invalid" , "tested" : "number_of_tested" , } for string in to_initiate : try : PyFunceble . INTERN [ "counter" ] [ "number" ] . update ( { string : self . backup_content [ file_to_restore ] [ string ] } ) except KeyError : PyFunceble . INTERN [ "counter" ] [ "number" ] . update ( { string : self . backup_content [ file_to_restore ] [ alternatives [ string ] ] } )
def stay_safe ( ) : random = int ( choice ( str ( int ( time ( ) ) ) ) ) if not CONFIGURATION [ "quiet" ] and random % 3 == 0 : print ( "\n" + Fore . GREEN + Style . BRIGHT + "Thanks for using PyFunceble!" ) print ( Fore . YELLOW + Style . BRIGHT + "Share your experience on " + Fore . CYAN + "Twitter" + Fore . YELLOW + " with " + Fore . CYAN + "#PyFunceble" + Fore . YELLOW + "!" ) print ( Fore . GREEN + Style . BRIGHT + "Have a feedback, an issue or an improvement idea ?" ) print ( Fore . YELLOW + Style . BRIGHT + "Let us know on " + Fore . CYAN + "GitHub" + Fore . YELLOW + "!" )
def _entry_management_url ( self ) : if ( self . url_file and not self . _entry_management_url_download ( self . url_file ) ) : PyFunceble . INTERN [ "file_to_test" ] = self . url_file
def _print_header ( cls ) : if ( not PyFunceble . CONFIGURATION [ "quiet" ] and not PyFunceble . CONFIGURATION [ "header_printed" ] ) : print ( "\n" ) if PyFunceble . CONFIGURATION [ "less" ] : Prints ( None , "Less" ) . header ( ) else : Prints ( None , "Generic" ) . header ( ) PyFunceble . CONFIGURATION [ "header_printed" ] = True
def handle ( self ) : source = "URL" if self . catched . lower ( ) not in PyFunceble . STATUS [ "list" ] [ "invalid" ] : Generate ( self . catched , source ) . status_file ( ) else : Generate ( self . catched , "SYNTAX" ) . status_file ( ) return self . catched
def delete_uneeded ( self ) : structure = self . _get_structure ( ) list_of_key = list ( structure . keys ( ) ) structure = structure [ list_of_key [ 0 ] ] parent_path = list_of_key [ 0 ] if not parent_path . endswith ( PyFunceble . directory_separator ) : parent_path += PyFunceble . directory_separator for root , _ , _ in PyFunceble . walk ( parent_path ) : root = Directory ( root ) . fix_path ( ) if root . replace ( parent_path , "" ) not in structure : PyFunceble . rmtree ( root )
def _load_config_file ( self ) : try : PyFunceble . CONFIGURATION . update ( Dict . from_yaml ( File ( self . path_to_config ) . read ( ) ) ) self . _install_iana_config ( ) self . _install_psl_config ( ) self . _install_directory_structure_file ( ) except FileNotFoundError as exception : if PyFunceble . path . isfile ( self . path_to_default_config ) : File ( self . path_to_default_config ) . copy ( self . path_to_config ) self . _load_config_file ( ) else : raise exception
def _install_iana_config ( cls ) : iana_link = PyFunceble . CONFIGURATION [ "links" ] [ "iana" ] iana_link = Version ( True ) . right_url_from_version ( iana_link ) destination = PyFunceble . CURRENT_DIRECTORY + "iana-domains-db.json" if not Version ( True ) . is_cloned ( ) or not PyFunceble . path . isfile ( destination ) : return Download ( iana_link , destination ) . text ( ) return None
def _install_psl_config ( cls ) : psl_link = PyFunceble . CONFIGURATION [ "links" ] [ "psl" ] psl_link = Version ( True ) . right_url_from_version ( psl_link ) destination = ( PyFunceble . CURRENT_DIRECTORY + PyFunceble . CONFIGURATION [ "outputs" ] [ "default_files" ] [ "public_suffix" ] ) if not Version ( True ) . is_cloned ( ) or not PyFunceble . path . isfile ( destination ) : return Download ( psl_link , destination ) . text ( ) return None
def _install_directory_structure_file ( cls ) : dir_structure_link = PyFunceble . CONFIGURATION [ "links" ] [ "dir_structure" ] dir_structure_link = Version ( True ) . right_url_from_version ( dir_structure_link ) destination = ( PyFunceble . CURRENT_DIRECTORY + PyFunceble . CONFIGURATION [ "outputs" ] [ "default_files" ] [ "dir_structure" ] ) if not Version ( True ) . is_cloned ( ) or not PyFunceble . path . isfile ( destination ) : data = Download ( dir_structure_link , destination , return_data = True ) . text ( ) File ( destination ) . write ( data , overwrite = True ) return True return None
def _merge_values ( self ) : to_remove = [ ] self . new_config = Dict ( Dict ( self . upstream_config ) . merge ( PyFunceble . CONFIGURATION ) ) . remove_key ( to_remove )
def _load ( self ) : if "PYFUNCEBLE_AUTO_CONFIGURATION" not in PyFunceble . environ : while True : response = input ( PyFunceble . Style . BRIGHT + PyFunceble . Fore . RED + "A configuration key is missing.\n" + PyFunceble . Fore . RESET + "Try to merge upstream configuration file into %s ? [y/n] " % ( PyFunceble . Style . BRIGHT + self . path_to_config + PyFunceble . Style . RESET_ALL ) ) if isinstance ( response , str ) : if response . lower ( ) == "y" : self . _merge_values ( ) self . _save ( ) print ( PyFunceble . Style . BRIGHT + PyFunceble . Fore . GREEN + "Done!\n" "Please try again, if it happens again," " please fill a new issue." ) break elif response . lower ( ) == "n" : raise Exception ( "Configuration key still missing." ) else : self . _merge_values ( ) self . _save ( )
def _handle_non_existant_index ( cls ) : try : PyFunceble . INTERN [ "http_code" ] except KeyError : PyFunceble . INTERN [ "http_code" ] = "*" * 3 try : PyFunceble . INTERN [ "referer" ] except KeyError : PyFunceble . INTERN [ "referer" ] = "Unknown"
def status_file ( self ) : if "file_to_test" in PyFunceble . INTERN : Generate ( self . domain_status , self . source , self . expiration_date ) . info_files ( ) Percentage ( self . domain_status ) . count ( ) self . _prints_status_screen ( ) if self . _do_not_produce_file ( ) : return None if ( not PyFunceble . CONFIGURATION [ "no_files" ] and PyFunceble . CONFIGURATION [ "split" ] ) : self . _prints_status_file ( ) else : self . unified_file ( )
def load ( self ) : if not PyFunceble . INTERN [ "psl_db" ] : PyFunceble . INTERN [ "psl_db" ] = Dict ( ) . from_json ( File ( self . destination ) . read ( ) )
def load ( self ) : if "iana_db" not in PyFunceble . INTERN or not PyFunceble . INTERN [ "iana_db" ] : PyFunceble . INTERN [ "iana_db" ] = self . iana_db
def update ( self ) : if not PyFunceble . CONFIGURATION [ "quiet" ] : print ( "Update of iana-domains-db" , end = " " ) for extension , referer in self . _extensions ( ) : if extension not in self . iana_db or self . iana_db [ extension ] != referer : self . iana_db [ extension ] = referer Dict ( self . iana_db ) . to_json ( self . destination ) if not PyFunceble . CONFIGURATION [ "quiet" ] : print ( PyFunceble . INTERN [ "done" ] )
def _retrieve ( self ) : if PyFunceble . CONFIGURATION [ "mining" ] : if "mined" not in PyFunceble . INTERN : PyFunceble . INTERN [ "mined" ] = { } if PyFunceble . path . isfile ( self . file ) : data = Dict ( ) . from_json ( File ( self . file ) . read ( ) ) for file_path in data : PyFunceble . INTERN [ "mined" ] [ file_path ] = { } for element in data [ file_path ] : if data [ file_path ] [ element ] : PyFunceble . INTERN [ "mined" ] [ file_path ] [ element ] = data [ file_path ] [ element ] return PyFunceble . INTERN [ "mined" ] = { } return
def _backup ( self ) : if PyFunceble . CONFIGURATION [ "mining" ] : Dict ( PyFunceble . INTERN [ "mined" ] ) . to_json ( self . file )
def process ( self ) : if PyFunceble . CONFIGURATION [ "mining" ] : mined = self . mine ( ) if mined : self . _add ( mined ) self . _backup ( )
def _json_print ( self ) : if self . output : if PyFunceble . path . isfile ( self . output ) : content = Dict ( ) . from_json ( File ( self . output ) . read ( ) ) if isinstance ( content , list ) : content . extend ( self . data_to_print ) content = List ( content ) . custom_format ( Sort . standard ) if PyFunceble . CONFIGURATION [ "hierarchical_sorting" ] : content = List ( content ) . custom_format ( Sort . hierarchical ) Dict ( content ) . to_json ( self . output ) else : raise Exception ( "Output not correctly formatted." ) else : # Dict ( self . data_to_print ) . to_json ( self . output ) else : raise Exception ( "Empty output given." )
def file_to_delete ( cls ) : directory = PyFunceble . OUTPUT_DIRECTORY + PyFunceble . OUTPUTS [ "parent_directory" ] if not directory . endswith ( PyFunceble . directory_separator ) : directory += PyFunceble . directory_separator result = [ ] for root , _ , files in PyFunceble . walk ( directory ) : for file in files : if file not in [ ".gitignore" , ".keep" ] : if root . endswith ( PyFunceble . directory_separator ) : result . append ( root + file ) else : result . append ( root + PyFunceble . directory_separator + file ) return result
def databases_to_delete ( cls ) : directory = PyFunceble . CURRENT_DIRECTORY result = [ ] result . append ( directory + PyFunceble . CONFIGURATION [ "outputs" ] [ "default_files" ] [ "dir_structure" ] ) result . append ( directory + PyFunceble . CONFIGURATION [ "outputs" ] [ "default_files" ] [ "iana" ] ) result . append ( directory + PyFunceble . CONFIGURATION [ "outputs" ] [ "default_files" ] [ "public_suffix" ] ) result . append ( directory + PyFunceble . CONFIGURATION [ "outputs" ] [ "default_files" ] [ "inactive_db" ] ) result . append ( directory + PyFunceble . CONFIGURATION [ "outputs" ] [ "default_files" ] [ "mining" ] ) result . append ( directory + PyFunceble . CONFIGURATION [ "outputs" ] [ "default_files" ] [ "whois_db" ] ) return result
def get ( self ) : result = { } if self . algorithm in self . valid_algorithms : if self . algorithm == "all" : del self . valid_algorithms [ 0 ] for algo in self . valid_algorithms : if self . path and path . isfile ( self . path ) : result [ algo ] = self . _hash_file ( algo ) elif self . data : result [ algo ] = self . _hash_data ( algo ) else : return None else : if self . path and path . isfile ( self . path ) : result [ self . algorithm ] = self . _hash_file ( self . algorithm ) elif self . data : result [ self . algorithm ] = self . _hash_data ( self . algorithm ) else : return None else : return None if self . algorithm != "all" and self . only_hash : return result [ self . algorithm ] return result
def count ( self ) : if self . status : PyFunceble . INTERN [ "counter" ] [ "number" ] [ "tested" ] += 1 if ( self . status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "up" ] or self . status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "valid" ] ) : PyFunceble . INTERN [ "counter" ] [ "number" ] [ "up" ] += 1 elif self . status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "down" ] : PyFunceble . INTERN [ "counter" ] [ "number" ] [ "down" ] += 1 else : PyFunceble . INTERN [ "counter" ] [ "number" ] [ "invalid" ] += 1
def _calculate ( cls ) : percentages = { "up" : PyFunceble . INTERN [ "counter" ] [ "number" ] [ "up" ] , "down" : PyFunceble . INTERN [ "counter" ] [ "number" ] [ "down" ] , "invalid" : PyFunceble . INTERN [ "counter" ] [ "number" ] [ "invalid" ] , } for percentage in percentages : calculation = ( percentages [ percentage ] * 100 // PyFunceble . INTERN [ "counter" ] [ "number" ] [ "tested" ] ) PyFunceble . INTERN [ "counter" ] [ "percentage" ] . update ( { percentage : calculation } )
def log ( self ) : if ( PyFunceble . CONFIGURATION [ "show_percentage" ] and PyFunceble . INTERN [ "counter" ] [ "number" ] [ "tested" ] > 0 ) : output = ( PyFunceble . OUTPUT_DIRECTORY + PyFunceble . OUTPUTS [ "parent_directory" ] + PyFunceble . OUTPUTS [ "logs" ] [ "directories" ] [ "parent" ] + PyFunceble . OUTPUTS [ "logs" ] [ "directories" ] [ "percentage" ] + PyFunceble . OUTPUTS [ "logs" ] [ "filenames" ] [ "percentage" ] ) File ( output ) . delete ( ) self . _calculate ( ) if not PyFunceble . CONFIGURATION [ "quiet" ] : print ( "\n" ) Prints ( None , "Percentage" , output ) . header ( ) lines_to_print = [ [ PyFunceble . STATUS [ "official" ] [ "up" ] , str ( PyFunceble . INTERN [ "counter" ] [ "percentage" ] [ "up" ] ) + "%" , PyFunceble . INTERN [ "counter" ] [ "number" ] [ "up" ] , ] , [ PyFunceble . STATUS [ "official" ] [ "down" ] , str ( PyFunceble . INTERN [ "counter" ] [ "percentage" ] [ "down" ] ) + "%" , PyFunceble . INTERN [ "counter" ] [ "number" ] [ "down" ] , ] , [ PyFunceble . STATUS [ "official" ] [ "invalid" ] , str ( PyFunceble . INTERN [ "counter" ] [ "percentage" ] [ "invalid" ] ) + "%" , PyFunceble . INTERN [ "counter" ] [ "number" ] [ "invalid" ] , ] , ] if PyFunceble . CONFIGURATION [ "syntax" ] : lines_to_print [ 0 ] [ 0 ] = PyFunceble . STATUS [ "official" ] [ "valid" ] del lines_to_print [ 1 ] for to_print in lines_to_print : Prints ( to_print , "Percentage" , output ) . data ( ) elif PyFunceble . INTERN [ "counter" ] [ "number" ] [ "tested" ] > 0 : self . _calculate ( )
def _reformat_historical_formating_error ( self ) : if PyFunceble . CONFIGURATION [ "inactive_database" ] : historical_formating_error = ( PyFunceble . CURRENT_DIRECTORY + "inactive-db.json" ) if PyFunceble . path . isfile ( historical_formating_error ) : data = Dict ( ) . from_json ( File ( historical_formating_error ) . read ( ) ) data_to_parse = { } top_keys = data . keys ( ) for top_key in top_keys : low_keys = data [ top_key ] . keys ( ) data_to_parse [ top_key ] = { } for low_key in low_keys : if low_key . isdigit ( ) : data_to_parse [ top_key ] [ int ( low_key ) - ( self . one_day_in_seconds * 30 ) ] = data [ top_key ] [ low_key ] else : data_to_parse [ top_key ] [ int ( PyFunceble . time ( ) ) - ( self . one_day_in_seconds * 30 ) ] = data [ top_key ] [ low_key ] if "inactive_db" in PyFunceble . INTERN : PyFunceble . INTERN [ "inactive_db" ] . update ( data_to_parse ) else : PyFunceble . INTERN [ "inactive_db" ] = data_to_parse File ( historical_formating_error ) . delete ( )
def _retrieve ( self ) : if PyFunceble . CONFIGURATION [ "inactive_database" ] : self . _reformat_historical_formating_error ( ) if PyFunceble . path . isfile ( self . inactive_db_path ) : self . _merge ( )
def _backup ( self ) : if PyFunceble . CONFIGURATION [ "inactive_database" ] : Dict ( PyFunceble . INTERN [ "inactive_db" ] ) . to_json ( self . inactive_db_path )
def is_present ( cls ) : if PyFunceble . CONFIGURATION [ "inactive_database" ] : if PyFunceble . INTERN [ "to_test" ] in PyFunceble . INTERN [ "flatten_inactive_db" ] or ( PyFunceble . INTERN [ "file_to_test" ] in PyFunceble . INTERN [ "inactive_db" ] and PyFunceble . INTERN [ "inactive_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] and "to_test" in PyFunceble . INTERN [ "inactive_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] and PyFunceble . INTERN [ "to_test" ] in PyFunceble . INTERN [ "inactive_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ "to_test" ] ) : return True return False
def _retrieve ( self ) : if self . _authorization ( ) and "whois_db" not in PyFunceble . INTERN : if PyFunceble . path . isfile ( self . whois_db_path ) : PyFunceble . INTERN [ "whois_db" ] = Dict ( ) . from_json ( File ( self . whois_db_path ) . read ( ) ) else : PyFunceble . INTERN [ "whois_db" ] = { }
def _backup ( self ) : if self . _authorization ( ) : Dict ( PyFunceble . INTERN [ "whois_db" ] ) . to_json ( self . whois_db_path )
def is_in_database ( self ) : if ( self . _authorization ( ) and PyFunceble . INTERN [ "file_to_test" ] in PyFunceble . INTERN [ "whois_db" ] and PyFunceble . INTERN [ "to_test" ] in PyFunceble . INTERN [ "whois_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] ) : return True return False
def is_time_older ( self ) : if ( self . _authorization ( ) and self . is_in_database ( ) and int ( PyFunceble . INTERN [ "whois_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ PyFunceble . INTERN [ "to_test" ] ] [ "epoch" ] ) < int ( PyFunceble . time ( ) ) ) : return True return False
def add ( self ) : if self . _authorization ( ) : if self . epoch < int ( PyFunceble . time ( ) ) : state = "past" else : state = "future" if self . is_in_database ( ) : if ( str ( self . epoch ) != PyFunceble . INTERN [ "whois_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ PyFunceble . INTERN [ "to_test" ] ] [ "epoch" ] ) : PyFunceble . INTERN [ "whois_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ PyFunceble . INTERN [ "to_test" ] ] . update ( { "epoch" : str ( self . epoch ) , "state" : state , "expiration_date" : self . expiration_date , } ) elif self . is_time_older ( ) : if ( PyFunceble . INTERN [ "whois_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ PyFunceble . INTERN [ "to_test" ] ] [ "state" ] != "past" ) : PyFunceble . INTERN [ "whois_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ PyFunceble . INTERN [ "to_test" ] ] . update ( { "state" : "past" } ) elif ( PyFunceble . INTERN [ "whois_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ PyFunceble . INTERN [ "to_test" ] ] [ "state" ] != "future" ) : PyFunceble . INTERN [ "whois_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ PyFunceble . INTERN [ "to_test" ] ] . update ( { "state" : "future" } ) else : if ( not PyFunceble . INTERN [ "file_to_test" ] in PyFunceble . INTERN [ "whois_db" ] ) : PyFunceble . INTERN [ "whois_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] = { } PyFunceble . INTERN [ "whois_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] . update ( { PyFunceble . INTERN [ "to_test" ] : { "epoch" : str ( self . epoch ) , "state" : state , "expiration_date" : self . expiration_date , } } ) self . _backup ( )
def travis_permissions ( cls ) : if PyFunceble . CONFIGURATION [ "travis" ] : try : build_dir = PyFunceble . environ [ "TRAVIS_BUILD_DIR" ] commands = [ "sudo chown -R travis:travis %s" % ( build_dir ) , "sudo chgrp -R travis %s" % ( build_dir ) , "sudo chmod -R g+rwX %s" % ( build_dir ) , "sudo chmod 777 -Rf %s.git" % ( build_dir + PyFunceble . directory_separator ) , r"sudo find %s -type d -exec chmod g+x '{}' \;" % ( build_dir ) , ] for command in commands : Command ( command ) . execute ( ) if Command ( "git config core.sharedRepository" ) . execute ( ) == "" : Command ( "git config core.sharedRepository group" ) . execute ( ) except KeyError : pass
def _travis ( self ) : if PyFunceble . CONFIGURATION [ "travis" ] : try : _ = PyFunceble . environ [ "TRAVIS_BUILD_DIR" ] time_autorisation = False try : time_autorisation = int ( PyFunceble . time ( ) ) >= int ( PyFunceble . INTERN [ "start" ] ) + ( int ( PyFunceble . CONFIGURATION [ "travis_autosave_minutes" ] ) * 60 ) except KeyError : if self . last and not self . bypass : raise Exception ( "Please review the way `ExecutionTime()` is called." ) if self . last or time_autorisation or self . bypass : Percentage ( ) . log ( ) self . travis_permissions ( ) command = 'git add --all && git commit -a -m "%s"' if self . last or self . bypass : if PyFunceble . CONFIGURATION [ "command_before_end" ] : for line in Command ( PyFunceble . CONFIGURATION [ "command_before_end" ] ) . run ( ) : sys_stdout . write ( "{}\n" . format ( line ) ) self . travis_permissions ( ) message = ( PyFunceble . CONFIGURATION [ "travis_autosave_final_commit" ] + " [ci skip]" ) Command ( command % message ) . execute ( ) else : if PyFunceble . CONFIGURATION [ "command" ] : for line in Command ( PyFunceble . CONFIGURATION [ "command" ] ) . run ( ) : sys_stdout . write ( "{}\n" . format ( line ) ) self . travis_permissions ( ) Command ( command % PyFunceble . CONFIGURATION [ "travis_autosave_commit" ] ) . execute ( ) print ( Command ( "git push origin %s" % PyFunceble . CONFIGURATION [ "travis_branch" ] ) . execute ( ) ) exit ( 0 ) except KeyError : pass
def nslookup ( cls ) : try : if "current_test_data" in PyFunceble . INTERN : if not Check ( ) . is_ip_valid ( ) : request = PyFunceble . socket . getaddrinfo ( PyFunceble . INTERN [ "to_test" ] , 80 , 0 , 0 , PyFunceble . socket . IPPROTO_TCP , ) for sequence in request : PyFunceble . INTERN [ "current_test_data" ] [ "nslookup" ] . append ( sequence [ - 1 ] [ 0 ] ) else : request = PyFunceble . socket . gethostbyaddr ( PyFunceble . INTERN [ "to_test" ] ) PyFunceble . INTERN [ "current_test_data" ] [ "nslookup" ] [ "hostname" ] = request [ 0 ] PyFunceble . INTERN [ "current_test_data" ] [ "nslookup" ] [ "aliases" ] = request [ 1 ] PyFunceble . INTERN [ "current_test_data" ] [ "nslookup" ] [ "ips" ] = request [ 2 ] else : if not Check ( ) . is_ip_valid ( ) : PyFunceble . socket . getaddrinfo ( PyFunceble . INTERN [ "to_test" ] , 80 , 0 , 0 , PyFunceble . socket . IPPROTO_TCP , ) else : PyFunceble . socket . gethostbyaddr ( PyFunceble . INTERN [ "to_test" ] ) return True except ( OSError , PyFunceble . socket . herror , PyFunceble . socket . gaierror ) : return False
def get ( self ) : if not PyFunceble . CONFIGURATION [ "local" ] : if self . domain_extension not in self . ignored_extension : referer = None if self . domain_extension in PyFunceble . INTERN [ "iana_db" ] : if not PyFunceble . CONFIGURATION [ "no_whois" ] : referer = PyFunceble . INTERN [ "iana_db" ] [ self . domain_extension ] if not referer : Logs ( ) . referer_not_found ( self . domain_extension ) return None return referer return None return False return None return None
def standard_paths ( ) : for is_plat_spec in [ True , False ] : path = distutils . sysconfig . get_python_lib ( standard_lib = True , plat_specific = is_plat_spec ) for name in os . listdir ( path ) : yield name try : for name in os . listdir ( os . path . join ( path , 'lib-dynload' ) ) : yield name except OSError : pass
def standard_package_names ( ) : for name in standard_paths ( ) : if name . startswith ( '_' ) or '-' in name : continue if '.' in name and name . rsplit ( '.' ) [ - 1 ] not in [ 'so' , 'py' , 'pyc' ] : continue yield name . split ( '.' ) [ 0 ]
def unused_import_line_numbers ( messages ) : for message in messages : if isinstance ( message , pyflakes . messages . UnusedImport ) : yield message . lineno
def unused_import_module_name ( messages ) : pattern = r'\'(.+?)\'' for message in messages : if isinstance ( message , pyflakes . messages . UnusedImport ) : module_name = re . search ( pattern , str ( message ) ) module_name = module_name . group ( ) [ 1 : - 1 ] if module_name : yield ( message . lineno , module_name )
def star_import_used_line_numbers ( messages ) : for message in messages : if isinstance ( message , pyflakes . messages . ImportStarUsed ) : yield message . lineno
def star_import_usage_undefined_name ( messages ) : for message in messages : if isinstance ( message , pyflakes . messages . ImportStarUsage ) : undefined_name = message . message_args [ 0 ] module_name = message . message_args [ 1 ] yield ( message . lineno , undefined_name , module_name )
def unused_variable_line_numbers ( messages ) : for message in messages : if isinstance ( message , pyflakes . messages . UnusedVariable ) : yield message . lineno
def duplicate_key_line_numbers ( messages , source ) : messages = [ message for message in messages if isinstance ( message , pyflakes . messages . MultiValueRepeatedKeyLiteral ) ] if messages : key_to_messages = create_key_to_messages_dict ( messages ) lines = source . split ( '\n' ) for ( key , messages ) in key_to_messages . items ( ) : good = True for message in messages : line = lines [ message . lineno - 1 ] key = message . message_args [ 0 ] if not dict_entry_has_key ( line , key ) : good = False if good : for message in messages : yield message . lineno
def create_key_to_messages_dict ( messages ) : dictionary = collections . defaultdict ( lambda : [ ] ) for message in messages : dictionary [ message . message_args [ 0 ] ] . append ( message ) return dictionary
def check ( source ) : if sys . version_info [ 0 ] == 2 and isinstance ( source , unicode ) : try : source = source . encode ( 'utf-8' ) except UnicodeError : return [ ] reporter = ListReporter ( ) try : pyflakes . api . check ( source , filename = '<string>' , reporter = reporter ) except ( AttributeError , RecursionError , UnicodeDecodeError ) : pass return reporter . messages
def extract_package_name ( line ) : assert '\\' not in line assert '(' not in line assert ')' not in line assert ';' not in line if line . lstrip ( ) . startswith ( ( 'import' , 'from' ) ) : word = line . split ( ) [ 1 ] else : return None package = word . split ( '.' ) [ 0 ] assert ' ' not in package return package
def multiline_import ( line , previous_line = '' ) : for symbol in '()' : if symbol in line : return True if line . lstrip ( ) . startswith ( '>' ) : return True return multiline_statement ( line , previous_line )
def multiline_statement ( line , previous_line = '' ) : for symbol in '\\:;' : if symbol in line : return True sio = io . StringIO ( line ) try : list ( tokenize . generate_tokens ( sio . readline ) ) return previous_line . rstrip ( ) . endswith ( '\\' ) except ( SyntaxError , tokenize . TokenError ) : return True
def break_up_import ( line ) : assert '\\' not in line assert '(' not in line assert ')' not in line assert ';' not in line assert '#' not in line assert not line . lstrip ( ) . startswith ( 'from' ) newline = get_line_ending ( line ) if not newline : return line ( indentation , imports ) = re . split ( pattern = r'\bimport\b' , string = line , maxsplit = 1 ) indentation += 'import ' assert newline return '' . join ( [ indentation + i . strip ( ) + newline for i in sorted ( imports . split ( ',' ) ) ] )
def filter_code ( source , additional_imports = None , expand_star_imports = False , remove_all_unused_imports = False , remove_duplicate_keys = False , remove_unused_variables = False , ignore_init_module_imports = False , ) : imports = SAFE_IMPORTS if additional_imports : imports |= frozenset ( additional_imports ) del additional_imports messages = check ( source ) if ignore_init_module_imports : marked_import_line_numbers = frozenset ( ) else : marked_import_line_numbers = frozenset ( unused_import_line_numbers ( messages ) ) marked_unused_module = collections . defaultdict ( lambda : [ ] ) for line_number , module_name in unused_import_module_name ( messages ) : marked_unused_module [ line_number ] . append ( module_name ) if expand_star_imports and not ( re . search ( r'\b__all__\b' , source ) or re . search ( r'\bdel\b' , source ) ) : marked_star_import_line_numbers = frozenset ( star_import_used_line_numbers ( messages ) ) if len ( marked_star_import_line_numbers ) > 1 : marked_star_import_line_numbers = frozenset ( ) else : undefined_names = [ ] for line_number , undefined_name , _ in star_import_usage_undefined_name ( messages ) : undefined_names . append ( undefined_name ) if not undefined_names : marked_star_import_line_numbers = frozenset ( ) else : marked_star_import_line_numbers = frozenset ( ) if remove_unused_variables : marked_variable_line_numbers = frozenset ( unused_variable_line_numbers ( messages ) ) else : marked_variable_line_numbers = frozenset ( ) if remove_duplicate_keys : marked_key_line_numbers = frozenset ( duplicate_key_line_numbers ( messages , source ) ) else : marked_key_line_numbers = frozenset ( ) line_messages = get_messages_by_line ( messages ) sio = io . StringIO ( source ) previous_line = '' for line_number , line in enumerate ( sio . readlines ( ) , start = 1 ) : if '#' in line : yield line elif line_number in marked_import_line_numbers : yield filter_unused_import ( line , unused_module = marked_unused_module [ line_number ] , remove_all_unused_imports = remove_all_unused_imports , imports = imports , previous_line = previous_line ) elif line_number in marked_variable_line_numbers : yield filter_unused_variable ( line ) elif line_number in marked_key_line_numbers : yield filter_duplicate_key ( line , line_messages [ line_number ] , line_number , marked_key_line_numbers , source ) elif line_number in marked_star_import_line_numbers : yield filter_star_import ( line , undefined_names ) else : yield line previous_line = line
def get_messages_by_line ( messages ) : line_messages = { } for message in messages : line_messages [ message . lineno ] = message return line_messages
def filter_star_import ( line , marked_star_import_undefined_name ) : undefined_name = sorted ( set ( marked_star_import_undefined_name ) ) return re . sub ( r'\*' , ', ' . join ( undefined_name ) , line )
def filter_unused_import ( line , unused_module , remove_all_unused_imports , imports , previous_line = '' ) : if multiline_import ( line , previous_line ) : return line is_from_import = line . lstrip ( ) . startswith ( 'from' ) if ',' in line and not is_from_import : return break_up_import ( line ) package = extract_package_name ( line ) if not remove_all_unused_imports and package not in imports : return line if ',' in line : assert is_from_import return filter_from_import ( line , unused_module ) else : return ( get_indentation ( line ) + 'pass' + get_line_ending ( line ) )
def filter_unused_variable ( line , previous_line = '' ) : if re . match ( EXCEPT_REGEX , line ) : return re . sub ( r' as \w+:$' , ':' , line , count = 1 ) elif multiline_statement ( line , previous_line ) : return line elif line . count ( '=' ) == 1 : split_line = line . split ( '=' ) assert len ( split_line ) == 2 value = split_line [ 1 ] . lstrip ( ) if ',' in split_line [ 0 ] : return line if is_literal_or_name ( value ) : value = 'pass' + get_line_ending ( line ) return get_indentation ( line ) + value else : return line
def filter_duplicate_key ( line , message , line_number , marked_line_numbers , source , previous_line = '' ) : if marked_line_numbers and line_number == sorted ( marked_line_numbers ) [ 0 ] : return '' return line
def is_literal_or_name ( value ) : try : ast . literal_eval ( value ) return True except ( SyntaxError , ValueError ) : pass if value . strip ( ) in [ 'dict()' , 'list()' , 'set()' ] : return True return re . match ( r'^\w+\s*$' , value )
def useless_pass_line_numbers ( source ) : sio = io . StringIO ( source ) previous_token_type = None last_pass_row = None last_pass_indentation = None previous_line = '' for token in tokenize . generate_tokens ( sio . readline ) : token_type = token [ 0 ] start_row = token [ 2 ] [ 0 ] line = token [ 4 ] is_pass = ( token_type == tokenize . NAME and line . strip ( ) == 'pass' ) if ( start_row - 1 == last_pass_row and get_indentation ( line ) == last_pass_indentation and token_type in ATOMS and not is_pass ) : yield start_row - 1 if is_pass : last_pass_row = start_row last_pass_indentation = get_indentation ( line ) if ( is_pass and previous_token_type != tokenize . INDENT and not previous_line . rstrip ( ) . endswith ( '\\' ) ) : yield start_row previous_token_type = token_type previous_line = line
def filter_useless_pass ( source ) : try : marked_lines = frozenset ( useless_pass_line_numbers ( source ) ) except ( SyntaxError , tokenize . TokenError ) : marked_lines = frozenset ( ) sio = io . StringIO ( source ) for line_number , line in enumerate ( sio . readlines ( ) , start = 1 ) : if line_number not in marked_lines : yield line
def get_indentation ( line ) : if line . strip ( ) : non_whitespace_index = len ( line ) - len ( line . lstrip ( ) ) return line [ : non_whitespace_index ] else : return ''
def get_line_ending ( line ) : non_whitespace_index = len ( line . rstrip ( ) ) - len ( line ) if not non_whitespace_index : return '' else : return line [ non_whitespace_index : ]
def fix_code ( source , additional_imports = None , expand_star_imports = False , remove_all_unused_imports = False , remove_duplicate_keys = False , remove_unused_variables = False , ignore_init_module_imports = False ) : if not source : return source if 'nonlocal' in source : remove_unused_variables = False filtered_source = None while True : filtered_source = '' . join ( filter_useless_pass ( '' . join ( filter_code ( source , additional_imports = additional_imports , expand_star_imports = expand_star_imports , remove_all_unused_imports = remove_all_unused_imports , remove_duplicate_keys = remove_duplicate_keys , remove_unused_variables = remove_unused_variables , ignore_init_module_imports = ignore_init_module_imports , ) ) ) ) if filtered_source == source : break source = filtered_source return filtered_source
def detect_encoding ( filename , limit_byte_check = - 1 ) : try : with open ( filename , 'rb' ) as input_file : encoding = _detect_encoding ( input_file . readline ) with open_with_encoding ( filename , encoding ) as input_file : input_file . read ( limit_byte_check ) return encoding except ( LookupError , SyntaxError , UnicodeDecodeError ) : return 'latin-1'
def _detect_encoding ( readline ) : try : from lib2to3 . pgen2 import tokenize as lib2to3_tokenize encoding = lib2to3_tokenize . detect_encoding ( readline ) [ 0 ] return encoding except ( LookupError , SyntaxError , UnicodeDecodeError ) : return 'latin-1'
def _split_comma_separated ( string ) : return set ( text . strip ( ) for text in string . split ( ',' ) if text . strip ( ) )
def is_python_file ( filename ) : if filename . endswith ( '.py' ) : return True try : with open_with_encoding ( filename , None , limit_byte_check = MAX_PYTHON_FILE_DETECTION_BYTES ) as f : text = f . read ( MAX_PYTHON_FILE_DETECTION_BYTES ) if not text : return False first_line = text . splitlines ( ) [ 0 ] except ( IOError , IndexError ) : return False if not PYTHON_SHEBANG_REGEX . match ( first_line ) : return False return True
def is_exclude_file ( filename , exclude ) : base_name = os . path . basename ( filename ) if base_name . startswith ( '.' ) : return True for pattern in exclude : if fnmatch . fnmatch ( base_name , pattern ) : return True if fnmatch . fnmatch ( filename , pattern ) : return True return False
def create ( cls , name_value , name_type ) : if isinstance ( name_value , Name . NameValue ) : value = name_value elif isinstance ( name_value , str ) : value = cls . NameValue ( name_value ) else : name = 'Name' msg = exceptions . ErrorStrings . BAD_EXP_RECV member = 'name_value' raise TypeError ( msg . format ( '{0}.{1}' . format ( name , member ) , 'name_value' , type ( Name . NameValue ) , type ( name_value ) ) ) if isinstance ( name_type , Name . NameType ) : n_type = name_type elif isinstance ( name_type , Enum ) : n_type = cls . NameType ( name_type ) else : name = 'Name' msg = exceptions . ErrorStrings . BAD_EXP_RECV member = 'name_type' raise TypeError ( msg . format ( '{0}.{1}' . format ( name , member ) , 'name_type' , type ( Name . NameType ) , type ( name_type ) ) ) return Name ( name_value = value , name_type = n_type )
def _get_attribute_from_managed_object ( self , managed_object , attr_name ) : if attr_name == 'Unique Identifier' : return str ( managed_object . unique_identifier ) elif attr_name == 'Name' : names = list ( ) for name in managed_object . names : name = attributes . Name ( attributes . Name . NameValue ( name ) , attributes . Name . NameType ( enums . NameType . UNINTERPRETED_TEXT_STRING ) ) names . append ( name ) return names elif attr_name == 'Object Type' : return managed_object . _object_type elif attr_name == 'Cryptographic Algorithm' : return managed_object . cryptographic_algorithm elif attr_name == 'Cryptographic Length' : return managed_object . cryptographic_length elif attr_name == 'Cryptographic Parameters' : return None elif attr_name == 'Cryptographic Domain Parameters' : return None elif attr_name == 'Certificate Type' : return managed_object . certificate_type elif attr_name == 'Certificate Length' : return None elif attr_name == 'X.509 Certificate Identifier' : return None elif attr_name == 'X.509 Certificate Subject' : return None elif attr_name == 'X.509 Certificate Issuer' : return None elif attr_name == 'Certificate Identifier' : return None elif attr_name == 'Certificate Subject' : return None elif attr_name == 'Certificate Issuer' : return None elif attr_name == 'Digital Signature Algorithm' : return None elif attr_name == 'Digest' : return None elif attr_name == 'Operation Policy Name' : return managed_object . operation_policy_name elif attr_name == 'Cryptographic Usage Mask' : return managed_object . cryptographic_usage_masks elif attr_name == 'Lease Time' : return None elif attr_name == 'Usage Limits' : return None elif attr_name == 'State' : return managed_object . state elif attr_name == 'Initial Date' : return managed_object . initial_date elif attr_name == 'Activation Date' : return None elif attr_name == 'Process Start Date' : return None elif attr_name == 'Protect Stop Date' : return None elif attr_name == 'Deactivation Date' : return None elif attr_name == 'Destroy Date' : return None elif attr_name == 'Compromise Occurrence Date' : return None elif attr_name == 'Compromise Date' : return None elif attr_name == 'Revocation Reason' : return None elif attr_name == 'Archive Date' : return None elif attr_name == 'Object Group' : return None elif attr_name == 'Fresh' : return None elif attr_name == 'Link' : return None elif attr_name == 'Application Specific Information' : return None elif attr_name == 'Contact Information' : return None elif attr_name == 'Last Change Date' : return None else : return None
def _set_attribute_on_managed_object ( self , managed_object , attribute ) : attribute_name = attribute [ 0 ] attribute_value = attribute [ 1 ] if self . _attribute_policy . is_attribute_multivalued ( attribute_name ) : if attribute_name == 'Name' : managed_object . names . extend ( [ x . name_value . value for x in attribute_value ] ) for name in managed_object . names : if managed_object . names . count ( name ) > 1 : raise exceptions . InvalidField ( "Cannot set duplicate name values." ) else : raise exceptions . InvalidField ( "The {0} attribute is unsupported." . format ( attribute_name ) ) else : field = None value = attribute_value . value if attribute_name == 'Cryptographic Algorithm' : field = 'cryptographic_algorithm' elif attribute_name == 'Cryptographic Length' : field = 'cryptographic_length' elif attribute_name == 'Cryptographic Usage Mask' : field = 'cryptographic_usage_masks' value = list ( ) for e in enums . CryptographicUsageMask : if e . value & attribute_value . value : value . append ( e ) elif attribute_name == 'Operation Policy Name' : field = 'operation_policy_name' if field : existing_value = getattr ( managed_object , field ) if existing_value : if existing_value != value : raise exceptions . InvalidField ( "Cannot overwrite the {0} attribute." . format ( attribute_name ) ) else : setattr ( managed_object , field , value ) else : raise exceptions . InvalidField ( "The {0} attribute is unsupported." . format ( attribute_name ) )
def validate ( self ) : if self . unique_identifier is not None : if not isinstance ( self . unique_identifier , attributes . UniqueIdentifier ) : msg = "invalid unique identifier" raise TypeError ( msg ) if self . compromise_occurrence_date is not None : if not isinstance ( self . compromise_occurrence_date , primitives . DateTime ) : msg = "invalid compromise time" raise TypeError ( msg ) if not isinstance ( self . revocation_reason , objects . RevocationReason ) : msg = "invalid revocation reason" raise TypeError ( msg )
def key_wrapping_data ( self , value ) : if value is None : value = { } elif not isinstance ( value , dict ) : raise TypeError ( "Key wrapping data must be a dictionary." ) self . _kdw_wrapping_method = value . get ( 'wrapping_method' ) eki = value . get ( 'encryption_key_information' ) if eki is None : eki = { } self . _kdw_eki_unique_identifier = eki . get ( 'unique_identifier' ) eki_cp = eki . get ( 'cryptographic_parameters' ) if eki_cp is None : eki_cp = { } self . _kdw_eki_cp_block_cipher_mode = eki_cp . get ( 'block_cipher_mode' ) self . _kdw_eki_cp_padding_method = eki_cp . get ( 'padding_method' ) self . _kdw_eki_cp_hashing_algorithm = eki_cp . get ( 'hashing_algorithm' ) self . _kdw_eki_cp_key_role_type = eki_cp . get ( 'key_role_type' ) self . _kdw_eki_cp_digital_signature_algorithm = eki_cp . get ( 'digital_signature_algorithm' ) self . _kdw_eki_cp_cryptographic_algorithm = eki_cp . get ( 'cryptographic_algorithm' ) self . _kdw_eki_cp_random_iv = eki_cp . get ( 'random_iv' ) self . _kdw_eki_cp_iv_length = eki_cp . get ( 'iv_length' ) self . _kdw_eki_cp_tag_length = eki_cp . get ( 'tag_length' ) self . _kdw_eki_cp_fixed_field_length = eki_cp . get ( 'fixed_field_length' ) self . _kdw_eki_cp_invocation_field_length = eki_cp . get ( 'invocation_field_length' ) self . _kdw_eki_cp_counter_length = eki_cp . get ( 'counter_length' ) self . _kdw_eki_cp_initial_counter_value = eki_cp . get ( 'initial_counter_value' ) mski = value . get ( 'mac_signature_key_information' ) if mski is None : mski = { } self . _kdw_mski_unique_identifier = mski . get ( 'unique_identifier' ) mski_cp = mski . get ( 'cryptographic_parameters' ) if mski_cp is None : mski_cp = { } self . _kdw_mski_cp_block_cipher_mode = mski_cp . get ( 'block_cipher_mode' ) self . _kdw_mski_cp_padding_method = mski_cp . get ( 'padding_method' ) self . _kdw_mski_cp_hashing_algorithm = mski_cp . get ( 'hashing_algorithm' ) self . _kdw_mski_cp_key_role_type = mski_cp . get ( 'key_role_type' ) self . _kdw_mski_cp_digital_signature_algorithm = mski_cp . get ( 'digital_signature_algorithm' ) self . _kdw_mski_cp_cryptographic_algorithm = mski_cp . get ( 'cryptographic_algorithm' ) self . _kdw_mski_cp_random_iv = mski_cp . get ( 'random_iv' ) self . _kdw_mski_cp_iv_length = mski_cp . get ( 'iv_length' ) self . _kdw_mski_cp_tag_length = mski_cp . get ( 'tag_length' ) self . _kdw_mski_cp_fixed_field_length = mski_cp . get ( 'fixed_field_length' ) self . _kdw_mski_cp_invocation_field_length = mski_cp . get ( 'invocation_field_length' ) self . _kdw_mski_cp_counter_length = mski_cp . get ( 'counter_length' ) self . _kdw_mski_cp_initial_counter_value = mski_cp . get ( 'initial_counter_value' ) self . _kdw_mac_signature = value . get ( 'mac_signature' ) self . _kdw_iv_counter_nonce = value . get ( 'iv_counter_nonce' ) self . _kdw_encoding_option = value . get ( 'encoding_option' )
def get_json_files ( p ) : f = [ os . path . join ( p , x ) for x in os . listdir ( p ) if x . endswith ( ".json" ) ] return sorted ( f )
def scan_policies ( self ) : policy_files = get_json_files ( self . policy_directory ) for f in set ( policy_files ) - set ( self . policy_files ) : self . file_timestamps [ f ] = 0 for f in set ( self . policy_files ) - set ( policy_files ) : self . logger . info ( "Removing policies for file: {}" . format ( f ) ) self . file_timestamps . pop ( f , None ) for p in self . policy_cache . keys ( ) : self . disassociate_policy_and_file ( p , f ) for p in [ k for k , v in self . policy_map . items ( ) if v == f ] : self . restore_or_delete_policy ( p ) self . policy_files = policy_files for f in sorted ( self . file_timestamps . keys ( ) ) : t = os . path . getmtime ( f ) if t > self . file_timestamps [ f ] : self . logger . info ( "Loading policies for file: {}" . format ( f ) ) self . file_timestamps [ f ] = t old_p = [ k for k , v in self . policy_map . items ( ) if v == f ] try : new_p = operation_policy . read_policy_from_file ( f ) except ValueError : self . logger . error ( "Failure loading file: {}" . format ( f ) ) self . logger . debug ( "" , exc_info = True ) continue for p in new_p . keys ( ) : self . logger . info ( "Loading policy: {}" . format ( p ) ) if p in self . reserved_policies : self . logger . warning ( "Policy '{}' overwrites a reserved policy and " "will be thrown out." . format ( p ) ) continue if p in sorted ( self . policy_store . keys ( ) ) : self . logger . debug ( "Policy '{}' overwrites an existing " "policy." . format ( p ) ) if f != self . policy_map . get ( p ) : self . policy_cache . get ( p ) . append ( ( time . time ( ) , self . policy_map . get ( p ) , self . policy_store . get ( p ) ) ) else : self . policy_cache [ p ] = [ ] self . policy_store [ p ] = new_p . get ( p ) self . policy_map [ p ] = f for p in set ( old_p ) - set ( new_p . keys ( ) ) : self . disassociate_policy_and_file ( p , f ) self . restore_or_delete_policy ( p )
def run ( self ) : self . initialize_tracking_structures ( ) if self . live_monitoring : self . logger . info ( "Starting up the operation policy file monitor." ) while not self . halt_trigger . is_set ( ) : time . sleep ( 1 ) self . scan_policies ( ) self . logger . info ( "Stopping the operation policy file monitor." ) else : self . scan_policies ( )
def get_certificate_from_connection ( connection ) : certificate = connection . getpeercert ( binary_form = True ) if certificate : return x509 . load_der_x509_certificate ( certificate , backends . default_backend ( ) ) return None
def get_common_names_from_certificate ( certificate ) : common_names = certificate . subject . get_attributes_for_oid ( x509 . oid . NameOID . COMMON_NAME ) return [ common_name . value for common_name in common_names ]
def get_client_identity_from_certificate ( certificate ) : client_ids = get_common_names_from_certificate ( certificate ) if len ( client_ids ) > 0 : if len ( client_ids ) > 1 : raise exceptions . PermissionDenied ( "Multiple client identities found." ) return client_ids [ 0 ] else : raise exceptions . PermissionDenied ( "The certificate does not define any subject common names. " "Client identity unavailable." )
def validate ( self ) : if not isinstance ( self . revocation_code , RevocationReasonCode ) : msg = "RevocationReaonCode expected" raise TypeError ( msg ) if self . revocation_message is not None : if not isinstance ( self . revocation_message , TextString ) : msg = "TextString expect" raise TypeError ( msg )
def validate ( self ) : if self . unique_identifier is not None : if not isinstance ( self . unique_identifier , attributes . UniqueIdentifier ) : msg = "invalid unique identifier" raise TypeError ( msg )
def load ( self ) : mod = import_module ( self . module_name ) obj = mod if self . object_name : for attr in self . object_name . split ( '.' ) : obj = getattr ( obj , attr ) return obj
def generate_controller ( args ) : controller_template = os . path . join ( dirname ( abspath ( __file__ ) ) , 'templates/controller.py' ) test_template = os . path . join ( dirname ( abspath ( __file__ ) ) , 'templates/unittest.py' ) controller_name = args . get ( '<controller>' ) current_path = os . getcwd ( ) logger . info ( 'Start generating controller.' ) if not controller_name : logger . warning ( 'Controller name cannot be empty.' ) return with open ( controller_template , 'r' ) as template_file : controller_file_path = os . path . join ( current_path , 'application/controllers' , controller_name + '.py' ) with open ( controller_file_path , 'w+' ) as controller_file : for line in template_file : new_line = line . replace ( '#{controller}' , controller_name ) controller_file . write ( new_line ) logger . info ( "New: %s" % _relative_path ( controller_file_path ) ) with open ( test_template , 'r' ) as template_file : test_file_path = os . path . join ( current_path , 'tests' , 'test_%s.py' % controller_name ) with open ( test_file_path , 'w+' ) as test_file : for line in template_file : new_line = line . replace ( '#{controller}' , controller_name ) . replace ( '#{controller|title}' , controller_name . title ( ) ) test_file . write ( new_line ) logger . info ( "New: %s" % _relative_path ( test_file_path ) ) assets_dir_path = os . path . join ( current_path , 'application/pages/%s' % controller_name ) _mkdir_p ( assets_dir_path ) _generate_form ( controller_name ) logger . info ( 'Finish generating controller.' )
def _mkdir_p ( path ) : try : os . makedirs ( path ) except OSError as exc : if exc . errno == errno . EEXIST and os . path . isdir ( path ) : pass else : raise else : logger . info ( "New: %s%s" , path , os . path . sep )
def _rewrite_and_copy ( src_file , dst_file , project_name ) : fh , abs_path = mkstemp ( ) with io . open ( abs_path , 'w' , encoding = 'utf-8' ) as new_file : with io . open ( src_file , 'r' , encoding = 'utf-8' ) as old_file : for line in old_file : new_line = line . replace ( '#{project}' , project_name ) . replace ( '#{project|title}' , project_name . title ( ) ) new_file . write ( new_line ) shutil . copy ( abs_path , dst_file ) os . close ( fh )
def check_url ( form , field ) : url = field . data . strip ( ) if not url : return result = urlparse ( url ) if result . scheme == "" : field . data = "http://%s" % re . sub ( r'^:?/*' , '' , url )
def encode ( something ) : secret_key = current_app . config . get ( 'SECRET_KEY' ) s = URLSafeSerializer ( secret_key ) return s . dumps ( something )
def decode ( something ) : secret_key = current_app . config . get ( 'SECRET_KEY' ) s = URLSafeSerializer ( secret_key ) try : return s . loads ( something ) except BadSignature : return None
def absolute_url_for ( endpoint , * * values ) : config = current_app . config site_domain = config . get ( 'SITE_DOMAIN' ) relative_url = url_for ( endpoint , * * values ) return join_url ( site_domain , relative_url )
def signin_user ( user , permenent = True ) : session . permanent = permenent session [ 'user_id' ] = user . id
def get_current_user ( ) : if not 'user_id' in session : return None user = User . query . filter ( User . id == session [ 'user_id' ] ) . first ( ) if not user : signout_user ( ) return None return user
def create_app ( ) : config = load_config ( ) app = Flask ( __name__ ) app . config . from_object ( config ) app . wsgi_app = ProxyFix ( app . wsgi_app ) CsrfProtect ( app ) if app . debug or app . testing : DebugToolbarExtension ( app ) app . wsgi_app = SharedDataMiddleware ( app . wsgi_app , { '/pages' : os . path . join ( app . config . get ( 'PROJECT_PATH' ) , 'application/pages' ) } ) else : app . logger . addHandler ( logging . StreamHandler ( ) ) app . logger . setLevel ( logging . ERROR ) if app . config . get ( 'SENTRY_DSN' ) : from . utils . sentry import sentry sentry . init_app ( app , dsn = app . config . get ( 'SENTRY_DSN' ) ) app . wsgi_app = SharedDataMiddleware ( app . wsgi_app , { '/static' : os . path . join ( app . config . get ( 'PROJECT_PATH' ) , 'output/static' ) , '/pkg' : os . path . join ( app . config . get ( 'PROJECT_PATH' ) , 'output/pkg' ) , '/pages' : os . path . join ( app . config . get ( 'PROJECT_PATH' ) , 'output/pages' ) } ) register_db ( app ) register_routes ( app ) register_jinja ( app ) register_error_handle ( app ) register_hooks ( app ) return app
def register_jinja ( app ) : import jinja2 from . utils import filters , permissions , helpers if app . debug or app . testing : my_loader = jinja2 . ChoiceLoader ( [ app . jinja_loader , jinja2 . FileSystemLoader ( [ os . path . join ( app . config . get ( 'PROJECT_PATH' ) , 'application/macros' ) , os . path . join ( app . config . get ( 'PROJECT_PATH' ) , 'application/pages' ) ] ) ] ) else : my_loader = jinja2 . ChoiceLoader ( [ app . jinja_loader , jinja2 . FileSystemLoader ( [ os . path . join ( app . config . get ( 'PROJECT_PATH' ) , 'output/macros' ) , os . path . join ( app . config . get ( 'PROJECT_PATH' ) , 'output/pages' ) ] ) ] ) app . jinja_loader = my_loader app . jinja_env . filters . update ( { 'timesince' : filters . timesince } ) def url_for_other_page ( page ) : """Generate url for pagination.""" view_args = request . view_args . copy ( ) args = request . args . copy ( ) . to_dict ( ) combined_args = dict ( view_args . items ( ) + args . items ( ) ) combined_args [ 'page' ] = page return url_for ( request . endpoint , * * combined_args ) rules = { } for endpoint , _rules in iteritems ( app . url_map . _rules_by_endpoint ) : if any ( item in endpoint for item in [ '_debug_toolbar' , 'debugtoolbar' , 'static' ] ) : continue rules [ endpoint ] = [ { 'rule' : rule . rule } for rule in _rules ] app . jinja_env . globals . update ( { 'absolute_url_for' : helpers . absolute_url_for , 'url_for_other_page' : url_for_other_page , 'rules' : rules , 'permissions' : permissions } )
def register_error_handle ( app ) : @ app . errorhandler ( 403 ) def page_403 ( error ) : return render_template ( 'site/403/403.html' ) , 403 @ app . errorhandler ( 404 ) def page_404 ( error ) : return render_template ( 'site/404/404.html' ) , 404 @ app . errorhandler ( 500 ) def page_500 ( error ) : return render_template ( 'site/500/500.html' ) , 500
def _dataframe_to_csv ( writer , dataframe , delimiter , with_header ) : encoding_writer = codecs . getwriter ( 'utf-8' ) ( writer ) dataframe . to_csv ( path_or_buf = encoding_writer , sep = delimiter , header = with_header , index = False )
def _dataframe_from_csv ( reader , delimiter , with_header , skipspace ) : sep = delimiter header = 0 if not with_header : header = None return pd . read_csv ( reader , header = header , sep = sep , skipinitialspace = skipspace , encoding = 'utf-8-sig' )
def contents_url ( self ) : loc = self . download_location return loc . base_uri + loc . location + loc . access_credential
def open ( self ) : return self . workspace . _rest . open_intermediate_dataset_contents ( self . workspace . workspace_id , self . experiment . experiment_id , self . node_id , self . port_name )
def read_as_binary ( self ) : return self . workspace . _rest . read_intermediate_dataset_contents_binary ( self . workspace . workspace_id , self . experiment . experiment_id , self . node_id , self . port_name )
def read_as_text ( self ) : return self . workspace . _rest . read_intermediate_dataset_contents_text ( self . workspace . workspace_id , self . experiment . experiment_id , self . node_id , self . port_name )
def _to_dataframe ( self ) : #TODO: figure out why passing in the opened stream directly gives invalid data data = self . read_as_binary ( ) reader = BytesIO ( data ) return deserialize_dataframe ( reader , self . data_type_id )
def get_experiments ( self , workspace_id ) : api_path = self . EXPERIMENTS_URI_FMT . format ( workspace_id ) return self . _send_get_req ( api_path )
def get_datasets ( self , workspace_id ) : api_path = self . DATASOURCES_URI_FMT . format ( workspace_id ) return self . _send_get_req ( api_path )
def get_dataset ( self , workspace_id , dataset_id ) : api_path = self . DATASOURCE_URI_FMT . format ( workspace_id , dataset_id ) return self . _send_get_req ( api_path )
def find_globals ( code ) : cur_byte = 0 byte_code = code . co_code names = set ( ) while cur_byte < len ( byte_code ) : op = ord ( byte_code [ cur_byte ] ) if op >= dis . HAVE_ARGUMENT : if op == _LOAD_GLOBAL : oparg = ord ( byte_code [ cur_byte + 1 ] ) + ( ord ( byte_code [ cur_byte + 2 ] ) << 8 ) name = code . co_names [ oparg ] names . add ( name ) cur_byte += 2 cur_byte += 1 return names
def copy ( self ) : pen = Pen ( ) pen . __dict__ = self . __dict__ . copy ( ) return pen
def draw ( self , cr , highlight = False , bounding = None ) : if bounding is None or self . _intersects ( bounding ) : self . _draw ( cr , highlight , bounding )
def _build_choices ( self ) : tree_token = u'sitetree_tree from "%s" template "%s"' % ( self . tree , self . template ) context_kwargs = { 'current_app' : 'admin' } context = template . Context ( context_kwargs ) if VERSION >= ( 1 , 8 ) else template . Context ( * * context_kwargs ) context . update ( { 'request' : object ( ) } ) choices_str = sitetree_tree ( Parser ( None ) , Token ( token_type = TOKEN_BLOCK , contents = tree_token ) ) . render ( context ) tree_choices = [ ( ITEMS_FIELD_ROOT_ID , self . root_title ) ] for line in choices_str . splitlines ( ) : if line . strip ( ) : splitted = line . split ( ':::' ) tree_choices . append ( ( splitted [ 0 ] , mark_safe ( splitted [ 1 ] ) ) ) return tree_choices
def init ( self ) : cache . get ( 'sitetrees_reset' ) and self . empty ( init = False ) self . cache = cache . get ( 'sitetrees' , { 'sitetrees' : { } , 'parents' : { } , 'items_by_ids' : { } , 'tree_aliases' : { } } )
def empty ( self , * * kwargs ) : cache . delete ( 'sitetrees' ) cache . delete ( 'sitetrees_reset' ) kwargs . get ( 'init' , True ) and self . init ( )
def for_tag ( cls , parser , token , preposition , error_hint ) : tokens = token . split_contents ( ) if len ( tokens ) >= 3 and tokens [ 1 ] == preposition : as_var = cls . get_as_var ( tokens ) tree_alias = parser . compile_filter ( tokens [ 2 ] ) return cls ( tree_alias , as_var ) raise template . TemplateSyntaxError ( '%r tag requires at least two arguments. E.g. {%% %s %%}.' % ( tokens [ 0 ] , error_hint ) )
def get_model_url_name ( model_nfo , page , with_namespace = False ) : prefix = '' if with_namespace : prefix = 'admin:' return ( '%s%s_%s' % ( prefix , '%s_%s' % model_nfo , page ) ) . lower ( )
def _reregister_tree_admin ( ) : try : admin . site . unregister ( MODEL_TREE_CLASS ) except NotRegistered : pass admin . site . register ( MODEL_TREE_CLASS , _TREE_ADMIN ( ) )
def _redirect ( self , request , response ) : if '_addanother' in request . POST : return HttpResponseRedirect ( '../item_add/' ) elif '_save' in request . POST : return HttpResponseRedirect ( '../' ) elif '_continue' in request . POST : return response return HttpResponseRedirect ( '' )
def get_tree ( self , request , tree_id , item_id = None ) : if tree_id is None : tree_id = self . get_object ( request , item_id ) . tree_id self . tree = MODEL_TREE_CLASS . _default_manager . get ( pk = tree_id ) self . tree . verbose_name_plural = self . tree . _meta . verbose_name_plural self . tree . urls = _TREE_URLS return self . tree
def item_move ( self , request , tree_id , item_id , direction ) : current_item = MODEL_TREE_ITEM_CLASS . _default_manager . get ( pk = item_id ) if direction == 'up' : sort_order = 'sort_order' else : sort_order = '-sort_order' siblings = MODEL_TREE_ITEM_CLASS . _default_manager . filter ( parent = current_item . parent , tree = current_item . tree ) . order_by ( sort_order ) previous_item = None for item in siblings : if item != current_item : previous_item = item else : break if previous_item is not None : current_item_sort_order = current_item . sort_order previous_item_sort_order = previous_item . sort_order current_item . sort_order = previous_item_sort_order previous_item . sort_order = current_item_sort_order current_item . save ( ) previous_item . save ( ) return HttpResponseRedirect ( '../../' )
def get_urls ( self ) : urls = super ( TreeAdmin , self ) . get_urls ( ) prefix_change = 'change/' if DJANGO_POST_19 else '' sitetree_urls = [ url ( r'^change/$' , redirects_handler , name = get_tree_item_url_name ( 'changelist' ) ) , url ( r'^((?P<tree_id>\d+)/)?%sitem_add/$' % prefix_change , self . admin_site . admin_view ( self . tree_admin . item_add ) , name = get_tree_item_url_name ( 'add' ) ) , url ( r'^(?P<tree_id>\d+)/%sitem_(?P<item_id>\d+)/$' % prefix_change , self . admin_site . admin_view ( self . tree_admin . item_edit ) , name = get_tree_item_url_name ( 'change' ) ) , url ( r'^%sitem_(?P<item_id>\d+)/$' % prefix_change , self . admin_site . admin_view ( self . tree_admin . item_edit ) , name = get_tree_item_url_name ( 'change' ) ) , url ( r'^((?P<tree_id>\d+)/)?%sitem_(?P<item_id>\d+)/delete/$' % prefix_change , self . admin_site . admin_view ( self . tree_admin . item_delete ) , name = get_tree_item_url_name ( 'delete' ) ) , url ( r'^((?P<tree_id>\d+)/)?%sitem_(?P<item_id>\d+)/history/$' % prefix_change , self . admin_site . admin_view ( self . tree_admin . item_history ) , name = get_tree_item_url_name ( 'history' ) ) , url ( r'^(?P<tree_id>\d+)/%sitem_(?P<item_id>\d+)/move_(?P<direction>(up|down))/$' % prefix_change , self . admin_site . admin_view ( self . tree_admin . item_move ) , name = get_tree_item_url_name ( 'move' ) ) , ] if not DJANGO_POST_19 : sitetree_urls = patterns_func ( '' , * sitetree_urls ) if SMUGGLER_INSTALLED : sitetree_urls += ( url ( r'^dump_all/$' , self . admin_site . admin_view ( self . dump_view ) , name = 'sitetree_dump' ) , ) return sitetree_urls + urls
async def asgi_send ( self , message : dict ) -> None : if message [ "type" ] == "http.response.start" and self . state == ASGIHTTPState . REQUEST : self . response = message elif message [ "type" ] == "http.response.body" and self . state in { ASGIHTTPState . REQUEST , ASGIHTTPState . RESPONSE , } : if self . state == ASGIHTTPState . REQUEST : headers = build_and_validate_headers ( self . response [ "headers" ] ) headers . extend ( self . response_headers ( ) ) await self . asend ( h11 . Response ( status_code = int ( self . response [ "status" ] ) , headers = headers ) ) self . state = ASGIHTTPState . RESPONSE if ( not suppress_body ( self . scope [ "method" ] , int ( self . response [ "status" ] ) ) and message . get ( "body" , b"" ) != b"" ) : await self . asend ( h11 . Data ( data = bytes ( message [ "body" ] ) ) ) if not message . get ( "more_body" , False ) : if self . state != ASGIHTTPState . CLOSED : await self . asend ( h11 . EndOfMessage ( ) ) await self . asgi_put ( { "type" : "http.disconnect" } ) self . state = ASGIHTTPState . CLOSED else : raise UnexpectedMessage ( self . state , message [ "type" ] )
async def asgi_send ( self , message : dict ) -> None : if message [ "type" ] == "websocket.accept" and self . state == ASGIWebsocketState . HANDSHAKE : headers = build_and_validate_headers ( message . get ( "headers" , [ ] ) ) raise_if_subprotocol_present ( headers ) headers . extend ( self . response_headers ( ) ) await self . asend ( AcceptConnection ( extensions = [ PerMessageDeflate ( ) ] , extra_headers = headers , subprotocol = message . get ( "subprotocol" ) , ) ) self . state = ASGIWebsocketState . CONNECTED self . config . access_logger . access ( self . scope , { "status" : 101 , "headers" : [ ] } , time ( ) - self . start_time ) elif ( message [ "type" ] == "websocket.http.response.start" and self . state == ASGIWebsocketState . HANDSHAKE ) : self . response = message self . config . access_logger . access ( self . scope , self . response , time ( ) - self . start_time ) elif message [ "type" ] == "websocket.http.response.body" and self . state in { ASGIWebsocketState . HANDSHAKE , ASGIWebsocketState . RESPONSE , } : await self . _asgi_send_rejection ( message ) elif message [ "type" ] == "websocket.send" and self . state == ASGIWebsocketState . CONNECTED : data : Union [ bytes , str ] if message . get ( "bytes" ) is not None : await self . asend ( BytesMessage ( data = bytes ( message [ "bytes" ] ) ) ) elif not isinstance ( message [ "text" ] , str ) : raise TypeError ( f"{message['text']} should be a str" ) else : await self . asend ( TextMessage ( data = message [ "text" ] ) ) elif message [ "type" ] == "websocket.close" and self . state == ASGIWebsocketState . HANDSHAKE : await self . send_http_error ( 403 ) self . state = ASGIWebsocketState . HTTPCLOSED elif message [ "type" ] == "websocket.close" : await self . asend ( CloseConnection ( code = int ( message [ "code" ] ) ) ) self . state = ASGIWebsocketState . CLOSED else : raise UnexpectedMessage ( self . state , message [ "type" ] )
def update_binary_annotations ( self , extra_annotations ) : if not self . logging_context : self . binary_annotations . update ( extra_annotations ) else : self . logging_context . tags . update ( extra_annotations )
def encode_span ( self , v2_span ) : span = v2_span . build_v1_span ( ) thrift_endpoint = thrift . create_endpoint ( span . endpoint . port , span . endpoint . service_name , span . endpoint . ipv4 , span . endpoint . ipv6 , ) thrift_annotations = thrift . annotation_list_builder ( span . annotations , thrift_endpoint , ) thrift_binary_annotations = thrift . binary_annotation_list_builder ( span . binary_annotations , thrift_endpoint , ) if v2_span . remote_endpoint : self . encode_remote_endpoint ( v2_span . remote_endpoint , v2_span . kind , thrift_binary_annotations , ) thrift_span = thrift . create_span ( span . id , span . parent_id , span . trace_id , span . name , thrift_annotations , thrift_binary_annotations , span . timestamp , span . duration , ) encoded_span = thrift . span_to_bytes ( thrift_span ) return encoded_span
def encode_span ( self , v2_span ) : span = v2_span . build_v1_span ( ) json_span = { 'traceId' : span . trace_id , 'name' : span . name , 'id' : span . id , 'annotations' : [ ] , 'binaryAnnotations' : [ ] , } if span . parent_id : json_span [ 'parentId' ] = span . parent_id if span . timestamp : json_span [ 'timestamp' ] = int ( span . timestamp * 1000000 ) if span . duration : json_span [ 'duration' ] = int ( span . duration * 1000000 ) v1_endpoint = self . _create_json_endpoint ( span . endpoint , True ) for key , timestamp in span . annotations . items ( ) : json_span [ 'annotations' ] . append ( { 'endpoint' : v1_endpoint , 'timestamp' : int ( timestamp * 1000000 ) , 'value' : key , } ) for key , value in span . binary_annotations . items ( ) : json_span [ 'binaryAnnotations' ] . append ( { 'key' : key , 'value' : value , 'endpoint' : v1_endpoint , } ) if v2_span . remote_endpoint : self . encode_remote_endpoint ( v2_span . remote_endpoint , v2_span . kind , json_span [ 'binaryAnnotations' ] , ) encoded_span = json . dumps ( json_span ) return encoded_span
def encode_span ( self , span ) : json_span = { 'traceId' : span . trace_id , 'id' : span . span_id , } if span . name : json_span [ 'name' ] = span . name if span . parent_id : json_span [ 'parentId' ] = span . parent_id if span . timestamp : json_span [ 'timestamp' ] = int ( span . timestamp * 1000000 ) if span . duration : json_span [ 'duration' ] = int ( span . duration * 1000000 ) if span . shared is True : json_span [ 'shared' ] = True if span . kind and span . kind . value is not None : json_span [ 'kind' ] = span . kind . value if span . local_endpoint : json_span [ 'localEndpoint' ] = self . _create_json_endpoint ( span . local_endpoint , False , ) if span . remote_endpoint : json_span [ 'remoteEndpoint' ] = self . _create_json_endpoint ( span . remote_endpoint , False , ) if span . tags and len ( span . tags ) > 0 : json_span [ 'tags' ] = span . tags if span . annotations : json_span [ 'annotations' ] = [ { 'timestamp' : int ( timestamp * 1000000 ) , 'value' : key , } for key , timestamp in span . annotations . items ( ) ] encoded_span = json . dumps ( json_span ) return encoded_span
def fits ( self , current_count , current_size , max_size , new_span ) : return current_size + len ( new_span ) <= max_size
def encode_span ( self , span ) : if not protobuf . installed ( ) : raise ZipkinError ( 'protobuf encoding requires installing the protobuf\'s extra ' 'requirements. Use py-zipkin[protobuf] in your requirements.txt.' ) pb_span = protobuf . create_protobuf_span ( span ) return protobuf . encode_pb_list ( [ pb_span ] )
def join_lines ( string , strip = Strip . BOTH ) : lines = [ ] for line in string . splitlines ( ) : if strip & Strip . RIGHT : line = line . rstrip ( ) if strip & Strip . LEFT : line = line . lstrip ( ) lines . append ( line ) return '' . join ( lines )
async def json_or_text ( response ) : text = await response . text ( ) if response . headers [ 'Content-Type' ] == 'application/json; charset=utf-8' : return json . loads ( text ) return text
async def limited ( until ) : duration = int ( round ( until - time . time ( ) ) ) mins = duration / 60 fmt = 'We have exhausted a ratelimit quota. Retrying in %.2f seconds (%.3f minutes).' log . warn ( fmt , duration , mins )
async def request ( self , method , url , * * kwargs ) : rate_limiter = RateLimiter ( max_calls = 59 , period = 60 , callback = limited ) async with rate_limiter : if not self . token : raise UnauthorizedDetected ( 'UnauthorizedDetected (status code: 401): No TOKEN provided' ) headers = { 'User-Agent' : self . user_agent , 'Content-Type' : 'application/json' } if 'json' in kwargs : kwargs [ 'data' ] = to_json ( kwargs . pop ( 'json' ) ) kwargs [ 'headers' ] = headers headers [ 'Authorization' ] = self . token for tries in range ( 5 ) : async with self . session . request ( method , url , * * kwargs ) as resp : log . debug ( '%s %s with %s has returned %s' , method , url , kwargs . get ( 'data' ) , resp . status ) data = await json_or_text ( resp ) if 300 > resp . status >= 200 : return data if resp . status == 429 : fmt = 'We are being rate limited. Retrying in %.2f seconds (%.3f minutes).' retry_after = json . loads ( resp . headers . get ( 'Retry-After' ) ) mins = retry_after / 60 log . warning ( fmt , retry_after , mins ) is_global = True if is_global : self . _global_over . clear ( ) await asyncio . sleep ( retry_after , loop = self . loop ) log . debug ( 'Done sleeping for the rate limit. Retrying...' ) if is_global : self . _global_over . set ( ) log . debug ( 'Global rate limit is now over.' ) continue if resp . status == 400 : raise HTTPException ( resp , data ) elif resp . status == 401 : raise Unauthorized ( resp , data ) elif resp . status == 403 : raise Forbidden ( resp , data ) elif resp . status == 404 : raise NotFound ( resp , data ) else : raise HTTPException ( resp , data ) raise HTTPException ( resp , data )
async def get_bot_info ( self , bot_id ) : resp = await self . request ( 'GET' , '{}/bots/{}' . format ( self . BASE , bot_id ) ) resp [ 'date' ] = datetime . strptime ( resp [ 'date' ] , '%Y-%m-%dT%H:%M:%S.%fZ' ) for k in resp : if resp [ k ] == '' : resp [ k ] = None return resp
async def get_bots ( self , limit , offset ) : if limit > 500 : limit = 50 return await self . request ( 'GET' , '{}/bots?limit={}&offset={}' . format ( self . BASE , limit , offset ) )
def read ( self ) : packet = self . packet with self . __read_lock : buffer = self . __buffer while len ( buffer ) < packet : buffer += self . _read_data ( ) length = self . __unpack ( buffer [ : packet ] ) [ 0 ] + packet while len ( buffer ) < length : buffer += self . _read_data ( ) term , self . __buffer = decode ( buffer [ packet : ] ) return term
def write ( self , message ) : data = encode ( message , compressed = self . compressed ) length = len ( data ) data = self . __pack ( length ) + data with self . __write_lock : while data : try : n = os . write ( self . out_d , data ) except OSError as why : if why . errno in ( errno . EPIPE , errno . EINVAL ) : raise EOFError ( ) raise if not n : raise EOFError ( ) data = data [ n : ] return length + self . packet
def decode ( string ) : if not string : raise IncompleteData ( string ) if string [ 0 ] != 131 : raise ValueError ( "unknown protocol version: %r" % string [ 0 ] ) if string [ 1 : 2 ] == b'P' : if len ( string ) < 16 : raise IncompleteData ( string ) d = decompressobj ( ) term_string = d . decompress ( string [ 6 : ] ) + d . flush ( ) uncompressed_size , = _int4_unpack ( string [ 2 : 6 ] ) if len ( term_string ) != uncompressed_size : raise ValueError ( "invalid compressed tag, " "%d bytes but got %d" % ( uncompressed_size , len ( term_string ) ) ) term , _tail = decode_term ( term_string ) return term , d . unused_data return decode_term ( string [ 1 : ] )
def encode ( term , compressed = False ) : encoded_term = encode_term ( term ) if compressed : if compressed is True : compressed = 6 elif compressed < 0 or compressed > 9 : raise ValueError ( "invalid compression level: %r" % ( compressed , ) ) zlib_term = compress ( encoded_term , compressed ) ln = len ( encoded_term ) if len ( zlib_term ) + 5 <= ln : return b"\x83P" + _int4_pack ( ln ) + zlib_term return b"\x83" + encoded_term
def addSourceAddr ( self , addr ) : try : self . _multiInSocket . setsockopt ( socket . IPPROTO_IP , socket . IP_ADD_MEMBERSHIP , self . _makeMreq ( addr ) ) except socket . error : pass sock = self . _createMulticastOutSocket ( addr , self . _observer . ttl ) self . _multiOutUniInSockets [ addr ] = sock self . _poll . register ( sock , select . POLLIN )
def _sendPendingMessages ( self ) : if len ( self . _queue ) == 0 : time . sleep ( 0.1 ) return msg = self . _queue . pop ( 0 ) if msg . canSend ( ) : self . _sendMsg ( msg ) msg . refresh ( ) if not ( msg . isFinished ( ) ) : self . _queue . append ( msg ) else : self . _queue . append ( msg ) time . sleep ( 0.01 )
def stop ( self ) : self . clearRemoteServices ( ) self . clearLocalServices ( ) self . _stopThreads ( ) self . _serverStarted = False
def clearLocalServices ( self ) : for service in list ( self . _localServices . values ( ) ) : self . _sendBye ( service ) self . _localServices . clear ( )
def searchServices ( self , types = None , scopes = None , timeout = 3 ) : if not self . _serverStarted : raise Exception ( "Server not started" ) self . _sendProbe ( types , scopes ) time . sleep ( timeout ) return self . _filterServices ( list ( self . _remoteServices . values ( ) ) , types , scopes )
def createSOAPMessage ( env ) : if env . getAction ( ) == ACTION_PROBE : return createProbeMessage ( env ) if env . getAction ( ) == ACTION_PROBE_MATCH : return createProbeMatchMessage ( env ) if env . getAction ( ) == ACTION_RESOLVE : return createResolveMessage ( env ) if env . getAction ( ) == ACTION_RESOLVE_MATCH : return createResolveMatchMessage ( env ) if env . getAction ( ) == ACTION_HELLO : return createHelloMessage ( env ) if env . getAction ( ) == ACTION_BYE : return createByeMessage ( env )
def discover ( scope , loglevel , capture ) : if loglevel : level = getattr ( logging , loglevel , None ) if not level : print ( "Invalid log level '%s'" % loglevel ) return logger . setLevel ( level ) run ( scope = scope , capture = capture )
def save ( self , * * kwargs ) : child_relation_names = [ rel . get_accessor_name ( ) for rel in get_all_child_relations ( self ) ] child_m2m_field_names = [ field . name for field in get_all_child_m2m_relations ( self ) ] update_fields = kwargs . pop ( 'update_fields' , None ) if update_fields is None : real_update_fields = None relations_to_commit = child_relation_names m2m_fields_to_commit = child_m2m_field_names else : real_update_fields = [ ] relations_to_commit = [ ] m2m_fields_to_commit = [ ] for field in update_fields : if field in child_relation_names : relations_to_commit . append ( field ) elif field in child_m2m_field_names : m2m_fields_to_commit . append ( field ) else : real_update_fields . append ( field ) super ( ClusterableModel , self ) . save ( update_fields = real_update_fields , * * kwargs ) for relation in relations_to_commit : getattr ( self , relation ) . commit ( ) for field in m2m_fields_to_commit : getattr ( self , field ) . commit ( )
def validate_unique ( self ) : all_unique_checks = set ( ) all_date_checks = set ( ) forms_to_delete = self . deleted_forms valid_forms = [ form for form in self . forms if form . is_valid ( ) and form not in forms_to_delete ] for form in valid_forms : unique_checks , date_checks = form . instance . _get_unique_checks ( ) all_unique_checks . update ( unique_checks ) all_date_checks . update ( date_checks ) errors = [ ] for uclass , unique_check in all_unique_checks : seen_data = set ( ) for form in valid_forms : row_data = ( field if field in self . unique_fields else form . cleaned_data [ field ] for field in unique_check if field in form . cleaned_data ) row_data = tuple ( d . _get_pk_val ( ) if hasattr ( d , '_get_pk_val' ) else d for d in row_data ) if row_data and None not in row_data : if row_data in seen_data : errors . append ( self . get_unique_error_message ( unique_check ) ) form . _errors [ NON_FIELD_ERRORS ] = self . error_class ( [ self . get_form_error ( ) ] ) for field in unique_check : if field in form . cleaned_data : del form . cleaned_data [ field ] seen_data . add ( row_data ) if errors : raise ValidationError ( errors )
def has_changed ( self ) : if self . formsets : for formset in self . formsets . values ( ) : for form in formset . forms : if form . has_changed ( ) : return True return bool ( self . changed_data )
def with_valid_checksum ( self ) : return Address ( trytes = self . address + self . _generate_checksum ( ) , balance = self . balance , key_index = self . key_index , security_level = self . security_level , )
def _generate_checksum ( self ) : checksum_trits = [ ] sponge = Kerl ( ) sponge . absorb ( self . address . as_trits ( ) ) sponge . squeeze ( checksum_trits ) checksum_length = AddressChecksum . LEN * TRITS_PER_TRYTE return AddressChecksum . from_trits ( checksum_trits [ - checksum_length : ] )
def prompt_for_seed ( ) : seed = secure_input ( 'Enter seed and press return (typing will not be shown).\n' 'If no seed is specified, a random one will be used instead.\n' ) if isinstance ( seed , text_type ) : seed = seed . encode ( 'ascii' ) return Seed ( seed ) if seed else Seed . random ( )
def _create_sponge ( self , index ) : seed = self . seed_as_trits [ : ] sponge = Kerl ( ) sponge . absorb ( add_trits ( seed , trits_from_int ( index ) ) ) sponge . squeeze ( seed ) sponge . reset ( ) sponge . absorb ( seed ) return sponge
def _transform ( self ) : # # state_length = STATE_LENGTH truth_table = TRUTH_TABLE # # prev_state = self . _state [ : ] new_state = prev_state [ : ] index = 0 for _ in range ( NUMBER_OF_ROUNDS ) : prev_trit = prev_state [ index ] for pos in range ( state_length ) : index += ( 364 if index < 365 else - 365 ) new_trit = prev_state [ index ] new_state [ pos ] = truth_table [ prev_trit + ( 3 * new_trit ) + 4 ] prev_trit = new_trit prev_state = new_state new_state = new_state [ : ] self . _state = new_state
def _full_add_trits ( left , right , carry ) : sum_both = _add_trits ( left , right ) cons_left = _cons_trits ( left , right ) cons_right = _cons_trits ( sum_both , carry ) return _add_trits ( sum_both , carry ) , _any_trits ( cons_left , cons_right )
def resolve_adapter ( uri ) : if isinstance ( uri , BaseAdapter ) : return uri parsed = compat . urllib_parse . urlsplit ( uri ) if not parsed . scheme : raise with_context ( exc = InvalidUri ( 'URI must begin with "<protocol>://" (e.g., "udp://").' , ) , context = { 'parsed' : parsed , 'uri' : uri , } , ) try : adapter_type = adapter_registry [ parsed . scheme ] except KeyError : raise with_context ( exc = InvalidUri ( 'Unrecognized protocol {protocol!r}.' . format ( protocol = parsed . scheme , ) ) , context = { 'parsed' : parsed , 'uri' : uri , } , ) return adapter_type . configure ( parsed )
def _log ( self , level , message , context = None ) : if self . _logger : self . _logger . log ( level , message , extra = { 'context' : context or { } } )
def address_from_digest ( digest ) : address_trits = [ 0 ] * ( Address . LEN * TRITS_PER_TRYTE ) sponge = Kerl ( ) sponge . absorb ( digest . as_trits ( ) ) sponge . squeeze ( address_trits ) return Address . from_trits ( trits = address_trits , key_index = digest . key_index , security_level = digest . security_level , )
def encode ( self , input , errors = 'strict' ) : if isinstance ( input , memoryview ) : input = input . tobytes ( ) if not isinstance ( input , ( binary_type , bytearray ) ) : raise with_context ( exc = TypeError ( "Can't encode {type}; byte string expected." . format ( type = type ( input ) . __name__ , ) ) , context = { 'input' : input , } , ) if not isinstance ( input , bytearray ) : input = bytearray ( input ) trytes = bytearray ( ) for c in input : second , first = divmod ( c , len ( self . alphabet ) ) trytes . append ( self . alphabet [ first ] ) trytes . append ( self . alphabet [ second ] ) return binary_type ( trytes ) , len ( input )
def decode ( self , input , errors = 'strict' ) : if isinstance ( input , memoryview ) : input = input . tobytes ( ) if not isinstance ( input , ( binary_type , bytearray ) ) : raise with_context ( exc = TypeError ( "Can't decode {type}; byte string expected." . format ( type = type ( input ) . __name__ , ) ) , context = { 'input' : input , } , ) if not isinstance ( input , bytearray ) : input = bytearray ( input ) bytes_ = bytearray ( ) for i in range ( 0 , len ( input ) , 2 ) : try : first , second = input [ i : i + 2 ] except ValueError : if errors == 'strict' : raise with_context ( exc = TrytesDecodeError ( "'{name}' codec can't decode value; " "tryte sequence has odd length." . format ( name = self . name , ) , ) , context = { 'input' : input , } , ) elif errors == 'replace' : bytes_ += b'?' continue try : bytes_ . append ( self . index [ first ] + ( self . index [ second ] * len ( self . index ) ) ) except ValueError : if errors == 'strict' : raise with_context ( exc = TrytesDecodeError ( "'{name}' codec can't decode trytes {pair} " "at position {i}-{j}: " "ordinal not in range(255)" . format ( name = self . name , pair = chr ( first ) + chr ( second ) , i = i , j = i + 1 , ) , ) , context = { 'input' : input , } ) elif errors == 'replace' : bytes_ += b'?' return binary_type ( bytes_ ) , len ( input )
def _find_addresses ( self , seed , index , count , security_level , checksum ) : generator = AddressGenerator ( seed , security_level , checksum ) if count is None : for addy in generator . create_iterator ( start = index ) : response = FindTransactionsCommand ( self . adapter ) ( addresses = [ addy . address ] , ) if not response . get ( 'hashes' ) : return [ addy ] return generator . get_addresses ( start = index , count = count )
def as_tryte_string ( self ) : return TransactionTrytes ( self . signature_message_fragment + self . address . address + self . value_as_trytes + self . legacy_tag + self . timestamp_as_trytes + self . current_index_as_trytes + self . last_index_as_trytes + self . bundle_hash + self . trunk_transaction_hash + self . branch_transaction_hash + self . tag + self . attachment_timestamp_as_trytes + self . attachment_timestamp_lower_bound_as_trytes + self . attachment_timestamp_upper_bound_as_trytes + self . nonce )
def is_confirmed ( self , new_is_confirmed ) : self . _is_confirmed = new_is_confirmed for txn in self : txn . is_confirmed = new_is_confirmed
def group_transactions ( self ) : groups = [ ] if self : last_txn = self . tail_transaction current_group = [ last_txn ] for current_txn in self . transactions [ 1 : ] : if current_txn . address == last_txn . address : current_group . append ( current_txn ) else : groups . append ( current_group ) current_group = [ current_txn ] last_txn = current_txn if current_group : groups . append ( current_group ) return groups
def errors ( self ) : try : self . _errors . extend ( self . _validator ) except StopIteration : pass return self . _errors
def is_valid ( self ) : if not self . _errors : try : self . _errors . append ( next ( self . _validator ) ) except StopIteration : pass return not self . _errors
def _create_validator ( self ) : grouped_transactions = self . bundle . group_transactions ( ) bundle_hash = self . bundle . hash last_index = len ( self . bundle ) - 1 balance = 0 counter = 0 for group in grouped_transactions : for txn in group : balance += txn . value if txn . bundle_hash != bundle_hash : yield 'Transaction {i} has invalid bundle hash.' . format ( i = counter , ) if txn . current_index != counter : yield ( 'Transaction {i} has invalid current index value ' '(expected {i}, actual {actual}).' . format ( actual = txn . current_index , i = counter , ) ) if txn . last_index != last_index : yield ( 'Transaction {i} has invalid last index value ' '(expected {expected}, actual {actual}).' . format ( actual = txn . last_index , expected = last_index , i = counter , ) ) counter += 1 if balance != 0 : yield ( 'Bundle has invalid balance ' '(expected 0, actual {actual}).' . format ( actual = balance , ) ) if not self . _errors : signature_validation_queue = [ ] for group in grouped_transactions : if group [ 0 ] . value >= 0 : continue validate_group_signature = True for j , txn in enumerate ( group ) : if ( j > 0 ) and ( txn . value != 0 ) : yield ( 'Transaction {i} has invalid value ' '(expected 0, actual {actual}).' . format ( actual = txn . value , i = txn . current_index , ) ) validate_group_signature = False continue # # # if validate_group_signature : signature_validation_queue . append ( group ) if signature_validation_queue : for error in self . _get_bundle_signature_errors ( signature_validation_queue ) : yield error
def _start_repl ( api ) : banner = ( 'IOTA API client for {uri} ({testnet}) ' 'initialized as variable `api`.\n' 'Type `help(api)` for list of API commands.' . format ( testnet = 'testnet' if api . testnet else 'mainnet' , uri = api . adapter . get_uri ( ) , ) ) scope_vars = { 'api' : api } try : import IPython except ImportError : from code import InteractiveConsole InteractiveConsole ( locals = scope_vars ) . interact ( banner , '' ) else : print ( banner ) IPython . start_ipython ( argv = [ ] , user_ns = scope_vars )
def SecurityLevel ( ) : return ( f . Type ( int ) | f . Min ( 1 ) | f . Max ( 3 ) | f . Optional ( default = AddressGenerator . DEFAULT_SECURITY_LEVEL ) )
def as_tryte_string ( self ) : if not self . bundle_hash : raise with_context ( exc = RuntimeError ( 'Cannot get TryteString representation of {cls} instance ' 'without a bundle hash; call ``bundle.finalize()`` first ' '(``exc.context`` has more info).' . format ( cls = type ( self ) . __name__ , ) , ) , context = { 'transaction' : self , } , ) return super ( ProposedTransaction , self ) . as_tryte_string ( )
def tag ( self ) : for txn in reversed ( self ) : if txn . tag : return txn . tag return Tag ( b'' )
def finalize ( self ) : if self . hash : raise RuntimeError ( 'Bundle is already finalized.' ) if not self : raise ValueError ( 'Bundle has no transactions.' ) balance = self . balance if balance < 0 : if self . change_address : self . add_transaction ( ProposedTransaction ( address = self . change_address , value = - balance , tag = self . tag , ) ) else : raise ValueError ( 'Bundle has unspent inputs (balance: {balance}); ' 'use ``send_unspent_inputs_to`` to create ' 'change transaction.' . format ( balance = balance , ) , ) elif balance > 0 : raise ValueError ( 'Inputs are insufficient to cover bundle spend ' '(balance: {balance}).' . format ( balance = balance , ) , ) while True : sponge = Kerl ( ) last_index = len ( self ) - 1 for i , txn in enumerate ( self ) : txn . current_index = i txn . last_index = last_index sponge . absorb ( txn . get_signature_validation_trytes ( ) . as_trits ( ) ) bundle_hash_trits = [ 0 ] * HASH_LENGTH sponge . squeeze ( bundle_hash_trits ) bundle_hash = BundleHash . from_trits ( bundle_hash_trits ) if any ( 13 in part for part in normalize ( bundle_hash ) ) : tail_transaction = ( self . tail_transaction ) tail_transaction . increment_legacy_tag ( ) else : break for txn in self : txn . bundle_hash = bundle_hash txn . signature_message_fragment = Fragment ( txn . message or b'' )
def sign_inputs ( self , key_generator ) : if not self . hash : raise RuntimeError ( 'Cannot sign inputs until bundle is finalized.' ) i = 0 while i < len ( self ) : txn = self [ i ] if txn . value < 0 : if txn . address . key_index is None : raise with_context ( exc = ValueError ( 'Unable to sign input {input}; ' '``key_index`` is None ' '(``exc.context`` has more info).' . format ( input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) if txn . address . security_level is None : raise with_context ( exc = ValueError ( 'Unable to sign input {input}; ' '``security_level`` is None ' '(``exc.context`` has more info).' . format ( input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) self . sign_input_at ( i , key_generator . get_key_for ( txn . address ) ) i += txn . address . security_level else : i += 1
def _create_input_transactions ( self , addy ) : self . _transactions . append ( ProposedTransaction ( address = addy , tag = self . tag , value = - addy . balance , ) ) for _ in range ( addy . security_level - 1 ) : self . _transactions . append ( ProposedTransaction ( address = addy , tag = self . tag , value = 0 , ) )
def decompress_G1 ( z : G1Compressed ) -> G1Uncompressed : b_flag = ( z % POW_2_383 ) // POW_2_382 if b_flag == 1 : return Z1 x = z % POW_2_381 y = pow ( ( x ** 3 + b . n ) % q , ( q + 1 ) // 4 , q ) if pow ( y , 2 , q ) != ( x ** 3 + b . n ) % q : raise ValueError ( "The given point is not on G1: y**2 = x**3 + b" ) a_flag = ( z % POW_2_382 ) // POW_2_381 if ( y * 2 ) // q != a_flag : y = q - y return ( FQ ( x ) , FQ ( y ) , FQ ( 1 ) )
def prime_field_inv ( a : int , n : int ) -> int : if a == 0 : return 0 lm , hm = 1 , 0 low , high = a % n , n while low > 1 : r = high // low nm , new = hm - lm * r , high - low * r lm , low , hm , high = nm , new , lm , low return lm % n
def _repr_html_ ( self ) : rows , c = '' , '' s = '<tr><td><strong>{k}</strong></td><td style="{stl}">{v}</td></tr>' for k , v in self . __dict__ . items ( ) : if k == '_colour' : k = 'colour' c = utils . text_colour_for_hex ( v ) style = 'color:{}; background-color:{}' . format ( c , v ) else : style = 'color:black; background-color:white' if k == 'component' : try : v = v . _repr_html_ ( ) except AttributeError : v = v . __repr__ ( ) rows += s . format ( k = k , v = v , stl = style ) html = '<table>{}</table>' . format ( rows ) return html
def random ( cls , component ) : colour = random . sample ( [ i for i in range ( 256 ) ] , 3 ) return cls ( { 'colour' : colour , 'component' : component , 'width' : 1.0 } )
def _repr_html_ ( self ) : all_keys = list ( set ( itertools . chain ( * [ d . keys for d in self ] ) ) ) rows = '' for decor in self : th , tr = decor . _repr_html_row_ ( keys = all_keys ) rows += '<tr>{}</tr>' . format ( tr ) header = '<tr>{}</tr>' . format ( th ) html = '<table>{}{}</table>' . format ( header , rows ) return html
def _repr_html_ ( self ) : rows = '' s = '<tr><td><strong>{k}</strong></td><td>{v}</td></tr>' for k , v in self . __dict__ . items ( ) : rows += s . format ( k = k , v = v ) html = '<table>{}</table>' . format ( rows ) return html
def Rock ( * args , * * kwargs ) : with warnings . catch_warnings ( ) : warnings . simplefilter ( "always" ) w = "The 'Rock' class was renamed 'Component'. " w += "Please update your code." warnings . warn ( w , DeprecationWarning , stacklevel = 2 ) return Component ( * args , * * kwargs )
def _process_row ( text , columns ) : if not text : return coldict = { k : { 'start' : s , 'len' : l , 'read' : r , 'write' : w } for k , ( s , l , r , w ) in columns . items ( ) } item = { } for field in coldict : value = _get_field ( text , coldict , field ) if value is not None : item [ field ] = value return item
def parse_canstrat ( text ) : result = { } for row in text . split ( '\n' ) : if not row : continue if len ( row ) < 8 : continue row_header = _process_row ( row , columns_ ) or { 'card' : None } card = row_header [ 'card' ] if card is not None : item = _process_row ( row , columns [ card ] ) this_list = result . get ( card , [ ] ) this_list . append ( item ) result [ card ] = this_list for c , d in result . items ( ) : if len ( d ) == 1 : result [ c ] = d [ 0 ] return result
def get_template ( name ) : text = re . sub ( r'\r\n' , r'\n' , name ) text = re . sub ( r'\{([FISDE°].*?)\}',   '{{\1}}',   ext)  return text
def _clean_longitudinal_data ( cls , data , null = None ) : if ( 'top' not in data . keys ( ) ) : data [ 'top' ] = data . pop ( 'depth' , data . pop ( 'MD' , None ) ) idx = list ( data . keys ( ) ) . index ( 'top' ) values = sorted ( zip ( * data . values ( ) ) , key = lambda x : x [ idx ] ) data = { k : list ( v ) for k , v in zip ( data . keys ( ) , zip ( * values ) ) } if data [ 'top' ] is None : raise StriplogError ( 'Could not get tops.' ) if null is not None : for k , v in data . items ( ) : data [ k ] = [ i if i != null else None for i in v ] return data
def from_csv ( cls , filename = None , text = None , dlm = ',' , lexicon = None , points = False , include = None , exclude = None , remap = None , function = None , null = None , ignore = None , source = None , stop = None , fieldnames = None ) : if ( filename is None ) and ( text is None ) : raise StriplogError ( "You must provide a filename or CSV text." ) if ( filename is not None ) : if source is None : source = filename with open ( filename , 'r' ) as f : text = f . read ( ) source = source or 'CSV' if dlm == ' ' : text = re . sub ( r'[ \t]+' , ' ' , text ) if fieldnames is not None : text = dlm . join ( fieldnames ) + '\n' + text try : f = StringIO ( text ) except TypeError : f = StringIO ( unicode ( text ) ) reader = csv . DictReader ( f , delimiter = dlm ) reorg = { k . strip ( ) . lower ( ) : [ ] for k in reader . fieldnames if k is not None } t = f . tell ( ) for key in reorg : f . seek ( t ) for r in reader : s = { k . strip ( ) . lower ( ) : v . strip ( ) for k , v in r . items ( ) } try : reorg [ key ] . append ( float ( s [ key ] ) ) except ValueError : reorg [ key ] . append ( s [ key ] ) f . close ( ) remap = remap or { } for k , v in remap . items ( ) : reorg [ v ] = reorg . pop ( k ) data = cls . _clean_longitudinal_data ( reorg , null = null ) list_of_Intervals = cls . _build_list_of_Intervals ( data , points = points , lexicon = lexicon , include = include , exclude = exclude , ignore = ignore , stop = stop ) return cls ( list_of_Intervals , source = source )
def from_img ( cls , * args , * * kwargs ) : with warnings . catch_warnings ( ) : warnings . simplefilter ( "always" ) w = "from_img() is deprecated; please use from_image()" warnings . warn ( w ) return cls . from_image ( * args , * * kwargs )
def from_canstrat ( cls , filename , source = 'canstrat' ) : with open ( filename ) as f : dat = f . read ( ) data = parse_canstrat ( dat ) list_of_Intervals = [ ] for d in data [ 7 ] : if d . pop ( 'skip' ) : continue top = d . pop ( 'top' ) base = d . pop ( 'base' ) comps = [ Component ( { 'lithology' : d [ 'rtc' ] , 'colour' : d [ 'colour_name' ] } ) ] iv = Interval ( top = top , base = base , components = comps , data = d ) list_of_Intervals . append ( iv ) return cls ( list_of_Intervals , source = source )
def copy ( self ) : return Striplog ( [ i . copy ( ) for i in self ] , order = self . order , source = self . source )
def get_data ( self , field , function = None , default = None ) : f = function or utils . null data = [ ] for iv in self : d = iv . data . get ( field ) if d is None : if default is not None : d = default else : d = np . nan data . append ( f ( d ) ) return np . array ( data )
def depth ( self , d ) : with warnings . catch_warnings ( ) : warnings . simplefilter ( "always" ) w = "depth() is deprecated; please use read_at()" warnings . warn ( w ) return self . read_at ( d )
def dict_repr_html ( dictionary ) : rows = '' s = '<tr><td><strong>{k}</strong></td><td>{v}</td></tr>' for k , v in dictionary . items ( ) : rows += s . format ( k = k , v = v ) html = '<table>{}</table>' . format ( rows ) return html
def convert_field ( self , value , conversion ) : try : s = super ( CustomFormatter , self ) return s . convert_field ( value , conversion ) except ValueError : funcs = { 's' : str , 'r' : repr , 'a' : ascii , 'u' : str . upper , 'l' : str . lower , 'c' : str . capitalize , 't' : str . title , 'm' : np . mean , 'µ':   p. m ean,  'v' : np . var , 'd' : np . std , '+' : np . sum , '∑':  n .s u m,  'x' : np . product , } return funcs . get ( conversion ) ( value )
def _get_random ( self , obj_type ) : return self . mutator [ obj_type ] [ random . randint ( 0 , self . config . level ) ]
def get_mutator ( self , obj , obj_type ) : if obj_type == unicode : obj_type = str obj = str ( obj ) return self . _get_random ( obj_type ) ( obj )
def get_string_polyglot_attack ( self , obj ) : return self . polyglot_attacks [ random . choice ( self . config . techniques ) ] % obj
def safe_unicode ( self , buf ) : tmp = "" buf = "" . join ( b for b in buf ) for character in buf : tmp += character return tmp
def custom_html ( self , filepath ) : try : response . headers . append ( "Access-Control-Allow-Origin" , "*" ) response . headers . append ( "Accept-Encoding" , "identity" ) response . headers . append ( "Content-Type" , "text/html" ) return static_file ( filepath , root = self . config . html ) except Exception as e : raise PJFBaseException ( e . message if hasattr ( e , "message" ) else str ( e ) )
def serve ( self ) : try : fuzzed = self . json . fuzzed if self . config . fuzz_web : self . client_queue . put ( ( request . environ . get ( 'REMOTE_ADDR' ) , fuzzed ) ) response . headers . append ( "Access-Control-Allow-Origin" , "*" ) response . headers . append ( "Accept-Encoding" , "identity" ) response . headers . append ( "Content-Type" , self . config . content_type ) if self . config . notify : PJFTestcaseServer . send_testcase ( fuzzed , '127.0.0.1' , self . config . ports [ "servers" ] [ "TCASE_PORT" ] ) yield fuzzed except Exception as e : raise PJFBaseException ( e . message if hasattr ( e , "message" ) else str ( e ) )
def fuzz ( self , obj ) : decorators = self . decorators @ decorators . mutate_object_decorate def mutate ( ) : return obj return mutate ( )
def spawn ( self , cmd , stdin_content = "" , stdin = False , shell = False , timeout = 2 ) : try : if type ( cmd ) != list : raise PJFInvalidType ( type ( cmd ) , list ) if type ( stdin_content ) != str : raise PJFInvalidType ( type ( stdin_content ) , str ) if type ( stdin ) != bool : raise PJFInvalidType ( type ( stdin ) , bool ) self . _in = stdin_content try : self . process = subprocess . Popen ( cmd , stdout = PIPE , stderr = PIPE , stdin = PIPE , shell = shell ) self . finish_read ( timeout , stdin_content , stdin ) if self . process . poll ( ) is not None : self . close ( ) except KeyboardInterrupt : return except OSError : raise PJFProcessExecutionError ( "Binary <%s> does not exist" % cmd [ 0 ] ) except Exception as e : raise PJFBaseException ( e . message if hasattr ( e , "message" ) else str ( e ) )
def get_output ( self , stdin_content , stdin ) : try : if stdin : if sys . version_info >= ( 3 , 0 ) : self . process . stdin . write ( bytes ( stdin_content , "utf-8" ) ) else : self . process . stdin . write ( stdin_content ) self . _out = self . process . communicate ( ) [ 0 ] except ( error , IOError ) : self . _out = self . _in pass
def finish_read ( self , timeout = 2 , stdin_content = "" , stdin = False ) : process = Thread ( target = self . get_output , args = ( stdin_content , stdin ) ) process . start ( ) if timeout > 0 : process . join ( timeout ) else : process . join ( ) if process . is_alive ( ) : self . close ( ) self . return_code = - signal . SIGHUP else : self . return_code = self . process . returncode
def close ( self ) : try : self . process . terminate ( ) self . return_code = self . process . returncode except OSError : pass self . process . stdin . close ( ) self . process . stdout . close ( ) self . process . stderr . close ( ) self . logger . debug ( "[{0}] - PJFExecutor successfully completed" . format ( time . strftime ( "%H:%M:%S" ) ) )
def start ( self ) : from . pjf_worker import PJFWorker worker = PJFWorker ( self ) if self . update_pjf : worker . update_library ( ) elif self . browser_auto : worker . browser_autopwn ( ) elif self . fuzz_web : worker . web_fuzzer ( ) elif self . json : if not self . web_server and not self . ext_fuzz and not self . cmd_fuzz : worker . fuzz ( ) elif self . ext_fuzz : if self . stdin : worker . fuzz_stdin ( ) else : worker . fuzz_command_line ( ) elif self . cmd_fuzz : if self . stdin : worker . fuzz_external ( True ) else : worker . fuzz_external ( ) else : worker . start_http_server ( ) elif self . json_file : worker . start_file_fuzz ( ) elif self . process_to_monitor : worker . start_process_monitor ( )
def execute ( self , obj ) : try : if self . config . stdin : self . spawn ( self . config . command , stdin_content = obj , stdin = True , timeout = 1 ) else : if "@@" not in self . config . command : raise PJFMissingArgument ( "Missing @@ filename indicator while using non-stdin fuzzing method" ) for x in self . config . command : if "@@" in x : self . config . command [ self . config . command . index ( x ) ] = x . replace ( "@@" , obj ) self . spawn ( self . config . command , timeout = 2 ) self . logger . debug ( "[{0}] - PJFExternalFuzzer successfully completed" . format ( time . strftime ( "%H:%M:%S" ) ) ) return self . _out except KeyboardInterrupt : return "" except Exception as e : raise PJFBaseException ( e . message if hasattr ( e , "message" ) else str ( e ) )
def shutdown ( self , * args ) : try : self . _shutdown ( ) if self . process : self . process . wait ( ) self . process . stdout . close ( ) self . process . stdin . close ( ) self . process . stderr . close ( ) self . finished = True self . send_testcase ( '' , '127.0.0.1' , self . config . ports [ "servers" ] [ "TCASE_PORT" ] ) self . logger . debug ( "[{0}] - PJFProcessMonitor successfully completed" . format ( time . strftime ( "%H:%M:%S" ) ) ) except Exception as e : raise PJFBaseException ( e . message if hasattr ( e , "message" ) else str ( e ) )
def run_and_monitor ( self ) : signal . signal ( signal . SIGINT , self . shutdown ) self . spawn ( self . config . process_to_monitor , timeout = 0 ) return self . _is_sigsegv ( self . return_code )
def start_monitor ( self , standalone = True ) : try : self . start ( ) cmdline = shlex . split ( self . config . process_to_monitor ) if standalone : signal . signal ( signal . SIGINT , self . shutdown ) self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) while self . process and not self . finished : self . process . wait ( ) if self . _is_sigsegv ( self . process . returncode ) : if self . config . debug : print ( "[\033[92mINFO\033[0m] Process crashed with \033[91mSIGSEGV\033[0m, waiting for testcase..." ) while not self . got_testcase ( ) : time . sleep ( 1 ) self . save_testcase ( self . testcase [ - 10 : ] ) if self . process : self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) except OSError : self . shutdown ( ) self . process = False self . got_testcase = lambda : True raise PJFProcessExecutionError ( "Binary <%s> does not exist" % cmdline [ 0 ] ) except Exception as e : raise PJFBaseException ( "Unknown error please send log to author" )
def fuzz_elements ( self , element ) : try : if type ( element ) == dict : tmp_element = { } for key in element : if len ( self . config . parameters ) > 0 : if self . config . exclude_parameters : fuzz = key not in self . config . parameters else : fuzz = key in self . config . parameters else : fuzz = True if fuzz : if type ( element [ key ] ) == dict : tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) elif type ( element [ key ] ) == list : tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) else : tmp_element . update ( { key : self . mutator . fuzz ( element [ key ] ) } ) else : tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) element = tmp_element del tmp_element elif type ( element ) == list : arr = [ ] for key in element : if type ( key ) == dict : arr . append ( self . fuzz_elements ( key ) ) elif type ( key ) == list : arr . append ( self . fuzz_elements ( key ) ) else : if len ( self . config . parameters ) <= 0 : arr . append ( self . mutator . fuzz ( key ) ) else : arr . append ( key ) element = arr del arr except Exception as e : raise PJFBaseException ( e . message if hasattr ( e , "message" ) else str ( e ) ) return element
def fuzzed ( self ) : try : if self . config . strong_fuzz : fuzzer = PJFMutators ( self . config ) if self . config . url_encode : if sys . version_info >= ( 3 , 0 ) : return urllib . parse . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) else : return urllib . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) else : if type ( self . config . json ) in [ list , dict ] : return fuzzer . fuzz ( json . dumps ( self . config . json ) ) else : return fuzzer . fuzz ( self . config . json ) else : if self . config . url_encode : if sys . version_info >= ( 3 , 0 ) : return urllib . parse . quote ( self . get_fuzzed ( self . config . indent , self . config . utf8 ) ) else : return urllib . quote ( self . get_fuzzed ( self . config . indent , self . config . utf8 ) ) else : return self . get_fuzzed ( self . config . indent , self . config . utf8 ) except Exception as e : raise PJFBaseException ( e . message if hasattr ( e , "message" ) else str ( e ) )
def get_fuzzed ( self , indent = False , utf8 = False ) : try : if "array" in self . json : return self . fuzz_elements ( dict ( self . json ) ) [ "array" ] else : return self . fuzz_elements ( dict ( self . json ) ) except Exception as e : raise PJFBaseException ( e . message if hasattr ( e , "message" ) else str ( e ) )
def mutate_object_decorate ( self , func ) : def mutate ( ) : obj = func ( ) return self . Mutators . get_mutator ( obj , type ( obj ) ) return mutate
def getUserId ( self ) : self . userId = self ( "GET" , "{0}/users/self/profile" . format ( self . API_USER ) , auth = self . Auth . SkypeToken ) . json ( ) . get ( "username" )
def syncEndpoints ( self ) : self . endpoints [ "all" ] = [ ] for json in self ( "GET" , "{0}/users/ME/presenceDocs/messagingService" . format ( self . msgsHost ) , params = { "view" : "expanded" } , auth = self . Auth . RegToken ) . json ( ) . get ( "endpointPresenceDocs" , [ ] ) : id = json . get ( "link" , "" ) . split ( "/" ) [ 7 ] self . endpoints [ "all" ] . append ( SkypeEndpoint ( self , id ) )
def u ( text , encoding = 'utf-8' ) : if isinstance ( text , six . binary_type ) : text = text . decode ( encoding ) text = text . replace ( '\r\n' , '\n' ) return text
def to_dict ( self ) : d = self . metadata . copy ( ) d [ 'content' ] = self . content return d
def load ( self , fm , * * kwargs ) : kwargs . setdefault ( 'Loader' , SafeLoader ) return yaml . load ( fm , * * kwargs )
def export ( self , metadata , * * kwargs ) : kwargs . setdefault ( 'Dumper' , SafeDumper ) kwargs . setdefault ( 'default_flow_style' , False ) kwargs . setdefault ( 'allow_unicode' , True ) metadata = yaml . dump ( metadata , * * kwargs ) . strip ( ) return u ( metadata )
def export ( self , metadata , * * kwargs ) : kwargs . setdefault ( 'indent' , 4 ) metadata = json . dumps ( metadata , * * kwargs ) return u ( metadata )
def _match ( self ) : cache_match , cache_string = self . _match_cache string = self . string if cache_string == string : return cache_match cache_match = fullmatch ( LIST_PATTERN_FORMAT . replace ( b'{pattern}' , self . pattern . encode ( ) ) , self . _shadow , MULTILINE , ) self . _match_cache = cache_match , string return cache_match
def convert ( self , newstart : str ) -> None : match = self . _match ms = match . start ( ) for s , e in reversed ( match . spans ( 'pattern' ) ) : self [ s - ms : e - ms ] = newstart self . pattern = escape ( newstart )
def arguments ( self ) -> List [ Argument ] : shadow = self . _shadow split_spans = self . _args_matcher ( shadow ) . spans ( 'arg' ) if not split_spans : return [ ] arguments = [ ] arguments_append = arguments . append type_to_spans = self . _type_to_spans ss , se = span = self . _span type_ = id ( span ) lststr = self . _lststr string = lststr [ 0 ] arg_spans = type_to_spans . setdefault ( type_ , [ ] ) span_tuple_to_span_get = { ( s [ 0 ] , s [ 1 ] ) : s for s in arg_spans } . get for arg_self_start , arg_self_end in split_spans : s , e = arg_span = [ ss + arg_self_start , ss + arg_self_end ] old_span = span_tuple_to_span_get ( ( s , e ) ) if old_span is None : insort ( arg_spans , arg_span ) else : arg_span = old_span arg = Argument ( lststr , type_to_spans , arg_span , type_ ) arg . _shadow_cache = ( string [ s : e ] , shadow [ arg_self_start : arg_self_end ] ) arguments_append ( arg ) return arguments
def _pattern ( trie : dict ) -> str : if '' in trie : if len ( trie ) == 1 : return '' optional = True del trie [ '' ] else : optional = False subpattern_to_chars = _defaultdict ( list ) for char , sub_trie in trie . items ( ) : subpattern = _pattern ( sub_trie ) subpattern_to_chars [ subpattern ] . append ( char ) alts = [ ] for subpattern , chars in subpattern_to_chars . items ( ) : if len ( chars ) == 1 : alts . append ( chars [ 0 ] + subpattern ) else : chars . sort ( reverse = True ) alts . append ( '[' + '' . join ( chars ) + ']' + subpattern ) if len ( alts ) == 1 : result = alts [ 0 ] if optional : if len ( result ) == 1 : result += '?+' else : result = '(?:' + result + ')?+' else : alts . sort ( reverse = True ) result = '(?>' + '|' . join ( alts ) + ')' if optional : result += '?+' return result
def _atomic_partition ( self , char : int ) -> Tuple [ str , str , str ] : s , e = self . _span index = self . _shadow . find ( char ) if index == - 1 : return self . _lststr [ 0 ] [ s : e ] , '' , '' lststr0 = self . _lststr [ 0 ] return lststr0 [ s : s + index ] , chr ( char ) , lststr0 [ s + index + 1 : e ]
def _subspans ( self , type_ : str ) -> List [ List [ int ] ] : return self . _type_to_spans [ type_ ]
def _insert_update ( self , index : int , length : int ) -> None : ss , se = self . _span for spans in self . _type_to_spans . values ( ) : for span in spans : if index < span [ 1 ] or span [ 1 ] == index == se : span [ 1 ] += length if index < span [ 0 ] or span [ 0 ] == index != ss : span [ 0 ] += length
def pprint ( self , indent : str = '    ' , remove_comments = False ) : warn ( 'pprint method is deprecated, use pformat instead.' , DeprecationWarning , ) return self . pformat ( indent , remove_comments )
def parameters ( self ) -> List [ 'Parameter' ] : _lststr = self . _lststr _type_to_spans = self . _type_to_spans return [ Parameter ( _lststr , _type_to_spans , span , 'Parameter' ) for span in self . _subspans ( 'Parameter' ) ]
def parser_functions ( self ) -> List [ 'ParserFunction' ] : _lststr = self . _lststr _type_to_spans = self . _type_to_spans return [ ParserFunction ( _lststr , _type_to_spans , span , 'ParserFunction' ) for span in self . _subspans ( 'ParserFunction' ) ]
def templates ( self ) -> List [ 'Template' ] : _lststr = self . _lststr _type_to_spans = self . _type_to_spans return [ Template ( _lststr , _type_to_spans , span , 'Template' ) for span in self . _subspans ( 'Template' ) ]
def wikilinks ( self ) -> List [ 'WikiLink' ] : _lststr = self . _lststr _type_to_spans = self . _type_to_spans return [ WikiLink ( _lststr , _type_to_spans , span , 'WikiLink' ) for span in self . _subspans ( 'WikiLink' ) ]
def comments ( self ) -> List [ 'Comment' ] : _lststr = self . _lststr _type_to_spans = self . _type_to_spans return [ Comment ( _lststr , _type_to_spans , span , 'Comment' ) for span in self . _subspans ( 'Comment' ) ]
def tables ( self ) -> List [ 'Table' ] : tables = [ ] tables_append = tables . append type_to_spans = self . _type_to_spans lststr = self . _lststr shadow = self . _shadow [ : ] ss , se = self . _span spans = type_to_spans . setdefault ( 'Table' , [ ] ) if not spans : m = True while m : m = False for m in TABLE_FINDITER ( shadow ) : ms , me = m . span ( ) span = [ ss + ms + len ( m [ 1 ] ) , ss + me ] spans . append ( span ) tables_append ( Table ( lststr , type_to_spans , span , 'Table' ) ) shadow [ ms : me ] = b'_' * ( me - ms ) return tables span_tuple_to_span_get = { ( s [ 0 ] , s [ 1 ] ) : s for s in spans } . get m = True while m : m = False for m in TABLE_FINDITER ( shadow ) : ms , me = m . span ( ) s , e = ss + ms + len ( m [ 1 ] ) , ss + me old_span = span_tuple_to_span_get ( ( s , e ) ) if old_span is None : span = [ s , e ] insort ( spans , span ) else : span = old_span tables_append ( Table ( lststr , type_to_spans , span , 'Table' ) ) shadow [ ms : me ] = b'_' * ( me - ms ) return tables
def tags ( self , name = None ) -> List [ 'Tag' ] : lststr = self . _lststr type_to_spans = self . _type_to_spans if name : if name in _tag_extensions : string = lststr [ 0 ] return [ Tag ( lststr , type_to_spans , span , 'ExtensionTag' ) for span in type_to_spans [ 'ExtensionTag' ] if string . startswith ( '<' + name , span [ 0 ] ) ] tags = [ ] else : tags = [ Tag ( lststr , type_to_spans , span , 'ExtensionTag' ) for span in type_to_spans [ 'ExtensionTag' ] ] tags_append = tags . append ss = self . _span [ 0 ] shadow = self . _shadow if name : reversed_start_matches = reversed ( [ m for m in regex_compile ( START_TAG_PATTERN . replace ( rb'{name}' , rb'(?P<name>' + name . encode ( ) + rb')' ) ) . finditer ( shadow ) ] ) end_search = regex_compile ( END_TAG_PATTERN . replace ( b'{name}' , name . encode ( ) ) ) . search else : reversed_start_matches = reversed ( [ m for m in START_TAG_FINDITER ( shadow ) ] ) shadow_copy = shadow [ : ] spans = type_to_spans . setdefault ( 'Tag' , [ ] ) span_tuple_to_span_get = { ( s [ 0 ] , s [ 1 ] ) : s for s in spans } . get spans_append = spans . append for start_match in reversed_start_matches : if start_match [ 'self_closing' ] : s , e = start_match . span ( ) span = [ ss + s , ss + e ] else : if name : end_match = end_search ( shadow_copy , start_match . end ( ) ) else : end_match = search ( END_TAG_PATTERN . replace ( b'{name}' , start_match [ 'name' ] ) , shadow_copy ) if end_match : s , e = end_match . span ( ) shadow_copy [ s : e ] = b'_' * ( e - s ) span = [ ss + start_match . start ( ) , ss + e ] else : s , e = start_match . span ( ) span = [ ss + s , ss + e ] old_span = span_tuple_to_span_get ( ( span [ 0 ] , span [ 1 ] ) ) if old_span is None : spans_append ( span ) else : span = old_span tags_append ( Tag ( lststr , type_to_spans , span , 'Tag' ) ) return sorted ( tags , key = attrgetter ( '_span' ) )
def _subspans ( self , _type : str ) -> Generator [ int , None , None ] : ss , se = self . _span spans = self . _type_to_spans [ _type ] b = bisect ( spans , [ ss ] ) for span in spans [ b : bisect ( spans , [ se ] , b ) ] : if span [ 1 ] <= se : yield span
def del_arg ( self , name : str ) -> None : for arg in reversed ( self . arguments ) : if arg . name . strip ( WS ) == name . strip ( WS ) : del arg [ : ]
def to_ogc_wkt ( self ) : return 'GEOGCS["%s", %s, %s, %s, AXIS["Lon", %s], AXIS["Lat", %s]]' % ( self . name , self . datum . to_ogc_wkt ( ) , self . prime_mer . to_ogc_wkt ( ) , self . angunit . to_ogc_wkt ( ) , self . twin_ax [ 0 ] . ogc_wkt , self . twin_ax [ 1 ] . ogc_wkt )
def to_esri_wkt ( self ) : return 'GEOGCS["%s", %s, %s, %s, AXIS["Lon", %s], AXIS["Lat", %s]]' % ( self . name , self . datum . to_esri_wkt ( ) , self . prime_mer . to_esri_wkt ( ) , self . angunit . to_esri_wkt ( ) , self . twin_ax [ 0 ] . esri_wkt , self . twin_ax [ 1 ] . esri_wkt )
def to_ogc_wkt ( self ) : string = 'PROJCS["%s", %s, %s, ' % ( self . name , self . geogcs . to_ogc_wkt ( ) , self . proj . to_ogc_wkt ( ) ) string += ", " . join ( param . to_ogc_wkt ( ) for param in self . params ) string += ', %s' % self . unit . to_ogc_wkt ( ) string += ', AXIS["X", %s], AXIS["Y", %s]]' % ( self . twin_ax [ 0 ] . ogc_wkt , self . twin_ax [ 1 ] . ogc_wkt ) return string
def to_esri_wkt ( self ) : string = 'PROJCS["%s", %s, %s, ' % ( self . name , self . geogcs . to_esri_wkt ( ) , self . proj . to_esri_wkt ( ) ) string += ", " . join ( param . to_esri_wkt ( ) for param in self . params ) string += ', %s' % self . unit . to_esri_wkt ( ) string += ', AXIS["X", %s], AXIS["Y", %s]]' % ( self . twin_ax [ 0 ] . esri_wkt , self . twin_ax [ 1 ] . esri_wkt ) return string
def parse_geo_tiff ( key_dir_vlr : GeoKeyDirectoryVlr , double_vlr : GeoDoubleParamsVlr , ascii_vlr : GeoAsciiParamsVlr , ) -> List [ GeoTiffKey ] : geotiff_keys = [ ] for k in key_dir_vlr . geo_keys : if k . tiff_tag_location == 0 : value = k . value_offset elif k . tiff_tag_location == 34736 : value = double_vlr . doubles [ k . value_offset ] elif k . tiff_tag_location == 34737 : try : value = ascii_vlr . strings [ k . value_offset ] [ k . count : ] except IndexError : value = ascii_vlr . strings [ 0 ] [ k . value_offset : k . value_offset + k . count ] else : logger . warning ( "GeoTiffKey with unknown tiff tag location ({})" . format ( k . tiff_tag_location ) ) continue geotiff_keys . append ( GeoTiffKey ( k . id , value ) ) return geotiff_keys
def copy_fields_from ( self , other_record ) : for dim_name in self . dimensions_names : try : self [ dim_name ] = other_record [ dim_name ] except ValueError : pass
def from_stream ( cls , stream , point_format , count ) : points_dtype = point_format . dtype point_data_buffer = bytearray ( stream . read ( count * points_dtype . itemsize ) ) try : data = np . frombuffer ( point_data_buffer , dtype = points_dtype , count = count ) except ValueError : expected_bytes_len = count * points_dtype . itemsize if len ( point_data_buffer ) % points_dtype . itemsize != 0 : missing_bytes_len = expected_bytes_len - len ( point_data_buffer ) raise_not_enough_bytes_error ( expected_bytes_len , missing_bytes_len , len ( point_data_buffer ) , points_dtype , ) else : actual_count = len ( point_data_buffer ) // points_dtype . itemsize logger . critical ( "Expected {} points, there are {} ({} missing)" . format ( count , actual_count , count - actual_count ) ) data = np . frombuffer ( point_data_buffer , dtype = points_dtype , count = actual_count ) return cls ( data , point_format )
def x ( self ) : return scale_dimension ( self . X , self . header . x_scale , self . header . x_offset )
def y ( self ) : return scale_dimension ( self . Y , self . header . y_scale , self . header . y_offset )
def z ( self ) : return scale_dimension ( self . Z , self . header . z_scale , self . header . z_offset )
def min_file_version_for_point_format ( point_format_id ) : for version , point_formats in sorted ( VERSION_TO_POINT_FMT . items ( ) ) : if point_format_id in point_formats : return version else : raise errors . PointFormatNotSupported ( point_format_id )
def is_point_fmt_compatible_with_version ( point_format_id , file_version ) : try : return point_format_id in VERSION_TO_POINT_FMT [ str ( file_version ) ] except KeyError : raise errors . FileVersionNotSupported ( file_version )
def files_have_same_point_format_id ( las_files ) : point_format_found = { las . header . point_format_id for las in las_files } return len ( point_format_found ) == 1
def files_have_same_dtype ( las_files ) : dtypes = { las . points . dtype for las in las_files } return len ( dtypes ) == 1
def _raise_if_wrong_file_signature ( stream ) : file_sig = stream . read ( len ( headers . LAS_FILE_SIGNATURE ) ) if file_sig != headers . LAS_FILE_SIGNATURE : raise errors . PylasError ( "File Signature ({}) is not {}" . format ( file_sig , headers . LAS_FILE_SIGNATURE ) )
def read_header ( self ) : self . stream . seek ( self . start_pos ) return headers . HeaderFactory ( ) . read_from_stream ( self . stream )
def read_vlrs ( self ) : self . stream . seek ( self . start_pos + self . header . size ) return VLRList . read_from ( self . stream , num_to_read = self . header . number_of_vlr )
def _read_compressed_points_data ( self , laszip_vlr , point_format ) : offset_to_chunk_table = struct . unpack ( "<q" , self . stream . read ( 8 ) ) [ 0 ] size_of_point_data = offset_to_chunk_table - self . stream . tell ( ) if offset_to_chunk_table <= 0 : logger . warning ( "Strange offset to chunk table: {}, ignoring it.." . format ( offset_to_chunk_table ) ) size_of_point_data = - 1 points = record . PackedPointRecord . from_compressed_buffer ( self . stream . read ( size_of_point_data ) , point_format , self . header . point_count , laszip_vlr , ) return points
def _read_internal_waveform_packet ( self ) : b = bytearray ( self . stream . read ( rawvlr . VLR_HEADER_SIZE ) ) waveform_header = rawvlr . RawVLRHeader . from_buffer ( b ) waveform_record = self . stream . read ( ) logger . debug ( "Read: {} MBytes of waveform_record" . format ( len ( waveform_record ) / 10 ** 6 ) ) return waveform_header , waveform_record
def _warn_if_not_at_expected_pos ( self , expected_pos , end_of , start_of ) : diff = expected_pos - self . stream . tell ( ) if diff != 0 : logger . warning ( "There are {} bytes between {} and {}" . format ( diff , end_of , start_of ) )
def date ( self , date ) : self . creation_year = date . year self . creation_day_of_year = date . timetuple ( ) . tm_yday
def mins ( self ) : return np . array ( [ self . x_min , self . y_min , self . z_min ] )
def mins ( self , value ) : self . x_min , self . y_min , self . z_min = value
def maxs ( self ) : return np . array ( [ self . x_max , self . y_max , self . z_max ] )
def maxs ( self , value ) : self . x_max , self . y_max , self . z_max = value
def scales ( self ) : return np . array ( [ self . x_scale , self . y_scale , self . z_scale ] )
def offsets ( self ) : return np . array ( [ self . x_offset , self . y_offset , self . z_offset ] )
def num_extra_bytes ( self ) : return sum ( np . dtype ( extra_dim [ 1 ] ) . itemsize for extra_dim in self . extra_dims )
def has_waveform_packet ( self ) : dimensions = set ( self . dimension_names ) return all ( name in dimensions for name in dims . WAVEFORM_FIELDS_NAMES )
def main ( port , ip , command , loglevel ) : numeric_level = getattr ( logging , loglevel . upper ( ) , None ) if not isinstance ( numeric_level , int ) : raise ValueError ( 'Invalid log level: %s' % loglevel ) logging . basicConfig ( level = numeric_level ) click . echo ( "Demo of satel_integra library" ) if command == "demo" : demo ( ip , port )
def checksum ( command ) : crc = 0x147A for b in command : crc = ( ( crc << 1 ) & 0xFFFF ) | ( crc & 0x8000 ) >> 15 crc = crc ^ 0xFFFF crc = ( crc + ( crc >> 8 ) + b ) & 0xFFFF return crc
def print_hex ( data ) : hex_msg = "" for c in data : hex_msg += "\\x" + format ( c , "02x" ) _LOGGER . debug ( hex_msg )
def verify_and_strip ( resp ) : if resp [ 0 : 2 ] != b'\xFE\xFE' : _LOGGER . error ( "Houston, we got problem:" ) print_hex ( resp ) raise Exception ( "Wrong header - got %X%X" % ( resp [ 0 ] , resp [ 1 ] ) ) if resp [ - 2 : ] != b'\xFE\x0D' : raise Exception ( "Wrong footer - got %X%X" % ( resp [ - 2 ] , resp [ - 1 ] ) ) output = resp [ 2 : - 2 ] . replace ( b'\xFE\xF0' , b'\xFE' ) c = checksum ( bytearray ( output [ 0 : - 2 ] ) ) if ( 256 * output [ - 2 : - 1 ] [ 0 ] + output [ - 1 : ] [ 0 ] ) != c : raise Exception ( "Wrong checksum - got %d expected %d" % ( ( 256 * output [ - 2 : - 1 ] [ 0 ] + output [ - 1 : ] [ 0 ] ) , c ) ) return output [ 0 : - 2 ]
def generate_query ( command ) : data = bytearray ( command ) c = checksum ( data ) data . append ( c >> 8 ) data . append ( c & 0xFF ) data . replace ( b'\xFE' , b'\xFE\xF0' ) data = bytearray . fromhex ( "FEFE" ) + data + bytearray . fromhex ( "FE0D" ) return data
def demo ( host , port ) : loop = asyncio . get_event_loop ( ) stl = AsyncSatel ( host , port , loop , [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 25 , 26 , 27 , 28 , 29 , 30 ] , [ 8 , 9 , 10 ] ) loop . run_until_complete ( stl . connect ( ) ) loop . create_task ( stl . arm ( "3333" , 1 ) ) loop . create_task ( stl . disarm ( "3333" ) ) loop . create_task ( stl . keep_alive ( ) ) loop . create_task ( stl . monitor_status ( ) ) loop . run_forever ( ) loop . close ( )
async def connect ( self ) : _LOGGER . debug ( "Connecting..." ) try : self . _reader , self . _writer = await asyncio . open_connection ( self . _host , self . _port , loop = self . _loop ) _LOGGER . debug ( "sucess connecting..." ) except Exception as e : _LOGGER . warning ( "Exception during connecting: %s." , e ) self . _writer = None self . _reader = None return False return True
async def start_monitoring ( self ) : data = generate_query ( b'\x7F\x01\xDC\x99\x80\x00\x04\x00\x00\x00\x00\x00\x00' ) await self . _send_data ( data ) resp = await self . _read_data ( ) if resp is None : _LOGGER . warning ( "Start monitoring - no data!" ) return if resp [ 1 : 2 ] != b'\xFF' : _LOGGER . warning ( "Monitoring not accepted." )
async def disarm ( self , code , partition_list ) : _LOGGER . info ( "Sending disarm command." ) while len ( code ) < 16 : code += 'F' code_bytes = bytearray . fromhex ( code ) data = generate_query ( b'\x84' + code_bytes + partition_bytes ( partition_list ) ) await self . _send_data ( data )
async def clear_alarm ( self , code , partition_list ) : _LOGGER . info ( "Sending clear the alarm command." ) while len ( code ) < 16 : code += 'F' code_bytes = bytearray . fromhex ( code ) data = generate_query ( b'\x85' + code_bytes + partition_bytes ( partition_list ) ) await self . _send_data ( data )
async def set_output ( self , code , output_id , state ) : _LOGGER . debug ( "Turn on, output: %s, code: %s" , output_id , code ) while len ( code ) < 16 : code += 'F' code_bytes = bytearray . fromhex ( code ) mode_command = 0x88 if state else 0x89 data = generate_query ( mode_command . to_bytes ( 1 , 'big' ) + code_bytes + output_bytes ( output_id ) ) await self . _send_data ( data )
def close ( self ) : _LOGGER . debug ( "Closing..." ) self . closed = True if self . connected : self . _writer . close ( )
def purge_db ( self ) : with self . engine . begin ( ) as db : purge_user ( db , self . user_id )
def _get_notebook ( self , path , content , format ) : with self . engine . begin ( ) as db : try : record = get_file ( db , self . user_id , path , content , self . crypto . decrypt , ) except NoSuchFile : self . no_such_entity ( path ) return self . _notebook_model_from_db ( record , content )
def _notebook_model_from_db ( self , record , content ) : path = to_api_path ( record [ 'parent_name' ] + record [ 'name' ] ) model = base_model ( path ) model [ 'type' ] = 'notebook' model [ 'last_modified' ] = model [ 'created' ] = record [ 'created_at' ] if content : content = reads_base64 ( record [ 'content' ] ) self . mark_trusted_cells ( content , path ) model [ 'content' ] = content model [ 'format' ] = 'json' self . validate_notebook_model ( model ) return model
def _get_directory ( self , path , content , format ) : with self . engine . begin ( ) as db : try : record = get_directory ( db , self . user_id , path , content ) except NoSuchDirectory : if self . file_exists ( path ) : self . do_400 ( "Wrong type: %s" % path ) else : self . no_such_entity ( path ) return self . _directory_model_from_db ( record , content )
def _directory_model_from_db ( self , record , content ) : model = base_directory_model ( to_api_path ( record [ 'name' ] ) ) if content : model [ 'format' ] = 'json' model [ 'content' ] = list ( chain ( self . _convert_file_records ( record [ 'files' ] ) , ( self . _directory_model_from_db ( subdir , False ) for subdir in record [ 'subdirs' ] ) , ) ) return model
def _file_model_from_db ( self , record , content , format ) : path = to_api_path ( record [ 'parent_name' ] + record [ 'name' ] ) model = base_model ( path ) model [ 'type' ] = 'file' model [ 'last_modified' ] = model [ 'created' ] = record [ 'created_at' ] if content : bcontent = record [ 'content' ] model [ 'content' ] , model [ 'format' ] , model [ 'mimetype' ] = from_b64 ( path , bcontent , format , ) return model
def _save_file ( self , db , model , path ) : save_file ( db , self . user_id , path , to_b64 ( model [ 'content' ] , model . get ( 'format' , None ) ) , self . crypto . encrypt , self . max_file_size_bytes , ) return None
def delete_file ( self , path ) : if self . file_exists ( path ) : self . _delete_non_directory ( path ) elif self . dir_exists ( path ) : self . _delete_directory ( path ) else : self . no_such_entity ( path )
def ensure_db_user ( db , user_id ) : with ignore_unique_violation ( ) : db . execute ( users . insert ( ) . values ( id = user_id ) , )
def purge_user ( db , user_id ) : db . execute ( files . delete ( ) . where ( files . c . user_id == user_id ) ) db . execute ( directories . delete ( ) . where ( directories . c . user_id == user_id ) ) db . execute ( users . delete ( ) . where ( users . c . id == user_id ) )
def create_directory ( db , user_id , api_path ) : name = from_api_dirname ( api_path ) if name == '/' : parent_name = null ( ) parent_user_id = null ( ) else : parent_name = name [ : name . rindex ( '/' , 0 , - 1 ) + 1 ] parent_user_id = user_id db . execute ( directories . insert ( ) . values ( name = name , user_id = user_id , parent_name = parent_name , parent_user_id = parent_user_id , ) )
def delete_directory ( db , user_id , api_path ) : db_dirname = from_api_dirname ( api_path ) try : result = db . execute ( directories . delete ( ) . where ( and_ ( directories . c . user_id == user_id , directories . c . name == db_dirname , ) ) ) except IntegrityError as error : if is_foreign_key_violation ( error ) : raise DirectoryNotEmpty ( api_path ) else : raise rowcount = result . rowcount if not rowcount : raise NoSuchDirectory ( api_path ) return rowcount
def files_in_directory ( db , user_id , db_dirname ) : fields = _file_default_fields ( ) rows = db . execute ( select ( fields , ) . where ( _is_in_directory ( files , user_id , db_dirname ) , ) . order_by ( files . c . user_id , files . c . parent_name , files . c . name , files . c . created_at , ) . distinct ( files . c . user_id , files . c . parent_name , files . c . name , ) ) return [ to_dict_no_content ( fields , row ) for row in rows ]
def directories_in_directory ( db , user_id , db_dirname ) : fields = _directory_default_fields ( ) rows = db . execute ( select ( fields , ) . where ( _is_in_directory ( directories , user_id , db_dirname ) , ) ) return [ to_dict_no_content ( fields , row ) for row in rows ]
def _file_where ( user_id , api_path ) : directory , name = split_api_filepath ( api_path ) return and_ ( files . c . name == name , files . c . user_id == user_id , files . c . parent_name == directory , )
def _select_file ( user_id , api_path , fields , limit ) : query = select ( fields ) . where ( _file_where ( user_id , api_path ) , ) . order_by ( _file_creation_order ( ) , ) if limit is not None : query = query . limit ( limit ) return query
def _file_default_fields ( ) : return [ files . c . name , files . c . created_at , files . c . parent_name , ]
def file_exists ( db , user_id , path ) : try : get_file ( db , user_id , path , include_content = False , decrypt_func = unused_decrypt_func , ) return True except NoSuchFile : return False
def rename_file ( db , user_id , old_api_path , new_api_path ) : if file_exists ( db , user_id , new_api_path ) : raise FileExists ( new_api_path ) old_dir , old_name = split_api_filepath ( old_api_path ) new_dir , new_name = split_api_filepath ( new_api_path ) if old_dir != new_dir : raise ValueError ( dedent ( . format ( old_api_path = old_api_path , new_api_path = new_api_path ) ) ) db . execute ( files . update ( ) . where ( _file_where ( user_id , old_api_path ) , ) . values ( name = new_name , created_at = func . now ( ) , ) )
def rename_directory ( db , user_id , old_api_path , new_api_path ) : old_db_path = from_api_dirname ( old_api_path ) new_db_path = from_api_dirname ( new_api_path ) if old_db_path == '/' : raise RenameRoot ( 'Renaming the root directory is not permitted.' ) if _dir_exists ( db , user_id , new_db_path ) : raise DirectoryExists ( new_api_path ) db . execute ( 'SET CONSTRAINTS ' 'pgcontents.directories_parent_user_id_fkey DEFERRED' ) db . execute ( directories . update ( ) . where ( and_ ( directories . c . user_id == user_id , directories . c . name == old_db_path , ) ) . values ( name = new_db_path , ) ) db . execute ( directories . update ( ) . where ( and_ ( directories . c . user_id == user_id , directories . c . name . startswith ( old_db_path ) , directories . c . parent_name . startswith ( old_db_path ) , ) ) . values ( name = func . concat ( new_db_path , func . right ( directories . c . name , - func . length ( old_db_path ) ) ) , parent_name = func . concat ( new_db_path , func . right ( directories . c . parent_name , - func . length ( old_db_path ) ) ) , ) )
def purge_remote_checkpoints ( db , user_id ) : db . execute ( remote_checkpoints . delete ( ) . where ( remote_checkpoints . c . user_id == user_id , ) )
def reencrypt_row_content ( db , table , row_id , decrypt_func , encrypt_func , logger ) : q = ( select ( [ table . c . content ] ) . with_for_update ( ) . where ( table . c . id == row_id ) ) [ ( content , ) ] = db . execute ( q ) logger . info ( "Begin encrypting %s row %s." , table . name , row_id ) db . execute ( table . update ( ) . where ( table . c . id == row_id ) . values ( content = encrypt_func ( decrypt_func ( content ) ) ) ) logger . info ( "Done encrypting %s row %s." , table . name , row_id )
def select_file_ids ( db , user_id ) : return list ( db . execute ( select ( [ files . c . id ] ) . where ( files . c . user_id == user_id ) ) )
def select_remote_checkpoint_ids ( db , user_id ) : return list ( db . execute ( select ( [ remote_checkpoints . c . id ] ) . where ( remote_checkpoints . c . user_id == user_id ) ) )
def reencrypt_user_content ( engine , user_id , old_decrypt_func , new_encrypt_func , logger ) : logger . info ( "Begin re-encryption for user %s" , user_id ) with engine . begin ( ) as db : logger . info ( "Re-encrypting files for %s" , user_id ) for ( file_id , ) in select_file_ids ( db , user_id ) : reencrypt_row_content ( db , files , file_id , old_decrypt_func , new_encrypt_func , logger , ) logger . info ( "Re-encrypting checkpoints for %s" , user_id ) for ( cp_id , ) in select_remote_checkpoint_ids ( db , user_id ) : reencrypt_row_content ( db , remote_checkpoints , cp_id , old_decrypt_func , new_encrypt_func , logger , ) logger . info ( "Finished re-encryption for user %s" , user_id )
def memoize_single_arg ( f ) : memo = { } @ wraps ( f ) def memoized_f ( arg ) : try : return memo [ arg ] except KeyError : result = memo [ arg ] = f ( arg ) return result return memoized_f
def delete_checkpoint ( self , checkpoint_id , path ) : with self . engine . begin ( ) as db : return delete_single_remote_checkpoint ( db , self . user_id , path , checkpoint_id , )
def get_checkpoint_content ( self , checkpoint_id , path ) : with self . engine . begin ( ) as db : return get_remote_checkpoint ( db , self . user_id , path , checkpoint_id , self . crypto . decrypt , ) [ 'content' ]
def list_checkpoints ( self , path ) : with self . engine . begin ( ) as db : return list_remote_checkpoints ( db , self . user_id , path )
def rename_all_checkpoints ( self , old_path , new_path ) : with self . engine . begin ( ) as db : return move_remote_checkpoints ( db , self . user_id , old_path , new_path , )
def delete_all_checkpoints ( self , path ) : with self . engine . begin ( ) as db : delete_remote_checkpoints ( db , self . user_id , path )
def purge_db ( self ) : with self . engine . begin ( ) as db : purge_remote_checkpoints ( db , self . user_id )
def _apply_prefix ( prefix , model ) : if not isinstance ( model , dict ) : raise TypeError ( "Expected dict for model, got %s" % type ( model ) ) model [ 'path' ] = '/' . join ( ( prefix , model [ 'path' ] ) ) . strip ( '/' ) if model [ 'type' ] in ( 'notebook' , 'file' ) : return model if model [ 'type' ] != 'directory' : raise ValueError ( "Unknown model type %s." % type ( model ) ) content = model . get ( 'content' , None ) if content is not None : for sub_model in content : _apply_prefix ( prefix , sub_model ) return model
def path_dispatch1 ( mname , returns_model ) : def _wrapper ( self , * args , * * kwargs ) : path , args = _get_arg ( 'path' , args , kwargs ) prefix , mgr , mgr_path = _resolve_path ( path , self . managers ) result = getattr ( mgr , mname ) ( mgr_path , * args , * * kwargs ) if returns_model and prefix : return _apply_prefix ( prefix , result ) else : return result return _wrapper
def path_dispatch_old_new ( mname , returns_model ) : def _wrapper ( self , old_path , new_path , * args , * * kwargs ) : old_prefix , old_mgr , old_mgr_path = _resolve_path ( old_path , self . managers ) new_prefix , new_mgr , new_mgr_path = _resolve_path ( new_path , self . managers , ) if old_mgr is not new_mgr : raise HTTPError ( 400 , "Can't move files between backends ({old} -> {new})" . format ( old = old_path , new = new_path , ) ) assert new_prefix == old_prefix result = getattr ( new_mgr , mname ) ( old_mgr_path , new_mgr_path , * args , * * kwargs ) if returns_model and new_prefix : return _apply_prefix ( new_prefix , result ) else : return result return _wrapper
def _managers_changed ( self , name , old , new ) : for key in new : if '/' in key : raise ValueError ( "Expected directory names w/o slashes.  Got [%s]" % key ) self . managers = { k . strip ( '/' ) : v for k , v in new . items ( ) }
def get ( self , path , content = True , type = None , format = None ) : path = normalize_api_path ( path ) if path : return self . __get ( path , content = content , type = type , format = format ) if not content : return base_directory_model ( '' ) extra_content = self . _extra_root_dirs ( ) rm = self . root_manager if rm is None : root_model = base_directory_model ( '' ) root_model . update ( format = 'json' , content = extra_content , ) else : root_model = rm . get ( path , content = content , type = type , format = format , ) root_model [ 'content' ] . extend ( extra_content ) return root_model
def split_api_filepath ( path ) : parts = path . rsplit ( '/' , 1 ) if len ( parts ) == 1 : name = parts [ 0 ] dirname = '/' else : name = parts [ 1 ] dirname = parts [ 0 ] + '/' return from_api_dirname ( dirname ) , name
def writes_base64 ( nb , version = NBFORMAT_VERSION ) : return b64encode ( writes ( nb , version = version ) . encode ( 'utf-8' ) )
def reads_base64 ( nb , as_version = NBFORMAT_VERSION ) : try : return reads ( b64decode ( nb ) . decode ( 'utf-8' ) , as_version = as_version ) except Exception as e : raise CorruptedFile ( e )
def prefix_dirs ( path ) : _dirname = posixpath . dirname path = path . strip ( '/' ) out = [ ] while path != '' : path = _dirname ( path ) out . append ( path ) return reversed ( out )
def outside_root_to_404 ( fn ) : @ wraps ( fn ) def wrapped ( * args , * * kwargs ) : try : return fn ( * args , * * kwargs ) except PathOutsideRoot as e : raise HTTPError ( 404 , "Path outside root: [%s]" % e . args [ 0 ] ) return wrapped
def create_user ( db_url , user ) : PostgresCheckpoints ( db_url = db_url , user_id = user , create_user_on_startup = True , )
def walk_dirs ( mgr , dirs ) : for directory in dirs : children = mgr . get ( directory , content = True , type = 'directory' , ) [ 'content' ] dirs , files = map ( sorted , _separate_dirs_files ( children ) ) yield directory , dirs , files if dirs : for entry in walk_dirs ( mgr , dirs ) : yield entry
def walk_files ( mgr ) : for dir_ , subdirs , files in walk_files ( mgr ) : for file_ in files : yield file_
def walk_files_with_content ( mgr ) : for _ , _ , files in walk ( mgr ) : for f in files : yield mgr . get ( f , content = True )
def reencrypt_single_user ( engine , user_id , old_crypto , new_crypto , logger ) : crypto = FallbackCrypto ( [ new_crypto , old_crypto ] ) reencrypt_user_content ( engine = engine , user_id = user_id , old_decrypt_func = crypto . decrypt , new_encrypt_func = crypto . encrypt , logger = logger , )
def unencrypt_single_user ( engine , user_id , old_crypto , logger ) : reencrypt_user_content ( engine = engine , user_id = user_id , old_decrypt_func = old_crypto . decrypt , new_encrypt_func = lambda s : s , logger = logger , )
def upgrade ( db_url , revision ) : with temp_alembic_ini ( ALEMBIC_DIR_LOCATION , db_url ) as alembic_ini : subprocess . check_call ( [ 'alembic' , '-c' , alembic_ini , 'upgrade' , revision ] )
def queue_instance ( self , embed_type , data ) : serializer = self . serializers . get ( embed_type , None ) if serializer is None : return instance_id = serializer . get_id ( data ) if embed_type not in self . ids : self . ids [ embed_type ] = [ ] self . ids [ embed_type ] . append ( instance_id )
def insert_instance ( self , block ) : embed_type = block . get ( 'type' , None ) data = block . get ( 'data' , { } ) serializer = self . serializers . get ( embed_type , None ) if serializer is None : return block try : instance_id = serializer . get_id ( data ) instance = self . instances [ embed_type ] [ instance_id ] data [ embed_type ] = serializer . serialize ( instance ) except : data [ embed_type ] = None block [ 'data' ] = data return block
def load_data ( self ) : for embed_type in self . ids . keys ( ) : self . load_instances ( embed_type , self . ids [ embed_type ] )
def validate ( self , data ) : from dispatch . theme import ThemeManager errors = { } if data . get ( 'widget' ) is not None : try : widget = ThemeManager . Widgets . get ( data [ 'widget' ] ) except WidgetNotFound as e : errors [ 'widget' ] = str ( e ) else : for field in widget . fields : field_data = data [ 'data' ] . get ( field . name ) if field_data is not None : try : field . validate ( field_data ) except InvalidField as e : errors [ field . name ] = str ( e ) elif field . required : errors [ field . name ] = '%s is required' % field . label if errors : raise ValidationError ( errors ) return data
def admin ( request ) : context = { 'api_url' : settings . API_URL , 'app_js_bundle' : 'manager-%s.js' % dispatch . __version__ , 'app_css_bundle' : 'manager-%s.css' % dispatch . __version__ } return render_to_response ( 'manager/index.html' , context )
def to_json ( self ) : result = { } for field in self . fields : result [ field . name ] = field . to_json ( self . data . get ( field . name ) ) return result
def exclude_fields ( self ) : request = self . context . get ( 'request' ) if request : exclude = request . query_params . get ( 'exclude' , None ) if exclude is None : return excluded_fields = exclude . split ( ',' ) for field in excluded_fields : self . fields . pop ( field )
def get ( self , * args , * * kwargs ) : if 'pk' in kwargs : kwargs [ 'parent' ] = kwargs [ 'pk' ] kwargs [ 'head' ] = True del kwargs [ 'pk' ] if 'request' in kwargs : request = kwargs [ 'request' ] version = request . GET . get ( 'version' , None ) preview_id = request . GET . get ( 'preview_id' , None ) if ( version is not None ) and ( preview_id is not None ) : kwargs [ 'revision_id' ] = version kwargs [ 'preview_id' ] = preview_id del kwargs [ 'is_published' ] del kwargs [ 'request' ] return super ( PublishableManager , self ) . get ( * args , * * kwargs )
def get_attribute ( self , instance ) : attr = super ( NullBooleanField , self ) . get_attribute ( instance ) return True if attr else False
def validate_widget ( widget ) : if not has_valid_id ( widget ) : raise InvalidWidget ( "%s must contain a valid 'id' attribute" % widget . __name__ ) if not has_valid_name ( widget ) : raise InvalidWidget ( "%s must contain a valid 'name' attribute" % widget . __name__ ) if not has_valid_template ( widget ) : raise InvalidWidget ( "%s must contain a valid 'template' attribute" % widget . __name__ ) if not hasattr ( widget , 'zones' ) or not widget . zones : raise InvalidWidget ( "%s must be compatible with at least one zone" % widget . __name__ )
def validate_zone ( zone ) : if not has_valid_id ( zone ) : raise InvalidZone ( "%s must contain a valid 'id' attribute" % zone . __name__ ) if not has_valid_name ( zone ) : raise InvalidZone ( "%s must contain a valid 'name' attribute" % zone . __name__ )
def is_valid_uuid ( id ) : if not isinstance ( id , basestring ) : return False try : val = UUID ( id , version = 4 ) except ValueError : return False return True
def get_permissions ( self ) : permissions = '' if self . groups . filter ( name = 'Admin' ) . exists ( ) or self . is_superuser : permissions = 'admin' return permissions
def modify_permissions ( self , permissions ) : group = Group . objects . get ( name = 'Admin' ) if permissions == 'admin' : self . groups . add ( group ) else : self . groups . remove ( group )
def AuthorValidator ( data ) : if not isinstance ( data , list ) : data = [ data ] for author in data : if 'person' not in author : raise ValidationError ( 'An author must contain a person.' ) if 'type' in author and not isinstance ( author [ 'type' ] , basestring ) : raise ValidationError ( 'The author type must be a string.' )
def save ( self , validated_data ) : ( zone , created ) = ZoneModel . objects . get_or_create ( zone_id = self . id ) zone . widget_id = validated_data [ 'widget' ] zone . data = validated_data [ 'data' ] for key in list ( zone . data . keys ( ) ) : if isinstance ( zone . data [ key ] , dict ) and ( 'id' in zone . data [ key ] . keys ( ) ) and ( 'data' in zone . data [ key ] . keys ( ) ) : zone . data [ key ] [ 'data' ] = self . before_save ( zone . data [ key ] [ 'id' ] , zone . data [ key ] [ 'data' ] ) zone . data = self . before_save ( zone . widget_id , zone . data ) return zone . save ( )
def get_data ( self ) : result = { } for field in self . fields : result [ field . name ] = self . data . get ( field . name ) return result
def prepare_data ( self ) : result = { } for field in self . fields : data = self . data . get ( field . name ) result [ field . name ] = field . prepare_data ( data ) return result
def render ( self , data = None , add_context = None ) : template = loader . get_template ( self . template ) if not data : data = self . context ( self . prepare_data ( ) ) if add_context is not None : for key , value in add_context . iteritems ( ) : if key in self . accepted_keywords : data [ key ] = value return template . render ( data )
def callback ( cls , user , query ) : settings = cls . get_settings ( show_hidden = True ) fb = Facebook ( ) payload = { 'client_id' : settings [ 'client_id' ] , 'client_secret' : settings [ 'client_secret' ] , 'code' : query [ 'code' ] , 'redirect_uri' : cls . REDIRECT_URI } try : fb . get_access_token ( payload ) pages = fb . list_pages ( 'me' ) except FacebookAPIError , e : raise IntegrationCallbackError ( e . message ) return { 'pages' : pages }
def get_settings ( self , integration_id ) : try : integration = self . get ( integration_id = integration_id ) return json . loads ( integration . settings ) except ( self . model . DoesNotExist , ValueError ) : return { }
def update_settings ( self , integration_id , settings ) : ( integration , created ) = self . get_or_create ( integration_id = integration_id ) try : current_settings = json . loads ( integration . settings ) except ValueError : current_settings = { } current_settings . update ( settings ) integration . settings = json . dumps ( current_settings ) integration . save ( )
def signup ( request , uuid = None ) : invite = get_object_or_404 ( Invite . objects . all ( ) , id = uuid ) if invite . expiration_date < timezone . now ( ) : invite . delete ( ) raise Http404 ( 'This page does not exist.' ) if request . method == 'POST' : form = SignUpForm ( request . POST ) if form . is_valid ( ) : user = form . save ( commit = False ) user . email = invite . email user . person = invite . person user . save ( ) if invite . permissions == 'admin' : group = Group . objects . get ( name = 'Admin' ) user . groups . add ( group ) invite . delete ( ) return redirect ( 'dispatch-admin' ) else : return render ( request , 'registration/signup.html' , { 'form' : form , 'email' : invite . email } ) else : form = SignUpForm ( ) return render ( request , 'registration/signup.html' , { 'form' : form , 'email' : invite . email } )
def zone ( zone_id , * * kwargs ) : try : zone = ThemeManager . Zones . get ( zone_id ) except ZoneNotFound : return '' try : return zone . widget . render ( add_context = kwargs ) except ( WidgetNotFound , AttributeError ) : pass return ''
def save_subsection ( self , subsection_id ) : Article . objects . filter ( parent_id = self . parent . id ) . update ( subsection_id = subsection_id )
def get_extension ( self ) : ext = os . path . splitext ( self . img . name ) [ 1 ] if ext : return ext [ 1 : ] return ext
def get_medium_url ( self ) : if self . is_gif ( ) : return self . get_absolute_url ( ) return '%s%s-%s.jpg' % ( settings . MEDIA_URL , self . get_name ( ) , 'medium' )
def save ( self , * * kwargs ) : is_new = self . pk is None if is_new : self . img . name = self . img . name . lower ( ) super ( Image , self ) . save ( * * kwargs ) if is_new and self . img : data = self . img . read ( ) if not data : return image = Img . open ( StringIO . StringIO ( data ) ) self . width , self . height = image . size super ( Image , self ) . save ( ) name = self . get_name ( ) ext = self . get_extension ( ) for size in self . SIZES . keys ( ) : self . save_thumbnail ( image , self . SIZES [ size ] , name , size , ext )
def save_thumbnail ( self , image , size , name , label , file_type ) : width , height = size ( imw , imh ) = image . size if ( imw > width ) or ( imh > height ) : image . thumbnail ( size , Img . ANTIALIAS ) name = "%s-%s.jpg" % ( name , label ) if file_type in self . JPG_FORMATS : file_type = 'JPEG' image_io = StringIO . StringIO ( ) image . save ( image_io , format = file_type , quality = 75 ) thumb_file = InMemoryUploadedFile ( image_io , None , name , 'image/jpeg' , image_io . len , None ) default_storage . save ( name , thumb_file )
def decrement ( self ) : with self . _lock : if self . _count == 0 : raise RuntimeError ( 'Counter is at zero. It cannot dip below zero' ) self . _count -= 1 if self . _is_finalized and self . _count == 0 : self . _callback ( )
def set_exception ( self , exception ) : if not self . done ( ) : raise TransferNotDoneError ( 'set_exception can only be called once the transfer is ' 'complete.' ) self . _coordinator . set_exception ( exception , override = True )
def add_done_callback ( self , function , * args , * * kwargs ) : with self . _done_callbacks_lock : self . _done_callbacks . append ( FunctionContainer ( function , * args , * * kwargs ) )
def add_failure_cleanup ( self , function , * args , * * kwargs ) : with self . _failure_cleanups_lock : self . _failure_cleanups . append ( FunctionContainer ( function , * args , * * kwargs ) )
def _iter_step_func_decorators ( self ) : func_defs = [ func for func in self . py_tree . iter_funcdefs ( ) ] + [ func for cls in self . py_tree . iter_classdefs ( ) for func in cls . iter_funcdefs ( ) ] for func in func_defs : for decorator in func . get_decorators ( ) : if decorator . children [ 1 ] . value == 'step' : yield func , decorator break
def iter_steps ( self ) : for func , decorator in self . _iter_step_func_decorators ( ) : step = self . _step_decorator_args ( decorator ) if step : span = self . _span_from_pos ( decorator . start_pos , func . end_pos ) yield step , func . name . value , span
def _find_step_node ( self , step_text ) : for func , decorator in self . _iter_step_func_decorators ( ) : step = self . _step_decorator_args ( decorator ) arg_node = decorator . children [ 3 ] if step == step_text : return arg_node , func elif isinstance ( step , list ) and step_text in step : idx = step . index ( step_text ) step_node = arg_node . children [ 1 ] . children [ idx * 2 ] return step_node , func return None , None
def _iter_step_func_decorators ( self ) : for node in self . py_tree . find_all ( 'def' ) : for decorator in node . decorators : if decorator . name . value == 'step' : yield node , decorator break
def _step_decorator_args ( self , decorator ) : args = decorator . call . value step = None if len ( args ) == 1 : try : step = args [ 0 ] . value . to_python ( ) except ( ValueError , SyntaxError ) : pass if isinstance ( step , six . string_types + ( list , ) ) : return step logging . error ( , self . file_path ) else : logging . error ( "Decorator step accepts only one argument - %s" , self . file_path )
def iter_steps ( self ) : for func , decorator in self . _iter_step_func_decorators ( ) : step = self . _step_decorator_args ( decorator ) if step : yield step , func . name , self . _span_for_node ( func , True )
def _find_step_node ( self , step_text ) : for func , decorator in self . _iter_step_func_decorators ( ) : step = self . _step_decorator_args ( decorator ) arg_node = decorator . call . value [ 0 ] . value if step == step_text : return arg_node , func elif isinstance ( step , list ) and step_text in step : step_node = arg_node [ step . index ( step_text ) ] return step_node , func return None , None
def POST ( self ) : json_data = web . data ( ) print ( "\nWEBHOOK POST RECEIVED:" ) print ( json_data , "\n" ) webhook_obj = Webhook ( json_data ) room = api . rooms . get ( webhook_obj . data . roomId ) message = api . messages . get ( webhook_obj . data . id ) person = api . people . get ( message . personId ) print ( "NEW MESSAGE IN ROOM '{}'" . format ( room . title ) ) print ( "FROM '{}'" . format ( person . displayName ) ) print ( "MESSAGE '{}'\n" . format ( message . text ) ) me = api . people . me ( ) if message . personId == me . id : return 'OK' else : if "/CAT" in message . text : print ( "FOUND '/CAT'" ) cat_fact = get_catfact ( ) print ( "SENDING CAT FACT '{}'" . format ( cat_fact ) ) api . messages . create ( room . id , text = cat_fact ) return 'OK'
def validate_base_url ( base_url ) : parsed_url = urllib . parse . urlparse ( base_url ) if parsed_url . scheme and parsed_url . netloc : return parsed_url . geturl ( ) else : error_message = "base_url must contain a valid scheme (protocol " "specifier) and network location (hostname)" raise ValueError ( error_message )
def is_web_url ( string ) : assert isinstance ( string , basestring ) parsed_url = urllib . parse . urlparse ( string ) return ( ( parsed_url . scheme . lower ( ) == 'http' or parsed_url . scheme . lower ( ) == 'https' ) and parsed_url . netloc )
def open_local_file ( file_path ) : assert isinstance ( file_path , basestring ) assert is_local_file ( file_path ) file_name = os . path . basename ( file_path ) file_object = open ( file_path , 'rb' ) content_type = mimetypes . guess_type ( file_name ) [ 0 ] or 'text/plain' return EncodableFile ( file_name = file_name , file_object = file_object , content_type = content_type )
def strptime ( cls , date_string , format = WEBEX_TEAMS_DATETIME_FORMAT ) : return super ( WebexTeamsDateTime , cls ) . strptime ( date_string , format ) . replace ( tzinfo = ZuluTimeZone ( ) )
def created ( self ) : created = self . _json_data . get ( 'created' ) if created : return WebexTeamsDateTime . strptime ( created ) else : return None
def wait_on_rate_limit ( self , value ) : check_type ( value , bool , may_be_none = False ) self . _wait_on_rate_limit = value
def _serialize ( cls , data ) : if hasattr ( data , "__hash__" ) and callable ( data . __hash__ ) : return data elif isinstance ( data , list ) : return tuple ( ( cls . _serialize ( item ) for item in data ) ) elif isinstance ( data , dict ) : key_value_tuples = [ ( key , cls . _serialize ( value ) ) for key , value in data . items ( ) ] key_value_tuples . sort ( ) return tuple ( key_value_tuples ) else : raise TypeError ( "Unable to freeze {} data type." . format ( type ( data ) ) )
def lastActivity ( self ) : last_activity = self . _json_data . get ( 'lastActivity' ) if last_activity : return WebexTeamsDateTime . strptime ( last_activity ) else : return None
def post_events_service ( request ) : json_data = request . json log . info ( "\n" ) log . info ( "WEBHOOK POST RECEIVED:" ) log . info ( json_data ) log . info ( "\n" ) webhook_obj = Webhook ( json_data ) room = api . rooms . get ( webhook_obj . data . roomId ) message = api . messages . get ( webhook_obj . data . id ) person = api . people . get ( message . personId ) log . info ( "NEW MESSAGE IN ROOM '{}'" . format ( room . title ) ) log . info ( "FROM '{}'" . format ( person . displayName ) ) log . info ( "MESSAGE '{}'\n" . format ( message . text ) ) me = api . people . me ( ) if message . personId == me . id : return { 'Message' : 'OK' } else : if "/CAT" in message . text : log . info ( "FOUND '/CAT'" ) catfact = get_catfact ( ) log . info ( "SENDING CAT FACT'{}'" . format ( catfact ) ) api . messages . create ( room . id , text = catfact ) return { 'Message' : 'OK' }
def get_ngrok_public_url ( ) : try : response = requests . get ( url = NGROK_CLIENT_API_BASE_URL + "/tunnels" , headers = { 'content-type' : 'application/json' } ) response . raise_for_status ( ) except requests . exceptions . RequestException : print ( "Could not connect to the ngrok client API; " "assuming not running." ) return None else : for tunnel in response . json ( ) [ "tunnels" ] : if tunnel . get ( "public_url" , "" ) . startswith ( "http://" ) : print ( "Found ngrok public HTTP URL:" , tunnel [ "public_url" ] ) return tunnel [ "public_url" ]
def delete_webhooks_with_name ( api , name ) : for webhook in api . webhooks . list ( ) : if webhook . name == name : print ( "Deleting Webhook:" , webhook . name , webhook . targetUrl ) api . webhooks . delete ( webhook . id )
def create_ngrok_webhook ( api , ngrok_public_url ) : print ( "Creating Webhook..." ) webhook = api . webhooks . create ( name = WEBHOOK_NAME , targetUrl = urljoin ( ngrok_public_url , WEBHOOK_URL_SUFFIX ) , resource = WEBHOOK_RESOURCE , event = WEBHOOK_EVENT , ) print ( webhook ) print ( "Webhook successfully created." ) return webhook
def main ( ) : api = WebexTeamsAPI ( ) delete_webhooks_with_name ( api , name = WEBHOOK_NAME ) public_url = get_ngrok_public_url ( ) if public_url is not None : create_ngrok_webhook ( api , public_url )
def console ( ) : parser = argparse . ArgumentParser ( description = console . __doc__ ) parser . add_argument ( '--device' , default = '/dev/ttyUSB0' , help = 'port to read DSMR data from' ) parser . add_argument ( '--host' , default = None , help = 'alternatively connect using TCP host.' ) parser . add_argument ( '--port' , default = None , help = 'TCP port to use for connection' ) parser . add_argument ( '--version' , default = '2.2' , choices = [ '2.2' , '4' ] , help = 'DSMR version (2.2, 4)' ) parser . add_argument ( '--verbose' , '-v' , action = 'count' ) args = parser . parse_args ( ) if args . verbose : level = logging . DEBUG else : level = logging . ERROR logging . basicConfig ( level = level ) loop = asyncio . get_event_loop ( ) def print_callback ( telegram ) : """Callback that prints telegram values.""" for obiref , obj in telegram . items ( ) : if obj : print ( obj . value , obj . unit ) print ( ) if args . host and args . port : create_connection = partial ( create_tcp_dsmr_reader , args . host , args . port , args . version , print_callback , loop = loop ) else : create_connection = partial ( create_dsmr_reader , args . device , args . version , print_callback , loop = loop ) try : while True : conn = create_connection ( ) transport , protocol = loop . run_until_complete ( conn ) loop . run_until_complete ( protocol . wait_closed ( ) ) loop . run_until_complete ( asyncio . sleep ( 5 ) ) except KeyboardInterrupt : transport . close ( ) loop . run_until_complete ( asyncio . sleep ( 0 ) ) finally : loop . close ( )
def create_dsmr_protocol ( dsmr_version , telegram_callback , loop = None ) : if dsmr_version == '2.2' : specification = telegram_specifications . V2_2 serial_settings = SERIAL_SETTINGS_V2_2 elif dsmr_version == '4' : specification = telegram_specifications . V4 serial_settings = SERIAL_SETTINGS_V4 elif dsmr_version == '5' : specification = telegram_specifications . V5 serial_settings = SERIAL_SETTINGS_V5 else : raise NotImplementedError ( "No telegram parser found for version: %s" , dsmr_version ) protocol = partial ( DSMRProtocol , loop , TelegramParser ( specification ) , telegram_callback = telegram_callback ) return protocol , serial_settings
def create_dsmr_reader ( port , dsmr_version , telegram_callback , loop = None ) : protocol , serial_settings = create_dsmr_protocol ( dsmr_version , telegram_callback , loop = None ) serial_settings [ 'url' ] = port conn = create_serial_connection ( loop , protocol , * * serial_settings ) return conn
def create_tcp_dsmr_reader ( host , port , dsmr_version , telegram_callback , loop = None ) : protocol , _ = create_dsmr_protocol ( dsmr_version , telegram_callback , loop = None ) conn = loop . create_connection ( protocol , host , port ) return conn
def data_received ( self , data ) : data = data . decode ( 'ascii' ) self . log . debug ( 'received data: %s' , data ) self . telegram_buffer . append ( data ) for telegram in self . telegram_buffer . get_all ( ) : self . handle_telegram ( telegram )
def connection_lost ( self , exc ) : if exc : self . log . exception ( 'disconnected due to exception' ) else : self . log . info ( 'disconnected because of close/abort.' ) self . _closed . set ( )
def handle_telegram ( self , telegram ) : self . log . debug ( 'got telegram: %s' , telegram ) try : parsed_telegram = self . telegram_parser . parse ( telegram ) except InvalidChecksumError as e : self . log . warning ( str ( e ) ) except ParseError : self . log . exception ( "failed to parse telegram" ) else : self . telegram_callback ( parsed_telegram )
def ensure_python ( specs ) : if not isinstance ( specs , ( list , tuple ) ) : specs = [ specs ] v = sys . version_info part = '%s.%s' % ( v . major , v . minor ) for spec in specs : if part == spec : return try : if eval ( part + spec ) : return except SyntaxError : pass raise ValueError ( 'Python version %s unsupported' % part )
def find_packages ( top = HERE ) : packages = [ ] for d , dirs , _ in os . walk ( top , followlinks = True ) : if os . path . exists ( pjoin ( d , '__init__.py' ) ) : packages . append ( os . path . relpath ( d , top ) . replace ( os . path . sep , '.' ) ) elif d != top : dirs [ : ] = [ ] return packages
def command_for_func ( func ) : class FuncCommand ( BaseCommand ) : def run ( self ) : func ( ) update_package_data ( self . distribution ) return FuncCommand
def run ( cmd , * * kwargs ) : log . info ( '> ' + list2cmdline ( cmd ) ) kwargs . setdefault ( 'cwd' , HERE ) kwargs . setdefault ( 'shell' , os . name == 'nt' ) if not isinstance ( cmd , ( list , tuple ) ) and os . name != 'nt' : cmd = shlex . split ( cmd ) cmd [ 0 ] = which ( cmd [ 0 ] ) return subprocess . check_call ( cmd , * * kwargs )
def _get_file_handler ( package_data_spec , data_files_spec ) : class FileHandler ( BaseCommand ) : def run ( self ) : package_data = self . distribution . package_data package_spec = package_data_spec or dict ( ) for ( key , patterns ) in package_spec . items ( ) : package_data [ key ] = _get_package_data ( key , patterns ) self . distribution . data_files = _get_data_files ( data_files_spec , self . distribution . data_files ) return FileHandler
def _compile_pattern ( pat , ignore_case = True ) : if isinstance ( pat , bytes ) : pat_str = pat . decode ( 'ISO-8859-1' ) res_str = _translate_glob ( pat_str ) res = res_str . encode ( 'ISO-8859-1' ) else : res = _translate_glob ( pat ) flags = re . IGNORECASE if ignore_case else 0 return re . compile ( res , flags = flags ) . match
def _translate_glob ( pat ) : translated_parts = [ ] for part in _iexplode_path ( pat ) : translated_parts . append ( _translate_glob_part ( part ) ) os_sep_class = '[%s]' % re . escape ( SEPARATORS ) res = _join_translated ( translated_parts , os_sep_class ) return '{res}\\Z(?ms)' . format ( res = res )
def _translate_glob_part ( pat ) : if pat == '**' : return '.*' i , n = 0 , len ( pat ) res = [ ] while i < n : c = pat [ i ] i = i + 1 if c == '*' : res . append ( '[^%s]*' % SEPARATORS ) elif c == '?' : res . append ( '[^%s]?' % SEPARATORS ) elif c == '[' : j = i if j < n and pat [ j ] == '!' : j = j + 1 if j < n and pat [ j ] == ']' : j = j + 1 while j < n and pat [ j ] != ']' : j = j + 1 if j >= n : res . append ( '\\[' ) else : stuff = pat [ i : j ] . replace ( '\\' , '\\\\' ) i = j + 1 if stuff [ 0 ] == '!' : stuff = '^' + stuff [ 1 : ] elif stuff [ 0 ] == '^' : stuff = '\\' + stuff res . append ( '[%s]' % stuff ) else : res . append ( re . escape ( c ) ) return '' . join ( res )
def qsize ( self , extra_predicate = None ) : count = self . _query_queued ( 'COUNT(*) AS count' , extra_predicate = extra_predicate ) return count [ 0 ] . count
def enqueue ( self , data ) : jsonified_data = json . dumps ( data ) with self . _db_conn ( ) as conn : return conn . execute ( 'INSERT INTO %s (created, data) VALUES (%%(created)s, %%(data)s)' % self . table_name , created = datetime . utcnow ( ) , data = jsonified_data )
def _build_extra_predicate ( self , extra_predicate ) : if extra_predicate is None : return '' if not isinstance ( extra_predicate [ 1 ] , ( list , dict , tuple ) ) : extra_predicate = [ extra_predicate [ 0 ] , ( extra_predicate [ 1 ] , ) ] extra_predicate = database . escape_query ( * extra_predicate ) return 'AND (' + extra_predicate + ')'
def simplejson_datetime_serializer ( obj ) : if hasattr ( obj , 'isoformat' ) : return obj . isoformat ( ) else : raise TypeError ( 'Object of type %s with value of %s is not JSON serializable' % ( type ( obj ) , repr ( obj ) ) )
def reconnect ( self ) : conn = _mysql . connect ( * * self . _db_args ) if conn is not None : self . close ( ) self . _db = conn
def get ( self , query , * parameters , * * kwparameters ) : rows = self . _query ( query , parameters , kwparameters ) if not rows : return None elif not isinstance ( rows , list ) : raise MySQLError ( "Query is not a select query" ) elif len ( rows ) > 1 : raise MySQLError ( "Multiple rows returned for Database.get() query" ) else : return rows [ 0 ]
def execute ( self , query , * parameters , * * kwparameters ) : return self . execute_lastrowid ( query , * parameters , * * kwparameters )
def execute_lastrowid ( self , query , * parameters , * * kwparameters ) : self . _execute ( query , parameters , kwparameters ) self . _result = self . _db . store_result ( ) return self . _db . insert_id ( )
def get_connection ( db = DATABASE ) : return database . connect ( host = HOST , port = PORT , user = USER , password = PASSWORD , database = db )
def run_benchmark ( ) : stopping = threading . Event ( ) workers = [ InsertWorker ( stopping ) for _ in range ( NUM_WORKERS ) ] print ( 'Launching %d workers' % NUM_WORKERS ) [ worker . start ( ) for worker in workers ] time . sleep ( WORKLOAD_TIME ) print ( 'Stopping workload' ) stopping . set ( ) [ worker . join ( ) for worker in workers ] with get_connection ( ) as conn : count = conn . get ( "SELECT COUNT(*) AS count FROM %s" % TABLE ) . count print ( "%d rows inserted using %d workers" % ( count , NUM_WORKERS ) ) print ( "%.1f rows per second" % ( count / float ( WORKLOAD_TIME ) ) )
def _connect ( self ) : with self . _lock : if self . _aggregator : try : return self . _pool_connect ( self . _aggregator ) except PoolConnectionException : self . _aggregator = None if not len ( self . _aggregators ) : with self . _pool_connect ( self . _primary_aggregator ) as conn : self . _update_aggregator_list ( conn ) conn . expire ( ) random . shuffle ( self . _aggregators ) last_exception = None for aggregator in self . _aggregators : self . logger . debug ( 'Attempting connection with %s:%s' % ( aggregator [ 0 ] , aggregator [ 1 ] ) ) try : conn = self . _pool_connect ( aggregator ) self . _aggregator = aggregator return conn except PoolConnectionException as e : last_exception = e else : self . _aggregator = None self . _aggregators = [ ] raise last_exception
def lookup_by_number ( errno ) : for key , val in globals ( ) . items ( ) : if errno == val : print ( key )
def size ( self ) : return sum ( q . qsize ( ) for q in self . _connections . values ( ) ) + len ( self . _fairies )
def ping ( self ) : with self . _db_conn ( ) as conn : affected_rows = conn . query ( % self . _manager . table_name , datetime . utcnow ( ) , self . _lock_id , self . _lock_hash ) return bool ( affected_rows == 1 )
def release ( self ) : if self . valid ( ) : with self . _db_conn ( ) as conn : affected_rows = conn . query ( % self . _manager . table_name , self . _lock_id , self . _lock_hash ) return bool ( affected_rows == 1 ) else : return False
def connect ( self , host = '127.0.0.1' , port = 3306 , user = 'root' , password = '' , database = None ) : if database is None : raise exceptions . RequiresDatabase ( ) self . _db_args = { 'host' : host , 'port' : port , 'user' : user , 'password' : password , 'database' : database } with self . _db_conn ( ) as conn : conn . query ( 'SELECT 1' ) return self
def setup ( self ) : with self . _db_conn ( ) as conn : for table_defn in self . _tables . values ( ) : conn . execute ( table_defn ) return self
def destroy ( self ) : with self . _db_conn ( ) as conn : for table_name in self . _tables : conn . execute ( 'DROP TABLE IF EXISTS %s' % table_name ) return self
def ready ( self ) : with self . _db_conn ( ) as conn : tables = [ row . t for row in conn . query ( , self . _db_args [ 'database' ] ) ] return all ( [ table_name in tables for table_name in self . _tables ] )
def valid ( self ) : if self . finished is not None : return False with self . _db_conn ( ) as conn : row = conn . get ( % self . _queue . table_name , now = datetime . utcnow ( ) , ttl = self . _queue . execution_ttl , task_id = self . task_id , execution_id = self . execution_id ) return bool ( row is not None and row . valid )
def ping ( self ) : if self . finished is not None : raise AlreadyFinished ( ) with self . _db_conn ( ) as conn : success = conn . query ( % self . _queue . table_name , now = datetime . utcnow ( ) , task_id = self . task_id , execution_id = self . execution_id , ttl = self . _queue . execution_ttl ) if success != 1 : raise TaskDoesNotExist ( )
def start_step ( self , step_name ) : if self . finished is not None : raise AlreadyFinished ( ) step_data = self . _get_step ( step_name ) if step_data is not None : if 'stop' in step_data : raise StepAlreadyFinished ( ) else : raise StepAlreadyStarted ( ) steps = copy . deepcopy ( self . steps ) steps . append ( { "start" : datetime . utcnow ( ) , "name" : step_name } ) self . _save ( steps = steps )
def stop_step ( self , step_name ) : if self . finished is not None : raise AlreadyFinished ( ) steps = copy . deepcopy ( self . steps ) step_data = self . _get_step ( step_name , steps = steps ) if step_data is None : raise StepNotStarted ( ) elif 'stop' in step_data : raise StepAlreadyFinished ( ) step_data [ 'stop' ] = datetime . utcnow ( ) step_data [ 'duration' ] = util . timedelta_total_seconds ( step_data [ 'stop' ] - step_data [ 'start' ] ) self . _save ( steps = steps )
def create ( self , deviceType ) : r = self . _apiClient . post ( "api/v0002/device/types" , deviceType ) if r . status_code == 201 : return DeviceType ( apiClient = self . _apiClient , * * r . json ( ) ) else : raise ApiException ( r )
def update ( self , deviceUid , metadata = None , deviceInfo = None , status = None ) : if not isinstance ( deviceUid , DeviceUid ) and isinstance ( deviceUid , dict ) : deviceUid = DeviceUid ( * * deviceUid ) deviceUrl = "api/v0002/device/types/%s/devices/%s" % ( deviceUid . typeId , deviceUid . deviceId ) data = { "status" : status , "deviceInfo" : deviceInfo , "metadata" : metadata } r = self . _apiClient . put ( deviceUrl , data ) if r . status_code == 200 : return Device ( apiClient = self . _apiClient , * * r . json ( ) ) else : raise ApiException ( r )
def find ( self , status = None , connectedAfter = None ) : queryParms = { } if status : queryParms [ "status" ] = status if connectedAfter : queryParms [ "connectedAfter" ] = connectedAfter return IterableClientStatusList ( self . _apiClient , filters = queryParms )
def list ( self ) : url = "api/v0002/mgmt/custom/bundle" r = self . _apiClient . get ( url ) if r . status_code == 200 : return r . json ( ) else : raise ApiException ( r )
def updateSchema ( self , schemaId , schemaDefinition ) : req = ApiClient . oneSchemaUrl % ( self . host , "/draft" , schemaId ) body = { "schemaDefinition" : schemaDefinition } resp = requests . put ( req , auth = self . credentials , headers = { "Content-Type" : "application/json" } , data = json . dumps ( body ) , verify = self . verify ) if resp . status_code == 200 : self . logger . debug ( "Schema updated" ) else : raise ibmiotf . APIException ( resp . status_code , "HTTP error updating schema" , resp ) return resp . json ( )
def disconnect ( self ) : self . client . disconnect ( ) self . client . loop_stop ( ) self . logger . info ( "Closed connection to the IBM Watson IoT Platform" )
def get ( self , deviceUid , eventId ) : if not isinstance ( deviceUid , DeviceUid ) and isinstance ( deviceUid , dict ) : deviceUid = DeviceUid ( * * deviceUid ) url = "api/v0002/device/types/%s/devices/%s/events/%s" % ( deviceUid . typeId , deviceUid . deviceId , eventId ) r = self . _apiClient . get ( url ) if r . status_code == 200 : return LastEvent ( * * r . json ( ) ) else : raise ApiException ( r )
def getAll ( self , deviceUid ) : if not isinstance ( deviceUid , DeviceUid ) and isinstance ( deviceUid , dict ) : deviceUid = DeviceUid ( * * deviceUid ) url = "api/v0002/device/types/%s/devices/%s/events" % ( deviceUid . typeId , deviceUid . deviceId ) r = self . _apiClient . get ( url ) if r . status_code == 200 : events = [ ] for event in r . json ( ) : events . append ( LastEvent ( * * event ) ) return events else : raise ApiException ( r )
def loadByteArray ( self , page , returnError ) : returnError . contents . value = self . IllegalStateError raise NotImplementedError ( "You must override this method." ) return ''
def check_return ( result , func , cargs ) : if result != 0 : s = rt . Error_GetLastErrorMsg ( ) . decode ( ) msg = 'LASError in "%s": %s' % ( func . __name__ , s ) rt . Error_Reset ( ) raise RTreeError ( msg ) return True
def check_void ( result , func , cargs ) : if not bool ( result ) : s = rt . Error_GetLastErrorMsg ( ) . decode ( ) msg = 'Error in "%s": %s' % ( func . __name__ , s ) rt . Error_Reset ( ) raise RTreeError ( msg ) return result
def check_void_done ( result , func , cargs ) : if rt . Error_GetErrorCount ( ) : s = rt . Error_GetLastErrorMsg ( ) . decode ( ) msg = 'Error in "%s": %s' % ( func . __name__ , s ) rt . Error_Reset ( ) raise RTreeError ( msg ) return result
def load ( self ) : if isinstance ( self . application , str ) : return util . import_app ( self . application ) else : return self . application
def init_app ( self , app ) : if not hasattr ( app , 'extensions' ) : app . extensions = { } if 'common' in app . extensions : raise RuntimeError ( "Flask-Common extension already initialized" ) app . extensions [ 'common' ] = self self . app = app if 'COMMON_FILESERVER_DISABLED' not in app . config : with app . test_request_context ( ) : app . wsgi_app = WhiteNoise ( app . wsgi_app , root = url_for ( 'static' , filename = '' ) [ 1 : ] ) self . cache = Cache ( app , config = { 'CACHE_TYPE' : app . config . get ( "COMMON_CACHE_TYPE" , 'simple' ) } ) @ app . before_request def before_request_callback ( ) : request . start_time = maya . now ( ) @ app . after_request def after_request_callback ( response ) : if 'COMMON_POWERED_BY_DISABLED' not in current_app . config : response . headers [ 'X-Powered-By' ] = 'Flask' if 'COMMON_PROCESSED_TIME_DISABLED' not in current_app . config : response . headers [ 'X-Processed-Time' ] = maya . now ( ) . epoch - request . start_time . epoch return response @ app . route ( '/favicon.ico' ) def favicon ( ) : return redirect ( url_for ( 'static' , filename = 'favicon.ico' ) , code = 301 )
def serve ( self , workers = None , * * kwargs ) : if self . app . debug : print ( crayons . yellow ( 'Booting Flask development server...' ) ) self . app . run ( ) else : print ( crayons . yellow ( 'Booting Gunicorn...' ) ) server = GunicornServer ( self . app , workers = workers or number_of_gunicorn_workers ( ) , worker_class = 'egg:meinheld#gunicorn_worker' , * * kwargs ) server . run ( )
def process_image ( self , image , image_format , save_kwargs = { } ) : imagefile = BytesIO ( ) inv_image = ImageOps . invert ( image ) inv_image . save ( imagefile , * * save_kwargs ) return imagefile
def to_python ( self , data ) : if data is not None : if hasattr ( data , 'open' ) : data . open ( ) return super ( VersatileImageFormField , self ) . to_python ( data )
def pre_save ( self , model_instance , add ) : file = super ( VersatileImageField , self ) . pre_save ( model_instance , add ) self . update_ppoi_field ( model_instance ) return file
def formfield ( self , * * kwargs ) : defaults = { } if self . ppoi_field : defaults [ 'form_class' ] = SizedImageCenterpointClickDjangoAdminField if kwargs . get ( 'widget' ) is AdminFileWidget : del kwargs [ 'widget' ] defaults . update ( kwargs ) return super ( VersatileImageField , self ) . formfield ( * * defaults )
def value_to_string ( self , obj ) : if DJANGO_VERSION > ( 1 , 9 ) : value = self . value_from_object ( obj ) else : value = self . _get_val_from_obj ( obj ) return self . get_prep_value ( value )
def build_filters_and_sizers ( self , ppoi_value , create_on_demand ) : name = self . name if not name and self . field . placeholder_image_name : name = self . field . placeholder_image_name self . filters = FilterLibrary ( name , self . storage , versatileimagefield_registry , ppoi_value , create_on_demand ) for ( attr_name , sizedimage_cls ) in iteritems ( versatileimagefield_registry . _sizedimage_registry ) : setattr ( self , attr_name , sizedimage_cls ( path_to_image = name , storage = self . storage , create_on_demand = create_on_demand , ppoi = ppoi_value ) )
def get_filtered_root_folder ( self ) : folder , filename = os . path . split ( self . name ) return os . path . join ( folder , VERSATILEIMAGEFIELD_FILTERED_DIRNAME , '' )
def get_sized_root_folder ( self ) : folder , filename = os . path . split ( self . name ) return os . path . join ( VERSATILEIMAGEFIELD_SIZED_DIRNAME , folder , '' )
def get_filtered_sized_root_folder ( self ) : sized_root_folder = self . get_sized_root_folder ( ) return os . path . join ( sized_root_folder , VERSATILEIMAGEFIELD_FILTERED_DIRNAME )
def retrieve_image ( self , path_to_image ) : image = self . storage . open ( path_to_image , 'rb' ) file_ext = path_to_image . rsplit ( '.' ) [ - 1 ] image_format , mime_type = get_image_metadata_from_file_ext ( file_ext ) return ( Image . open ( image ) , file_ext , image_format , mime_type )
def ppoi_as_str ( self ) : return "%s__%s" % ( str ( self . ppoi [ 0 ] ) . replace ( '.' , '-' ) , str ( self . ppoi [ 1 ] ) . replace ( '.' , '-' ) )
def get_context ( self , name , value , attrs ) : if self . has_template_widget_rendering : context = super ( ClearableFileInputWithImagePreview , self ) . get_context ( name , value , attrs ) else : context = { } context [ 'widget' ] = { 'name' : name , 'is_hidden' : self . is_hidden , 'required' : self . is_required , 'value' : self . _format_value ( value ) , 'attrs' : self . build_attrs ( self . attrs , attrs ) , 'template_name' : self . template_name , 'type' : self . input_type , } checkbox_name = self . clear_checkbox_name ( name ) checkbox_id = self . clear_checkbox_id ( checkbox_name ) context [ 'widget' ] . update ( { 'checkbox_name' : checkbox_name , 'checkbox_id' : checkbox_id , 'is_initial' : self . is_initial ( value ) , 'input_text' : self . input_text , 'initial_text' : self . initial_text , 'clear_checkbox_label' : self . clear_checkbox_label , } ) if value and hasattr ( value , "url" ) : context [ 'widget' ] . update ( { 'hidden_field_id' : self . get_hidden_field_id ( name ) , 'point_stage_id' : self . get_point_stage_id ( name ) , 'ppoi_id' : self . get_ppoi_id ( name ) , 'sized_url' : self . get_sized_url ( value ) , 'image_preview_id' : self . image_preview_id ( name ) , } ) return context
def build_attrs ( self , base_attrs , extra_attrs = None ) : attrs = base_attrs . copy ( ) if extra_attrs is not None : attrs . update ( extra_attrs ) return attrs
def get_filtered_path ( path_to_image , filename_key , storage ) : containing_folder , filename = os . path . split ( path_to_image ) filtered_filename = get_filtered_filename ( filename , filename_key ) path_to_return = os . path . join ( * [ containing_folder , VERSATILEIMAGEFIELD_FILTERED_DIRNAME , filtered_filename ] ) path_to_return = path_to_return . replace ( ' ' , '' ) return path_to_return
def get_url_from_image_key ( image_instance , image_key ) : img_key_split = image_key . split ( '__' ) if 'x' in img_key_split [ - 1 ] : size_key = img_key_split . pop ( - 1 ) else : size_key = None img_url = reduce ( getattr , img_key_split , image_instance ) if size_key : img_url = img_url [ size_key ] . url return img_url
def decode_bytecode ( bytecode ) : bytecode_wnd = memoryview ( bytecode ) while bytecode_wnd : opcode_id = byte2int ( bytecode_wnd [ 0 ] ) opcode = OPCODE_MAP [ opcode_id ] if opcode . imm_struct is not None : offs , imm , _ = opcode . imm_struct . from_raw ( None , bytecode_wnd [ 1 : ] ) else : imm = None offs = 0 insn_len = 1 + offs yield Instruction ( opcode , imm , insn_len ) bytecode_wnd = bytecode_wnd [ insn_len : ]
def decode_module ( module , decode_name_subsections = False ) : module_wnd = memoryview ( module ) hdr = ModuleHeader ( ) hdr_len , hdr_data , _ = hdr . from_raw ( None , module_wnd ) yield ModuleFragment ( hdr , hdr_data ) module_wnd = module_wnd [ hdr_len : ] while module_wnd : sec = Section ( ) sec_len , sec_data , _ = sec . from_raw ( None , module_wnd ) if ( decode_name_subsections and sec_data . id == SEC_UNK and sec_data . name == SEC_NAME ) : sec_wnd = sec_data . payload while sec_wnd : subsec = NameSubSection ( ) subsec_len , subsec_data , _ = subsec . from_raw ( None , sec_wnd ) yield ModuleFragment ( subsec , subsec_data ) sec_wnd = sec_wnd [ subsec_len : ] else : yield ModuleFragment ( sec , sec_data ) module_wnd = module_wnd [ sec_len : ]
def deprecated_func ( func ) : first_usage = [ True ] @ functools . wraps ( func ) def wrapper ( * args , * * kwargs ) : if first_usage [ 0 ] : warnings . warn ( "Call to deprecated function {}." . format ( func . __name__ ) , DeprecationWarning , ) first_usage [ 0 ] = False return func ( * args , * * kwargs ) return wrapper
def connect ( self ) : if self . loop is None : self . loop = asyncio . get_event_loop ( ) t = asyncio . Task ( self . loop . create_connection ( self . config [ 'protocol_factory' ] , self . config [ 'host' ] , self . config [ 'port' ] , ssl = self . config [ 'ssl' ] ) , loop = self . loop ) t . add_done_callback ( self . connection_made ) return t
def agi_code_check ( code = None , response = None , line = None ) : code = int ( code ) response = response or "" result = { 'status_code' : code , 'result' : ( '' , '' ) , 'msg' : '' } if code == 100 : result [ 'msg' ] = line elif code == 200 : for key , value , data in re_kv . findall ( response ) : result [ key ] = ( value , data ) if data == 'hangup' : return { 'error' : 'AGIResultHangup' , 'msg' : 'User hungup during execution' } elif key == 'result' and value == '-1' : return { 'error' : 'AGIAppError' , 'msg' : 'Error executing application, or hangup' } elif code == 510 : result [ 'error' ] = 'AGIInvalidCommand' elif code == 520 : result [ 'error' ] = 'AGIUsageError' result [ 'msg' ] = line else : result [ 'error' ] = 'AGIUnknownError' result [ 'msg' ] = line return result
def get_instances ( self ) : return [ "<%s prefix:%s (uid:%s)>" % ( self . __class__ . __name__ , i . prefix , self . uid ) for i in self . instances ]
def gc ( ) : def after_delete ( database ) : click . echo ( "Deleted table %s" % database ) app = get_app ( ) upgrade_from_old_version ( app ) app . delete_orphan_snapshots ( after_delete )
def snapshot ( name ) : app = get_app ( ) upgrade_from_old_version ( app ) name = name or app . default_snapshot_name if app . get_snapshot ( name ) : click . echo ( "Snapshot with name %s already exists" % name ) sys . exit ( 1 ) else : def before_copy ( table_name ) : click . echo ( "Snapshotting database %s" % table_name ) app . create_snapshot ( name , before_copy = before_copy )
def list ( ) : snapshots = get_app ( ) . get_snapshots ( ) click . echo ( '\n' . join ( '%s: %s' % ( s . snapshot_name , humanize . naturaltime ( datetime . utcnow ( ) - s . created_at ) ) for s in snapshots ) )
def restore ( name ) : app = get_app ( ) if not name : snapshot = app . get_latest_snapshot ( ) if not snapshot : click . echo ( "Couldn't find any snapshots for project %s" % load_config ( ) [ 'project_name' ] ) sys . exit ( 1 ) else : snapshot = app . get_snapshot ( name ) if not snapshot : click . echo ( "Couldn't find snapshot with name %s.\n" "You can list snapshots with 'stellar list'" % name ) sys . exit ( 1 ) if not snapshot . slaves_ready : if app . is_copy_process_running ( snapshot ) : sys . stdout . write ( 'Waiting for background process(%s) to finish' % snapshot . worker_pid ) sys . stdout . flush ( ) while not snapshot . slaves_ready : sys . stdout . write ( '.' ) sys . stdout . flush ( ) sleep ( 1 ) app . db . session . refresh ( snapshot ) click . echo ( '' ) else : click . echo ( 'Background process missing, doing slow restore.' ) app . inline_slave_copy ( snapshot ) app . restore ( snapshot ) click . echo ( 'Restore complete.' )
def init ( ) : while True : url = click . prompt ( "Please enter the url for your database.\n\n" "For example:\n" "PostgreSQL: postgresql://localhost:5432/\n" "MySQL: mysql+pymysql://root@localhost/" ) if url . count ( '/' ) == 2 and not url . endswith ( '/' ) : url = url + '/' if ( url . count ( '/' ) == 3 and url . endswith ( '/' ) and url . startswith ( 'postgresql://' ) ) : connection_url = url + 'template1' else : connection_url = url engine = create_engine ( connection_url , echo = False ) try : conn = engine . connect ( ) except OperationalError as err : click . echo ( "Could not connect to database: %s" % url ) click . echo ( "Error message: %s" % err . message ) click . echo ( '' ) else : break if engine . dialect . name not in SUPPORTED_DIALECTS : click . echo ( "Your engine dialect %s is not supported." % ( engine . dialect . name ) ) click . echo ( "Supported dialects: %s" % ( ', ' . join ( SUPPORTED_DIALECTS ) ) ) if url . count ( '/' ) == 3 and url . endswith ( '/' ) : while True : click . echo ( "You have the following databases: %s" % ', ' . join ( [ db for db in list_of_databases ( conn ) if not db . startswith ( 'stellar_' ) ] ) ) db_name = click . prompt ( "Please enter the name of the database (eg. projectdb)" ) if database_exists ( conn , db_name ) : break else : click . echo ( "Could not find database %s" % db_name ) click . echo ( '' ) else : db_name = url . rsplit ( '/' , 1 ) [ - 1 ] url = url . rsplit ( '/' , 1 ) [ 0 ] + '/' name = click . prompt ( 'Please enter your project name (used internally, eg. %s)' % db_name , default = db_name ) raw_url = url if engine . dialect . name == 'postgresql' : raw_url = raw_url + 'template1' with open ( 'stellar.yaml' , 'w' ) as project_file : project_file . write ( . strip ( ) % { 'name' : name , 'raw_url' : raw_url , 'url' : url , 'db_name' : db_name } ) click . echo ( "Wrote stellar.yaml" ) click . echo ( '' ) if engine . dialect . name == 'mysql' : click . echo ( "Warning: MySQL support is still in beta." ) click . echo ( "Tip: You probably want to take a snapshot: stellar snapshot" )
def on_epoch_end ( self ) -> None : self . indexes = np . arange ( self . nrows ) if self . shuffle : np . random . shuffle ( self . indexes )
def process_text_constructor ( cleaner : Callable , tokenizer : Callable , append_indicators : bool , start_tok : str , end_tok : str ) : def process_text ( text ) : if append_indicators : return [ [ start_tok ] + tokenizer ( cleaner ( doc ) ) + [ end_tok ] for doc in text ] return [ tokenizer ( cleaner ( doc ) ) for doc in text ] return process_text
def process_text ( self , text : List [ str ] ) -> List [ List [ str ] ] : process_text = process_text_constructor ( cleaner = self . cleaner , tokenizer = self . tokenizer , append_indicators = self . append_indicators , start_tok = self . start_tok , end_tok = self . end_tok ) return process_text ( text )
def generate_doc_length_stats ( self ) : heuristic = self . heuristic_pct histdf = ( pd . DataFrame ( [ ( a , b ) for a , b in self . document_length_histogram . items ( ) ] , columns = [ 'bin' , 'doc_count' ] ) . sort_values ( by = 'bin' ) ) histdf [ 'cumsum_pct' ] = histdf . doc_count . cumsum ( ) / histdf . doc_count . sum ( ) self . document_length_stats = histdf self . doc_length_huerestic = histdf . query ( f'cumsum_pct >= {heuristic}' ) . bin . head ( 1 ) . values [ 0 ] logging . warning ( ' ' . join ( [ "Setting maximum document length to" , f'{self.doc_length_huerestic} based upon' , f'heuristic of {heuristic} percentile.\n' , 'See full histogram by insepecting the' , "`document_length_stats` attribute." ] ) ) self . padding_maxlen = self . doc_length_huerestic
def token_count_pandas ( self ) : freq_df = pd . DataFrame . from_dict ( self . indexer . word_counts , orient = 'index' ) freq_df . columns = [ 'count' ] return freq_df . sort_values ( 'count' , ascending = False )
def _inv_cls ( cls ) : if cls . _fwdm_cls is cls . _invm_cls : return cls if not getattr ( cls , '_inv_cls_' , None ) : class _Inv ( cls ) : _fwdm_cls = cls . _invm_cls _invm_cls = cls . _fwdm_cls _inv_cls_ = cls _Inv . __name__ = cls . __name__ + 'Inv' cls . _inv_cls_ = _Inv return cls . _inv_cls_
def _update_with_rollback ( self , on_dup , * args , * * kw ) : writelog = [ ] appendlog = writelog . append dedup_item = self . _dedup_item write_item = self . _write_item for ( key , val ) in _iteritems_args_kw ( * args , * * kw ) : try : dedup_result = dedup_item ( key , val , on_dup ) except DuplicationError : undo_write = self . _undo_write for dedup_result , write_result in reversed ( writelog ) : undo_write ( dedup_result , write_result ) raise if dedup_result is not _NOOP : write_result = write_item ( key , val , dedup_result ) appendlog ( ( dedup_result , write_result ) )
def copy ( self ) : copy = self . __class__ . __new__ ( self . __class__ ) copy . _fwdm = self . _fwdm . copy ( ) copy . _invm = self . _invm . copy ( ) copy . _init_inv ( ) return copy
def copy ( self ) : copy = self . __class__ . __new__ ( self . __class__ ) sntl = _Sentinel ( ) fwdm = self . _fwdm . copy ( ) invm = self . _invm . copy ( ) cur = sntl nxt = sntl . nxt for ( key , val ) in iteritems ( self ) : nxt = _Node ( cur , sntl ) cur . nxt = fwdm [ key ] = invm [ val ] = nxt cur = nxt sntl . prv = nxt copy . _sntl = sntl copy . _fwdm = fwdm copy . _invm = invm copy . _init_inv ( ) return copy
def clear ( self ) : self . _fwdm . clear ( ) self . _invm . clear ( ) self . _sntl . nxt = self . _sntl . prv = self . _sntl
def new_contact ( cls , address_book , supported_private_objects , version , localize_dates ) : return cls ( address_book , None , supported_private_objects , version , localize_dates )
def from_user_input ( cls , address_book , user_input , supported_private_objects , version , localize_dates ) : contact = cls ( address_book , None , supported_private_objects , version , localize_dates ) contact . _process_user_input ( user_input ) return contact
def _add_category ( self , categories ) : categories_obj = self . vcard . add ( 'categories' ) categories_obj . value = helpers . convert_to_vcard ( "category" , categories , ObjectType . list_with_strings )
def avail_archs ( self ) : return { ARM32 : ( KS_ARCH_ARM , KS_MODE_ARM ) , ARM64 : ( KS_ARCH_ARM64 , KS_MODE_LITTLE_ENDIAN ) , ARM_TB : ( KS_ARCH_ARM , KS_MODE_THUMB ) , HEXAGON : ( KS_ARCH_HEXAGON , KS_MODE_BIG_ENDIAN ) , MIPS32 : ( KS_ARCH_MIPS , KS_MODE_MIPS32 ) , MIPS64 : ( KS_ARCH_MIPS , KS_MODE_MIPS64 ) , PPC32 : ( KS_ARCH_PPC , KS_MODE_PPC32 ) , PPC64 : ( KS_ARCH_PPC , KS_MODE_PPC64 ) , SPARC32 : ( KS_ARCH_SPARC , KS_MODE_SPARC32 ) , SPARC64 : ( KS_ARCH_SPARC , KS_MODE_SPARC64 ) , SYSTEMZ : ( KS_ARCH_SYSTEMZ , KS_MODE_BIG_ENDIAN ) , X86_16 : ( KS_ARCH_X86 , KS_MODE_16 ) , X86_32 : ( KS_ARCH_X86 , KS_MODE_32 ) , X86_64 : ( KS_ARCH_X86 , KS_MODE_64 ) , }
def avail_archs ( self ) : return { ARM32 : ( CS_ARCH_ARM , CS_MODE_ARM ) , ARM64 : ( CS_ARCH_ARM64 , CS_MODE_LITTLE_ENDIAN ) , ARM_TB : ( CS_ARCH_ARM , CS_MODE_THUMB ) , MIPS32 : ( CS_ARCH_MIPS , CS_MODE_MIPS32 ) , MIPS64 : ( CS_ARCH_MIPS , CS_MODE_MIPS64 ) , SPARC32 : ( CS_ARCH_SPARC , CS_MODE_BIG_ENDIAN ) , SPARC64 : ( CS_ARCH_SPARC , CS_MODE_V9 ) , SYSTEMZ : ( CS_ARCH_SYSZ , CS_MODE_BIG_ENDIAN ) , X86_16 : ( CS_ARCH_X86 , CS_MODE_16 ) , X86_32 : ( CS_ARCH_X86 , CS_MODE_32 ) , X86_64 : ( CS_ARCH_X86 , CS_MODE_64 ) , }
def safe_input ( prompt ) : if sys . version_info < ( 3 , 0 ) : if isinstance ( prompt , compat . text_type ) : encoding = locale . getpreferredencoding ( ) or 'utf-8' prompt = prompt . encode ( encoding ) else : if not isinstance ( prompt , compat . text_type ) : prompt = prompt . decode ( ) return _input ( prompt )
def first ( self ) : lim = [ 0 , 1 ] if self . _limit : lim [ 0 ] = self . _limit [ 0 ] if not self . _filters and not self . _order_by : for ent in self : return ent return None ids = self . limit ( * lim ) . _search ( ) if ids : return self . _model . get ( ids [ 0 ] ) return None
def redis_prefix_lua ( conn , dest , index , prefix , is_first , pattern = None ) : tkey = '%s:%s' % ( index . partition ( ':' ) [ 0 ] , uuid . uuid4 ( ) ) start , end = _start_end ( prefix ) return _redis_prefix_lua ( conn , [ dest , tkey , index ] , [ start , end , pattern or prefix , int ( pattern is not None ) , int ( bool ( is_first ) ) ] )
def add ( self , obj ) : if self . null_session : return self . _init ( ) pk = obj . _pk if not pk . endswith ( ':None' ) : self . known [ pk ] = obj self . wknown [ pk ] = obj
def get ( self , pk ) : self . _init ( ) return self . known . get ( pk ) or self . wknown . get ( pk )
def register ( cls , type , reduce_func ) : if sys . version_info < ( 3 , ) : def dispatcher ( cls , obj ) : reduced = reduce_func ( obj ) cls . save_reduce ( obj = obj , * reduced ) cls . dispatch_table [ type ] = dispatcher else : cls . dispatch_table [ type ] = reduce_func
def Queue ( self , maxsize = 0 , reducers = None ) : from . queues import Queue return Queue ( maxsize , reducers = reducers , ctx = self . get_context ( ) )
def SimpleQueue ( self , reducers = None ) : from . queues import SimpleQueue return SimpleQueue ( reducers = reducers , ctx = self . get_context ( ) )
def _sendback_result ( result_queue , work_id , result = None , exception = None ) : try : result_queue . put ( _ResultItem ( work_id , result = result , exception = exception ) ) except BaseException as e : exc = _ExceptionWithTraceback ( e ) result_queue . put ( _ResultItem ( work_id , exception = exc ) )
def _ensure_executor_running ( self ) : with self . _processes_management_lock : if len ( self . _processes ) != self . _max_workers : self . _adjust_process_count ( ) self . _start_queue_management_thread ( )
def start ( self , initializer = None , initargs = ( ) ) : assert self . _state . value == State . INITIAL if ( initializer is not None and not hasattr ( initializer , '__call__' ) ) : raise TypeError ( 'initializer must be a callable' ) reader , writer = mp . Pipe ( duplex = False ) self . _process = Process ( target = type ( self ) . _run_server , args = ( self . _registry , self . _address , bytes ( self . _authkey ) , self . _serializer , writer , initializer , initargs ) , ) ident = ':' . join ( str ( i ) for i in self . _process . _identity ) self . _process . name = type ( self ) . __name__ + '-' + ident self . _process . start ( ) writer . close ( ) self . _address = reader . recv ( ) reader . close ( ) self . _state . value = State . STARTED self . shutdown = mp . util . Finalize ( self , type ( self ) . _finalize_manager , args = ( self . _process , self . _address , self . _authkey , self . _state , self . _Client ) , exitpriority = 0 )
def DupFd ( fd ) : popen_obj = get_spawning_popen ( ) if popen_obj is not None : return popen_obj . DupFd ( popen_obj . duplicate_for_child ( fd ) ) elif HAVE_SEND_HANDLE and sys . version_info [ : 2 ] > ( 3 , 3 ) : from multiprocessing import resource_sharer return resource_sharer . DupFd ( fd ) else : raise TypeError ( 'Cannot pickle connection object. This object can only be ' 'passed when spawning a new process' )
def _wait_job_completion ( self ) : if len ( self . _pending_work_items ) > 0 : warnings . warn ( "Trying to resize an executor with running jobs: " "waiting for jobs completion before resizing." , UserWarning ) mp . util . debug ( "Executor {} waiting for jobs completion before" " resizing" . format ( self . executor_id ) ) while len ( self . _pending_work_items ) > 0 : time . sleep ( 1e-3 )
def get_preparation_data ( name , init_main_module = True ) : _check_not_importing_main ( ) d = dict ( log_to_stderr = util . _log_to_stderr , authkey = bytes ( process . current_process ( ) . authkey ) , ) if util . _logger is not None : d [ 'log_level' ] = util . _logger . getEffectiveLevel ( ) if len ( util . _logger . handlers ) > 0 : h = util . _logger . handlers [ 0 ] d [ 'log_fmt' ] = h . formatter . _fmt sys_path = [ p for p in sys . path ] try : i = sys_path . index ( '' ) except ValueError : pass else : sys_path [ i ] = process . ORIGINAL_DIR d . update ( name = name , sys_path = sys_path , sys_argv = sys . argv , orig_dir = process . ORIGINAL_DIR , dir = os . getcwd ( ) ) if sys . platform != "win32" : from . import semaphore_tracker semaphore_tracker . ensure_running ( ) d [ 'tracker_pid' ] = semaphore_tracker . _semaphore_tracker . _pid if init_main_module : main_module = sys . modules [ '__main__' ] try : main_mod_name = getattr ( main_module . __spec__ , "name" , None ) except BaseException : main_mod_name = None if main_mod_name is not None : d [ 'init_main_from_name' ] = main_mod_name elif sys . platform != 'win32' or ( not WINEXE and not WINSERVICE ) : main_path = getattr ( main_module , '__file__' , None ) if main_path is not None : if ( not os . path . isabs ( main_path ) and process . ORIGINAL_DIR is not None ) : main_path = os . path . join ( process . ORIGINAL_DIR , main_path ) d [ 'init_main_from_path' ] = os . path . normpath ( main_path ) d [ 'main_path' ] = d [ 'init_main_from_path' ] return d
def prepare ( data ) : if 'name' in data : process . current_process ( ) . name = data [ 'name' ] if 'authkey' in data : process . current_process ( ) . authkey = data [ 'authkey' ] if 'log_to_stderr' in data and data [ 'log_to_stderr' ] : util . log_to_stderr ( ) if 'log_level' in data : util . get_logger ( ) . setLevel ( data [ 'log_level' ] ) if 'log_fmt' in data : import logging util . get_logger ( ) . handlers [ 0 ] . setFormatter ( logging . Formatter ( data [ 'log_fmt' ] ) ) if 'sys_path' in data : sys . path = data [ 'sys_path' ] if 'sys_argv' in data : sys . argv = data [ 'sys_argv' ] if 'dir' in data : os . chdir ( data [ 'dir' ] ) if 'orig_dir' in data : process . ORIGINAL_DIR = data [ 'orig_dir' ] if 'tracker_pid' in data : from . import semaphore_tracker semaphore_tracker . _semaphore_tracker . _pid = data [ "tracker_pid" ] if 'init_main_from_name' in data : _fixup_main_from_name ( data [ 'init_main_from_name' ] ) elif 'init_main_from_path' in data : _fixup_main_from_path ( data [ 'init_main_from_path' ] )
def close_fds ( keep_fds ) : keep_fds = set ( keep_fds ) . union ( [ 1 , 2 ] ) try : open_fds = set ( int ( fd ) for fd in os . listdir ( '/proc/self/fd' ) ) except FileNotFoundError : import resource max_nfds = resource . getrlimit ( resource . RLIMIT_NOFILE ) [ 0 ] open_fds = set ( fd for fd in range ( 3 , max_nfds ) ) open_fds . add ( 0 ) for i in open_fds - keep_fds : try : os . close ( i ) except OSError : pass
def _recursive_terminate_without_psutil ( process ) : try : _recursive_terminate ( process . pid ) except OSError as e : warnings . warn ( "Failed to kill subprocesses on this platform. Please" ) process . terminate ( ) process . join ( )
def _recursive_terminate ( pid ) : if sys . platform == "win32" : try : subprocess . check_output ( [ "taskkill" , "/F" , "/T" , "/PID" , str ( pid ) ] , stderr = None ) except subprocess . CalledProcessError as e : if e . returncode not in [ 1 , 128 , 255 ] : raise elif e . returncode == 1 : try : os . kill ( pid , signal . SIGTERM ) except OSError as e : if e . errno != errno . ESRCH : raise else : try : children_pids = subprocess . check_output ( [ "pgrep" , "-P" , str ( pid ) ] , stderr = None ) except subprocess . CalledProcessError as e : if e . returncode == 1 : children_pids = b'' else : raise children_pids = children_pids . decode ( ) . split ( '\n' ) [ : - 1 ] for cpid in children_pids : cpid = int ( cpid ) _recursive_terminate ( cpid ) try : os . kill ( pid , signal . SIGTERM ) except OSError as e : if e . errno != errno . ESRCH : raise
def _format_exitcodes ( exitcodes ) : str_exitcodes = [ "{}({})" . format ( _get_exitcode_name ( e ) , e ) for e in exitcodes if e is not None ] return "{" + ", " . join ( str_exitcodes ) + "}"
def main ( fd , verbose = 0 ) : signal . signal ( signal . SIGINT , signal . SIG_IGN ) signal . signal ( signal . SIGTERM , signal . SIG_IGN ) if _HAVE_SIGMASK : signal . pthread_sigmask ( signal . SIG_UNBLOCK , _IGNORED_SIGNALS ) for f in ( sys . stdin , sys . stdout ) : try : f . close ( ) except Exception : pass if verbose : sys . stderr . write ( "Main semaphore tracker is running\n" ) sys . stderr . flush ( ) cache = set ( ) try : with os . fdopen ( fd , 'rb' ) as f : for line in f : try : cmd , name = line . strip ( ) . split ( b':' ) if cmd == b'REGISTER' : name = name . decode ( 'ascii' ) cache . add ( name ) if verbose : sys . stderr . write ( "[SemaphoreTracker] register {}\n" . format ( name ) ) sys . stderr . flush ( ) elif cmd == b'UNREGISTER' : name = name . decode ( 'ascii' ) cache . remove ( name ) if verbose : sys . stderr . write ( "[SemaphoreTracker] unregister {}" ": cache({})\n" . format ( name , len ( cache ) ) ) sys . stderr . flush ( ) elif cmd == b'PROBE' : pass else : raise RuntimeError ( 'unrecognized command %r' % cmd ) except BaseException : try : sys . excepthook ( * sys . exc_info ( ) ) except BaseException : pass finally : if cache : try : warnings . warn ( 'semaphore_tracker: There appear to be %d ' 'leaked semaphores to clean up at shutdown' % len ( cache ) ) except Exception : pass for name in cache : try : try : sem_unlink ( name ) if verbose : sys . stderr . write ( "[SemaphoreTracker] unlink {}\n" . format ( name ) ) sys . stderr . flush ( ) except Exception as e : warnings . warn ( 'semaphore_tracker: %s: %r' % ( name , e ) ) finally : pass if verbose : sys . stderr . write ( "semaphore tracker shut down\n" ) sys . stderr . flush ( )
def event_processor ( self , frame , event , arg ) : out = self . debugger . intf [ - 1 ] . output lineno = frame . f_lineno filename = self . core . canonic_filename ( frame ) filename = self . core . filename ( filename ) if not out : print ( "%s - %s:%d" % ( event , filename , lineno ) ) else : out . write ( "%s - %s:%d" % ( event , filename , lineno ) ) if arg is not None : out . writeline ( ', %s ' % repr ( arg ) ) else : out . writeline ( '' ) pass pass return self . event_processor
def run_hooks ( obj , hooks , * args ) : for hook in hooks : if hook ( obj , * args ) : return True pass return False
def forget ( self ) : self . stack = [ ] self . curindex = 0 self . curframe = None self . thread_name = None self . frame_thread_name = None return
def process_commands ( self ) : if self . core . execution_status != 'No program' : self . setup ( ) self . location ( ) pass leave_loop = run_hooks ( self , self . preloop_hooks ) self . continue_running = False while not leave_loop : try : run_hooks ( self , self . precmd_hooks ) leave_loop = self . process_command ( ) if leave_loop or self . continue_running : break except EOFError : if len ( self . debugger . intf ) > 1 : del self . debugger . intf [ - 1 ] self . last_command = '' else : if self . debugger . intf [ - 1 ] . output : self . debugger . intf [ - 1 ] . output . writeline ( 'Leaving' ) raise Mexcept . DebuggerQuit pass break pass pass return run_hooks ( self , self . postcmd_hooks )
def read_history_file ( self ) : histfile = self . debugger . intf [ - 1 ] . histfile try : import readline readline . read_history_file ( histfile ) except IOError : pass except ImportError : pass return
def write_history_file ( self ) : settings = self . debugger . settings histfile = self . debugger . intf [ - 1 ] . histfile if settings [ 'hist_save' ] : try : import readline try : readline . write_history_file ( histfile ) except IOError : pass except ImportError : pass pass return
def errmsg ( self , msg , prefix = "** " ) : if not self . verbose : location = ( "%s:%s: Error in source command file" % ( self . script_name , self . input_lineno ) ) msg = "%s%s:\n%s%s" % ( prefix , location , prefix , msg ) else : msg = "%s%s" % ( prefix , msg ) pass self . msg ( msg ) if self . abort_on_error : raise EOFError return
def close ( self ) : self . state = 'closing' if self . input : self . input . close ( ) pass if self . output : self . output . close ( ) pass self . state = 'disconnnected' return
def disassemble ( msg , msg_nocr , section , co , lasti = - 1 , start_line = - 1 , end_line = None , relative_pos = False , highlight = 'light' , start_offset = 0 , end_offset = None ) : return disassemble_bytes ( msg , msg_nocr , co . co_code , lasti , co . co_firstlineno , start_line , end_line , relative_pos , co . co_varnames , co . co_names , co . co_consts , co . co_cellvars , co . co_freevars , dict ( findlinestarts ( co ) ) , highlight , start_offset = start_offset , end_offset = end_offset )
def count_frames ( frame , count_start = 0 ) : count = - count_start while frame : count += 1 frame = frame . f_back return count
def print_stack_trace ( proc_obj , count = None , color = 'plain' , opts = { } ) : if count is None : n = len ( proc_obj . stack ) else : n = min ( len ( proc_obj . stack ) , count ) try : for i in range ( n ) : print_stack_entry ( proc_obj , i , color = color , opts = opts ) except KeyboardInterrupt : pass return
def eval_print_obj ( arg , frame , format = None , short = False ) : try : if not frame : val = eval ( arg , None , None ) else : val = eval ( arg , frame . f_globals , frame . f_locals ) pass except : return 'No symbol "' + arg + '" in current context.' return print_obj ( arg , val , format , short )
def print_obj ( arg , val , format = None , short = False ) : what = arg if format : what = format + ' ' + arg val = Mprint . printf ( val , format ) pass s = '%s = %s' % ( what , val ) if not short : s += '\n  type = %s' % type ( val ) s = print_dict ( s , val , "object variables" ) if hasattr ( val , "__class__" ) : s = print_dict ( s , val . __class__ , "class variables" ) pass pass return s
def lookup ( self , subcmd_prefix ) : for subcmd_name in list ( self . subcmds . keys ( ) ) : if subcmd_name . startswith ( subcmd_prefix ) and len ( subcmd_prefix ) >= self . subcmds [ subcmd_name ] . __class__ . min_abbrev : return self . subcmds [ subcmd_name ] pass return None
def short_help ( self , subcmd_cb , subcmd_name , label = False ) : entry = self . lookup ( subcmd_name ) if entry : if label : prefix = entry . name else : prefix = '' pass if hasattr ( entry , 'short_help' ) : if prefix : prefix += ' -- ' self . cmd_obj . msg ( prefix + entry . short_help ) pass pass else : self . undefined_subcmd ( "help" , subcmd_name ) pass return
def run ( self , subcmd_name , arg ) : entry = self . lookup ( subcmd_name ) if entry : entry [ 'callback' ] ( arg ) else : self . cmdproc . undefined_cmd ( entry . __class__ . name , subcmd_name ) pass return
def help ( self , * args ) : print ( args ) subcmd_prefix = args [ 0 ] if not subcmd_prefix or len ( subcmd_prefix ) == 0 : self . msg ( self . doc ) self . msg ( % ( self . name ) ) for subcmd_name in self . list ( ) : self . _subcmd_helper ( subcmd_name , self , True , True ) return entry = self . lookup ( subcmd_prefix ) if entry and hasattr ( entry , 'help' ) : entry . help ( args ) else : self . cmd_obj . errmsg ( "Unknown 'help %s' subcommand %s" % ( self . name , subcmd_prefix ) )
def list_categories ( self ) : self . section ( "Classes of commands:" ) cats = list ( categories . keys ( ) ) cats . sort ( ) for cat in cats : self . msg ( "  %-13s -- %s" % ( cat , categories [ cat ] ) ) pass final_msg = for line in re . compile ( '\n' ) . split ( final_msg . rstrip ( '\n' ) ) : self . rst_msg ( line ) pass return
def show_category ( self , category , args ) : n2cmd = self . proc . commands names = list ( n2cmd . keys ( ) ) if len ( args ) == 1 and args [ 0 ] == '*' : self . section ( "Commands in class %s:" % category ) cmds = [ cmd for cmd in names if category == n2cmd [ cmd ] . category ] cmds . sort ( ) self . msg_nocr ( self . columnize_commands ( cmds ) ) return self . msg ( "%s.\n" % categories [ category ] ) self . section ( "List of commands:" ) names . sort ( ) for name in names : if category != n2cmd [ name ] . category : continue self . msg ( "%-13s -- %s" % ( name , n2cmd [ name ] . short_help , ) ) pass return
def run ( self , args ) : if not self . proc . curframe : self . errmsg ( "No line number information available." ) return if len ( args ) == 3 : answer = self . lineinfo ( args [ 2 ] ) if answer [ 0 ] : item , filename , lineno = answer if not os . path . isfile ( filename ) : filename = Mclifns . search_file ( filename , self . core . search_path , self . main_dirname ) self . msg ( 'Line %s of "%s" <%s>' % ( lineno , filename , item ) ) return filename = self . core . canonic_filename ( self . proc . curframe ) if not os . path . isfile ( filename ) : filename = Mclifns . search_file ( filename , self . core . search_path , self . main_dirname ) pass filename = self . core . canonic_filename ( self . proc . curframe ) msg1 = 'Line %d of \"%s\"' % ( inspect . getlineno ( self . proc . curframe ) , self . core . filename ( filename ) ) msg2 = ( 'at instruction %d' % self . proc . curframe . f_lasti ) if self . proc . event : msg2 += ', %s event' % self . proc . event pass self . msg ( Mmisc . wrapped_lines ( msg1 , msg2 , self . settings [ 'width' ] ) ) return False
def map_thread_names ( ) : name2id = { } for thread_id in list ( threading . _active . keys ( ) ) : thread = threading . _active [ thread_id ] name = thread . getName ( ) if name not in list ( name2id . keys ( ) ) : name2id [ name ] = thread_id pass pass return name2id
def open ( self , inp , opts = None ) : if isinstance ( inp , list ) : self . input = inp else : raise IOError ( "Invalid input type (%s) for %s" % ( type ( inp ) , inp ) ) return
def get_int ( errmsg , arg , default = 1 , cmdname = None ) : if arg : try : default = int ( eval ( arg ) ) except ( SyntaxError , NameError , ValueError ) : if cmdname : errmsg ( "Command '%s' expects an integer; got: %s." % ( cmdname , str ( arg ) ) ) else : errmsg ( 'Expecting an integer, got: %s.' % str ( arg ) ) pass raise ValueError return default
def run_show_int ( obj , what = None ) : val = obj . debugger . settings [ obj . name ] if not what : what = obj . name return obj . msg ( "%s is %d." % ( what , val ) )
def run_show_val ( obj , name ) : val = obj . debugger . settings [ obj . name ] obj . msg ( "%s is %s." % ( obj . name , obj . cmd . proc . _saferepr ( val ) , ) ) return False
def is_def_stmt ( line , frame ) : return ( line and _re_def . match ( line ) and op_at_frame ( frame ) == 'LOAD_CONST' and stmt_contains_opcode ( frame . f_code , frame . f_lineno , 'MAKE_FUNCTION' ) )
def is_class_def ( line , frame ) : return ( line and _re_class . match ( line ) and stmt_contains_opcode ( frame . f_code , frame . f_lineno , 'BUILD_CLASS' ) )
def nothread_quit ( self , arg ) : self . debugger . core . stop ( ) self . debugger . core . execution_status = 'Quit command' raise Mexcept . DebuggerQuit
def threaded_quit ( self , arg ) : threading_list = threading . enumerate ( ) mythread = threading . currentThread ( ) for t in threading_list : if t != mythread : ctype_async_raise ( t , Mexcept . DebuggerQuit ) pass pass raise Mexcept . DebuggerQuit
def main ( dbg = None , sys_argv = list ( sys . argv ) ) : global __title__ orig_sys_argv = list ( sys_argv ) opts , dbg_opts , sys_argv = process_options ( __title__ , __version__ , sys_argv ) dbg_opts [ 'orig_sys_argv' ] = sys_argv dbg_opts [ 'interface' ] = Mbullwinkle . BWInterface ( ) dbg_opts [ 'processor' ] = 'bullwinkle' if dbg is None : dbg = Mdebugger . Trepan ( dbg_opts ) dbg . core . add_ignore ( main ) pass _postprocess_options ( dbg , opts ) if len ( sys_argv ) == 0 : mainpyfile = None else : mainpyfile = sys_argv [ 0 ] if not os . path . isfile ( mainpyfile ) : mainpyfile = Mclifns . whence_file ( mainpyfile ) is_readable = Mfile . readable ( mainpyfile ) if is_readable is None : print ( "%s: Python script file '%s' does not exist" % ( __title__ , mainpyfile , ) ) sys . exit ( 1 ) elif not is_readable : print ( "%s: Can't read Python script file '%s'" % ( __title__ , mainpyfile , ) ) sys . exit ( 1 ) return mainpyfile_noopt = Mfile . file_pyc2py ( mainpyfile ) if mainpyfile != mainpyfile_noopt and Mfile . readable ( mainpyfile_noopt ) : print ( "%s: Compiled Python script given and we can't use that." % __title__ ) print ( "%s: Substituting non-compiled name: %s" % ( __title__ , mainpyfile_noopt , ) ) mainpyfile = mainpyfile_noopt pass sys . path [ 0 ] = dbg . main_dirname = os . path . dirname ( mainpyfile ) dbg . sig_received = False while True : try : if dbg . program_sys_argv and mainpyfile : normal_termination = dbg . run_script ( mainpyfile ) if not normal_termination : break else : dbg . core . execution_status = 'No program' dbg . core . processor . process_commands ( ) pass dbg . core . execution_status = 'Terminated' dbg . intf [ - 1 ] . msg ( "The program finished - quit or restart" ) dbg . core . processor . process_commands ( ) except Mexcept . DebuggerQuit : break except Mexcept . DebuggerRestart : dbg . core . execution_status = 'Restart requested' if dbg . program_sys_argv : sys . argv = list ( dbg . program_sys_argv ) part1 = ( 'Restarting %s with arguments:' % dbg . core . filename ( mainpyfile ) ) args = ' ' . join ( dbg . program_sys_argv [ 1 : ] ) dbg . intf [ - 1 ] . msg ( Mmisc . wrapped_lines ( part1 , args , dbg . settings [ 'width' ] ) ) else : break except SystemExit : break pass sys . argv = orig_sys_argv return
def signature ( frame ) : if not frame : return None code = frame . f_code return ( code . co_name , code . co_filename , code . co_firstlineno )
def all ( self ) : found = False s = [ ] for display in self . list : if not found : s . append ( ) found = True pass s . append ( display . format ( ) ) return s
def delete_index ( self , display_number ) : old_size = len ( self . list ) self . list = [ disp for disp in self . list if display_number != disp . number ] return old_size != len ( self . list )
def display ( self , frame ) : if not frame : return s = [ ] sig = signature ( frame ) for display in self . list : if display . signature == sig and display . enabled : s . append ( display . to_s ( frame ) ) pass pass return s
def debug ( frame = None ) : if frame is None : frame = _frame ( ) . f_back dbg = RemoteCeleryTrepan ( ) dbg . say ( BANNER . format ( self = dbg ) ) trepan . api . debug ( dbg_opts = dbg . dbg_opts )
def run ( self , args ) : if len ( args ) < 2 : self . section ( "List of %s commands (with minimum abbreviation in " "parenthesis):" % self . name ) for subcmd_name in self . cmds . list ( ) : subcmd = self . cmds . subcmds [ subcmd_name ] self . summary_help ( subcmd_name , subcmd ) pass return False subcmd_prefix = args [ 1 ] subcmd = self . cmds . lookup ( subcmd_prefix ) if subcmd : nargs = len ( args ) - 2 if nargs < subcmd . min_args : self . errmsg ( ( "Subcommand '%s %s' needs at least %d argument(s); " + "got %d." ) % ( self . name , subcmd . name , subcmd . min_args , nargs ) ) return False if subcmd . max_args is not None and nargs > subcmd . max_args : self . errmsg ( ( "Subcommand '%s %s' takes at most %d argument(s); " + "got %d." ) % ( self . name , subcmd . name , subcmd . max_args , nargs ) ) return False return subcmd . run ( args [ 2 : ] ) else : return self . undefined_subcmd ( self . name , subcmd_prefix ) return
def undefined_subcmd ( self , cmd , subcmd ) : self . proc . intf [ - 1 ] . errmsg ( ( 'Undefined "%s" subcommand: "%s". ' + 'Try "help %s *".' ) % ( cmd , subcmd , cmd ) ) return
def info_signal ( self , args ) : if len ( args ) == 0 : return None signame = args [ 0 ] if signame in [ 'handle' , 'signal' ] : if len ( args ) == 1 : self . dbgr . core . processor . section ( self . header ) for signame in self . siglist : self . print_info_signal_entry ( signame ) return True else : signame = args [ 1 ] pass pass signame = self . is_name_or_number ( signame ) self . dbgr . core . processor . section ( self . header ) self . print_info_signal_entry ( signame ) return True
def handle_print ( self , signame , set_print ) : if set_print : self . sigs [ signame ] . print_method = self . dbgr . intf [ - 1 ] . msg else : self . sigs [ signame ] . print_method = None pass return set_print
def handle ( self , signum , frame ) : if self . print_method : self . print_method ( '\nProgram received signal %s.' % self . signame ) if self . print_stack : import traceback strings = traceback . format_stack ( frame ) for s in strings : if s [ - 1 ] == '\n' : s = s [ 0 : - 1 ] self . print_method ( s ) pass pass if self . b_stop : core = self . dbgr . core old_trace_hook_suspend = core . trace_hook_suspend core . trace_hook_suspend = True core . stop_reason = ( 'intercepting signal %s (%d)' % ( self . signame , signum ) ) core . processor . event_processor ( frame , 'signal' , signum ) core . trace_hook_suspend = old_trace_hook_suspend pass if self . pass_along : if self . old_handler : self . old_handler ( signum , frame ) pass pass return
def file2module ( filename ) : basename = osp . basename ( filename ) if '.' in basename : pos = basename . rfind ( '.' ) return basename [ : pos ] else : return basename return None
def print_obj ( arg , frame , format = None , short = False ) : try : if not frame : obj = eval ( arg , None , None ) else : obj = eval ( arg , frame . f_globals , frame . f_locals ) pass except : return 'No symbol "' + arg + '" in current context.' what = arg if format : what = format + ' ' + arg obj = printf ( obj , format ) s = '%s = %s' % ( what , obj ) if not short : s += '\ntype = %s' % type ( obj ) if callable ( obj ) : argspec = print_argspec ( obj , arg ) if argspec : s += ':\n\t' if inspect . isclass ( obj ) : s += 'Class constructor information:\n\t' obj = obj . __init__ elif isinstance ( obj , types . InstanceType ) : obj = obj . __call__ pass s += argspec pass s = print_dict ( s , obj , "object variables" ) if hasattr ( obj , "__class__" ) : s = print_dict ( s , obj . __class__ , "class variables" ) pass return s
def pyfiles ( callername , level = 2 ) : d = os . path . dirname ( callername ) glob ( os . path . join ( d , '[a-zA-Z]*.py' ) ) py_files = glob ( os . path . join ( d , '[a-zA-Z]*.py' ) ) return [ os . path . basename ( filename [ 0 : - 3 ] ) for filename in py_files ]
def _populate_cmd_lists ( self ) : self . commands = { } for cmd_instance in self . cmd_instances : cmd_name = cmd_instance . name self . commands [ cmd_name ] = cmd_instance pass return
def run ( self , args ) : mainfile = self . core . filename ( None ) if self . core . is_running ( ) : if mainfile : part1 = "Python program '%s' is stopped" % mainfile else : part1 = 'Program is stopped' pass if self . proc . event : msg = 'via a %s event.' % self . proc . event else : msg = '.' self . msg ( Mmisc . wrapped_lines ( part1 , msg , self . settings [ 'width' ] ) ) if self . proc . curframe : self . msg ( "PC offset is %d." % self . proc . curframe . f_lasti ) if self . proc . event == 'return' : val = self . proc . event_arg part1 = 'Return value is' self . msg ( Mmisc . wrapped_lines ( part1 , self . proc . _saferepr ( val ) , self . settings [ 'width' ] ) ) pass elif self . proc . event == 'exception' : exc_type , exc_value , exc_tb = self . proc . event_arg self . msg ( 'Exception type: %s' % self . proc . _saferepr ( exc_type ) ) if exc_value : self . msg ( 'Exception value: %s' % self . proc . _saferepr ( exc_value ) ) pass pass self . msg ( 'It stopped %s.' % self . core . stop_reason ) if self . proc . event in [ 'signal' , 'exception' , 'c_exception' ] : self . msg ( 'Note: we are stopped *after* running the ' 'line shown.' ) pass else : if mainfile : part1 = "Python program '%s'" % mainfile msg = "is not currently running. " self . msg ( Mmisc . wrapped_lines ( part1 , msg , self . settings [ 'width' ] ) ) else : self . msg ( 'No Python program is currently running.' ) pass self . msg ( self . core . execution_status ) pass return False
def columnize_commands ( self , commands ) : commands . sort ( ) width = self . debugger . settings [ 'width' ] return columnize . columnize ( commands , displaywidth = width , lineprefix = '    ' )
def close ( self ) : self . state = 'closing' if self . inout : self . inout . close ( ) pass self . state = 'closing connection' if self . conn : self . conn . close ( ) self . state = 'disconnected' return
def complete_identifier ( cmd , prefix ) : if not cmd . proc . curframe : return [ None ] ns = cmd . proc . curframe . f_globals . copy ( ) ns . update ( cmd . proc . curframe . f_locals ) if '.' in prefix : dotted = prefix . split ( '.' ) try : obj = ns [ dotted [ 0 ] ] for part in dotted [ 1 : - 1 ] : obj = getattr ( obj , part ) except ( KeyError , AttributeError ) : return [ ] pre_prefix = '.' . join ( dotted [ : - 1 ] ) + '.' return [ pre_prefix + n for n in dir ( obj ) if n . startswith ( dotted [ - 1 ] ) ] else : return Mcomplete . complete_token ( ns . keys ( ) , prefix )
def nothread_quit ( self , arg ) : self . debugger . core . stop ( ) self . debugger . core . execution_status = 'Quit command' self . proc . response [ 'event' ] = 'terminated' self . proc . response [ 'name' ] = 'status' self . proc . intf [ - 1 ] . msg ( self . proc . response ) raise Mexcept . DebuggerQuit
def is_started ( self ) : return ( tracer . is_started ( ) and not self . trace_hook_suspend and tracer . find_hook ( self . trace_dispatch ) )
def set_next ( self , frame , step_ignore = 0 , step_events = None ) : self . step_events = None self . stop_level = Mstack . count_frames ( frame ) self . last_level = self . stop_level self . last_frame = frame self . stop_on_finish = False self . step_ignore = step_ignore return
def stack_trace ( self , f ) : while f : if ( not self . core . ignore_filter . is_included ( f ) or self . settings [ 'dbg_trepan' ] ) : s = Mstack . format_stack_entry ( self , ( f , f . f_lineno ) ) self . msg ( " " * 4 + s ) pass f = f . f_back pass return
def checkfuncname ( b , frame ) : if not b . funcname : if b . line != frame . f_lineno : return False return True if frame . f_code . co_name != b . funcname : return False if not b . func_first_executable_line : b . func_first_executable_line = frame . f_lineno if b . func_first_executable_line != frame . f_lineno : return False return True
def delete_breakpoint_by_number ( self , bpnum ) : success , msg , bp = self . get_breakpoint ( bpnum ) if not success : return False , msg self . delete_breakpoint ( bp ) return ( True , '' )
def en_disable_all_breakpoints ( self , do_enable = True ) : bp_list = [ bp for bp in self . bpbynumber if bp ] bp_nums = [ ] if do_enable : endis = 'en' else : endis = 'dis' pass if not bp_list : return "No breakpoints to %sable" % endis for bp in bp_list : bp . enabled = do_enable bp_nums . append ( str ( bp . number ) ) pass return ( "Breakpoints %sabled: %s" % ( endis , ", " . join ( bp_nums ) ) )
def en_disable_breakpoint_by_number ( self , bpnum , do_enable = True ) : success , msg , bp = self . get_breakpoint ( bpnum ) if not success : return success , msg if do_enable : endis = 'en' else : endis = 'dis' pass if bp . enabled == do_enable : return ( False , ( 'Breakpoint (%r) previously %sabled' % ( str ( bpnum ) , endis , ) ) ) bp . enabled = do_enable return ( True , '' )
def open ( self , inp , opts = None ) : if isinstance ( inp , io . TextIOWrapper ) : self . input = inp elif isinstance ( inp , 'string' . __class__ ) : self . name = inp self . input = open ( inp , 'r' ) else : raise IOError ( "Invalid input type (%s) for %s" % ( inp . __class__ . __name__ , inp ) ) return
def restore_original_login ( request ) : original_session = request . session . get ( la_settings . USER_SESSION_FLAG ) logout ( request ) if not original_session : return try : original_user_pk = signer . unsign ( original_session , max_age = timedelta ( days = la_settings . USER_SESSION_DAYS_TIMESTAMP ) . total_seconds ( ) ) user = get_user_model ( ) . objects . get ( pk = original_user_pk ) messages . info ( request , la_settings . MESSAGE_LOGIN_REVERT . format ( username = user . __dict__ [ username_field ] ) , extra_tags = la_settings . MESSAGE_EXTRA_TAGS , ) login_as ( user , request , store_original_user = False ) if la_settings . USER_SESSION_FLAG in request . session : del request . session [ la_settings . USER_SESSION_FLAG ] except SignatureExpired : pass
def _load_module ( path ) : i = path . rfind ( "." ) module , attr = path [ : i ] , path [ i + 1 : ] try : mod = import_module ( module ) except ImportError : raise ImproperlyConfigured ( "Error importing CAN_LOGIN_AS function: {}" . format ( module ) ) except ValueError : raise ImproperlyConfigured ( "Error importing CAN_LOGIN_AS" " function. Is CAN_LOGIN_AS a" " string?" ) try : can_login_as = getattr ( mod , attr ) except AttributeError : raise ImproperlyConfigured ( "Module {0} does not define a {1} " "function." . format ( module , attr ) ) return can_login_as
def _main ( argv ) : parser = argparse . ArgumentParser ( description = DESCRIPTION , formatter_class = argparse . RawDescriptionHelpFormatter ) parser . add_argument ( '-b' , '--base-url' , default = URL_BASE , help = 'API root url, default: %s' % URL_BASE , ) parser . add_argument ( '-e' , '--expanded' , help = "Include Luminoso's analysis of each document, such as terms and" ' document vectors' , action = 'store_true' , ) parser . add_argument ( '-t' , '--token' , help = 'API authentication token' ) parser . add_argument ( '-s' , '--save-token' , action = 'store_true' , help = 'save --token for --base-url to ~/.luminoso/tokens.json' , ) parser . add_argument ( 'project_id' , help = 'The ID of the project in the Daylight API' ) parser . add_argument ( 'output_file' , nargs = '?' , default = None , help = 'The JSON lines (.jsons) file to write to' ) args = parser . parse_args ( argv ) if args . save_token : if not args . token : raise ValueError ( "error: no token provided" ) LuminosoClient . save_token ( args . token , domain = urlparse ( args . base_url ) . netloc ) client = LuminosoClient . connect ( url = args . base_url , token = args . token ) proj_client = client . client_for_path ( 'projects/{}' . format ( args . project_id ) ) download_docs ( proj_client , args . output_file , args . expanded )
def stream_json_lines ( file ) : if isinstance ( file , string_type ) : file = open ( file , 'rb' ) for line in file : line = line . strip ( ) if line : if isinstance ( line , bytes ) : line = line . decode ( 'utf-8' ) yield json . loads ( line )
def _get_default_account ( self ) : newclient = self . __class__ ( self . session , self . root_url ) account_info = newclient . get ( '/accounts/' ) if account_info [ 'default_account' ] is not None : return account_info [ 'default_account' ] valid_accounts = [ a [ 'account_id' ] for a in account_info [ 'accounts' ] if a [ 'account_id' ] != 'public' ] if len ( valid_accounts ) == 0 : raise ValueError ( "Can't determine your default URL. " "Please request a specific URL or ask " "Luminoso for support." ) return valid_accounts [ 0 ]
def documentation ( self ) : newclient = self . __class__ ( self . session , self . root_url ) return newclient . get_raw ( '/' )
def _print_csv ( result ) : if type ( result ) is not list : raise TypeError ( "output not able to be displayed as CSV." ) first_line = result [ 0 ] w = csv . DictWriter ( sys . stdout , fieldnames = sorted ( first_line . keys ( ) ) ) w . writeheader ( ) for line in result : w . writerow ( line )
def _read_params ( input_file , json_body , p_params ) : params = { } try : if input_file : params . update ( json . load ( input_file ) ) if json_body is not None : params . update ( json . loads ( json_body ) ) except ValueError as e : raise ValueError ( "input is not valid JSON: %s" % e ) try : params . update ( { p . split ( '=' , 1 ) [ 0 ] : p . split ( '=' , 1 ) [ 1 ] for p in p_params } ) except IndexError : raise ValueError ( "--param arguments must have key=value format" ) return params
def _batches ( iterable , size ) : sourceiter = iter ( iterable ) while True : try : batchiter = islice ( sourceiter , size ) yield chain ( [ next ( batchiter ) ] , batchiter ) except StopIteration : return
def _simplify_doc ( doc ) : doc = dict ( doc ) if 'text' not in doc : raise ValueError ( "The document {!r} has no text field" . format ( doc ) ) return { 'text' : doc [ 'text' ] , 'metadata' : doc . get ( 'metadata' , [ ] ) , 'title' : doc . get ( 'title' , '' ) }
def create_project_with_docs ( client , docs , language , name , account = None , progress = False ) : description = 'Uploaded using lumi-upload at {}' . format ( time . asctime ( ) ) if account is not None : proj_record = client . post ( 'projects' , name = name , language = language , description = description , account_id = account , ) else : proj_record = client . post ( 'projects' , name = name , language = language , description = description ) proj_id = proj_record [ 'project_id' ] proj_client = client . client_for_path ( 'projects/' + proj_id ) try : if progress : progress_bar = tqdm ( desc = 'Uploading documents' ) else : progress_bar = None for batch in _batches ( docs , BATCH_SIZE ) : docs_to_upload = [ _simplify_doc ( doc ) for doc in batch ] proj_client . post ( 'upload' , docs = docs_to_upload ) if progress : progress_bar . update ( BATCH_SIZE ) finally : if progress : progress_bar . close ( ) print ( 'The server is building project {!r}.' . format ( proj_id ) ) proj_client . post ( 'build' ) while True : time . sleep ( 10 ) proj_status = proj_client . get ( ) build_info = proj_status [ 'last_build_info' ] if 'success' in build_info : if not build_info [ 'success' ] : raise LuminosoServerError ( build_info [ 'reason' ] ) return proj_status
def _main ( argv ) : parser = argparse . ArgumentParser ( description = DESCRIPTION , formatter_class = argparse . RawDescriptionHelpFormatter , ) parser . add_argument ( '-b' , '--base-url' , default = URL_BASE , help = 'API root url, default: %s' % URL_BASE , ) parser . add_argument ( '-a' , '--account-id' , default = None , help = 'Account ID that should own the project, if not the default' , ) parser . add_argument ( '-l' , '--language' , default = 'en' , help = 'The language code for the language the text is in. Default: en' , ) parser . add_argument ( '-t' , '--token' , help = "API authentication token" ) parser . add_argument ( '-s' , '--save-token' , action = 'store_true' , help = 'save --token for --base-url to ~/.luminoso/tokens.json' , ) parser . add_argument ( 'input_filename' , help = 'The JSON-lines (.jsons) file of documents to upload' , ) parser . add_argument ( 'project_name' , nargs = '?' , default = None , help = 'What the project should be called' , ) args = parser . parse_args ( argv ) if args . save_token : if not args . token : raise ValueError ( "error: no token provided" ) LuminosoClient . save_token ( args . token , domain = urlparse ( args . base_url ) . netloc ) client = LuminosoClient . connect ( url = args . base_url , token = args . token ) name = args . project_name if name is None : name = input ( 'Enter a name for the project: ' ) if not name : print ( 'Aborting because no name was provided.' ) return result = upload_docs ( client , args . input_filename , args . language , name , account = args . account_id , progress = True , ) print ( 'Project {!r} created with {} documents' . format ( result [ 'project_id' ] , result [ 'document_count' ] ) )
def batches ( iterable , size ) : sourceiter = iter ( iterable ) while True : batchiter = islice ( sourceiter , size ) yield chain ( [ next ( batchiter ) ] , batchiter )
def _post_login_page ( self ) : data = { 'IDToken1' : self . username , 'IDToken2' : self . password , 'SunQueryParamsString' : base64 . b64encode ( b'realm=particuliers' ) , 'encoded' : 'true' , 'gx_charset' : 'UTF-8' } try : self . _session . post ( LOGIN_URL , data = data , allow_redirects = False , timeout = self . _timeout ) except OSError : raise PyLinkyError ( "Can not submit login form" ) if 'iPlanetDirectoryPro' not in self . _session . cookies : raise PyLinkyError ( "Login error: Please check your username/password." ) return True
def fetch_data ( self ) : for t in [ HOURLY , DAILY , MONTHLY , YEARLY ] : self . _data [ t ] = self . get_data_per_period ( t )
def prepare ( self ) : if self . __class__ . view : return #: Load the View class from the dotted view name with enaml . imports ( ) : View = pydoc . locate ( self . page . view ) assert View , "Failed to import View: {}" . format ( self . page . view ) #: Set initial view properties self . __class__ . view = View ( site = self . site , page = self . page , request = self . request , )
def initialize ( self ) : if self . __class__ . view : self . view . handler = self self . view . request = self . request return #: Load the View class from the dotted view name with enaml . imports ( ) : from views . index import View #: Set initial view properties self . __class__ . view = View ( company = current_company , request = self . request , handler = self , )
def get ( self , * args , * * kwargs ) : #: Render view for get request, view is cached for websocket if self . is_websocket ( ) : return super ( DemoHandler , self ) . get ( * args , * * kwargs ) else : #return tornado.web.RequestHandler.get(self, *args, **kwargs) self . write ( self . view . render ( ) )
def on_message ( self , message ) : #: Decode message change = tornado . escape . json_decode ( message ) #print change #: Get the owner ID ref = change . get ( 'ref' ) if not ref : return #: Get the server side representation of the node #: If found will return the View declaration node node = self . view . xpath ( '//*[@ref="{}"]' . format ( ref ) , first = True ) if node is None : return #: Handle the event if change . get ( 'type' ) and change . get ( 'name' ) : if change [ 'type' ] == 'event' : #: Trigger the event trigger = getattr ( node , change [ 'name' ] ) trigger ( ) if change [ 'type' ] == 'update' : #: Trigger the update setattr ( node , change [ 'name' ] , change [ 'value' ] )
def _update_menus ( self , change ) : menus = { } #: Get all links links = [ p . link for p in self . pages if p . link ] + self . links #: Put all links in the correct menu for link in links : for menu in link . menus : if menu not in menus : menus [ menu ] = [ ] menus [ menu ] . append ( link ) #: Update the menus for name , menu in menus . items ( ) : k = '{}_menu' . format ( name ) if hasattr ( self , k ) : setattr ( self , k , menu )
def _default_handlers ( self ) : static_path = os . path . abspath ( os . path . join ( os . path . dirname ( __file__ ) , "static" ) ) urls = [ ( r"/static/(.*)" , cyclone . web . StaticFileHandler , { "path" : static_path } ) , ] for p in self . pages : handler = p . handler handler . site = self handler . page = p urls . append ( ( p . link . url , handler ) ) return urls
def set_attribute ( self , name , value ) : if value is True : self . widget . set ( name , name ) elif value is False : del self . widget . attrib [ name ] else : self . widget . set ( name , str ( value ) )
def xpath ( self , query , * * kwargs ) : nodes = self . proxy . find ( query , * * kwargs ) return [ n . declaration for n in nodes ]
def init_widget ( self ) : d = self . declaration if d . source : self . set_source ( d . source ) else : super ( RawComponent , self ) . init_widget ( )
def _observe_mode ( self , change ) : block = self . block if block and self . is_initialized and change [ 'type' ] == 'update' : if change [ 'oldvalue' ] == 'replace' : raise NotImplementedError for c in self . children : block . children . remove ( c ) c . set_parent ( None ) self . refresh_items ( )
def read ( * pathcomponents ) : with open ( join ( abspath ( dirname ( __file__ ) ) , * pathcomponents ) ) as thefile : return thefile . read ( )
def error ( msg , exit_code ) : sys . stderr . write ( "%s\ntry 'mongotail --help' for more information\n" % msg ) sys . stderr . flush ( ) exit ( exit_code )
def error_parsing ( msg = "unknown options" ) : sys . stderr . write ( "Error parsing command line: %s\ntry 'mongotail --help' for more information\n" % msg ) sys . stderr . flush ( ) exit ( EINVAL )
def draw ( self ) : self . screen . border ( 0 ) if self . title is not None : self . screen . addstr ( 2 , 2 , self . title , curses . A_STANDOUT ) if self . subtitle is not None : self . screen . addstr ( 4 , 2 , self . subtitle , curses . A_BOLD ) for index , item in enumerate ( self . items ) : if self . current_option == index : text_style = self . highlight else : text_style = self . normal self . screen . addstr ( 5 + index , 4 , item . show ( index ) , text_style ) screen_rows , screen_cols = CursesMenu . stdscr . getmaxyx ( ) top_row = 0 if 6 + len ( self . items ) > screen_rows : if screen_rows + self . current_option < 6 + len ( self . items ) : top_row = self . current_option else : top_row = 6 + len ( self . items ) - screen_rows self . screen . refresh ( top_row , 0 , 0 , 0 , screen_rows - 1 , screen_cols - 1 )
def process_user_input ( self ) : user_input = self . get_input ( ) go_to_max = ord ( "9" ) if len ( self . items ) >= 9 else ord ( str ( len ( self . items ) ) ) if ord ( '1' ) <= user_input <= go_to_max : self . go_to ( user_input - ord ( '0' ) - 1 ) elif user_input == curses . KEY_DOWN : self . go_down ( ) elif user_input == curses . KEY_UP : self . go_up ( ) elif user_input == ord ( "\n" ) : self . select ( ) return user_input
def select ( self ) : self . selected_option = self . current_option self . selected_item . set_up ( ) self . selected_item . action ( ) self . selected_item . clean_up ( ) self . returned_value = self . selected_item . get_return ( ) self . should_exit = self . selected_item . should_exit if not self . should_exit : self . draw ( )
def show ( self , index ) : if self . menu and self . menu . parent : self . text = "Return to %s menu" % self . menu . parent . title else : self . text = "Exit" return super ( ExitItem , self ) . show ( index )
def action ( self ) : self . return_value = self . function ( * self . args , * * self . kwargs )
def action ( self ) : commandline = "{0} {1}" . format ( self . command , " " . join ( self . arguments ) ) try : completed_process = subprocess . run ( commandline , shell = True ) self . exit_status = completed_process . returncode except AttributeError : self . exit_status = subprocess . call ( commandline , shell = True )
def set_up ( self ) : self . menu . pause ( ) curses . def_prog_mode ( ) self . menu . clear_screen ( )
def clean_up ( self ) : self . submenu . join ( ) self . menu . clear_screen ( ) curses . reset_prog_mode ( ) curses . curs_set ( 1 ) curses . curs_set ( 0 ) self . menu . resume ( )
def add ( df , new_column , column_1 , column_2 ) : return _basic_math_operation ( df , new_column , column_1 , column_2 , op = 'add' )
def subtract ( df , new_column , column_1 , column_2 ) : return _basic_math_operation ( df , new_column , column_1 , column_2 , op = 'sub' )
def multiply ( df , new_column , column_1 , column_2 ) : return _basic_math_operation ( df , new_column , column_1 , column_2 , op = 'mul' )
def divide ( df , new_column , column_1 , column_2 ) : return _basic_math_operation ( df , new_column , column_1 , column_2 , op = 'truediv' )
def cumsum ( df , new_column : str , column : str , index : list , date_column : str , date_format : str ) : logging . getLogger ( __name__ ) . warning ( f"DEPRECATED: use compute_cumsum" ) date_temp = '__date_temp__' if isinstance ( index , str ) : index = [ index ] levels = list ( range ( 0 , len ( index ) ) ) df [ date_temp ] = pd . to_datetime ( df [ date_column ] , format = date_format ) reference_cols = [ date_temp , date_column ] df = df . groupby ( index + reference_cols ) . sum ( ) df [ new_column ] = df . groupby ( level = levels ) [ column ] . cumsum ( ) df . reset_index ( inplace = True ) del df [ date_temp ] return df
def log_message ( logger , message = "" ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : _log_message ( logger , func . __name__ , message ) result = func ( * args , * * kwargs ) return result return wrapper return decorator
def log_time ( logger ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : start = time . time ( ) result = func ( * args , * * kwargs ) end = time . time ( ) _log_time ( logger , func . __name__ , start , end ) return result return wrapper return decorator
def clean_cachedir_old_entries ( cachedir : StoreBackendBase , func_name : str , limit : int ) -> int : if limit < 1 : raise ValueError ( "'limit' must be greater or equal to 1" ) cache_entries = get_cachedir_entries ( cachedir , func_name ) cache_entries = sorted ( cache_entries , key = lambda e : e . last_access , reverse = True ) cache_entries_to_remove = cache_entries [ limit : ] for entry in cache_entries_to_remove : shutil . rmtree ( entry . path , ignore_errors = True ) return len ( cache_entries_to_remove )
def ada_family_core ( params , gparams , learning_rate = 0.01 , eps = 1e-6 , rho = 0.95 , method = "ADADELTA" , beta = 0.0 , gsum_regularization = 0.0001 ) : _ , _ , _ , args = inspect . getargvalues ( inspect . currentframe ( ) ) logging . info ( "ada_family_core: %s" % str ( args . items ( ) ) ) free_parameters = [ ] if method == "FINETUNING_ADAGRAD" : method = "ADAGRAD" gsum_regularization = 0 oneMinusBeta = 1 - beta gsums = [ theano . shared ( np . zeros_like ( param . get_value ( borrow = True ) , dtype = FLOATX ) , name = "gsum_%s" % param . name ) if ( method == 'ADADELTA' or method == 'ADAGRAD' ) else None for param in params ] xsums = [ theano . shared ( np . zeros_like ( param . get_value ( borrow = True ) , dtype = FLOATX ) , name = "xsum_%s" % param . name ) if method == 'ADADELTA' else None for param in params ] if method == 'ADAGRAD' : for gsum in gsums : gsum . set_value ( gsum . get_value ( ) ** 0 ) updates = OrderedDict ( ) for gparam , param , gsum , xsum in zip ( gparams , params , gsums , xsums ) : if method == 'ADADELTA' : updates [ gsum ] = rho * gsum + ( 1. - rho ) * ( gparam ** 2 ) dparam = - T . sqrt ( ( xsum + eps ) / ( updates [ gsum ] + eps ) ) * gparam updates [ xsum ] = rho * xsum + ( 1. - rho ) * ( dparam ** 2 ) updates [ param ] = param * oneMinusBeta + dparam elif method == 'ADAGRAD' : updates [ gsum ] = gsum + ( gparam ** 2 ) - gsum_regularization * gsum updates [ param ] = param * oneMinusBeta - learning_rate * ( gparam / ( T . sqrt ( updates [ gsum ] + eps ) ) ) else : updates [ param ] = param * oneMinusBeta - gparam * learning_rate if method == 'ADADELTA' : free_parameters . extend ( gsums + xsums ) elif method == 'ADAGRAD' : free_parameters . extend ( gsums ) for k in updates : if updates [ k ] . dtype != FLOATX : updates [ k ] = updates [ k ] . astype ( FLOATX ) return updates . items ( ) , free_parameters
def _learning_updates ( self ) : params = self . training_params ( ) gradients = self . get_gradients ( params ) return self . optimization_updates ( params , gradients )
def training_params ( self ) : params = self . network . parameters if self . config . fixed_parameters : logging . info ( "fixed parameters: %s" % ", " . join ( map ( str , self . config . fixed_parameters ) ) ) params = [ p for p in params if p not in self . config . fixed_parameters ] return params
def optimization_updates ( self , params , gradients ) : updates , free_parameters = optimize_updates ( params , gradients , self . config ) self . network . free_parameters . extend ( free_parameters ) logging . info ( "Added %d free parameters for optimization" % len ( free_parameters ) ) return updates
def _first_glimpse_sensor ( self , x_t ) : downsampled_img = theano . tensor . signal . downsample . max_pool_2d ( x_t , ( 4 , 4 ) ) downsampled_img = downsampled_img . flatten ( ) first_l = T . dot ( downsampled_img , self . W_f ) if self . disable_reinforce : wf_grad = self . W_f if self . random_glimpse : first_l = self . srng . uniform ( ( 2 , ) , low = - 1.7 , high = 1.7 ) else : sampled_l_t = self . _sample_gaussian ( first_l , self . cov ) sampled_pdf = self . _multi_gaussian_pdf ( disconnected_grad ( sampled_l_t ) , first_l ) wf_grad = T . grad ( T . log ( sampled_pdf ) , self . W_f ) first_l = sampled_l_t return first_l , wf_grad
def prepare ( self ) : self . output_dim = 10 self . encoder = Chain ( self . input_dim ) . stack ( Dense ( self . internal_layer_size , 'tanh' ) ) self . decoder = Chain ( self . internal_layer_size ) . stack ( Dense ( self . input_dim ) ) self . classifier = Chain ( self . internal_layer_size ) . stack ( Dense ( 50 , 'tanh' ) , Dense ( self . output_dim ) , Softmax ( ) ) self . register_inner_layers ( self . encoder , self . decoder , self . classifier ) self . target_input = T . ivector ( 'target' ) self . register_external_inputs ( self . target_input )
def compute_tensor ( self , x ) : internal_variable = self . encoder . compute_tensor ( x ) decoding_output = self . decoder . compute_tensor ( internal_variable ) classification_output = self . classifier . compute_tensor ( internal_variable ) auto_encoder_cost = AutoEncoderCost ( decoding_output , x ) . get ( ) classification_cost = CrossEntropyCost ( classification_output , self . target_input ) . get ( ) final_cost = 0.01 * auto_encoder_cost + classification_cost error_rate = ErrorRateCost ( classification_output , self . target_input ) . get ( ) self . register_monitors ( ( "err" , error_rate ) , ( "encoder_cost" , auto_encoder_cost ) , ( "classify_cost" , classification_cost ) ) return final_cost
def vectorize_target ( self , size ) : if self . _train_set : self . _train_set = self . _vectorize_set ( self . _train_set , size ) if self . _valid_set : self . _valid_set = self . _vectorize_set ( self . _valid_set , size ) if self . _test_set : self . _test_set = self . _vectorize_set ( self . _test_set , size )
def report ( self ) : logging . info ( "%s train=%d valid=%d test=%d" % ( self . __class__ . __name__ , len ( list ( self . _train_set ) ) if self . _train_set else 0 , len ( list ( self . _valid_set ) ) if self . _valid_set else 0 , len ( list ( self . _test_set ) ) if self . _test_set else 0 ) )
def train ( self , train_set , valid_set = None , test_set = None , train_size = None ) : iteration = 0 while True : if not iteration % self . config . test_frequency and test_set : try : self . test ( iteration , test_set ) except KeyboardInterrupt : logging . info ( 'interrupted!' ) break if not iteration % self . validation_frequency and valid_set : try : if not self . evaluate ( iteration , valid_set ) : logging . info ( 'patience elapsed, bailing out' ) break except KeyboardInterrupt : logging . info ( 'interrupted!' ) break train_message = "" try : train_message = self . train_func ( train_set ) except KeyboardInterrupt : logging . info ( 'interrupted!' ) break if not iteration % self . config . monitor_frequency : logging . info ( 'monitor (iter=%i) %s' , iteration + 1 , train_message ) iteration += 1 if hasattr ( self . network , "iteration_callback" ) : self . network . iteration_callback ( ) yield train_message if valid_set : self . set_params ( self . best_params ) if test_set : self . test ( 0 , test_set )
def sample ( self , input , steps ) : inputs = [ [ onehot ( self . input_dim , x ) for x in input ] ] for _ in range ( steps ) : target = self . compute ( inputs ) [ 0 , - 1 ] . argmax ( ) input . append ( target ) inputs [ 0 ] . append ( onehot ( self . input_dim , target ) ) return input
def compute_alignments ( self , prev_state , precomputed_values , mask = None ) : WaSp = T . dot ( prev_state , self . Wa ) UaH = precomputed_values if UaH . ndim == 2 : preact = WaSp [ : , None , : ] + UaH [ None , : , : ] else : preact = WaSp [ : , None , : ] + UaH act = T . activate ( preact , 'tanh' ) align_scores = T . dot ( act , self . Va ) if mask : mask = ( 1 - mask ) * - 99.00 if align_scores . ndim == 3 : align_scores += mask [ None , : ] else : align_scores += mask align_weights = T . nnet . softmax ( align_scores ) return align_weights
def compute_context_vector ( self , prev_state , inputs , precomputed_values = None , mask = None ) : precomputed_values = precomputed_values if precomputed_values else self . precompute ( inputs ) align_weights = self . compute_alignments ( prev_state , precomputed_values , mask ) context_vector = T . sum ( align_weights [ : , : , None ] * inputs , axis = 1 ) return context_vector
def concatenate ( vars , axis = - 1 ) : from deepy . core . neural_var import NeuralVariable if isinstance ( vars [ 0 ] , NeuralVariable ) : concat_var = Concatenate ( axis = axis ) . compute ( * vars ) if axis == - 1 or axis == vars [ 0 ] . tensor . ndim - 1 : concat_var . output_dim = sum ( [ x . output_dim for x in vars ] , 0 ) else : concat_var = TT . concatenate ( vars , axis ) return concat_var
def _pad ( self , side , length ) : if self . _train_set : self . _train_set = pad_dataset ( self . _train_set , side , length ) if self . _valid_set : self . _valid_set = pad_dataset ( self . _valid_set , side , length ) if self . _test_set : self . _test_set = pad_dataset ( self . _test_set , side , length )
def rmsprop_core ( params , gradients , momentum = 0.9 , learning_rate = 0.01 ) : for param , grad in zip ( params , gradients ) : rms_ = theano . shared ( np . zeros_like ( param . get_value ( ) ) , name = param . name + '_rms' ) rms = momentum * rms_ + ( 1 - momentum ) * grad * grad yield rms_ , rms yield param , param - learning_rate * grad / T . sqrt ( rms + 1e-8 )
def report ( self ) : if not self . end_time : self . end ( ) print ( "Time: {} mins" . format ( ( self . end_time - self . start_time ) / 60 ) )
def run ( self , data_x ) : output_vars = self . compute ( * data_x ) return self . _extract_costs ( output_vars )
def invoke ( self ) : self . _counter += 1 if self . _counter % self . _freq == 0 : cnt = 0. sum_map = defaultdict ( float ) for x in self . _trainer . get_data ( self . _data_split ) : val_map = self . run ( x ) if not isinstance ( val_map , dict ) : raise Exception ( "Monitor.run must return a dict." ) for k , val in val_map . items ( ) : sum_map [ k ] += val cnt += 1 for k in sum_map : sum_map [ k ] /= cnt new_best = self . compare ( sum_map ) self . _trainer . report ( sum_map , self . _data_split , new_best = new_best ) if new_best : self . _trainer . save_checkpoint ( self . _save_path )
def _build_loop_vars ( self ) : from theano . tensor . var import TensorVariable from deepy . core . neural_var import NeuralVariable if not self . _loop_vars : self . _ordered_out_keys = self . _outputs . keys ( ) seq_keys = self . _sequences . keys ( ) filled_out_keys = [ k for k in self . _ordered_out_keys if self . _outputs [ k ] ] nonseq_keys = self . _non_sequences . keys ( ) dummy_tensors , self . _scan_local_vars = get_dummy_args ( sequences = [ self . _sequences [ k ] . tensor for k in seq_keys ] , outputs_info = [ self . _outputs [ k ] . tensor for k in self . _ordered_out_keys ] , non_sequences = [ self . _non_sequences [ k ] . tensor for k in nonseq_keys ] , * * self . _kwargs ) dummy_map = dict ( zip ( seq_keys + filled_out_keys + nonseq_keys , dummy_tensors ) ) arg_map = self . _sequences . copy ( ) arg_map . update ( self . _outputs ) arg_map . update ( self . _non_sequences ) self . _loop_vars = LoopVars ( ) for k , dummy_tensor in dummy_map . items ( ) : dummy_var = NeuralVariable ( dummy_tensor , dim = arg_map [ k ] . dim ( ) ) self . _loop_vars [ k ] = dummy_var
def _scan_step ( self , vars ) : from neural_var import NeuralVariable if not self . _loop_vars : raise Exception ( "The loop is not initialized. To initialize the loop, use `with loop as vars`" ) replace_map = { } for k , var in vars . items ( ) : if var is not None : replace_map [ self . _dummy_nodes [ k ] . tensor ] = var . tensor outputs = { } for k in self . _outputs : if k not in self . _loop_vars : raise Exception ( "{} can not be found in loop vars." . format ( k ) ) output_node = theano . clone ( self . _loop_vars [ k ] . tensor , replace_map ) outputs [ k ] = NeuralVariable ( output_node , self . _loop_vars [ k ] . dim ( ) ) return outputs
def momentum_core ( params , gradients , momentum = 0.9 , learning_rate = 0.01 ) : free_parameters = [ ] updates = [ ] for param , grad in zip ( params , gradients ) : delta = learning_rate * grad velocity = theano . shared ( np . zeros_like ( param . get_value ( ) ) , name = param . name + '_vel' ) updates . append ( ( velocity , momentum * velocity - delta ) ) updates . append ( ( param , param + velocity ) ) free_parameters . append ( velocity ) return updates , free_parameters
def iftrain ( self , then_branch , else_branch ) : return ifelse ( self . _training_flag , then_branch , else_branch , name = "iftrain" )
def skip ( self , n_batches , n_epochs = 0 ) : logging . info ( "skip %d epochs and %d batches" % ( n_epochs , n_batches ) ) self . _skip_batches = n_batches self . _skip_epochs = n_epochs
def train ( self , train_set , valid_set = None , test_set = None , train_size = None ) : self . _epoch = 0 while True : if self . _skip_epochs > 0 : logging . info ( "skipping one epoch ..." ) self . _skip_epochs -= 1 self . _epoch += 1 yield None continue if not self . _epoch % self . config . test_frequency and test_set : try : self . _run_test ( self . _epoch , test_set ) except KeyboardInterrupt : logging . info ( 'interrupted!' ) break if not self . _epoch % self . validation_frequency and valid_set : try : if not self . _run_valid ( self . _epoch , valid_set ) : logging . info ( 'patience elapsed, bailing out' ) break except KeyboardInterrupt : logging . info ( 'interrupted!' ) break try : costs = self . _run_train ( self . _epoch , train_set , train_size ) except KeyboardInterrupt : logging . info ( 'interrupted!' ) break if np . isnan ( costs [ 0 ] [ 1 ] ) : logging . info ( "NaN detected in costs, rollback to last parameters" ) self . set_params ( * self . checkpoint ) else : self . _epoch += 1 self . network . epoch_callback ( ) yield dict ( costs ) if valid_set and self . config . get ( "save_best_parameters" , True ) : self . set_params ( * self . best_params ) if test_set : self . _run_test ( - 1 , test_set )
def _run_train ( self , epoch , train_set , train_size = None ) : self . network . train_logger . record_epoch ( epoch + 1 ) costs = self . train_step ( train_set , train_size ) if not epoch % self . config . monitor_frequency : self . report ( dict ( costs ) , "train" , epoch ) self . last_run_costs = costs return costs
def _run_valid ( self , epoch , valid_set , dry_run = False , save_path = None ) : costs = self . valid_step ( valid_set ) _ , J = costs [ 0 ] new_best = False if self . best_cost - J > self . best_cost * self . min_improvement : self . best_params = self . copy_params ( ) new_best = True if not dry_run : self . best_cost = J self . best_epoch = epoch self . save_checkpoint ( save_path ) self . report ( dict ( costs ) , type = "valid" , epoch = 0 if dry_run else epoch , new_best = new_best ) self . last_run_costs = costs return epoch - self . best_epoch < self . patience
def report ( self , score_map , type = "valid" , epoch = - 1 , new_best = False ) : type_str = type if len ( type_str ) < 5 : type_str += " " * ( 5 - len ( type_str ) ) info = " " . join ( "%s=%.2f" % el for el in score_map . items ( ) ) current_epoch = epoch if epoch > 0 else self . current_epoch ( ) epoch_str = "epoch={}" . format ( current_epoch + 1 ) if epoch < 0 : epoch_str = "dryrun" sys . stdout . write ( "\r" ) sys . stdout . flush ( ) marker = " *" if new_best else "" message = "{} ({}) {}{}" . format ( type_str , epoch_str , info , marker ) self . network . train_logger . record ( message ) logging . info ( message )
def get_data ( self , data_split = "train" ) : if data_split == 'train' : return self . _current_train_set elif data_split == 'valid' : return self . _current_valid_set elif data_split == 'test' : return self . _current_test_set else : return None
def apply ( self , func , dim = None ) : output_dim = dim if dim else self . output_dim return NeuralVariable ( func ( self . tensor ) , output_dim )
def report ( self ) : if self . logger : self . logger . info ( "accessed parameters:" ) for key in self . used_parameters : self . logger . info ( " - %s %s" % ( key , "(undefined)" if key in self . undefined_parameters else "" ) )
def var ( self , tensor_type , last_dim = 0 , test_shape = None ) : from deepy . tensor import var return var ( tensor_type , last_dim = last_dim , test_shape = test_shape )
def shared ( self , value , name = None ) : if type ( value ) == int : final_value = np . array ( value , dtype = "int32" ) elif type ( value ) == float : final_value = np . array ( value , dtype = env . FLOATX ) else : final_value = value return theano . shared ( final_value , name = name )
def invoke ( self ) : self . _iter += 1 if self . _iter - max ( self . _trainer . best_iter , self . _annealed_iter ) >= self . _patience : if self . _annealed_times >= self . _anneal_times : logging . info ( "ending" ) self . _trainer . exit ( ) else : self . _trainer . set_params ( * self . _trainer . best_params ) self . _learning_rate . set_value ( self . _learning_rate . get_value ( ) * 0.5 ) self . _annealed_times += 1 self . _annealed_iter = self . _iter logging . info ( "annealed learning rate to %f" % self . _learning_rate . get_value ( ) )
def invoke ( self ) : self . _iter += 1 logging . info ( "{} epochs left to run" . format ( self . _patience - self . _iter ) ) if self . _iter >= self . _patience : self . _trainer . exit ( )
def stack_encoders ( self , * layers ) : self . stack ( * layers ) self . encoding_layes . extend ( layers )
def stack_decoders ( self , * layers ) : self . stack ( * layers ) self . decoding_layers . extend ( layers )
def encode ( self , x ) : if not self . encoding_network : self . encoding_network = NeuralNetwork ( self . input_dim , self . input_tensor ) self . encoding_network . input_variables = self . input_variables for layer in self . encoding_layes : self . encoding_network . stack_layer ( layer , no_setup = True ) return self . encoding_network . compute ( * x )
def decode ( self , x ) : if not self . rep_dim : raise Exception ( "rep_dim must be set to decode." ) if not self . decoding_network : self . decoding_network = NeuralNetwork ( self . rep_dim ) for layer in self . decoding_layers : self . decoding_network . stack_layer ( layer , no_setup = True ) return self . decoding_network . compute ( x )
def all_parameters ( self ) : params = [ ] params . extend ( self . parameters ) params . extend ( self . free_parameters ) return params
def setup_variables ( self ) : if self . input_tensor : if type ( self . input_tensor ) == int : x = dim_to_var ( self . input_tensor , name = "x" ) else : x = self . input_tensor else : x = T . matrix ( 'x' ) self . input_variables . append ( x ) self . _output = x self . _test_output = x
def compute ( self , * x ) : self . _compile ( ) outs = self . _compute ( * x ) if self . _output_keys : return MapDict ( dict ( zip ( self . _output_keys , outs ) ) ) else : return outs
def save_params ( self , path , new_thread = False ) : save_logger . info ( path ) param_variables = self . all_parameters params = [ p . get_value ( ) . copy ( ) for p in param_variables ] if new_thread : thread = Thread ( target = save_network_params , args = ( params , path ) ) thread . start ( ) else : save_network_params ( params , path ) self . train_logger . save ( path )
def load_params ( self , path , exclude_free_params = False ) : if not os . path . exists ( path ) : return logging . info ( "loading parameters from %s" % path ) if exclude_free_params : params_to_load = self . parameters else : params_to_load = self . all_parameters if path . endswith ( ".gz" ) : opener = gzip . open if path . lower ( ) . endswith ( '.gz' ) else open handle = opener ( path , 'rb' ) saved_params = pickle . load ( handle ) handle . close ( ) for target , source in zip ( params_to_load , saved_params ) : logging . info ( '%s: setting value %s' , target . name , source . shape ) target . set_value ( source ) elif path . endswith ( ".npz" ) : arrs = np . load ( path ) for target , idx in zip ( params_to_load , range ( len ( arrs . keys ( ) ) ) ) : source = arrs [ 'arr_%d' % idx ] logging . info ( '%s: setting value %s' , target . name , source . shape ) target . set_value ( source ) else : raise Exception ( "File format of %s is not supported, use '.gz' or '.npz' or '.uncompressed.gz'" % path ) self . train_logger . load ( path )
def report ( self ) : logging . info ( "network inputs: %s" , " " . join ( map ( str , self . input_variables ) ) ) logging . info ( "network targets: %s" , " " . join ( map ( str , self . target_variables ) ) ) logging . info ( "network parameters: %s" , " " . join ( map ( str , self . all_parameters ) ) ) logging . info ( "parameter count: %d" , self . parameter_count )
def register_updates ( self , * updates ) : for key , node in updates : if key not in self . _registered_updates : self . updates . append ( ( key , node ) ) self . _registered_updates . add ( key )
def register_training_updates ( self , * updates ) : for key , node in updates : if key not in self . _registered_training_updates : self . training_updates . append ( ( key , node ) ) self . _registered_training_updates . add ( key )
def register_monitors ( self , * monitors ) : for key , node in monitors : if key not in self . _registered_monitors : node *= 1.0 self . training_monitors . append ( ( key , node ) ) self . testing_monitors . append ( ( key , node ) ) self . _registered_monitors . add ( key )
def dump_one ( elt_to_pickle , file_obj ) : pickled_elt_str = dumps ( elt_to_pickle ) file_obj . write ( pickled_elt_str ) file_obj . write ( '\n\n' )
def load_params ( self , path , exclude_free_params = False ) : from deepy . core import graph from deepy . core . comp_graph import ComputationalGraph model = graph . compile ( blocks = [ self ] ) model . load_params ( path , exclude_free_params = exclude_free_params )
def onehot_tensor ( i_matrix , vocab_size ) : dim0 , dim1 = i_matrix . shape i_vector = i_matrix . reshape ( ( - 1 , ) ) hot_matrix = T . extra_ops . to_one_hot ( i_vector , vocab_size ) . reshape ( ( dim0 , dim1 , vocab_size ) ) return hot_matrix
def create_request_elements ( cls , request_type , credentials , url , method = 'GET' , params = None , headers = None , body = '' , secret = None , redirect_uri = '' , scope = '' , csrf = '' , user_state = '' ) : headers = headers or { } params = params or { } consumer_key = credentials . consumer_key or '' consumer_secret = credentials . consumer_secret or '' token = credentials . token or '' refresh_token = credentials . refresh_token or credentials . token or '' url , base_params = cls . _split_url ( url ) params . update ( dict ( base_params ) ) if request_type == cls . USER_AUTHORIZATION_REQUEST_TYPE : if consumer_key and redirect_uri and ( csrf or not cls . supports_csrf_protection ) : params [ 'client_id' ] = consumer_key params [ 'redirect_uri' ] = redirect_uri params [ 'scope' ] = scope if cls . supports_user_state : params [ 'state' ] = base64 . urlsafe_b64encode ( json . dumps ( { "csrf" : csrf , "user_state" : user_state } ) . encode ( 'utf-8' ) ) else : params [ 'state' ] = csrf params [ 'response_type' ] = 'code' headers . update ( cls . _authorization_header ( credentials ) ) else : raise OAuth2Error ( 'Credentials with valid consumer_key and arguments ' 'redirect_uri, scope and state are required to create ' 'OAuth 2.0 user authorization request elements!' ) elif request_type == cls . ACCESS_TOKEN_REQUEST_TYPE : if consumer_key and consumer_secret : params [ 'code' ] = token params [ 'client_id' ] = consumer_key params [ 'client_secret' ] = consumer_secret params [ 'redirect_uri' ] = redirect_uri params [ 'grant_type' ] = 'authorization_code' headers . update ( cls . _authorization_header ( credentials ) ) else : raise OAuth2Error ( 'Credentials with valid token, consumer_key, ' 'consumer_secret and argument redirect_uri are required ' 'to create OAuth 2.0 access token request elements!' ) elif request_type == cls . REFRESH_TOKEN_REQUEST_TYPE : if refresh_token and consumer_key and consumer_secret : params [ 'refresh_token' ] = refresh_token params [ 'client_id' ] = consumer_key params [ 'client_secret' ] = consumer_secret params [ 'grant_type' ] = 'refresh_token' else : raise OAuth2Error ( 'Credentials with valid refresh_token, consumer_key, ' 'consumer_secret are required to create OAuth 2.0 ' 'refresh token request elements!' ) elif request_type == cls . PROTECTED_RESOURCE_REQUEST_TYPE : if credentials . token_type == cls . BEARER : headers . update ( { 'Authorization' : 'Bearer {0}' . format ( credentials . token ) } ) elif token : params [ 'access_token' ] = token else : raise OAuth2Error ( 'Credentials with valid token are required to create ' 'OAuth 2.0 protected resources request elements!' ) request_elements = core . RequestElements ( url , method , params , headers , body ) return cls . _x_request_elements_filter ( request_type , request_elements , credentials )
def _x_credentials_parser ( credentials , data ) : credentials . expire_in = data . get ( 'expires' ) if data . get ( 'token_type' ) == 'bearer' : credentials . token_type = 'Bearer' return credentials
def login ( provider_name ) : response = make_response ( ) result = authomatic . login ( WerkzeugAdapter ( request , response ) , provider_name ) if result : if result . user : result . user . update ( ) return render_template ( 'login.html' , result = result ) return response
def save ( self ) : if self . data : cookie = self . create_cookie ( ) cookie_len = len ( cookie ) if cookie_len > 4093 : raise SessionError ( 'Cookie too long! The cookie size {0} ' 'is more than 4093 bytes.' . format ( cookie_len ) ) self . adapter . set_header ( 'Set-Cookie' , cookie ) self . _data = { }
def _get_data ( self ) : cookie = self . adapter . cookies . get ( self . name ) return self . _deserialize ( cookie ) if cookie else { }
def data ( self ) : if not self . _data : self . _data = self . _get_data ( ) if self . _data is None : self . _data = { } return self . _data
def _signature ( self , * parts ) : signature = hmac . new ( six . b ( self . secret ) , digestmod = hashlib . sha1 ) signature . update ( six . b ( '|' . join ( parts ) ) ) return signature . hexdigest ( )
def valid ( self ) : if self . expiration_time : return self . expiration_time > int ( time . time ( ) ) else : return True
def is_binary_string ( content ) : textchars = ( bytearray ( [ 7 , 8 , 9 , 10 , 12 , 13 , 27 ] ) + bytearray ( range ( 0x20 , 0x100 ) ) ) return bool ( content . translate ( None , textchars ) )
def content ( self ) : if not self . _content : content = self . httplib_response . read ( ) if self . is_binary_string ( content ) : self . _content = content else : self . _content = content . decode ( 'utf-8' ) return self . _content
def create_request_elements ( cls , request_type , credentials , url , params = None , headers = None , body = '' , method = 'GET' , verifier = '' , callback = '' ) : params = params or { } headers = headers or { } consumer_key = credentials . consumer_key or '' consumer_secret = credentials . consumer_secret or '' token = credentials . token or '' token_secret = credentials . token_secret or '' url , base_params = cls . _split_url ( url ) params . update ( dict ( base_params ) ) if request_type == cls . USER_AUTHORIZATION_REQUEST_TYPE : if token : params [ 'oauth_token' ] = token else : raise OAuth1Error ( 'Credentials with valid token are required to create ' 'User Authorization URL!' ) else : if request_type == cls . REQUEST_TOKEN_REQUEST_TYPE : if consumer_key and consumer_secret and callback : params [ 'oauth_consumer_key' ] = consumer_key params [ 'oauth_callback' ] = callback else : raise OAuth1Error ( 'Credentials with valid consumer_key, consumer_secret ' 'and callback are required to create Request Token ' 'URL!' ) elif request_type == cls . ACCESS_TOKEN_REQUEST_TYPE : if consumer_key and consumer_secret and token and verifier : params [ 'oauth_token' ] = token params [ 'oauth_consumer_key' ] = consumer_key params [ 'oauth_verifier' ] = verifier else : raise OAuth1Error ( 'Credentials with valid consumer_key, ' 'consumer_secret, token and argument verifier' ' are required to create Access Token URL!' ) elif request_type == cls . PROTECTED_RESOURCE_REQUEST_TYPE : if consumer_key and consumer_secret and token and token_secret : params [ 'oauth_token' ] = token params [ 'oauth_consumer_key' ] = consumer_key else : raise OAuth1Error ( 'Credentials with valid consumer_key, ' + 'consumer_secret, token and token_secret are required ' 'to create Protected Resources URL!' ) params [ 'oauth_signature_method' ] = cls . _signature_generator . method params [ 'oauth_timestamp' ] = str ( int ( time . time ( ) ) ) params [ 'oauth_nonce' ] = cls . csrf_generator ( str ( uuid . uuid4 ( ) ) ) params [ 'oauth_version' ] = '1.0' params [ 'oauth_signature' ] = cls . _signature_generator . create_signature ( method , url , params , consumer_secret , token_secret ) request_elements = core . RequestElements ( url , method , params , headers , body ) return cls . _x_request_elements_filter ( request_type , request_elements , credentials )
def _access_user_info ( self ) : response = super ( Bitbucket , self ) . _access_user_info ( ) response . data . setdefault ( "email" , None ) email_response = self . access ( self . user_email_url ) if email_response . data : for item in email_response . data : if item . get ( "primary" , False ) : response . data . update ( email = item . get ( "email" , None ) ) return response
def login ( self , * login_args , * * login_kwargs ) : def decorator ( f ) : @ wraps ( f ) def decorated ( * args , * * kwargs ) : self . response = make_response ( ) adapter = WerkzeugAdapter ( request , self . response ) login_kwargs . setdefault ( 'session' , session ) login_kwargs . setdefault ( 'session_saver' , self . session_saver ) self . result = super ( FlaskAuthomatic , self ) . login ( adapter , * login_args , * * login_kwargs ) return f ( * args , * * kwargs ) return decorated return decorator
def login ( self ) : if self . params . get ( self . identifier_param ) : self . _log ( logging . INFO , u'Starting OpenID authentication procedure.' ) url = users . create_login_url ( dest_url = self . url , federated_identity = self . identifier ) self . _log ( logging . INFO , u'Redirecting user to {0}.' . format ( url ) ) self . redirect ( url ) else : self . _log ( logging . INFO , u'Continuing OpenID authentication procedure after redirect.' ) user = users . get_current_user ( ) if user : self . _log ( logging . INFO , u'Authentication successful.' ) self . _log ( logging . INFO , u'Creating user.' ) self . user = core . User ( self , id = user . federated_identity ( ) , email = user . email ( ) , gae_user = user ) else : raise FailureError ( 'Unable to authenticate identifier "{0}"!' . format ( self . identifier ) )
def _session_set ( self , key , value ) : self . session [ self . _session_key ( key ) ] = value
def _split_url ( url ) : split = parse . urlsplit ( url ) base = parse . urlunsplit ( ( split . scheme , split . netloc , split . path , 0 , 0 ) ) params = parse . parse_qsl ( split . query , True ) return base , params
def get_app_kwarg_dict ( appInstance ) : app_config = getattr ( appInstance , 'config' , { } ) return dict ( ( k . lower ( ) . replace ( 'cors_' , '' ) , app_config . get ( k ) ) for k in CONFIG_OPTIONS if app_config . get ( k ) is not None )
def ensure_iterable ( inst ) : if isinstance ( inst , str ) : return [ inst ] elif not isinstance ( inst , collections . abc . Iterable ) : return [ inst ] else : return inst
def isclose ( a , b , * , rel_tol = 1e-09 , abs_tol = 0.0 ) : try : return math . isclose ( a , b , rel_tol = rel_tol , abs_tol = abs_tol ) except AttributeError : if ( rel_tol < 0.0 ) or ( abs_tol < 0.0 ) : raise ValueError ( "Tolerances must be non-negative, but are rel_tol: {} and abs_tol: {}" . format ( rel_tol , abs_tol ) ) if math . isnan ( a ) or math . isnan ( b ) : return False if ( a == b ) : return True if math . isinf ( a ) or math . isinf ( b ) : return False diff = abs ( a - b ) return ( diff <= rel_tol * abs ( b ) ) or ( diff <= rel_tol * abs ( a ) ) or ( diff <= abs_tol )
def _get_corresponding_offsets ( onset_fronts , onset_front_id , onsets , offsets ) : corresponding_offsets = [ ] for index in _get_front_idxs_from_id ( onset_fronts , onset_front_id ) : offset_fidx , offset_sidx = _lookup_offset_by_onset_idx ( index , onsets , offsets ) corresponding_offsets . append ( ( offset_fidx , offset_sidx ) ) return corresponding_offsets
def _remove_overlaps ( segmentation_mask , fronts ) : fidxs , sidxs = np . where ( ( segmentation_mask != fronts ) & ( segmentation_mask != 0 ) & ( fronts != 0 ) ) fronts [ fidxs , sidxs ] = 0
def _merge_adjacent_segments ( mask ) : mask_ids = [ id for id in np . unique ( mask ) if id != 0 ] for id in mask_ids : myfidxs , mysidxs = np . where ( mask == id ) for other in mask_ids : if id == other : continue else : other_fidxs , other_sidxs = np . where ( mask == other ) if _segments_are_adjacent ( ( myfidxs , mysidxs ) , ( other_fidxs , other_sidxs ) ) : mask [ other_fidxs , other_sidxs ] = id
def _asa_task ( q , masks , stft , sample_width , frame_rate , nsamples_for_each_fft ) : for mask in masks : mask = np . where ( mask > 0 , 1 , 0 ) masks = [ mask * stft for mask in masks ] nparrs = [ ] dtype_dict = { 1 : np . int8 , 2 : np . int16 , 4 : np . int32 } dtype = dtype_dict [ sample_width ] for m in masks : _times , nparr = signal . istft ( m , frame_rate , nperseg = nsamples_for_each_fft ) nparr = nparr . astype ( dtype ) nparrs . append ( nparr ) for m in nparrs : q . put ( m ) q . put ( "DONE" )
def list_to_tf_input ( data , response_index , num_outcomes ) : matrix = np . matrix ( [ row [ : response_index ] + row [ response_index + 1 : ] for row in data ] ) outcomes = np . asarray ( [ row [ response_index ] for row in data ] , dtype = np . uint8 ) outcomes_onehot = ( np . arange ( num_outcomes ) == outcomes [ : , None ] ) . astype ( np . float32 ) return matrix , outcomes_onehot
def expand_and_standardize_dataset ( response_index , response_header , data_set , col_vals , headers , standardizers , feats_to_ignore , columns_to_expand , outcome_trans_dict ) : modified_set = [ ] for row_index , row in enumerate ( data_set ) : new_row = [ ] for col_index , val in enumerate ( row ) : header = headers [ col_index ] if col_index == response_index : new_outcome = outcome_trans_dict [ val ] new_row . append ( new_outcome ) elif header in feats_to_ignore : pass elif header in columns_to_expand : for poss_val in col_vals [ header ] : if val == poss_val : new_cat_val = 1.0 else : new_cat_val = - 1.0 new_row . append ( new_cat_val ) else : new_cont_val = float ( ( val - standardizers [ header ] [ 'mean' ] ) / standardizers [ header ] [ 'std_dev' ] ) new_row . append ( new_cont_val ) modified_set . append ( new_row ) expanded_headers = [ ] for header in headers : if header in feats_to_ignore : pass elif ( header in columns_to_expand ) and ( header is not response_header ) : for poss_val in col_vals [ header ] : new_header = '{}_{}' . format ( header , poss_val ) expanded_headers . append ( new_header ) else : expanded_headers . append ( header ) return modified_set , expanded_headers
def list_to_tf_input ( data , response_index , num_outcomes ) : matrix = np . matrix ( [ row [ : response_index ] + row [ response_index + 1 : ] for row in data ] ) outcomes = np . asarray ( [ row [ response_index ] for row in data ] , dtype = np . uint8 ) return matrix , outcomes
def _update_index_url_from_configs ( self ) : if 'VIRTUAL_ENV' in os . environ : self . pip_config_locations . append ( os . path . join ( os . environ [ 'VIRTUAL_ENV' ] , 'pip.conf' ) ) self . pip_config_locations . append ( os . path . join ( os . environ [ 'VIRTUAL_ENV' ] , 'pip.ini' ) ) if site_config_files : self . pip_config_locations . extend ( site_config_files ) index_url = None custom_config = None if 'PIP_INDEX_URL' in os . environ and os . environ [ 'PIP_INDEX_URL' ] : index_url = os . environ [ 'PIP_INDEX_URL' ] custom_config = 'PIP_INDEX_URL environment variable' else : for pip_config_filename in self . pip_config_locations : if pip_config_filename . startswith ( '~' ) : pip_config_filename = os . path . expanduser ( pip_config_filename ) if os . path . isfile ( pip_config_filename ) : config = ConfigParser ( ) config . read ( [ pip_config_filename ] ) try : index_url = config . get ( 'global' , 'index-url' ) custom_config = pip_config_filename break except ( NoOptionError , NoSectionError ) : pass if index_url : self . PYPI_API_URL = self . _prepare_api_url ( index_url ) print ( Color ( 'Setting API url to {{autoyellow}}{}{{/autoyellow}} as found in {{autoyellow}}{}{{/autoyellow}}' '. Use --default-index-url to use pypi default index' . format ( self . PYPI_API_URL , custom_config ) ) )
def main ( ) : options = get_options ( ) Windows . enable ( auto_colors = True , reset_atexit = True ) try : check_for_virtualenv ( options ) filenames = RequirementsDetector ( options . get ( '<requirements_file>' ) ) . get_filenames ( ) if filenames : print ( Color ( '{{autoyellow}}Found valid requirements file(s):{{/autoyellow}} ' '{{autocyan}}\n{}{{/autocyan}}' . format ( '\n' . join ( filenames ) ) ) ) else : print ( Color ( '{autoyellow}No requirements files found in current directory. CD into your project ' 'or manually specify requirements files as arguments.{/autoyellow}' ) ) return packages = PackagesDetector ( filenames ) . get_packages ( ) packages_status_map = PackagesStatusDetector ( packages , options . get ( '--use-default-index' ) ) . detect_available_upgrades ( options ) selected_packages = PackageInteractiveSelector ( packages_status_map , options ) . get_packages ( ) upgraded_packages = PackagesUpgrader ( selected_packages , filenames , options ) . do_upgrade ( ) print ( Color ( '{{autogreen}}Successfully upgraded (and updated requirements) for the following packages: ' '{}{{/autogreen}}' . format ( ',' . join ( [ package [ 'name' ] for package in upgraded_packages ] ) ) ) ) if options [ '--dry-run' ] : print ( Color ( '{automagenta}Actually, no, because this was a simulation using --dry-run{/automagenta}' ) ) except KeyboardInterrupt : print ( Color ( '\n{autored}Upgrade interrupted.{/autored}' ) )
def autodetect_files ( self ) : if self . _is_valid_requirements_file ( 'requirements.txt' ) : self . filenames . append ( 'requirements.txt' ) if self . _is_valid_requirements_file ( 'requirements.pip' ) : self . filenames . append ( 'requirements.pip' ) if os . path . isdir ( 'requirements' ) : for filename in os . listdir ( 'requirements' ) : file_path = os . path . join ( 'requirements' , filename ) if self . _is_valid_requirements_file ( file_path ) : self . filenames . append ( file_path ) self . _check_inclusions_recursively ( )
def handle_error ( errcode ) : if type ( errcode ) is c_int : errcode = errcode . value if errcode == 0 : pass elif errcode == - 1 : raise TimeoutError ( "the operation failed due to a timeout." ) elif errcode == - 2 : raise LostError ( "the stream has been lost." ) elif errcode == - 3 : raise InvalidArgumentError ( "an argument was incorrectly specified." ) elif errcode == - 4 : raise InternalError ( "an internal error has occurred." ) elif errcode < 0 : raise RuntimeError ( "an unknown error has occurred." )
def child ( self , name ) : return XMLElement ( lib . lsl_child ( self . e , str . encode ( name ) ) )
def set_name ( self , name ) : return bool ( lib . lsl_set_name ( self . e , str . encode ( name ) ) )
def set_value ( self , value ) : return bool ( lib . lsl_set_value ( self . e , str . encode ( value ) ) )
def append_child ( self , name ) : return XMLElement ( lib . lsl_append_child ( self . e , str . encode ( name ) ) )
def prepend_child ( self , name ) : return XMLElement ( lib . lsl_prepend_child ( self . e , str . encode ( name ) ) )
def append_copy ( self , elem ) : return XMLElement ( lib . lsl_append_copy ( self . e , elem . e ) )
def prepend_copy ( self , elem ) : return XMLElement ( lib . lsl_prepend_copy ( self . e , elem . e ) )
def remove_child ( self , rhs ) : if type ( rhs ) is XMLElement : lib . lsl_remove_child ( self . e , rhs . e ) else : lib . lsl_remove_child_n ( self . e , rhs )
def do_AUTOCOMPLETE ( cmd , s ) : s = list ( preprocess_query ( s ) ) [ 0 ] keys = [ k . decode ( ) for k in DB . smembers ( edge_ngram_key ( s ) ) ] print ( white ( keys ) ) print ( magenta ( '({} elements)' . format ( len ( keys ) ) ) )
def compute_edge_ngrams ( token , min = None ) : if min is None : min = config . MIN_EDGE_NGRAMS token = token [ : config . MAX_EDGE_NGRAMS + 1 ] return [ token [ : i ] for i in range ( min , len ( token ) ) ]
def iter_pipe ( pipe , processors ) : if isinstance ( pipe , str ) : pipe = [ pipe ] for it in processors : pipe = it ( pipe ) yield from pipe
def make_fuzzy ( word , max = 1 ) : neighbors = [ ] for i in range ( 0 , len ( word ) - 1 ) : neighbor = list ( word ) neighbor [ i ] , neighbor [ i + 1 ] = neighbor [ i + 1 ] , neighbor [ i ] neighbors . append ( '' . join ( neighbor ) ) for letter in string . ascii_lowercase : for i in range ( 0 , len ( word ) ) : neighbor = list ( word ) if letter != neighbor [ i ] : neighbor [ i ] = letter neighbors . append ( '' . join ( neighbor ) ) for letter in string . ascii_lowercase : for i in range ( 0 , len ( word ) + 1 ) : neighbor = list ( word ) neighbor . insert ( i , letter ) neighbors . append ( '' . join ( neighbor ) ) if len ( word ) > 3 : for i in range ( 0 , len ( word ) ) : neighbor = list ( word ) del neighbor [ i ] neighbors . append ( '' . join ( neighbor ) ) return neighbors
def do_help ( self , command ) : if command : doc = getattr ( self , 'do_' + command ) . __doc__ print ( cyan ( doc . replace ( ' ' * 8 , '' ) ) ) else : print ( magenta ( 'Available commands:' ) ) print ( magenta ( 'Type "HELP <command>" to get more info.' ) ) names = self . get_names ( ) names . sort ( ) for name in names : if name [ : 3 ] != 'do_' : continue doc = getattr ( self , name ) . __doc__ doc = doc . split ( '\n' ) [ 0 ] print ( '{} {}' . format ( yellow ( name [ 3 : ] ) , cyan ( doc . replace ( ' ' * 8 , ' ' ) . replace ( '\n' , '' ) ) ) )
def do_DBINFO ( self , * args ) : info = DB . info ( ) keys = [ 'keyspace_misses' , 'keyspace_hits' , 'used_memory_human' , 'total_commands_processed' , 'total_connections_received' , 'connected_clients' ] for key in keys : print ( '{}: {}' . format ( white ( key ) , blue ( info [ key ] ) ) ) nb_of_redis_db = int ( DB . config_get ( 'databases' ) [ 'databases' ] ) for db_index in range ( nb_of_redis_db - 1 ) : db_name = 'db{}' . format ( db_index ) if db_name in info : label = white ( 'nb keys (db {})' . format ( db_index ) ) print ( '{}: {}' . format ( label , blue ( info [ db_name ] [ 'keys' ] ) ) )
def send ( r , stream = False ) : r . send ( stream = stream ) return r . response
def reinterptet_harray_to_bits ( typeFrom , sigOrVal , bitsT ) : size = int ( typeFrom . size ) widthOfElm = typeFrom . elmType . bit_length ( ) w = bitsT . bit_length ( ) if size * widthOfElm != w : raise TypeConversionErr ( "Size of types is different" , size * widthOfElm , w ) partT = Bits ( widthOfElm ) parts = [ p . _reinterpret_cast ( partT ) for p in sigOrVal ] return Concat ( * reversed ( parts ) ) . _reinterpret_cast ( bitsT )
def slice_to_SLICE ( sliceVals , width ) : if sliceVals . step is not None : raise NotImplementedError ( ) start = sliceVals . start stop = sliceVals . stop if sliceVals . start is None : start = INT . fromPy ( width ) else : start = toHVal ( sliceVals . start ) if sliceVals . stop is None : stop = INT . fromPy ( 0 ) else : stop = toHVal ( sliceVals . stop ) startIsVal = isinstance ( start , Value ) stopIsVal = isinstance ( stop , Value ) indexesAreValues = startIsVal and stopIsVal if indexesAreValues : updateTime = max ( start . updateTime , stop . updateTime ) else : updateTime = - 1 return Slice . getValueCls ( ) ( ( start , stop ) , SLICE , 1 , updateTime )
def find_files ( directory , pattern , recursive = True ) : if not os . path . isdir ( directory ) : if os . path . exists ( directory ) : raise IOError ( directory + ' is not directory' ) else : raise IOError ( directory + " does not exists" ) if recursive : for root , _ , files in os . walk ( directory ) : for basename in files : if fnmatch . fnmatch ( basename , pattern ) : filename = os . path . join ( root , basename ) yield filename else : root = directory for basename in os . listdir ( root ) : if fnmatch . fnmatch ( basename , pattern ) : filename = os . path . join ( root , basename ) if os . path . isfile ( filename ) : yield filename
def isPow2 ( num ) -> bool : if not isinstance ( num , int ) : num = int ( num ) return num != 0 and ( ( num & ( num - 1 ) ) == 0 )
def Case ( self , caseVal , * statements ) : assert self . parentStm is None caseVal = toHVal ( caseVal , self . switchOn . _dtype ) assert isinstance ( caseVal , Value ) , caseVal assert caseVal . _isFullVld ( ) , "Cmp with invalid value" assert caseVal not in self . _case_value_index , ( "Switch statement already has case for value " , caseVal ) self . rank += 1 case = [ ] self . _case_value_index [ caseVal ] = len ( self . cases ) self . cases . append ( ( caseVal , case ) ) cond = self . switchOn . _eq ( caseVal ) self . _inputs . append ( cond ) cond . endpoints . append ( self ) self . _register_stements ( statements , case ) return self
def Default ( self , * statements ) : assert self . parentStm is None self . rank += 1 self . default = [ ] self . _register_stements ( statements , self . default ) return self
def vcdRegisterInterfaces ( self , obj : Union [ Interface , Unit ] , parent : Optional [ VcdVarWritingScope ] ) : if hasattr ( obj , "_interfaces" ) and obj . _interfaces : name = obj . _name parent_ = self . vcdWriter if parent is None else parent subScope = parent_ . varScope ( name ) self . _obj2scope [ obj ] = subScope with subScope : for chIntf in obj . _interfaces : self . vcdRegisterInterfaces ( chIntf , subScope ) if isinstance ( obj , ( Unit , SimModel ) ) : for u in obj . _units : self . vcdRegisterInterfaces ( u , subScope ) return subScope else : t = obj . _dtype if isinstance ( t , self . supported_type_classes ) : tName , width , formatter = vcdTypeInfoForHType ( t ) try : parent . addVar ( obj , getSignalName ( obj ) , tName , width , formatter ) except VarAlreadyRegistered : pass
def beforeSim ( self , simulator , synthesisedUnit ) : vcd = self . vcdWriter vcd . date ( datetime . now ( ) ) vcd . timescale ( 1 ) self . vcdRegisterInterfaces ( synthesisedUnit , None ) self . vcdRegisterRemainingSignals ( synthesisedUnit ) vcd . enddefinitions ( )
def logChange ( self , nowTime , sig , nextVal ) : try : self . vcdWriter . logChange ( nowTime , sig , nextVal ) except KeyError : pass
def distinctBy ( iterable , fn ) : s = set ( ) for i in iterable : r = fn ( i ) if r not in s : s . add ( r ) yield i
def removeUnconnectedSignals ( netlist ) : toDelete = set ( ) toSearch = netlist . signals while toSearch : _toSearch = set ( ) for sig in toSearch : if not sig . endpoints : try : if sig . _interface is not None : continue except AttributeError : pass for e in sig . drivers : if isinstance ( e , Operator ) : inputs = e . operands if e . result is sig : e . result = None else : inputs = e . _inputs netlist . statements . discard ( e ) for op in inputs : if not isinstance ( op , Value ) : try : op . endpoints . remove ( e ) except KeyError : continue _toSearch . add ( op ) toDelete . add ( sig ) if toDelete : for sig in toDelete : if sig . ctx == netlist : netlist . signals . remove ( sig ) _toSearch . discard ( sig ) toDelete = set ( ) toSearch = _toSearch
def onWriteReq ( self , sim , addr , data ) : self . requests . append ( ( WRITE , addr , data ) )
def name_for_process_and_mark_outputs ( statements : List [ HdlStatement ] ) -> str : out_names = [ ] for stm in statements : for sig in stm . _outputs : if not sig . hasGenericName : out_names . append ( sig . name ) if out_names : return min ( out_names ) else : return ""
def cut_off_drivers_of ( dstSignal , statements ) : separated = [ ] stm_filter = [ ] for stm in statements : stm . _clean_signal_meta ( ) d = stm . _cut_off_drivers_of ( dstSignal ) if d is not None : separated . append ( d ) f = d is not stm stm_filter . append ( f ) return list ( compress ( statements , stm_filter ) ) , separated
def synthesize ( self , name , interfaces , targetPlatform ) : ent = Entity ( name ) ent . _name = name + "_inst" for _ , v in self . params . items ( ) : ent . generics . append ( v ) if isinstance ( interfaces , set ) : intfSet = interfaces else : intfSet = set ( interfaces ) for s in interfaces : pi = portItemfromSignal ( s , ent ) pi . registerInternSig ( s ) ent . ports . append ( pi ) s . hidden = False removeUnconnectedSignals ( self ) markVisibilityOfSignals ( self , name , self . signals , intfSet ) for proc in targetPlatform . beforeHdlArchGeneration : proc ( self ) arch = Architecture ( ent ) for p in statements_to_HWProcesses ( self . statements ) : arch . processes . append ( p ) for s in self . signals : if s not in intfSet and not s . hidden : arch . variables . append ( s ) for u in self . subUnits : arch . componentInstances . append ( u ) for su in distinctBy ( self . subUnits , lambda x : x . name ) : arch . components . append ( su ) self . synthesised = True return [ ent , arch ]
def getMaxStmIdForStm ( stm ) : maxId = 0 if isinstance ( stm , Assignment ) : return stm . _instId elif isinstance ( stm , WaitStm ) : return maxId else : for _stm in stm . _iter_stms ( ) : maxId = max ( maxId , getMaxStmIdForStm ( _stm ) ) return maxId
def monitor ( self , sim ) : if self . notReset ( sim ) and self . _enabled : self . wrRd ( sim . write , 1 ) yield sim . waitOnCombUpdate ( ) d = self . doRead ( sim ) self . data . append ( d ) else : self . wrRd ( sim . write , 0 )
def doWrite ( self , sim , data ) : sim . write ( data , self . intf . data )
def driver ( self , sim ) : r = sim . read if self . actualData is NOP and self . data : self . actualData = self . data . popleft ( ) do = self . actualData is not NOP if do : self . doWrite ( sim , self . actualData ) else : self . doWrite ( sim , None ) en = self . notReset ( sim ) and self . _enabled if not ( en and do ) : return yield sim . waitOnCombUpdate ( ) rd = self . isRd ( r ) if en : assert rd . vldMask , ( ( "%r: ready signal for interface %r is in invalid state," " this would cause desynchronization" ) % ( sim . now , self . intf ) ) if rd . val : if self . _debugOutput is not None : self . _debugOutput . write ( "%s, wrote, %d: %r\n" % ( self . intf . _getFullName ( ) , sim . now , self . actualData ) ) if self . data : self . actualData = self . data . popleft ( ) else : self . actualData = NOP
def _getPhysicalName ( self ) : if hasattr ( self , "_boundedEntityPort" ) : return self . _boundedEntityPort . name else : return self . _getFullName ( ) . replace ( '.' , self . _NAME_SEPARATOR )
def _bit_length ( self ) : try : interfaces = self . _interfaces except AttributeError : interfaces = None if interfaces is None : _intf = self . _clone ( ) _intf . _loadDeclarations ( ) interfaces = _intf . _interfaces if interfaces : w = 0 for i in interfaces : w += i . _bit_length ( ) return w else : return self . _dtype . bit_length ( )
def sensitivityByOp ( op ) : if op == AllOps . RISING_EDGE : return SENSITIVITY . RISING elif op == AllOps . FALLING_EDGE : return SENSITIVITY . FALLING else : raise TypeError ( )
def eval ( self , operator , simulator = None ) : def getVal ( v ) : while not isinstance ( v , Value ) : v = v . _val return v operands = list ( map ( getVal , operator . operands ) ) if isEventDependentOp ( operator . operator ) : operands . append ( simulator . now ) elif operator . operator == AllOps . IntToBits : operands . append ( operator . result . _dtype ) return self . _evalFn ( * operands )
def convertBits ( self , sigOrVal , toType ) : if isinstance ( sigOrVal , Value ) : return convertBits__val ( self , sigOrVal , toType ) elif isinstance ( toType , HBool ) : if self . bit_length ( ) == 1 : v = 0 if sigOrVal . _dtype . negated else 1 return sigOrVal . _eq ( self . getValueCls ( ) . fromPy ( v , self ) ) elif isinstance ( toType , Bits ) : if self . bit_length ( ) == toType . bit_length ( ) : return sigOrVal . _convSign ( toType . signed ) elif toType == INT : return Operator . withRes ( AllOps . BitsToInt , [ sigOrVal ] , toType ) return default_auto_cast_fn ( self , sigOrVal , toType )
def reinterpret_bits_to_hstruct ( sigOrVal , hStructT ) : container = hStructT . fromPy ( None ) offset = 0 for f in hStructT . fields : t = f . dtype width = t . bit_length ( ) if f . name is not None : s = sigOrVal [ ( width + offset ) : offset ] s = s . _reinterpret_cast ( t ) setattr ( container , f . name , s ) offset += width return container
def fullWordCnt ( self , start : int , end : int ) : assert end >= start , ( start , end ) gap = max ( 0 , ( end - start ) - ( start % self . wordWidth ) ) return gap // self . wordWidth
def _discover_sensitivity_seq ( self , signals : List [ RtlSignalBase ] , seen : set , ctx : SensitivityCtx ) -> None : casualSensitivity = set ( ) for s in signals : s . _walk_sensitivity ( casualSensitivity , seen , ctx ) if ctx . contains_ev_dependency : break if not ctx . contains_ev_dependency : ctx . extend ( casualSensitivity )
def _get_rtl_context ( self ) : for sig in chain ( self . _inputs , self . _outputs ) : if sig . ctx : return sig . ctx else : continue raise HwtSyntaxError ( "Statement does not have any signal in any context" , self )
def _is_mergable_statement_list ( cls , stmsA , stmsB ) : if stmsA is None and stmsB is None : return True elif stmsA is None or stmsB is None : return False a_it = iter ( stmsA ) b_it = iter ( stmsB ) a = _get_stm_with_branches ( a_it ) b = _get_stm_with_branches ( b_it ) while a is not None or b is not None : if a is None or b is None or not a . _is_mergable ( b ) : return False a = _get_stm_with_branches ( a_it ) b = _get_stm_with_branches ( b_it ) return True
def _try_reduce_list ( statements : List [ "HdlStatement" ] ) : io_change = False new_statements = [ ] for stm in statements : reduced , _io_change = stm . _try_reduce ( ) new_statements . extend ( reduced ) io_change |= _io_change new_statements , rank_decrease = HdlStatement . _merge_statements ( new_statements ) return new_statements , rank_decrease , io_change
def _set_parent_stm ( self , parentStm : "HdlStatement" ) : was_top = self . parentStm is None self . parentStm = parentStm if not self . _now_is_event_dependent and parentStm . _now_is_event_dependent : self . _on_parent_event_dependent ( ) topStatement = parentStm while topStatement . parentStm is not None : topStatement = topStatement . parentStm parent_out_add = topStatement . _outputs . append parent_in_add = topStatement . _inputs . append if was_top : for inp in self . _inputs : inp . endpoints . discard ( self ) inp . endpoints . append ( topStatement ) parent_in_add ( inp ) for outp in self . _outputs : outp . drivers . discard ( self ) outp . drivers . append ( topStatement ) parent_out_add ( outp ) ctx = self . _get_rtl_context ( ) ctx . statements . discard ( self ) parentStm . rank += self . rank
def _sig ( self , name , dtype = BIT , defVal = None ) : if isinstance ( dtype , HStruct ) : if defVal is not None : raise NotImplementedError ( ) container = dtype . fromPy ( None ) for f in dtype . fields : if f . name is not None : r = self . _sig ( "%s_%s" % ( name , f . name ) , f . dtype ) setattr ( container , f . name , r ) return container return self . _ctx . sig ( name , dtype = dtype , defVal = defVal )
def _cleanAsSubunit ( self ) : for pi in self . _entity . ports : pi . connectInternSig ( ) for i in chain ( self . _interfaces , self . _private_interfaces ) : i . _clean ( )
def walkFlattenFields ( sigOrVal , skipPadding = True ) : t = sigOrVal . _dtype if isinstance ( t , Bits ) : yield sigOrVal elif isinstance ( t , HUnion ) : yield from walkFlattenFields ( sigOrVal . _val , skipPadding = skipPadding ) elif isinstance ( t , HStruct ) : for f in t . fields : isPadding = f . name is None if not isPadding or not skipPadding : if isPadding : v = f . dtype . fromPy ( None ) else : v = getattr ( sigOrVal , f . name ) yield from walkFlattenFields ( v ) elif isinstance ( t , HArray ) : for item in sigOrVal : yield from walkFlattenFields ( item ) else : raise NotImplementedError ( t )
def sensitivity ( proc : HWProcess , * sensitiveTo ) : for s in sensitiveTo : if isinstance ( s , tuple ) : sen , s = s if sen == SENSITIVITY . ANY : s . simSensProcs . add ( proc ) elif sen == SENSITIVITY . RISING : s . simRisingSensProcs . add ( proc ) elif sen == SENSITIVITY . FALLING : s . simFallingSensProcs . add ( proc ) else : raise AssertionError ( sen ) else : s . simSensProcs . add ( proc )
def simEvalCond ( simulator , * conds ) : _cond = True _vld = True for v in conds : val = bool ( v . val ) fullVld = v . vldMask == 1 if fullVld : if not val : return False , True else : return False , False _cond = _cond and val _vld = _vld and fullVld return _cond , _vld
def connectSimPort ( simUnit , subSimUnit , srcName , dstName , direction ) : if direction == DIRECTION . OUT : origPort = getattr ( subSimUnit , srcName ) newPort = getattr ( simUnit , dstName ) setattr ( subSimUnit , srcName , newPort ) else : origPort = getattr ( subSimUnit , dstName ) newPort = getattr ( simUnit , srcName ) setattr ( subSimUnit , dstName , newPort ) subSimUnit . _ctx . signals . remove ( origPort )
def vec ( val , width , signed = None ) : return Bits ( width , signed , forceVector = True ) . fromPy ( val )
def monitor ( self , sim ) : r = sim . read if self . notReset ( sim ) : if self . _lastRd is not 1 : self . wrRd ( sim . write , 1 ) self . _lastRd = 1 try : onMonitorReady = self . onMonitorReady except AttributeError : onMonitorReady = None if onMonitorReady is not None : onMonitorReady ( sim ) yield sim . waitOnCombUpdate ( ) vld = self . isVld ( r ) assert vld . vldMask , ( sim . now , self . intf , "vld signal is in invalid state" ) if vld . val : d = self . doRead ( sim ) if self . _debugOutput is not None : self . _debugOutput . write ( "%s, read, %d: %r\n" % ( self . intf . _getFullName ( ) , sim . now , d ) ) self . data . append ( d ) if self . _afterRead is not None : self . _afterRead ( sim ) else : if self . _lastRd is not 0 : self . wrRd ( sim . write , 0 ) self . _lastRd = 0
def HWProcess ( cls , proc : HWProcess , ctx : ResourceContext ) -> None : seen = ctx . seen for stm in proc . statements : encl = stm . _enclosed_for full_ev_dep = stm . _is_completly_event_dependent now_ev_dep = stm . _now_is_event_dependent ev_dep = full_ev_dep or now_ev_dep out_mux_dim = count_mux_inputs_for_outputs ( stm ) for o in stm . _outputs : if o in seen : continue i = out_mux_dim [ o ] if isinstance ( o . _dtype , HArray ) : assert i == 1 , ( o , i , " only one ram port per HWProcess" ) for a in walk_assignments ( stm , o ) : assert len ( a . indexes ) == 1 , "one address per RAM port" addr = a . indexes [ 0 ] ctx . registerRAM_write_port ( o , addr , ev_dep ) elif ev_dep : ctx . registerFF ( o ) if i > 1 : ctx . registerMUX ( stm , o , i ) elif o not in encl : ctx . registerLatch ( o ) if i > 1 : ctx . registerMUX ( stm , o , i ) elif i > 1 : ctx . registerMUX ( stm , o , i ) else : continue if isinstance ( stm , SwitchContainer ) : caseEqs = set ( [ stm . switchOn . _eq ( c [ 0 ] ) for c in stm . cases ] ) inputs = chain ( [ sig for sig in stm . _inputs if sig not in caseEqs ] , [ stm . switchOn ] ) else : inputs = stm . _inputs for i in inputs : if not i . hidden or i in seen : continue cls . HWProcess_operators ( i , ctx , ev_dep )
def evalParam ( p ) : while isinstance ( p , Param ) : p = p . get ( ) if isinstance ( p , RtlSignalBase ) : return p . staticEval ( ) return toHVal ( p )
def set ( self , val ) : assert not self . __isReadOnly , ( "This parameter(%s) was locked" " and now it can not be changed" % self . name ) assert self . replacedWith is None , ( "This param was replaced with new one and this " "should not exists" ) val = toHVal ( val ) self . defVal = val self . _val = val . staticEval ( ) self . _dtype = self . _val . _dtype
def finalize ( self ) : ff_to_remove = 0 res = self . resources for m , addrDict in self . memories . items ( ) : rwSyncPorts , rSyncPorts , wSyncPorts = 0 , 0 , 0 rwAsyncPorts , rAsyncPorts , wAsyncPorts = 0 , 0 , 0 rSync_wAsyncPorts , rAsync_wSyncPorts = 0 , 0 for _ , ( rSync , wSync , rAsync , wAsync ) in addrDict . items ( ) : if rSync : ff_to_remove += rSync * m . _dtype . elmType . bit_length ( ) rwSync = min ( rSync , wSync ) rSync -= rwSync wSync -= rwSync rwAsync = min ( rAsync , wAsync ) rAsync -= rwAsync wAsync -= rwAsync rSync_wAsync = min ( rSync , wAsync ) rSync -= rSync_wAsync wAsync -= rSync_wAsync rAsync_wSync = min ( rAsync , wSync ) rAsync -= rAsync_wSync wSync -= rAsync_wSync rwSyncPorts += rwSync rSyncPorts += rSync wSyncPorts += wSync rwAsyncPorts += rwAsync rAsyncPorts += rAsync wAsyncPorts += wAsync rSync_wAsyncPorts += rSync_wAsync rAsync_wSyncPorts += rAsync_wSync k = ResourceRAM ( m . _dtype . elmType . bit_length ( ) , int ( m . _dtype . size ) , rwSyncPorts , rSyncPorts , wSyncPorts , rSync_wAsyncPorts , rwAsyncPorts , rAsyncPorts , wAsyncPorts , rAsync_wSyncPorts ) res [ k ] = res . get ( k , 0 ) + 1 self . memories . clear ( ) if ff_to_remove : ff_cnt = res [ ResourceFF ] ff_cnt -= ff_to_remove if ff_cnt : res [ ResourceFF ] = ff_cnt else : del res [ ResourceFF ]
def _eq ( self , other ) : return self . naryOp ( AllOps . EQ , tv ( self ) . _eq , other )
def _getIndexCascade ( self ) : try : d = self . singleDriver ( ) try : op = d . operator except AttributeError : return if op == AllOps . INDEX : indexedOn = d . operands [ 0 ] if isinstance ( indexedOn , RtlSignalBase ) : return indexedOn , [ d . operands [ 1 ] ] else : raise Exception ( "can not drive static value %r" % indexedOn ) except ( MultipleDriversErr , NoDriverErr ) : pass
def walkParams ( intf , discovered ) : for si in intf . _interfaces : yield from walkParams ( si , discovered ) for p in intf . _params : if p not in discovered : discovered . add ( p ) yield p
def _registerIntfInImpl ( self , iName , intf ) : self . _registerInterface ( iName , intf , isPrivate = True ) self . _loadInterface ( intf , False ) intf . _signalsForInterface ( self . _ctx )
def getBaseNameScope ( cls ) : s = NameScope ( False ) s . setLevel ( 1 ) s [ 0 ] . update ( cls . _keywords_dict ) return s
def getBaseCond ( c ) : isNegated = False try : drivers = c . drivers except AttributeError : return ( c , isNegated ) if len ( drivers ) == 1 : d = list ( c . drivers ) [ 0 ] if isinstance ( d , Operator ) and d . operator == AllOps . NOT : c = d . operands [ 0 ] isNegated = True return ( c , isNegated )
def simBitsT ( width : int , signed : Union [ bool , None ] ) : k = ( width , signed ) try : return __simBitsTCache [ k ] except KeyError : t = SimBitsT ( width , signed ) __simBitsTCache [ k ] = t return t
def _cut_off_drivers_of ( self , sig : RtlSignalBase ) : if self . dst is sig : self . parentStm = None return self else : return None
def _loadFromHType ( self , dtype : HdlType , bitAddr : int ) -> None : self . bitAddr = bitAddr childrenAreChoice = False if isinstance ( dtype , Bits ) : ld = self . _loadFromBits elif isinstance ( dtype , HStruct ) : ld = self . _loadFromHStruct elif isinstance ( dtype , HArray ) : ld = self . _loadFromArray elif isinstance ( dtype , HStream ) : ld = self . _loadFromHStream elif isinstance ( dtype , HUnion ) : ld = self . _loadFromUnion childrenAreChoice = True else : raise TypeError ( "expected instance of HdlType" , dtype ) self . bitAddrEnd = ld ( dtype , bitAddr ) self . childrenAreChoice = childrenAreChoice
def signFix ( val , width ) : if val > 0 : msb = 1 << ( width - 1 ) if val & msb : val -= mask ( width ) + 1 return val
def _merge_with_other_stm ( self , other : "IfContainer" ) -> None : merge = self . _merge_statement_lists newCases = [ ] for ( c , caseA ) , ( _ , caseB ) in zip ( self . cases , other . cases ) : newCases . append ( ( c , merge ( caseA , caseB ) ) ) self . cases = newCases if self . default is not None : self . default = merge ( self . default , other . default ) self . _on_merge ( other )
def getIndent ( indentNum ) : try : return _indentCache [ indentNum ] except KeyError : i = "" . join ( [ _indent for _ in range ( indentNum ) ] ) _indentCache [ indentNum ] = i return i
def verilogTypeOfSig ( signalItem ) : driver_cnt = len ( signalItem . drivers ) if signalItem . _const or driver_cnt > 1 or arr_any ( signalItem . drivers , _isEventDependentDriver ) : return SIGNAL_TYPE . REG else : if driver_cnt == 1 : d = signalItem . drivers [ 0 ] if not isinstance ( d , ( Assignment , PortItem ) ) : return SIGNAL_TYPE . REG return SIGNAL_TYPE . WIRE
def nameAvailabilityCheck ( obj , propName , prop ) : if getattr ( obj , propName , None ) is not None : raise IntfLvlConfErr ( "%r already has property %s old:%s new:%s" % ( obj , propName , repr ( getattr ( obj , propName ) ) , prop ) )
def _registerParameter ( self , pName , parameter ) -> None : nameAvailabilityCheck ( self , pName , parameter ) try : hasName = parameter . _name is not None except AttributeError : hasName = False if not hasName : parameter . _name = pName parameter . _registerScope ( pName , self ) if parameter . hasGenericName : parameter . name = pName if parameter . _parent is None : parameter . _parent = self self . _params . append ( parameter )
def _registerUnit ( self , uName , unit ) : nameAvailabilityCheck ( self , uName , unit ) assert unit . _parent is None unit . _parent = self unit . _name = uName self . _units . append ( unit )
def _registerInterface ( self , iName , intf , isPrivate = False ) : nameAvailabilityCheck ( self , iName , intf ) assert intf . _parent is None intf . _parent = self intf . _name = iName intf . _ctx = self . _ctx if isPrivate : self . _private_interfaces . append ( intf ) intf . _isExtern = False else : self . _interfaces . append ( intf ) intf . _isExtern = True
def _registerArray ( self , name , items ) : items . _parent = self items . _name = name for i , item in enumerate ( items ) : setattr ( self , "%s_%d" % ( name , i ) , item )
def singleDriver ( self ) : drv_cnt = len ( self . drivers ) if not drv_cnt : raise NoDriverErr ( self ) elif drv_cnt != 1 : raise MultipleDriversErr ( self ) return self . drivers [ 0 ]
def staticEval ( self ) : for o in self . operands : o . staticEval ( ) self . result . _val = self . evalFn ( )
def withIndent ( self , indent = 1 ) : ctx = copy ( self ) ctx . indent += indent return ctx
def propagateClk ( obj ) : clk = obj . clk for u in obj . _units : _tryConnect ( clk , u , 'clk' )
def propagateClkRst ( obj ) : clk = obj . clk rst = obj . rst for u in obj . _units : _tryConnect ( clk , u , 'clk' ) _tryConnect ( ~ rst , u , 'rst_n' ) _tryConnect ( rst , u , 'rst' )
def _getFullName ( self ) : name = "" tmp = self while isinstance ( tmp , ( InterfaceBase , HObjList ) ) : if hasattr ( tmp , "_name" ) : n = tmp . _name else : n = '' if name == '' : name = n else : name = n + '.' + name if hasattr ( tmp , "_parent" ) : tmp = tmp . _parent else : tmp = None return name
def onTWriteCallback__init ( self , sim ) : yield from self . onTWriteCallback ( sim ) self . intf . t . _sigInside . registerWriteCallback ( self . onTWriteCallback , self . getEnable ) self . intf . o . _sigInside . registerWriteCallback ( self . onTWriteCallback , self . getEnable )
def connectSig ( self , signal ) : if self . direction == DIRECTION . IN : if self . src is not None : raise HwtSyntaxError ( "Port %s is already associated with %r" % ( self . name , self . src ) ) self . src = signal signal . endpoints . append ( self ) elif self . direction == DIRECTION . OUT : if self . dst is not None : raise HwtSyntaxError ( "Port %s is already associated with %r" % ( self . name , self . dst ) ) self . dst = signal signal . drivers . append ( self ) else : raise NotImplementedError ( self ) signal . hidden = False signal . ctx . subUnits . add ( self . unit )
def connectInternSig ( self ) : d = self . direction if d == DIRECTION . OUT : self . src . endpoints . append ( self ) elif d == DIRECTION . IN or d == DIRECTION . INOUT : self . dst . drivers . append ( self ) else : raise NotImplementedError ( d )
def getInternSig ( self ) : d = self . direction if d == DIRECTION . IN : return self . dst elif d == DIRECTION . OUT : return self . src else : raise NotImplementedError ( d )
def isEvDependentOn ( sig , process ) -> bool : if sig is None : return False return process in sig . simFallingSensProcs or process in sig . simRisingSensProcs
def _add_process ( self , proc , priority ) -> None : self . _events . push ( self . now , priority , proc )
def _scheduleApplyValues ( self ) -> None : assert not self . _applyValPlaned , self . now self . _add_process ( self . _applyValues ( ) , PRIORITY_APPLY_COMB ) self . _applyValPlaned = True if self . _runSeqProcessesPlaned : return assert not self . _seqProcsToRun and not self . _runSeqProcessesPlaned , self . now self . _add_process ( self . _runSeqProcesses ( ) , PRIORITY_APPLY_SEQ ) self . _runSeqProcessesPlaned = True
def _runCombProcesses ( self ) -> None : for proc in self . _combProcsToRun : cont = self . _outputContainers [ proc ] proc ( self , cont ) for sigName , sig in cont . _all_signals : newVal = getattr ( cont , sigName ) if newVal is not None : res = self . _conflictResolveStrategy ( newVal ) updater , isEvDependent = res self . _valuesToApply . append ( ( sig , updater , isEvDependent , proc ) ) setattr ( cont , sigName , None ) self . _combProcsToRun = UniqList ( )
def _runSeqProcesses ( self ) -> Generator [ None , None , None ] : updates = [ ] for proc in self . _seqProcsToRun : try : outContainer = self . _outputContainers [ proc ] except KeyError : outContainer = None proc ( self , outContainer ) if outContainer is not None : updates . append ( outContainer ) self . _seqProcsToRun = UniqList ( ) self . _runSeqProcessesPlaned = False for cont in updates : for sigName , sig in cont . _all_signals : newVal = getattr ( cont , sigName ) if newVal is not None : v = self . _conflictResolveStrategy ( newVal ) updater , _ = v sig . simUpdateVal ( self , updater ) setattr ( cont , sigName , None ) return yield
def _applyValues ( self ) -> Generator [ None , None , None ] : va = self . _valuesToApply self . _applyValPlaned = False lav = self . config . logApplyingValues if va and lav : lav ( self , va ) self . _valuesToApply = [ ] addSp = self . _seqProcsToRun . append for s , vUpdater , isEventDependent , comesFrom in va : if isEventDependent : addSp ( comesFrom ) else : s . simUpdateVal ( self , vUpdater ) self . _runCombProcesses ( ) if self . _valuesToApply and not self . _applyValPlaned : self . _scheduleApplyValues ( ) return yield
def read ( self , sig ) -> Value : try : v = sig . _val except AttributeError : v = sig . _sigInside . _val return v . clone ( )
def write ( self , val , sig : SimSignal ) -> None : try : simSensProcs = sig . simSensProcs except AttributeError : sig = sig . _sigInside simSensProcs = sig . simSensProcs t = sig . _dtype if isinstance ( val , Value ) : v = val . clone ( ) v = v . _auto_cast ( t ) else : v = t . fromPy ( val ) sig . simUpdateVal ( self , lambda curentV : ( valueHasChanged ( curentV , v ) , v ) ) if not self . _applyValPlaned : if not ( simSensProcs or sig . simRisingSensProcs or sig . simFallingSensProcs ) : self . _scheduleApplyValues ( ) elif ( sig . _writeCallbacks or sig . _writeCallbacksToEn ) : self . _scheduleApplyValues ( )
def add_process ( self , proc ) -> None : self . _events . push ( self . now , PRIORITY_NORMAL , proc )
def simUnit ( self , synthesisedUnit : Unit , until : float , extraProcesses = [ ] ) : beforeSim = self . config . beforeSim if beforeSim is not None : beforeSim ( self , synthesisedUnit ) add_proc = self . add_process for p in extraProcesses : add_proc ( p ( self ) ) self . _initUnitSignals ( synthesisedUnit ) self . run ( until )
def systemCTypeOfSig ( signalItem ) : if signalItem . _const or arr_any ( signalItem . drivers , lambda d : isinstance ( d , HdlStatement ) and d . _now_is_event_dependent ) : return SIGNAL_TYPE . REG else : return SIGNAL_TYPE . WIRE
def ternaryOpsToIf ( statements ) : stms = [ ] for st in statements : if isinstance ( st , Assignment ) : try : if not isinstance ( st . src , RtlSignalBase ) : raise DoesNotContainsTernary ( ) d = st . src . singleDriver ( ) if not isinstance ( d , Operator ) or d . operator != AllOps . TERNARY : raise DoesNotContainsTernary ( ) else : ops = d . operands ifc = IfContainer ( ops [ 0 ] , [ Assignment ( ops [ 1 ] , st . dst ) ] , [ Assignment ( ops [ 2 ] , st . dst ) ] ) stms . append ( ifc ) continue except ( MultipleDriversErr , DoesNotContainsTernary ) : pass except NoDriverErr : assert ( hasattr ( st . src , "_interface" ) and st . src . _interface is not None ) or st . src . defVal . vldMask , st . src stms . append ( st ) return stms
def hash_distance ( left_hash , right_hash ) : if len ( left_hash ) != len ( right_hash ) : raise ValueError ( 'Hamming distance requires two strings of equal length' ) return sum ( map ( lambda x : 0 if x [ 0 ] == x [ 1 ] else 1 , zip ( left_hash , right_hash ) ) )
def average_hash ( image_path , hash_size = 8 ) : with open ( image_path , 'rb' ) as f : image = Image . open ( f ) . resize ( ( hash_size , hash_size ) , Image . ANTIALIAS ) . convert ( 'L' ) pixels = list ( image . getdata ( ) ) avg = sum ( pixels ) / len ( pixels ) bits = "" . join ( map ( lambda pixel : '1' if pixel > avg else '0' , pixels ) ) hashformat = "0{hashlength}x" . format ( hashlength = hash_size ** 2 // 4 ) return int ( bits , 2 ) . __format__ ( hashformat )
def distance ( image_path , other_image_path ) : image_hash = average_hash ( image_path ) other_image_hash = average_hash ( other_image_path ) return hash_distance ( image_hash , other_image_hash )
def setup_platform ( hass , config , add_entities , discovery_info = None ) : host = config . get ( CONF_HOST ) token = config . get ( CONF_ACCESS_TOKEN ) name = config . get ( CONF_NAME ) volume_step = config . get ( CONF_VOLUME_STEP ) device_type = config . get ( CONF_DEVICE_CLASS ) device = VizioDevice ( host , token , name , volume_step , device_type ) if device . validate_setup ( ) is False : _LOGGER . error ( "Failed to set up Vizio platform, " "please check if host and API key are correct" ) return elif ( token is None or token == "" ) and device_type == "tv" : _LOGGER . error ( "Failed to set up Vizio platform, " "if device_class is 'tv' then an auth_token needs " "to be provided, otherwise if device_class is " "'soundbar' then add the right device_class to config" ) return if config . get ( CONF_SUPPRESS_WARNING ) : from requests . packages import urllib3 _LOGGER . warning ( "InsecureRequestWarning is disabled " "because of Vizio platform configuration" ) urllib3 . disable_warnings ( urllib3 . exceptions . InsecureRequestWarning ) add_entities ( [ device ] , True )
def update ( self ) : is_on = self . _device . get_power_state ( ) if is_on : self . _state = STATE_ON volume = self . _device . get_current_volume ( ) if volume is not None : self . _volume_level = float ( volume ) / self . _max_volume input_ = self . _device . get_current_input ( ) if input_ is not None : self . _current_input = input_ . meta_name inputs = self . _device . get_inputs ( ) if inputs is not None : self . _available_inputs = [ input_ . name for input_ in inputs ] else : if is_on is None : self . _state = None else : self . _state = STATE_OFF self . _volume_level = None self . _current_input = None self . _available_inputs = None
def mute_volume ( self , mute ) : if mute : self . _device . mute_on ( ) else : self . _device . mute_off ( )
def volume_up ( self ) : self . _volume_level += self . _volume_step / self . _max_volume self . _device . vol_up ( num = self . _volume_step )
def volume_down ( self ) : self . _volume_level -= self . _volume_step / self . _max_volume self . _device . vol_down ( num = self . _volume_step )
def set_volume_level ( self , volume ) : if self . _volume_level is not None : if volume > self . _volume_level : num = int ( self . _max_volume * ( volume - self . _volume_level ) ) self . _volume_level = volume self . _device . vol_up ( num = num ) elif volume < self . _volume_level : num = int ( self . _max_volume * ( self . _volume_level - volume ) ) self . _volume_level = volume self . _device . vol_down ( num = num )
def reset ( self ) : self . piece_bb = [ BB_VOID , BB_RANK_C | BB_RANK_G , BB_A1 | BB_I1 | BB_A9 | BB_I9 , BB_A2 | BB_A8 | BB_I2 | BB_I8 , BB_A3 | BB_A7 | BB_I3 | BB_I7 , BB_A4 | BB_A6 | BB_I4 | BB_I6 , BB_B2 | BB_H8 , BB_B8 | BB_H2 , BB_A5 | BB_I5 , BB_VOID , BB_VOID , BB_VOID , BB_VOID , BB_VOID , BB_VOID , ] self . pieces_in_hand = [ collections . Counter ( ) , collections . Counter ( ) ] self . occupied = Occupied ( BB_RANK_G | BB_H2 | BB_H8 | BB_RANK_I , BB_RANK_A | BB_B2 | BB_B8 | BB_RANK_C ) self . king_squares = [ I5 , A5 ] self . pieces = [ NONE for i in SQUARES ] for i in SQUARES : mask = BB_SQUARES [ i ] for piece_type in PIECE_TYPES : if mask & self . piece_bb [ piece_type ] : self . pieces [ i ] = piece_type self . turn = BLACK self . move_number = 1 self . captured_piece_stack = collections . deque ( ) self . move_stack = collections . deque ( ) self . incremental_zobrist_hash = self . board_zobrist_hash ( DEFAULT_RANDOM_ARRAY ) self . transpositions = collections . Counter ( ( self . zobrist_hash ( ) , ) )
def piece_at ( self , square ) : mask = BB_SQUARES [ square ] color = int ( bool ( self . occupied [ WHITE ] & mask ) ) piece_type = self . piece_type_at ( square ) if piece_type : return Piece ( piece_type , color )
def remove_piece_at ( self , square , into_hand = False ) : piece_type = self . piece_type_at ( square ) if piece_type == NONE : return if into_hand : self . add_piece_into_hand ( piece_type , self . turn ) mask = BB_SQUARES [ square ] self . piece_bb [ piece_type ] ^= mask color = int ( bool ( self . occupied [ WHITE ] & mask ) ) self . pieces [ square ] = NONE self . occupied . ixor ( mask , color , square ) if color == BLACK : piece_index = ( piece_type - 1 ) * 2 else : piece_index = ( piece_type - 1 ) * 2 + 1 self . incremental_zobrist_hash ^= DEFAULT_RANDOM_ARRAY [ 81 * piece_index + 9 * rank_index ( square ) + file_index ( square ) ]
def set_piece_at ( self , square , piece , from_hand = False , into_hand = False ) : if from_hand : self . remove_piece_from_hand ( piece . piece_type , self . turn ) self . remove_piece_at ( square , into_hand ) self . pieces [ square ] = piece . piece_type mask = BB_SQUARES [ square ] piece_type = piece . piece_type self . piece_bb [ piece_type ] |= mask if piece_type == KING : self . king_squares [ piece . color ] = square self . occupied . ixor ( mask , piece . color , square ) if piece . color == BLACK : piece_index = ( piece . piece_type - 1 ) * 2 else : piece_index = ( piece . piece_type - 1 ) * 2 + 1 self . incremental_zobrist_hash ^= DEFAULT_RANDOM_ARRAY [ 81 * piece_index + 9 * rank_index ( square ) + file_index ( square ) ]
def is_checkmate ( self ) : if not self . is_check ( ) : return False try : next ( self . generate_legal_moves ( ) . __iter__ ( ) ) return False except StopIteration : return True
def pop ( self ) : move = self . move_stack . pop ( ) self . transpositions . subtract ( ( self . zobrist_hash ( ) , ) ) self . move_number -= 1 captured_piece_type = self . captured_piece_stack . pop ( ) captured_piece_color = self . turn if not move : self . turn ^= 1 return move piece_type = self . piece_type_at ( move . to_square ) if move . promotion : piece_type = PIECE_PROMOTED . index ( piece_type ) if move . from_square is None : self . add_piece_into_hand ( piece_type , self . turn ^ 1 ) else : self . set_piece_at ( move . from_square , Piece ( piece_type , self . turn ^ 1 ) ) if captured_piece_type : self . remove_piece_from_hand ( captured_piece_type , captured_piece_color ^ 1 ) self . set_piece_at ( move . to_square , Piece ( captured_piece_type , captured_piece_color ) ) else : self . remove_piece_at ( move . to_square ) self . turn ^= 1 return move
def sfen ( self ) : sfen = [ ] empty = 0 for square in SQUARES : piece = self . piece_at ( square ) if not piece : empty += 1 else : if empty : sfen . append ( str ( empty ) ) empty = 0 sfen . append ( piece . symbol ( ) ) if BB_SQUARES [ square ] & BB_FILE_1 : if empty : sfen . append ( str ( empty ) ) empty = 0 if square != I1 : sfen . append ( '/' ) sfen . append ( ' ' ) if self . turn == WHITE : sfen . append ( 'w' ) else : sfen . append ( 'b' ) sfen . append ( ' ' ) pih_len = 0 for color in COLORS : p = self . pieces_in_hand [ color ] pih_len += len ( p ) for piece_type in sorted ( p . keys ( ) , reverse = True ) : if p [ piece_type ] >= 1 : if p [ piece_type ] > 1 : sfen . append ( str ( p [ piece_type ] ) ) piece = Piece ( piece_type , color ) sfen . append ( piece . symbol ( ) ) if pih_len == 0 : sfen . append ( '-' ) sfen . append ( ' ' ) sfen . append ( str ( self . move_number ) ) return '' . join ( sfen )
def zobrist_hash ( self , array = None ) : zobrist_hash = self . board_zobrist_hash ( array ) if array is None : array = DEFAULT_RANDOM_ARRAY if self . turn == WHITE : zobrist_hash ^= array [ 2268 ] i = ( self . pieces_in_hand [ BLACK ] [ ROOK ] * 35625 + self . pieces_in_hand [ BLACK ] [ BISHOP ] * 11875 + self . pieces_in_hand [ BLACK ] [ GOLD ] * 2375 + self . pieces_in_hand [ BLACK ] [ SILVER ] * 475 + self . pieces_in_hand [ BLACK ] [ KNIGHT ] * 95 + self . pieces_in_hand [ BLACK ] [ LANCE ] * 19 + self . pieces_in_hand [ BLACK ] [ PAWN ] ) bit = bit_scan ( i ) while bit != - 1 and bit is not None : zobrist_hash ^= array [ 2269 + bit ] bit = bit_scan ( i , bit + 1 ) return zobrist_hash
def symbol ( self ) : if self . color == BLACK : return PIECE_SYMBOLS [ self . piece_type ] . upper ( ) else : return PIECE_SYMBOLS [ self . piece_type ]
def load_config_from_cli ( config : GoodConf , argv : List [ str ] ) -> List [ str ] : from django . core . management . base import BaseCommand original_parser = BaseCommand . create_parser def patched_parser ( self , prog_name , subcommand ) : parser = original_parser ( self , prog_name , subcommand ) argparser_add_argument ( parser , config ) return parser BaseCommand . create_parser = patched_parser try : parser = argparse . ArgumentParser ( add_help = False ) argparser_add_argument ( parser , config ) config_arg , default_args = parser . parse_known_args ( argv ) config . load ( config_arg . config ) yield default_args finally : BaseCommand . create_parser = original_parser
def execute_from_command_line_with_config ( config : GoodConf , argv : List [ str ] ) : with load_config_from_cli ( config , argv ) as args : from django . core . management import execute_from_command_line execute_from_command_line ( args )
def argparser_add_argument ( parser : argparse . ArgumentParser , config : GoodConf ) : help = "Config file." if config . file_env_var : help += ( " Can also be configured via the " "environment variable: {}" . format ( config . file_env_var ) ) if config . default_files : help += ( " Defaults to the first file that exists from " "[{}]." . format ( ', ' . join ( config . default_files ) ) ) parser . add_argument ( '-C' , '--config' , metavar = 'FILE' , help = help )
def load ( self , filename : str = None ) : if filename : self . config_file = _find_file ( filename ) else : if self . file_env_var and self . file_env_var in os . environ : self . config_file = _find_file ( os . environ [ self . file_env_var ] ) if not self . config_file : for filename in self . default_files : self . config_file = _find_file ( filename , require = False ) if self . config_file : break if self . config_file : config = _load_config ( self . config_file ) log . info ( "Loading config from %s" , self . config_file ) else : config = { } log . info ( "No config file specified. " "Loading with environment variables." ) self . set_values ( config )
def generate_yaml ( cls , * * override ) : import ruamel . yaml yaml = ruamel . yaml . YAML ( ) yaml_str = StringIO ( ) yaml . dump ( cls . get_initial ( * * override ) , stream = yaml_str ) yaml_str . seek ( 0 ) dict_from_yaml = yaml . load ( yaml_str ) if cls . __doc__ : dict_from_yaml . yaml_set_start_comment ( '\n' + cls . __doc__ + '\n\n' ) for k in dict_from_yaml . keys ( ) : if cls . _values [ k ] . help : dict_from_yaml . yaml_set_comment_before_after_key ( k , before = '\n' + cls . _values [ k ] . help ) yaml_str = StringIO ( ) yaml . dump ( dict_from_yaml , yaml_str ) yaml_str . seek ( 0 ) return yaml_str . read ( )
def generate_markdown ( cls ) : lines = [ ] if cls . __doc__ : lines . extend ( [ . format ( cls . __doc__ ) , '' ] ) for k , v in cls . _values . items ( ) : lines . append ( '* **{}**  ' . format ( k ) ) if v . required : lines [ - 1 ] = lines [ - 1 ] + '_REQUIRED_  ' if v . help : lines . append ( '  {}  ' . format ( v . help ) ) lines . append ( '  type: `{}`  ' . format ( v . cast_as . __name__ ) ) if v . default is not None : lines . append ( '  default: `{}`  ' . format ( v . default ) ) return '\n' . join ( lines )
def cast ( self , val : str ) : try : return getattr ( self , 'cast_as_{}' . format ( self . cast_as . __name__ . lower ( ) ) ) ( val ) except AttributeError : return self . cast_as ( val )
def list_dates_between ( first_date , last_date ) : return [ first_date + timedelta ( days = n ) for n in range ( 1 + ( last_date - first_date ) . days ) ]
def parse_date ( s ) : try : return datetime . date ( int ( s [ : 4 ] ) , int ( s [ 5 : 7 ] ) , int ( s [ 8 : 10 ] ) ) except ValueError : return datetime . datetime . strptime ( s , '%d %B %Y' ) . date ( )
def load_file ( self , currency_file ) : if currency_file . startswith ( ( 'http://' , 'https://' ) ) : content = urlopen ( currency_file ) . read ( ) else : with open ( currency_file , 'rb' ) as f : content = f . read ( ) if currency_file . endswith ( '.zip' ) : self . load_lines ( get_lines_from_zip ( content ) ) else : self . load_lines ( content . decode ( 'utf-8' ) . splitlines ( ) )
def _set_missing_to_none ( self , currency ) : rates = self . _rates [ currency ] first_date , last_date = self . bounds [ currency ] for date in list_dates_between ( first_date , last_date ) : if date not in rates : rates [ date ] = None if self . verbose : missing = len ( [ r for r in itervalues ( rates ) if r is None ] ) if missing : print ( '{0}: {1} missing rates from {2} to {3} ({4} days)' . format ( currency , missing , first_date , last_date , 1 + ( last_date - first_date ) . days ) )
def read_record ( self , n ) : self . file . seek ( n * K - K ) return self . file . read ( K )
def write_record ( self , n , data ) : self . file . seek ( n * K - K ) return self . file . write ( data )
def comments ( self ) : record_numbers = range ( 2 , self . fward ) if not record_numbers : return '' data = b'' . join ( self . read_record ( n ) [ 0 : 1000 ] for n in record_numbers ) try : return data [ : data . find ( b'\4' ) ] . decode ( 'ascii' ) . replace ( '\0' , '\n' ) except IndexError : raise ValueError ( 'DAF file comment area is missing its EOT byte' ) except UnicodeDecodeError : raise ValueError ( 'DAF file comment area is not ASCII text' )
def close ( self ) : self . daf . file . close ( ) for segment in self . segments : if hasattr ( segment , '_data' ) : del segment . _data self . daf . _array = None self . daf . _map = None
def describe ( self , verbose = True ) : center = titlecase ( target_names . get ( self . center , 'Unknown center' ) ) target = titlecase ( target_names . get ( self . target , 'Unknown target' ) ) text = ( '{0.start_jd:.2f}..{0.end_jd:.2f}  {1} ({0.center})' ' -> {2} ({0.target})' . format ( self , center , target ) ) if verbose : text += ( '\n  frame={0.frame} data_type={0.data_type} source={1}' . format ( self , self . source . decode ( 'ascii' ) ) ) return text
def compute ( self , tdb , tdb2 = 0.0 ) : for position in self . generate ( tdb , tdb2 ) : return position
def close ( self ) : self . daf . file . close ( ) for segment in self . segments : if hasattr ( segment , '_data' ) : del segment . _data
def describe ( self , verbose = True ) : body = titlecase ( target_names . get ( self . body , 'Unknown body' ) ) text = ( '{0.start_jd:.2f}..{0.end_jd:.2f} frame={0.frame}' '  {1} ({0.body})' . format ( self , body ) ) if verbose : text += ( '\n  data_type={0.data_type} source={1}' . format ( self , self . source . decode ( 'ascii' ) ) ) return text
def _load ( self ) : if self . data_type == 2 : component_count = 3 else : raise ValueError ( 'only binary PCK data type 2 is supported' ) init , intlen , rsize , n = self . daf . read_array ( self . end_i - 3 , self . end_i ) initial_epoch = jd ( init ) interval_length = intlen / S_PER_DAY coefficient_count = int ( rsize - 2 ) // component_count coefficients = self . daf . map_array ( self . start_i , self . end_i - 4 ) coefficients . shape = ( int ( n ) , int ( rsize ) ) coefficients = coefficients [ : , 2 : ] coefficients . shape = ( int ( n ) , component_count , coefficient_count ) coefficients = rollaxis ( coefficients , 1 ) return initial_epoch , interval_length , coefficients
def visit_BinOp ( self , node ) : if self . within_logging_statement ( ) and self . within_logging_argument ( ) : if isinstance ( node . op , Mod ) : self . violations . append ( ( node , PERCENT_FORMAT_VIOLATION ) ) if isinstance ( node . op , Add ) : self . violations . append ( ( node , STRING_CONCAT_VIOLATION ) ) super ( LoggingVisitor , self ) . generic_visit ( node )
def visit_Dict ( self , node ) : if self . should_check_whitelist ( node ) : for key in node . keys : if key . s in self . whitelist or key . s . startswith ( "debug_" ) : continue self . violations . append ( ( self . current_logging_call , WHITELIST_VIOLATION . format ( key . s ) ) ) if self . should_check_extra_exception ( node ) : for value in node . values : self . check_exception_arg ( value ) super ( LoggingVisitor , self ) . generic_visit ( node )
def visit_JoinedStr ( self , node ) : if version_info >= ( 3 , 6 ) : if self . within_logging_statement ( ) : if any ( isinstance ( i , FormattedValue ) for i in node . values ) : if self . within_logging_argument ( ) : self . violations . append ( ( node , FSTRING_VIOLATION ) ) super ( LoggingVisitor , self ) . generic_visit ( node )
def visit_keyword ( self , node ) : if self . should_check_whitelist ( node ) : if node . arg not in self . whitelist and not node . arg . startswith ( "debug_" ) : self . violations . append ( ( self . current_logging_call , WHITELIST_VIOLATION . format ( node . arg ) ) ) if self . should_check_extra_exception ( node ) : self . check_exception_arg ( node . value ) super ( LoggingVisitor , self ) . generic_visit ( node )
def visit_ExceptHandler ( self , node ) : name = self . get_except_handler_name ( node ) if not name : super ( LoggingVisitor , self ) . generic_visit ( node ) return self . current_except_names . append ( name ) super ( LoggingVisitor , self ) . generic_visit ( node ) self . current_except_names . pop ( )
def detect_logging_level ( self , node ) : try : if self . get_id_attr ( node . func . value ) == "warnings" : return None if node . func . attr in LOGGING_LEVELS : return node . func . attr except AttributeError : pass return None
def get_except_handler_name ( self , node ) : name = node . name if not name : return None if version_info < ( 3 , ) : return name . id return name
def is_bare_exception ( self , node ) : return isinstance ( node , Name ) and node . id in self . current_except_names
def check_exc_info ( self , node ) : if self . current_logging_level not in ( 'error' , 'exception' ) : return for kw in node . keywords : if kw . arg == 'exc_info' : if self . current_logging_level == 'error' : violation = ERROR_EXC_INFO_VIOLATION else : violation = REDUNDANT_EXC_INFO_VIOLATION self . violations . append ( ( node , violation ) )
def db_file_widget ( cls ) : def get_link_display ( url ) : unquoted = unquote ( url . split ( '%2F' ) [ - 1 ] ) if sys . version_info . major == 2 : from django . utils . encoding import force_unicode unquoted = force_unicode ( unquoted ) return escape ( unquoted ) def get_template_substitution_values ( self , value ) : subst = super ( cls , self ) . get_template_substitution_values ( value ) subst [ 'initial' ] = get_link_display ( value . url ) return subst setattr ( cls , 'get_template_substitution_values' , get_template_substitution_values ) def get_context ( self , name , value , attrs ) : context = super ( cls , self ) . get_context ( name , value , attrs ) if value and hasattr ( value , 'url' ) : context [ 'widget' ] [ 'display' ] = get_link_display ( value . url ) return context setattr ( cls , 'get_context' , get_context ) return cls
def render_to_response ( self , context , * * response_kwargs ) : filename = response_kwargs . pop ( 'filename' , None ) cmd_options = response_kwargs . pop ( 'cmd_options' , None ) if issubclass ( self . response_class , PDFTemplateResponse ) : if filename is None : filename = self . get_filename ( ) if cmd_options is None : cmd_options = self . get_cmd_options ( ) return super ( PDFTemplateView , self ) . render_to_response ( context = context , filename = filename , show_content_in_browser = self . show_content_in_browser , header_template = self . header_template , footer_template = self . footer_template , cmd_options = cmd_options , cover_template = self . cover_template , * * response_kwargs ) else : return super ( PDFTemplateView , self ) . render_to_response ( context = context , * * response_kwargs )
def parse_file ( self , file_path , currency ) -> List [ PriceModel ] : contents = self . load_file ( file_path ) prices = [ ] for line in contents : price = self . parse_line ( line ) assert isinstance ( price , PriceModel ) price . currency = currency prices . append ( price ) return prices
def load_file ( self , file_path ) -> List [ str ] : content = [ ] content = read_lines_from_file ( file_path ) return content
def parse_line ( self , line : str ) -> PriceModel : line = line . rstrip ( ) parts = line . split ( ',' ) result = PriceModel ( ) result . symbol = self . translate_symbol ( parts [ 0 ] ) result . value = Decimal ( parts [ 1 ] ) date_str = parts [ 2 ] date_str = date_str . replace ( '"' , '' ) date_parts = date_str . split ( '/' ) year_str = date_parts [ 2 ] month_str = date_parts [ 1 ] day_str = date_parts [ 0 ] logging . debug ( f"parsing {date_parts} into date" ) result . datetime = datetime ( int ( year_str ) , int ( month_str ) , int ( day_str ) ) return result
def translate_symbol ( self , in_symbol : str ) -> str : if not self . symbol_maps : self . __load_symbol_maps ( ) result = self . symbol_maps [ in_symbol ] if in_symbol in self . symbol_maps else in_symbol return result
def __load_symbol_maps ( self ) : repo = SymbolMapRepository ( self . __get_session ( ) ) all_maps = repo . get_all ( ) self . symbol_maps = { } for item in all_maps : self . symbol_maps [ item . in_symbol ] = item . out_symbol
def __get_session ( self ) : if not self . session : self . session = dal . get_default_session ( ) return self . session
def import_csv ( filepath : str , currency : str ) : logger . debug ( f"currency = {currency}" ) currency = currency . upper ( ) app = PriceDbApplication ( ) app . logger = logger app . import_prices ( filepath , currency )
def last ( symbol : str ) : app = PriceDbApplication ( ) if symbol : symbol = symbol . upper ( ) sec_symbol = SecuritySymbol ( "" , "" ) sec_symbol . parse ( symbol ) latest = app . get_latest_price ( sec_symbol ) assert isinstance ( latest , PriceModel ) print ( f"{latest}" ) else : latest = app . get_latest_prices ( ) for price in latest : print ( f"{price}" )
def download ( ctx , help : bool , symbol : str , namespace : str , agent : str , currency : str ) : if help : click . echo ( ctx . get_help ( ) ) ctx . exit ( ) app = PriceDbApplication ( ) app . logger = logger if currency : currency = currency . strip ( ) currency = currency . upper ( ) app . download_prices ( currency = currency , agent = agent , symbol = symbol , namespace = namespace )
def prune ( symbol : str , all : str ) : app = PriceDbApplication ( ) app . logger = logger count = 0 if symbol is not None : sec_symbol = SecuritySymbol ( "" , "" ) sec_symbol . parse ( symbol ) deleted = app . prune ( sec_symbol ) if deleted : count = 1 else : count = app . prune_all ( ) print ( f"Removed {count} old price entries." )
def get_default_session ( ) : from . config import Config , ConfigKeys db_path = Config ( ) . get ( ConfigKeys . price_database ) if not db_path : raise ValueError ( "Price database not set in the configuration file!" ) return get_session ( db_path )
def add_map ( incoming , outgoing ) : db_path = Config ( ) . get ( ConfigKeys . pricedb_path ) session = get_session ( db_path ) new_map = SymbolMap ( ) new_map . in_symbol = incoming new_map . out_symbol = outgoing session . add ( new_map ) session . commit ( ) click . echo ( "Record saved." )
def list_maps ( ) : db_path = Config ( ) . get ( ConfigKeys . price_database ) session = get_session ( db_path ) maps = session . query ( SymbolMap ) . all ( ) for item in maps : click . echo ( item )
def get_by_id ( self , symbol : str ) -> SymbolMap : return self . query . filter ( SymbolMap . in_symbol == symbol ) . first ( )
def read_lines_from_file ( file_path : str ) -> List [ str ] : with open ( file_path ) as csv_file : content = csv_file . readlines ( ) return content
def map_entity ( self , entity : dal . Price ) -> PriceModel : if not entity : return None result = PriceModel ( ) result . currency = entity . currency dt_string = entity . date format_string = "%Y-%m-%d" if entity . time : dt_string += f"T{entity.time}" format_string += "T%H:%M:%S" price_datetime = datetime . strptime ( dt_string , format_string ) result . datum = Datum ( ) result . datum . from_datetime ( price_datetime ) assert isinstance ( result . datum , Datum ) #result.namespace = entity.namespace #result.symbol = entity.symbol result . symbol = SecuritySymbol ( entity . namespace , entity . symbol ) value = Decimal ( entity . value ) / Decimal ( entity . denom ) result . value = Decimal ( value ) return result
def map_model ( self , model : PriceModel ) -> Price : assert isinstance ( model . symbol , SecuritySymbol ) assert isinstance ( model . datum , Datum ) entity = Price ( ) date_iso = f"{model.datum.value.year}-{model.datum.value.month:02d}-{model.datum.value.day:02d}" entity . date = date_iso entity . time = f"{model.datum.value.hour:02d}:{model.datum.value.minute:02d}:{model.datum.value.second:02d}" if model . symbol . namespace : entity . namespace = model . symbol . namespace . upper ( ) entity . symbol = model . symbol . mnemonic . upper ( ) assert isinstance ( model . value , Decimal ) dec_places = abs ( model . value . as_tuple ( ) . exponent ) entity . denom = 10 ** dec_places entity . value = int ( model . value * entity . denom ) entity . currency = model . currency . upper ( ) return entity
def __read_config ( self , file_path : str ) : if not os . path . exists ( file_path ) : raise FileNotFoundError ( f"File path not found: {file_path}" ) if not os . path . isfile ( file_path ) : self . logger . error ( f"file not found: {file_path}" ) raise FileNotFoundError ( f"configuration file not found {file_path}" ) self . config . read ( file_path )
def __get_config_template_path ( self ) -> str : filename = resource_filename ( Requirement . parse ( package_name ) , template_path + config_filename ) return filename
def __create_user_config ( self ) : src_path = self . __get_config_template_path ( ) src = os . path . abspath ( src_path ) if not os . path . exists ( src ) : message = f"Config template not found {src}" self . logger . error ( message ) raise FileNotFoundError ( message ) dst = os . path . abspath ( self . get_config_path ( ) ) shutil . copyfile ( src , dst ) if not os . path . exists ( dst ) : raise FileNotFoundError ( "Config file could not be copied to user dir!" )
def get_contents ( self ) -> str : content = None in_memory = io . StringIO ( "" ) self . config . write ( in_memory ) in_memory . seek ( 0 ) content = in_memory . read ( ) in_memory . close ( ) return content
def set ( self , option : ConfigKeys , value ) : assert isinstance ( option , ConfigKeys ) section = SECTION self . config . set ( section , option . name , value ) self . save ( )
def get ( self , option : ConfigKeys ) : assert isinstance ( option , ConfigKeys ) section = SECTION return self . config . get ( section , option . name )
def save ( self ) : file_path = self . get_config_path ( ) contents = self . get_contents ( ) with open ( file_path , mode = 'w' ) as cfg_file : cfg_file . write ( contents )
def parse ( self , symbol : str ) -> ( str , str ) : symbol_parts = symbol . split ( ":" ) namespace = None mnemonic = symbol if len ( symbol_parts ) > 1 : namespace = symbol_parts [ 0 ] mnemonic = symbol_parts [ 1 ] self . namespace = namespace self . mnemonic = mnemonic return namespace , mnemonic
def add_price ( self , price : PriceModel ) : if not price : raise ValueError ( "Cannot add price. The received model is null!" ) mapper = mappers . PriceMapper ( ) entity = mapper . map_model ( price ) self . add_price_entity ( entity )
def download_price ( self , symbol : str , currency : str , agent : str ) -> PriceModel : price = self . __download_price ( symbol , currency , agent ) self . save ( ) return price
def session ( self ) : if not self . __session : self . __session = dal . get_default_session ( ) return self . __session
def get_prices ( self , date : str , currency : str ) -> List [ PriceModel ] : from . repositories import PriceRepository session = self . session repo = PriceRepository ( session ) query = repo . query if date : query = query . filter ( dal . Price . date == date ) if currency : query = query . filter ( dal . Price . currency == currency ) query = query . order_by ( dal . Price . namespace , dal . Price . symbol ) price_entities = query . all ( ) mapper = mappers . PriceMapper ( ) result = [ ] for entity in price_entities : model = mapper . map_entity ( entity ) result . append ( model ) return result
def get_prices_on ( self , on_date : str , namespace : str , symbol : str ) : repo = self . get_price_repository ( ) query = ( repo . query . filter ( dal . Price . namespace == namespace ) . filter ( dal . Price . symbol == symbol ) . filter ( dal . Price . date == on_date ) . order_by ( dal . Price . time . desc ( ) ) ) result = query . first ( ) return result
def __download_price ( self , symbol : str , currency : str , agent : str ) : from finance_quote_python import Quote assert isinstance ( symbol , str ) assert isinstance ( currency , str ) assert isinstance ( agent , str ) if not symbol : return None #self.logger.info(f"Downloading {symbol}... ") dl = Quote ( ) dl . logger = self . logger dl . set_source ( agent ) dl . set_currency ( currency ) result = dl . fetch ( agent , [ symbol ] ) if not result : raise ValueError ( f"Did not receive a response for {symbol}." ) price = result [ 0 ] if not price : raise ValueError ( f"Price not downloaded/parsed for {symbol}." ) else : self . add_price ( price ) return price
def __get_securities ( self , currency : str , agent : str , symbol : str , namespace : str ) -> List [ dal . Security ] : repo = self . get_security_repository ( ) query = repo . query if currency is not None : query = query . filter ( dal . Security . currency == currency ) if agent is not None : query = query . filter ( dal . Security . updater == agent ) if symbol is not None : query = query . filter ( dal . Security . symbol == symbol ) if namespace is not None : query = query . filter ( dal . Security . namespace == namespace ) query = query . order_by ( dal . Security . namespace , dal . Security . symbol ) securities = query . all ( ) return securities
def partial ( self ) : ba = self . data [ "bound_args" ] return state_partial ( self . data [ "func" ] , * ba . args [ 1 : ] , * * ba . kwargs )
def update_child_calls ( self ) : for node in filter ( lambda n : len ( n . arg_name ) , self . child_list ) : self . data [ "bound_args" ] . arguments [ node . arg_name ] = node . partial ( ) self . updated = True
def descend ( self , include_me = True ) : if include_me : yield self for child in self . child_list : yield child yield from child . descend ( )
def multi_dec ( f ) : @ wraps ( f ) def wrapper ( * args , * * kwargs ) : args = ( args [ 0 ] if len ( args ) == 1 and isinstance ( args [ 0 ] , ( list , tuple ) ) else args ) for arg in args : if isinstance ( arg , Node ) and arg . parent . name is "root" : arg . parent . remove_child ( arg ) arg . update_child_calls ( ) return f ( * args , * * kwargs ) return wrapper
def getResultFromProcess ( res , tempname , process ) : if not isinstance ( res , ( UndefinedValue , Exception ) ) : value = getRepresentation ( tempname , process ) return value , res else : return res , str ( res )
def defined_items ( self ) : return self . __class__ ( [ ( k , v ) for k , v in self . items ( ) if v is not self . EMPTY ] , is_empty = False )
def _getx ( self , Parser , ext_attr , tree ) : cache_key = Parser . __name__ + str ( hash ( tree ) ) if self . _parser_cache . get ( cache_key ) : p = self . _parser_cache [ cache_key ] else : p = Parser ( ) if ext_attr != "mappings" and Parser in [ FunctionParser , ObjectAccessParser , ] : p . mappings = self . context_mappings . copy ( ) p . visit ( tree ) self . _parser_cache [ cache_key ] = p return getattr ( p , ext_attr )
def check_part ( state , name , part_msg , missing_msg = None , expand_msg = None ) : if missing_msg is None : missing_msg = "Are you sure you defined the {{part}}? " if expand_msg is None : expand_msg = "Did you correctly specify the {{part}}? " if not part_msg : part_msg = name append_message = { "msg" : expand_msg , "kwargs" : { "part" : part_msg } } has_part ( state , name , missing_msg , append_message [ "kwargs" ] ) stu_part = state . student_parts [ name ] sol_part = state . solution_parts [ name ] assert_ast ( state , sol_part , append_message [ "kwargs" ] ) return part_to_child ( stu_part , sol_part , append_message , state )
def detect_openmp ( ) : compiler = new_compiler ( ) print ( "Checking for OpenMP support... " ) hasopenmp = hasfunction ( compiler , 'omp_get_num_threads()' ) needs_gomp = hasopenmp if not hasopenmp : compiler . add_library ( 'gomp' ) hasopenmp = hasfunction ( compiler , 'omp_get_num_threads()' ) needs_gomp = hasopenmp if hasopenmp : print ( "Compiler supports OpenMP" ) else : print ( "Did not detect OpenMP support." ) return hasopenmp , needs_gomp
def get_true_anomaly ( self ) : self . f = _rsky . _getf ( self . t_supersample , self . t0 , self . per , self . a , self . inc * pi / 180. , self . ecc , self . w * pi / 180. , self . transittype , self . nthreads ) return self . f
def detect ( ) : compiler = new_compiler ( ) hasopenmp = hasfunction ( compiler , 'omp_get_num_threads()' ) needs_gomp = hasopenmp if not hasopenmp : compiler . add_library ( 'gomp' ) hasopenmp = hasfunction ( compiler , 'omp_get_num_threads()' ) needs_gomp = hasopenmp return hasopenmp
def teardown ( self , exception ) : ctx = stack . top if ctx is not None : if hasattr ( ctx , 'ldap3_manager_connections' ) : for connection in ctx . ldap3_manager_connections : self . destroy_connection ( connection ) if hasattr ( ctx , 'ldap3_manager_main_connection' ) : log . debug ( "Unbinding a connection used within the request context." ) ctx . ldap3_manager_main_connection . unbind ( ) ctx . ldap3_manager_main_connection = None
def list_all ( self , * * kwargs ) : quiet = False if "quiet" in kwargs : quiet = kwargs [ 'quiet' ] bot . spinner . start ( ) url = '%s/collections/' % self . base results = self . _paginate_get ( url ) bot . spinner . stop ( ) if len ( results ) == 0 : bot . info ( "No container collections found." ) sys . exit ( 1 ) rows = [ ] for result in results : if "containers" in result : if result [ 'id' ] not in [ 37 , 38 , 39 ] : for c in result [ 'containers' ] : rows . append ( [ c [ 'detail' ] , "%s:%s" % ( c [ 'name' ] , c [ 'tag' ] ) ] ) if quiet is False : bot . info ( "Collections" ) bot . table ( rows ) return rows
def update_headers ( self , fields = None ) : do_reset = True if hasattr ( self , 'headers' ) : if self . headers is not None : do_reset = False if do_reset is True : self . _reset_headers ( ) if fields is not None : for key , value in fields . items ( ) : self . headers [ key ] = value header_names = "," . join ( list ( self . headers . keys ( ) ) ) bot . debug ( "Headers found: %s" % header_names )
def post ( url , data = None , return_json = True ) : bot . debug ( "POST %s" % url ) return call ( url , headers = headers , func = requests . post , data = data , return_json = return_json )
def get ( url , headers = None , token = None , data = None , return_json = True ) : bot . debug ( "GET %s" % url ) return call ( url , headers = headers , func = requests . get , data = data , return_json = return_json )
def _load_secrets ( self ) : self . auth = self . _get_and_update_setting ( 'GLOBUS_AUTH_RESPONSE' ) self . transfer = self . _get_and_update_setting ( 'GLOBUS_TRANSFER_RESPONSE' )
def list_logs ( self ) : results = [ ] for image in self . _bucket . list_blobs ( ) : if image . name . endswith ( 'log' ) : results . append ( image ) if len ( results ) == 0 : bot . info ( "No containers found, based on extension .log" ) return results
def init_transfer_client ( self ) : if self . _tokens_need_update ( ) : self . _update_tokens ( ) access_token = self . transfer [ 'access_token' ] authorizer = globus_sdk . RefreshTokenAuthorizer ( self . transfer [ 'refresh_token' ] , self . _client , access_token = self . transfer [ 'access_token' ] , expires_at = self . transfer [ 'expires_at_seconds' ] ) self . transfer_client = globus_sdk . TransferClient ( authorizer = authorizer )
def status ( backend ) : print ( '[backend status]' ) settings = read_client_secrets ( ) print ( 'There are %s clients found in secrets.' % len ( settings ) ) if 'SREGISTRY_CLIENT' in settings : print ( 'active: %s' % settings [ 'SREGISTRY_CLIENT' ] ) update_secrets ( settings ) else : print ( 'There is no active client.' )
def add ( backend , variable , value , force = False ) : print ( '[add]' ) settings = read_client_secrets ( ) prefix = 'SREGISTRY_%s_' % backend . upper ( ) if not variable . startswith ( prefix ) : variable = '%s%s' % ( prefix , variable ) variable = variable . upper ( ) bot . info ( "%s %s" % ( variable , value ) ) if backend in settings : if variable in settings [ backend ] and force is False : previous = settings [ backend ] [ variable ] bot . error ( '%s is already set as %s. Use --force to override.' % ( variable , previous ) ) sys . exit ( 1 ) if backend not in settings : settings [ backend ] = { } settings [ backend ] [ variable ] = value update_secrets ( settings )
def remove ( backend , variable ) : print ( '[remove]' ) settings = read_client_secrets ( ) prefixed = variable prefix = 'SREGISTRY_%s_' % backend . upper ( ) if not variable . startswith ( prefix ) : prefixed = '%s%s' % ( prefix , variable ) variable = variable . upper ( ) bot . info ( variable ) if backend in settings : if variable in settings [ backend ] : del settings [ backend ] [ variable ] if prefixed in settings [ backend ] : del settings [ backend ] [ prefixed ] update_secrets ( settings )
def activate ( backend ) : settings = read_client_secrets ( ) if backend is not None : settings [ 'SREGISTRY_CLIENT' ] = backend update_secrets ( settings ) print ( '[activate] %s' % backend )
def delete_backend ( backend ) : settings = read_client_secrets ( ) if backend in settings : del settings [ backend ] if 'SREGISTRY_CLIENT' in settings : if settings [ 'SREGISTRY_CLIENT' ] == backend : del settings [ 'SREGISTRY_CLIENT' ] update_secrets ( settings ) print ( '[delete] %s' % backend ) else : if backend is not None : print ( '%s is not a known client.' % backend ) else : print ( 'Please specify a backend to delete.' )
def delete ( self , url , headers = None , return_json = True , default_headers = True ) : bot . debug ( 'DELETE %s' % url ) return self . _call ( url , headers = headers , func = requests . delete , return_json = return_json , default_headers = default_headers )
def head ( self , url ) : bot . debug ( 'HEAD %s' % url ) return self . _call ( url , func = requests . head )
def post ( self , url , headers = None , data = None , return_json = True , default_headers = True ) : bot . debug ( "POST %s" % url ) return self . _call ( url , headers = headers , func = requests . post , data = data , return_json = return_json , default_headers = default_headers )
def get ( self , url , headers = None , token = None , data = None , return_json = True , default_headers = True , quiet = False ) : bot . debug ( "GET %s" % url ) return self . _call ( url , headers = headers , func = requests . get , data = data , return_json = return_json , default_headers = default_headers , quiet = quiet )
def paginate_get ( self , url , headers = None , return_json = True , start_page = None ) : geturl = '%s&page=1' % ( url ) if start_page is not None : geturl = '%s&page=%s' % ( url , start_page ) results = [ ] while geturl is not None : result = self . _get ( url , headers = headers , return_json = return_json ) if isinstance ( result , dict ) : if 'results' in result : results = results + result [ 'results' ] geturl = result [ 'next' ] else : return result return results
def remove ( self , image , force = False ) : q = parse_image_name ( remove_uri ( image ) ) if q [ 'registry' ] == None : q [ 'registry' ] = self . base q = self . _add_https ( q ) url = '%s/container/%s/%s:%s' % ( q [ 'registry' ] , q [ "collection" ] , q [ "image" ] , q [ "tag" ] ) SREGISTRY_EVENT = self . authorize ( request_type = "delete" , names = q ) headers = { 'Authorization' : SREGISTRY_EVENT } self . _update_headers ( fields = headers ) continue_delete = True if force is False : response = input ( "Are you sure you want to delete %s?" % q [ 'uri' ] ) while len ( response ) < 1 or response [ 0 ] . lower ( ) . strip ( ) not in "ynyesno" : response = input ( "Please answer yes or no: " ) if response [ 0 ] . lower ( ) . strip ( ) in "no" : continue_delete = False if continue_delete is True : response = self . _delete ( url ) message = self . _read_response ( response ) bot . info ( "Response %s, %s" % ( response . status_code , message ) ) else : bot . info ( "Delete cancelled." )
def get_installdir ( ) : return os . path . abspath ( os . path . dirname ( os . path . dirname ( __file__ ) ) )
def get_collections ( self ) : collections = [ ] for container in self . conn . get_account ( ) [ 1 ] : collections . append ( container [ 'name' ] ) return collections
def ipython ( args ) : from sregistry . main import get_client client = get_client ( args . endpoint ) client . announce ( args . command ) from IPython import embed embed ( )
def _update_base ( self ) : self . base = self . _get_and_update_setting ( 'SREGISTRY_GITLAB_BASE' , "https://gitlab.com/" ) self . api_base = "%s/api/v4" % self . base . strip ( '/' ) self . artifacts = self . _get_and_update_setting ( 'SREGISTRY_GITLAB_FOLDER' , 'build' ) self . job = self . _get_and_update_setting ( 'SREGISTRY_GITLAB_JOB' , 'build' ) bot . debug ( '      Api: %s' % self . api_base ) bot . debug ( 'Artifacts: %s' % self . artifacts ) bot . debug ( '      Job: %s' % self . job )
def _update_secrets ( self ) : self . token = self . _required_get_and_update ( 'SREGISTRY_GITLAB_TOKEN' ) self . headers [ "Private-Token" ] = self . token
def update_setting ( self , name , value ) : if value is not None : updates = { name : value } update_client_secrets ( backend = self . client_name , updates = updates )
def search_all ( self ) : results = set ( ) for container in self . conn . get_account ( ) [ 1 ] : for result in self . conn . get_container ( container [ 'name' ] ) [ 1 ] : results . add ( '%s/%s' % ( container [ 'name' ] , result [ 'name' ] ) ) if len ( results ) == 0 : bot . info ( "No container collections found." ) sys . exit ( 1 ) bot . info ( "Collections" ) bot . table ( [ [ x ] for x in list ( results ) ] ) return list ( results )
def search_all ( self ) : results = [ ] for entry in self . dbx . files_list_folder ( '' ) . entries : for item in self . dbx . files_list_folder ( entry . path_lower ) . entries : name = item . name . replace ( '.simg' , '' ) results . append ( [ "%s/%s" % ( entry . name , name ) ] ) if len ( results ) == 0 : bot . info ( "No container collections found." ) sys . exit ( 1 ) bot . info ( "Collections" ) bot . table ( results ) return results
def get_build_template ( ) : base = get_installdir ( ) name = "%s/main/templates/build/singularity-cloudbuild.json" % base if os . path . exists ( name ) : bot . debug ( "Found template %s" % name ) return read_json ( name ) bot . warning ( "Template %s not found." % name )
def _get_bucket ( self ) : try : self . _bucket = self . _bucket_service . get_bucket ( self . _bucket_name ) except google . cloud . exceptions . NotFound : self . _bucket = self . _bucket_service . create_bucket ( self . _bucket_name ) except : bot . error ( 'Cannot get or create %s' % self . _bucket_name ) sys . exit ( 1 ) return self . _bucket
def get_subparsers ( parser ) : actions = [ action for action in parser . _actions if isinstance ( action , argparse . _SubParsersAction ) ] subparsers = dict ( ) for action in actions : for choice , subparser in action . choices . items ( ) : subparsers [ choice ] = subparser return subparsers
def get_file_hash ( filename ) : hasher = hashlib . sha256 ( ) with open ( filename , "rb" ) as f : for chunk in iter ( lambda : f . read ( 4096 ) , b"" ) : hasher . update ( chunk ) return hasher . hexdigest ( )
def clean_up ( files ) : if not isinstance ( files , list ) : files = [ files ] for f in files : if os . path . exists ( f ) : bot . verbose3 ( "Cleaning up %s" % f ) os . remove ( f )
def push ( self , path , name , tag = None ) : path = os . path . abspath ( path ) image = os . path . basename ( path ) bot . debug ( "PUSH %s" % path ) if not os . path . exists ( path ) : bot . error ( '%s does not exist.' % path ) sys . exit ( 1 ) names = parse_image_name ( remove_uri ( name ) , tag = tag ) image_size = os . path . getsize ( path ) >> 20 metadata = { 'sizemb' : "%s" % image_size , 'client' : 'sregistry' } self . bucket . upload_file ( path , names [ 'storage_uri' ] , { "Metadata" : metadata } )
def get_collection ( self , name ) : from sregistry . database . models import Collection return Collection . query . filter ( Collection . name == name ) . first ( )
def get_container ( self , name , collection_id , tag = "latest" , version = None ) : from sregistry . database . models import Container if version is None : container = Container . query . filter_by ( collection_id = collection_id , name = name , tag = tag ) . first ( ) else : container = Container . query . filter_by ( collection_id = collection_id , name = name , tag = tag , version = version ) . first ( ) return container
def rmi ( self , image_name ) : container = self . rm ( image_name , delete = True ) if container is not None : bot . info ( "[rmi] %s" % container )
def run_build ( self , config , bucket , names ) : project = self . _get_project ( ) bot . custom ( 'PROJECT' , project , "CYAN" ) bot . custom ( 'BUILD  ' , config [ 'steps' ] [ 0 ] [ 'name' ] , "CYAN" ) response = self . _build_service . projects ( ) . builds ( ) . create ( body = config , projectId = project ) . execute ( ) build_id = response [ 'metadata' ] [ 'build' ] [ 'id' ] status = response [ 'metadata' ] [ 'build' ] [ 'status' ] bot . log ( "build %s: %s" % ( build_id , status ) ) start = time . time ( ) while status not in [ 'COMPLETE' , 'FAILURE' , 'SUCCESS' ] : time . sleep ( 15 ) response = self . _build_service . projects ( ) . builds ( ) . get ( id = build_id , projectId = project ) . execute ( ) build_id = response [ 'id' ] status = response [ 'status' ] bot . log ( "build %s: %s" % ( build_id , status ) ) end = time . time ( ) bot . log ( 'Total build time: %s seconds' % ( round ( end - start , 2 ) ) ) if status == 'SUCCESS' : env = 'SREGISTRY_GOOGLE_STORAGE_PRIVATE' blob = bucket . blob ( response [ 'artifacts' ] [ 'objects' ] [ 'paths' ] [ 0 ] ) if self . _get_and_update_setting ( env ) == None : blob . make_public ( ) response [ 'public_url' ] = blob . public_url update_blob_metadata ( blob , response , config , bucket , names ) response [ 'media_link' ] = blob . media_link response [ 'size' ] = blob . size response [ 'file_hash' ] = blob . md5_hash return response
def search_all ( self ) : url = '...' results = self . _paginate_get ( url ) if len ( results ) == 0 : bot . info ( "No container collections found." ) sys . exit ( 1 ) bot . info ( "Collections" ) rows = [ ] for result in results : if "containers" in result : for c in result [ 'containers' ] : rows . append ( [ c [ 'uri' ] , c [ 'detail' ] ] ) bot . table ( rows ) return rows
def get_manifest ( self , repo_name , tag ) : image = None repo = self . aws . describe_images ( repositoryName = repo_name ) if 'imageDetails' in repo : for contender in repo . get ( 'imageDetails' ) : if tag in contender [ 'imageTags' ] : image = contender break if image is None : bot . exit ( 'Cannot find %s:%s, is the uri correct?' % ( repo_name , digest ) ) digest = image [ 'imageDigest' ] digests = self . aws . batch_get_image ( repositoryName = repo_name , imageIds = [ { "imageDigest" : digest , "imageTag" : tag } ] ) self . manifest = json . loads ( digests [ 'images' ] [ 0 ] [ 'imageManifest' ] ) return self . manifest
def s3errors ( path ) : try : yield except ClientError as error : _error = error . response . get ( "Error" , { } ) error_code = _error . get ( "Code" , None ) response_meta = error . response . get ( "ResponseMetadata" , { } ) http_status = response_meta . get ( "HTTPStatusCode" , 200 ) error_msg = _error . get ( "Message" , None ) if error_code == "NoSuchBucket" : raise errors . ResourceError ( path , exc = error , msg = error_msg ) if http_status == 404 : raise errors . ResourceNotFound ( path ) elif http_status == 403 : raise errors . PermissionDenied ( path = path , msg = error_msg ) else : raise errors . OperationFailed ( path = path , exc = error ) except SSLError as error : raise errors . OperationFailed ( path , exc = error ) except EndpointConnectionError as error : raise errors . RemoteConnectionError ( path , exc = error , msg = "{}" . format ( error ) )
def factory ( cls , filename , mode , on_close ) : _temp_file = tempfile . TemporaryFile ( ) proxy = cls ( _temp_file , filename , mode , on_close = on_close ) return proxy
def gravatar_url ( user_or_email , size = GRAVATAR_DEFAULT_SIZE ) : if hasattr ( user_or_email , 'email' ) : email = user_or_email . email else : email = user_or_email try : return escape ( get_gravatar_url ( email = email , size = size ) ) except : return ''
def has_gravatar ( email ) : url = get_gravatar_url ( email , default = GRAVATAR_DEFAULT_IMAGE_404 ) try : request = Request ( url ) request . get_method = lambda : 'HEAD' return 200 == urlopen ( request ) . code except ( HTTPError , URLError ) : return False
def chimera_blocks ( M = 16 , N = 16 , L = 4 ) : for x in xrange ( M ) : for y in xrange ( N ) : for u in ( 0 , 1 ) : yield tuple ( ( x , y , u , k ) for k in xrange ( L ) )
def main ( ) : parser = MolvsParser ( epilog = 'use "molvs <command> -h" to show help for a specific command' ) subparsers = parser . add_subparsers ( title = 'Available commands' ) common_parser = MolvsParser ( add_help = False ) common_parser . add_argument ( 'infile' , nargs = '?' , help = 'input filename' , type = argparse . FileType ( 'r' ) , default = sys . stdin ) common_parser . add_argument ( '-i' , '--intype' , help = 'input filetype' , choices = FILETYPES ) common_parser . add_argument ( '-:' , '--smiles' , help = 'input SMILES instead of file' , metavar = '<smiles>' ) common_parser . add_argument ( '-O' , '--outfile' , help = 'output filename' , type = argparse . FileType ( 'w' ) , default = sys . stdout , metavar = '<outfile>' ) standardize_parser = subparsers . add_parser ( 'standardize' , help = 'standardize a molecule' , parents = [ common_parser ] ) standardize_parser . add_argument ( '-o' , '--outtype' , help = 'output filetype' , choices = FILETYPES ) standardize_parser . set_defaults ( func = standardize_main ) validate_parser = subparsers . add_parser ( 'validate' , help = 'validate a molecule' , parents = [ common_parser ] ) validate_parser . set_defaults ( func = validate_main ) args = parser . parse_args ( ) try : args . func ( args ) except Exception as e : sys . stderr . write ( 'Error: %s\n\n' . encode ( ) % e . message ) parser . print_help ( ) sys . exit ( 2 )
def integrate_ivp ( u0 = 1.0 , v0 = 0.0 , mu = 1.0 , tend = 10.0 , dt0 = 1e-8 , nt = 0 , nsteps = 600 , t0 = 0.0 , atol = 1e-8 , rtol = 1e-8 , plot = False , savefig = 'None' , method = 'bdf' , dpi = 100 , verbose = False ) : f , j = get_f_and_j ( mu ) if nt > 1 : tout = np . linspace ( t0 , tend , nt ) yout , nfo = integrate_predefined ( f , j , [ u0 , v0 ] , tout , dt0 , atol , rtol , nsteps = nsteps , check_indexing = False , method = method ) else : tout , yout , nfo = integrate_adaptive ( f , j , [ u0 , v0 ] , t0 , tend , dt0 , atol , rtol , nsteps = nsteps , check_indexing = False , method = method ) if verbose : print ( nfo ) if plot : import matplotlib . pyplot as plt plt . plot ( tout , yout [ : , 1 ] , 'g--' ) plt . plot ( tout , yout [ : , 0 ] , 'k-' , linewidth = 2 ) if savefig == 'None' : plt . show ( ) else : plt . savefig ( savefig , dpi = dpi )
def get_mems_of_org ( self ) : print 'Getting members.' counter = 0 for member in self . org_retrieved . iter_members ( ) : self . members_json [ member . id ] = member . to_json ( ) counter += 1 return counter
def get_teams_of_org ( self ) : print 'Getting teams.' counter = 0 for team in self . org_retrieved . iter_teams ( ) : self . teams_json [ team . id ] = team . to_json ( ) counter += 1 return counter
def repos ( self , repo_type = 'public' , organization = 'llnl' ) : print 'Getting repos.' for repo in self . org_retrieved . iter_repos ( type = repo_type ) : #JSON json = repo . to_json ( ) self . repos_json [ repo . name ] = json #CSV temp_repo = my_repo . My_Repo ( ) temp_repo . name = repo . full_name self . total_repos += 1 temp_repo . contributors = my_github . get_total_contributors ( repo ) self . total_contributors += temp_repo . contributors temp_repo . forks = repo . forks_count self . total_forks += temp_repo . forks temp_repo . stargazers = repo . stargazers self . total_stars += temp_repo . stargazers temp_repo . pull_requests_open , temp_repo . pull_requests_closed = my_github . get_pull_reqs ( repo ) temp_repo . pull_requests = ( temp_repo . pull_requests_open + temp_repo . pull_requests_closed ) self . total_pull_reqs += temp_repo . pull_requests_open self . total_pull_reqs += temp_repo . pull_requests_closed self . total_pull_reqs_open += temp_repo . pull_requests_open self . total_pull_reqs_closed += temp_repo . pull_requests_closed temp_repo . open_issues = repo . open_issues_count self . total_open_issues += temp_repo . open_issues temp_repo . closed_issues = my_github . get_issues ( repo , organization = organization ) temp_repo . issues = temp_repo . closed_issues + temp_repo . open_issues self . total_closed_issues += temp_repo . closed_issues self . total_issues += temp_repo . issues my_github . get_languages ( repo , temp_repo ) temp_repo . readme = my_github . get_readme ( repo ) #temp_repo.license = my_github.get_license(repo) temp_repo . commits = self . get_commits ( repo = repo , organization = organization ) self . total_commits += temp_repo . commits self . all_repos . append ( temp_repo )
def get_pull_reqs ( self , repo ) : pull_reqs_open = 0 pull_reqs_closed = 0 for pull_request in repo . iter_pulls ( state = 'all' ) : self . pull_requests_json [ repo . name ] . append ( pull_request . to_json ( ) ) if pull_request . closed_at is not None : pull_reqs_closed += 1 else : pull_reqs_open += 1 return pull_reqs_open , pull_reqs_closed
def get_issues ( self , repo , organization = 'llnl' ) : #JSON path = ( '../github-data/' + organization + '/' + repo . name + '/issues' ) is_only_today = False if not os . path . exists ( path ) : #no previous path, get all issues all_issues = repo . iter_issues ( state = 'all' ) is_only_today = True else : files = os . listdir ( path ) date = str ( files [ - 1 ] [ : - 5 ] ) if date == str ( datetime . date . today ( ) ) : #most recent date is actually today, get previous most recent date if len ( files ) > 2 : date = str ( files [ - 2 ] [ : - 5 ] ) else : #This means there is only one file, today. Retrieve every issue all_issues = repo . iter_issues ( state = 'all' ) is_only_today = True if not is_only_today : #there's a previous saved JSON that's not today all_issues = repo . iter_issues ( since = date , state = 'all' ) for issue in all_issues : self . issues_json [ repo . name ] . append ( issue . to_json ( ) ) #CSV closed_issues = 0 for issue in repo . iter_issues ( state = 'closed' ) : if issue is not None : closed_issues += 1 return closed_issues
def get_license ( self , repo ) : if self . search_limit >= 28 : print 'Hit search limit. Sleeping for 60 sec.' time . sleep ( 60 ) self . search_limit = 0 self . search_limit += 1 search_results = self . logged_in_gh . search_code ( 'license' + 'in:path repo:' + repo . full_name ) try : for result in search_results : path = result . path [ 1 : ] if '/' not in path and 'license' in path . lower ( ) : self . total_licenses += 1 return path return 'MISS' except ( StopIteration ) as e : return 'MISS'
def write_org_json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict_to_write = { } , path_ending_type = '' , is_list = False ) : path = ( '../github-data/' + organization + '-org/' + path_ending_type + '/' + str ( date ) + '.json' ) self . checkDir ( path ) with open ( path , 'w' ) as out_clear : #clear old data out_clear . close ( ) with open ( path , 'a' ) as out : if is_list : #used for list of items out . write ( '[' ) for item in dict_to_write : out . write ( json . dumps ( dict_to_write [ item ] , sort_keys = True , indent = 4 , separators = ( ',' , ': ' ) ) + ',' ) out . seek ( - 1 , os . SEEK_END ) #kill last comma out . truncate ( ) if is_list : out . write ( ']' ) out . close ( )
def write_repo_json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict_to_write = { } , path_ending_type = '' , is_list = False , is_dict = False ) : for repo in dict_to_write : path = ( '../github-data/' + organization + '/' + repo + '/' + path_ending_type + '/' + str ( date ) + '.json' ) self . checkDir ( path ) with open ( path , 'w' ) as out : if is_list : out . write ( '[' ) for value in dict_to_write [ repo ] : if is_dict : for inner_dict in value : out . write ( json . dumps ( inner_dict , sort_keys = True , indent = 4 , separators = ( ',' , ': ' ) ) + ',' ) else : out . write ( json . dumps ( value , sort_keys = True , indent = 4 , separators = ( ',' , ': ' ) ) + ',' ) out . seek ( - 1 , os . SEEK_END ) #kill last comma out . truncate ( ) out . write ( ']' ) else : out . write ( json . dumps ( dict_to_write [ repo ] , sort_keys = True , indent = 4 , separators = ( ',' , ': ' ) ) ) out . close ( )
def write_totals ( self , file_path = '' , date = str ( datetime . date . today ( ) ) , organization = 'N/A' , members = 0 , teams = 0 ) : total_exists = os . path . isfile ( file_path ) with open ( file_path , 'a' ) as out_total : if not total_exists : out_total . write ( 'date,organization,repos,members,teams,' + 'unique_contributors,total_contributors,forks,' + 'stargazers,pull_requests,open_issues,has_readme,' + 'has_license,pull_requests_open,pull_requests_closed,' + 'commits,id,closed_issues,issues\n' ) self . delete_last_line ( date = date , file_path = file_path ) out_total . close ( ) with open ( file_path , 'r' ) as file_read : row_count = sum ( 1 for row in file_read ) - 1 file_read . close ( ) with open ( file_path , 'a' ) as out_total : out_total . write ( date + ',' + organization + ',' + str ( self . total_repos ) + ',' + str ( members ) + ',' + str ( teams ) + ',' + str ( len ( self . unique_contributors ) ) + ',' + str ( self . total_contributors ) + ',' + str ( self . total_forks ) + ',' + str ( self . total_stars ) + ',' + str ( self . total_pull_reqs ) + ',' + str ( self . total_open_issues ) + ',' + str ( self . total_readmes ) + ',' + str ( self . total_licenses ) + ',' + str ( self . total_pull_reqs_open ) + ',' + str ( self . total_pull_reqs_closed ) + ',' + str ( self . total_commits ) + ',' + str ( row_count ) + ',' + str ( self . total_closed_issues ) + ',' + str ( self . total_issues ) + '\n' ) out_total . close ( )
def write_languages ( self , file_path = '' , date = str ( datetime . date . today ( ) ) ) : self . remove_date ( file_path = file_path , date = date ) languages_exists = os . path . isfile ( file_path ) with open ( file_path , 'a' ) as out_languages : if not languages_exists : out_languages . write ( 'date,language,count,size,size_log\n' ) languages_sorted = sorted ( self . languages_size ) #self.delete_last_line(date=date, file_path=file_path) for language in languages_sorted : try : out_languages . write ( date + ',' + language + ',' + str ( self . languages [ language ] ) + ',' + str ( self . languages_size [ language ] ) + ',' + str ( math . log10 ( int ( self . languages_size [ language ] ) ) ) + '\n' ) except ( TypeError , KeyError ) as e : out_languages . write ( date + ',' + language + ',' + str ( 0 ) + ',' + str ( self . languages_size [ language ] ) + ',' + str ( math . log10 ( int ( self . languages_size [ language ] ) ) ) + '\n' )
def connect ( url = , token = None ) : gh_session = None if url == : gh_session = create_session ( token ) else : gh_session = create_enterprise_session ( url , token ) if gh_session is None : msg = 'Unable to connect to (%s) with provided token.' raise RuntimeError ( msg , url ) logger . info ( 'Connected to: %s' , url ) return gh_session
def write_to_file ( self , file_path = '' , date = ( datetime . date . today ( ) ) , organization = 'llnl' ) : with open ( file_path , 'w+' ) as out : out . write ( 'date,organization,stargazers\n' ) sorted_stargazers = sorted ( self . stargazers ) #sort based on lowercase for star in sorted_stargazers : out . write ( star + ',' + str ( self . stargazers [ star ] ) + '\n' ) out . close ( )
def from_gitlab ( klass , repository , labor_hours = True ) : if not isinstance ( repository , gitlab . v4 . objects . Project ) : raise TypeError ( 'Repository must be a gitlab Repository object' ) project = klass ( ) logger . debug ( 'GitLab: repository_id=%d path_with_namespace=%s' , repository . id , repository . path_with_namespace , ) project [ 'name' ] = repository . name project [ 'repositoryURL' ] = repository . http_url_to_repo project [ 'description' ] = repository . description project [ 'permissions' ] [ 'licenses' ] = None web_url = repository . web_url public_server = web_url . startswith ( 'https://gitlab.com' ) if repository . visibility in ( 'public' ) and public_server : project [ 'permissions' ] [ 'usageType' ] = 'openSource' elif date_parse ( repository . created_at ) < POLICY_START_DATE : project [ 'permissions' ] [ 'usageType' ] = 'exemptByPolicyDate' if labor_hours : project [ 'laborHours' ] = labor_hours_from_url ( project [ 'repositoryURL' ] ) else : project [ 'laborHours' ] = 0 project [ 'tags' ] = [ 'gitlab' ] + repository . tag_list project [ 'contact' ] = { 'email' : '' , 'URL' : web_url , } project [ 'organization' ] = repository . namespace [ 'name' ] project [ 'status' ] = 'Development' project [ 'vcs' ] = 'git' project [ 'homepageURL' ] = repository . web_url api_url = repository . manager . gitlab . _url archive_suffix = '/projects/%s/repository/archive' % repository . get_id ( ) project [ 'downloadURL' ] = api_url + archive_suffix project [ 'date' ] = { 'created' : date_parse ( repository . created_at ) . date ( ) . isoformat ( ) , 'lastModified' : date_parse ( repository . last_activity_at ) . date ( ) . isoformat ( ) , 'metadataLastUpdated' : '' , } _prune_dict_null_str ( project ) return project
def from_stashy ( klass , repository , labor_hours = True ) : if not isinstance ( repository , dict ) : raise TypeError ( 'Repository must be a dict' ) project = klass ( ) logger . debug ( 'Stashy: project_key=%s repository_slug=%s' , repository [ 'name' ] , repository [ 'project' ] [ 'key' ] , ) project [ 'name' ] = repository [ 'name' ] clone_urls = [ clone [ 'href' ] for clone in repository [ 'links' ] [ 'clone' ] ] for url in clone_urls : if url . startswith ( 'ssh://' ) : project [ 'repositoryURL' ] = url break description = repository [ 'project' ] . get ( 'description' , '' ) if description : project [ 'description' ] = 'Project description: %s' % description project [ 'permissions' ] [ 'licenses' ] = None web_url = repository [ 'links' ] [ 'self' ] [ 0 ] [ 'href' ] public_server = web_url . startswith ( 'https://bitbucket.org' ) if repository [ 'public' ] and public_server : project [ 'permissions' ] [ 'usageType' ] = 'openSource' if labor_hours : project [ 'laborHours' ] = labor_hours_from_url ( project [ 'repositoryURL' ] ) else : project [ 'laborHours' ] = 0 project [ 'tags' ] = [ 'bitbucket' ] project [ 'contact' ] [ 'email' ] = '' project [ 'contact' ] [ 'URL' ] = repository [ 'links' ] [ 'self' ] [ 0 ] [ 'href' ] project [ 'status' ] = 'Development' project [ 'vcs' ] = repository [ 'scmId' ] project [ 'homepageURL' ] = repository [ 'links' ] [ 'self' ] [ 0 ] [ 'href' ] _prune_dict_null_str ( project ) return project
def force_attributes ( metadata , config ) : organization = config . get ( 'organization' , '' ) logger . debug ( 'Organization: %s' , organization ) contact_email = config . get ( 'contact_email' ) logger . debug ( 'Contact Email: %s' , contact_email ) permissions = config . get ( 'permissions' , { } ) default_usage = permissions . get ( 'usageType' , '' ) default_exemption_text = permissions . get ( 'exemptionText' , '' ) logger . debug ( 'Default usageType: %s' , default_usage ) logger . debug ( 'Default exemptionText: %s' , default_exemption_text ) if organization : logger . debug ( 'Forcing Organization to: %s' , organization ) if contact_email : logger . debug ( 'Forcing Contact Email to: %s' , contact_email ) for release in metadata [ 'releases' ] : if organization : release [ 'organization' ] = organization if contact_email : release [ 'contact' ] [ 'email' ] = contact_email if 'licenses' not in release [ 'permissions' ] : release [ 'permissions' ] [ 'licenses' ] = None if 'description' not in release : release [ 'description' ] = 'No description available...' if 'usageType' not in release [ 'permissions' ] : release [ 'permissions' ] [ 'usageType' ] = default_usage release [ 'permissions' ] [ 'exemptionText' ] = default_exemption_text return metadata
def get_traffic ( self ) : print 'Getting traffic.' #Uses the developer API. Note this could change. headers = { 'Accept' : 'application/vnd.github.spiderman-preview' , 'Authorization' : 'token ' + self . token } headers_release = { 'Authorization' : 'token ' + self . token } for repo in self . org_retrieved . iter_repos ( type = 'public' ) : url = ( 'https://api.github.com/repos/' + self . organization_name + '/' + repo . name ) self . get_referrers ( url = url , headers = headers , repo_name = repo . name ) self . get_paths ( url = url , headers = headers ) self . get_data ( url = url , headers = headers , dict_to_store = self . views , type = 'views' , repo_name = repo . name ) self . get_data ( url = url , headers = headers , dict_to_store = self . clones , type = 'clones' , repo_name = repo . name ) self . get_releases ( url = url , headers = headers_release , repo_name = repo . name )
def get_releases ( self , url = '' , headers = { } , repo_name = '' ) : url_releases = ( url + '/releases' ) r = requests . get ( url_releases , headers = headers ) self . releases_json [ repo_name ] = r . json ( )
def write_json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict_to_write = { } , path_ending_type = '' ) : for repo in dict_to_write : if len ( dict_to_write [ repo ] ) != 0 : #don't need to write out empty lists path = ( '../github-data/' + organization + '/' + repo + '/' + path_ending_type + '/' + str ( date ) + '.json' ) self . checkDir ( path ) with open ( path , 'w' ) as out : out . write ( json . dumps ( dict_to_write [ repo ] , sort_keys = True , indent = 4 , separators = ( ',' , ': ' ) ) ) out . close ( )
def write_to_file ( self , referrers_file_path = '' , views_file_path = '' , clones_file_path = '' , date = ( datetime . date . today ( ) ) , organization = 'llnl' , views_row_count = 0 , clones_row_count = 0 ) : self . write_referrers_to_file ( file_path = referrers_file_path ) self . write_data_to_file ( file_path = views_file_path , dict_to_write = self . views , name = 'views' , row_count = views_row_count ) self . write_data_to_file ( file_path = clones_file_path , dict_to_write = self . clones , name = 'clones' , row_count = clones_row_count )
def write_data_to_file ( self , file_path = '' , date = str ( datetime . date . today ( ) ) , organization = 'llnl' , dict_to_write = { } , name = '' , row_count = 0 ) : exists = os . path . isfile ( file_path ) with open ( file_path , 'a' ) as out : if not exists : out . write ( 'date,organization,' + name + ',unique_' + name + ',id\n' ) sorted_dict = sorted ( dict_to_write ) for day in sorted_dict : day_formatted = datetime . datetime . utcfromtimestamp ( day ) . strftime ( '%Y-%m-%d' ) out . write ( day_formatted + ',' + organization + ',' + str ( dict_to_write [ day ] [ 0 ] ) + ',' + str ( dict_to_write [ day ] [ 1 ] ) + ',' + str ( row_count ) + '\n' ) row_count += 1
def write_referrers_to_file ( self , file_path = '' , date = str ( datetime . date . today ( ) ) , organization = 'llnl' ) : self . remove_date ( file_path = file_path , date = date ) referrers_exists = os . path . isfile ( file_path ) with open ( file_path , 'a' ) as out : if not referrers_exists : out . write ( 'date,organization,referrer,count,count_log,uniques,' + 'uniques_logged\n' ) sorted_referrers = sorted ( self . referrers_lower ) #sort based on lowercase for referrer in sorted_referrers : ref_name = self . referrers_lower [ referrer ] #grab real name from count = self . referrers [ ref_name ] [ 0 ] uniques = self . referrers [ ref_name ] [ 1 ] if count == 1 : #so we don't display 0 for count of 1 count = 1.5 if uniques == 1 : uniques = 1.5 count_logged = math . log ( count ) uniques_logged = math . log ( uniques ) out . write ( date + ',' + organization + ',' + ref_name + ',' + str ( count ) + ',' + str ( count_logged ) + ',' + str ( uniques ) + ',' + str ( uniques_logged ) + '\n' ) out . close ( )
def write_to_file ( self , file_path = '' ) : with open ( file_path , 'w+' ) as out : out . write ( 'user, email\n' ) sorted_names = sorted ( self . logins_lower ) #sort based on lowercase for login in sorted_names : out . write ( self . logins_lower [ login ] + ',' + self . emails [ self . logins_lower [ login ] ] + '\n' ) out . close ( )
def connect ( url , username , password ) : bb_session = stashy . connect ( url , username , password ) logger . info ( 'Connected to: %s as %s' , url , username ) return bb_session
def query_repos ( gl_session , repos = None ) : if repos is None : repos = [ ] for repo in repos : yield gl_session . projects . get ( repo ) if not repos : for project in gl_session . projects . list ( as_list = False ) : yield project
def _prune_dict_null_str ( dictionary ) : for key , value in list ( dictionary . items ( ) ) : if value is None or str ( value ) == '' : del dictionary [ key ] if isinstance ( value , dict ) : dictionary [ key ] = _prune_dict_null_str ( dictionary [ key ] ) return dictionary
def create_tfs_connection ( url , token ) : if token is None : token = os . environ . get ( 'TFS_API_TOKEN' , None ) tfs_credentials = BasicAuthentication ( '' , token ) tfs_connection = VssConnection ( base_url = url , creds = tfs_credentials ) return tfs_connection
def create_tfs_git_client ( url , token = None ) : if token is None : token = os . environ . get ( 'TFS_API_TOKEN' , None ) tfs_connection = create_tfs_connection ( url , token ) tfs_git_client = tfs_connection . get_client ( 'vsts.git.v4_1.git_client.GitClient' ) if tfs_git_client is None : msg = 'Unable to create TFS Git Client, failed to connect to TFS Enterprise (%s) with provided token.' raise RuntimeError ( msg , url ) return tfs_git_client
def create_tfs_tfvc_client ( url , token = None ) : if token is None : token = os . environ . get ( 'TFS_API_TOKEN' , None ) tfs_connection = create_tfs_connection ( url , token ) tfs_tfvc_client = tfs_connection . get_client ( 'vsts.tfvc.v4_1.tfvc_client.TfvcClient' ) if tfs_tfvc_client is None : msg = 'Unable to create TFS Git Client, failed to connect to TFS Enterprise (%s) with provided token.' raise RuntimeError ( msg , url ) return tfs_tfvc_client
def get_git_repos ( url , token , collection , project ) : git_client = create_tfs_git_client ( '{url}/{collection_name}' . format ( url = url , collection_name = collection . name ) , token ) logger . debug ( 'Retrieving Git Repos for Project: {project_name}' . format ( project_name = project . name ) ) return git_client . get_repositories ( project . id )
def get_tfvc_repos ( url , token , collection , project ) : branch_list = [ ] tfvc_client = create_tfs_tfvc_client ( '{url}/{collection_name}' . format ( url = url , collection_name = collection . name ) , token ) logger . debug ( 'Retrieving Tfvc Branches for Project: {project_name}' . format ( project_name = project . name ) ) branches = tfvc_client . get_branches ( project . id , True , True , False , True ) if branches : branch_list . extend ( branches ) else : logger . debug ( 'No Tfvcc Branches in Project: {project_name}' . format ( project_name = project . name ) ) return branch_list
def write_to_file ( self ) : with open ( '../github_stats_output/last_year_commits.csv' , 'w+' ) as output : output . write ( 'date,organization,repos,members,teams,' + 'unique_contributors,total_contributors,forks,' + 'stargazers,pull_requests,open_issues,has_readme,' + 'has_license,pull_requests_open,pull_requests_closed,' + 'commits\n' ) #no reverse this time to print oldest first previous_commits = 0 for week in self . sorted_weeks : if str ( self . commits [ week ] ) != previous_commits : #delete dups week_formatted = datetime . datetime . utcfromtimestamp ( week ) . strftime ( '%Y-%m-%d' ) output . write ( week_formatted + ',llnl,0,0,0,0,0,0,0,0,0,0,0,0,0,' + str ( self . commits [ week ] ) + '\n' ) previous_commits = str ( self . commits [ week ] )
def incr ( self , stat , value = 1 , tags = None ) : self . client . incr ( stat = stat , count = value )
def timing ( self , stat , value , tags = None ) : self . client . timing ( stat = stat , delta = value )
def incr ( self , stat , value = 1 , tags = None ) : self . client . increment ( metric = stat , value = value , tags = tags )
def gauge ( self , stat , value , tags = None ) : self . client . gauge ( metric = stat , value = value , tags = tags )
def timing ( self , stat , value , tags = None ) : self . client . timing ( metric = stat , value = value , tags = tags )
def histogram ( self , stat , value , tags = None ) : self . client . histogram ( metric = stat , value = value , tags = tags )
def incr ( self , stat , value = 1 , tags = None ) : self . _log ( 'incr' , stat , value , tags )
def gauge ( self , stat , value , tags = None ) : self . _log ( 'gauge' , stat , value , tags )
def timing ( self , stat , value , tags = None ) : self . _log ( 'timing' , stat , value , tags )
def histogram ( self , stat , value , tags = None ) : self . _log ( 'histogram' , stat , value , tags )
def rollup ( self ) : now = time . time ( ) if now < self . next_rollup : return self . next_rollup = now + self . flush_interval for key , values in sorted ( self . incr_stats . items ( ) ) : self . logger . info ( '%s INCR %s: count:%d|rate:%d/%d' , self . leader , key , len ( values ) , sum ( values ) , self . flush_interval ) self . incr_stats [ key ] = [ ] for key , values in sorted ( self . gauge_stats . items ( ) ) : if values : self . logger . info ( '%s GAUGE %s: count:%d|current:%s|min:%s|max:%s' , self . leader , key , len ( values ) , values [ - 1 ] , min ( values ) , max ( values ) , ) else : self . logger . info ( '%s (gauge) %s: no data' , self . leader , key ) self . gauge_stats [ key ] = [ ] for key , values in sorted ( self . histogram_stats . items ( ) ) : if values : self . logger . info ( ( '%s HISTOGRAM %s: ' 'count:%d|min:%.2f|avg:%.2f|median:%.2f|ninety-five:%.2f|max:%.2f' ) , self . leader , key , len ( values ) , min ( values ) , statistics . mean ( values ) , statistics . median ( values ) , values [ int ( len ( values ) * 95 / 100 ) ] , max ( values ) ) else : self . logger . info ( '%s (histogram) %s: no data' , self . leader , key ) self . histogram_stats [ key ] = [ ]
def incr ( self , stat , value = 1 , tags = None ) : self . rollup ( ) self . incr_stats . setdefault ( stat , [ ] ) . append ( value )
def gauge ( self , stat , value , tags = None ) : self . rollup ( ) self . gauge_stats . setdefault ( stat , [ ] ) . append ( value )
def histogram ( self , stat , value , tags = None ) : self . rollup ( ) self . histogram_stats . setdefault ( stat , [ ] ) . append ( value )
def from_db_value ( self , value , expression , connection , context ) : if value is None : return value return self . enum [ value ]
def to_python ( self , value ) : if value is None : return value if isinstance ( value , self . enum ) : return value return self . enum [ value ]
def get_prep_value ( self , value ) : if value is None : return None if isinstance ( value , self . enum ) : return value . name raise ValueError ( "Unknown value {value:r} of type {cls}" . format ( value = value , cls = type ( value ) ) )
def t_parse ( self , s ) : with self . lock : try : return self . parser . parse ( s , lexer = self . lexer , debug = False ) except CannotParse as e : e . s = s raise e
def parse ( self , s ) : with self . lock : try : return self . parser . parse ( s , lexer = self . lexer ) except InvalidIEMLObjectArgument as e : raise CannotParse ( s , str ( e ) ) except CannotParse as e : e . s = s raise e
def _resolve_path ( obj , path ) : if obj . __class__ not in path . context . accept : result = set ( ) for ctx in path . context . accept : result |= { e for u in obj [ ctx ] for e in _resolve_path ( u , path ) } return result if isinstance ( obj , Text ) : if path . index is not None : return { obj . children [ path . index ] } return set ( obj . children ) if isinstance ( obj , ( Fact , Theory ) ) : return _resolve_path_tree_graph ( obj . tree_graph , path ) if isinstance ( obj , Topic ) : if path . kind == 'r' : if path . index is not None : return { obj . root [ path . index ] } return set ( obj . root ) else : if path . index is not None : return { obj . flexing [ path . index ] } return set ( obj . flexing )
def mean ( self ) : if self . counter . value > 0 : return self . sum . value / self . counter . value return 0.0
def mean_rate ( self ) : if self . counter . value == 0 : return 0.0 else : elapsed = time ( ) - self . start_time return self . counter . value / elapsed
def send_metric ( self , name , metric ) : config = SERIALIZER_CONFIG [ class_name ( metric ) ] mmap ( self . _buffered_send_metric , self . serialize_metric ( metric , name , config [ 'keys' ] , config [ 'serialized_type' ] ) ) if hasattr ( metric , 'snapshot' ) and config . get ( 'snapshot_keys' ) : mmap ( self . _buffered_send_metric , self . serialize_metric ( metric . snapshot , name , config [ 'snapshot_keys' ] , config [ 'serialized_type' ] ) )
def serialize_metric ( self , metric , m_name , keys , m_type ) : return [ self . format_metric_string ( m_name , getattr ( metric , key ) , m_type ) for key in keys ]
def format_metric_string ( self , name , value , m_type ) : template = '{name}:{value}|{m_type}\n' if self . prefix : name = "{prefix}.{m_name}" . format ( prefix = self . prefix , m_name = name ) return template . format ( name = name , value = value , m_type = m_type )
def _buffered_send_metric ( self , metric_str ) : self . batch_count += 1 self . batch_buffer += metric_str if self . batch_count >= self . batch_size : self . _send ( )
def _json_safe ( data ) : if not hasattr ( data , 'encode' ) : try : data = data . decode ( 'utf-8' ) except UnicodeDecodeError : raise ValueError ( 'Expected valid UTF8 for JSON data, got %r' % ( data , ) ) return data
def solve ( grid ) : clauses = sudoku_clauses ( ) for i in range ( 1 , 10 ) : for j in range ( 1 , 10 ) : d = grid [ i - 1 ] [ j - 1 ] if d : clauses . append ( [ v ( i , j , d ) ] ) sol = set ( pycosat . solve ( clauses ) ) def read_cell ( i , j ) : for d in range ( 1 , 10 ) : if v ( i , j , d ) in sol : return d for i in range ( 1 , 10 ) : for j in range ( 1 , 10 ) : grid [ i - 1 ] [ j - 1 ] = read_cell ( i , j )
def view ( injector ) : handler = create_handler ( View , injector ) apply_http_methods ( handler , injector ) return injector . let ( as_view = handler . as_view )
def form_view ( injector ) : handler = create_handler ( FormView , injector ) apply_form_methods ( handler , injector ) return injector . let ( as_view = handler . as_view )
def method_view ( injector ) : handler = create_handler ( MethodView ) apply_http_methods ( handler , injector ) return injector . let ( as_view = handler . as_view )
def api_view ( injector ) : handler = create_handler ( APIView , injector ) apply_http_methods ( handler , injector ) apply_api_view_methods ( handler , injector ) return injector . let ( as_view = handler . as_view )
def generic_api_view ( injector ) : handler = create_handler ( GenericAPIView , injector ) apply_http_methods ( handler , injector ) apply_api_view_methods ( handler , injector ) apply_generic_api_view_methods ( handler , injector ) return injector . let ( as_view = handler . as_view )
def model_view_set ( injector ) : handler = create_handler ( ModelViewSet , injector ) apply_api_view_methods ( handler , injector ) apply_generic_api_view_methods ( handler , injector ) apply_model_view_set_methods ( handler , injector ) return injector . let ( as_viewset = lambda : handler )
def stream_from_fd ( fd , loop ) : reader = asyncio . StreamReader ( loop = loop ) protocol = asyncio . StreamReaderProtocol ( reader , loop = loop ) waiter = asyncio . futures . Future ( loop = loop ) transport = UnixFileDescriptorTransport ( loop = loop , fileno = fd , protocol = protocol , waiter = waiter , ) try : yield from waiter except Exception : transport . close ( ) if loop . get_debug ( ) : logger . debug ( "Read fd %r connected: (%r, %r)" , fd , transport , protocol ) return reader , transport
def _read_ready ( self ) : try : data = os . read ( self . _fileno , self . max_size ) except InterruptedError : pass except OSError as exc : self . _fatal_error ( exc , "Fatal read error on file descriptor read" ) else : if data : self . _protocol . data_received ( data ) else : if self . _loop . get_debug ( ) : logger . info ( "%r was closed by the kernel" , self ) self . _closing = False self . pause_reading ( ) self . _loop . call_soon ( self . _protocol . eof_received ) self . _loop . call_soon ( self . _call_connection_lost , None )
def _close ( self , error = None ) : self . _closing = True self . pause_reading ( ) self . _loop . call_soon ( self . _call_connection_lost , error )
def watch ( self , path , flags , * , alias = None ) : if alias is None : alias = path if alias in self . requests : raise ValueError ( "A watch request is already scheduled for alias %s" % alias ) self . requests [ alias ] = ( path , flags ) if self . _fd is not None : self . _setup_watch ( alias , path , flags )
def unwatch ( self , alias ) : if alias not in self . descriptors : raise ValueError ( "Unknown watch alias %s; current set is %r" % ( alias , list ( self . descriptors . keys ( ) ) ) ) wd = self . descriptors [ alias ] errno = LibC . inotify_rm_watch ( self . _fd , wd ) if errno != 0 : raise IOError ( "Failed to close watcher %d: errno=%d" % ( wd , errno ) ) del self . descriptors [ alias ] del self . requests [ alias ] del self . aliases [ wd ]
def _setup_watch ( self , alias , path , flags ) : assert alias not in self . descriptors , "Registering alias %s twice!" % alias wd = LibC . inotify_add_watch ( self . _fd , path , flags ) if wd < 0 : raise IOError ( "Error setting up watch on %s with flags %s: wd=%s" % ( path , flags , wd ) ) self . descriptors [ alias ] = wd self . aliases [ wd ] = alias
def setup ( self , loop ) : self . _loop = loop self . _fd = LibC . inotify_init ( ) for alias , ( path , flags ) in self . requests . items ( ) : self . _setup_watch ( alias , path , flags ) self . _stream , self . _transport = yield from aioutils . stream_from_fd ( self . _fd , loop )
def touch ( self ) : assert not self . _has_responded self . trigger ( event . TOUCH , message = self )
def success ( self ) : if self . interval == 0.0 : return self . short_interval -= self . short_unit self . long_interval -= self . long_unit self . short_interval = max ( self . short_interval , Decimal ( 0 ) ) self . long_interval = max ( self . long_interval , Decimal ( 0 ) ) self . update_interval ( )
def failure ( self ) : self . short_interval += self . short_unit self . long_interval += self . long_unit self . short_interval = min ( self . short_interval , self . max_short_timer ) self . long_interval = min ( self . long_interval , self . max_long_timer ) self . update_interval ( )
def close ( self ) : for conn in self . conns . values ( ) : conn . close ( ) self . redist_periodic . stop ( ) if self . query_periodic is not None : self . query_periodic . stop ( )
def set_max_in_flight ( self , max_in_flight ) : assert isinstance ( max_in_flight , int ) self . max_in_flight = max_in_flight if max_in_flight == 0 : for conn in itervalues ( self . conns ) : if conn . rdy > 0 : logger . debug ( '[%s:%s] rdy: %d -> 0' , conn . id , self . name , conn . rdy ) self . _send_rdy ( conn , 0 ) self . total_rdy = 0 else : self . need_rdy_redistributed = True self . _redistribute_rdy_state ( )
def score_function ( self , x , W ) : if ( self . svm_kernel == 'polynomial_kernel' or self . svm_kernel == 'gaussian_kernel' or self . svm_kernel == 'soft_polynomial_kernel' or self . svm_kernel == 'soft_gaussian_kernel' ) : x = x [ 1 : ] score = np . sign ( np . sum ( self . sv_alpha * self . sv_Y * utility . Kernel . kernel_matrix_xX ( self , x , self . sv_X ) ) + self . sv_avg_b ) else : score = np . sign ( np . inner ( x , W ) ) return score
def score_function ( self , x , W ) : score = super ( BinaryClassifier , self ) . score_function ( x , W ) if score >= 0.5 : score = 1.0 else : score = - 1.0 return score
def score_function ( self , x , W ) : score = self . sign * np . sign ( x [ self . feature_index ] - self . theta ) return score
def set_feature_transform ( self , mode = 'polynomial' , degree = 1 ) : if self . status != 'load_train_data' : print ( "Please load train data first." ) return self . train_X self . feature_transform_mode = mode self . feature_transform_degree = degree self . train_X = self . train_X [ : , 1 : ] self . train_X = utility . DatasetLoader . feature_transform ( self . train_X , self . feature_transform_mode , self . feature_transform_degree ) return self . train_X
def score_function ( self , x , W ) : score = self . theta ( np . inner ( x , W ) ) return score
def clean_up ( fastq_pairs , clear ) : unpaired_fastq = [ f for f in os . listdir ( "." ) if f . endswith ( "_U.fastq.gz" ) ] for fpath in unpaired_fastq : os . remove ( fpath ) expected_out = [ f for f in os . listdir ( "." ) if f . endswith ( "_trim.fastq.gz" ) ] if clear == "true" and len ( expected_out ) == 2 : for fq in fastq_pairs : rp = os . path . realpath ( fq ) logger . debug ( "Removing temporary fastq file path: {}" . format ( rp ) ) if re . match ( ".*/work/.{2}/.{30}/.*" , rp ) : os . remove ( rp )
def _check_required_files ( self ) : if not os . path . exists ( self . trace_file ) : raise eh . InspectionError ( "The provided trace file could not be " "opened: {}" . format ( self . trace_file ) ) if not os . path . exists ( self . log_file ) : raise eh . InspectionError ( "The .nextflow.log files could not be " "opened. Are you sure you are in a " "nextflow project directory?" )
def _clear_inspect ( self ) : self . trace_info = defaultdict ( list ) self . process_tags = { } self . process_stats = { } self . samples = [ ] self . stored_ids = [ ] self . stored_log_ids = [ ] self . time_start = None self . time_stop = None self . execution_command = None self . nextflow_version = None self . abort_cause = None self . _c = 0 for p in self . processes . values ( ) : p [ "barrier" ] = "W" for i in [ "submitted" , "finished" , "failed" , "retry" ] : p [ i ] = set ( )
def _update_barrier_status ( self ) : with open ( self . log_file ) as fh : for line in fh : if "Session aborted" in line : return if "<<< barrier arrive" in line : process_m = re . match ( ".*process: (.*)\)" , line ) if process_m : process = process_m . group ( 1 ) if process in self . processes : self . processes [ process ] [ "barrier" ] = "C"
def display_overview ( self ) : stay_alive = True self . screen = curses . initscr ( ) self . screen . keypad ( True ) self . screen . nodelay ( - 1 ) curses . cbreak ( ) curses . noecho ( ) curses . start_color ( ) self . screen_lines = self . screen . getmaxyx ( ) [ 0 ] try : while stay_alive : self . _curses_keybindings ( ) self . update_inspection ( ) self . flush_overview ( ) sleep ( self . refresh_rate ) except FileNotFoundError : sys . stderr . write ( colored_print ( "ERROR: nextflow log and/or trace files are no longer " "reachable!" , "red_bold" ) ) except Exception as e : sys . stderr . write ( str ( e ) ) finally : curses . nocbreak ( ) self . screen . keypad ( 0 ) curses . echo ( ) curses . endwin ( )
def _updown ( self , direction ) : if direction == "up" and self . top_line != 0 : self . top_line -= 1 elif direction == "down" and self . screen . getmaxyx ( ) [ 0 ] + self . top_line <= self . content_lines + 3 : self . top_line += 1
def _rightleft ( self , direction ) : if direction == "left" and self . padding != 0 : self . padding -= 1 if direction == "right" and self . screen . getmaxyx ( ) [ 1 ] + self . padding < self . max_width : self . padding += 1
def _get_run_hash ( self ) : pipeline_path = get_nextflow_filepath ( self . log_file ) pipeline_hash = hashlib . md5 ( ) with open ( pipeline_path , "rb" ) as fh : for chunk in iter ( lambda : fh . read ( 4096 ) , b"" ) : pipeline_hash . update ( chunk ) workdir = self . workdir . encode ( "utf8" ) hostname = socket . gethostname ( ) . encode ( "utf8" ) hardware_addr = str ( uuid . getnode ( ) ) . encode ( "utf8" ) dir_hash = hashlib . md5 ( workdir + hostname + hardware_addr ) return pipeline_hash . hexdigest ( ) + dir_hash . hexdigest ( )
def write_report_data ( self ) : json_plot = self . get_plot_data ( ) json_table = self . get_table_data ( ) json_dic = { * * json_plot , * * json_table } with open ( ".report.json" , "w" ) as json_report : json_report . write ( json . dumps ( json_dic , separators = ( "," , ":" ) ) )
def _build_header ( self ) : logger . debug ( "===============" ) logger . debug ( "Building header" ) logger . debug ( "===============" ) self . template += hs . header
def _build_footer ( self ) : logger . debug ( "===============" ) logger . debug ( "Building header" ) logger . debug ( "===============" ) self . template += fs . footer
def _set_status_channels ( self ) : status_inst = pc . StatusCompiler ( template = "status_compiler" ) report_inst = pc . ReportCompiler ( template = "report_compiler" ) status_channels = [ ] for p in [ p for p in self . processes ] : if not any ( [ isinstance ( p , x ) for x in self . skip_class ] ) : status_channels . extend ( p . status_strs ) if not status_channels : logger . debug ( "No status channels found. Skipping status compiler" "process" ) return logger . debug ( "Setting status channels: {}" . format ( status_channels ) ) if len ( status_channels ) != len ( set ( status_channels ) ) : raise eh . ProcessError ( "Duplicate status channels detected. Please ensure that " "the 'status_channels' attributes of each process are " "unique. Here are the status channels:\n\n{}" . format ( ", " . join ( status_channels ) ) ) status_inst . set_compiler_channels ( status_channels ) report_channels = [ "REPORT_{}" . format ( x . lstrip ( "STATUS_" ) ) for x in status_channels ] report_inst . set_compiler_channels ( report_channels ) self . processes . extend ( [ status_inst , report_inst ] )
def export_directives ( self ) : directives_json = { } for p in self . processes [ 1 : ] : directives_json [ p . template ] = p . directives sys . stdout . write ( json . dumps ( directives_json ) )
def _get_report_id ( self ) : if self . watch : pipeline_path = get_nextflow_filepath ( self . log_file ) pipeline_hash = hashlib . md5 ( ) with open ( pipeline_path , "rb" ) as fh : for chunk in iter ( lambda : fh . read ( 4096 ) , b"" ) : pipeline_hash . update ( chunk ) workdir = os . getcwd ( ) . encode ( "utf8" ) hostname = socket . gethostname ( ) . encode ( "utf8" ) hardware_addr = str ( uuid . getnode ( ) ) . encode ( "utf8" ) dir_hash = hashlib . md5 ( workdir + hostname + hardware_addr ) return pipeline_hash . hexdigest ( ) + dir_hash . hexdigest ( ) else : with open ( self . report_file ) as fh : report_json = json . loads ( fh . read ( ) ) metadata = report_json [ "data" ] [ "results" ] [ 0 ] [ "nfMetadata" ] try : report_id = metadata [ "scriptId" ] + metadata [ "sessionId" ] except KeyError : raise eh . ReportError ( "Incomplete or corrupt report JSON file " "missing the 'scriptId' and/or 'sessionId' " "metadata information" ) return report_id
def update_log_watch ( self ) : size_stamp = os . path . getsize ( self . log_file ) self . trace_retry = 0 if size_stamp and size_stamp == self . log_sizestamp : return else : logger . debug ( "Updating log size stamp to: {}" . format ( size_stamp ) ) self . log_sizestamp = size_stamp self . _update_pipeline_status ( )
def _map_w_to_data ( self ) : self . _Wmapped_index = vq ( self . data , self . W ) self . Wmapped = np . zeros ( self . W . shape ) for i , s in enumerate ( self . _Wmapped_index ) : self . Wmapped [ : , i ] = self . data [ : , s ]
def median_filter ( X , M = 8 ) : for i in range ( X . shape [ 1 ] ) : X [ : , i ] = filters . median_filter ( X [ : , i ] , size = M ) return X
def compute_gaussian_krnl ( M ) : g = signal . gaussian ( M , M // 3. , sym = True ) G = np . dot ( g . reshape ( - 1 , 1 ) , g . reshape ( 1 , - 1 ) ) G [ M // 2 : , : M // 2 ] = - G [ M // 2 : , : M // 2 ] G [ : M // 2 , M // 2 : ] = - G [ : M // 2 , M // 2 : ] return G
def compute_ssm ( X , metric = "seuclidean" ) : D = distance . pdist ( X , metric = metric ) D = distance . squareform ( D ) D /= D . max ( ) return 1 - D
def pick_peaks ( nc , L = 16 ) : offset = nc . mean ( ) / 20. nc = filters . gaussian_filter1d ( nc , sigma = 4 ) th = filters . median_filter ( nc , size = L ) + offset #th = filters.gaussian_filter(nc, sigma=L/2., mode="nearest") + offset peaks = [ ] for i in range ( 1 , nc . shape [ 0 ] - 1 ) : if nc [ i - 1 ] < nc [ i ] and nc [ i ] > nc [ i + 1 ] : if nc [ i ] > th [ i ] : peaks . append ( i ) #plt.plot(nc) #plt.plot(th) #for peak in peaks: #plt.axvline(peak) #plt.show() return peaks
def gaussian_filter ( X , M = 8 , axis = 0 ) : for i in range ( X . shape [ axis ] ) : if axis == 1 : X [ : , i ] = filters . gaussian_filter ( X [ : , i ] , sigma = M / 2. ) elif axis == 0 : X [ i , : ] = filters . gaussian_filter ( X [ i , : ] , sigma = M / 2. ) return X
def compute_nc ( X ) : N = X . shape [ 0 ] nc = np . zeros ( N ) for i in range ( N - 1 ) : nc [ i ] = distance . euclidean ( X [ i , : ] , X [ i + 1 , : ] ) nc += np . abs ( nc . min ( ) ) nc /= float ( nc . max ( ) ) return nc
def pick_peaks ( nc , L = 16 , offset_denom = 0.1 ) : offset = nc . mean ( ) * float ( offset_denom ) th = filters . median_filter ( nc , size = L ) + offset #th = filters.gaussian_filter(nc, sigma=L/2., mode="nearest") + offset #import pylab as plt #plt.plot(nc) #plt.plot(th) #plt.show() peaks = [ ] for i in range ( 1 , nc . shape [ 0 ] - 1 ) : if nc [ i - 1 ] < nc [ i ] and nc [ i ] > nc [ i + 1 ] : if nc [ i ] > th [ i ] : peaks . append ( i ) return peaks
def embedded_space ( X , m , tau = 1 ) : N = X . shape [ 0 ] - int ( np . ceil ( m ) ) Y = np . zeros ( ( N , int ( np . ceil ( X . shape [ 1 ] * m ) ) ) ) for i in range ( N ) : rem = int ( ( m % 1 ) * X . shape [ 1 ] ) Y [ i , : ] = np . concatenate ( ( X [ i : i + int ( m ) , : ] . flatten ( ) , X [ i + int ( m ) , : rem ] ) ) return Y
def plot_one_track ( file_struct , est_times , est_labels , boundaries_id , labels_id , title = None ) : import matplotlib . pyplot as plt bid_lid = boundaries_id if labels_id is not None : bid_lid += " + " + labels_id try : jam = jams . load ( file_struct . ref_file ) ann = jam . search ( namespace = 'segment_.*' ) [ 0 ] ref_inters , ref_labels = ann . to_interval_values ( ) ref_times = utils . intervals_to_times ( ref_inters ) all_boundaries = [ ref_times , est_times ] all_labels = [ ref_labels , est_labels ] algo_ids = [ "GT" , bid_lid ] except : logging . warning ( "No references found in %s. Not plotting groundtruth" % file_struct . ref_file ) all_boundaries = [ est_times ] all_labels = [ est_labels ] algo_ids = [ bid_lid ] N = len ( all_boundaries ) for i , labels in enumerate ( all_labels ) : all_labels [ i ] = mir_eval . util . index_labels ( labels ) [ 0 ] cm = plt . get_cmap ( 'gist_rainbow' ) max_label = max ( max ( labels ) for labels in all_labels ) figsize = ( 8 , 4 ) plt . figure ( 1 , figsize = figsize , dpi = 120 , facecolor = 'w' , edgecolor = 'k' ) for i , boundaries in enumerate ( all_boundaries ) : color = "b" if i == 0 : color = "g" for b in boundaries : plt . axvline ( b , i / float ( N ) , ( i + 1 ) / float ( N ) , color = color ) if labels_id is not None : labels = all_labels [ i ] inters = utils . times_to_intervals ( boundaries ) for label , inter in zip ( labels , inters ) : plt . axvspan ( inter [ 0 ] , inter [ 1 ] , ymin = i / float ( N ) , ymax = ( i + 1 ) / float ( N ) , alpha = 0.6 , color = cm ( label / float ( max_label ) ) ) plt . axhline ( i / float ( N ) , color = "k" , linewidth = 1 ) _plot_formatting ( title , os . path . basename ( file_struct . audio_file ) , algo_ids , all_boundaries [ 0 ] [ - 1 ] , N , None )
def get_dataset_files ( in_path ) : audio_files = [ ] for ext in ds_config . audio_exts : audio_files += glob . glob ( os . path . join ( in_path , ds_config . audio_dir , "*" + ext ) ) utils . ensure_dir ( os . path . join ( in_path , ds_config . features_dir ) ) utils . ensure_dir ( os . path . join ( in_path , ds_config . estimations_dir ) ) utils . ensure_dir ( os . path . join ( in_path , ds_config . references_dir ) ) file_structs = [ ] for audio_file in audio_files : file_structs . append ( FileStruct ( audio_file ) ) file_structs = sorted ( file_structs , key = lambda file_struct : file_struct . audio_file ) return file_structs
def _get_dataset_file ( self , dir , ext ) : audio_file_ext = "." + self . audio_file . split ( "." ) [ - 1 ] base_file = os . path . basename ( self . audio_file ) . replace ( audio_file_ext , ext ) return os . path . join ( self . ds_path , dir , base_file )
def write_features ( self ) : out_json = collections . OrderedDict ( ) try : self . read_features ( ) except ( WrongFeaturesFormatError , FeaturesNotFound , NoFeaturesFileError ) : out_json = collections . OrderedDict ( { "metadata" : { "versions" : { "librosa" : librosa . __version__ , "msaf" : msaf . __version__ , "numpy" : np . __version__ } , "timestamp" : datetime . datetime . today ( ) . strftime ( "%Y/%m/%d %H:%M:%S" ) } } ) out_json [ "globals" ] = { "dur" : self . dur , "sample_rate" : self . sr , "hop_length" : self . hop_length , "audio_file" : self . file_struct . audio_file } out_json [ "est_beats" ] = self . _est_beats_times . tolist ( ) out_json [ "est_beatsync_times" ] = self . _est_beatsync_times . tolist ( ) if self . _ann_beats_times is not None : out_json [ "ann_beats" ] = self . _ann_beats_times . tolist ( ) out_json [ "ann_beatsync_times" ] = self . _ann_beatsync_times . tolist ( ) except FeatureParamsError : with open ( self . file_struct . features_file ) as f : out_json = json . load ( f ) finally : out_json [ self . get_id ( ) ] = { } out_json [ self . get_id ( ) ] [ "params" ] = { } for param_name in self . get_param_names ( ) : value = getattr ( self , param_name ) if hasattr ( value , '__call__' ) : value = value . __name__ else : value = str ( value ) out_json [ self . get_id ( ) ] [ "params" ] [ param_name ] = value out_json [ self . get_id ( ) ] [ "framesync" ] = self . _framesync_features . tolist ( ) out_json [ self . get_id ( ) ] [ "est_beatsync" ] = self . _est_beatsync_features . tolist ( ) if self . _ann_beatsync_features is not None : out_json [ self . get_id ( ) ] [ "ann_beatsync" ] = self . _ann_beatsync_features . tolist ( ) with open ( self . file_struct . features_file , "w" ) as f : json . dump ( out_json , f , indent = 2 )
def _compute_framesync_times ( self ) : self . _framesync_times = librosa . core . frames_to_time ( np . arange ( self . _framesync_features . shape [ 0 ] ) , self . sr , self . hop_length )
def _preprocess ( self , valid_features = [ "pcp" , "tonnetz" , "mfcc" , "cqt" , "tempogram" ] ) : if self . feature_str not in valid_features : raise RuntimeError ( "Feature %s in not valid for algorithm: %s " "(valid features are %s)." % ( self . feature_str , __name__ , valid_features ) ) else : try : F = self . features . features except KeyError : raise RuntimeError ( "Feature %s in not supported by MSAF" % ( self . feature_str ) ) return F
def main ( ) : parser = argparse . ArgumentParser ( description = "Runs the speficied algorithm(s) on the MSAF " "formatted dataset." , formatter_class = argparse . ArgumentDefaultsHelpFormatter ) parser . add_argument ( "in_path" , action = "store" , help = "Input dataset" ) parser . add_argument ( "-f" , action = "store" , dest = "feature" , default = "pcp" , type = str , help = "Type of features" , choices = [ "pcp" , "tonnetz" , "mfcc" , "cqt" , "tempogram" ] ) parser . add_argument ( "-b" , action = "store_true" , dest = "annot_beats" , help = "Use annotated beats" , default = False ) parser . add_argument ( "-fs" , action = "store_true" , dest = "framesync" , help = "Use frame-synchronous features" , default = False ) parser . add_argument ( "-bid" , action = "store" , help = "Boundary algorithm identifier" , dest = "boundaries_id" , default = "gt" , choices = [ "gt" ] + io . get_all_boundary_algorithms ( ) ) parser . add_argument ( "-lid" , action = "store" , help = "Label algorithm identifier" , dest = "labels_id" , default = None , choices = io . get_all_label_algorithms ( ) ) parser . add_argument ( "-j" , action = "store" , dest = "n_jobs" , default = 4 , type = int , help = "The number of threads to use" ) args = parser . parse_args ( ) start_time = time . time ( ) process ( args . in_path , annot_beats = args . annot_beats , feature = args . feature , framesync = args . framesync , boundaries_id = args . boundaries_id , labels_id = args . labels_id , n_jobs = args . n_jobs ) logging . info ( "Done! Took %.2f seconds." % ( time . time ( ) - start_time ) )
def main ( ) : parser = argparse . ArgumentParser ( description = "Runs the speficied algorithm(s) on the input file and " "the results using the MIREX format." , formatter_class = argparse . ArgumentDefaultsHelpFormatter ) parser . add_argument ( "-bid" , action = "store" , help = "Boundary algorithm identifier" , dest = "boundaries_id" , default = msaf . config . default_bound_id , choices = [ "gt" ] + msaf . io . get_all_boundary_algorithms ( ) ) parser . add_argument ( "-lid" , action = "store" , help = "Label algorithm identifier" , dest = "labels_id" , default = msaf . config . default_label_id , choices = msaf . io . get_all_label_algorithms ( ) ) parser . add_argument ( "-i" , action = "store" , dest = "in_file" , help = "Input audio file" ) parser . add_argument ( "-o" , action = "store" , dest = "out_file" , help = "Output file with the results" , default = "out.txt" ) args = parser . parse_args ( ) start_time = time . time ( ) logging . basicConfig ( format = '%(asctime)s: %(levelname)s: %(message)s' , level = logging . INFO ) params = { "annot_beats" : False , "feature" : "cqt" , "framesync" : False , "boundaries_id" : args . boundaries_id , "labels_id" : args . labels_id , "n_jobs" : 1 , "hier" : False , "sonify_bounds" : False , "plot" : False } res = msaf . run . process ( args . in_file , * * params ) msaf . io . write_mirex ( res [ 0 ] , res [ 1 ] , args . out_file ) logging . info ( "Done! Took %.2f seconds." % ( time . time ( ) - start_time ) )
def compute_all_features ( file_struct , framesync ) : for feature_id in msaf . features_registry : logging . info ( "Computing %s for file %s" % ( feature_id , file_struct . audio_file ) ) feats = Features . select_features ( feature_id , file_struct , False , framesync ) feats . features
def process ( in_path , out_file , n_jobs , framesync ) : if os . path . isfile ( in_path ) : file_struct = msaf . io . FileStruct ( in_path ) file_struct . features_file = out_file compute_all_features ( file_struct , framesync ) else : file_structs = msaf . io . get_dataset_files ( in_path ) return Parallel ( n_jobs = n_jobs ) ( delayed ( compute_all_features ) ( file_struct , framesync ) for file_struct in file_structs )
def main ( ) : parser = argparse . ArgumentParser ( description = "Extracts a set of features from a given dataset " "or audio file and saves them into the 'features' folder of " "the dataset or the specified single file." , formatter_class = argparse . ArgumentDefaultsHelpFormatter ) parser . add_argument ( "in_path" , action = "store" , help = "Input dataset dir or audio file" ) parser . add_argument ( "-j" , action = "store" , dest = "n_jobs" , type = int , help = "Number of jobs (only for collection mode)" , default = 4 ) parser . add_argument ( "-o" , action = "store" , dest = "out_file" , type = str , help = "Output file (only for single file mode)" , default = "out.json" ) parser . add_argument ( "-d" , action = "store" , dest = "ds_name" , default = "*" , help = "The prefix of the dataset to use " "(e.g. Isophonics, SALAMI)" ) parser . add_argument ( "-fs" , action = "store_true" , dest = "framesync" , help = "Use frame-synchronous features" , default = False ) args = parser . parse_args ( ) start_time = time . time ( ) logging . basicConfig ( format = '%(asctime)s: %(levelname)s: %(message)s' , level = logging . INFO ) process ( args . in_path , out_file = args . out_file , n_jobs = args . n_jobs , framesync = args . framesync ) logging . info ( "Done! Took %.2f seconds." % ( time . time ( ) - start_time ) )
def gaussian_cost ( X ) : d , n = X . shape if n < 2 : return 0 sigma = np . var ( X , axis = 1 , ddof = 1 ) cost = - 0.5 * d * n * np . log ( 2. * np . pi ) - 0.5 * ( n - 1. ) * np . sum ( sigma ) return cost
def lognormalize ( F , floor = 0.1 , min_db = - 80 ) : assert min_db < 0 F = min_max_normalize ( F , floor = floor ) F = np . abs ( min_db ) * np . log10 ( F ) return F
def min_max_normalize ( F , floor = 0.001 ) : F += - F . min ( ) + floor F = F / F . max ( axis = 0 ) return F
def get_time_frames ( dur , anal ) : n_frames = get_num_frames ( dur , anal ) return np . linspace ( 0 , dur , num = n_frames )
def remove_empty_segments ( times , labels ) : assert len ( times ) - 1 == len ( labels ) inters = times_to_intervals ( times ) new_inters = [ ] new_labels = [ ] for inter , label in zip ( inters , labels ) : if inter [ 0 ] < inter [ 1 ] : new_inters . append ( inter ) new_labels . append ( label ) return intervals_to_times ( np . asarray ( new_inters ) ) , new_labels
def _distance ( self , idx ) : if scipy . sparse . issparse ( self . data ) : step = self . data . shape [ 1 ] else : step = 50000 d = np . zeros ( ( self . data . shape [ 1 ] ) ) if idx == - 1 : vec = np . zeros ( ( self . data . shape [ 0 ] , 1 ) ) if scipy . sparse . issparse ( self . data ) : vec = scipy . sparse . csc_matrix ( vec ) else : vec = self . data [ : , idx : idx + 1 ] self . _logger . info ( 'compute distance to node ' + str ( idx ) ) for idx_start in range ( 0 , self . data . shape [ 1 ] , step ) : if idx_start + step > self . data . shape [ 1 ] : idx_end = self . data . shape [ 1 ] else : idx_end = idx_start + step d [ idx_start : idx_end ] = self . _distfunc ( self . data [ : , idx_start : idx_end ] , vec ) self . _logger . info ( 'completed:' + str ( idx_end / ( self . data . shape [ 1 ] / 100.0 ) ) + "%" ) return d
def run_kmeans ( self , X , K ) : wX = vq . whiten ( X ) means , dist = vq . kmeans ( wX , K , iter = 100 ) labels , dist = vq . vq ( wX , means ) return means , labels
def compute_bic ( self , D , means , labels , K , R ) : D = vq . whiten ( D ) Rn = D . shape [ 0 ] M = D . shape [ 1 ] if R == K : return 1 mle_var = 0 for k in range ( len ( means ) ) : X = D [ np . argwhere ( labels == k ) ] X = X . reshape ( ( X . shape [ 0 ] , X . shape [ - 1 ] ) ) for x in X : mle_var += distance . euclidean ( x , means [ k ] ) #print x, means[k], mle_var mle_var /= float ( R - K ) l_D = - Rn / 2. * np . log ( 2 * np . pi ) - ( Rn * M ) / 2. * np . log ( mle_var ) - ( Rn - K ) / 2. + Rn * np . log ( Rn ) - Rn * np . log ( R ) p = ( K - 1 ) + M * K + mle_var #print "BIC:", l_D, p, R, K return l_D - p / 2. * np . log ( R )
def magnitude ( X ) : r = np . real ( X ) i = np . imag ( X ) return np . sqrt ( r * r + i * i )
def compute_ffmc2d ( X ) : fft2 = scipy . fftpack . fft2 ( X ) fft2m = magnitude ( fft2 ) fftshift = scipy . fftpack . fftshift ( fft2m ) . flatten ( ) #cmap = plt.cm.get_cmap('hot') #plt.imshow(np.log1p(scipy.fftpack.fftshift(fft2m)).T, interpolation="nearest", #plt.show() return fftshift [ : fftshift . shape [ 0 ] // 2 + 1 ]
def compute_labels ( X , rank , R , bound_idxs , niter = 300 ) : try : F , G = cnmf ( X , rank , niter = niter , hull = False ) except : return [ 1 ] label_frames = filter_activation_matrix ( G . T , R ) label_frames = np . asarray ( label_frames , dtype = int ) #labels = [label_frames[0]] labels = [ ] bound_inters = zip ( bound_idxs [ : - 1 ] , bound_idxs [ 1 : ] ) for bound_inter in bound_inters : if bound_inter [ 1 ] - bound_inter [ 0 ] <= 0 : labels . append ( np . max ( label_frames ) + 1 ) else : labels . append ( most_frequent ( label_frames [ bound_inter [ 0 ] : bound_inter [ 1 ] ] ) ) #print bound_inter, labels[-1] #labels.append(label_frames[-1]) return labels
def filter_activation_matrix ( G , R ) : #import pylab as plt #plt.imshow(G, interpolation="nearest", aspect="auto") #plt.show() idx = np . argmax ( G , axis = 1 ) max_idx = np . arange ( G . shape [ 0 ] ) max_idx = ( max_idx , idx . flatten ( ) ) G [ : , : ] = 0 G [ max_idx ] = idx + 1 G = np . sum ( G , axis = 1 ) G = median_filter ( G [ : , np . newaxis ] , R ) return G . flatten ( )
def main ( ) : args = command_line ( ) translate = partial ( translator , args . source , args . dest , version = ' ' . join ( [ __version__ , __build__ ] ) ) return source ( spool ( set_task ( translate , translit = args . translit ) ) , args . text )
def print_table ( language ) : table = translation_table ( language ) for code , name in sorted ( table . items ( ) , key = operator . itemgetter ( 0 ) ) : print ( u'{language:<8} {name:\u3000<20}' . format ( name = name , language = code ) ) return None
def disable ( self ) : if not self . active : return None self . mock_engine . disable ( ) self . active = False
def get_setting ( connection , key ) : if key in connection . settings_dict : return connection . settings_dict [ key ] else : return getattr ( settings , key )
def as_sql ( self , compiler , connection ) : sql , params = super ( DecryptedCol , self ) . as_sql ( compiler , connection ) sql = self . target . get_decrypt_sql ( connection ) % ( sql , self . target . get_cast_sql ( ) ) return sql , params
def pre_save ( self , model_instance , add ) : if self . original : original_value = getattr ( model_instance , self . original ) setattr ( model_instance , self . attname , original_value ) return super ( HashMixin , self ) . pre_save ( model_instance , add )
def get_col ( self , alias , output_field = None ) : if output_field is None : output_field = self if alias != self . model . _meta . db_table or output_field != self : return DecryptedCol ( alias , self , output_field ) else : return self . cached_col
def get_placeholder ( self , value = None , compiler = None , connection = None ) : return self . encrypt_sql . format ( get_setting ( connection , 'PUBLIC_PGP_KEY' ) )
def attach_to_tree ( self ) : for clade in self . tree . find_clades ( ) : if clade . up is not None : clade . branch_length_interpolator . merger_cost = self . cost
def optimize_Tc ( self ) : from scipy . optimize import minimize_scalar initial_Tc = self . Tc def cost ( Tc ) : self . set_Tc ( Tc ) return - self . total_LH ( ) sol = minimize_scalar ( cost , bounds = [ ttconf . TINY_NUMBER , 10.0 ] ) if "success" in sol and sol [ "success" ] : self . set_Tc ( sol [ 'x' ] ) else : self . logger ( "merger_models:optimze_Tc: optimization of coalescent time scale failed: " + str ( sol ) , 0 , warn = True ) self . set_Tc ( initial_Tc . y , T = initial_Tc . x )
def _prepare_nodes ( self ) : self . tree . root . up = None self . tree . root . bad_branch = self . tree . root . bad_branch if hasattr ( self . tree . root , 'bad_branch' ) else False internal_node_count = 0 for clade in self . tree . get_nonterminals ( order = 'preorder' ) : internal_node_count += 1 if clade . name is None : clade . name = "NODE_" + format ( self . _internal_node_count , '07d' ) self . _internal_node_count += 1 for c in clade . clades : if c . is_terminal ( ) : c . bad_branch = c . bad_branch if hasattr ( c , 'bad_branch' ) else False c . up = clade for clade in self . tree . get_nonterminals ( order = 'postorder' ) : clade . bad_branch = all ( [ c . bad_branch for c in clade ] ) self . _calc_dist2root ( ) self . _internal_node_count = max ( internal_node_count , self . _internal_node_count )
def create_gtr ( params ) : model = params . gtr gtr_params = params . gtr_params if model == 'infer' : gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) else : try : kwargs = { } if gtr_params is not None : for param in gtr_params : keyval = param . split ( '=' ) if len ( keyval ) != 2 : continue if keyval [ 0 ] in [ 'pis' , 'pi' , 'Pi' , 'Pis' ] : keyval [ 0 ] = 'pi' keyval [ 1 ] = list ( map ( float , keyval [ 1 ] . split ( ',' ) ) ) elif keyval [ 0 ] not in [ 'alphabet' ] : keyval [ 1 ] = float ( keyval [ 1 ] ) kwargs [ keyval [ 0 ] ] = keyval [ 1 ] else : print ( "GTR params are not specified. Creating GTR model with default parameters" ) gtr = GTR . standard ( model , * * kwargs ) infer_gtr = False except : print ( "Could not create GTR model from input arguments. Using default (Jukes-Cantor 1969)" ) gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) infer_gtr = False return gtr
def read_if_vcf ( params ) : ref = None aln = params . aln fixed_pi = None if hasattr ( params , 'aln' ) and params . aln is not None : if any ( [ params . aln . lower ( ) . endswith ( x ) for x in [ '.vcf' , '.vcf.gz' ] ] ) : if not params . vcf_reference : print ( "ERROR: a reference Fasta is required with VCF-format alignments" ) return - 1 compress_seq = read_vcf ( params . aln , params . vcf_reference ) sequences = compress_seq [ 'sequences' ] ref = compress_seq [ 'reference' ] aln = sequences if not hasattr ( params , 'gtr' ) or params . gtr == "infer" : #if not specified, set it: alpha = alphabets [ 'aa' ] if params . aa else alphabets [ 'nuc' ] fixed_pi = [ ref . count ( base ) / len ( ref ) for base in alpha ] if fixed_pi [ - 1 ] == 0 : fixed_pi [ - 1 ] = 0.05 fixed_pi = [ v - 0.01 for v in fixed_pi ] return aln , ref , fixed_pi
def delta_function ( cls , x_pos , weight = 1. , min_width = MIN_INTEGRATION_PEAK ) : distribution = cls ( x_pos , 0. , is_log = True , min_width = min_width ) distribution . weight = weight return distribution
def multiply ( dists ) : if not all ( [ isinstance ( k , Distribution ) for k in dists ] ) : raise NotImplementedError ( "Can only multiply Distribution objects" ) n_delta = np . sum ( [ k . is_delta for k in dists ] ) min_width = np . max ( [ k . min_width for k in dists ] ) if n_delta > 1 : raise ArithmeticError ( "Cannot multiply more than one delta functions!" ) elif n_delta == 1 : delta_dist_ii = np . where ( [ k . is_delta for k in dists ] ) [ 0 ] [ 0 ] delta_dist = dists [ delta_dist_ii ] new_xpos = delta_dist . peak_pos new_weight = np . prod ( [ k . prob ( new_xpos ) for k in dists if k != delta_dist_ii ] ) * delta_dist . weight res = Distribution . delta_function ( new_xpos , weight = new_weight , min_width = min_width ) else : new_xmin = np . max ( [ k . xmin for k in dists ] ) new_xmax = np . min ( [ k . xmax for k in dists ] ) x_vals = np . unique ( np . concatenate ( [ k . x for k in dists ] ) ) x_vals = x_vals [ ( x_vals > new_xmin - TINY_NUMBER ) & ( x_vals < new_xmax + TINY_NUMBER ) ] y_vals = np . sum ( [ k . __call__ ( x_vals ) for k in dists ] , axis = 0 ) peak = y_vals . min ( ) ind = ( y_vals - peak ) < BIG_NUMBER / 1000 n_points = ind . sum ( ) if n_points == 0 : print ( "ERROR in distribution multiplication: Distributions do not overlap" ) x_vals = [ 0 , 1 ] y_vals = [ BIG_NUMBER , BIG_NUMBER ] res = Distribution ( x_vals , y_vals , is_log = True , min_width = min_width , kind = 'linear' ) elif n_points == 1 : res = Distribution . delta_function ( x_vals [ 0 ] ) else : res = Distribution ( x_vals [ ind ] , y_vals [ ind ] , is_log = True , min_width = min_width , kind = 'linear' , assume_sorted = True ) return res
def timetree_likelihood ( self ) : LH = 0 for node in self . tree . find_clades ( order = 'preorder' ) : if node . up is None : continue LH -= node . branch_length_interpolator ( node . branch_length ) if self . aln : LH += self . gtr . sequence_logLH ( self . tree . root . cseq , pattern_multiplicity = self . multiplicity ) return LH
def min_interp ( interp_object ) : try : return interp_object . x [ interp_object ( interp_object . x ) . argmin ( ) ] except Exception as e : s = "Cannot find minimum of the interpolation object" + str ( interp_object . x ) + "Minimal x: " + str ( interp_object . x . min ( ) ) + "Maximal x: " + str ( interp_object . x . max ( ) ) raise e
def median_interp ( interp_object ) : new_grid = np . sort ( np . concatenate ( [ interp_object . x [ : - 1 ] + 0.1 * ii * np . diff ( interp_object . x ) for ii in range ( 10 ) ] ) . flatten ( ) ) tmp_prop = np . exp ( - ( interp_object ( new_grid ) - interp_object . y . min ( ) ) ) tmp_cumsum = np . cumsum ( 0.5 * ( tmp_prop [ 1 : ] + tmp_prop [ : - 1 ] ) * np . diff ( new_grid ) ) median_index = min ( len ( tmp_cumsum ) - 3 , max ( 2 , np . searchsorted ( tmp_cumsum , tmp_cumsum [ - 1 ] * 0.5 ) + 1 ) ) return new_grid [ median_index ]
def close ( self ) : self . client . close ( ) self . _client = None self . connected = False self . logger . debug ( 'Connection closed.' )
def receive ( self ) : start = 0 while True : idx = self . _buffer . find ( INST_TERM . encode ( ) , start ) if idx != - 1 : line = self . _buffer [ : idx + 1 ] . decode ( ) self . _buffer = self . _buffer [ idx + 1 : ] self . logger . debug ( 'Received instruction: %s' % line ) return line else : start = len ( self . _buffer ) buf = self . client . recv ( BUF_LEN ) if not buf : self . close ( ) self . logger . debug ( 'Failed to receive instruction. Closing.' ) return None self . _buffer . extend ( buf )
def send ( self , data ) : self . logger . debug ( 'Sending data: %s' % data ) self . client . sendall ( data . encode ( ) )
def send_instruction ( self , instruction ) : self . logger . debug ( 'Sending instruction: %s' % str ( instruction ) ) return self . send ( instruction . encode ( ) )
def handshake ( self , protocol = 'vnc' , width = 1024 , height = 768 , dpi = 96 , audio = None , video = None , image = None , * * kwargs ) : if protocol not in PROTOCOLS : self . logger . debug ( 'Invalid protocol: %s' % protocol ) raise GuacamoleError ( 'Cannot start Handshake. Missing protocol.' ) if audio is None : audio = list ( ) if video is None : video = list ( ) if image is None : image = list ( ) self . logger . debug ( 'Send `select` instruction.' ) self . send_instruction ( Instruction ( 'select' , protocol ) ) instruction = self . read_instruction ( ) self . logger . debug ( 'Expecting `args` instruction, received: %s' % str ( instruction ) ) if not instruction : self . close ( ) raise GuacamoleError ( 'Cannot establish Handshake. Connection Lost!' ) if instruction . opcode != 'args' : self . close ( ) raise GuacamoleError ( 'Cannot establish Handshake. Expected opcode `args`, ' 'received `%s` instead.' % instruction . opcode ) self . logger . debug ( 'Send `size` instruction (%s, %s, %s)' % ( width , height , dpi ) ) self . send_instruction ( Instruction ( 'size' , width , height , dpi ) ) self . logger . debug ( 'Send `audio` instruction (%s)' % audio ) self . send_instruction ( Instruction ( 'audio' , * audio ) ) self . logger . debug ( 'Send `video` instruction (%s)' % video ) self . send_instruction ( Instruction ( 'video' , * video ) ) self . logger . debug ( 'Send `image` instruction (%s)' % image ) self . send_instruction ( Instruction ( 'image' , * image ) ) connection_args = [ kwargs . get ( arg . replace ( '-' , '_' ) , '' ) for arg in instruction . args ] self . logger . debug ( 'Send `connect` instruction (%s)' % connection_args ) self . send_instruction ( Instruction ( 'connect' , * connection_args ) ) instruction = self . read_instruction ( ) self . logger . debug ( 'Expecting `ready` instruction, received: %s' % str ( instruction ) ) if instruction . opcode != 'ready' : self . logger . warning ( 'Expected `ready` instruction, received: %s instead' ) if instruction . args : self . _id = instruction . args [ 0 ] self . logger . debug ( 'Established connection with client id: %s' % self . id ) self . logger . debug ( 'Handshake completed.' ) self . connected = True
def class_url ( cls ) : base = 'v{0}' . format ( getattr ( cls , 'RESOURCE_VERSION' , '1' ) ) return "/{0}/{1}" . format ( base , class_to_api_name ( cls . class_name ( ) ) )
def instance_url ( self ) : id_ = self . get ( self . ID_ATTR ) base = self . class_url ( ) if id_ : return '/' . join ( [ base , six . text_type ( id_ ) ] ) else : raise Exception ( 'Could not determine which URL to request: %s instance ' 'has invalid ID: %r' % ( type ( self ) . __name__ , id_ ) , self . ID_ATTR )
def parent_object ( self ) : from . import types parent_klass = types . get ( self . parent_job_model . split ( '.' ) [ 1 ] ) return parent_klass . retrieve ( self . parent_job_id , client = self . _client )
def _ask_for_credentials ( ) : _print_msg ( 'Please enter your SolveBio credentials' ) domain = raw_input ( 'Domain (e.g. <domain>.solvebio.com): ' ) try : account = client . request ( 'get' , '/p/accounts/{}' . format ( domain ) ) auth = account [ 'authentication' ] except : raise SolveError ( 'Invalid domain: {}' . format ( domain ) ) if auth . get ( 'login' ) or auth . get ( 'SAML' , { } ) . get ( 'simple_login' ) : email = raw_input ( 'Email: ' ) password = getpass . getpass ( 'Password (typing will be hidden): ' ) return ( domain , email , password ) else : _print_msg ( 'Your domain uses Single Sign-On (SSO). ' 'Please visit https://{}.solvebio.com/settings/security ' 'for instructions on how to log in.' . format ( domain ) ) sys . exit ( 1 )
def print_user ( user ) : email = user [ 'email' ] domain = user [ 'account' ] [ 'domain' ] role = user [ 'role' ] print ( 'You are logged-in to the "{0}" domain ' 'as {1} with role {2}.' . format ( domain , email , role ) )
def range ( self , chromosome , start , stop , exact = False ) : return self . _clone ( filters = [ GenomicFilter ( chromosome , start , stop , exact ) ] )
def position ( self , chromosome , position , exact = False ) : return self . _clone ( filters = [ GenomicFilter ( chromosome , position , exact = exact ) ] )
def main ( argv = sys . argv [ 1 : ] ) : parser = SolveArgumentParser ( ) args = parser . parse_solvebio_args ( argv ) if args . api_host : solvebio . api_host = args . api_host if args . api_key : solvebio . api_key = args . api_key if not solvebio . api_key : try : from . credentials import get_credentials solvebio . api_key = get_credentials ( ) except : pass client . set_host ( ) client . set_token ( ) return args . func ( args )
def construct_from ( cls , values , * * kwargs ) : instance = cls ( values . get ( cls . ID_ATTR ) , * * kwargs ) instance . refresh_from ( values ) return instance
def logout ( self ) : if self . _oauth_client_secret : try : oauth_token = flask . request . cookies [ self . TOKEN_COOKIE_NAME ] requests . post ( urljoin ( self . _api_host , self . OAUTH2_REVOKE_TOKEN_PATH ) , data = { 'client_id' : self . _oauth_client_id , 'client_secret' : self . _oauth_client_secret , 'token' : oauth_token } ) except : pass response = flask . redirect ( '/' ) self . clear_cookies ( response ) return response
def child_object ( self ) : from . import types child_klass = types . get ( self . task_type . split ( '.' ) [ 1 ] ) return child_klass . retrieve ( self . task_id , client = self . _client )
def row_to_dict ( self , row , allele , alternate_alleles ) : def _variant_sbid ( * * kwargs ) : """Generates a SolveBio variant ID (SBID).""" return '{build}-{chromosome}-{start}-{stop}-{allele}' . format ( * * kwargs ) . upper ( ) if allele == '.' : allele = row . REF or allele genomic_coordinates = { 'build' : self . genome_build , 'chromosome' : row . CHROM , 'start' : row . POS , 'stop' : row . POS + len ( row . REF ) - 1 } variant_sbid = _variant_sbid ( allele = allele , * * genomic_coordinates ) return { 'genomic_coordinates' : genomic_coordinates , 'variant' : variant_sbid , 'allele' : allele , 'row_id' : row . ID , 'reference_allele' : row . REF , 'alternate_alleles' : alternate_alleles , 'info' : self . _parse_info ( row . INFO ) , 'qual' : row . QUAL , 'filter' : row . FILTER }
def save ( self , path ) : rep = "" for host in self . hosts . keys ( ) : attrs = self . hosts [ host ] rep = rep + "machine " + host + "\n\tlogin " + six . text_type ( attrs [ 0 ] ) + "\n" if attrs [ 1 ] : rep = rep + "account " + six . text_type ( attrs [ 1 ] ) rep = rep + "\tpassword " + six . text_type ( attrs [ 2 ] ) + "\n" for macro in self . macros . keys ( ) : rep = rep + "macdef " + macro + "\n" for line in self . macros [ macro ] : rep = rep + line rep = rep + "\n" f = open ( path , 'w' ) f . write ( rep ) f . close ( )
def _build_row ( cells , padding , begin , sep , end ) : pad = " " * padding padded_cells = [ pad + cell + pad for cell in cells ] rendered_cells = ( begin + sep . join ( padded_cells ) + end ) . rstrip ( ) if len ( rendered_cells ) > TTY_COLS : if not cells [ - 1 ] . endswith ( " " ) and not cells [ - 1 ] . endswith ( "-" ) : terminating_str = " ... " else : terminating_str = "" rendered_cells = "{0}{1}{2}" . format ( rendered_cells [ : TTY_COLS - len ( terminating_str ) - 1 ] , terminating_str , end ) return rendered_cells
def _build_line ( colwidths , padding , begin , fill , sep , end ) : cells = [ fill * ( w + 2 * padding ) for w in colwidths ] return _build_row ( cells , 0 , begin , sep , end )
def _mediawiki_cell_attrs ( row , colaligns ) : alignment = { "left" : '' , "right" : 'align="right"| ' , "center" : 'align="center"| ' , "decimal" : 'align="right"| ' } row2 = [ alignment [ a ] + c for c , a in zip ( row , colaligns ) ] return row2
def _format_table ( fmt , headers , rows , colwidths , colaligns ) : lines = [ ] hidden = fmt . with_header_hide if headers else fmt . without_header_hide pad = fmt . padding headerrow = fmt . headerrow if fmt . headerrow else fmt . datarow if fmt . lineabove and "lineabove" not in hidden : lines . append ( _build_line ( colwidths , pad , * fmt . lineabove ) ) if headers : lines . append ( _build_row ( headers , pad , * headerrow ) ) if fmt . linebelowheader and "linebelowheader" not in hidden : begin , fill , sep , end = fmt . linebelowheader if fmt . usecolons : segs = [ _line_segment_with_colons ( fmt . linebelowheader , a , w + 2 * pad ) for w , a in zip ( colwidths , colaligns ) ] lines . append ( _build_row ( segs , 0 , begin , sep , end ) ) else : lines . append ( _build_line ( colwidths , pad , * fmt . linebelowheader ) ) if rows and fmt . linebetweenrows and "linebetweenrows" not in hidden : for row in rows [ : - 1 ] : lines . append ( _build_row ( row , pad , * fmt . datarow ) ) lines . append ( _build_line ( colwidths , pad , * fmt . linebetweenrows ) ) lines . append ( _build_row ( rows [ - 1 ] , pad , * fmt . datarow ) ) else : for row in rows : lines . append ( _build_row ( row , pad , * fmt . datarow ) ) if fmt . linebelow and "linebelow" not in hidden : lines . append ( _build_line ( colwidths , pad , * fmt . linebelow ) ) return "\n" . join ( lines )
def evaluate ( self , data = None , data_type = 'string' , is_list = False ) : payload = { 'data' : data , 'expression' : self . expr , 'data_type' : data_type , 'is_list' : is_list } res = self . _client . post ( '/v1/evaluate' , payload ) return res [ 'result' ]
def _get_column_types ( self , data ) : columns = list ( zip_longest ( * data ) ) return [ self . _get_column_type ( column ) for column in columns ]
def _get_column_type ( self , column ) : type_values = [ TYPES [ self . _get_type ( v ) ] for v in column ] inverse_types = { v : k for k , v in TYPES . items ( ) } return inverse_types [ max ( type_values ) ]
def _get_type ( self , value ) : if value is None : return type ( None ) elif type ( value ) in int_types : return int elif type ( value ) in float_types : return float elif isinstance ( value , binary_type ) : return binary_type else : return text_type
def adapter ( data , headers , table_format = None , preserve_whitespace = False , * * kwargs ) : keys = ( 'floatfmt' , 'numalign' , 'stralign' , 'showindex' , 'disable_numparse' ) tkwargs = { 'tablefmt' : table_format } tkwargs . update ( filter_dict_by_key ( kwargs , keys ) ) if table_format in supported_markup_formats : tkwargs . update ( numalign = None , stralign = None ) tabulate . PRESERVE_WHITESPACE = preserve_whitespace return iter ( tabulate . tabulate ( data , headers , * * tkwargs ) . split ( '\n' ) )
def user_config_file ( self ) : return os . path . join ( get_user_config_dir ( self . app_name , self . app_author ) , self . filename )
def system_config_files ( self ) : return [ os . path . join ( f , self . filename ) for f in get_system_config_dirs ( self . app_name , self . app_author ) ]
def additional_files ( self ) : return [ os . path . join ( f , self . filename ) for f in self . additional_dirs ]
def truncate_string ( value , max_width = None ) : if isinstance ( value , text_type ) and max_width is not None and len ( value ) > max_width : return value [ : max_width ] return value
def filter_dict_by_key ( d , keys ) : return { k : v for k , v in d . items ( ) if k in keys }
def replace ( s , replace ) : for r in replace : s = s . replace ( * r ) return s
def adapter ( data , headers , * * kwargs ) : for row in chain ( ( headers , ) , data ) : yield "\t" . join ( ( replace ( r , ( ( '\n' , r'\n' ) , ( '\t' , r'\t' ) ) ) for r in row ) )
def call_and_exit ( self , cmd , shell = True ) : sys . exit ( subprocess . call ( cmd , shell = shell ) )
def call_in_sequence ( self , cmds , shell = True ) : for cmd in cmds : if subprocess . call ( cmd , shell = shell ) == 1 : sys . exit ( 1 )
def apply_options ( self , cmd , options = ( ) ) : for option in ( self . default_cmd_options + options ) : cmd = self . apply_option ( cmd , option , active = getattr ( self , option , False ) ) return cmd
def apply_option ( self , cmd , option , active = True ) : return re . sub ( r'{{{}\:(?P<option>[^}}]*)}}' . format ( option ) , '\g<option>' if active else '' , cmd )
def initialize_options ( self ) : self . branch = 'master' self . fix = False super ( lint , self ) . initialize_options ( )
def run ( self ) : cmd = 'pep8radius {branch} {{fix: --in-place}}{{verbose: -vv}}' cmd = cmd . format ( branch = self . branch ) self . call_and_exit ( self . apply_options ( cmd , ( 'fix' , ) ) )
def run ( self ) : cmds = ( self . clean_docs_cmd , self . html_docs_cmd , self . view_docs_cmd ) self . call_in_sequence ( cmds )
def _get_separator ( num , sep_title , sep_character , sep_length ) : left_divider_length = right_divider_length = sep_length if isinstance ( sep_length , tuple ) : left_divider_length , right_divider_length = sep_length left_divider = sep_character * left_divider_length right_divider = sep_character * right_divider_length title = sep_title . format ( n = num + 1 ) return "{left_divider}[ {title} ]{right_divider}\n" . format ( left_divider = left_divider , right_divider = right_divider , title = title )
def _format_row ( headers , row ) : formatted_row = [ ' | ' . join ( field ) for field in zip ( headers , row ) ] return '\n' . join ( formatted_row )
def adapter ( data , headers , * * kwargs ) : keys = ( 'sep_title' , 'sep_character' , 'sep_length' ) return vertical_table ( data , headers , * * filter_dict_by_key ( kwargs , keys ) )
def adapter ( data , headers , table_format = 'csv' , * * kwargs ) : keys = ( 'dialect' , 'delimiter' , 'doublequote' , 'escapechar' , 'quotechar' , 'quoting' , 'skipinitialspace' , 'strict' ) if table_format == 'csv' : delimiter = ',' elif table_format == 'csv-tab' : delimiter = '\t' else : raise ValueError ( 'Invalid table_format specified.' ) ckwargs = { 'delimiter' : delimiter , 'lineterminator' : '' } ckwargs . update ( filter_dict_by_key ( kwargs , keys ) ) l = linewriter ( ) writer = csv . writer ( l , * * ckwargs ) writer . writerow ( headers ) yield l . line for row in data : l . reset ( ) writer . writerow ( row ) yield l . line
def adapter ( data , headers , table_format = None , * * kwargs ) : keys = ( 'title' , ) table = table_format_handler [ table_format ] t = table ( [ headers ] + list ( data ) , * * filter_dict_by_key ( kwargs , keys ) ) dimensions = terminaltables . width_and_alignment . max_dimensions ( t . table_data , t . padding_left , t . padding_right ) [ : 3 ] for r in t . gen_table ( * dimensions ) : yield u'' . join ( r )
def to_dict ( self ) : all_attributes = PyKCS11 . CKA . keys ( ) all_attributes = [ attr for attr in all_attributes if isinstance ( attr , int ) ] attributes = self . session . getAttributeValue ( self , all_attributes ) dico = dict ( ) for key , attr in zip ( all_attributes , attributes ) : if attr is None : continue if key == CKA_CLASS : dico [ PyKCS11 . CKA [ key ] ] = PyKCS11 . CKO [ attr ] elif key == CKA_CERTIFICATE_TYPE : dico [ PyKCS11 . CKA [ key ] ] = PyKCS11 . CKC [ attr ] elif key == CKA_KEY_TYPE : dico [ PyKCS11 . CKA [ key ] ] = PyKCS11 . CKK [ attr ] else : dico [ PyKCS11 . CKA [ key ] ] = attr return dico
def to_dict ( self ) : dico = dict ( ) for field in self . fields . keys ( ) : if field == "flags" : dico [ field ] = self . flags2text ( ) elif field == "state" : dico [ field ] = self . state2text ( ) else : dico [ field ] = eval ( "self." + field ) return dico
def _insert_img ( qr_img , icon_img = None , factor = 4 , icon_box = None , static_dir = None ) : img_w , img_h = qr_img . size size_w = int ( img_w ) / int ( factor ) size_h = int ( img_h ) / int ( factor ) try : icon_fp = os . path . join ( icon_img ) if static_dir : icon_fp = os . path . join ( static_dir , icon_img ) if icon_img . split ( "://" ) [ 0 ] in [ "http" , "https" , "ftp" ] : icon_fp = BytesIO ( urlopen ( icon_img ) . read ( ) ) icon = Image . open ( icon_fp ) except : return qr_img icon_w , icon_h = icon . size icon_w = size_w if icon_w > size_w else icon_w icon_h = size_h if icon_h > size_h else icon_h icon = icon . resize ( ( int ( icon_w ) , int ( icon_h ) ) , Image . ANTIALIAS ) icon = icon . convert ( "RGBA" ) left = int ( ( img_w - icon_w ) / 2 ) top = int ( ( img_h - icon_h ) / 2 ) icon_box = ( int ( icon_box [ 0 ] ) , int ( icon_box [ 1 ] ) ) if icon_box else ( left , top ) qr_img . paste ( im = icon , box = icon_box , mask = icon ) return qr_img
def _biweekly_helper ( self ) : self . num = 14 mycount = self . repeat_biweekly ( ) if mycount : if self . event . is_chunk ( ) and min ( mycount ) not in xrange ( 1 , 8 ) : mycount = _chunk_fill_out_first_week ( self . year , self . month , mycount , self . event , diff = self . event . start_end_diff , ) for k , v in mycount . items ( ) : for item in v : self . count [ k ] . append ( item )
def user ( context , user_id , update_role , add_institute , remove_admin , remove_institute ) : adapter = context . obj [ 'adapter' ] user_obj = adapter . user ( user_id ) if not user_obj : LOG . warning ( "User %s could not be found" , user_id ) context . abort ( ) existing_roles = set ( user_obj . get ( 'roles' , [ ] ) ) if update_role : if not update_role in user_obj [ 'roles' ] : existing_roles = set ( user_obj [ 'roles' ] ) existing_roles . add ( update_role ) LOG . info ( "Adding role %s to user" , update_role ) else : LOG . warning ( "User already have role %s" , update_role ) if remove_admin : try : existing_roles . remove ( 'admin' ) LOG . info ( "Removing admin rights from user %s" , user_id ) except KeyError as err : LOG . info ( "User %s does not have admin rights" , user_id ) user_obj [ 'roles' ] = list ( existing_roles ) existing_institutes = set ( user_obj . get ( 'institutes' , [ ] ) ) for institute_id in add_institute : institute_obj = adapter . institute ( institute_id ) if not institute_obj : LOG . warning ( "Institute %s could not be found" , institute_id ) else : existing_institutes . add ( institute_id ) LOG . info ( "Adding institute %s to user" , institute_id ) for institute_id in remove_institute : try : existing_institutes . remove ( institute_id ) LOG . info ( "Removing institute %s from user" , institute_id ) except KeyError as err : LOG . info ( "User does not have access to institute %s" , institute_id ) user_obj [ 'institutes' ] = list ( existing_institutes ) updated_user = adapter . update_user ( user_obj )
def variant ( institute_id , case_name , variant_id ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) log . debug ( "Variants view requesting data for variant {}" . format ( variant_id ) ) data = controllers . variant ( store , institute_obj , case_obj , variant_id = variant_id ) if data is None : log . warning ( "An error occurred: variants view requesting data for variant {}" . format ( variant_id ) ) flash ( 'An error occurred while retrieving variant object' , 'danger' ) return redirect ( request . referrer ) if current_app . config . get ( 'LOQUSDB_SETTINGS' ) : data [ 'observations' ] = controllers . observations ( store , loqusdb , case_obj , data [ 'variant' ] ) data [ 'cancer' ] = request . args . get ( 'cancer' ) == 'yes' return dict ( institute = institute_obj , case = case_obj , * * data )
def str_variants ( institute_id , case_name ) : page = int ( request . args . get ( 'page' , 1 ) ) variant_type = request . args . get ( 'variant_type' , 'clinical' ) form = StrFiltersForm ( request . args ) institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) query = form . data query [ 'variant_type' ] = variant_type variants_query = store . variants ( case_obj [ '_id' ] , category = 'str' , query = query ) data = controllers . str_variants ( store , institute_obj , case_obj , variants_query , page ) return dict ( institute = institute_obj , case = case_obj , variant_type = variant_type , form = form , page = page , * * data )
def sv_variant ( institute_id , case_name , variant_id ) : data = controllers . sv_variant ( store , institute_id , case_name , variant_id ) return data
def str_variant ( institute_id , case_name , variant_id ) : data = controllers . str_variant ( store , institute_id , case_name , variant_id ) return data
def verify ( institute_id , case_name , variant_id , variant_category , order ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) variant_obj = store . variant ( variant_id ) user_obj = store . user ( current_user . email ) comment = request . form . get ( 'verification_comment' ) try : controllers . variant_verification ( store = store , mail = mail , institute_obj = institute_obj , case_obj = case_obj , user_obj = user_obj , comment = comment , variant_obj = variant_obj , sender = current_app . config [ 'MAIL_USERNAME' ] , variant_url = request . referrer , order = order , url_builder = url_for ) except controllers . MissingVerificationRecipientError : flash ( 'No verification recipients added to institute.' , 'danger' ) return redirect ( request . referrer )
def clinvar ( institute_id , case_name , variant_id ) : data = controllers . clinvar_export ( store , institute_id , case_name , variant_id ) if request . method == 'GET' : return data else : #POST form_dict = request . form . to_dict ( ) submission_objects = set_submission_objects ( form_dict ) open_submission = store . get_open_clinvar_submission ( current_user . email , institute_id ) updated_submission = store . add_to_submission ( open_submission [ '_id' ] , submission_objects ) return redirect ( url_for ( 'cases.clinvar_submissions' , institute_id = institute_id ) )
def cancer_variants ( institute_id , case_name ) : data = controllers . cancer_variants ( store , request . args , institute_id , case_name ) return data
def variant_acmg ( institute_id , case_name , variant_id ) : if request . method == 'GET' : data = controllers . variant_acmg ( store , institute_id , case_name , variant_id ) return data else : criteria = [ ] criteria_terms = request . form . getlist ( 'criteria' ) for term in criteria_terms : criteria . append ( dict ( term = term , comment = request . form . get ( "comment-{}" . format ( term ) ) , links = [ request . form . get ( "link-{}" . format ( term ) ) ] , ) ) acmg = controllers . variant_acmg_post ( store , institute_id , case_name , variant_id , current_user . email , criteria ) flash ( "classified as: {}" . format ( acmg ) , 'info' ) return redirect ( url_for ( '.variant' , institute_id = institute_id , case_name = case_name , variant_id = variant_id ) )
def evaluation ( evaluation_id ) : evaluation_obj = store . get_evaluation ( evaluation_id ) controllers . evaluation ( store , evaluation_obj ) if request . method == 'POST' : link = url_for ( '.variant' , institute_id = evaluation_obj [ 'institute' ] [ '_id' ] , case_name = evaluation_obj [ 'case' ] [ 'display_name' ] , variant_id = evaluation_obj [ 'variant_specific' ] ) store . delete_evaluation ( evaluation_obj ) return redirect ( link ) return dict ( evaluation = evaluation_obj , institute = evaluation_obj [ 'institute' ] , case = evaluation_obj [ 'case' ] , variant = evaluation_obj [ 'variant' ] , CRITERIA = ACMG_CRITERIA )
def acmg ( ) : criteria = request . args . getlist ( 'criterion' ) classification = get_acmg ( criteria ) return jsonify ( dict ( classification = classification ) )
def upload_panel ( institute_id , case_name ) : file = form . symbol_file . data if file . filename == '' : flash ( 'No selected file' , 'warning' ) return redirect ( request . referrer ) try : stream = io . StringIO ( file . stream . read ( ) . decode ( 'utf-8' ) , newline = None ) except UnicodeDecodeError as error : flash ( "Only text files are supported!" , 'warning' ) return redirect ( request . referrer ) category = request . args . get ( 'category' ) if ( category == 'sv' ) : form = SvFiltersForm ( request . args ) else : form = FiltersForm ( request . args ) hgnc_symbols = set ( form . hgnc_symbols . data ) new_hgnc_symbols = controllers . upload_panel ( store , institute_id , case_name , stream ) hgnc_symbols . update ( new_hgnc_symbols ) form . hgnc_symbols . data = ',' . join ( hgnc_symbols ) form . gene_panels . data = '' if ( category == 'sv' ) : return redirect ( url_for ( '.sv_variants' , institute_id = institute_id , case_name = case_name , * * form . data ) , code = 307 ) else : return redirect ( url_for ( '.variants' , institute_id = institute_id , case_name = case_name , * * form . data ) , code = 307 )
def download_verified ( ) : user_obj = store . user ( current_user . email ) user_institutes = user_obj . get ( 'institutes' ) temp_excel_dir = os . path . join ( variants_bp . static_folder , 'verified_folder' ) os . makedirs ( temp_excel_dir , exist_ok = True ) written_files = controllers . verified_excel_file ( store , user_institutes , temp_excel_dir ) if written_files : today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) data = io . BytesIO ( ) with zipfile . ZipFile ( data , mode = 'w' ) as z : for f_name in pathlib . Path ( temp_excel_dir ) . iterdir ( ) : zipfile . ZipFile z . write ( f_name , os . path . basename ( f_name ) ) data . seek ( 0 ) shutil . rmtree ( temp_excel_dir ) return send_file ( data , mimetype = 'application/zip' , as_attachment = True , attachment_filename = '_' . join ( [ 'scout' , 'verified_variants' , today ] ) + '.zip' ) else : flash ( "No verified variants could be exported for user's institutes" , 'warning' ) return redirect ( request . referrer )
def add_incomplete_penetrance ( genes , alias_genes , hpo_lines ) : LOG . info ( "Add incomplete penetrance info" ) for hgnc_symbol in get_incomplete_penetrance_genes ( hpo_lines ) : for hgnc_id in get_correct_ids ( hgnc_symbol , alias_genes ) : genes [ hgnc_id ] [ 'incomplete_penetrance' ] = True
def panels ( ) : if request . method == 'POST' : csv_file = request . files [ 'csv_file' ] content = csv_file . stream . read ( ) lines = None try : if b'\n' in content : lines = content . decode ( 'utf-8' , 'ignore' ) . split ( '\n' ) else : lines = content . decode ( 'windows-1252' ) . split ( '\r' ) except Exception as err : flash ( 'Something went wrong while parsing the panel CSV file! ({})' . format ( err ) , 'danger' ) return redirect ( request . referrer ) new_panel_name = request . form . get ( 'new_panel_name' ) if new_panel_name : #create a new panel new_panel_id = controllers . new_panel ( store = store , institute_id = request . form [ 'institute' ] , panel_name = new_panel_name , display_name = request . form [ 'display_name' ] , csv_lines = lines , ) if new_panel_id is None : flash ( 'Something went wrong and the panel list was not updated!' , 'warning' ) return redirect ( request . referrer ) else : flash ( "new gene panel added, {}!" . format ( new_panel_name ) , 'success' ) return redirect ( url_for ( 'panels.panel' , panel_id = new_panel_id ) ) else : update_option = request . form [ 'modify_option' ] panel_obj = controllers . update_panel ( store = store , panel_name = request . form [ 'panel_name' ] , csv_lines = lines , option = update_option ) if panel_obj is None : return abort ( 404 , "gene panel not found: {}" . format ( request . form [ 'panel_name' ] ) ) else : return redirect ( url_for ( 'panels.panel' , panel_id = panel_obj [ '_id' ] ) ) institutes = list ( user_institutes ( store , current_user ) ) panel_names = [ name for institute in institutes for name in store . gene_panels ( institute_id = institute [ '_id' ] ) . distinct ( 'panel_name' ) ] panel_versions = { } for name in panel_names : panel_versions [ name ] = store . gene_panels ( panel_id = name ) panel_groups = [ ] for institute_obj in institutes : institute_panels = store . latest_panels ( institute_obj [ '_id' ] ) panel_groups . append ( ( institute_obj , institute_panels ) ) return dict ( panel_groups = panel_groups , panel_names = panel_names , panel_versions = panel_versions , institutes = institutes )
def panel_update ( panel_id ) : panel_obj = store . panel ( panel_id ) update_version = request . form . get ( 'version' , None ) new_panel_id = store . apply_pending ( panel_obj , update_version ) return redirect ( url_for ( 'panels.panel' , panel_id = new_panel_id ) )
def panel_export ( panel_id ) : panel_obj = store . panel ( panel_id ) data = controllers . panel_export ( store , panel_obj ) data [ 'report_created_at' ] = datetime . datetime . now ( ) . strftime ( "%Y-%m-%d" ) html_report = render_template ( 'panels/panel_pdf_simple.html' , * * data ) return render_pdf ( HTML ( string = html_report ) , download_filename = data [ 'panel' ] [ 'panel_name' ] + '_' + str ( data [ 'panel' ] [ 'version' ] ) + '_' + datetime . datetime . now ( ) . strftime ( "%Y-%m-%d" ) + '_scout.pdf' )
def gene_edit ( panel_id , hgnc_id ) : panel_obj = store . panel ( panel_id ) hgnc_gene = store . hgnc_gene ( hgnc_id ) panel_gene = controllers . existing_gene ( store , panel_obj , hgnc_id ) form = PanelGeneForm ( ) transcript_choices = [ ] for transcript in hgnc_gene [ 'transcripts' ] : if transcript . get ( 'refseq_id' ) : refseq_id = transcript . get ( 'refseq_id' ) transcript_choices . append ( ( refseq_id , refseq_id ) ) form . disease_associated_transcripts . choices = transcript_choices if form . validate_on_submit ( ) : action = 'edit' if panel_gene else 'add' info_data = form . data . copy ( ) if 'csrf_token' in info_data : del info_data [ 'csrf_token' ] store . add_pending ( panel_obj , hgnc_gene , action = action , info = info_data ) return redirect ( url_for ( '.panel' , panel_id = panel_id ) ) if panel_gene : for field_key in [ 'disease_associated_transcripts' , 'reduced_penetrance' , 'mosaicism' , 'inheritance_models' , 'database_entry_version' , 'comment' ] : form_field = getattr ( form , field_key ) if not form_field . data : panel_value = panel_gene . get ( field_key ) if panel_value is not None : form_field . process_data ( panel_value ) return dict ( panel = panel_obj , form = form , gene = hgnc_gene , panel_gene = panel_gene )
def delivery_report ( context , case_id , report_path , update ) : adapter = context . obj [ 'adapter' ] try : load_delivery_report ( adapter = adapter , case_id = case_id , report_path = report_path , update = update ) LOG . info ( "saved report to case!" ) except Exception as e : LOG . error ( e ) context . abort ( )
def whitelist ( context ) : LOG . info ( "Running scout view users" ) adapter = context . obj [ 'adapter' ] for whitelist_obj in adapter . whitelist_collection . find ( ) : click . echo ( whitelist_obj [ '_id' ] )
def gene ( store , hgnc_id ) : res = { 'builds' : { '37' : None , '38' : None } , 'symbol' : None , 'description' : None , 'ensembl_id' : None , 'record' : None } for build in res [ 'builds' ] : record = store . hgnc_gene ( hgnc_id , build = build ) if record : record [ 'position' ] = "{this[chromosome]}:{this[start]}-{this[end]}" . format ( this = record ) res [ 'aliases' ] = record [ 'aliases' ] res [ 'hgnc_id' ] = record [ 'hgnc_id' ] res [ 'description' ] = record [ 'description' ] res [ 'builds' ] [ build ] = record res [ 'symbol' ] = record [ 'hgnc_symbol' ] res [ 'description' ] = record [ 'description' ] res [ 'entrez_id' ] = record . get ( 'entrez_id' ) res [ 'pli_score' ] = record . get ( 'pli_score' ) add_gene_links ( record , int ( build ) ) res [ 'omim_id' ] = record . get ( 'omim_id' ) res [ 'incomplete_penetrance' ] = record . get ( 'incomplete_penetrance' , False ) res [ 'inheritance_models' ] = record . get ( 'inheritance_models' , [ ] ) for transcript in record [ 'transcripts' ] : transcript [ 'position' ] = ( "{this[chrom]}:{this[start]}-{this[end]}" . format ( this = transcript ) ) add_tx_links ( transcript , build ) for phenotype in record . get ( 'phenotypes' , [ ] ) : phenotype [ 'omim_link' ] = omim ( phenotype . get ( 'mim_number' ) ) if not res [ 'record' ] : res [ 'record' ] = record if not any ( res . values ( ) ) : raise ValueError return res
def genes_to_json ( store , query ) : gene_query = store . hgnc_genes ( query , search = True ) json_terms = [ { 'name' : "{} | {} ({})" . format ( gene [ 'hgnc_id' ] , gene [ 'hgnc_symbol' ] , ', ' . join ( gene [ 'aliases' ] ) ) , 'id' : gene [ 'hgnc_id' ] } for gene in gene_query ] return json_terms
def index ( ) : accessible_institutes = current_user . institutes if not 'admin' in current_user . roles : accessible_institutes = current_user . institutes if not accessible_institutes : flash ( 'Not allowed to see information - please visit the dashboard later!' ) return redirect ( url_for ( 'cases.dahboard_general.html' ) ) LOG . debug ( 'User accessible institutes: {}' . format ( accessible_institutes ) ) institutes = [ inst for inst in store . institutes ( accessible_institutes ) ] institutes . insert ( 0 , { '_id' : None , 'display_name' : 'All institutes' } ) institute_id = None slice_query = None panel = 1 if request . method == 'POST' : institute_id = request . form . get ( 'institute' ) slice_query = request . form . get ( 'query' ) panel = request . form . get ( 'pane_id' ) elif request . method == 'GET' : institute_id = request . args . get ( 'institute' ) slice_query = request . args . get ( 'query' ) #1) Their default institute when the page is first loaded #2) if they ask for an institute that they don't belong to #3) if they want perform a query on all institutes if not institute_id : institute_id = accessible_institutes [ 0 ] elif ( not current_user . is_admin ) and ( slice_query and institute_id == 'None' ) : institute_id = accessible_institutes [ 0 ] elif ( not institute_id in accessible_institutes ) and not ( institute_id == 'None' ) : institute_id = accessible_institutes [ 0 ] LOG . info ( "Fetch all cases with institute: %s" , institute_id ) data = get_dashboard_info ( store , institute_id , slice_query ) data [ 'institutes' ] = institutes data [ 'choice' ] = institute_id total_cases = data [ 'total_cases' ] LOG . info ( "Found %s cases" , total_cases ) if total_cases == 0 : flash ( 'no cases found for institute {} (with that query) - please visit the dashboard later!' . format ( institute_id ) , 'info' ) return render_template ( 'dashboard/dashboard_general.html' , institute = institute_id , query = slice_query , panel = panel , * * data )
def transcripts ( context , build , hgnc_id , json ) : LOG . info ( "Running scout view transcripts" ) adapter = context . obj [ 'adapter' ] if not json : click . echo ( "Chromosome\tstart\tend\ttranscript_id\thgnc_id\trefseq\tis_primary" ) for tx_obj in adapter . transcripts ( build = build , hgnc_id = hgnc_id ) : if json : pp ( tx_obj ) continue click . echo ( "{0}\t{1}\t{2}\t{3}\t{4}\t{5}\t{6}" . format ( tx_obj [ 'chrom' ] , tx_obj [ 'start' ] , tx_obj [ 'end' ] , tx_obj [ 'ensembl_transcript_id' ] , tx_obj [ 'hgnc_id' ] , tx_obj . get ( 'refseq_id' , '' ) , tx_obj . get ( 'is_primary' ) or '' , ) )
def variants ( store , institute_obj , case_obj , variants_query , page = 1 , per_page = 50 ) : variant_count = variants_query . count ( ) skip_count = per_page * max ( page - 1 , 0 ) more_variants = True if variant_count > ( skip_count + per_page ) else False variant_res = variants_query . skip ( skip_count ) . limit ( per_page ) genome_build = case_obj . get ( 'genome_build' , '37' ) if genome_build not in [ '37' , '38' ] : genome_build = '37' variants = [ ] for variant_obj in variant_res : overlapping_svs = [ sv for sv in store . overlapping ( variant_obj ) ] variant_obj [ 'overlapping' ] = overlapping_svs or None variants . append ( parse_variant ( store , institute_obj , case_obj , variant_obj , update = True , genome_build = genome_build ) ) return { 'variants' : variants , 'more_variants' : more_variants , }
def sv_variants ( store , institute_obj , case_obj , variants_query , page = 1 , per_page = 50 ) : skip_count = ( per_page * max ( page - 1 , 0 ) ) more_variants = True if variants_query . count ( ) > ( skip_count + per_page ) else False genome_build = case_obj . get ( 'genome_build' , '37' ) if genome_build not in [ '37' , '38' ] : genome_build = '37' return { 'variants' : ( parse_variant ( store , institute_obj , case_obj , variant , genome_build = genome_build ) for variant in variants_query . skip ( skip_count ) . limit ( per_page ) ) , 'more_variants' : more_variants , }
def str_variants ( store , institute_obj , case_obj , variants_query , page = 1 , per_page = 50 ) : return variants ( store , institute_obj , case_obj , variants_query , page , per_page )
def get_predictions ( genes ) : data = { 'sift_predictions' : [ ] , 'polyphen_predictions' : [ ] , 'region_annotations' : [ ] , 'functional_annotations' : [ ] } for gene_obj in genes : for pred_key in data : gene_key = pred_key [ : - 1 ] if len ( genes ) == 1 : value = gene_obj . get ( gene_key , '-' ) else : gene_id = gene_obj . get ( 'hgnc_symbol' ) or str ( gene_obj [ 'hgnc_id' ] ) value = ':' . join ( [ gene_id , gene_obj . get ( gene_key , '-' ) ] ) data [ pred_key ] . append ( value ) return data
def find_bai_file ( bam_file ) : bai_file = bam_file . replace ( '.bam' , '.bai' ) if not os . path . exists ( bai_file ) : bai_file = "{}.bai" . format ( bam_file ) return bai_file
def observations ( store , loqusdb , case_obj , variant_obj ) : composite_id = ( "{this[chromosome]}_{this[position]}_{this[reference]}_" "{this[alternative]}" . format ( this = variant_obj ) ) obs_data = loqusdb . get_variant ( { '_id' : composite_id } ) or { } obs_data [ 'total' ] = loqusdb . case_count ( ) obs_data [ 'cases' ] = [ ] institute_id = variant_obj [ 'institute' ] for case_id in obs_data . get ( 'families' , [ ] ) : if case_id != variant_obj [ 'case_id' ] and case_id . startswith ( institute_id ) : other_variant = store . variant ( variant_obj [ 'variant_id' ] , case_id = case_id ) other_case = store . case ( case_id ) obs_data [ 'cases' ] . append ( dict ( case = other_case , variant = other_variant ) ) return obs_data
def parse_gene ( gene_obj , build = None ) : build = build or 37 if gene_obj . get ( 'common' ) : add_gene_links ( gene_obj , build ) refseq_transcripts = [ ] for tx_obj in gene_obj [ 'transcripts' ] : parse_transcript ( gene_obj , tx_obj , build ) if not tx_obj . get ( 'refseq_id' ) : continue refseq_transcripts . append ( tx_obj ) gene_obj [ 'primary_transcripts' ] = ( refseq_transcripts if refseq_transcripts else [ ] )
def transcript_str ( transcript_obj , gene_name = None ) : if transcript_obj . get ( 'exon' ) : gene_part , part_count_raw = 'exon' , transcript_obj [ 'exon' ] elif transcript_obj . get ( 'intron' ) : gene_part , part_count_raw = 'intron' , transcript_obj [ 'intron' ] else : gene_part , part_count_raw = 'intergenic' , '0' part_count = part_count_raw . rpartition ( '/' ) [ 0 ] change_str = "{}:{}{}:{}:{}" . format ( transcript_obj . get ( 'refseq_id' , '' ) , gene_part , part_count , transcript_obj . get ( 'coding_sequence_name' , 'NA' ) , transcript_obj . get ( 'protein_sequence_name' , 'NA' ) , ) if gene_name : change_str = "{}:" . format ( gene_name ) + change_str return change_str
def end_position ( variant_obj ) : alt_bases = len ( variant_obj [ 'alternative' ] ) num_bases = max ( len ( variant_obj [ 'reference' ] ) , alt_bases ) return variant_obj [ 'position' ] + ( num_bases - 1 )
def clinsig_human ( variant_obj ) : for clinsig_obj in variant_obj [ 'clnsig' ] : if isinstance ( clinsig_obj [ 'accession' ] , int ) : link = "https://www.ncbi.nlm.nih.gov/clinvar/variation/{}" else : link = "https://www.ncbi.nlm.nih.gov/clinvar/{}" human_str = 'not provided' if clinsig_obj . get ( 'value' ) : try : int ( clinsig_obj [ 'value' ] ) human_str = CLINSIG_MAP . get ( clinsig_obj [ 'value' ] , 'not provided' ) except ValueError : human_str = clinsig_obj [ 'value' ] clinsig_obj [ 'human' ] = human_str clinsig_obj [ 'link' ] = link . format ( clinsig_obj [ 'accession' ] ) yield clinsig_obj
def thousandg_link ( variant_obj , build = None ) : dbsnp_id = variant_obj . get ( 'dbsnp_id' ) build = build or 37 if not dbsnp_id : return None if build == 37 : url_template = ( "http://grch37.ensembl.org/Homo_sapiens/Variation/Explore" "?v={};vdb=variation" ) else : url_template = ( "http://www.ensembl.org/Homo_sapiens/Variation/Explore" "?v={};vdb=variation" ) return url_template . format ( dbsnp_id )
def beacon_link ( variant_obj , build = None ) : build = build or 37 url_template = ( "https://beacon-network.org/#/search?pos={this[position]}&" "chrom={this[chromosome]}&allele={this[alternative]}&" "ref={this[reference]}&rs=GRCh37" ) return url_template . format ( this = variant_obj )
def ucsc_link ( variant_obj , build = None ) : build = build or 37 url_template = ( "http://genome.ucsc.edu/cgi-bin/hgTracks?db=hg19&" "position=chr{this[chromosome]}:{this[position]}" "-{this[position]}&dgv=pack&knownGene=pack&omimGene=pack" ) if build == 38 : url_template = ( "http://genome.ucsc.edu/cgi-bin/hgTracks?db=hg20&" "position=chr{this[chromosome]}:{this[position]}" "-{this[position]}&dgv=pack&knownGene=pack&omimGene=pack" ) return url_template . format ( this = variant_obj )
def spidex_human ( variant_obj ) : if variant_obj . get ( 'spidex' ) is None : return 'not_reported' elif abs ( variant_obj [ 'spidex' ] ) < SPIDEX_HUMAN [ 'low' ] [ 'pos' ] [ 1 ] : return 'low' elif abs ( variant_obj [ 'spidex' ] ) < SPIDEX_HUMAN [ 'medium' ] [ 'pos' ] [ 1 ] : return 'medium' else : return 'high'
def expected_inheritance ( variant_obj ) : manual_models = set ( ) for gene in variant_obj . get ( 'genes' , [ ] ) : manual_models . update ( gene . get ( 'manual_inheritance' , [ ] ) ) return list ( manual_models )
def callers ( variant_obj , category = 'snv' ) : calls = set ( ) for caller in CALLERS [ category ] : if variant_obj . get ( caller [ 'id' ] ) : calls . add ( ( caller [ 'name' ] , variant_obj [ caller [ 'id' ] ] ) ) return list ( calls )
def cancer_variants ( store , request_args , institute_id , case_name ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) form = CancerFiltersForm ( request_args ) variants_query = store . variants ( case_obj [ '_id' ] , category = 'cancer' , query = form . data ) . limit ( 50 ) data = dict ( institute = institute_obj , case = case_obj , variants = ( parse_variant ( store , institute_obj , case_obj , variant , update = True ) for variant in variants_query ) , form = form , variant_type = request_args . get ( 'variant_type' , 'clinical' ) , ) return data
def variant_acmg ( store , institute_id , case_name , variant_id ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) variant_obj = store . variant ( variant_id ) return dict ( institute = institute_obj , case = case_obj , variant = variant_obj , CRITERIA = ACMG_CRITERIA , ACMG_OPTIONS = ACMG_OPTIONS )
def variant_acmg_post ( store , institute_id , case_name , variant_id , user_email , criteria ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) variant_obj = store . variant ( variant_id ) user_obj = store . user ( user_email ) variant_link = url_for ( 'variants.variant' , institute_id = institute_id , case_name = case_name , variant_id = variant_id ) classification = store . submit_evaluation ( institute_obj = institute_obj , case_obj = case_obj , variant_obj = variant_obj , user_obj = user_obj , link = variant_link , criteria = criteria , ) return classification
def evaluation ( store , evaluation_obj ) : evaluation_obj [ 'institute' ] = store . institute ( evaluation_obj [ 'institute_id' ] ) evaluation_obj [ 'case' ] = store . case ( evaluation_obj [ 'case_id' ] ) evaluation_obj [ 'variant' ] = store . variant ( evaluation_obj [ 'variant_specific' ] ) evaluation_obj [ 'criteria' ] = { criterion [ 'term' ] : criterion for criterion in evaluation_obj [ 'criteria' ] } evaluation_obj [ 'classification' ] = ACMG_COMPLETE_MAP [ evaluation_obj [ 'classification' ] ] return evaluation_obj
def upload_panel ( store , institute_id , case_name , stream ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) raw_symbols = [ line . strip ( ) . split ( '\t' ) [ 0 ] for line in stream if line and not line . startswith ( '#' ) ] hgnc_symbols = [ ] for raw_symbol in raw_symbols : if store . hgnc_genes ( raw_symbol ) . count ( ) == 0 : flash ( "HGNC symbol not found: {}" . format ( raw_symbol ) , 'warning' ) else : hgnc_symbols . append ( raw_symbol ) return hgnc_symbols
def export_genes ( adapter , build = '37' ) : LOG . info ( "Exporting all genes to .bed format" ) for gene_obj in adapter . all_genes ( build = build ) : yield gene_obj
def index ( context , collection_name ) : LOG . info ( "Running scout view index" ) adapter = context . obj [ 'adapter' ] i = 0 click . echo ( "collection\tindex" ) for collection_name in adapter . collections ( ) : for index in adapter . indexes ( collection_name ) : click . echo ( "{0}\t{1}" . format ( collection_name , index ) ) i += 1 if i == 0 : LOG . info ( "No indexes found" )
def genes ( context , build , json ) : LOG . info ( "Running scout export genes" ) adapter = context . obj [ 'adapter' ] result = adapter . all_genes ( build = build ) if json : click . echo ( dumps ( result ) ) return gene_string = ( "{0}\t{1}\t{2}\t{3}\t{4}" ) click . echo ( "#Chromosom\tStart\tEnd\tHgnc_id\tHgnc_symbol" ) for gene_obj in result : click . echo ( gene_string . format ( gene_obj [ 'chromosome' ] , gene_obj [ 'start' ] , gene_obj [ 'end' ] , gene_obj [ 'hgnc_id' ] , gene_obj [ 'hgnc_symbol' ] , ) )
def case ( institute_id , case_name ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) if case_obj is None : return abort ( 404 ) return Response ( json_util . dumps ( case_obj ) , mimetype = 'application/json' )
def variant ( institute_id , case_name , variant_id ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) variant_obj = store . variant ( variant_id ) return Response ( json_util . dumps ( variant_obj ) , mimetype = 'application/json' )
def collections ( context ) : LOG . info ( "Running scout view collections" ) adapter = context . obj [ 'adapter' ] for collection_name in adapter . collections ( ) : click . echo ( collection_name )
def institute ( ctx , internal_id , display_name , sanger_recipients ) : adapter = ctx . obj [ 'adapter' ] if not internal_id : logger . warning ( "A institute has to have an internal id" ) ctx . abort ( ) if not display_name : display_name = internal_id if sanger_recipients : sanger_recipients = list ( sanger_recipients ) try : load_institute ( adapter = adapter , internal_id = internal_id , display_name = display_name , sanger_recipients = sanger_recipients ) except Exception as e : logger . warning ( e ) ctx . abort ( )
def get_file_handle ( file_path ) : if file_path . endswith ( '.gz' ) : file_handle = getreader ( 'utf-8' ) ( gzip . open ( file_path , 'r' ) , errors = 'replace' ) else : file_handle = open ( file_path , 'r' , encoding = 'utf-8' ) return file_handle
def get_net ( req ) : try : nxt , prev = map ( int , ( req . GET . get ( 'cal_next' , 0 ) , req . GET . get ( 'cal_prev' , 0 ) ) ) net = nxt - prev except Exception : net = 0 return net
def get_next_and_prev ( net ) : if net == 0 : nxt = prev = 1 elif net > 0 : nxt = net + 1 prev = - ( net - 1 ) else : nxt = net + 1 prev = abs ( net ) + 1 return nxt , prev
def _check_year ( year , month , error , error_msg ) : if year not in xrange ( ( now . year - 50 ) , ( now . year + 51 ) ) : year = now . year month = now . month error = error_msg return year , month , error
def add_peddy_information ( config_data ) : ped_info = { } ped_check = { } sex_check = { } relations = [ ] if config_data . get ( 'peddy_ped' ) : file_handle = open ( config_data [ 'peddy_ped' ] , 'r' ) for ind_info in parse_peddy_ped ( file_handle ) : ped_info [ ind_info [ 'sample_id' ] ] = ind_info if config_data . get ( 'peddy_ped_check' ) : file_handle = open ( config_data [ 'peddy_ped_check' ] , 'r' ) for pair_info in parse_peddy_ped_check ( file_handle ) : ped_check [ ( pair_info [ 'sample_a' ] , pair_info [ 'sample_b' ] ) ] = pair_info if config_data . get ( 'peddy_sex_check' ) : file_handle = open ( config_data [ 'peddy_sex_check' ] , 'r' ) for ind_info in parse_peddy_sex_check ( file_handle ) : sex_check [ ind_info [ 'sample_id' ] ] = ind_info if not ped_info : return analysis_inds = { } for ind in config_data [ 'samples' ] : ind_id = ind [ 'sample_id' ] analysis_inds [ ind_id ] = ind for ind_id in analysis_inds : ind = analysis_inds [ ind_id ] if ind_id in ped_info : ind [ 'predicted_ancestry' ] = ped_info [ ind_id ] . get ( 'ancestry-prediction' , 'UNKNOWN' ) if ind_id in sex_check : if sex_check [ ind_id ] [ 'error' ] : ind [ 'confirmed_sex' ] = False else : ind [ 'confirmed_sex' ] = True for parent in [ 'mother' , 'father' ] : if ind [ parent ] != '0' : for pair in ped_check : if ( ind_id in pair and ind [ parent ] in pair ) : if ped_check [ pair ] [ 'parent_error' ] : analysis_inds [ ind [ parent ] ] [ 'confirmed_parent' ] = False else : if 'confirmed_parent' not in analysis_inds [ ind [ parent ] ] : analysis_inds [ ind [ parent ] ] [ 'confirmed_parent' ] = True
def panel ( context , path , date , display_name , version , panel_type , panel_id , institute , omim , api_key , panel_app ) : adapter = context . obj [ 'adapter' ] institute = institute or 'cust000' if omim : api_key = api_key or context . obj . get ( 'omim_api_key' ) if not api_key : LOG . warning ( "Please provide a omim api key to load the omim gene panel" ) context . abort ( ) #Check if OMIM-AUTO exists if adapter . gene_panel ( panel_id = 'OMIM-AUTO' ) : LOG . warning ( "OMIM-AUTO already exists in database" ) LOG . info ( "To create a new version use scout update omim" ) return try : adapter . load_omim_panel ( api_key , institute = institute ) except Exception as err : LOG . error ( err ) context . abort ( ) if panel_app : load_panel_app ( adapter , panel_id , institute = institute ) if ( omim or panel_app ) : return if path is None : LOG . info ( "Please provide a panel" ) return try : load_panel ( path , adapter , date , display_name , version , panel_type , panel_id , institute ) except Exception as err : LOG . warning ( err ) context . abort ( )
def panel ( context , panel_id , version ) : LOG . info ( "Running scout delete panel" ) adapter = context . obj [ 'adapter' ] panel_objs = adapter . gene_panels ( panel_id = panel_id , version = version ) if panel_objs . count ( ) == 0 : LOG . info ( "No panels found" ) for panel_obj in panel_objs : adapter . delete_panel ( panel_obj )
def index ( context ) : LOG . info ( "Running scout delete index" ) adapter = context . obj [ 'adapter' ] for collection in adapter . db . collection_names ( ) : adapter . db [ collection ] . drop_indexes ( ) LOG . info ( "All indexes deleted" )
def user ( context , mail ) : LOG . info ( "Running scout delete user" ) adapter = context . obj [ 'adapter' ] user_obj = adapter . user ( mail ) if not user_obj : LOG . warning ( "User {0} could not be found in database" . format ( mail ) ) else : adapter . delete_user ( mail )
def genes ( context , build ) : LOG . info ( "Running scout delete genes" ) adapter = context . obj [ 'adapter' ] if build : LOG . info ( "Dropping genes collection for build: %s" , build ) else : LOG . info ( "Dropping genes collection" ) adapter . drop_genes ( )
def exons ( context , build ) : LOG . info ( "Running scout delete exons" ) adapter = context . obj [ 'adapter' ] adapter . drop_exons ( build )
def case ( context , institute , case_id , display_name ) : adapter = context . obj [ 'adapter' ] if not ( case_id or display_name ) : click . echo ( "Please specify what case to delete" ) context . abort ( ) if display_name : if not institute : click . echo ( "Please specify the owner of the case that should be " "deleted with flag '-i/--institute'." ) context . abort ( ) case_id = "{0}-{1}" . format ( institute , display_name ) LOG . info ( "Running deleting case {0}" . format ( case_id ) ) case = adapter . delete_case ( case_id = case_id , institute_id = institute , display_name = display_name ) if case . deleted_count == 1 : adapter . delete_variants ( case_id = case_id , variant_type = 'clinical' ) adapter . delete_variants ( case_id = case_id , variant_type = 'research' ) else : LOG . warning ( "Case does not exist in database" ) context . abort ( )
def individuals ( context , institute , causatives , case_id ) : LOG . info ( "Running scout view individuals" ) adapter = context . obj [ 'adapter' ] individuals = [ ] if case_id : case = adapter . case ( case_id = case_id ) if case : cases = [ case ] else : LOG . info ( "Could not find case %s" , case_id ) return else : cases = [ case_obj for case_obj in adapter . cases ( collaborator = institute , has_causatives = causatives ) ] if len ( cases ) == 0 : LOG . info ( "Could not find cases that match criteria" ) return individuals = ( ind_obj for case_obj in cases for ind_obj in case_obj [ 'individuals' ] ) click . echo ( "#case_id\tind_id\tdisplay_name\tsex\tphenotype\tmother\tfather" ) for case in cases : for ind_obj in case [ 'individuals' ] : ind_info = [ case [ '_id' ] , ind_obj [ 'individual_id' ] , ind_obj [ 'display_name' ] , SEX_MAP [ int ( ind_obj [ 'sex' ] ) ] , PHENOTYPE_MAP [ ind_obj [ 'phenotype' ] ] , ind_obj [ 'mother' ] , ind_obj [ 'father' ] ] click . echo ( '\t' . join ( ind_info ) )
def cases ( context , institute , display_name , case_id , nr_variants , variants_treshold ) : LOG . info ( "Running scout view institutes" ) adapter = context . obj [ 'adapter' ] models = [ ] if case_id : case_obj = adapter . case ( case_id = case_id ) if case_obj : models . append ( case_obj ) else : models = adapter . cases ( collaborator = institute , name_query = display_name ) models = [ case_obj for case_obj in models ] if not models : LOG . info ( "No cases could be found" ) return header = [ 'case_id' , 'display_name' , 'institute' ] if variants_treshold : LOG . info ( "Only show cases with more than %s variants" , variants_treshold ) nr_variants = True if nr_variants : LOG . info ( "Displaying number of variants for each case" ) header . append ( 'clinical' ) header . append ( 'research' ) click . echo ( "#" + '\t' . join ( header ) ) for model in models : output_str = "{:<12}\t{:<12}\t{:<12}" output_values = [ model [ '_id' ] , model [ 'display_name' ] , model [ 'owner' ] ] if nr_variants : output_str += "\t{:<12}\t{:<12}" nr_clinical = 0 nr_research = 0 variants = adapter . variant_collection . find ( { 'case_id' : model [ '_id' ] } ) i = 0 for i , var in enumerate ( variants , 1 ) : if var [ 'variant_type' ] == 'clinical' : nr_clinical += 1 else : nr_research += 1 output_values . extend ( [ nr_clinical , nr_research ] ) if variants_treshold and i < variants_treshold : LOG . debug ( "Case %s had to few variants, skipping" , model [ '_id' ] ) continue click . echo ( output_str . format ( * output_values ) )
def load_user ( user_email ) : user_obj = store . user ( user_email ) user_inst = LoginUser ( user_obj ) if user_obj else None return user_inst
def login ( ) : if 'next' in request . args : session [ 'next_url' ] = request . args [ 'next' ] if current_app . config . get ( 'GOOGLE' ) : callback_url = url_for ( '.authorized' , _external = True ) return google . authorize ( callback = callback_url ) user_email = request . args . get ( 'email' ) user_obj = store . user ( user_email ) if user_obj is None : flash ( "email not whitelisted: {}" . format ( user_email ) , 'warning' ) return redirect ( url_for ( 'public.index' ) ) return perform_login ( user_obj )
def user_events ( self , user_obj = None ) : query = dict ( user_id = user_obj [ '_id' ] ) if user_obj else dict ( ) return self . event_collection . find ( query )
def hpo_terms ( ) : if request . method == 'GET' : data = controllers . hpo_terms ( store = store , limit = 100 ) return data else : search_term = request . form . get ( 'hpo_term' ) limit = request . form . get ( 'limit' ) data = controllers . hpo_terms ( store = store , query = search_term , limit = limit ) return dict ( data , query = search_term , limit = limit )
def transcripts ( context , build ) : LOG . info ( "Running scout export transcripts" ) adapter = context . obj [ 'adapter' ] header = [ "#Chrom\tStart\tEnd\tTranscript\tRefSeq\tHgncID" ] for line in header : click . echo ( line ) transcript_string = ( "{0}\t{1}\t{2}\t{3}\t{4}\t{5}" ) for tx_obj in export_transcripts ( adapter ) : click . echo ( transcript_string . format ( tx_obj [ 'chrom' ] , tx_obj [ 'start' ] , tx_obj [ 'end' ] , tx_obj [ 'ensembl_transcript_id' ] , tx_obj . get ( 'refseq_id' , '' ) , tx_obj [ 'hgnc_id' ] , ) )
def exons ( context , build ) : adapter = context . obj [ 'adapter' ] start = datetime . now ( ) nr_exons = adapter . exons ( build = build ) . count ( ) if nr_exons : LOG . warning ( "Dropping all exons " ) adapter . drop_exons ( build = build ) LOG . info ( "Exons dropped" ) ensembl_exons = fetch_ensembl_exons ( build = build ) load_exons ( adapter , ensembl_exons , build ) adapter . update_indexes ( ) LOG . info ( "Time to load exons: {0}" . format ( datetime . now ( ) - start ) )
def intervals ( context , build ) : LOG . info ( "Running scout view index" ) adapter = context . obj [ 'adapter' ] intervals = adapter . get_coding_intervals ( build ) nr_intervals = 0 longest = 0 for chrom in CHROMOSOMES : for iv in intervals [ chrom ] : iv_len = iv . end - iv . begin if iv_len > longest : longest = iv_len int_nr = len ( intervals . get ( chrom , [ ] ) ) click . echo ( "{0}\t{1}" . format ( chrom , int_nr ) ) nr_intervals += int_nr LOG . info ( "Total nr intervals:%s" , nr_intervals ) LOG . info ( "Total nr genes:%s" , adapter . all_genes ( build ) . count ( ) ) LOG . info ( "Longest interval:%s" , longest )
def region ( context , hgnc_id , case_id , chromosome , start , end ) : adapter = context . obj [ 'adapter' ] load_region ( adapter = adapter , case_id = case_id , hgnc_id = hgnc_id , chrom = chromosome , start = start , end = end )
def parse_reqs ( req_path = './requirements.txt' ) : install_requires = [ ] with io . open ( os . path . join ( here , 'requirements.txt' ) , encoding = 'utf-8' ) as handle : lines = ( line . strip ( ) for line in handle if line . strip ( ) and not line . startswith ( '#' ) ) for line in lines : if line . startswith ( '-r' ) : install_requires += parse_reqs ( req_path = line [ 3 : ] ) else : install_requires . append ( line ) return install_requires
def existing_gene ( store , panel_obj , hgnc_id ) : existing_genes = { gene [ 'hgnc_id' ] : gene for gene in panel_obj [ 'genes' ] } return existing_genes . get ( hgnc_id )
def panel_export ( store , panel_obj ) : panel_obj [ 'institute' ] = store . institute ( panel_obj [ 'institute' ] ) full_name = "{}({})" . format ( panel_obj [ 'display_name' ] , panel_obj [ 'version' ] ) panel_obj [ 'name_and_version' ] = full_name return dict ( panel = panel_obj )
def archive_info ( database : Database , archive_case : dict ) -> dict : data = { 'collaborators' : archive_case [ 'collaborators' ] , 'synopsis' : archive_case . get ( 'synopsis' ) , 'assignees' : [ ] , 'suspects' : [ ] , 'causatives' : [ ] , 'phenotype_terms' : [ ] , 'phenotype_groups' : [ ] , } if archive_case . get ( 'assignee' ) : archive_user = database . user . find_one ( { '_id' : archive_case [ 'assignee' ] } ) data [ 'assignee' ] . append ( archive_user [ 'email' ] ) for key in [ 'suspects' , 'causatives' ] : for variant_id in archive_case . get ( key , [ ] ) : archive_variant = database . variant . find_one ( { '_id' : variant_id } ) data [ key ] . append ( { 'chromosome' : archive_variant [ 'chromosome' ] , 'position' : archive_variant [ 'position' ] , 'reference' : archive_variant [ 'reference' ] , 'alternative' : archive_variant [ 'alternative' ] , 'variant_type' : archive_variant [ 'variant_type' ] , } ) for key in [ 'phenotype_terms' , 'phenotype_groups' ] : for archive_term in archive_case . get ( key , [ ] ) : data [ key ] . append ( { 'phenotype_id' : archive_term [ 'phenotype_id' ] , 'feature' : archive_term [ 'feature' ] , } ) return data
def migrate_case ( adapter : MongoAdapter , scout_case : dict , archive_data : dict ) : collaborators = list ( set ( scout_case [ 'collaborators' ] + archive_data [ 'collaborators' ] ) ) if collaborators != scout_case [ 'collaborators' ] : LOG . info ( f"set collaborators: {', '.join(collaborators)}" ) scout_case [ 'collaborators' ] = collaborators if len ( scout_case . get ( 'assignees' , [ ] ) ) == 0 : scout_user = adapter . user ( archive_data [ 'assignee' ] ) if scout_user : scout_case [ 'assignees' ] = [ archive_data [ 'assignee' ] ] else : LOG . warning ( f"{archive_data['assignee']}: unable to find assigned user" ) for key in [ 'suspects' , 'causatives' ] : scout_case [ key ] = scout_case . get ( key , [ ] ) for archive_variant in archive_data [ key ] : variant_id = get_variantid ( archive_variant , scout_case [ '_id' ] ) scout_variant = adapter . variant ( variant_id ) if scout_variant : if scout_variant [ '_id' ] in scout_case [ key ] : LOG . info ( f"{scout_variant['_id']}: variant already in {key}" ) else : LOG . info ( f"{scout_variant['_id']}: add to {key}" ) scout_variant [ key ] . append ( scout_variant [ '_id' ] ) else : LOG . warning ( f"{scout_variant['_id']}: unable to find variant ({key})" ) scout_variant [ key ] . append ( variant_id ) if not scout_case . get ( 'synopsis' ) : scout_case [ 'synopsis' ] = archive_data [ 'synopsis' ] scout_case [ 'is_migrated' ] = True adapter . case_collection . find_one_and_replace ( { '_id' : scout_case [ '_id' ] } , scout_case , ) scout_institute = adapter . institute ( scout_case [ 'owner' ] ) scout_user = adapter . user ( 'mans.magnusson@scilifelab.se' ) for key in [ 'phenotype_terms' , 'phenotype_groups' ] : for archive_term in archive_data [ key ] : adapter . add_phenotype ( institute = scout_institute , case = scout_case , user = scout_user , link = f"/{scout_case['owner']}/{scout_case['display_name']}" , hpo_term = archive_term [ 'phenotype_id' ] , is_group = key == 'phenotype_groups' , )
def migrate ( uri : str , archive_uri : str , case_id : str , dry : bool , force : bool ) : scout_client = MongoClient ( uri ) scout_database = scout_client [ uri . rsplit ( '/' , 1 ) [ - 1 ] ] scout_adapter = MongoAdapter ( database = scout_database ) scout_case = scout_adapter . case ( case_id ) if not force and scout_case . get ( 'is_migrated' ) : print ( "case already migrated" ) return archive_client = MongoClient ( archive_uri ) archive_database = archive_client [ archive_uri . rsplit ( '/' , 1 ) [ - 1 ] ] archive_case = archive_database . case . find_one ( { 'owner' : scout_case [ 'owner' ] , 'display_name' : scout_case [ 'display_name' ] } ) archive_data = archive_info ( archive_database , archive_case ) if dry : print ( ruamel . yaml . safe_dump ( archive_data ) ) else : #migrate_case(scout_adapter, scout_case, archive_data) pass
def hpo ( context , term , description ) : LOG . info ( "Running scout view hpo" ) adapter = context . obj [ 'adapter' ] if term : term = term . upper ( ) if not term . startswith ( 'HP:' ) : while len ( term ) < 7 : term = '0' + term term = 'HP:' + term LOG . info ( "Searching for term %s" , term ) hpo_terms = adapter . hpo_terms ( hpo_term = term ) elif description : sorted_terms = sorted ( adapter . hpo_terms ( query = description ) , key = itemgetter ( 'hpo_number' ) ) for term in sorted_terms : term . pop ( 'genes' ) print ( "name: {} | {} | {}" . format ( term [ '_id' ] , term [ 'description' ] , term [ 'hpo_number' ] ) ) context . abort ( ) else : hpo_terms = adapter . hpo_terms ( ) if hpo_terms . count ( ) == 0 : LOG . warning ( "No matching terms found" ) return click . echo ( "hpo_id\tdescription\tnr_genes" ) for hpo_obj in hpo_terms : click . echo ( "{0}\t{1}\t{2}" . format ( hpo_obj [ 'hpo_id' ] , hpo_obj [ 'description' ] , len ( hpo_obj . get ( 'genes' , [ ] ) ) ) )
def create_app ( config_file = None , config = None ) : app = Flask ( __name__ ) app . config . from_pyfile ( 'config.py' ) app . jinja_env . add_extension ( 'jinja2.ext.do' ) if config : app . config . update ( config ) if config_file : app . config . from_pyfile ( config_file ) app . mme_nodes = mme_nodes ( app . config . get ( 'MME_URL' ) , app . config . get ( 'MME_TOKEN' ) ) app . config [ "JSON_SORT_KEYS" ] = False current_log_level = logger . getEffectiveLevel ( ) coloredlogs . install ( level = 'DEBUG' if app . debug else current_log_level ) configure_extensions ( app ) register_blueprints ( app ) register_filters ( app ) if not ( app . debug or app . testing ) and app . config . get ( 'MAIL_USERNAME' ) : configure_email_logging ( app ) @ app . before_request def check_user ( ) : if not app . config . get ( 'LOGIN_DISABLED' ) and request . endpoint : static_endpoint = 'static' in request . endpoint or 'report' in request . endpoint public_endpoint = getattr ( app . view_functions [ request . endpoint ] , 'is_public' , False ) relevant_endpoint = not ( static_endpoint or public_endpoint ) if relevant_endpoint and not current_user . is_authenticated : next_url = "{}?{}" . format ( request . path , request . query_string . decode ( ) ) login_url = url_for ( 'login.login' , next = next_url ) return redirect ( login_url ) return app
def configure_extensions ( app ) : extensions . toolbar . init_app ( app ) extensions . bootstrap . init_app ( app ) extensions . mongo . init_app ( app ) extensions . store . init_app ( app ) extensions . login_manager . init_app ( app ) extensions . oauth . init_app ( app ) extensions . mail . init_app ( app ) Markdown ( app ) if app . config . get ( 'SQLALCHEMY_DATABASE_URI' ) : configure_coverage ( app ) if app . config . get ( 'LOQUSDB_SETTINGS' ) : extensions . loqusdb . init_app ( app )
def register_blueprints ( app ) : app . register_blueprint ( public . public_bp ) app . register_blueprint ( genes . genes_bp ) app . register_blueprint ( cases . cases_bp ) app . register_blueprint ( login . login_bp ) app . register_blueprint ( variants . variants_bp ) app . register_blueprint ( panels . panels_bp ) app . register_blueprint ( dashboard . dashboard_bp ) app . register_blueprint ( api . api_bp ) app . register_blueprint ( alignviewers . alignviewers_bp ) app . register_blueprint ( phenotypes . hpo_bp ) app . register_blueprint ( institutes . overview )
def configure_coverage ( app ) : app . config [ 'SQLALCHEMY_TRACK_MODIFICATIONS' ] = True if app . debug else False if chanjo_api : chanjo_api . init_app ( app ) configure_template_filters ( app ) app . register_blueprint ( report_bp , url_prefix = '/reports' ) babel = Babel ( app ) @ babel . localeselector def get_locale ( ) : """Determine locale to use for translations.""" accept_languages = current_app . config . get ( 'ACCEPT_LANGUAGES' , [ 'en' ] ) session_language = request . args . get ( 'lang' ) if session_language in accept_languages : current_app . logger . info ( "using session language: %s" , session_language ) return session_language user_language = current_app . config . get ( 'REPORT_LANGUAGE' ) if user_language : return user_language return request . accept_languages . best_match ( accept_languages )
def aliases ( context , build , symbol ) : LOG . info ( "Running scout view aliases" ) adapter = context . obj [ 'adapter' ] if symbol : alias_genes = { } res = adapter . gene_by_alias ( symbol , build = build ) for gene_obj in res : hgnc_id = gene_obj [ 'hgnc_id' ] hgnc_symbol = gene_obj [ 'hgnc_symbol' ] for alias in gene_obj [ 'aliases' ] : true_id = None if alias == hgnc_symbol : true_id = hgnc_id if alias in alias_genes : alias_genes [ alias ] [ 'ids' ] . add ( hgnc_id ) if true_id : alias_genes [ alias ] [ 'true' ] = hgnc_id else : alias_genes [ alias ] = { 'true' : hgnc_id , 'ids' : set ( [ hgnc_id ] ) } else : alias_genes = adapter . genes_by_alias ( build = build ) if len ( alias_genes ) == 0 : LOG . info ( "No gene found for build %s" , build ) return click . echo ( "#hgnc_symbol\ttrue_id\thgnc_ids" ) for alias_symbol in alias_genes : info = alias_genes [ alias_symbol ] click . echo ( "{0}\t{1}\t{2}\t" . format ( alias_symbol , ( alias_genes [ alias_symbol ] [ 'true' ] or 'None' ) , ', ' . join ( [ str ( gene_id ) for gene_id in alias_genes [ alias_symbol ] [ 'ids' ] ] ) ) )
def variants ( context , collaborator , document_id , case_id , json ) : LOG . info ( "Running scout export variants" ) adapter = context . obj [ 'adapter' ] collaborator = collaborator or 'cust000' variants = export_variants ( adapter , collaborator , document_id = document_id , case_id = case_id ) if json : click . echo ( dumps ( [ var for var in variants ] ) ) return vcf_header = VCF_HEADER #If case_id is given, print more complete vcf entries, with INFO, #and genotypes if case_id : vcf_header [ - 1 ] = vcf_header [ - 1 ] + "\tFORMAT" case_obj = adapter . case ( case_id = case_id ) for individual in case_obj [ 'individuals' ] : vcf_header [ - 1 ] = vcf_header [ - 1 ] + "\t" + individual [ 'individual_id' ] #print header for line in vcf_header : click . echo ( line ) for variant_obj in variants : variant_string = get_vcf_entry ( variant_obj , case_id = case_id ) click . echo ( variant_string )
def serve ( context , config , host , port , debug , livereload ) : pymongo_config = dict ( MONGO_HOST = context . obj [ 'host' ] , MONGO_PORT = context . obj [ 'port' ] , MONGO_DBNAME = context . obj [ 'mongodb' ] , MONGO_USERNAME = context . obj [ 'username' ] , MONGO_PASSWORD = context . obj [ 'password' ] , ) valid_connection = check_connection ( host = pymongo_config [ 'MONGO_HOST' ] , port = pymongo_config [ 'MONGO_PORT' ] , username = pymongo_config [ 'MONGO_USERNAME' ] , password = pymongo_config [ 'MONGO_PASSWORD' ] , authdb = context . obj [ 'authdb' ] , ) log . info ( "Test if mongod is running" ) if not valid_connection : log . warning ( "Connection could not be established" ) log . info ( "Is mongod running?" ) context . abort ( ) config = os . path . abspath ( config ) if config else None app = create_app ( config = pymongo_config , config_file = config ) if livereload : server = Server ( app . wsgi_app ) server . serve ( host = host , port = port , debug = debug ) else : app . run ( host = host , port = port , debug = debug )
def init_app ( self , app ) : host = app . config . get ( 'MONGO_HOST' , 'localhost' ) port = app . config . get ( 'MONGO_PORT' , 27017 ) dbname = app . config [ 'MONGO_DBNAME' ] log . info ( "connecting to database: %s:%s/%s" , host , port , dbname ) self . setup ( app . config [ 'MONGO_DATABASE' ] )
def setup ( self , database ) : self . db = database self . hgnc_collection = database . hgnc_gene self . user_collection = database . user self . whitelist_collection = database . whitelist self . institute_collection = database . institute self . event_collection = database . event self . case_collection = database . case self . panel_collection = database . gene_panel self . hpo_term_collection = database . hpo_term self . disease_term_collection = database . disease_term self . variant_collection = database . variant self . acmg_collection = database . acmg self . clinvar_collection = database . clinvar self . clinvar_submission_collection = database . clinvar_submission self . exon_collection = database . exon self . transcript_collection = database . transcript
def index ( context , update ) : LOG . info ( "Running scout index" ) adapter = context . obj [ 'adapter' ] if update : adapter . update_indexes ( ) else : adapter . load_indexes ( )
def database ( context , institute_name , user_name , user_mail , api_key ) : LOG . info ( "Running scout setup database" ) api_key = api_key or context . obj . get ( 'omim_api_key' ) if not api_key : LOG . warning ( "Please provide a omim api key with --api-key" ) context . abort ( ) institute_name = institute_name or context . obj [ 'institute_name' ] user_name = user_name or context . obj [ 'user_name' ] user_mail = user_mail or context . obj [ 'user_mail' ] adapter = context . obj [ 'adapter' ] LOG . info ( "Setting up database %s" , context . obj [ 'mongodb' ] ) setup_scout ( adapter = adapter , institute_id = institute_name , user_name = user_name , user_mail = user_mail , api_key = api_key )
def setup ( context , institute , user_mail , user_name ) : context . obj [ 'institute_name' ] = institute context . obj [ 'user_name' ] = user_name context . obj [ 'user_mail' ] = user_mail if context . invoked_subcommand == 'demo' : LOG . debug ( "Change database name to scout-demo" ) context . obj [ 'mongodb' ] = 'scout-demo' LOG . info ( "Setting database name to %s" , context . obj [ 'mongodb' ] ) LOG . debug ( "Setting host to %s" , context . obj [ 'host' ] ) LOG . debug ( "Setting port to %s" , context . obj [ 'port' ] ) try : client = get_connection ( host = context . obj [ 'host' ] , port = context . obj [ 'port' ] , username = context . obj [ 'username' ] , password = context . obj [ 'password' ] , mongodb = context . obj [ 'mongodb' ] ) except ConnectionFailure : context . abort ( ) LOG . info ( "connecting to database %s" , context . obj [ 'mongodb' ] ) database = client [ context . obj [ 'mongodb' ] ] LOG . info ( "Test if mongod is running" ) try : LOG . info ( "Test if mongod is running" ) database . test . find_one ( ) except ServerSelectionTimeoutError as err : LOG . warning ( "Connection could not be established" ) LOG . warning ( "Please check if mongod is running" ) context . abort ( ) LOG . info ( "Setting up a mongo adapter" ) mongo_adapter = MongoAdapter ( database ) context . obj [ 'adapter' ] = mongo_adapter
def institutes ( context , institute_id , json ) : LOG . info ( "Running scout view institutes" ) adapter = context . obj [ 'adapter' ] if institute_id : institute_objs = [ ] institute_obj = adapter . institute ( institute_id ) if not institute_obj : LOG . info ( "Institute %s does not exost" , institute_id ) return institute_objs . append ( institute_obj ) else : institute_objs = [ ins_obj for ins_obj in adapter . institutes ( ) ] if len ( institute_objs ) == 0 : click . echo ( "No institutes found" ) context . abort ( ) header = '' if not json : for key in institute_objs [ 0 ] . keys ( ) : header = header + "{0}\t" . format ( key ) click . echo ( header ) for institute_obj in institute_objs : if json : click . echo ( institute_obj ) continue row = '' for value in institute_obj . values ( ) : row = row + "{0}\t" . format ( value ) click . echo ( row )
def panels ( context , institute ) : LOG . info ( "Running scout view panels" ) adapter = context . obj [ 'adapter' ] panel_objs = adapter . gene_panels ( institute_id = institute ) if panel_objs . count ( ) == 0 : LOG . info ( "No panels found" ) context . abort ( ) click . echo ( "#panel_name\tversion\tnr_genes\tdate" ) for panel_obj in panel_objs : click . echo ( "{0}\t{1}\t{2}\t{3}" . format ( panel_obj [ 'panel_name' ] , str ( panel_obj [ 'version' ] ) , len ( panel_obj [ 'genes' ] ) , str ( panel_obj [ 'date' ] . strftime ( '%Y-%m-%d' ) ) ) )
def hpo_genes ( context , hpo_term ) : LOG . info ( "Running scout export hpo_genes" ) adapter = context . obj [ 'adapter' ] header = [ "#Gene_id\tCount" ] if not hpo_term : LOG . warning ( "Please use at least one hpo term" ) context . abort ( ) for line in header : click . echo ( line ) for term in adapter . generate_hpo_gene_list ( * hpo_term ) : click . echo ( "{0}\t{1}" . format ( term [ 0 ] , term [ 1 ] ) )
def user ( context , institute_id , user_name , user_mail , admin ) : adapter = context . obj [ 'adapter' ] institutes = [ ] for institute in institute_id : institute_obj = adapter . institute ( institute_id = institute ) if not institute_obj : LOG . warning ( "Institute % does not exist" , institute ) context . abort ( ) institutes . append ( institute ) roles = [ ] if admin : LOG . info ( "User is admin" ) roles . append ( 'admin' ) user_info = dict ( email = user_mail . lower ( ) , name = user_name , roles = roles , institutes = institutes ) user_obj = build_user ( user_info ) try : adapter . add_user ( user_obj ) except Exception as err : LOG . warning ( err ) context . abort ( )
def institutes ( ) : institute_objs = user_institutes ( store , current_user ) institutes = [ ] for ins_obj in institute_objs : sanger_recipients = [ ] for user_mail in ins_obj . get ( 'sanger_recipients' , [ ] ) : user_obj = store . user ( user_mail ) if not user_obj : continue sanger_recipients . append ( user_obj [ 'name' ] ) institutes . append ( { 'display_name' : ins_obj [ 'display_name' ] , 'internal_id' : ins_obj [ '_id' ] , 'coverage_cutoff' : ins_obj . get ( 'coverage_cutoff' , 'None' ) , 'sanger_recipients' : sanger_recipients , 'frequency_cutoff' : ins_obj . get ( 'frequency_cutoff' , 'None' ) , 'phenotype_groups' : ins_obj . get ( 'phenotype_groups' , PHENOTYPE_GROUPS ) } ) data = dict ( institutes = institutes ) return render_template ( 'overview/institutes.html' , * * data )
def remote_static ( ) : file_path = request . args . get ( 'file' ) range_header = request . headers . get ( 'Range' , None ) if not range_header and file_path . endswith ( '.bam' ) : return abort ( 500 ) new_resp = send_file_partial ( file_path ) return new_resp
def pileup ( ) : vcf_file = request . args . get ( 'vcf' ) bam_files = request . args . getlist ( 'bam' ) bai_files = request . args . getlist ( 'bai' ) samples = request . args . getlist ( 'sample' ) alignments = [ { 'bam' : bam , 'bai' : bai , 'sample' : sample } for bam , bai , sample in zip ( bam_files , bai_files , samples ) ] position = { 'contig' : request . args [ 'contig' ] , 'start' : request . args [ 'start' ] , 'stop' : request . args [ 'stop' ] } genome = current_app . config . get ( 'PILEUP_GENOME' ) if genome : if not os . path . isfile ( genome ) : flash ( "The pilup genome path ({}) provided does not exist" . format ( genome ) ) genome = None LOG . debug ( "Use pileup genome %s" , genome ) exons = current_app . config . get ( 'PILEUP_EXONS' ) if exons : if not os . path . isfile ( exons ) : flash ( "The pilup exons path ({}) provided does not exist" . format ( exons ) ) genome = None LOG . debug ( "Use pileup exons %s" , exons ) LOG . debug ( "View alignment for positions Chrom:{0}, Start:{1}, End: {2}" . format ( position [ 'contig' ] , position [ 'start' ] , position [ 'stop' ] ) ) LOG . debug ( "Use alignment files {}" . format ( alignments ) ) return render_template ( 'alignviewers/pileup.html' , alignments = alignments , position = position , vcf_file = vcf_file , genome = genome , exons = exons )
def compounds ( context , case_id ) : adapter = context . obj [ 'adapter' ] LOG . info ( "Running scout update compounds" ) case_obj = adapter . case ( case_id ) if not case_obj : LOG . warning ( "Case %s could not be found" , case_id ) context . abort ( ) try : adapter . update_case_compounds ( case_obj ) except Exception as err : LOG . warning ( err ) context . abort ( )
def hgnc ( ctx , hgnc_symbol , hgnc_id , build ) : adapter = ctx . obj [ 'adapter' ] if not ( hgnc_symbol or hgnc_id ) : log . warning ( "Please provide a hgnc symbol or hgnc id" ) ctx . abort ( ) if hgnc_id : result = adapter . hgnc_gene ( hgnc_id , build = build ) if result : hgnc_symbol = result [ 'hgnc_symbol' ] else : log . warning ( "Gene with id %s could not be found" , hgnc_id ) ctx . abort ( ) result = adapter . hgnc_genes ( hgnc_symbol , build = build ) if result . count ( ) == 0 : log . info ( "No results found" ) else : click . echo ( "#hgnc_id\thgnc_symbol\taliases\ttranscripts" ) for gene in result : click . echo ( "{0}\t{1}\t{2}\t{3}" . format ( gene [ 'hgnc_id' ] , gene [ 'hgnc_symbol' ] , ', ' . join ( gene [ 'aliases' ] ) , ', ' . join ( tx [ 'ensembl_transcript_id' ] for tx in gene [ 'transcripts' ] ) , ) )
def parse_hpo_obo ( hpo_lines ) : term = { } for line in hpo_lines : if len ( line ) == 0 : continue line = line . rstrip ( ) if line == '[Term]' : if term : yield term term = { } elif line . startswith ( 'id' ) : term [ 'hpo_id' ] = line [ 4 : ] elif line . startswith ( 'name' ) : term [ 'description' ] = line [ 6 : ] elif line . startswith ( 'alt_id' ) : if 'aliases' not in term : term [ 'aliases' ] = [ ] term [ 'aliases' ] . append ( line [ 8 : ] ) elif line . startswith ( 'is_a' ) : if 'ancestors' not in term : term [ 'ancestors' ] = [ ] term [ 'ancestors' ] . append ( line [ 6 : 16 ] ) if term : yield term
def genes ( ) : query = request . args . get ( 'query' , '' ) if '|' in query : hgnc_id = int ( query . split ( ' | ' , 1 ) [ 0 ] ) return redirect ( url_for ( '.gene' , hgnc_id = hgnc_id ) ) gene_q = store . all_genes ( ) . limit ( 20 ) return dict ( genes = gene_q )
def gene ( hgnc_id = None , hgnc_symbol = None ) : if hgnc_symbol : query = store . hgnc_genes ( hgnc_symbol ) if query . count ( ) == 1 : hgnc_id = query . first ( ) [ 'hgnc_id' ] else : return redirect ( url_for ( '.genes' , query = hgnc_symbol ) ) try : genes = controllers . gene ( store , hgnc_id ) except ValueError as error : return abort ( 404 ) return genes
def api_genes ( ) : query = request . args . get ( 'query' ) json_out = controllers . genes_to_json ( store , query ) return jsonify ( json_out )
def institute_and_case ( store , institute_id , case_name = None ) : institute_obj = store . institute ( institute_id ) if institute_obj is None and institute_id != 'favicon.ico' : flash ( "Can't find institute: {}" . format ( institute_id ) , 'warning' ) return abort ( 404 ) if case_name : if case_name : case_obj = store . case ( institute_id = institute_id , display_name = case_name ) if case_obj is None : return abort ( 404 ) if not current_user . is_admin : if institute_id not in current_user . institutes : if not case_name or not any ( inst_id in case_obj [ 'collaborators' ] for inst_id in current_user . institutes ) : flash ( "You don't have acccess to: {}" . format ( institute_id ) , 'danger' ) return abort ( 403 ) if case_name : return institute_obj , case_obj else : return institute_obj
def user_institutes ( store , login_user ) : if login_user . is_admin : institutes = store . institutes ( ) else : institutes = [ store . institute ( inst_id ) for inst_id in login_user . institutes ] return institutes
def panel ( context , panel , version , update_date , update_version ) : adapter = context . obj [ 'adapter' ] panel_obj = adapter . gene_panel ( panel , version = version ) if not panel_obj : LOG . warning ( "Panel %s (version %s) could not be found" % ( panel , version ) ) context . abort ( ) date_obj = None if update_date : try : date_obj = get_date ( update_date ) except Exception as err : LOG . warning ( err ) context . abort ( ) update_panel ( adapter , panel , panel_version = panel_obj [ 'version' ] , new_version = update_version , new_date = date_obj )
def diseases ( context , api_key ) : adapter = context . obj [ 'adapter' ] api_key = api_key or context . obj . get ( 'omim_api_key' ) if not api_key : LOG . warning ( "Please provide a omim api key to load the omim gene panel" ) context . abort ( ) try : mim_files = fetch_mim_files ( api_key , genemap2 = True ) except Exception as err : LOG . warning ( err ) context . abort ( ) LOG . info ( "Dropping DiseaseTerms" ) adapter . disease_term_collection . drop ( ) LOG . debug ( "DiseaseTerms dropped" ) load_disease_terms ( adapter = adapter , genemap_lines = mim_files [ 'genemap2' ] , ) LOG . info ( "Successfully loaded all disease terms" )
def users ( context ) : LOG . info ( "Running scout view users" ) adapter = context . obj [ 'adapter' ] user_objs = adapter . users ( ) if user_objs . count ( ) == 0 : LOG . info ( "No users found" ) context . abort ( ) click . echo ( "#name\temail\troles\tinstitutes" ) for user_obj in user_objs : click . echo ( "{0}\t{1}\t{2}\t{3}\t" . format ( user_obj [ 'name' ] , user_obj . get ( 'mail' , user_obj [ '_id' ] ) , ', ' . join ( user_obj . get ( 'roles' , [ ] ) ) , ', ' . join ( user_obj . get ( 'institutes' , [ ] ) ) , ) )
def load_omim_panel ( self , api_key , institute = None ) : existing_panel = self . gene_panel ( panel_id = 'OMIM-AUTO' ) if not existing_panel : LOG . warning ( "OMIM-AUTO does not exists in database" ) LOG . info ( 'Creating a first version' ) version = 1.0 if existing_panel : version = float ( math . floor ( existing_panel [ 'version' ] ) + 1 ) LOG . info ( "Setting version to %s" , version ) try : mim_files = fetch_mim_files ( api_key = api_key , genemap2 = True , mim2genes = True ) except Exception as err : raise err date_string = None for line in mim_files [ 'genemap2' ] : if 'Generated' in line : date_string = line . split ( ':' ) [ - 1 ] . lstrip ( ) . rstrip ( ) date_obj = get_date ( date_string ) if existing_panel : if existing_panel [ 'date' ] == date_obj : LOG . warning ( "There is no new version of OMIM" ) return panel_data = { } panel_data [ 'path' ] = None panel_data [ 'type' ] = 'clinical' panel_data [ 'date' ] = date_obj panel_data [ 'panel_id' ] = 'OMIM-AUTO' panel_data [ 'institute' ] = institute or 'cust002' panel_data [ 'version' ] = version panel_data [ 'display_name' ] = 'OMIM-AUTO' panel_data [ 'genes' ] = [ ] alias_genes = self . genes_by_alias ( ) genes = get_omim_panel_genes ( genemap2_lines = mim_files [ 'genemap2' ] , mim2gene_lines = mim_files [ 'mim2genes' ] , alias_genes = alias_genes , ) for gene in genes : panel_data [ 'genes' ] . append ( gene ) panel_obj = build_panel ( panel_data , self ) if existing_panel : new_genes = self . compare_mim_panels ( existing_panel , panel_obj ) if new_genes : self . update_mim_version ( new_genes , panel_obj , old_version = existing_panel [ 'version' ] ) else : LOG . info ( "The new version of omim does not differ from the old one" ) LOG . info ( "No update is added" ) return self . add_gene_panel ( panel_obj )
def clinical_symbols ( self , case_obj ) : panel_ids = [ panel [ 'panel_id' ] for panel in case_obj [ 'panels' ] ] query = self . panel_collection . aggregate ( [ { '$match' : { '_id' : { '$in' : panel_ids } } } , { '$unwind' : '$genes' } , { '$group' : { '_id' : '$genes.symbol' } } ] ) return set ( item [ '_id' ] for item in query )
def cases ( context , case_id , institute , reruns , finished , causatives , research_requested , is_research , status , json ) : adapter = context . obj [ 'adapter' ] models = [ ] if case_id : case_obj = adapter . case ( case_id = case_id ) if case_obj : models . append ( case_obj ) else : LOG . info ( "No case with id {}" . format ( case_id ) ) else : models = adapter . cases ( collaborator = institute , reruns = reruns , finished = finished , has_causatives = causatives , research_requested = research_requested , is_research = is_research , status = status ) models = [ case_obj for case_obj in models ] if len ( models ) == 0 : LOG . info ( "No cases could be found" ) if json : click . echo ( dumps ( models ) ) return for model in models : pp ( model )
def drop_indexes ( self ) : LOG . warning ( "Dropping all indexe" ) for collection_name in INDEXES : LOG . warning ( "Dropping all indexes for collection name %s" , collection_name ) self . db [ collection_name ] . drop_indexes ( )
def wipe ( ctx ) : LOG . info ( "Running scout wipe" ) db_name = ctx . obj [ 'mongodb' ] LOG . info ( "Dropping database %s" , db_name ) try : ctx . obj [ 'client' ] . drop_database ( db_name ) except Exception as err : LOG . warning ( err ) ctx . abort ( ) LOG . info ( "Dropped whole database" )
def parse_panel ( csv_stream ) : reader = csv . DictReader ( csv_stream , delimiter = ';' , quoting = csv . QUOTE_NONE ) genes = [ ] for gene_row in reader : if not gene_row [ 'HGNC_IDnumber' ] . strip ( ) . isdigit ( ) : continue transcripts_raw = gene_row . get ( 'Disease_associated_transcript' ) if transcripts_raw : transcripts_list = [ tx . split ( ':' , 1 ) [ - 1 ] . strip ( ) for tx in transcripts_raw . split ( ',' ) ] else : transcripts_list = [ ] models_raw = gene_row . get ( 'Genetic_disease_model' ) models_list = [ model . strip ( ) for model in models_raw . split ( ',' ) ] if models_raw else [ ] panel_gene = dict ( symbol = gene_row [ 'HGNC_symbol' ] . strip ( ) if gene_row . get ( 'HGNC_symbol' ) else None , hgnc_id = int ( gene_row [ 'HGNC_IDnumber' ] . strip ( ) ) , disease_associated_transcripts = transcripts_list , reduced_penetrance = True if gene_row . get ( 'Reduced_penetrance' ) else None , mosaicism = True if gene_row . get ( 'Mosaicism' ) else None , inheritance_models = models_list , database_entry_version = gene_row . get ( 'Database_entry_version' ) , ) genes . append ( panel_gene ) return genes
def drop_genes ( self , build = None ) : if build : LOG . info ( "Dropping the hgnc_gene collection, build %s" , build ) self . hgnc_collection . delete_many ( { 'build' : build } ) else : LOG . info ( "Dropping the hgnc_gene collection" ) self . hgnc_collection . drop ( )
def drop_transcripts ( self , build = None ) : if build : LOG . info ( "Dropping the transcripts collection, build %s" , build ) self . transcript_collection . delete_many ( { 'build' : build } ) else : LOG . info ( "Dropping the transcripts collection" ) self . transcript_collection . drop ( )
def drop_exons ( self , build = None ) : if build : LOG . info ( "Dropping the exons collection, build %s" , build ) self . exon_collection . delete_many ( { 'build' : build } ) else : LOG . info ( "Dropping the exons collection" ) self . exon_collection . drop ( )
def omim ( context , api_key , institute ) : LOG . info ( "Running scout update omim" ) adapter = context . obj [ 'adapter' ] api_key = api_key or context . obj . get ( 'omim_api_key' ) if not api_key : LOG . warning ( "Please provide a omim api key to load the omim gene panel" ) context . abort ( ) institute_obj = adapter . institute ( institute ) if not institute_obj : LOG . info ( "Institute %s could not be found in database" , institute ) LOG . warning ( "Please specify an existing institute" ) context . abort ( ) try : adapter . load_omim_panel ( api_key , institute = institute ) except Exception as err : LOG . error ( err ) context . abort ( )
def index ( ) : institute_objs = user_institutes ( store , current_user ) institutes_count = ( ( institute_obj , store . cases ( collaborator = institute_obj [ '_id' ] ) . count ( ) ) for institute_obj in institute_objs if institute_obj ) return dict ( institutes = institutes_count )
def cases ( institute_id ) : institute_obj = institute_and_case ( store , institute_id ) query = request . args . get ( 'query' ) limit = 100 if request . args . get ( 'limit' ) : limit = int ( request . args . get ( 'limit' ) ) skip_assigned = request . args . get ( 'skip_assigned' ) is_research = request . args . get ( 'is_research' ) all_cases = store . cases ( collaborator = institute_id , name_query = query , skip_assigned = skip_assigned , is_research = is_research ) data = controllers . cases ( store , all_cases , limit ) sanger_unevaluated = controllers . get_sanger_unevaluated ( store , institute_id , current_user . email ) if len ( sanger_unevaluated ) > 0 : data [ 'sanger_unevaluated' ] = sanger_unevaluated return dict ( institute = institute_obj , skip_assigned = skip_assigned , is_research = is_research , query = query , * * data )
def case ( institute_id , case_name ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) data = controllers . case ( store , institute_obj , case_obj ) return dict ( institute = institute_obj , case = case_obj , * * data )
def matchmaker_matches ( institute_id , case_name ) : user_obj = store . user ( current_user . email ) if 'mme_submitter' not in user_obj [ 'roles' ] : flash ( 'unauthorized request' , 'warning' ) return redirect ( request . referrer ) mme_base_url = current_app . config . get ( 'MME_URL' ) mme_token = current_app . config . get ( 'MME_TOKEN' ) if not mme_base_url or not mme_token : flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) return redirect ( request . referrer ) institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) data = controllers . mme_matches ( case_obj , institute_obj , mme_base_url , mme_token ) if data and data . get ( 'server_errors' ) : flash ( 'MatchMaker server returned error:{}' . format ( data [ 'server_errors' ] ) , 'danger' ) return redirect ( request . referrer ) elif not data : data = { 'institute' : institute_obj , 'case' : case_obj } return data
def matchmaker_match ( institute_id , case_name , target ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) user_obj = store . user ( current_user . email ) if 'mme_submitter' not in user_obj [ 'roles' ] : flash ( 'unauthorized request' , 'warning' ) return redirect ( request . referrer ) mme_base_url = current_app . config . get ( 'MME_URL' ) mme_accepts = current_app . config . get ( 'MME_ACCEPTS' ) mme_token = current_app . config . get ( 'MME_TOKEN' ) nodes = current_app . mme_nodes if not mme_base_url or not mme_token or not mme_accepts : flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) return redirect ( request . referrer ) match_results = controllers . mme_match ( case_obj , target , mme_base_url , mme_token , nodes , mme_accepts ) ok_responses = 0 for match_results in match_results : match_results [ 'status_code' ] == 200 ok_responses += 1 if ok_responses : flash ( "Match request sent. Look for eventual matches in 'Matches' page." , 'info' ) else : flash ( 'An error occurred while sending match request.' , 'danger' ) return redirect ( request . referrer )
def matchmaker_delete ( institute_id , case_name ) : user_obj = store . user ( current_user . email ) if 'mme_submitter' not in user_obj [ 'roles' ] : flash ( 'unauthorized request' , 'warning' ) return redirect ( request . referrer ) institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) mme_base_url = current_app . config . get ( 'MME_URL' ) mme_token = current_app . config . get ( 'MME_TOKEN' ) if not mme_base_url or not mme_token : flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) return redirect ( request . referrer ) delete_result = controllers . mme_delete ( case_obj , mme_base_url , mme_token ) n_deleted = 0 category = 'warning' for resp in delete_result : if resp [ 'status_code' ] == 200 : n_deleted += 1 else : flash ( resp [ 'message' ] , category ) if n_deleted : category = 'success' user_obj = store . user ( current_user . email ) store . case_mme_delete ( case_obj = case_obj , user_obj = user_obj ) flash ( 'Number of patients deleted from Matchmaker: {} out of {}' . format ( n_deleted , len ( delete_result ) ) , category ) return redirect ( request . referrer )
def gene_variants ( institute_id ) : page = int ( request . form . get ( 'page' , 1 ) ) institute_obj = institute_and_case ( store , institute_id ) if ( request . method == "POST" ) : form = GeneVariantFiltersForm ( request . form ) else : form = GeneVariantFiltersForm ( request . args ) variant_type = form . data . get ( 'variant_type' , 'clinical' ) hgnc_symbols = [ ] non_clinical_symbols = [ ] not_found_symbols = [ ] not_found_ids = [ ] data = { } if ( form . hgnc_symbols . data ) and len ( form . hgnc_symbols . data ) > 0 : is_clinical = form . data . get ( 'variant_type' , 'clinical' ) == 'clinical' clinical_symbols = store . clinical_symbols ( case_obj ) if is_clinical else None for hgnc_symbol in form . hgnc_symbols . data : if hgnc_symbol . isdigit ( ) : hgnc_gene = store . hgnc_gene ( int ( hgnc_symbol ) ) if hgnc_gene is None : not_found_ids . append ( hgnc_symbol ) else : hgnc_symbols . append ( hgnc_gene [ 'hgnc_symbol' ] ) elif store . hgnc_genes ( hgnc_symbol ) . count ( ) == 0 : not_found_symbols . append ( hgnc_symbol ) elif is_clinical and ( hgnc_symbol not in clinical_symbols ) : non_clinical_symbols . append ( hgnc_symbol ) else : hgnc_symbols . append ( hgnc_symbol ) if ( not_found_ids ) : flash ( "HGNC id not found: {}" . format ( ", " . join ( not_found_ids ) ) , 'warning' ) if ( not_found_symbols ) : flash ( "HGNC symbol not found: {}" . format ( ", " . join ( not_found_symbols ) ) , 'warning' ) if ( non_clinical_symbols ) : flash ( "Gene not included in clinical list: {}" . format ( ", " . join ( non_clinical_symbols ) ) , 'warning' ) form . hgnc_symbols . data = hgnc_symbols log . debug ( "query {}" . format ( form . data ) ) variants_query = store . gene_variants ( query = form . data , category = 'snv' , variant_type = variant_type ) data = controllers . gene_variants ( store , variants_query , page ) return dict ( institute = institute_obj , form = form , page = page , * * data )
def pdf_case_report ( institute_id , case_name ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) data = controllers . case_report_content ( store , institute_obj , case_obj ) if current_app . config . get ( 'SQLALCHEMY_DATABASE_URI' ) : data [ 'coverage_report' ] = controllers . coverage_report_contents ( store , institute_obj , case_obj , request . url_root ) if case_obj . get ( 'madeline_info' ) is not None : with open ( os . path . join ( cases_bp . static_folder , 'madeline.svg' ) , 'w' ) as temp_madeline : temp_madeline . write ( case_obj [ 'madeline_info' ] ) html_report = render_template ( 'cases/case_report.html' , institute = institute_obj , case = case_obj , format = 'pdf' , * * data ) return render_pdf ( HTML ( string = html_report ) , download_filename = case_obj [ 'display_name' ] + '_' + datetime . datetime . now ( ) . strftime ( "%Y-%m-%d" ) + '_scout.pdf' )
def case_diagnosis ( institute_id , case_name ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) user_obj = store . user ( current_user . email ) link = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) level = 'phenotype' if 'phenotype' in request . form else 'gene' omim_id = request . form [ 'omim_id' ] remove = True if request . args . get ( 'remove' ) == 'yes' else False store . diagnose ( institute_obj , case_obj , user_obj , link , level = level , omim_id = omim_id , remove = remove ) return redirect ( request . referrer )
def phenotypes_actions ( institute_id , case_name ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) case_url = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) action = request . form [ 'action' ] hpo_ids = request . form . getlist ( 'hpo_id' ) user_obj = store . user ( current_user . email ) if action == 'DELETE' : for hpo_id in hpo_ids : store . remove_phenotype ( institute_obj , case_obj , user_obj , case_url , hpo_id ) elif action == 'PHENOMIZER' : if len ( hpo_ids ) == 0 : hpo_ids = [ term [ 'phenotype_id' ] for term in case_obj . get ( 'phenotype_terms' , [ ] ) ] username = current_app . config [ 'PHENOMIZER_USERNAME' ] password = current_app . config [ 'PHENOMIZER_PASSWORD' ] diseases = controllers . hpo_diseases ( username , password , hpo_ids ) return render_template ( 'cases/diseases.html' , diseases = diseases , institute = institute_obj , case = case_obj ) elif action == 'GENES' : hgnc_symbols = set ( ) for raw_symbols in request . form . getlist ( 'genes' ) : if raw_symbols : hgnc_symbols . update ( raw_symbol . split ( ' ' , 1 ) [ 0 ] for raw_symbol in raw_symbols . split ( '|' ) ) store . update_dynamic_gene_list ( case_obj , hgnc_symbols = hgnc_symbols ) elif action == 'GENERATE' : if len ( hpo_ids ) == 0 : hpo_ids = [ term [ 'phenotype_id' ] for term in case_obj . get ( 'phenotype_terms' , [ ] ) ] results = store . generate_hpo_gene_list ( * hpo_ids ) hpo_count = int ( request . form . get ( 'min_match' ) or 1 ) hgnc_ids = [ result [ 0 ] for result in results if result [ 1 ] >= hpo_count ] store . update_dynamic_gene_list ( case_obj , hgnc_ids = hgnc_ids , phenotype_ids = hpo_ids ) return redirect ( case_url )
def status ( institute_id , case_name ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) user_obj = store . user ( current_user . email ) status = request . form . get ( 'status' , case_obj [ 'status' ] ) link = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) if status == 'archive' : store . archive_case ( institute_obj , case_obj , user_obj , status , link ) else : store . update_status ( institute_obj , case_obj , user_obj , status , link ) return redirect ( request . referrer )
def assign ( institute_id , case_name , user_id = None ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) link = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) if user_id : user_obj = store . user ( user_id ) else : user_obj = store . user ( current_user . email ) if request . form . get ( 'action' ) == 'DELETE' : store . unassign ( institute_obj , case_obj , user_obj , link ) else : store . assign ( institute_obj , case_obj , user_obj , link ) return redirect ( request . referrer )
def hpoterms ( ) : query = request . args . get ( 'query' ) if query is None : return abort ( 500 ) terms = sorted ( store . hpo_terms ( query = query ) , key = itemgetter ( 'hpo_number' ) ) json_terms = [ { 'name' : '{} | {}' . format ( term [ '_id' ] , term [ 'description' ] ) , 'id' : term [ '_id' ] } for term in terms [ : 7 ] ] return jsonify ( json_terms )
def mark_validation ( institute_id , case_name , variant_id ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) variant_obj = store . variant ( variant_id ) user_obj = store . user ( current_user . email ) validate_type = request . form [ 'type' ] or None link = url_for ( 'variants.variant' , institute_id = institute_id , case_name = case_name , variant_id = variant_id ) store . validate ( institute_obj , case_obj , user_obj , link , variant_obj , validate_type ) return redirect ( request . referrer or link )
def mark_causative ( institute_id , case_name , variant_id ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) variant_obj = store . variant ( variant_id ) user_obj = store . user ( current_user . email ) link = url_for ( 'variants.variant' , institute_id = institute_id , case_name = case_name , variant_id = variant_id ) if request . form [ 'action' ] == 'ADD' : store . mark_causative ( institute_obj , case_obj , user_obj , link , variant_obj ) elif request . form [ 'action' ] == 'DELETE' : store . unmark_causative ( institute_obj , case_obj , user_obj , link , variant_obj ) case_url = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) return redirect ( case_url )
def delivery_report ( institute_id , case_name ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) if case_obj . get ( 'delivery_report' ) is None : return abort ( 404 ) date_str = request . args . get ( 'date' ) if date_str : delivery_report = None analysis_date = parse_date ( date_str ) for analysis_data in case_obj [ 'analyses' ] : if analysis_data [ 'date' ] == analysis_date : delivery_report = analysis_data [ 'delivery_report' ] if delivery_report is None : return abort ( 404 ) else : delivery_report = case_obj [ 'delivery_report' ] out_dir = os . path . dirname ( delivery_report ) filename = os . path . basename ( delivery_report ) return send_from_directory ( out_dir , filename )
def share ( institute_id , case_name ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) user_obj = store . user ( current_user . email ) collaborator_id = request . form [ 'collaborator' ] revoke_access = 'revoke' in request . form link = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) if revoke_access : store . unshare ( institute_obj , case_obj , collaborator_id , user_obj , link ) else : store . share ( institute_obj , case_obj , collaborator_id , user_obj , link ) return redirect ( request . referrer )
def rerun ( institute_id , case_name ) : sender = current_app . config [ 'MAIL_USERNAME' ] recipient = current_app . config [ 'TICKET_SYSTEM_EMAIL' ] controllers . rerun ( store , mail , current_user , institute_id , case_name , sender , recipient ) return redirect ( request . referrer )
def research ( institute_id , case_name ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) user_obj = store . user ( current_user . email ) link = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) store . open_research ( institute_obj , case_obj , user_obj , link ) return redirect ( request . referrer )
def default_panels ( institute_id , case_name ) : panel_ids = request . form . getlist ( 'panel_ids' ) controllers . update_default_panels ( store , current_user , institute_id , case_name , panel_ids ) return redirect ( request . referrer )
def vcf2cytosure ( institute_id , case_name , individual_id ) : ( display_name , vcf2cytosure ) = controllers . vcf2cytosure ( store , institute_id , case_name , individual_id ) outdir = os . path . abspath ( os . path . dirname ( vcf2cytosure ) ) filename = os . path . basename ( vcf2cytosure ) log . debug ( "Attempt to deliver file {0} from dir {1}" . format ( filename , outdir ) ) attachment_filename = display_name + ".vcf2cytosure.cgh" return send_from_directory ( outdir , filename , attachment_filename = attachment_filename , as_attachment = True )
def multiqc ( institute_id , case_name ) : data = controllers . multiqc ( store , institute_id , case_name ) if data [ 'case' ] . get ( 'multiqc' ) is None : return abort ( 404 ) out_dir = os . path . abspath ( os . path . dirname ( data [ 'case' ] [ 'multiqc' ] ) ) filename = os . path . basename ( data [ 'case' ] [ 'multiqc' ] ) return send_from_directory ( out_dir , filename )
def clinvar_submissions ( store , user_id , institute_id ) : submissions = list ( store . clinvar_submissions ( user_id , institute_id ) ) return submissions
def rerun ( store , mail , current_user , institute_id , case_name , sender , recipient ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) user_obj = store . user ( current_user . email ) link = url_for ( 'cases.case' , institute_id = institute_id , case_name = case_name ) store . request_rerun ( institute_obj , case_obj , user_obj , link ) html = . format ( institute = institute_obj [ 'display_name' ] , case = case_obj [ 'display_name' ] , case_id = case_obj [ '_id' ] , name = user_obj [ 'name' ] . encode ( ) ) msg = Message ( subject = ( "SCOUT: request RERUN for {}" . format ( case_obj [ 'display_name' ] ) ) , html = html , sender = sender , recipients = [ recipient ] , cc = [ user_obj [ 'email' ] ] ) mail . send ( msg )
def update_default_panels ( store , current_user , institute_id , case_name , panel_ids ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) user_obj = store . user ( current_user . email ) link = url_for ( 'cases.case' , institute_id = institute_id , case_name = case_name ) panel_objs = [ store . panel ( panel_id ) for panel_id in panel_ids ] store . update_default_panels ( institute_obj , case_obj , user_obj , link , panel_objs )
def vcf2cytosure ( store , institute_id , case_name , individual_id ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) for individual in case_obj [ 'individuals' ] : if individual [ 'individual_id' ] == individual_id : individual_obj = individual return ( individual_obj [ 'display_name' ] , individual_obj [ 'vcf2cytosure' ] )
def multiqc ( store , institute_id , case_name ) : institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) return dict ( institute = institute_obj , case = case_obj , )
def genes ( context , build , api_key ) : LOG . info ( "Running scout update genes" ) adapter = context . obj [ 'adapter' ] api_key = api_key or context . obj . get ( 'omim_api_key' ) if not api_key : LOG . warning ( "Please provide a omim api key to load the omim gene panel" ) context . abort ( ) try : mim_files = fetch_mim_files ( api_key , mim2genes = True , morbidmap = True , genemap2 = True ) except Exception as err : LOG . warning ( err ) context . abort ( ) LOG . warning ( "Dropping all gene information" ) adapter . drop_genes ( build ) LOG . info ( "Genes dropped" ) LOG . warning ( "Dropping all transcript information" ) adapter . drop_transcripts ( build ) LOG . info ( "transcripts dropped" ) hpo_genes = fetch_hpo_genes ( ) if build : builds = [ build ] else : builds = [ '37' , '38' ] hgnc_lines = fetch_hgnc ( ) exac_lines = fetch_exac_constraint ( ) for build in builds : ensembl_genes = fetch_ensembl_genes ( build = build ) hgnc_genes = load_hgnc_genes ( adapter = adapter , ensembl_lines = ensembl_genes , hgnc_lines = hgnc_lines , exac_lines = exac_lines , mim2gene_lines = mim_files [ 'mim2genes' ] , genemap_lines = mim_files [ 'genemap2' ] , hpo_lines = hpo_genes , build = build , ) ensembl_genes = { } for gene_obj in hgnc_genes : ensembl_id = gene_obj [ 'ensembl_id' ] ensembl_genes [ ensembl_id ] = gene_obj ensembl_transcripts = fetch_ensembl_transcripts ( build = build ) transcripts = load_transcripts ( adapter , ensembl_transcripts , build , ensembl_genes ) adapter . update_indexes ( ) LOG . info ( "Genes, transcripts and Exons loaded" )
def parse_cadd ( variant , transcripts ) : cadd = 0 cadd_keys = [ 'CADD' , 'CADD_PHRED' ] for key in cadd_keys : cadd = variant . INFO . get ( key , 0 ) if cadd : return float ( cadd ) for transcript in transcripts : cadd_entry = transcript . get ( 'cadd' ) if ( cadd_entry and cadd_entry > cadd ) : cadd = cadd_entry return cadd
def convert ( context , panel ) : adapter = context . obj [ 'adapter' ] new_header = [ "hgnc_id" , "hgnc_symbol" , "disease_associated_transcripts" , "reduced_penetrance" , "genetic_disease_models" , "mosaicism" , "database_entry_version" ] genes = parse_genes ( panel ) adapter . add_hgnc_id ( genes ) click . echo ( "#{0}" . format ( '\t' . join ( new_header ) ) ) for gene in genes : if gene . get ( 'hgnc_id' ) : print_info = [ ] for head in new_header : print_info . append ( str ( gene [ head ] ) if gene . get ( head ) else '' ) click . echo ( '\t' . join ( print_info ) )
def cli ( context , morbid , genemap , mim2gene , mim_titles , phenotypes ) : from scout . utils . handle import get_file_handle from pprint import pprint as pp print ( "Morbid file: %s" % morbid ) print ( "Genemap file: %s" % genemap ) print ( "mim2gene file: %s" % mim2gene ) print ( "MimTitles file: %s" % mim_titles ) if morbid : morbid_handle = get_file_handle ( morbid ) if genemap : genemap_handle = get_file_handle ( genemap ) if mim2gene : mim2gene_handle = get_file_handle ( mim2gene ) if mim_titles : mimtitles_handle = get_file_handle ( mim_titles ) mim_genes = get_mim_genes ( genemap_handle , mim2gene_handle ) for entry in mim_genes : if entry == 'C10orf11' : pp ( mim_genes [ entry ] ) context . abort ( ) if phenotypes : if not genemap : click . echo ( "Please provide the genemap file" ) context . abort ( ) phenotypes = get_mim_phenotypes ( genemap_handle ) for i , mim_term in enumerate ( phenotypes ) : pass print ( "Number of phenotypes found: %s" % i ) context . abort ( ) genes = get_mim_genes ( genemap_handle , mim2gene_handle ) for hgnc_symbol in genes : if hgnc_symbol == 'OPA1' : print ( genes [ hgnc_symbol ] )
def formatmonth ( self , theyear , themonth , withyear = True , net = None , qs = None , template = 'happenings/partials/calendar/month_table.html' ) : context = self . get_context ( ) context [ 'month_start_date' ] = date ( self . yr , self . mo , 1 ) context [ 'week_rows' ] = [ ] for week in self . monthdays2calendar ( theyear , themonth ) : week_row = [ ] for day , weekday in week : week_row . append ( self . formatday ( day , weekday ) ) context [ 'week_rows' ] . append ( week_row ) nxt , prev = get_next_and_prev ( net ) extra_qs = ( '&' + '&' . join ( qs ) ) if qs else '' context [ 'prev_qs' ] = mark_safe ( '?cal_prev=%d%s' % ( prev , extra_qs ) ) context [ 'next_qs' ] = mark_safe ( '?cal_next=%d%s' % ( nxt , extra_qs ) ) context [ 'withyear' ] = withyear return render_to_string ( template , context )
def formatday ( self , day , weekday , day_template = 'happenings/partials/calendar/day_cell.html' , noday_template = 'happenings/partials/calendar/day_noday_cell.html' , popover_template = 'happenings/partials/calendar/popover.html' , ) : super ( EventCalendar , self ) . formatday ( day , weekday ) now = get_now ( ) context = self . get_context ( ) context [ 'events' ] = [ ] context [ 'day' ] = day context [ 'day_url' ] = self . get_day_url ( day ) context [ 'month_start_date' ] = date ( self . yr , self . mo , 1 ) context [ 'weekday' ] = weekday context [ 'cssclass' ] = self . cssclasses [ weekday ] context [ 'popover_template' ] = popover_template context [ 'num_events' ] = len ( self . count . get ( day , [ ] ) ) , try : processed_date = date ( self . yr , self . mo , day ) except ValueError : processed_date = None context [ 'month_start_date' ] = date ( self . yr , self . mo , 1 ) if day == 0 : template = noday_template else : template = day_template if now . date ( ) == processed_date : context [ 'is_current_day' ] = True if processed_date and ( day in self . count ) : for item in self . count [ day ] : self . pk = item [ 1 ] self . title = item [ 0 ] for event in self . events : if event . pk == self . pk : event . check_if_cancelled ( processed_date ) context [ 'events' ] . append ( event ) return render_to_string ( template , context )
def formatday ( self , day , weekday ) : return super ( MiniEventCalendar , self ) . formatday ( day , weekday , day_template = 'happenings/partials/calendar/mini_day_cell.html' , popover_template = 'happenings/partials/calendar/mini_popover.html' , )
def formatday ( self , day , weekday ) : self . wkday_not_today = '<td class="%s"><div class="td-inner">' % ( self . cssclasses [ weekday ] ) self . wkday_today = ( '<td class="%s calendar-today"><div class="td-inner">' % ( self . cssclasses [ weekday ] ) ) if URLS_NAMESPACE : url_name = '%s:day_list' % ( URLS_NAMESPACE ) else : url_name = 'day_list' self . day_url = reverse ( url_name , args = ( self . yr , self . mo , day ) ) self . day = day self . anch = '<a href="%s">%d</a>' % ( self . day_url , day ) self . end = '</div></td>'
def popover_helper ( self ) : display_month = month_name [ self . mo ] if isinstance ( display_month , six . binary_type ) and self . encoding : display_month = display_month . decode ( 'utf-8' ) self . when = ( '<p><b>When:</b> ' + display_month + ' ' + str ( self . day ) + ', ' + self . event . l_start_date . strftime ( LEGACY_CALENDAR_TIME_FORMAT ) . lstrip ( '0' ) + ' - ' + self . event . l_end_date . strftime ( LEGACY_CALENDAR_TIME_FORMAT ) . lstrip ( '0' ) + '</p>' ) if self . event . location . exists ( ) : self . where = '<p><b>Where:</b> ' for l in self . event . location . all ( ) : self . where += l . name self . where += '</p>' else : self . where = '' self . desc = '<p><b>Description:</b> ' + self . event . description [ : 100 ] self . desc += ( '...</p>' if len ( self . event . description ) > 100 else '</p>' ) self . event_url = self . event . get_absolute_url ( ) t = LEGACY_CALENDAR_TIME_FORMAT if self . event . l_start_date . minute else LEGACY_CALENDAR_HOUR_FORMAT self . title2 = ( self . event . l_start_date . strftime ( t ) . lstrip ( '0' ) + ' ' + self . title )
def formatday ( self , day , weekday ) : super ( EventCalendar , self ) . formatday ( day , weekday ) now = get_now ( ) self . day = day out = '' if day == 0 : return '<td class="noday">&nbsp;</td>' elif now . month == self . mo and now . year == self . yr and day == now . day : if day in self . count : out = self . wkday_today + self . anch else : return self . wkday_today + self . anch + self . end elif day in self . count : out = self . wkday_not_today + self . anch else : return self . wkday_not_today + self . anch + self . end detail = "%s%s%s<br><a href='%s'>View details</a>" extras = ( '<div title="%s" data-content="%s" data-container="body"' ' data-toggle="popover" class="calendar-event"%s>' ) common = ' style=background:%s;color:%s;' for item in self . count [ day ] : self . pk = item [ 1 ] self . title = item [ 0 ] for event in self . events : if event . pk == self . pk : self . event = event self . check_if_cancelled ( ) self . popover_helper ( ) bg , fnt = self . event . get_colors ( ) out += ( '<a class="event-anch" href="' + self . event_url + '">' + extras % ( self . title , detail % ( self . when , self . where , self . desc , self . event_url ) , common % ( bg , fnt ) ) + self . title2 + '</div></a>' ) return out + self . end
def formatday ( self , day , weekday ) : super ( MiniEventCalendar , self ) . formatday ( day , weekday ) now = get_now ( ) self . day = day if day == 0 : return '<td class="noday">&nbsp;</td>' elif now . month == self . mo and now . year == self . yr and day == now . day : if day in self . count : self . popover_helper ( ) return self . wkday_today + self . anch + self . cal_event + self . end else : return self . wkday_today + self . anch + self . end elif day in self . count : self . popover_helper ( ) return self . wkday_not_today + self . anch + self . cal_event + self . end else : return self . wkday_not_today + self . anch + self . end
def diseases ( context ) : LOG . info ( "Running scout view diseases" ) adapter = context . obj [ 'adapter' ] disease_objs = adapter . disease_terms ( ) nr_diseases = disease_objs . count ( ) if nr_diseases == 0 : click . echo ( "No diseases found" ) else : click . echo ( "Disease" ) for disease_obj in adapter . disease_terms ( ) : click . echo ( "{0}" . format ( disease_obj [ '_id' ] ) ) LOG . info ( "{0} diseases found" . format ( nr_diseases ) )
def hpo ( context ) : LOG . info ( "Running scout update hpo" ) adapter = context . obj [ 'adapter' ] LOG . info ( "Dropping HPO terms" ) adapter . hpo_term_collection . drop ( ) LOG . debug ( "HPO terms dropped" ) load_hpo_terms ( adapter )
def users ( store ) : user_objs = list ( store . users ( ) ) total_events = store . user_events ( ) . count ( ) for user_obj in user_objs : if user_obj . get ( 'institutes' ) : user_obj [ 'institutes' ] = [ store . institute ( inst_id ) for inst_id in user_obj . get ( 'institutes' ) ] else : user_obj [ 'institutes' ] = [ ] user_obj [ 'events' ] = store . user_events ( user_obj ) . count ( ) user_obj [ 'events_rank' ] = event_rank ( user_obj [ 'events' ] ) return dict ( users = sorted ( user_objs , key = lambda user : - user [ 'events' ] ) , total_events = total_events , )
def render_to_json_response ( self , context , * * kwargs ) : return HttpResponse ( self . convert_context_to_json ( context ) , content_type = 'application/json' , * * kwargs )
def check_for_cancelled_events ( self , d ) : for event in self . events : for cn in event . cancellations . all ( ) : if cn . date == d : event . title += ' (CANCELLED)'
def _setup_time_axis ( self , t_start = None , t_stop = None ) : ii_start , ii_stop = 0 , self . n_ints_in_file if t_start : ii_start = t_start if t_stop : ii_stop = t_stop n_ints = ii_stop - ii_start t0 = self . header [ b'tstart' ] t_delt = self . header [ b'tsamp' ] self . timestamps = np . arange ( 0 , n_ints ) * t_delt / 24. / 60. / 60 + t0 return ii_start , ii_stop , n_ints
def compute_lst ( self ) : if self . header [ b'telescope_id' ] == 6 : self . coords = gbt_coords elif self . header [ b'telescope_id' ] == 4 : self . coords = parkes_coords else : raise RuntimeError ( "Currently only Parkes and GBT supported" ) if HAS_SLALIB : dut1 = 0.0 mjd = self . header [ b'tstart' ] tellong = np . deg2rad ( self . coords [ 1 ] ) last = s . sla_gmst ( mjd ) - tellong + s . sla_eqeqx ( mjd ) + dut1 if last < 0.0 : last = last + 2.0 * np . pi return last else : raise RuntimeError ( "This method requires pySLALIB" )
def _calc_extent ( self , plot_f = None , plot_t = None , MJD_time = False ) : plot_f_begin = plot_f [ 0 ] plot_f_end = plot_f [ - 1 ] + ( plot_f [ 1 ] - plot_f [ 0 ] ) plot_t_begin = self . timestamps [ 0 ] plot_t_end = self . timestamps [ - 1 ] + ( self . timestamps [ 1 ] - self . timestamps [ 0 ] ) if MJD_time : extent = ( plot_f_begin , plot_f_begin_end , plot_t_begin , plot_t_end ) else : extent = ( plot_f_begin , plot_f_end , 0.0 , ( plot_t_end - plot_t_begin ) * 24. * 60. * 60 ) return extent
def closest ( xarr , val ) : idx_closest = np . argmin ( np . abs ( np . array ( xarr ) - val ) ) return idx_closest
def get_diff ( dio_cross , feedtype , * * kwargs ) : #Get Stokes parameters, frequencies, and time sample length obs = Waterfall ( dio_cross , max_load = 150 ) freqs = obs . populate_freqs ( ) tsamp = obs . header [ 'tsamp' ] data = obs . data obs = None I , Q , U , V = get_stokes ( data , feedtype ) #Fold noise diode data I_OFF , I_ON = foldcal ( I , tsamp , * * kwargs ) Q_OFF , Q_ON = foldcal ( Q , tsamp , * * kwargs ) U_OFF , U_ON = foldcal ( U , tsamp , * * kwargs ) V_OFF , V_ON = foldcal ( V , tsamp , * * kwargs ) #Do ON-OFF subtraction Idiff = I_ON - I_OFF Qdiff = Q_ON - Q_OFF Udiff = U_ON - U_OFF Vdiff = V_ON - V_OFF return Idiff , Qdiff , Udiff , Vdiff , freqs
def _calc_selection_size ( self ) : #Check to see how many integrations requested n_ints = self . t_stop - self . t_start #Check to see how many frequency channels requested n_chan = ( self . f_stop - self . f_start ) / abs ( self . header [ b'foff' ] ) n_bytes = self . _n_bytes selection_size = int ( n_ints * n_chan * n_bytes ) return selection_size
def _calc_selection_shape ( self ) : #Check how many integrations requested n_ints = int ( self . t_stop - self . t_start ) #Check how many frequency channels requested n_chan = int ( np . round ( ( self . f_stop - self . f_start ) / abs ( self . header [ b'foff' ] ) ) ) selection_shape = ( n_ints , int ( self . header [ b'nifs' ] ) , n_chan ) return selection_shape
def _setup_freqs ( self ) : if self . header [ b'foff' ] > 0 : self . f_start = self . f_begin + self . chan_start_idx * abs ( self . header [ b'foff' ] ) self . f_stop = self . f_begin + self . chan_stop_idx * abs ( self . header [ b'foff' ] ) else : self . f_start = self . f_end - self . chan_stop_idx * abs ( self . header [ b'foff' ] ) self . f_stop = self . f_end - self . chan_start_idx * abs ( self . header [ b'foff' ] )
def calc_n_blobs ( self , blob_dim ) : n_blobs = int ( np . ceil ( 1.0 * np . prod ( self . selection_shape ) / np . prod ( blob_dim ) ) ) return n_blobs
def isheavy ( self ) : selection_size_bytes = self . _calc_selection_size ( ) if selection_size_bytes > self . MAX_DATA_ARRAY_SIZE : return True else : return False
def _find_blob_start ( self , blob_dim , n_blob ) : #Convert input frequencies into what their corresponding channel number would be. self . _setup_chans ( ) #Check which is the blob time offset blob_time_start = self . t_start + blob_dim [ self . time_axis ] * n_blob #Check which is the blob frequency offset (in channels) blob_freq_start = self . chan_start_idx + ( blob_dim [ self . freq_axis ] * n_blob ) % self . selection_shape [ self . freq_axis ] blob_start = np . array ( [ blob_time_start , 0 , blob_freq_start ] ) return blob_start
def read_blob ( self , blob_dim , n_blob = 0 ) : n_blobs = self . calc_n_blobs ( blob_dim ) if n_blob > n_blobs or n_blob < 0 : raise ValueError ( 'Please provide correct n_blob value. Given %i, but max values is %i' % ( n_blob , n_blobs ) ) #This prevents issues when the last blob is smaller than the others in time if blob_dim [ self . time_axis ] * ( n_blob + 1 ) > self . selection_shape [ self . time_axis ] : updated_blob_dim = ( self . selection_shape [ self . time_axis ] - blob_dim [ self . time_axis ] * n_blob , 1 , blob_dim [ self . freq_axis ] ) else : updated_blob_dim = [ int ( i ) for i in blob_dim ] blob_start = self . _find_blob_start ( blob_dim , n_blob ) blob_end = blob_start + np . array ( updated_blob_dim ) blob = self . h5 [ "data" ] [ int ( blob_start [ self . time_axis ] ) : int ( blob_end [ self . time_axis ] ) , : , int ( blob_start [ self . freq_axis ] ) : int ( blob_end [ self . freq_axis ] ) ] return blob
def _find_blob_start ( self ) : self . _setup_chans ( ) blob_time_start = self . t_start blob_freq_start = self . chan_start_idx blob_start = blob_time_start * self . n_channels_in_file + blob_freq_start return blob_start
def read_blob ( self , blob_dim , n_blob = 0 ) : n_blobs = self . calc_n_blobs ( blob_dim ) if n_blob > n_blobs or n_blob < 0 : raise ValueError ( 'Please provide correct n_blob value. Given %i, but max values is %i' % ( n_blob , n_blobs ) ) if blob_dim [ self . time_axis ] * ( n_blob + 1 ) > self . selection_shape [ self . time_axis ] : updated_blob_dim = ( int ( self . selection_shape [ self . time_axis ] - blob_dim [ self . time_axis ] * n_blob ) , 1 , int ( blob_dim [ self . freq_axis ] ) ) else : updated_blob_dim = [ int ( i ) for i in blob_dim ] blob_start = self . _find_blob_start ( ) blob = np . zeros ( updated_blob_dim , dtype = self . _d_type ) if self . f_start == self . f_begin and self . f_stop == self . f_end : blob_flat_size = np . prod ( blob_dim ) updated_blob_flat_size = np . prod ( updated_blob_dim ) with open ( self . filename , 'rb' ) as f : f . seek ( int ( self . idx_data + self . _n_bytes * ( blob_start + n_blob * blob_flat_size ) ) ) dd = np . fromfile ( f , count = updated_blob_flat_size , dtype = self . _d_type ) if dd . shape [ 0 ] == updated_blob_flat_size : blob = dd . reshape ( updated_blob_dim ) else : logger . info ( 'DD shape != blob shape.' ) blob = dd . reshape ( ( int ( dd . shape [ 0 ] / blob_dim [ self . freq_axis ] ) , blob_dim [ self . beam_axis ] , blob_dim [ self . freq_axis ] ) ) else : for blobt in range ( updated_blob_dim [ self . time_axis ] ) : #Load binary data with open ( self . filename , 'rb' ) as f : f . seek ( int ( self . idx_data + self . _n_bytes * ( blob_start + n_blob * blob_dim [ self . time_axis ] * self . n_channels_in_file + blobt * self . n_channels_in_file ) ) ) dd = np . fromfile ( f , count = blob_dim [ self . freq_axis ] , dtype = self . _d_type ) blob [ blobt ] = dd return blob
def read_data ( self , f_start = None , f_stop = None , t_start = None , t_stop = None ) : self . container . read_data ( f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop ) self . __load_data ( )
def __update_header ( self ) : #Updating frequency of first channel from selection if self . header [ b'foff' ] < 0 : self . header [ b'fch1' ] = self . container . f_stop else : self . header [ b'fch1' ] = self . container . f_start #Updating number of coarse channels. self . header [ b'nchans' ] = self . container . selection_shape [ self . freq_axis ] #Updating time stamp for first time bin from selection self . header [ b'tstart' ] = self . container . populate_timestamps ( update_header = True )
def info ( self ) : print ( "\n--- File Info ---" ) for key , val in self . file_header . items ( ) : if key == 'src_raj' : val = val . to_string ( unit = u . hour , sep = ':' ) if key == 'src_dej' : val = val . to_string ( unit = u . deg , sep = ':' ) print ( "%16s : %32s" % ( key , val ) ) print ( "\n%16s : %32s" % ( "Num ints in file" , self . n_ints_in_file ) ) print ( "%16s : %32s" % ( "File shape" , self . file_shape ) ) print ( "--- Selection Info ---" ) print ( "%16s : %32s" % ( "Data selection shape" , self . selection_shape ) ) print ( "%16s : %32s" % ( "Minimum freq (MHz)" , self . container . f_start ) ) print ( "%16s : %32s" % ( "Maximum freq (MHz)" , self . container . f_stop ) )
def __get_chunk_dimensions ( self ) : #Usually '.0000.' is in self.filename if np . abs ( self . header [ b'foff' ] ) < 1e-5 : logger . info ( 'Detecting high frequency resolution data.' ) chunk_dim = ( 1 , 1 , 1048576 ) #1048576 is the number of channels in a coarse channel. return chunk_dim #Usually '.0001.' is in self.filename elif np . abs ( self . header [ b'tsamp' ] ) < 1e-3 : logger . info ( 'Detecting high time resolution data.' ) chunk_dim = ( 2048 , 1 , 512 ) #512 is the total number of channels per single band (ie. blc00) return chunk_dim #Usually '.0002.' is in self.filename elif np . abs ( self . header [ b'foff' ] ) < 1e-2 and np . abs ( self . header [ b'foff' ] ) >= 1e-5 : logger . info ( 'Detecting intermediate frequency and time resolution data.' ) chunk_dim = ( 10 , 1 , 65536 ) #65536 is the total number of channels per single band (ie. blc00) return chunk_dim else : logger . warning ( 'File format not known. Will use minimum chunking. NOT OPTIMAL.' ) chunk_dim = ( 1 , 1 , 512 ) return chunk_dim
def cmd_tool ( args = None ) : from argparse import ArgumentParser parser = ArgumentParser ( description = "Command line utility for creating spectra from GuppiRaw files." ) parser . add_argument ( 'filename' , type = str , help = 'Name of file to read' ) parser . add_argument ( '-o' , dest = 'outdir' , type = str , default = './' , help = 'output directory for PNG files' ) args = parser . parse_args ( ) r = GuppiRaw ( args . filename ) r . print_stats ( ) bname = os . path . splitext ( os . path . basename ( args . filename ) ) [ 0 ] bname = os . path . join ( args . outdir , bname ) r . plot_histogram ( filename = "%s_hist.png" % bname ) r . plot_spectrum ( filename = "%s_spec.png" % bname )
def print_stats ( self ) : header , data = self . read_next_data_block ( ) data = data . view ( 'float32' ) print ( "AVG: %2.3f" % data . mean ( ) ) print ( "STD: %2.3f" % data . std ( ) ) print ( "MAX: %2.3f" % data . max ( ) ) print ( "MIN: %2.3f" % data . min ( ) ) import pylab as plt
def plot_histogram ( self , filename = None ) : header , data = self . read_next_data_block ( ) data = data . view ( 'float32' ) plt . figure ( "Histogram" ) plt . hist ( data . flatten ( ) , 65 , facecolor = '#cc0000' ) if filename : plt . savefig ( filename ) plt . show ( )
def generate_filterbank_header ( self , nchans = 1 , ) : gp_head = self . read_first_header ( ) fb_head = { } telescope_str = gp_head . get ( "TELESCOP" , "unknown" ) if telescope_str in ( 'GBT' , 'GREENBANK' ) : fb_head [ "telescope_id" ] = 6 elif telescope_str in ( 'PKS' , 'PARKES' ) : fb_head [ "telescop_id" ] = 7 else : fb_head [ "telescop_id" ] = 0 fb_head [ "source_name" ] = gp_head . get ( "SRC_NAME" , "unknown" ) fb_head [ "az_start" ] = gp_head . get ( "AZ" , 0 ) fb_head [ "za_start" ] = gp_head . get ( "ZA" , 0 ) fb_head [ "src_raj" ] = Angle ( str ( gp_head . get ( "RA" , 0.0 ) ) + "hr" ) fb_head [ "src_dej" ] = Angle ( str ( gp_head . get ( "DEC" , 0.0 ) ) + "deg" ) fb_head [ "rawdatafile" ] = self . filename fb_head [ "machine_id" ] = 20 fb_head [ "data_type" ] = 1 fb_head [ "barycentric" ] = 0 fb_head [ "pulsarcentric" ] = 0 fb_head [ "nbits" ] = 32 fb_head [ "tstart" ] = 0.0 fb_head [ "tsamp" ] = 1.0 fb_head [ "fch1" ] = 0.0 fb_head [ "foff" ] = 187.5 / nchans fb_head [ "nchans" ] = nchans fb_head [ "nifs" ] = 1 fb_head [ "nbeams" ] = 1 return fb_head
def find_header_size ( filename ) : filfile = open ( filename , 'rb' ) filfile . seek ( 0 ) #read some region larger than the header. round1 = filfile . read ( 1000 ) headersize = round1 . find ( 'HEADER_END' ) + len ( 'HEADER_END' ) return headersize
def cmd_tool ( args = None ) : if 'bl' in local_host : header_loc = '/usr/local/sigproc/bin/header' #Current location of header command in GBT. else : raise IOError ( 'Script only able to run in BL systems.' ) p = OptionParser ( ) p . set_usage ( 'matchfils <FIL_FILE1> <FIL_FILE2>' ) opts , args = p . parse_args ( sys . argv [ 1 : ] ) file1 = args [ 0 ] file2 = args [ 1 ] #------------------------------------ #Create batch script make_batch_script ( ) #------------------------------------ #First checksum headersize1 = find_header_size ( file1 ) file_size1 = os . path . getsize ( file1 ) #Strip header from file, and calculate the md5sum of the rest. #command=['tail','-c',str(file_size1-headersize1),file1,'|','md5sum'] command = [ './tail_sum.sh' , file1 , str ( file_size1 - headersize1 ) ] print ( '[matchfils] ' + ' ' . join ( command ) ) proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( out , err ) = proc . communicate ( ) check_sum1 = out . split ( ) [ 0 ] print ( '[matchfils] Checksum is:' , check_sum1 ) if err : raise Error ( 'There is an error.' ) #--- out , err = reset_outs ( ) command = [ header_loc , file1 ] print ( '[matchfils] Header information:' ) proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( out , err ) = proc . communicate ( ) header1 = out print ( header1 ) #------------------------------------ #Second checksum out , err = reset_outs ( ) headersize2 = find_header_size ( file2 ) file_size2 = os . path . getsize ( file2 ) #Strip header from file, and calculate the md5sum of the rest. command = [ './tail_sum.sh' , file2 , str ( file_size2 - headersize2 ) ] print ( '[matchfils] ' + ' ' . join ( command ) ) proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( out , err ) = proc . communicate ( ) check_sum2 = out . split ( ) [ 0 ] print ( '[matchfils] Checksum is:' , check_sum2 ) if err : raise Error ( 'There is an error.' ) #--- out , err = reset_outs ( ) command = [ header_loc , file2 ] print ( '[matchfils] Header information:' ) proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( out , err ) = proc . communicate ( ) header2 = out print ( header2 ) #------------------------------------ #check the checksums if check_sum1 != check_sum2 : print ( '[matchfils] Booo! Checksum does not match between files.' ) else : print ( '[matchfils] Hooray! Checksum matches between files.' ) #------------------------------------ #Remove batch script os . remove ( 'tail_sum.sh' )
def cmd_tool ( args = None ) : from argparse import ArgumentParser if not HAS_BITSHUFFLE : print ( "Error: the bitshuffle library is required to run this script." ) exit ( ) parser = ArgumentParser ( description = "Command line utility for creating HDF5 Raw files." ) parser . add_argument ( 'filename' , type = str , help = 'Name of filename to read' ) args = parser . parse_args ( ) fileroot = args . filename . split ( '.0000.raw' ) [ 0 ] filelist = glob . glob ( fileroot + '*.raw' ) filelist = sorted ( filelist ) r = GuppiRaw ( filelist [ 0 ] ) header , data = r . read_next_data_block ( ) dshape = data . shape #r.read_next_data_block_shape() print ( dshape ) n_blocks_total = 0 for filename in filelist : print ( filename ) r = GuppiRaw ( filename ) n_blocks_total += r . n_blocks print ( n_blocks_total ) full_dshape = np . concatenate ( ( ( n_blocks_total , ) , dshape ) ) h5 = h5py . File ( fileroot + '.h5' , 'w' ) h5 . attrs [ 'CLASS' ] = 'GUPPIRAW' block_size = 0 dset = h5 . create_dataset ( 'data' , shape = full_dshape , #compression=bitshuffle.h5.H5FILTER, #compression_opts=(block_size, bitshuffle.h5.H5_COMPRESS_LZ4), dtype = data . dtype ) h5_idx = 0 for filename in filelist : print ( "\nReading %s header..." % filename ) r = GuppiRaw ( filename ) h5 = h5py . File ( filename + '.h5' , 'w' ) header , data = r . read_next_data_block ( ) for ii in range ( 0 , r . n_blocks ) : t0 = time . time ( ) print ( "Reading block %i of %i" % ( h5_idx + 1 , full_dshape [ 0 ] ) ) header , data = r . read_next_data_block ( ) t1 = time . time ( ) t2 = time . time ( ) print ( "Writing block %i of %i" % ( h5_idx + 1 , full_dshape [ 0 ] ) ) dset [ h5_idx , : ] = data t3 = time . time ( ) print ( "Read: %2.2fs, Write %2.2fs" % ( ( t1 - t0 ) , ( t3 - t2 ) ) ) h5_idx += 1 for key , value in header . items ( ) : dset . attrs [ key ] = value h5 . close ( ) t1 = time . time ( ) print ( "Conversion time: %2.2fs" % ( t1 - t0 ) )
def is_filterbank ( filename ) : with open ( filename , 'rb' ) as fh : is_fil = True try : keyword , value , idx = read_next_header_keyword ( fh ) try : assert keyword == b'HEADER_START' except AssertionError : is_fil = False except KeyError : is_fil = False return is_fil
def to_sigproc_angle ( angle_val ) : x = str ( angle_val ) if '.' in x : if 'h' in x : d , m , s , ss = int ( x [ 0 : x . index ( 'h' ) ] ) , int ( x [ x . index ( 'h' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( '.' ) ] ) , float ( x [ x . index ( '.' ) : x . index ( 's' ) ] ) if 'd' in x : d , m , s , ss = int ( x [ 0 : x . index ( 'd' ) ] ) , int ( x [ x . index ( 'd' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( '.' ) ] ) , float ( x [ x . index ( '.' ) : x . index ( 's' ) ] ) else : if 'h' in x : d , m , s = int ( x [ 0 : x . index ( 'h' ) ] ) , int ( x [ x . index ( 'h' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( 's' ) ] ) if 'd' in x : d , m , s = int ( x [ 0 : x . index ( 'd' ) ] ) , int ( x [ x . index ( 'd' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( 's' ) ] ) ss = 0 num = str ( d ) . zfill ( 2 ) + str ( m ) . zfill ( 2 ) + str ( s ) . zfill ( 2 ) + '.' + str ( ss ) . split ( "." ) [ - 1 ] return np . float64 ( num ) . tostring ( )
def calc_n_ints_in_file ( filename ) : h = read_header ( filename ) n_bytes = int ( h [ b'nbits' ] / 8 ) n_chans = h [ b'nchans' ] n_ifs = h [ b'nifs' ] idx_data = len_header ( filename ) f = open ( filename , 'rb' ) f . seek ( idx_data ) filesize = os . path . getsize ( filename ) n_bytes_data = filesize - idx_data if h [ b'nbits' ] == 2 : n_ints = int ( 4 * n_bytes_data / ( n_chans * n_ifs ) ) else : n_ints = int ( n_bytes_data / ( n_bytes * n_chans * n_ifs ) ) return n_ints
def to_dict ( self ) : if self . tb_next is None : tb_next = None else : tb_next = self . tb_next . to_dict ( ) code = { 'co_filename' : self . tb_frame . f_code . co_filename , 'co_name' : self . tb_frame . f_code . co_name , } frame = { 'f_globals' : self . tb_frame . f_globals , 'f_code' : code , } return { 'tb_frame' : frame , 'tb_lineno' : self . tb_lineno , 'tb_next' : tb_next , }
def make_rr_subparser ( subparsers , rec_type , args_and_types ) : sp = subparsers . add_parser ( rec_type ) sp . add_argument ( "name" , type = str ) sp . add_argument ( "ttl" , type = int , nargs = '?' ) sp . add_argument ( rec_type , type = str ) for my_spec in args_and_types : ( argname , argtype ) = my_spec [ : 2 ] if len ( my_spec ) > 2 : nargs = my_spec [ 2 ] sp . add_argument ( argname , type = argtype , nargs = nargs ) else : sp . add_argument ( argname , type = argtype ) return sp
def make_parser ( ) : line_parser = ZonefileLineParser ( ) subparsers = line_parser . add_subparsers ( ) sp = subparsers . add_parser ( "$ORIGIN" ) sp . add_argument ( "$ORIGIN" , type = str ) sp = subparsers . add_parser ( "$TTL" ) sp . add_argument ( "$TTL" , type = int ) args_and_types = [ ( "mname" , str ) , ( "rname" , str ) , ( "serial" , int ) , ( "refresh" , int ) , ( "retry" , int ) , ( "expire" , int ) , ( "minimum" , int ) ] make_rr_subparser ( subparsers , "SOA" , args_and_types ) make_rr_subparser ( subparsers , "NS" , [ ( "host" , str ) ] ) make_rr_subparser ( subparsers , "A" , [ ( "ip" , str ) ] ) make_rr_subparser ( subparsers , "AAAA" , [ ( "ip" , str ) ] ) make_rr_subparser ( subparsers , "CNAME" , [ ( "alias" , str ) ] ) make_rr_subparser ( subparsers , "ALIAS" , [ ( "host" , str ) ] ) make_rr_subparser ( subparsers , "MX" , [ ( "preference" , str ) , ( "host" , str ) ] ) make_txt_subparser ( subparsers ) make_rr_subparser ( subparsers , "PTR" , [ ( "host" , str ) ] ) make_rr_subparser ( subparsers , "SRV" , [ ( "priority" , int ) , ( "weight" , int ) , ( "port" , int ) , ( "target" , str ) ] ) make_rr_subparser ( subparsers , "SPF" , [ ( "data" , str ) ] ) make_rr_subparser ( subparsers , "URI" , [ ( "priority" , int ) , ( "weight" , int ) , ( "target" , str ) ] ) return line_parser
def remove_comments ( text ) : ret = [ ] lines = text . split ( "\n" ) for line in lines : if len ( line ) == 0 : continue line = serialize ( tokenize_line ( line ) ) ret . append ( line ) return "\n" . join ( ret )
def parse_zone_file ( text , ignore_invalid = False ) : text = remove_comments ( text ) text = flatten ( text ) text = remove_class ( text ) text = add_default_name ( text ) json_zone_file = parse_lines ( text , ignore_invalid = ignore_invalid ) return json_zone_file
def process_origin ( data , template ) : record = "" if data is not None : record += "$ORIGIN %s" % data return template . replace ( "{$origin}" , record )
def process_ttl ( data , template ) : record = "" if data is not None : record += "$TTL %s" % data return template . replace ( "{$ttl}" , record )
def process_soa ( data , template ) : record = template [ : ] if data is not None : assert len ( data ) == 1 , "Only support one SOA RR at this time" data = data [ 0 ] soadat = [ ] domain_fields = [ 'mname' , 'rname' ] param_fields = [ 'serial' , 'refresh' , 'retry' , 'expire' , 'minimum' ] for f in domain_fields + param_fields : assert f in data . keys ( ) , "Missing '%s' (%s)" % ( f , data ) data_name = str ( data . get ( 'name' , '@' ) ) soadat . append ( data_name ) if data . get ( 'ttl' ) is not None : soadat . append ( str ( data [ 'ttl' ] ) ) soadat . append ( "IN" ) soadat . append ( "SOA" ) for key in domain_fields : value = str ( data [ key ] ) soadat . append ( value ) soadat . append ( "(" ) for key in param_fields : value = str ( data [ key ] ) soadat . append ( value ) soadat . append ( ")" ) soa_txt = " " . join ( soadat ) record = record . replace ( "{soa}" , soa_txt ) else : record = record . replace ( "{soa}" , "" ) return record
def process_txt ( data , template ) : if data is None : to_process = None else : to_process = copy . deepcopy ( data ) for datum in to_process : if isinstance ( datum [ "txt" ] , list ) : datum [ "txt" ] = " " . join ( [ '"%s"' % entry . replace ( ";" , "\;" ) for entry in datum [ "txt" ] ] ) else : datum [ "txt" ] = '"%s"' % datum [ "txt" ] . replace ( ";" , "\;" ) return process_rr ( to_process , "TXT" , "txt" , "{txt}" , template )
def parse_schema_string ( schema_string ) : if isinstance ( schema_string , str ) : schema_string = schema_string . decode ( "utf8" ) schema_struct = json . loads ( schema_string ) return AvroSchemaParser ( ) . parse_schema_struct ( schema_struct )
def to_json_compatible ( record ) : d = { } for fname , f in record . _fields . iteritems ( ) : val = getattr ( record , fname ) if val is not None : d [ fname ] = f . dump ( val ) return d
def from_json_compatible ( schema , dct ) : kwargs = { } for key in dct : field_type = schema . _fields . get ( key ) if field_type is None : raise ParseError ( "Unexpected field encountered in line for record %s: %s" % ( schema . __name__ , key ) ) kwargs [ key ] = field_type . load ( dct [ key ] ) return schema ( * * kwargs )
def from_json_compatible ( schema , dct ) : kwargs = { } for key in dct : field_type = schema . _fields . get ( key ) if field_type is None : warnings . warn ( "Unexpected field encountered in line for record %s: %r" % ( schema . __name__ , key ) ) continue kwargs [ key ] = field_type . avro_load ( dct [ key ] ) return schema ( * * kwargs )
def all_include_attributes ( self , attributes ) : self . reload ( expand = True , attributes = attributes ) entities = [ Entity ( self , r , attributes = attributes ) for r in self . _resources ] self . reload ( ) return entities
def give_another_quote ( q ) : for qc in QUOTES : if qc != q : return qc else : raise ValueError ( u'Could not find a different quote for {}' . format ( q ) )
def parseCommandLineArguments ( ) : parser = argparse . ArgumentParser ( description = "Plot predicted Gaia sky averaged proper motion errors as a function of V" ) parser . add_argument ( "-p" , action = "store_true" , dest = "pdfOutput" , help = "Make PDF plot" ) parser . add_argument ( "-b" , action = "store_true" , dest = "pngOutput" , help = "Make PNG plot" ) parser . add_argument ( "-g" , action = "store_true" , dest = "gmagAbscissa" , help = "Plot performance vs G instead of V" ) args = vars ( parser . parse_args ( ) ) return args
def parseCommandLineArguments ( ) : parser = argparse . ArgumentParser ( description = "Calculate parallax error for given G and (V-I)" ) parser . add_argument ( "gmag" , help = "G-band magnitude of source" , type = float ) parser . add_argument ( "vmini" , help = "(V-I) colour of source" , type = float ) args = vars ( parser . parse_args ( ) ) return args
def _uniquote ( value ) : if isinstance ( value , six . binary_type ) : try : value = value . decode ( 'utf-8' ) except UnicodeDecodeError : value = six . text_type ( _dequote ( repr ( value ) ) ) result = six . text_type ( value ) if isinstance ( value , six . text_type ) : result = "'%s'" % result return result
def serach_path ( ) : operating_system = get_os ( ) return [ os . path . expanduser ( "~/.kerncraft/iaca/{}/" . format ( operating_system ) ) , os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/iaca/{}/' . format ( operating_system ) ]
def build_minimal_runs ( events ) : events = [ e for i , e in enumerate ( events ) if events . index ( e ) == i ] scheduled_runs = { } scheduled_events = [ ] cur_run = 0 while len ( scheduled_events ) != len ( events ) : for event_tpl in events : event , registers , parameters = event_tpl if event_tpl in scheduled_events : continue for possible_reg in register_options ( registers ) : s = scheduled_runs . setdefault ( cur_run , { } ) if possible_reg not in s : s [ possible_reg ] = ( event , possible_reg , parameters ) scheduled_events . append ( event_tpl ) break cur_run += 1 runs = [ list ( v . values ( ) ) for v in scheduled_runs . values ( ) ] return runs
def report ( self , output_file = sys . stdout ) : max_perf = self . results [ 'max_perf' ] if self . _args and self . _args . verbose >= 3 : print ( '{}' . format ( pformat ( self . results ) ) , file = output_file ) if self . _args and self . _args . verbose >= 1 : print ( '{}' . format ( pformat ( self . results [ 'verbose infos' ] ) ) , file = output_file ) print ( 'Bottlenecks:' , file = output_file ) print ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output_file ) print ( '--------+--------------+-----------------+-------------------+----------------------' , file = output_file ) print ( '    CPU |              | {!s:>15} |                   |' . format ( max_perf [ self . _args . unit ] ) , file = output_file ) for b in self . results [ 'mem bottlenecks' ] : print ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . _args . unit ] , * * b ) , file = output_file ) print ( '' , file = output_file ) if self . results [ 'min performance' ] [ 'FLOP/s' ] > max_perf [ 'FLOP/s' ] : print ( 'CPU bound. {!s} due to CPU max. FLOP/s' . format ( max_perf ) , file = output_file ) else : print ( 'Cache or mem bound.' , file = output_file ) bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] print ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . _args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output_file ) print ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output_file )
def analyze ( self ) : self . results = self . calculate_cache_access ( ) try : iaca_analysis , asm_block = self . kernel . iaca_analysis ( micro_architecture = self . machine [ 'micro-architecture' ] , asm_block = self . asm_block , pointer_increment = self . pointer_increment , verbose = self . verbose > 2 ) except RuntimeError as e : print ( "IACA analysis failed: " + str ( e ) ) sys . exit ( 1 ) block_throughput = iaca_analysis [ 'throughput' ] uops = iaca_analysis [ 'uops' ] iaca_output = iaca_analysis [ 'output' ] port_cycles = iaca_analysis [ 'port cycles' ] elements_per_block = abs ( asm_block [ 'pointer_increment' ] / self . kernel . datatypes_size [ self . kernel . datatype ] ) block_size = elements_per_block * self . kernel . datatypes_size [ self . kernel . datatype ] try : block_to_cl_ratio = float ( self . machine [ 'cacheline size' ] ) / block_size except ZeroDivisionError as e : print ( "Too small block_size / pointer_increment:" , e , file = sys . stderr ) sys . exit ( 1 ) port_cycles = dict ( [ ( i [ 0 ] , i [ 1 ] * block_to_cl_ratio ) for i in list ( port_cycles . items ( ) ) ] ) uops = uops * block_to_cl_ratio cl_throughput = block_throughput * block_to_cl_ratio flops_per_element = sum ( self . kernel . _flops . values ( ) ) self . results [ 'mem bottlenecks' ] [ 0 ] = None self . results [ 'min performance' ] = self . conv_perf ( PrefixedUnit ( float ( 'inf' ) , 'FLOP/s' ) ) self . results [ 'bottleneck level' ] = None for level , bottleneck in enumerate ( self . results [ 'mem bottlenecks' ] ) : if level == 0 : continue if bottleneck [ 'performance' ] [ 'FLOP/s' ] < self . results [ 'min performance' ] [ 'FLOP/s' ] : self . results [ 'bottleneck level' ] = level self . results [ 'min performance' ] = bottleneck [ 'performance' ] self . results . update ( { 'cpu bottleneck' : { 'port cycles' : port_cycles , 'cl throughput' : cl_throughput , 'uops' : uops , 'performance throughput' : self . conv_perf ( PrefixedUnit ( self . machine [ 'clock' ] / block_throughput * elements_per_block * flops_per_element * self . cores , "FLOP/s" ) ) , 'IACA output' : iaca_output } } )
def report ( self , output_file = sys . stdout ) : cpu_perf = self . results [ 'cpu bottleneck' ] [ 'performance throughput' ] if self . verbose >= 3 : print ( '{}' . format ( pformat ( self . results ) ) , file = output_file ) if self . verbose >= 1 : print ( 'Bottlenecks:' , file = output_file ) print ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output_file ) print ( '--------+--------------+-----------------+-------------------+----------------------' , file = output_file ) print ( '    CPU |              | {!s:>15} |                   |' . format ( cpu_perf [ self . _args . unit ] ) , file = output_file ) for b in self . results [ 'mem bottlenecks' ] : if b is None : continue print ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . _args . unit ] , * * b ) , file = output_file ) print ( '' , file = output_file ) print ( 'IACA analisys:' , file = output_file ) print ( '{!s}' . format ( { k : v for k , v in list ( self . results [ 'cpu bottleneck' ] . items ( ) ) if k not in [ 'IACA output' ] } ) , file = output_file ) if self . results [ 'min performance' ] [ 'FLOP/s' ] > cpu_perf [ 'FLOP/s' ] : print ( 'CPU bound. {!s} due to CPU bottleneck' . format ( cpu_perf [ self . _args . unit ] ) , file = output_file ) else : print ( 'Cache or mem bound.' , file = output_file ) bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] print ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . _args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output_file ) print ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output_file )
def analyze ( self ) : loop_stack = list ( self . kernel . get_loop_stack ( ) ) if any ( [ l [ 'increment' ] != 1 for l in loop_stack ] ) : raise ValueError ( "Can not apply layer condition, since not all loops are of step " "length 1." ) for aref in list ( self . kernel . index_order ( ) ) : while aref and len ( aref [ 0 ] ) == 0 : aref . pop ( 0 ) for i , idx_names in enumerate ( aref ) : if i >= len ( loop_stack ) or any ( [ loop_stack [ i ] [ 'index' ] != idx . name for idx in idx_names ] ) : raise ValueError ( "Can not apply layer condition, order of indices in array " "does not follow order of loop indices. Single-dimension is " "currently not supported." ) for arefs in chain ( chain ( * self . kernel . sources . values ( ) ) , chain ( * self . kernel . destinations . values ( ) ) ) : if not arefs : continue while arefs and not arefs [ 0 ] . free_symbols : arefs = arefs [ 1 : ] for i , expr in enumerate ( arefs ) : diff = sympy . diff ( expr , sympy . Symbol ( loop_stack [ i ] [ 'index' ] ) ) if diff != 0 and diff != 1 : raise ValueError ( "Can not apply layer condition, array references may not " "increment more then one per iteration." ) self . results = self . calculate_cache_access ( )
def report ( self , output_file = sys . stdout ) : if self . _args and self . _args . verbose > 2 : pprint ( self . results ) for dimension , lc_info in self . results [ 'dimensions' ] . items ( ) : print ( "{}D layer condition:" . format ( dimension ) , file = output_file ) for cache , lc_solution in sorted ( lc_info [ 'caches' ] . items ( ) ) : print ( cache + ": " , end = '' , file = output_file ) if lc_solution [ 'lt' ] is sympy . true : print ( "unconditionally fulfilled" , file = output_file ) else : if lc_solution [ 'eq' ] is None : print ( "{}" . format ( lc_solution [ 'lt' ] ) , file = output_file ) elif type ( lc_solution [ 'eq' ] ) is not list : print ( "{}" . format ( lc_solution [ 'eq' ] ) , file = output_file ) else : for solu in lc_solution [ 'eq' ] : for s , v in solu . items ( ) : print ( "{} <= {}" . format ( s , v ) , file = output_file )
def round_to_next ( x , base ) : return int ( base * math . ceil ( float ( x ) / base ) )
def calculate_cache_access ( self ) : self . results . update ( { 'cycles' : [ ] , 'misses' : self . predictor . get_misses ( ) , 'hits' : self . predictor . get_hits ( ) , 'evicts' : self . predictor . get_evicts ( ) , 'verbose infos' : self . predictor . get_infos ( ) } )
def analyze ( self ) : self . calculate_cache_access ( ) self . calculate_cycles ( ) self . results [ 'flops per iteration' ] = sum ( self . kernel . _flops . values ( ) ) return self . results
def report ( self , output_file = sys . stdout ) : if self . verbose > 1 : print ( '{}' . format ( pprint . pformat ( self . results [ 'verbose infos' ] ) ) , file = output_file ) for level , cycles in self . results [ 'cycles' ] : print ( '{} = {}' . format ( level , self . conv_cy ( cycles ) [ self . _args . unit ] ) , file = output_file ) if self . verbose > 1 : if 'memory bandwidth kernel' in self . results : print ( 'memory cycles based on {} kernel with {}' . format ( self . results [ 'memory bandwidth kernel' ] , self . results [ 'memory bandwidth' ] ) , file = output_file ) if self . verbose > 1 : print ( file = output_file ) print ( self . report_data_transfers ( ) , file = output_file )
def analyze ( self ) : try : incore_analysis , asm_block = self . kernel . iaca_analysis ( micro_architecture = self . machine [ 'micro-architecture' ] , asm_block = self . asm_block , pointer_increment = self . pointer_increment , verbose = self . verbose > 2 ) except RuntimeError as e : print ( "IACA analysis failed: " + str ( e ) ) sys . exit ( 1 ) block_throughput = incore_analysis [ 'throughput' ] port_cycles = incore_analysis [ 'port cycles' ] uops = incore_analysis [ 'uops' ] elements_per_block = abs ( asm_block [ 'pointer_increment' ] // self . kernel . datatypes_size [ self . kernel . datatype ] ) block_size = elements_per_block * self . kernel . datatypes_size [ self . kernel . datatype ] try : block_to_cl_ratio = float ( self . machine [ 'cacheline size' ] ) / block_size except ZeroDivisionError as e : print ( "Too small block_size / pointer_increment:" , e , file = sys . stderr ) sys . exit ( 1 ) port_cycles = dict ( [ ( i [ 0 ] , i [ 1 ] * block_to_cl_ratio ) for i in list ( port_cycles . items ( ) ) ] ) uops = uops * block_to_cl_ratio cl_throughput = block_throughput * block_to_cl_ratio T_OL = max ( [ v for k , v in list ( port_cycles . items ( ) ) if k in self . machine [ 'overlapping model' ] [ 'ports' ] ] ) T_nOL = max ( [ v for k , v in list ( port_cycles . items ( ) ) if k in self . machine [ 'non-overlapping model' ] [ 'ports' ] ] ) if T_nOL < cl_throughput : T_OL = cl_throughput self . results = { 'port cycles' : port_cycles , 'cl throughput' : self . conv_cy ( cl_throughput ) , 'uops' : uops , 'T_nOL' : T_nOL , 'T_OL' : T_OL , 'IACA output' : incore_analysis [ 'output' ] , 'elements_per_block' : elements_per_block , 'pointer_increment' : asm_block [ 'pointer_increment' ] , 'flops per iteration' : sum ( self . kernel . _flops . values ( ) ) } return self . results
def report ( self , output_file = sys . stdout ) : if self . verbose > 2 : print ( "IACA Output:" , file = output_file ) print ( self . results [ 'IACA output' ] , file = output_file ) print ( '' , file = output_file ) if self . verbose > 1 : print ( 'Detected pointer increment: {}' . format ( self . results [ 'pointer_increment' ] ) , file = output_file ) print ( 'Derived elements stored to per asm block iteration: {}' . format ( self . results [ 'elements_per_block' ] ) , file = output_file ) print ( 'Ports and cycles:' , str ( self . results [ 'port cycles' ] ) , file = output_file ) print ( 'Uops:' , str ( self . results [ 'uops' ] ) , file = output_file ) print ( 'Throughput: {}' . format ( self . results [ 'cl throughput' ] [ self . _args . unit ] ) , file = output_file ) print ( 'T_nOL = {:.1f} cy/CL' . format ( self . results [ 'T_nOL' ] ) , file = output_file ) print ( 'T_OL = {:.1f} cy/CL' . format ( self . results [ 'T_OL' ] ) , file = output_file )
def strip_and_uncomment ( asm_lines ) : asm_stripped = [ ] for line in asm_lines : asm_stripped . append ( line . split ( '#' ) [ 0 ] . strip ( ) ) return asm_stripped
def strip_unreferenced_labels ( asm_lines ) : asm_stripped = [ ] for line in asm_lines : if re . match ( r'^\S+:' , line ) : label = line [ 0 : line . find ( ':' ) ] if not any ( [ re . match ( r'^[^#]*\s' + re . escape ( label ) + '[\s,]?.*$' , l ) for l in asm_lines ] ) : line = '' asm_stripped . append ( line ) return asm_stripped
def select_best_block ( blocks ) : if not blocks : raise ValueError ( "No suitable blocks were found in assembly." ) best_block = max ( blocks , key = lambda b : b [ 1 ] [ 'packed_instr' ] ) if best_block [ 1 ] [ 'packed_instr' ] == 0 : best_block = max ( blocks , key = lambda b : ( b [ 1 ] [ 'ops' ] + b [ 1 ] [ 'packed_instr' ] + b [ 1 ] [ 'avx_instr' ] , b [ 1 ] [ 'ZMM' ] , b [ 1 ] [ 'YMM' ] , b [ 1 ] [ 'XMM' ] ) ) return best_block [ 0 ]
def userselect_increment ( block ) : print ( "Selected block:" ) print ( '\n    ' + ( '\n    ' . join ( block [ 'lines' ] ) ) ) print ( ) increment = None while increment is None : increment = input ( "Choose store pointer increment (number of bytes): " ) try : increment = int ( increment ) except ValueError : increment = None block [ 'pointer_increment' ] = increment return increment
def userselect_block ( blocks , default = None , debug = False ) : print ( "Blocks found in assembly file:" ) print ( "      block     | OPs | pck. | AVX || Registers |    ZMM   |    YMM   |    XMM   |    GP   ||ptr.inc|\n" "----------------+-----+------+-----++-----------+----------+----------+----------+---------++-------|" ) for idx , b in blocks : print ( '{:>2} {b[labels]!r:>12} | {b[ops]:>3} | {b[packed_instr]:>4} | {b[avx_instr]:>3} |' '| {b[regs][0]:>3} ({b[regs][1]:>3}) | {b[ZMM][0]:>3} ({b[ZMM][1]:>2}) | ' '{b[YMM][0]:>3} ({b[YMM][1]:>2}) | ' '{b[XMM][0]:>3} ({b[XMM][1]:>2}) | {b[GP][0]:>2} ({b[GP][1]:>2}) || ' '{b[pointer_increment]!s:>5} |' . format ( idx , b = b ) ) if debug : ln = b [ 'first_line' ] print ( ' ' * 4 + 'Code:' ) for l in b [ 'lines' ] : print ( ' ' * 8 + '{:>5} | {}' . format ( ln , l ) ) ln += 1 print ( ' ' * 4 + 'Metadata:' ) print ( textwrap . indent ( pformat ( { k : v for k , v in b . items ( ) if k not in [ 'lines' ] } ) , ' ' * 8 ) ) block_idx = - 1 while not ( 0 <= block_idx < len ( blocks ) ) : block_idx = input ( "Choose block to be marked [" + str ( default ) + "]: " ) or default try : block_idx = int ( block_idx ) except ValueError : block_idx = - 1 return block_idx
def insert_markers ( asm_lines , start_line , end_line ) : asm_lines = ( asm_lines [ : start_line ] + START_MARKER + asm_lines [ start_line : end_line + 1 ] + END_MARKER + asm_lines [ end_line + 1 : ] ) return asm_lines
def main ( ) : parser = argparse . ArgumentParser ( description = 'Find and analyze basic loop blocks and mark for IACA.' , epilog = '/RRZE-HPC/kerncraft\nLicense: AGPLv3' ) parser . add_argument ( '--version' , action = 'version' , version = '%(prog)s {}' . format ( __version__ ) ) parser . add_argument ( 'source' , type = argparse . FileType ( ) , nargs = '?' , default = sys . stdin , help = 'assembly file to analyze (default: stdin)' ) parser . add_argument ( '--outfile' , '-o' , type = argparse . FileType ( 'w' ) , nargs = '?' , default = sys . stdout , help = 'output file location (default: stdout)' ) parser . add_argument ( '--debug' , action = 'store_true' , help = 'Output nternal analysis information for debugging.' ) args = parser . parse_args ( ) iaca_instrumentation ( input_file = args . source , output_file = args . outfile , block_selection = 'manual' , pointer_increment = 1 , debug = args . debug )
def simulate ( kernel , model , define_dict , blocking_constant , blocking_length ) : kernel . clear_state ( ) for k , v in define_dict . items ( ) : kernel . set_constant ( k , v ) kernel . set_constant ( blocking_constant , blocking_length ) model . analyze ( ) return sum ( [ cy for dscr , cy in model . results [ 'cycles' ] ] )
def get_last_modified_datetime ( dir_path = os . path . dirname ( __file__ ) ) : max_mtime = 0 for root , dirs , files in os . walk ( dir_path ) : for f in files : p = os . path . join ( root , f ) try : max_mtime = max ( max_mtime , os . stat ( p ) . st_mtime ) except FileNotFoundError : pass return datetime . utcfromtimestamp ( max_mtime )
def create_parser ( ) : parser = argparse . ArgumentParser ( description = 'Analytical performance modelling and benchmarking toolkit.' , epilog = '/RRZE-HPC/kerncraft\nLicense: AGPLv3' ) parser . add_argument ( '--version' , action = 'version' , version = '%(prog)s {}' . format ( __version__ ) ) parser . add_argument ( '--machine' , '-m' , type = argparse . FileType ( 'r' ) , required = True , help = 'Path to machine description yaml file.' ) parser . add_argument ( '--pmodel' , '-p' , choices = models . __all__ , required = True , action = 'append' , default = [ ] , help = 'Performance model to apply' ) parser . add_argument ( '-D' , '--define' , nargs = 2 , metavar = ( 'KEY' , 'VALUE' ) , default = [ ] , action = AppendStringRange , help = 'Define constant to be used in C code. Values must be integer or ' 'match start-stop[:num[log[base]]]. If range is given, all ' 'permutation s will be tested. Overwrites constants from testcase ' 'file.' ) parser . add_argument ( '--verbose' , '-v' , action = 'count' , default = 0 , help = 'Increases verbosity level.' ) parser . add_argument ( 'code_file' , metavar = 'FILE' , type = argparse . FileType ( ) , help = 'File with loop kernel C code' ) parser . add_argument ( '--asm-block' , metavar = 'BLOCK' , default = 'auto' , help = 'Number of ASM block to mark for IACA, "auto" for automatic ' 'selection or "manual" for interactiv selection.' ) parser . add_argument ( '--pointer-increment' , metavar = 'INCR' , default = 'auto' , type = int_or_str , help = 'Increment of store pointer within one ASM block in bytes. If "auto": ' 'automatic detection, error on failure to detect, if ' '"auto_with_manual_fallback": fallback to manual input, or if ' '"manual": always prompt user.' ) parser . add_argument ( '--store' , metavar = 'PICKLE' , type = argparse . FileType ( 'a+b' ) , help = 'Addes results to PICKLE file for later processing.' ) parser . add_argument ( '--unit' , '-u' , choices = [ 'cy/CL' , 'cy/It' , 'It/s' , 'FLOP/s' ] , help = 'Select the output unit, defaults to model specific if not given.' ) parser . add_argument ( '--cores' , '-c' , metavar = 'CORES' , type = int , default = 1 , help = 'Number of cores to be used in parallel. (default: 1) ' 'ECM model will consider the scaling of the last level cache and ' 'predict the overall performance in addition to single-core behavior. ' 'The benchmark mode will run the code with OpenMP on as many physical ' 'cores.' ) parser . add_argument ( '--kernel-description' , action = 'store_true' , help = 'Use kernel description instead of analyzing the kernel code.' ) parser . add_argument ( '--clean-intermediates' , action = 'store_true' , help = 'If set, will delete all intermediate files after completion.' ) parser . add_argument ( '--cache-predictor' , '-P' , choices = [ 'LC' , 'SIM' ] , default = 'SIM' , help = 'Change cache predictor to use, options are LC (layer conditions) and ' 'SIM (cache simulation with pycachesim), default is SIM.' ) parser . add_argument ( '--compiler' , '-C' , type = str , default = None , help = 'Compiler to use, default is first in machine description file.' ) parser . add_argument ( '--compiler-flags' , type = str , default = None , help = 'Compiler flags to use. If not set, flags are taken from machine ' 'description file (-std=c99 is always added).' ) for m in models . __all__ : ag = parser . add_argument_group ( 'arguments for ' + m + ' model' , getattr ( models , m ) . name ) getattr ( models , m ) . configure_arggroup ( ag ) return parser
def check_arguments ( args , parser ) : if args . asm_block not in [ 'auto' , 'manual' ] : try : args . asm_block = int ( args . asm_block ) except ValueError : parser . error ( '--asm-block can only be "auto", "manual" or an integer' ) if not args . unit : if 'Roofline' in args . pmodel or 'RooflineIACA' in args . pmodel : args . unit = 'FLOP/s' else : args . unit = 'cy/CL'
def main ( ) : parser = create_parser ( ) args = parser . parse_args ( ) check_arguments ( args , parser ) run ( parser , args )
def main ( ) : parser = argparse . ArgumentParser ( description = 'Recursively merges two or more pickle files. Only supports pickles consisting ' 'of a single dictionary object.' ) parser . add_argument ( 'destination' , type = argparse . FileType ( 'r+b' ) , help = 'File to write to and include in resulting pickle. (WILL BE CHANGED)' ) parser . add_argument ( 'source' , type = argparse . FileType ( 'rb' ) , nargs = '+' , help = 'File to include in resulting pickle.' ) args = parser . parse_args ( ) result = pickle . load ( args . destination ) assert isinstance ( result , collections . Mapping ) , "only Mapping types can be handled." for s in args . source : data = pickle . load ( s ) assert isinstance ( data , collections . Mapping ) , "only Mapping types can be handled." update ( result , data ) args . destination . seek ( 0 ) args . destination . truncate ( ) pickle . dump ( result , args . destination )
def symbol_pos_int ( * args , * * kwargs ) : kwargs . update ( { 'positive' : True , 'integer' : True } ) return sympy . Symbol ( * args , * * kwargs )
def find_node_type ( ast , node_type ) : if type ( ast ) is node_type : return [ ast ] elif type ( ast ) is list : return reduce ( operator . add , list ( map ( lambda a : find_node_type ( a , node_type ) , ast ) ) , [ ] ) elif ast is None : return [ ] else : return reduce ( operator . add , [ find_node_type ( o [ 1 ] , node_type ) for o in ast . children ( ) ] , [ ] )
def force_iterable ( f ) : def wrapper ( * args , * * kwargs ) : r = f ( * args , * * kwargs ) if hasattr ( r , '__iter__' ) : return r else : return [ r ] return wrapper
def check ( self ) : datatypes = [ v [ 0 ] for v in self . variables . values ( ) ] assert len ( set ( datatypes ) ) <= 1 , 'mixing of datatypes within a kernel is not supported.'
def subs_consts ( self , expr ) : if isinstance ( expr , numbers . Number ) : return expr else : return expr . subs ( self . constants )
def _remove_duplicate_accesses ( self ) : self . destinations = { var_name : set ( acs ) for var_name , acs in self . destinations . items ( ) } self . sources = { var_name : set ( acs ) for var_name , acs in self . sources . items ( ) }
def get_loop_stack ( self , subs_consts = False ) : for l in self . _loop_stack : if subs_consts : yield { 'index' : l [ 0 ] , 'start' : self . subs_consts ( l [ 1 ] ) , 'stop' : self . subs_consts ( l [ 2 ] ) , 'increment' : self . subs_consts ( l [ 3 ] ) } else : yield { 'index' : l [ 0 ] , 'start' : l [ 1 ] , 'stop' : l [ 2 ] , 'increment' : l [ 3 ] }
def global_iterator ( self ) : global_iterator = sympy . Integer ( 0 ) total_length = sympy . Integer ( 1 ) for var_name , start , end , incr in reversed ( self . _loop_stack ) : loop_var = symbol_pos_int ( var_name ) length = end - start global_iterator += ( loop_var - start ) * total_length total_length *= length return global_iterator
def max_global_iteration ( self ) : return self . indices_to_global_iterator ( { symbol_pos_int ( var_name ) : end - 1 for var_name , start , end , incr in self . _loop_stack } )
def print_kernel_info ( self , output_file = sys . stdout ) : table = ( '     idx |        min        max       step\n' + '---------+---------------------------------\n' ) for l in self . _loop_stack : table += '{:>8} | {!r:>10} {!r:>10} {!r:>10}\n' . format ( * l ) print ( prefix_indent ( 'loop stack:        ' , table ) , file = output_file ) table = ( '    name |  offsets   ...\n' + '---------+------------...\n' ) for name , offsets in list ( self . sources . items ( ) ) : prefix = '{:>8} | ' . format ( name ) right_side = '\n' . join ( [ '{!r:}' . format ( o ) for o in offsets ] ) table += prefix_indent ( prefix , right_side , later_prefix = '         | ' ) print ( prefix_indent ( 'data sources:      ' , table ) , file = output_file ) table = ( '    name |  offsets   ...\n' + '---------+------------...\n' ) for name , offsets in list ( self . destinations . items ( ) ) : prefix = '{:>8} | ' . format ( name ) right_side = '\n' . join ( [ '{!r:}' . format ( o ) for o in offsets ] ) table += prefix_indent ( prefix , right_side , later_prefix = '         | ' ) print ( prefix_indent ( 'data destinations: ' , table ) , file = output_file ) table = ( ' op | count \n' + '----+-------\n' ) for op , count in list ( self . _flops . items ( ) ) : table += '{:>3} | {:>4}\n' . format ( op , count ) table += '     =======\n' table += '      {:>4}' . format ( sum ( self . _flops . values ( ) ) ) print ( prefix_indent ( 'FLOPs:     ' , table ) , file = output_file )
def print_variables_info ( self , output_file = sys . stdout ) : table = ( '    name |   type size             \n' + '---------+-------------------------\n' ) for name , var_info in list ( self . variables . items ( ) ) : table += '{:>8} | {:>6} {!s:<10}\n' . format ( name , var_info [ 0 ] , var_info [ 1 ] ) print ( prefix_indent ( 'variables: ' , table ) , file = output_file )
def print_constants_info ( self , output_file = sys . stdout ) : table = ( '    name | value     \n' + '---------+-----------\n' ) for name , value in list ( self . constants . items ( ) ) : table += '{!s:>8} | {:<10}\n' . format ( name , value ) print ( prefix_indent ( 'constants: ' , table ) , file = output_file )
def print_kernel_code ( self , output_file = sys . stdout ) : print ( self . kernel_code , file = output_file )
def get_array_declarations ( self ) : return [ d for d in self . kernel_ast . block_items if type ( d ) is c_ast . Decl and type ( d . type ) is c_ast . ArrayDecl ]
def get_kernel_loop_nest ( self ) : loop_nest = [ s for s in self . kernel_ast . block_items if type ( s ) in [ c_ast . For , c_ast . Pragma , c_ast . FuncCall ] ] assert len ( loop_nest ) >= 1 , "Found to few for statements in kernel" return loop_nest
def _find_inner_most_loop ( self , loop_nest ) : r = None for s in loop_nest : if type ( s ) is c_ast . For : return self . _find_inner_most_loop ( s ) or s else : r = r or self . _find_inner_most_loop ( s ) return r
def _build_kernel_function_declaration ( self , name = 'kernel' ) : array_declarations , array_dimensions = self . _build_array_declarations ( with_init = False ) scalar_declarations = self . _build_scalar_declarations ( with_init = False ) const_declarations = self . _build_const_declartions ( with_init = False ) return c_ast . FuncDecl ( args = c_ast . ParamList ( params = array_declarations + scalar_declarations + const_declarations ) , type = c_ast . TypeDecl ( declname = name , quals = [ ] , type = c_ast . IdentifierType ( names = [ 'void' ] ) ) )
def _build_scalar_declarations ( self , with_init = True ) : scalar_declarations = [ deepcopy ( d ) for d in self . kernel_ast . block_items if type ( d ) is c_ast . Decl and type ( d . type ) is c_ast . TypeDecl ] if with_init : random . seed ( 2342 ) for d in scalar_declarations : if d . type . type . names [ 0 ] in [ 'double' , 'float' ] : d . init = c_ast . Constant ( 'float' , str ( random . uniform ( 1.0 , 0.1 ) ) ) elif d . type . type . names [ 0 ] in [ 'int' , 'long' , 'long long' , 'unsigned int' , 'unsigned long' , 'unsigned long long' ] : d . init = c_ast . Constant ( 'int' , 2 ) return scalar_declarations
def _build_kernel_call ( self , name = 'kernel' ) : return c_ast . FuncCall ( name = c_ast . ID ( name = name ) , args = c_ast . ExprList ( exprs = [ c_ast . ID ( name = d . name ) for d in ( self . _build_array_declarations ( ) [ 0 ] + self . _build_scalar_declarations ( ) + self . _build_const_declartions ( ) ) ] ) )
def get_main_code ( self , as_filename = False , kernel_function_name = 'kernel' ) : assert self . kernel_ast is not None , "AST does not exist, this could be due to running " "based on a kernel description rather than code." fp , already_available = self . _get_intermediate_file ( 'main.c' , machine_and_compiler_dependent = False ) if already_available : code = fp . read ( ) else : parser = CParser ( ) template_code = self . CODE_TEMPLATE template_ast = parser . parse ( clean_code ( template_code , macros = True , comments = True , pragmas = False ) ) ast = deepcopy ( template_ast ) replace_id ( ast , "DECLARE_CONSTS" , self . _build_const_declartions ( with_init = True ) ) array_declarations , array_dimensions = self . _build_array_declarations ( ) replace_id ( ast , "DECLARE_ARRAYS" , array_declarations ) replace_id ( ast , "DECLARE_INIT_SCALARS" , self . _build_scalar_declarations ( ) ) replace_id ( ast , "DUMMY_CALLS" , self . _build_dummy_calls ( ) ) ast . ext . insert ( 0 , self . _build_kernel_function_declaration ( name = kernel_function_name ) ) replace_id ( ast , "KERNEL_CALL" , self . _build_kernel_call ( ) ) replace_id ( ast , "INIT_ARRAYS" , self . _build_array_initializations ( array_dimensions ) ) code = CGenerator ( ) . visit ( ast ) code = '\n' . join ( [ l for l in template_code . split ( '\n' ) if l . startswith ( "#include" ) ] ) + '\n\n' + code fp . write ( code ) fp . close ( ) if as_filename : return fp . name else : return code
def build_executable ( self , lflags = None , verbose = False , openmp = False ) : compiler , compiler_args = self . _machine . get_compiler ( ) kernel_obj_filename = self . compile_kernel ( openmp = openmp , verbose = verbose ) out_filename , already_exists = self . _get_intermediate_file ( os . path . splitext ( os . path . basename ( kernel_obj_filename ) ) [ 0 ] , binary = True , fp = False ) if not already_exists : main_source_filename = self . get_main_code ( as_filename = True ) if not ( ( 'LIKWID_INCLUDE' in os . environ or 'LIKWID_INC' in os . environ ) and 'LIKWID_LIB' in os . environ ) : print ( 'Could not find LIKWID_INCLUDE (e.g., "-I/app/likwid/4.1.2/include") and ' 'LIKWID_LIB (e.g., "-L/apps/likwid/4.1.2/lib") environment variables' , file = sys . stderr ) sys . exit ( 1 ) compiler_args += [ '-std=c99' , '-I' + reduce_path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/' ) , os . environ . get ( 'LIKWID_INCLUDE' , '' ) , os . environ . get ( 'LIKWID_INC' , '' ) , '-llikwid' ] if os . environ . get ( 'LIKWID_LIB' ) == '' : compiler_args = compiler_args [ : - 1 ] if lflags is None : lflags = [ ] lflags += os . environ [ 'LIKWID_LIB' ] . split ( ' ' ) + [ '-pthread' ] compiler_args += os . environ [ 'LIKWID_LIB' ] . split ( ' ' ) + [ '-pthread' ] infiles = [ reduce_path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/dummy.c' ) , kernel_obj_filename , main_source_filename ] cmd = [ compiler ] + infiles + compiler_args + [ '-o' , out_filename ] cmd = list ( filter ( bool , cmd ) ) if verbose : print ( 'Executing (build_executable): ' , ' ' . join ( cmd ) ) try : subprocess . check_output ( cmd ) except subprocess . CalledProcessError as e : print ( "Build failed:" , e , file = sys . stderr ) sys . exit ( 1 ) else : if verbose : print ( 'Executing (build_executable): ' , 'using cached' , out_filename ) return out_filename
def string_to_sympy ( cls , s ) : if isinstance ( s , int ) : return sympy . Integer ( s ) elif isinstance ( s , list ) : return tuple ( [ cls . string_to_sympy ( e ) for e in s ] ) elif s is None : return None else : local_dict = { c : symbol_pos_int ( c ) for c in s if c in string . ascii_letters } preliminary_expr = parse_expr ( s , local_dict = local_dict ) local_dict . update ( { s . name : symbol_pos_int ( s . name ) for s in preliminary_expr . free_symbols } ) return parse_expr ( s , local_dict = local_dict )
def get_identifier ( self ) : if self . _path : return os . path . basename ( self . _path ) else : return hashlib . sha256 ( hashlib . sha256 ( repr ( self . _data ) . encode ( ) ) ) . hexdigest ( )
def get_last_modified_datetime ( self ) : if self . _path : statbuf = os . stat ( self . _path ) return datetime . utcfromtimestamp ( statbuf . st_mtime ) else : return datetime . now ( )
def _enforce_no_overlap ( self , start_at = 0 ) : i = start_at while i + 1 < len ( self . data ) : if self . data [ i ] [ 1 ] >= self . data [ i + 1 ] [ 0 ] : if self . data [ i ] [ 1 ] < self . data [ i + 1 ] [ 1 ] : self . data [ i ] [ 1 ] = self . data [ i + 1 ] [ 1 ] del self . data [ i + 1 ] i += 1
def get_header_path ( ) -> str : import os return os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/'
def _align_iteration_with_cl_boundary ( self , iteration , subtract = True ) : element_size = self . kernel . datatypes_size [ self . kernel . datatype ] cacheline_size = self . machine [ 'cacheline size' ] elements_per_cacheline = int ( cacheline_size // element_size ) inner_loop = list ( self . kernel . get_loop_stack ( subs_consts = True ) ) [ - 1 ] inner_increment = inner_loop [ 'increment' ] o = self . kernel . compile_global_offsets ( iteration = iteration ) [ 0 ] if len ( o [ 1 ] ) : first_offset = min ( o [ 1 ] ) else : first_offset = min ( o [ 0 ] ) diff = first_offset - ( int ( first_offset ) >> self . csim . first_level . cl_bits << self . csim . first_level . cl_bits ) if diff == 0 : return iteration elif subtract : return iteration - ( diff // element_size ) // inner_increment else : return iteration + ( elements_per_cacheline - diff // element_size ) // inner_increment
def get_loads ( self ) : return [ self . stats [ cache_level ] [ 'LOAD_count' ] / self . first_dim_factor for cache_level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]
def get_hits ( self ) : return [ self . stats [ cache_level ] [ 'HIT_count' ] / self . first_dim_factor for cache_level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]
def get_misses ( self ) : return [ self . stats [ cache_level ] [ 'MISS_count' ] / self . first_dim_factor for cache_level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]
def get_stores ( self ) : return [ self . stats [ cache_level ] [ 'STORE_count' ] / self . first_dim_factor for cache_level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]
def get_evicts ( self ) : return [ self . stats [ cache_level ] [ 'EVICT_count' ] / self . first_dim_factor for cache_level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]
def get_infos ( self ) : first_dim_factor = self . first_dim_factor infos = { 'memory hierarchy' : [ ] , 'cache stats' : self . stats , 'cachelines in stats' : first_dim_factor } for cache_level , cache_info in list ( enumerate ( self . machine [ 'memory hierarchy' ] ) ) : infos [ 'memory hierarchy' ] . append ( { 'index' : len ( infos [ 'memory hierarchy' ] ) , 'level' : '{}' . format ( cache_info [ 'level' ] ) , 'total loads' : self . stats [ cache_level ] [ 'LOAD_byte' ] / first_dim_factor , 'total misses' : self . stats [ cache_level ] [ 'MISS_byte' ] / first_dim_factor , 'total hits' : self . stats [ cache_level ] [ 'HIT_byte' ] / first_dim_factor , 'total stores' : self . stats [ cache_level ] [ 'STORE_byte' ] / first_dim_factor , 'total evicts' : self . stats [ cache_level ] [ 'EVICT_byte' ] / first_dim_factor , 'total lines load' : self . stats [ cache_level ] [ 'LOAD_count' ] / first_dim_factor , 'total lines misses' : self . stats [ cache_level ] [ 'MISS_count' ] / first_dim_factor , 'total lines hits' : self . stats [ cache_level ] [ 'HIT_count' ] / first_dim_factor , 'total lines stores' : self . stats [ cache_level ] [ 'STORE_count' ] / first_dim_factor , 'total lines evicts' : self . stats [ cache_level ] [ 'EVICT_count' ] / first_dim_factor , 'cycles' : None } ) return infos
def measure_bw ( type_ , total_size , threads_per_core , max_threads_per_core , cores_per_socket , sockets ) : groups = [ ] for s in range ( sockets ) : groups += [ '-w' , 'S' + str ( s ) + ':' + str ( total_size ) + 'kB:' + str ( threads_per_core * cores_per_socket ) + ':1:' + str ( int ( max_threads_per_core / threads_per_core ) ) ] cmd = [ 'likwid-bench' , '-t' , type_ ] + groups sys . stderr . write ( ' ' . join ( cmd ) ) output = subprocess . Popen ( cmd , stdout = subprocess . PIPE ) . communicate ( ) [ 0 ] . decode ( 'utf-8' ) if not output : print ( ' ' . join ( cmd ) + ' returned no output, possibly wrong version installed ' '(requires 4.0 or later)' , file = sys . stderr ) sys . exit ( 1 ) bw = float ( get_match_or_break ( r'^MByte/s:\s+([0-9]+(?:\.[0-9]+)?)\s*$' , output ) [ 0 ] ) print ( ' ' , PrefixedUnit ( bw , 'MB/s' ) , file = sys . stderr ) return PrefixedUnit ( bw , 'MB/s' )
def fix_env_variable ( name , value ) : orig = os . environ . get ( name , None ) if value is not None : os . environ [ name ] = value elif name in os . environ : del os . environ [ name ] try : yield finally : if orig is not None : os . environ [ name ] = orig elif name in os . environ : del os . environ [ name ]
def configure_arggroup ( cls , parser ) : parser . add_argument ( '--no-phenoecm' , action = 'store_true' , help = 'Disables the phenomenological ECM model building.' ) parser . add_argument ( '--iterations' , type = int , default = 10 , help = 'Number of outer-loop iterations (e.g. time loop) during benchmarking. ' 'Default is 10, but actual number will be adapted to at least 0.2s runtime.' ) parser . add_argument ( '--ignore-warnings' , action = 'store_true' , help = 'Ignore warnings about missmatched CPU model and frequency.' )
def report ( self , output_file = sys . stdout ) : if self . verbose > 1 : with pprint_nosort ( ) : pprint . pprint ( self . results ) if self . verbose > 0 : print ( 'Runtime (per repetition): {:.2g} s' . format ( self . results [ 'Runtime (per repetition) [s]' ] ) , file = output_file ) if self . verbose > 0 : print ( 'Iterations per repetition: {!s}' . format ( self . results [ 'Iterations per repetition' ] ) , file = output_file ) print ( 'Runtime (per cacheline update): {:.2f} cy/CL' . format ( self . results [ 'Runtime (per cacheline update) [cy/CL]' ] ) , file = output_file ) print ( 'MEM volume (per repetition): {:.0f} Byte' . format ( self . results [ 'MEM volume (per repetition) [B]' ] ) , file = output_file ) print ( 'Performance: {:.2f} MFLOP/s' . format ( self . results [ 'Performance [MFLOP/s]' ] ) , file = output_file ) print ( 'Performance: {:.2f} MLUP/s' . format ( self . results [ 'Performance [MLUP/s]' ] ) , file = output_file ) print ( 'Performance: {:.2f} It/s' . format ( self . results [ 'Performance [MIt/s]' ] ) , file = output_file ) if self . verbose > 0 : print ( 'MEM bandwidth: {:.2f} MByte/s' . format ( self . results [ 'MEM BW [MByte/s]' ] ) , file = output_file ) print ( '' , file = output_file ) if not self . no_phenoecm : print ( "Data Transfers:" ) print ( "{:^8} |" . format ( "cache" ) , end = '' ) for metrics in self . results [ 'data transfers' ] . values ( ) : for metric_name in sorted ( metrics ) : print ( " {:^14}" . format ( metric_name ) , end = '' ) print ( ) break for cache , metrics in sorted ( self . results [ 'data transfers' ] . items ( ) ) : print ( "{!s:^8} |" . format ( cache ) , end = '' ) for k , v in sorted ( metrics . items ( ) ) : print ( " {!s:^14}" . format ( v ) , end = '' ) print ( ) print ( ) print ( 'Phenomenological ECM model: {{ {T_OL:.1f} || {T_nOL:.1f} | {T_L1L2:.1f} | ' '{T_L2L3:.1f} | {T_L3MEM:.1f} }} cy/CL' . format ( * * { k : float ( v ) for k , v in self . results [ 'ECM' ] . items ( ) } ) , file = output_file ) print ( 'T_OL assumes that two loads per cycle may be retiered, which is true for ' '128bit SSE/half-AVX loads on SNB and IVY, and 256bit full-AVX loads on HSW, ' 'BDW, SKL and SKX, but it also depends on AGU availability.' , file = output_file )
def _build_purchase_item ( course_id , course_url , cost_in_cents , mode , course_data , sku ) : item = { 'id' : "{}-{}" . format ( course_id , mode ) , 'url' : course_url , 'price' : cost_in_cents , 'qty' : 1 , } if 'title' in course_data : item [ 'title' ] = course_data [ 'title' ] else : item [ 'title' ] = 'Course {} mode: {}' . format ( course_id , mode ) if 'tags' in course_data : item [ 'tags' ] = course_data [ 'tags' ] item [ 'vars' ] = dict ( course_data . get ( 'vars' , { } ) , mode = mode , course_run_id = course_id ) item [ 'vars' ] [ 'purchase_sku' ] = sku return item
def _send_offer_assignment_notification_email ( config , user_email , subject , email_body , site_code , task ) : try : sailthru_client = get_sailthru_client ( site_code ) except SailthruError : logger . exception ( '[Offer Assignment] A client error occurred while attempting to send a offer assignment notification.' ' Message: {message}' . format ( message = email_body ) ) return None email_vars = { 'subject' : subject , 'email_body' : email_body , } try : response = sailthru_client . send ( template = config [ 'templates' ] [ 'assignment_email' ] , email = user_email , _vars = email_vars ) except SailthruClientError : logger . exception ( '[Offer Assignment] A client error occurred while attempting to send a offer assignment notification.' ' Message: {message}' . format ( message = email_body ) ) return None if not response . is_ok ( ) : error = response . get_error ( ) logger . error ( '[Offer Assignment] A {token_error_code} - {token_error_message} error occurred' ' while attempting to send a offer assignment notification.' ' Message: {message}' . format ( message = email_body , token_error_code = error . get_error_code ( ) , token_error_message = error . get_message ( ) ) ) if can_retry_sailthru_request ( error ) : logger . info ( '[Offer Assignment] An attempt will be made to resend the offer assignment notification.' ' Message: {message}' . format ( message = email_body ) ) schedule_retry ( task , config ) else : logger . warning ( '[Offer Assignment] No further attempts will be made to send the offer assignment notification.' ' Failed Message: {message}' . format ( message = email_body ) ) return response
def _file_refs ( self ) : if self . _prepared_file_refs is None : self . _prepared_file_refs = { FILE_REFS . idf : FileInfo ( constructor = lambda path : self . _epm_cls . from_idf ( path , idd_or_buffer_or_path = self . _idd ) , get_path = lambda : get_input_file_path ( self . dir_path , FILE_REFS . idf ) ) , FILE_REFS . epw : FileInfo ( constructor = lambda path : self . _weather_data_cls . from_epw ( path ) , get_path = lambda : get_input_file_path ( self . dir_path , FILE_REFS . epw ) ) , FILE_REFS . eio : FileInfo ( constructor = lambda path : self . _eio_cls ( path ) , get_path = lambda : get_output_file_path ( self . dir_path , FILE_REFS . eio ) ) , FILE_REFS . eso : FileInfo ( constructor = lambda path : self . _standard_output_cls ( path ) , get_path = lambda : get_output_file_path ( self . dir_path , FILE_REFS . eso ) ) , FILE_REFS . mtr : FileInfo ( constructor = lambda path : self . _standard_output_cls ( path ) , get_path = lambda : get_output_file_path ( self . dir_path , FILE_REFS . mtr ) ) , FILE_REFS . mtd : FileInfo ( constructor = lambda path : self . _mtd_cls ( path ) , get_path = lambda : get_output_file_path ( self . dir_path , FILE_REFS . mtd ) ) , FILE_REFS . mdd : FileInfo ( constructor = lambda path : open ( path ) . read ( ) , get_path = lambda : get_output_file_path ( self . dir_path , FILE_REFS . mdd ) ) , FILE_REFS . err : FileInfo ( constructor = lambda path : self . _err_cls ( path ) , get_path = lambda : get_output_file_path ( self . dir_path , FILE_REFS . err ) ) , FILE_REFS . summary_table : FileInfo ( constructor = lambda path : self . _summary_table_cls ( path ) , get_path = lambda : get_output_file_path ( self . dir_path , FILE_REFS . summary_table ) ) } return self . _prepared_file_refs
def _dev_populate_from_json_data ( self , json_data ) : comment = json_data . pop ( "_comment" , None ) if comment is not None : self . _comment = comment external_files_data = json_data . pop ( "_external_files" , dict ( ) ) self . _dev_external_files_manager . populate_from_json_data ( external_files_data ) added_records = [ ] for table_ref , json_data_records in json_data . items ( ) : table = getattr ( self , table_ref ) records = table . _dev_add_inert ( json_data_records ) added_records . extend ( records ) for r in added_records : r . _dev_activate_hooks ( ) for r in added_records : r . _dev_activate_links ( ) r . _dev_activate_external_files ( )
def get_external_files ( self ) : external_files = [ ] for table in self . _tables . values ( ) : for r in table : external_files . extend ( [ ef for ef in r . get_external_files ( ) ] ) return external_files
def set_defaults ( self ) : for table in self . _tables . values ( ) : for r in table : r . set_defaults ( )
def prepare_extensible ( self ) : for k in self . _tags : if "extensible" in k : cycle_len = int ( k . split ( ":" ) [ 1 ] ) break else : return cycle_start = None cycle_patterns = [ ] for i , field_descriptor in enumerate ( self . _field_descriptors ) : if ( cycle_start is not None ) and ( i >= ( cycle_start + cycle_len ) ) : break if ( cycle_start is None ) and ( "begin-extensible" in field_descriptor . tags ) : cycle_start = i if cycle_start is None : continue cycle_patterns . append ( field_descriptor . ref . replace ( "1" , r"(\d+)" ) ) else : raise RuntimeError ( "cycle start not found" ) self . _field_descriptors = self . _field_descriptors [ : cycle_start + cycle_len ] self . extensible_info = ( cycle_start , cycle_len , tuple ( cycle_patterns ) ) for i , fd in enumerate ( self . _field_descriptors [ cycle_start : ] ) : fd . set_extensible_info ( cycle_start , cycle_len , cycle_patterns [ i ] )
def get_value ( self , column_name_or_i , filter_column_name_or_i , filter_criterion ) : column_i = self . _get_column_index ( column_name_or_i ) filter_column_i = self . _get_column_index ( filter_column_name_or_i ) filter_fct = { float : lambda x : float ( x ) == filter_criterion , int : lambda x : int ( x ) == filter_criterion , str : lambda x : x . lower ( ) == filter_criterion . lower ( ) } [ type ( filter_criterion ) ] for row_i , row in enumerate ( self . _data ) : if filter_fct ( row [ filter_column_i ] ) : break else : raise ValueError ( "Filter did not return any values." ) return self . _data [ row_i ] [ column_i ]
def _update_value_inert ( self , index , value ) : field_descriptor = self . _table . _dev_descriptor . get_field_descriptor ( index ) value = field_descriptor . deserialize ( value , index ) if isinstance ( value , Link ) : current_link = self . _data . get ( index ) if current_link is not None : current_link . unregister ( ) if isinstance ( value , RecordHook ) : current_record_hook = self . _data . get ( index ) if current_record_hook is not None : current_record_hook . unregister ( ) if isinstance ( value , ExternalFile ) : current_external_file = self . _data . get ( index ) if current_external_file is not None : current_external_file . _dev_unregister ( ) if value in ( None , NONE_RECORD_HOOK , NONE_LINK , NONE_EXTERNAL_FILE ) : self . _dev_set_none_without_unregistering ( index , check_not_required = False ) return old_hook = None if index == 0 and not self . _table . _dev_auto_pk : old_hook = self . _data . get ( 0 ) self . _data [ index ] = value if old_hook is not None : self . _table . _dev_record_pk_was_updated ( old_hook . target_value )
def set_defaults ( self ) : defaults = { } for i in range ( len ( self ) ) : if i in self . _data : continue default = self . get_field_descriptor ( i ) . tags . get ( "default" , [ None ] ) [ 0 ] if default is not None : defaults [ i ] = default self . update ( defaults )
def delete ( self ) : self . _unregister_links ( ) self . _unregister_hooks ( ) self . _unregister_external_files ( ) self . get_table ( ) . _dev_remove_record_without_unregistering ( self ) self . _table = None self . _data = None
def register_record_hook ( self , hook ) : for key in hook . keys : if key in self . _record_hooks : field_descriptor = hook . target_record . get_field_descriptor ( hook . target_index ) raise FieldValidationError ( f"Reference key already exists, can't create: {key}. " f"{field_descriptor.get_error_location_message(hook.target_value, hook.target_index)}" ) self . _record_hooks [ key ] = hook
def register_link ( self , link ) : keys = tuple ( ( ref , link . initial_hook_value ) for ref in link . hook_references ) for k in keys : if k in self . _record_hooks : link . set_target ( target_record = self . _record_hooks [ k ] . target_record ) break else : for k in keys : if k in self . _table_hooks : link . set_target ( target_table = self . _table_hooks [ k ] ) break else : field_descriptor = link . source_record . get_field_descriptor ( link . source_index ) raise FieldValidationError ( f"No object found with any of given references : {keys}. " f"{field_descriptor.get_error_location_message(link.initial_hook_value)}" ) if link . source_record not in self . _links_by_source : self . _links_by_source [ link . source_record ] = set ( ) self . _links_by_source [ link . source_record ] . add ( link ) if link . target not in self . _links_by_target : self . _links_by_target [ link . target ] = set ( ) self . _links_by_target [ link . target ] . add ( link )
def _create_regex ( self , line , intent_name ) : try : return re . compile ( self . _create_intent_pattern ( line , intent_name ) , re . IGNORECASE ) except sre_constants . error as e : LOG . warning ( 'Failed to parse the line "{}" ' 'for {}' . format ( line , intent_name ) ) return None
def remaining_duration ( self , time ) : return max ( 0 , self . end - max ( self . start , time ) )
def http_request ( url , post_data = None ) : logger . debug ( 'Requesting URL: %s' % url ) buf = bio ( ) curl = pycurl . Curl ( ) curl . setopt ( curl . URL , url . encode ( 'ascii' , 'ignore' ) ) if config ( ) [ 'server' ] [ 'insecure' ] : curl . setopt ( curl . SSL_VERIFYPEER , 0 ) curl . setopt ( curl . SSL_VERIFYHOST , 0 ) if config ( ) [ 'server' ] [ 'certificate' ] : curl . setopt ( curl . SSL_VERIFYPEER , 1 ) curl . setopt ( curl . SSL_VERIFYHOST , 2 ) curl . setopt ( pycurl . CAINFO , config ( ) [ 'server' ] [ 'certificate' ] ) if post_data : curl . setopt ( curl . HTTPPOST , post_data ) curl . setopt ( curl . WRITEFUNCTION , buf . write ) curl . setopt ( pycurl . HTTPAUTH , pycurl . HTTPAUTH_DIGEST ) curl . setopt ( pycurl . USERPWD , "%s:%s" % ( config ( ) [ 'server' ] [ 'username' ] , config ( ) [ 'server' ] [ 'password' ] ) ) curl . setopt ( curl . HTTPHEADER , [ 'X-Requested-Auth: Digest' ] ) curl . setopt ( curl . FAILONERROR , True ) curl . setopt ( curl . FOLLOWLOCATION , True ) curl . perform ( ) curl . close ( ) result = buf . getvalue ( ) buf . close ( ) return result
def try_mkdir ( directory ) : try : os . mkdir ( directory ) except OSError as err : if err . errno != errno . EEXIST : raise err
def update_event_status ( event , status ) : dbs = db . get_session ( ) dbs . query ( db . RecordedEvent ) . filter ( db . RecordedEvent . start == event . start ) . update ( { 'status' : status } ) event . status = status dbs . commit ( )
def set_service_status ( service , status ) : srv = db . ServiceStates ( ) srv . type = service srv . status = status dbs = db . get_session ( ) dbs . merge ( srv ) dbs . commit ( ) dbs . close ( )
def get_service_status ( service ) : dbs = db . get_session ( ) srvs = dbs . query ( db . ServiceStates ) . filter ( db . ServiceStates . type == service ) if srvs . count ( ) : return srvs [ 0 ] . status return db . ServiceStatus . STOPPED
def update_agent_state ( ) : configure_service ( 'capture.admin' ) status = 'idle' if get_service_status ( db . Service . SCHEDULE ) == db . ServiceStatus . STOPPED : status = 'offline' elif get_service_status ( db . Service . CAPTURE ) == db . ServiceStatus . BUSY : status = 'capturing' elif get_service_status ( db . Service . INGEST ) == db . ServiceStatus . BUSY : status = 'uploading' register_ca ( status = status )
def configuration_file ( cfgfile ) : if cfgfile is not None : return cfgfile cfg = './etc/pyca.conf' if not os . path . isfile ( cfg ) : return '/etc/pyca.conf' return cfg
def check ( ) : if config ( 'server' ) [ 'insecure' ] : logger . warning ( 'HTTPS CHECKS ARE TURNED OFF. A SECURE CONNECTION IS ' 'NOT GUARANTEED' ) if config ( 'server' ) [ 'certificate' ] : open ( config ( 'server' ) [ 'certificate' ] , 'rb' ) . close ( ) if config ( 'agent' ) [ 'backup_mode' ] : logger . info ( 'Agent runs in backup mode. No data will be sent to ' 'Opencast' )
def logger_init ( ) : handlers = [ ] logconf = config ( 'logging' ) if logconf [ 'syslog' ] : handlers . append ( logging . handlers . SysLogHandler ( address = '/dev/log' ) ) if logconf [ 'stderr' ] : handlers . append ( logging . StreamHandler ( sys . stderr ) ) if logconf [ 'file' ] : handlers . append ( logging . handlers . WatchedFileHandler ( logconf [ 'file' ] ) ) for handler in handlers : handler . setFormatter ( logging . Formatter ( logconf [ 'format' ] ) ) logging . root . addHandler ( handler ) logging . root . setLevel ( logconf [ 'level' ] . upper ( ) ) logger . info ( 'Log level set to %s' % logconf [ 'level' ] )
def home ( ) : preview = config ( ) [ 'capture' ] [ 'preview' ] previewdir = config ( ) [ 'capture' ] [ 'preview_dir' ] preview = [ p . replace ( '{{previewdir}}' , previewdir ) for p in preview ] preview = zip ( preview , range ( len ( preview ) ) ) preview = [ p [ 1 ] for p in preview if os . path . isfile ( p [ 0 ] ) ] try : limit_upcoming = int ( request . args . get ( 'limit_upcoming' , 5 ) ) limit_processed = int ( request . args . get ( 'limit_processed' , 15 ) ) except ValueError : limit_upcoming = 5 limit_processed = 15 db = get_session ( ) upcoming_events = db . query ( UpcomingEvent ) . order_by ( UpcomingEvent . start ) . limit ( limit_upcoming ) recorded_events = db . query ( RecordedEvent ) . order_by ( RecordedEvent . start . desc ( ) ) . limit ( limit_processed ) recording = get_service_status ( Service . CAPTURE ) == ServiceStatus . BUSY uploading = get_service_status ( Service . INGEST ) == ServiceStatus . BUSY processed = db . query ( RecordedEvent ) . count ( ) upcoming = db . query ( UpcomingEvent ) . count ( ) return render_template ( 'home.html' , preview = preview , config = config ( ) , recorded_events = recorded_events , upcoming_events = upcoming_events , recording = recording , uploading = uploading , processed = processed , upcoming = upcoming , limit_upcoming = limit_upcoming , limit_processed = limit_processed , dtfmt = dtfmt )
def serve_image ( image_id ) : try : preview_dir = config ( ) [ 'capture' ] [ 'preview_dir' ] filepath = config ( ) [ 'capture' ] [ 'preview' ] [ image_id ] filepath = filepath . replace ( '{{previewdir}}' , preview_dir ) filepath = os . path . abspath ( filepath ) if os . path . isfile ( filepath ) : directory , filename = filepath . rsplit ( '/' , 1 ) return send_from_directory ( directory , filename ) except ( IndexError , KeyError ) : pass return '' , 404
def sigterm_handler ( signum , frame ) : sigint_handler ( signum , frame ) for process in multiprocessing . active_children ( ) : process . terminate ( ) sys . exit ( 0 )
def run_all ( * modules ) : processes = [ multiprocessing . Process ( target = mod . run ) for mod in modules ] for p in processes : p . start ( ) for p in processes : p . join ( )
def parse_ical ( vcal ) : vcal = vcal . replace ( '\r\n ' , '' ) . replace ( '\r\n\r\n' , '\r\n' ) vevents = vcal . split ( '\r\nBEGIN:VEVENT\r\n' ) del ( vevents [ 0 ] ) events = [ ] for vevent in vevents : event = { } for line in vevent . split ( '\r\n' ) : line = line . split ( ':' , 1 ) key = line [ 0 ] . lower ( ) if len ( line ) <= 1 or key == 'end' : continue if key . startswith ( 'dt' ) : event [ key ] = unix_ts ( dateutil . parser . parse ( line [ 1 ] ) ) continue if not key . startswith ( 'attach' ) : event [ key ] = line [ 1 ] continue event [ 'attach' ] = event . get ( 'attach' , [ ] ) attachment = { } for x in [ x . split ( '=' ) for x in line [ 0 ] . split ( ';' ) ] : if x [ 0 ] . lower ( ) in [ 'fmttype' , 'x-apple-filename' ] : attachment [ x [ 0 ] . lower ( ) ] = x [ 1 ] attachment [ 'data' ] = b64decode ( line [ 1 ] ) . decode ( 'utf-8' ) event [ 'attach' ] . append ( attachment ) events . append ( event ) return events
def control_loop ( ) : set_service_status ( Service . SCHEDULE , ServiceStatus . BUSY ) notify . notify ( 'READY=1' ) while not terminate ( ) : notify . notify ( 'WATCHDOG=1' ) get_schedule ( ) session = get_session ( ) next_event = session . query ( UpcomingEvent ) . filter ( UpcomingEvent . end > timestamp ( ) ) . order_by ( UpcomingEvent . start ) . first ( ) if next_event : logger . info ( 'Next scheduled recording: %s' , datetime . fromtimestamp ( next_event . start ) ) notify . notify ( 'STATUS=Next scheduled recording: %s' % datetime . fromtimestamp ( next_event . start ) ) else : logger . info ( 'No scheduled recording' ) notify . notify ( 'STATUS=No scheduled recording' ) session . close ( ) next_update = timestamp ( ) + config ( ) [ 'agent' ] [ 'update_frequency' ] while not terminate ( ) and timestamp ( ) < next_update : time . sleep ( 0.1 ) logger . info ( 'Shutting down schedule service' ) set_service_status ( Service . SCHEDULE , ServiceStatus . STOPPED )
def control_loop ( ) : set_service_status ( Service . AGENTSTATE , ServiceStatus . BUSY ) notify . notify ( 'READY=1' ) notify . notify ( 'STATUS=Running' ) while not terminate ( ) : notify . notify ( 'WATCHDOG=1' ) update_agent_state ( ) next_update = timestamp ( ) + config ( ) [ 'agent' ] [ 'update_frequency' ] while not terminate ( ) and timestamp ( ) < next_update : time . sleep ( 0.1 ) logger . info ( 'Shutting down agentstate service' ) set_service_status ( Service . AGENTSTATE , ServiceStatus . STOPPED )
def make_error_response ( error , status = 500 ) : content = { 'errors' : [ { 'status' : status , 'title' : error } ] } return make_response ( jsonify ( content ) , status )
def make_data_response ( data , status = 200 ) : content = { 'data' : ensurelist ( data ) } return make_response ( jsonify ( content ) , status )
def internal_state ( ) : data = { 'services' : { 'capture' : ServiceStatus . str ( get_service_status ( Service . CAPTURE ) ) , 'ingest' : ServiceStatus . str ( get_service_status ( Service . INGEST ) ) , 'schedule' : ServiceStatus . str ( get_service_status ( Service . SCHEDULE ) ) , 'agentstate' : ServiceStatus . str ( get_service_status ( Service . AGENTSTATE ) ) } } return make_response ( jsonify ( { 'meta' : data } ) )
def events ( ) : db = get_session ( ) upcoming_events = db . query ( UpcomingEvent ) . order_by ( UpcomingEvent . start ) recorded_events = db . query ( RecordedEvent ) . order_by ( RecordedEvent . start . desc ( ) ) result = [ event . serialize ( ) for event in upcoming_events ] result += [ event . serialize ( ) for event in recorded_events ] return make_data_response ( result )
def event ( uid ) : db = get_session ( ) event = db . query ( RecordedEvent ) . filter ( RecordedEvent . uid == uid ) . first ( ) or db . query ( UpcomingEvent ) . filter ( UpcomingEvent . uid == uid ) . first ( ) if event : return make_data_response ( event . serialize ( ) ) return make_error_response ( 'No event with specified uid' , 404 )
def ingest ( event ) : set_service_status ( Service . INGEST , ServiceStatus . BUSY ) notify . notify ( 'STATUS=Uploading' ) recording_state ( event . uid , 'uploading' ) update_event_status ( event , Status . UPLOADING ) service = config ( 'service-ingest' ) service = service [ randrange ( 0 , len ( service ) ) ] logger . info ( 'Selecting ingest service to use: ' + service ) logger . info ( 'Creating new mediapackage' ) mediapackage = http_request ( service + '/createMediaPackage' ) prop = 'org.opencastproject.capture.agent.properties' dcns = 'http://www.opencastproject.org/xsd/1.0/dublincore/' for attachment in event . get_data ( ) . get ( 'attach' ) : data = attachment . get ( 'data' ) if attachment . get ( 'x-apple-filename' ) == prop : workflow_def , workflow_config = get_config_params ( data ) elif attachment . get ( 'fmttype' ) == 'application/xml' and dcns in data : name = attachment . get ( 'x-apple-filename' , '' ) . rsplit ( '.' , 1 ) [ 0 ] logger . info ( 'Adding %s DC catalog' % name ) fields = [ ( 'mediaPackage' , mediapackage ) , ( 'flavor' , 'dublincore/%s' % name ) , ( 'dublinCore' , data . encode ( 'utf-8' ) ) ] mediapackage = http_request ( service + '/addDCCatalog' , fields ) for ( flavor , track ) in event . get_tracks ( ) : logger . info ( 'Adding track ({0} -> {1})' . format ( flavor , track ) ) track = track . encode ( 'ascii' , 'ignore' ) fields = [ ( 'mediaPackage' , mediapackage ) , ( 'flavor' , flavor ) , ( 'BODY1' , ( pycurl . FORM_FILE , track ) ) ] mediapackage = http_request ( service + '/addTrack' , fields ) logger . info ( 'Ingest recording' ) fields = [ ( 'mediaPackage' , mediapackage ) ] if workflow_def : fields . append ( ( 'workflowDefinitionId' , workflow_def ) ) if event . uid : fields . append ( ( 'workflowInstanceId' , event . uid . encode ( 'ascii' , 'ignore' ) ) ) fields += workflow_config mediapackage = http_request ( service + '/ingest' , fields ) recording_state ( event . uid , 'upload_finished' ) update_event_status ( event , Status . FINISHED_UPLOADING ) notify . notify ( 'STATUS=Running' ) set_service_status_immediate ( Service . INGEST , ServiceStatus . IDLE ) logger . info ( 'Finished ingest' )
def sigterm_handler ( signum , frame ) : if captureproc and captureproc . poll ( ) is None : captureproc . terminate ( ) terminate ( True ) sys . exit ( 0 )
def render_to_fragment ( self , request , * * kwargs ) : fragment = Fragment ( TEST_HTML ) fragment . add_javascript ( TEST_JS ) fragment . add_css ( TEST_CSS ) return fragment
def resources ( self ) : seen = set ( ) return [ x for x in self . _resources if x not in seen and not seen . add ( x ) ]
def to_dict ( self ) : return { 'content' : self . content , 'resources' : [ r . _asdict ( ) for r in self . resources ] , 'js_init_fn' : self . js_init_fn , 'js_init_version' : self . js_init_version , 'json_init_args' : self . json_init_args }
def from_dict ( cls , pods ) : frag = cls ( ) frag . content = pods [ 'content' ] frag . _resources = [ FragmentResource ( * * d ) for d in pods [ 'resources' ] ] frag . js_init_fn = pods [ 'js_init_fn' ] frag . js_init_version = pods [ 'js_init_version' ] frag . json_init_args = pods [ 'json_init_args' ] return frag
def resource_to_html ( resource ) : if resource . mimetype == "text/css" : if resource . kind == "text" : return u"<style type='text/css'>\n%s\n</style>" % resource . data elif resource . kind == "url" : return u"<link rel='stylesheet' href='%s' type='text/css'>" % resource . data else : raise Exception ( "Unrecognized resource kind %r" % resource . kind ) elif resource . mimetype == "application/javascript" : if resource . kind == "text" : return u"<script>\n%s\n</script>" % resource . data elif resource . kind == "url" : return u"<script src='%s' type='application/javascript'></script>" % resource . data else : raise Exception ( "Unrecognized resource kind %r" % resource . kind ) elif resource . mimetype == "text/html" : assert resource . kind == "text" return resource . data else : raise Exception ( "Unrecognized mimetype %r" % resource . mimetype )
def get ( self , request , * args , * * kwargs ) : fragment = self . render_to_fragment ( request , * * kwargs ) response_format = request . GET . get ( 'format' ) or request . POST . get ( 'format' ) or 'html' if response_format == 'json' or WEB_FRAGMENT_RESPONSE_TYPE in request . META . get ( 'HTTP_ACCEPT' , 'text/html' ) : return JsonResponse ( fragment . to_dict ( ) ) else : return self . render_standalone_response ( request , fragment , * * kwargs )
def render_standalone_response ( self , request , fragment , * * kwargs ) : if fragment is None : return HttpResponse ( status = 204 ) html = self . render_to_standalone_html ( request , fragment , * * kwargs ) return HttpResponse ( html )
def render_to_standalone_html ( self , request , fragment , * * kwargs ) : template = get_template ( STANDALONE_TEMPLATE_NAME ) context = { 'head_html' : fragment . head_html ( ) , 'body_html' : fragment . body_html ( ) , 'foot_html' : fragment . foot_html ( ) , } return template . render ( context )
def calc ( pvalues , lamb ) : m = len ( pvalues ) pi0 = ( pvalues > lamb ) . sum ( ) / ( ( 1 - lamb ) * m ) pFDR = np . ones ( m ) print ( "pFDR    y        Pr     fastPow" ) for i in range ( m ) : y = pvalues [ i ] Pr = max ( 1 , m - i ) / float ( m ) pFDR [ i ] = ( pi0 * y ) / ( Pr * ( 1 - math . pow ( 1 - y , m ) ) ) print ( i , pFDR [ i ] , y , Pr , 1.0 - math . pow ( 1 - y , m ) ) num_null = pi0 * m num_alt = m - num_null num_negs = np . array ( range ( m ) ) num_pos = m - num_negs pp = num_pos / float ( m ) qvalues = np . ones ( m ) qvalues [ 0 ] = pFDR [ 0 ] for i in range ( m - 1 ) : qvalues [ i + 1 ] = min ( qvalues [ i ] , pFDR [ i + 1 ] ) sens = ( ( 1.0 - qvalues ) * num_pos ) / num_alt sens [ sens > 1.0 ] = 1.0 df = pd . DataFrame ( dict ( pvalue = pvalues , qvalue = qvalues , FDR = pFDR , percentile_positive = pp , sens = sens ) ) df [ "svalue" ] = df . sens [ : : - 1 ] . cummax ( ) [ : : - 1 ] return df , num_null , m
def to_one_dim_array ( values , as_type = None ) : if isinstance ( values , ( list , tuple ) ) : values = np . array ( values , dtype = np . float32 ) elif isinstance ( values , pd . Series ) : values = values . values values = values . flatten ( ) assert values . ndim == 1 , "values has wrong dimension" if as_type is not None : return values . astype ( as_type ) return values
def lookup_values_from_error_table ( scores , err_df ) : ix = find_nearest_matches ( np . float32 ( err_df . cutoff . values ) , np . float32 ( scores ) ) return err_df . pvalue . iloc [ ix ] . values , err_df . svalue . iloc [ ix ] . values , err_df . pep . iloc [ ix ] . values , err_df . qvalue . iloc [ ix ] . values
def summary_err_table ( df , qvalues = [ 0 , 0.01 , 0.02 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ] ) : qvalues = to_one_dim_array ( qvalues ) ix = find_nearest_matches ( np . float32 ( df . qvalue . values ) , qvalues ) df_sub = df . iloc [ ix ] . copy ( ) for i_sub , ( i0 , i1 ) in enumerate ( zip ( ix , ix [ 1 : ] ) ) : if i1 == i0 : df_sub . iloc [ i_sub + 1 , : ] = None df_sub . qvalue = qvalues df_sub . reset_index ( inplace = True , drop = True ) return df_sub [ [ 'qvalue' , 'pvalue' , 'svalue' , 'pep' , 'fdr' , 'fnr' , 'fpr' , 'tp' , 'tn' , 'fp' , 'fn' , 'cutoff' ] ]
def error_statistics ( target_scores , decoy_scores , parametric , pfdr , pi0_lambda , pi0_method = "smoother" , pi0_smooth_df = 3 , pi0_smooth_log_pi0 = False , compute_lfdr = False , lfdr_trunc = True , lfdr_monotone = True , lfdr_transf = "probit" , lfdr_adj = 1.5 , lfdr_eps = np . power ( 10.0 , - 8 ) ) : target_scores = to_one_dim_array ( target_scores ) target_scores = np . sort ( target_scores [ ~ np . isnan ( target_scores ) ] ) decoy_scores = to_one_dim_array ( decoy_scores ) decoy_scores = np . sort ( decoy_scores [ ~ np . isnan ( decoy_scores ) ] ) if parametric : target_pvalues = pnorm ( target_scores , decoy_scores ) else : target_pvalues = pemp ( target_scores , decoy_scores ) pi0 = pi0est ( target_pvalues , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 ) target_qvalues = qvalue ( target_pvalues , pi0 [ 'pi0' ] , pfdr ) metrics = stat_metrics ( target_pvalues , pi0 [ 'pi0' ] , pfdr ) error_stat = pd . DataFrame ( { 'cutoff' : target_scores , 'pvalue' : target_pvalues , 'qvalue' : target_qvalues , 'svalue' : metrics [ 'svalue' ] , 'tp' : metrics [ 'tp' ] , 'fp' : metrics [ 'fp' ] , 'tn' : metrics [ 'tn' ] , 'fn' : metrics [ 'fn' ] , 'fpr' : metrics [ 'fpr' ] , 'fdr' : metrics [ 'fdr' ] , 'fnr' : metrics [ 'fnr' ] } ) if compute_lfdr : error_stat [ 'pep' ] = lfdr ( target_pvalues , pi0 [ 'pi0' ] , lfdr_trunc , lfdr_monotone , lfdr_transf , lfdr_adj , lfdr_eps ) return error_stat , pi0
def find_cutoff ( tt_scores , td_scores , cutoff_fdr , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 ) : error_stat , pi0 = error_statistics ( tt_scores , td_scores , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , False ) if not len ( error_stat ) : raise click . ClickException ( "Too little data for calculating error statistcs." ) i0 = ( error_stat . qvalue - cutoff_fdr ) . abs ( ) . idxmin ( ) cutoff = error_stat . iloc [ i0 ] [ "cutoff" ] return cutoff
def score ( infile , outfile , classifier , xgb_autotune , apply_weights , xeval_fraction , xeval_num_iter , ss_initial_fdr , ss_iteration_fdr , ss_num_iter , ss_main_score , group_id , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps , level , ipf_max_peakgroup_rank , ipf_max_peakgroup_pep , ipf_max_transition_isotope_overlap , ipf_min_transition_sn , tric_chromprob , threads , test ) : if outfile is None : outfile = infile else : outfile = outfile xgb_hyperparams = { 'autotune' : xgb_autotune , 'autotune_num_rounds' : 10 , 'num_boost_round' : 100 , 'early_stopping_rounds' : 10 , 'test_size' : 0.33 } xgb_params = { 'eta' : 0.3 , 'gamma' : 0 , 'max_depth' : 6 , 'min_child_weight' : 1 , 'subsample' : 1 , 'colsample_bytree' : 1 , 'colsample_bylevel' : 1 , 'colsample_bynode' : 1 , 'lambda' : 1 , 'alpha' : 0 , 'scale_pos_weight' : 1 , 'silent' : 1 , 'objective' : 'binary:logitraw' , 'nthread' : 1 , 'eval_metric' : 'auc' } xgb_params_space = { 'eta' : hp . uniform ( 'eta' , 0.0 , 0.3 ) , 'gamma' : hp . uniform ( 'gamma' , 0.0 , 0.5 ) , 'max_depth' : hp . quniform ( 'max_depth' , 2 , 8 , 1 ) , 'min_child_weight' : hp . quniform ( 'min_child_weight' , 1 , 5 , 1 ) , 'subsample' : 1 , 'colsample_bytree' : 1 , 'colsample_bylevel' : 1 , 'colsample_bynode' : 1 , 'lambda' : hp . uniform ( 'lambda' , 0.0 , 1.0 ) , 'alpha' : hp . uniform ( 'alpha' , 0.0 , 1.0 ) , 'scale_pos_weight' : 1.0 , 'silent' : 1 , 'objective' : 'binary:logitraw' , 'nthread' : 1 , 'eval_metric' : 'auc' } if not apply_weights : PyProphetLearner ( infile , outfile , classifier , xgb_hyperparams , xgb_params , xgb_params_space , xeval_fraction , xeval_num_iter , ss_initial_fdr , ss_iteration_fdr , ss_num_iter , ss_main_score , group_id , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps , level , ipf_max_peakgroup_rank , ipf_max_peakgroup_pep , ipf_max_transition_isotope_overlap , ipf_min_transition_sn , tric_chromprob , threads , test ) . run ( ) else : PyProphetWeightApplier ( infile , outfile , classifier , xgb_hyperparams , xgb_params , xgb_params_space , xeval_fraction , xeval_num_iter , ss_initial_fdr , ss_iteration_fdr , ss_num_iter , ss_main_score , group_id , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps , level , ipf_max_peakgroup_rank , ipf_max_peakgroup_pep , ipf_max_transition_isotope_overlap , ipf_min_transition_sn , tric_chromprob , threads , test , apply_weights ) . run ( )
def ipf ( infile , outfile , ipf_ms1_scoring , ipf_ms2_scoring , ipf_h0 , ipf_grouped_fdr , ipf_max_precursor_pep , ipf_max_peakgroup_pep , ipf_max_precursor_peakgroup_pep , ipf_max_transition_pep ) : if outfile is None : outfile = infile else : outfile = outfile infer_peptidoforms ( infile , outfile , ipf_ms1_scoring , ipf_ms2_scoring , ipf_h0 , ipf_grouped_fdr , ipf_max_precursor_pep , ipf_max_peakgroup_pep , ipf_max_precursor_peakgroup_pep , ipf_max_transition_pep )
def peptide ( infile , outfile , context , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps ) : if outfile is None : outfile = infile else : outfile = outfile infer_peptides ( infile , outfile , context , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps )
def protein ( infile , outfile , context , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps ) : if outfile is None : outfile = infile else : outfile = outfile infer_proteins ( infile , outfile , context , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps )
def subsample ( infile , outfile , subsample_ratio , test ) : if outfile is None : outfile = infile else : outfile = outfile subsample_osw ( infile , outfile , subsample_ratio , test )
def reduce ( infile , outfile ) : if outfile is None : outfile = infile else : outfile = outfile reduce_osw ( infile , outfile )
def backpropagate ( infile , outfile , apply_scores ) : if outfile is None : outfile = infile else : outfile = outfile backpropagate_oswr ( infile , outfile , apply_scores )
def create_group ( self , group ) : self . _valid_group_id ( group . id ) body = { "data" : group . json_data ( ) } url = "{}/group/{}" . format ( self . API , group . name ) data = self . _put_resource ( url , headers = { } , body = body ) return self . _group_from_json ( data . get ( "data" ) )
def delete_group ( self , group_id ) : self . _valid_group_id ( group_id ) url = "{}/group/{}" . format ( self . API , group_id ) self . _delete_resource ( url ) return True
def is_effective_member ( self , group_id , netid ) : self . _valid_group_id ( group_id ) netid = re . sub ( '@washington.edu' , '' , netid ) url = "{}/group/{}/effective_member/{}" . format ( self . API , group_id , netid ) try : data = self . _get_resource ( url ) return True except DataFailureException as ex : if ex . status == 404 : return False else : raise
def parse_version ( ) : from os . path import dirname , join import ast modname = setupkw [ 'name' ] init_fpath = join ( dirname ( __file__ ) , modname , '__init__.py' ) with open ( init_fpath ) as file_ : sourcecode = file_ . read ( ) pt = ast . parse ( sourcecode ) class VersionVisitor ( ast . NodeVisitor ) : def visit_Assign ( self , node ) : for target in node . targets : if target . id == '__version__' : self . version = node . value . s visitor = VersionVisitor ( ) visitor . visit ( pt ) return visitor . version
def _create_container ( context , path , l_mtime , size ) : new_context = context . copy ( ) new_context . input_ = None new_context . headers = None new_context . query = None container = path . split ( '/' , 1 ) [ 0 ] + '_segments' cli_put_container ( new_context , container ) prefix = container + '/' + path . split ( '/' , 1 ) [ 1 ] prefix = '%s/%s/%s/' % ( prefix , l_mtime , size ) return prefix
def is_empty ( self ) : something = self . read ( 1 ) if something : if self . buf : self . buf = something + self . buf else : self . buf = something return False else : return True
def forwards ( self , orm ) : for title in orm [ 'hero_slider.SliderItemTitle' ] . objects . all ( ) : title . is_published = True title . save ( )
def get_slider_items ( context , amount = None ) : req = context . get ( 'request' ) qs = SliderItem . objects . published ( req ) . order_by ( 'position' ) if amount : qs = qs [ : amount ] return qs
def render_hero_slider ( context ) : req = context . get ( 'request' ) qs = SliderItem . objects . published ( req ) . order_by ( 'position' ) return { 'slider_items' : qs , }
def reader_acquire ( self ) : self . _order_mutex . acquire ( ) self . _readers_mutex . acquire ( ) if self . _readers == 0 : self . _access_mutex . acquire ( ) self . _readers += 1 self . _order_mutex . release ( ) self . _readers_mutex . release ( )
def reader_release ( self ) : self . _readers_mutex . acquire ( ) self . _readers -= 1 if self . _readers == 0 : self . _access_mutex . release ( ) self . _readers_mutex . release ( )
def writer_acquire ( self ) : self . _order_mutex . acquire ( ) self . _access_mutex . acquire ( ) self . _order_mutex . release ( )
def tasks ( self ) : self . _rwlock . reader_acquire ( ) tl = [ v for v in self . _tasks . values ( ) ] tl . sort ( key = lambda x : x . task_id ) self . _rwlock . reader_release ( ) return tl
def to_dict ( self ) : properties = find_class_properties ( self . __class__ ) config = { name : self . __getattribute__ ( name ) for name , _ in properties } return config
def create_index ( idx_url , clean = False ) : try : r = requests . get ( idx_url ) except requests . exceptions . ConnectionError : cause = "Error connecting to Elastic Search (index: %s)" % idx_url raise ElasticSearchError ( cause = cause ) if r . status_code != 200 : r = requests . put ( idx_url ) if r . status_code != 200 : logger . info ( "Can't create index %s (%s)" , idx_url , r . status_code ) cause = "Error creating Elastic Search index %s" % idx_url raise ElasticSearchError ( cause = cause ) logger . info ( "Index %s created" , idx_url ) return True elif r . status_code == 200 and clean : requests . delete ( idx_url ) requests . put ( idx_url ) logger . info ( "Index deleted and created (index: %s)" , idx_url ) return True return False
def json_encoder ( * args , * * kwargs ) : obj = cherrypy . serving . request . _json_inner_handler ( * args , * * kwargs ) for chunk in JSONEncoder ( ) . iterencode ( obj ) : yield chunk . encode ( 'utf-8' )
def items ( self ) : pipe = self . conn . pipeline ( ) pipe . lrange ( Q_STORAGE_ITEMS , 0 , - 1 ) pipe . ltrim ( Q_STORAGE_ITEMS , 1 , 0 ) items = pipe . execute ( ) [ 0 ] for item in items : item = pickle . loads ( item ) yield item
def __validate_args ( task_id , backend , category , backend_args ) : if not task_id or task_id . strip ( ) == "" : msg = "Missing task_id for task" raise ValueError ( msg ) if not backend or backend . strip ( ) == "" : msg = "Missing backend for task '%s'" % task_id raise ValueError ( msg ) if backend_args and not isinstance ( backend_args , dict ) : msg = "Backend_args is not a dict, task '%s'" % task_id raise ValueError ( msg ) if not category or category . strip ( ) == "" : msg = "Missing category for task '%s'" % task_id raise ValueError ( msg )
def __parse_archive_args ( self , archive_args ) : if not archive_args : return None archiving_args = copy . deepcopy ( archive_args ) if self . archive_path : archiving_args [ 'archive_path' ] = self . archive_path else : archiving_args [ 'archive_path' ] = os . path . expanduser ( ARCHIVES_DEFAULT_PATH ) return ArchivingTaskConfig . from_dict ( archiving_args )
def schedule_job_task ( self , queue_id , task_id , job_args , delay = 0 ) : self . _rwlock . writer_acquire ( ) job_id = self . _generate_job_id ( task_id ) event = self . _scheduler . enter ( delay , 1 , self . _enqueue_job , argument = ( queue_id , job_id , job_args , ) ) self . _jobs [ job_id ] = event self . _tasks [ task_id ] = job_id self . _rwlock . writer_release ( ) logging . debug ( "Job #%s (task: %s) scheduled on %s (wait: %s)" , job_id , task_id , queue_id , delay ) return job_id
def cancel_job_task ( self , task_id ) : try : self . _rwlock . writer_acquire ( ) job_id = self . _tasks . get ( task_id , None ) if job_id : self . _cancel_job ( job_id ) else : logger . warning ( "Task %s set to be removed was not found" , task_id ) finally : self . _rwlock . writer_release ( )
def run ( self ) : try : self . listen ( ) except Exception as e : logger . critical ( "JobListener instence crashed. Error: %s" , str ( e ) ) logger . critical ( traceback . format_exc ( ) )
def listen ( self ) : pubsub = self . conn . pubsub ( ) pubsub . subscribe ( self . pubsub_channel ) logger . debug ( "Listening on channel %s" , self . pubsub_channel ) for msg in pubsub . listen ( ) : logger . debug ( "New message received of type %s" , str ( msg [ 'type' ] ) ) if msg [ 'type' ] != 'message' : logger . debug ( "Ignoring job message" ) continue data = pickle . loads ( msg [ 'data' ] ) job_id = data [ 'job_id' ] job = rq . job . Job . fetch ( job_id , connection = self . conn ) if data [ 'status' ] == 'finished' : logging . debug ( "Job #%s completed" , job_id ) handler = self . result_handler elif data [ 'status' ] == 'failed' : logging . debug ( "Job #%s failed" , job_id ) handler = self . result_handler_err else : continue if handler : logging . debug ( "Calling handler for job #%s" , job_id ) handler ( job )
def schedule ( self ) : if self . async_mode : self . _scheduler . start ( ) self . _listener . start ( ) else : self . _scheduler . schedule ( )
def _build_job_arguments ( task ) : job_args = { } job_args [ 'qitems' ] = Q_STORAGE_ITEMS job_args [ 'task_id' ] = task . task_id job_args [ 'backend' ] = task . backend backend_args = copy . deepcopy ( task . backend_args ) if 'next_from_date' in backend_args : backend_args [ 'from_date' ] = backend_args . pop ( 'next_from_date' ) if 'next_offset' in backend_args : backend_args [ 'offset' ] = backend_args . pop ( 'next_offset' ) job_args [ 'backend_args' ] = backend_args job_args [ 'category' ] = task . category archiving_cfg = task . archiving_cfg job_args [ 'archive_args' ] = archiving_cfg . to_dict ( ) if archiving_cfg else None sched_cfg = task . scheduling_cfg job_args [ 'max_retries' ] = sched_cfg . max_retries if sched_cfg else MAX_JOB_RETRIES return job_args
def reverse_action ( self , url_name , * args , * * kwargs ) : if self . request and not self . request . version : return reverse ( self . get_url_name ( url_name ) , * args , * * kwargs ) return super ( ) . reverse_action ( url_name , * args , * * kwargs )
def get_version ( version = None ) : if version is None : version = VERSION assert len ( version ) == 5 assert version [ 3 ] in ( "alpha" , "beta" , "rc" , "final" ) parts = 2 if version [ 2 ] == 0 else 3 main = "." . join ( str ( x ) for x in version [ : parts ] ) sub = "" if version [ 3 ] != "final" : mapping = { "alpha" : "a" , "beta" : "b" , "rc" : "c" } sub = mapping [ version [ 3 ] ] + str ( version [ 4 ] ) return main + sub
def create ( self , request ) : login_form = AuthenticationForm ( request , data = request . data ) if not login_form . is_valid ( ) : raise serializers . ValidationError ( login_form . errors ) auth_login ( request , login_form . get_user ( ) ) serializer = UserSerializer ( request . user ) return Response ( serializer . data , status = status . HTTP_200_OK )
def list ( self , request ) : serializer = self . get_serializer ( request . user ) return Response ( serializer . data , status = status . HTTP_200_OK )
def create ( self , request ) : password_form = PasswordChangeForm ( request . user , data = request . data ) if not password_form . is_valid ( ) : raise serializers . ValidationError ( password_form . errors ) password_form . save ( ) update_session_auth_hash ( request , password_form . user ) return Response ( status = status . HTTP_204_NO_CONTENT )
def create_field ( field_info ) : field_type = field_info . get ( 'type' ) if field_type not in FIELDS_NAME_MAP : raise ValueError ( _ ( 'not support this field: {}' ) . format ( field_type ) ) field_class = FIELDS_NAME_MAP . get ( field_type ) params = dict ( field_info ) params . pop ( 'type' ) return field_class . from_dict ( params )
def _change_logging_kwargs ( kwargs ) : log_levels = kwargs . pop ( 'log_level' , None ) log_folder = kwargs . pop ( 'log_folder' , 'logs' ) logger_names = kwargs . pop ( 'logger_names' , '' ) if log_levels is None : log_levels = kwargs . pop ( 'log_levels' , logging . INFO ) log_multiproc = kwargs . pop ( 'log_multiproc' , True ) if not isinstance ( logger_names , ( tuple , list ) ) : logger_names = [ logger_names ] if not isinstance ( log_levels , ( tuple , list ) ) : log_levels = [ log_levels ] if len ( log_levels ) == 1 : log_levels = [ log_levels [ 0 ] for _ in logger_names ] dictionary = copy . deepcopy ( LOGGING_DICT ) prefixes = [ '' ] if not log_multiproc : for key in list ( dictionary . keys ( ) ) : if key . startswith ( 'multiproc_' ) : del dictionary [ key ] else : prefixes . append ( 'multiproc_' ) for prefix in prefixes : for handler_dict in dictionary [ prefix + 'handlers' ] . values ( ) : if 'filename' in handler_dict : filename = os . path . join ( log_folder , handler_dict [ 'filename' ] ) filename = os . path . normpath ( filename ) handler_dict [ 'filename' ] = filename dictionary [ prefix + 'loggers' ] = { } logger_dict = dictionary [ prefix + 'loggers' ] for idx , logger_name in enumerate ( logger_names ) : logger_dict [ logger_name ] = { 'level' : log_levels [ idx ] , 'handlers' : list ( dictionary [ prefix + 'handlers' ] . keys ( ) ) } kwargs [ 'log_config' ] = dictionary
def get_strings ( args ) : string_list = [ ] for elem in ast . walk ( ast . parse ( args ) ) : if isinstance ( elem , ast . Str ) : string_list . append ( elem . s ) return string_list
def extract_replacements ( self , trajectory ) : self . env_name = trajectory . v_environment_name self . traj_name = trajectory . v_name self . set_name = trajectory . f_wildcard ( '$set' ) self . run_name = trajectory . f_wildcard ( '$' )
def _parser_to_string_io ( parser ) : memory_file = StringIO ( ) parser . write ( memory_file ) memory_file . flush ( ) memory_file . seek ( 0 ) return memory_file
def make_logging_handlers_and_tools ( self , multiproc = False ) : log_stdout = self . log_stdout if sys . stdout is self . _stdout_to_logger : log_stdout = False if self . log_config : if multiproc : proc_log_config = self . _mp_config else : proc_log_config = self . _sp_config if proc_log_config : if isinstance ( proc_log_config , dict ) : new_dict = self . _handle_dict_config ( proc_log_config ) dictConfig ( new_dict ) else : parser = self . _handle_config_parsing ( proc_log_config ) memory_file = self . _parser_to_string_io ( parser ) fileConfig ( memory_file , disable_existing_loggers = False ) if log_stdout : std_name , std_level = self . log_stdout stdout = StdoutToLogger ( std_name , log_level = std_level ) stdout . start ( ) self . _tools . append ( stdout )
def finalize ( self , remove_all_handlers = True ) : for tool in self . _tools : tool . finalize ( ) self . _tools = [ ] self . _stdout_to_logger = None for config in ( self . _sp_config , self . _mp_config ) : if hasattr ( config , 'close' ) : config . close ( ) self . _sp_config = None self . _mp_config = None if remove_all_handlers : self . tabula_rasa ( )
def start ( self ) : if sys . stdout is not self : self . _original_steam = sys . stdout sys . stdout = self self . _redirection = True if self . _redirection : print ( 'Established redirection of `stdout`.' )
def write ( self , buf ) : if not self . _recursion : self . _recursion = True try : for line in buf . rstrip ( ) . splitlines ( ) : self . _logger . log ( self . _log_level , line . rstrip ( ) ) finally : self . _recursion = False else : sys . __stderr__ . write ( 'ERROR: Recursion in Stream redirection!' )
def get_all_attributes ( instance ) : try : result_dict = instance . __dict__ . copy ( ) except AttributeError : result_dict = { } if hasattr ( instance , '__all_slots__' ) : all_slots = instance . __all_slots__ else : all_slots = slots . get_all_slots ( instance . __class__ ) for slot in all_slots : result_dict [ slot ] = getattr ( instance , slot ) result_dict . pop ( '__dict__' , None ) result_dict . pop ( '__weakref__' , None ) return result_dict
def kwargs_mutual_exclusive ( param1_name , param2_name , map2to1 = None ) : def wrapper ( func ) : @ functools . wraps ( func ) def new_func ( * args , * * kwargs ) : if param2_name in kwargs : if param1_name in kwargs : raise ValueError ( 'You cannot specify `%s` and `%s` at the same time, ' 'they are mutually exclusive.' % ( param1_name , param2_name ) ) param2 = kwargs . pop ( param2_name ) if map2to1 is not None : param1 = map2to1 ( param2 ) else : param1 = param2 kwargs [ param1_name ] = param1 return func ( * args , * * kwargs ) return new_func return wrapper
def not_in_run ( func ) : doc = func . __doc__ na_string = '''\nATTENTION: This function is not available during a single run!\n''' if doc is not None : func . __doc__ = '\n' . join ( [ doc , na_string ] ) func . _not_in_run = True @ functools . wraps ( func ) def new_func ( self , * args , * * kwargs ) : if self . _is_run : raise TypeError ( 'Function `%s` is not available during a single run.' % func . __name__ ) return func ( self , * args , * * kwargs ) return new_func
def with_open_store ( func ) : doc = func . __doc__ na_string = '''\nATTENTION: This function can only be used if the store is open!\n''' if doc is not None : func . __doc__ = '\n' . join ( [ doc , na_string ] ) func . _with_open_store = True @ functools . wraps ( func ) def new_func ( self , * args , * * kwargs ) : if not self . traj . v_storage_service . is_open : raise TypeError ( 'Function `%s` is only available if the storage is open.' % func . __name__ ) return func ( self , * args , * * kwargs ) return new_func
def prefix_naming ( cls ) : if hasattr ( cls , '__getattr__' ) : raise TypeError ( '__getattr__ already defined' ) cls . __getattr__ = _prfx_getattr_ cls . __setattr__ = _prfx_setattr_ return cls
def add_params ( traj ) : traj . v_standard_parameter = Brian2Parameter traj . v_fast_access = True traj . f_add_parameter ( 'Net.C' , 281 * pF ) traj . f_add_parameter ( 'Net.gL' , 30 * nS ) traj . f_add_parameter ( 'Net.EL' , - 70.6 * mV ) traj . f_add_parameter ( 'Net.VT' , - 50.4 * mV ) traj . f_add_parameter ( 'Net.DeltaT' , 2 * mV ) traj . f_add_parameter ( 'Net.tauw' , 40 * ms ) traj . f_add_parameter ( 'Net.a' , 4 * nS ) traj . f_add_parameter ( 'Net.b' , 0.08 * nA ) traj . f_add_parameter ( 'Net.I' , .8 * nA ) traj . f_add_parameter ( 'Net.Vcut' , 'vm > 0*mV' ) traj . f_add_parameter ( 'Net.N' , 50 ) eqs = traj . f_add_parameter ( 'Net.eqs' , eqs ) traj . f_add_parameter ( 'reset' , 'vm=Vr;w+=b' )
def run_net ( traj ) : eqs = traj . eqs namespace = traj . Net . f_to_dict ( short_names = True , fast_access = True ) neuron = NeuronGroup ( traj . N , model = eqs , threshold = traj . Vcut , reset = traj . reset , namespace = namespace ) neuron . vm = traj . EL neuron . w = traj . a * ( neuron . vm - traj . EL ) neuron . Vr = linspace ( - 48.3 * mV , - 47.7 * mV , traj . N ) print ( 'Initial Run' ) net = Network ( neuron ) net . run ( 100 * ms , report = 'text' ) MSpike = SpikeMonitor ( neuron ) net . add ( MSpike ) MStateV = StateMonitor ( neuron , variables = [ 'vm' ] , record = [ 1 , 2 , 3 ] ) net . add ( MStateV ) print ( 'Measurement run' ) net . run ( 500 * ms , report = 'text' ) traj . v_standard_result = Brian2MonitorResult traj . f_add_result ( 'SpikeMonitor' , MSpike ) traj . f_add_result ( 'StateMonitorV' , MStateV )
def add_parameters ( traj ) : traj . f_add_parameter ( 'steps' , 10000 , comment = 'Number of time steps to simulate' ) traj . f_add_parameter ( 'dt' , 0.01 , comment = 'Step size' ) traj . f_add_parameter ( ArrayParameter , 'initial_conditions' , np . array ( [ 0.0 , 0.0 , 0.0 ] ) , comment = 'Our initial conditions, as default we will start from' ' origin!' ) traj . f_add_parameter ( 'func_params.sigma' , 10.0 ) traj . f_add_parameter ( 'func_params.beta' , 8.0 / 3.0 ) traj . f_add_parameter ( 'func_params.rho' , 28.0 ) #For the fun of it we will annotate the  group traj . func_params . v_annotations . info = 'This group contains as default the original values chosen ' 'by Edward Lorenz in 1963. Check it out on wikipedia ' '(https://en.wikipedia.org/wiki/Lorenz_attractor)!'
def _create_storage ( storage_service , trajectory = None , * * kwargs ) : kwargs_copy = kwargs . copy ( ) kwargs_copy [ 'trajectory' ] = trajectory matching_kwargs = get_matching_kwargs ( storage_service , kwargs_copy ) storage_service = storage_service ( * * matching_kwargs ) unused_kwargs = set ( kwargs . keys ( ) ) - set ( matching_kwargs . keys ( ) ) return storage_service , unused_kwargs
def add_parameters ( traj ) : assert ( isinstance ( traj , Trajectory ) ) scale = traj . simulation . scale traj . v_standard_parameter = Brian2Parameter model_eqs = conn_eqs = traj . f_add_parameter ( 'model.eqs' , model_eqs , comment = 'The differential equation for the neuron model' ) traj . f_add_parameter ( 'model.synaptic.eqs' , conn_eqs , comment = 'The differential equation for the synapses. ' 'PRE will be replaced by `i` or `e` depending ' 'on the source population' ) traj . f_add_parameter ( 'model.synaptic.tau1' , 1 * ms , comment = 'The decay time' ) traj . f_add_parameter ( 'model.synaptic.tau2_e' , 3 * ms , comment = 'The rise time, excitatory' ) traj . f_add_parameter ( 'model.synaptic.tau2_i' , 2 * ms , comment = 'The rise time, inhibitory' ) traj . f_add_parameter ( 'model.V_th' , 'V >= 1.0' , comment = "Threshold value" ) traj . f_add_parameter ( 'model.reset_func' , 'V=0.0' , comment = "String representation of reset function" ) traj . f_add_parameter ( 'model.refractory' , 5 * ms , comment = "Absolute refractory period" ) traj . f_add_parameter ( 'model.N_e' , int ( 2000 * scale ) , comment = "Amount of excitatory neurons" ) traj . f_add_parameter ( 'model.N_i' , int ( 500 * scale ) , comment = "Amount of inhibitory neurons" ) traj . f_add_parameter ( 'model.tau_e' , 15 * ms , comment = "Membrane time constant, excitatory" ) traj . f_add_parameter ( 'model.tau_i' , 10 * ms , comment = "Membrane time constant, inhibitory" ) traj . f_add_parameter ( 'model.mu_e_min' , 1.1 , comment = "Lower bound for bias, excitatory" ) traj . f_add_parameter ( 'model.mu_e_max' , 1.2 , comment = "Upper bound for bias, excitatory" ) traj . f_add_parameter ( 'model.mu_i_min' , 1.0 , comment = "Lower bound for bias, inhibitory" ) traj . f_add_parameter ( 'model.mu_i_max' , 1.05 , comment = "Upper bound for bias, inhibitory" )
def add_parameters ( traj ) : assert ( isinstance ( traj , Trajectory ) ) traj . v_standard_parameter = Brian2Parameter scale = traj . simulation . scale traj . f_add_parameter ( 'connections.R_ee' , 1.0 , comment = 'Scaling factor for clustering' ) traj . f_add_parameter ( 'connections.clustersize_e' , 100 , comment = 'Size of a cluster' ) traj . f_add_parameter ( 'connections.strength_factor' , 2.5 , comment = 'Factor for scaling cluster weights' ) traj . f_add_parameter ( 'connections.p_ii' , 0.25 , comment = 'Connection probability from inhibitory to inhibitory' ) traj . f_add_parameter ( 'connections.p_ei' , 0.25 , comment = 'Connection probability from inhibitory to excitatory' ) traj . f_add_parameter ( 'connections.p_ie' , 0.25 , comment = 'Connection probability from excitatory to inhibitory' ) traj . f_add_parameter ( 'connections.p_ee' , 0.1 , comment = 'Connection probability from excitatory to excitatory' ) traj . f_add_parameter ( 'connections.J_ii' , 0.027 / np . sqrt ( scale ) , comment = 'Connection strength from inhibitory to inhibitory' ) traj . f_add_parameter ( 'connections.J_ei' , 0.032 / np . sqrt ( scale ) , comment = 'Connection strength from inhibitory to excitatroy' ) traj . f_add_parameter ( 'connections.J_ie' , 0.009 / np . sqrt ( scale ) , comment = 'Connection strength from excitatory to inhibitory' ) traj . f_add_parameter ( 'connections.J_ee' , 0.012 / np . sqrt ( scale ) , comment = 'Connection strength from excitatory to excitatory' )
def add_parameters ( self , traj ) : par = traj . f_add_parameter ( Brian2Parameter , 'simulation.durations.initial_run' , 500 * ms , comment = 'Initialisation run for more realistic ' 'measurement conditions.' ) par . v_annotations . order = 0 par = traj . f_add_parameter ( Brian2Parameter , 'simulation.durations.measurement_run' , 1500 * ms , comment = 'Measurement run that is considered for ' 'statistical evaluation' ) par . v_annotations . order = 1
def _add_monitors ( self , traj , network , network_dict ) : neurons_e = network_dict [ 'neurons_e' ] monitor_list = [ ] self . spike_monitor = SpikeMonitor ( neurons_e ) monitor_list . append ( self . spike_monitor ) self . V_monitor = StateMonitor ( neurons_e , 'V' , record = list ( traj . neuron_records ) ) monitor_list . append ( self . V_monitor ) self . I_syn_e_monitor = StateMonitor ( neurons_e , 'I_syn_e' , record = list ( traj . neuron_records ) ) monitor_list . append ( self . I_syn_e_monitor ) self . I_syn_i_monitor = StateMonitor ( neurons_e , 'I_syn_i' , record = list ( traj . neuron_records ) ) monitor_list . append ( self . I_syn_i_monitor ) network . add ( * monitor_list ) network_dict [ 'monitors' ] = monitor_list
def _plot_result ( self , traj , result_name ) : result = traj . f_get ( result_name ) varname = result . record_variables [ 0 ] values = result [ varname ] times = result . t record = result . record for idx , celia_neuron in enumerate ( record ) : plt . subplot ( len ( record ) , 1 , idx + 1 ) plt . plot ( times , values [ idx , : ] ) if idx == 0 : plt . title ( '%s' % varname ) if idx == 1 : plt . ylabel ( '%s' % ( varname ) ) if idx == len ( record ) - 1 : plt . xlabel ( 't' )
def _print_graphs ( self , traj ) : print_folder = self . _make_folder ( traj ) plt . figure ( ) plt . scatter ( self . spike_monitor . t , self . spike_monitor . i , s = 1 ) plt . xlabel ( 't' ) plt . ylabel ( 'Exc. Neurons' ) plt . title ( 'Spike Raster Plot' ) filename = os . path . join ( print_folder , 'spike.png' ) print ( 'Current plot: %s ' % filename ) plt . savefig ( filename ) plt . close ( ) fig = plt . figure ( ) self . _plot_result ( traj , 'monitors.V' ) filename = os . path . join ( print_folder , 'V.png' ) print ( 'Current plot: %s ' % filename ) fig . savefig ( filename ) plt . close ( ) plt . figure ( ) self . _plot_result ( traj , 'monitors.I_syn_e' ) filename = os . path . join ( print_folder , 'I_syn_e.png' ) print ( 'Current plot: %s ' % filename ) plt . savefig ( filename ) plt . close ( ) plt . figure ( ) self . _plot_result ( traj , 'monitors.I_syn_i' ) filename = os . path . join ( print_folder , 'I_syn_i.png' ) print ( 'Current plot: %s ' % filename ) plt . savefig ( filename ) plt . close ( ) if not traj . analysis . show_plots : plt . close ( 'all' ) else : plt . show ( )
def get_batch ( ) : optlist , args = getopt . getopt ( sys . argv [ 1 : ] , '' , longopts = 'batch=' ) batch = 0 for o , a in optlist : if o == '--batch' : batch = int ( a ) print ( 'Found batch %d' % batch ) return batch
def explore_batch ( traj , batch ) : explore_dict = { } explore_dict [ 'sigma' ] = np . arange ( 10.0 * batch , 10.0 * ( batch + 1 ) , 1.0 ) . tolist ( ) traj . f_explore ( explore_dict )
def vars ( self ) : if self . _vars is None : self . _vars = NNTreeNodeVars ( self ) return self . _vars
def func ( self ) : if self . _func is None : self . _func = NNTreeNodeFunc ( self ) return self . _func
def _rename ( self , full_name ) : self . _full_name = full_name if full_name : self . _name = full_name . rsplit ( '.' , 1 ) [ - 1 ]
def _set_details ( self , depth , branch , run_branch ) : self . _depth = depth self . _branch = branch self . _run_branch = run_branch
def _determine_types ( start_node , first_name , add_leaf , add_link ) : if start_node . v_is_root : where = first_name else : where = start_node . _branch if where in SUBTREE_MAPPING : type_tuple = SUBTREE_MAPPING [ where ] else : type_tuple = ( GROUP , LEAF ) if add_link : return type_tuple [ 0 ] , LINK if add_leaf : return type_tuple else : return type_tuple [ 0 ] , type_tuple [ 0 ]
def _create_link ( self , act_node , name , instance ) : act_node . _links [ name ] = instance act_node . _children [ name ] = instance full_name = instance . v_full_name if full_name not in self . _root_instance . _linked_by : self . _root_instance . _linked_by [ full_name ] = { } linking = self . _root_instance . _linked_by [ full_name ] if act_node . v_full_name not in linking : linking [ act_node . v_full_name ] = ( act_node , set ( ) ) linking [ act_node . v_full_name ] [ 1 ] . add ( name ) if name not in self . _links_count : self . _links_count [ name ] = 0 self . _links_count [ name ] = self . _links_count [ name ] + 1 self . _logger . debug ( 'Added link `%s` under `%s` pointing ' 'to `%s`.' % ( name , act_node . v_full_name , instance . v_full_name ) ) return instance
def _create_any_group ( self , parent_node , name , type_name , instance = None , constructor = None , args = None , kwargs = None ) : if args is None : args = [ ] if kwargs is None : kwargs = { } full_name = self . _make_full_name ( parent_node . v_full_name , name ) if instance is None : if constructor is None : if type_name == RESULT_GROUP : constructor = ResultGroup elif type_name == PARAMETER_GROUP : constructor = ParameterGroup elif type_name == CONFIG_GROUP : constructor = ConfigGroup elif type_name == DERIVED_PARAMETER_GROUP : constructor = DerivedParameterGroup elif type_name == GROUP : constructor = NNGroupNode else : raise RuntimeError ( 'You shall not pass!' ) instance = self . _root_instance . _construct_instance ( constructor , full_name , * args , * * kwargs ) else : instance . _rename ( full_name ) if type_name == RESULT_GROUP : if type ( instance ) in ( NNGroupNode , ParameterGroup , ConfigGroup , DerivedParameterGroup ) : raise TypeError ( 'You cannot add a `%s` type of group under results' % str ( type ( instance ) ) ) elif type_name == PARAMETER_GROUP : if type ( instance ) in ( NNGroupNode , ResultGroup , ConfigGroup , DerivedParameterGroup ) : raise TypeError ( 'You cannot add a `%s` type of group under parameters' % str ( type ( instance ) ) ) elif type_name == CONFIG_GROUP : if type ( instance ) in ( NNGroupNode , ParameterGroup , ResultGroup , DerivedParameterGroup ) : raise TypeError ( 'You cannot add a `%s` type of group under config' % str ( type ( instance ) ) ) elif type_name == DERIVED_PARAMETER_GROUP : if type ( instance ) in ( NNGroupNode , ParameterGroup , ConfigGroup , ResultGroup ) : raise TypeError ( 'You cannot add a `%s` type of group under derived ' 'parameters' % str ( type ( instance ) ) ) elif type_name == GROUP : if type ( instance ) in ( ResultGroup , ParameterGroup , ConfigGroup , DerivedParameterGroup ) : raise TypeError ( 'You cannot add a `%s` type of group under other data' % str ( type ( instance ) ) ) else : raise RuntimeError ( 'You shall not pass!' ) self . _set_details_tree_node ( parent_node , name , instance ) instance . _nn_interface = self self . _root_instance . _all_groups [ instance . v_full_name ] = instance self . _add_to_nodes_and_leaves ( instance ) parent_node . _children [ name ] = instance parent_node . _groups [ name ] = instance return instance
def _add_group_from_storage ( self , args , kwargs ) : return self . _nn_interface . _add_generic ( self , type_name = GROUP , group_type_name = GROUP , args = args , kwargs = kwargs , add_prefix = False , check_naming = False )
def _add_leaf_from_storage ( self , args , kwargs ) : return self . _nn_interface . _add_generic ( self , type_name = LEAF , group_type_name = GROUP , args = args , kwargs = kwargs , add_prefix = False , check_naming = False )
def f_dir_data ( self ) : if ( self . _nn_interface is not None and self . _nn_interface . _root_instance is not None and self . v_root . v_auto_load ) : try : if self . v_is_root : self . f_load ( recursive = True , max_depth = 1 , load_data = pypetconstants . LOAD_SKELETON , with_meta_data = False , with_run_information = False ) else : self . f_load ( recursive = True , max_depth = 1 , load_data = pypetconstants . LOAD_SKELETON ) except Exception as exc : pass return list ( self . _children . keys ( ) )
def unit_from_expression ( expr ) : if expr == '1' : return get_unit_fast ( 1 ) elif isinstance ( expr , str ) : mod = ast . parse ( expr , mode = 'eval' ) expr = mod . body return unit_from_expression ( expr ) elif expr . __class__ is ast . Name : return ALLUNITS [ expr . id ] elif expr . __class__ is ast . Num : return expr . n elif expr . __class__ is ast . UnaryOp : op = expr . op . __class__ . __name__ operand = unit_from_expression ( expr . operand ) if op == 'USub' : return - operand else : raise SyntaxError ( "Unsupported operation " + op ) elif expr . __class__ is ast . BinOp : op = expr . op . __class__ . __name__ left = unit_from_expression ( expr . left ) right = unit_from_expression ( expr . right ) if op == 'Add' : u = left + right elif op == 'Sub' : u = left - right elif op == 'Mult' : u = left * right elif op == 'Div' : u = left / right elif op == 'Pow' : n = unit_from_expression ( expr . right ) u = left ** n elif op == 'Mod' : u = left % right else : raise SyntaxError ( "Unsupported operation " + op ) return u else : raise RuntimeError ( 'You shall not pass' )
def f_supports ( self , data ) : if isinstance ( data , Quantity ) : return True elif super ( Brian2Parameter , self ) . f_supports ( data ) : return True return False
def _supports ( self , data ) : if isinstance ( data , Quantity ) : return True elif super ( Brian2Result , self ) . _supports ( data ) : return True return False
def add_commit_variables ( traj , commit ) : git_time_value = time . strftime ( '%Y_%m_%d_%Hh%Mm%Ss' , time . localtime ( commit . committed_date ) ) git_short_name = str ( commit . hexsha [ 0 : 7 ] ) git_commit_name = 'commit_%s_' % git_short_name git_commit_name = 'git.' + git_commit_name + git_time_value if not traj . f_contains ( 'config.' + git_commit_name , shortcuts = False ) : git_commit_name += '.' traj . f_add_config ( git_commit_name + 'hexsha' , commit . hexsha , comment = 'SHA-1 hash of commit' ) traj . f_add_config ( git_commit_name + 'name_rev' , commit . name_rev , comment = 'String describing the commits hex sha based on ' 'the closest Reference' ) traj . f_add_config ( git_commit_name + 'committed_date' , commit . committed_date , comment = 'Date of commit as unix epoch seconds' ) traj . f_add_config ( git_commit_name + 'message' , str ( commit . message ) , comment = 'The commit message' )
def _get_argspec ( func ) : if inspect . isclass ( func ) : func = func . __init__ if not inspect . isfunction ( func ) : return [ ] , False parameters = inspect . signature ( func ) . parameters args = [ ] uses_starstar = False for par in parameters . values ( ) : if ( par . kind == inspect . Parameter . POSITIONAL_OR_KEYWORD or par . kind == inspect . Parameter . KEYWORD_ONLY ) : args . append ( par . name ) elif par . kind == inspect . Parameter . VAR_KEYWORD : uses_starstar = True return args , uses_starstar
def get_matching_kwargs ( func , kwargs ) : args , uses_startstar = _get_argspec ( func ) if uses_startstar : return kwargs . copy ( ) else : matching_kwargs = dict ( ( k , kwargs [ k ] ) for k in args if k in kwargs ) return matching_kwargs
def format_time ( timestamp ) : format_string = '%Y_%m_%d_%Hh%Mm%Ss' formatted_time = datetime . datetime . fromtimestamp ( timestamp ) . strftime ( format_string ) return formatted_time
def port_to_tcp ( port = None ) : #address = 'tcp://' + socket.gethostbyname(socket.getfqdn()) domain_name = socket . getfqdn ( ) try : addr_list = socket . getaddrinfo ( domain_name , None ) except Exception : addr_list = socket . getaddrinfo ( '127.0.0.1' , None ) family , socktype , proto , canonname , sockaddr = addr_list [ 0 ] host = convert_ipv6 ( sockaddr [ 0 ] ) address = 'tcp://' + host if port is None : port = ( ) if not isinstance ( port , int ) : context = zmq . Context ( ) try : socket_ = context . socket ( zmq . REP ) socket_ . ipv6 = is_ipv6 ( address ) port = socket_ . bind_to_random_port ( address , * port ) except Exception : print ( 'Could not connect to {} using {}' . format ( address , addr_list ) ) pypet_root_logger = logging . getLogger ( 'pypet' ) pypet_root_logger . exception ( 'Could not connect to {}' . format ( address ) ) raise socket_ . close ( ) context . term ( ) return address + ':' + str ( port )
def racedirs ( path ) : if os . path . isfile ( path ) : raise IOError ( 'Path `%s` is already a file not a directory' ) while True : try : if os . path . isdir ( path ) : break os . makedirs ( path ) except EnvironmentError as exc : if exc . errno != 17 : raise
def _reset ( self , index , total , percentage_step , length ) : self . _start_time = datetime . datetime . now ( ) self . _start_index = index self . _current_index = index self . _percentage_step = percentage_step self . _total = float ( total ) self . _total_minus_one = total - 1 self . _length = length self . _norm_factor = total * percentage_step / 100.0 self . _current_interval = int ( ( index + 1.0 ) / self . _norm_factor )
def _get_remaining ( self , index ) : try : current_time = datetime . datetime . now ( ) time_delta = current_time - self . _start_time try : total_seconds = time_delta . total_seconds ( ) except AttributeError : total_seconds = ( ( time_delta . microseconds + ( time_delta . seconds + time_delta . days * 24 * 3600 ) * 10 ** 6 ) / 10.0 ** 6 ) remaining_seconds = int ( ( self . _total - self . _start_index - 1.0 ) * total_seconds / float ( index - self . _start_index ) - total_seconds ) remaining_delta = datetime . timedelta ( seconds = remaining_seconds ) remaining_str = ', remaining: ' + str ( remaining_delta ) except ZeroDivisionError : remaining_str = '' return remaining_str
def f_remove ( self , key ) : key = self . _translate_key ( key ) try : del self . _dict [ key ] except KeyError : raise AttributeError ( 'Your annotations do not contain %s' % key )
def f_ann_to_str ( self ) : resstr = '' for key in sorted ( self . _dict . keys ( ) ) : resstr += '%s=%s; ' % ( key , str ( self . _dict [ key ] ) ) return resstr [ : - 2 ]
def _supports ( self , item ) : result = super ( SharedResult , self ) . _supports ( item ) result = result or type ( item ) in SharedResult . SUPPORTED_DATA return result
def create_shared_data ( self , name = None , * * kwargs ) : if name is None : item = self . f_get ( ) else : item = self . f_get ( name ) return item . create_shared_data ( * * kwargs )
def send_done ( self ) : self . start ( test_connection = False ) self . _logger . debug ( 'Sending shutdown signal' ) self . _req_rep ( ZMQServer . DONE )
def _req_rep_retry ( self , request ) : retries_left = self . RETRIES while retries_left : self . _logger . log ( 1 , 'Sending REQ `%s`' , request ) self . _send_request ( request ) socks = dict ( self . _poll . poll ( self . TIMEOUT ) ) if socks . get ( self . _socket ) == zmq . POLLIN : response = self . _receive_response ( ) self . _logger . log ( 1 , 'Received REP `%s`' , response ) return response , self . RETRIES - retries_left else : self . _logger . debug ( 'No response from server (%d retries left)' % retries_left ) self . _close_socket ( confused = True ) retries_left -= 1 if retries_left == 0 : raise RuntimeError ( 'Server seems to be offline!' ) time . sleep ( self . SLEEP ) self . _start_socket ( )
def _put_on_queue ( self , to_put ) : old = self . pickle_queue self . pickle_queue = False try : self . queue . put ( to_put , block = True ) finally : self . pickle_queue = old
def _put_on_pipe ( self , to_put ) : self . acquire_lock ( ) self . _send_chunks ( to_put ) self . release_lock ( )
def _handle_data ( self , msg , args , kwargs ) : stop = False try : if msg == 'DONE' : stop = True elif msg == 'STORE' : if 'msg' in kwargs : store_msg = kwargs . pop ( 'msg' ) else : store_msg = args [ 0 ] args = args [ 1 : ] if 'stuff_to_store' in kwargs : stuff_to_store = kwargs . pop ( 'stuff_to_store' ) else : stuff_to_store = args [ 0 ] args = args [ 1 : ] trajectory_name = kwargs [ 'trajectory_name' ] if self . _trajectory_name != trajectory_name : if self . _storage_service . is_open : self . _close_file ( ) self . _trajectory_name = trajectory_name self . _open_file ( ) self . _storage_service . store ( store_msg , stuff_to_store , * args , * * kwargs ) self . _storage_service . store ( pypetconstants . FLUSH , None ) self . _check_and_collect_garbage ( ) else : raise RuntimeError ( 'You queued something that was not ' 'intended to be queued. I did not understand message ' '`%s`.' % msg ) except Exception : self . _logger . exception ( 'ERROR occurred during storing!' ) time . sleep ( 0.01 ) pass return stop
def run ( self ) : try : while True : msg , args , kwargs = self . _receive_data ( ) stop = self . _handle_data ( msg , args , kwargs ) if stop : break finally : if self . _storage_service . is_open : self . _close_file ( ) self . _trajectory_name = ''
def _receive_data ( self ) : result = self . queue . get ( block = True ) if hasattr ( self . queue , 'task_done' ) : self . queue . task_done ( ) return result
def _receive_data ( self ) : while True : while len ( self . _buffer ) < self . max_size and self . conn . poll ( ) : data = self . _read_chunks ( ) if data is not None : self . _buffer . append ( data ) if len ( self . _buffer ) > 0 : return self . _buffer . popleft ( )
def store ( self , * args , * * kwargs ) : try : self . acquire_lock ( ) return self . _storage_service . store ( * args , * * kwargs ) finally : if self . lock is not None : try : self . release_lock ( ) except RuntimeError : self . _logger . error ( 'Could not release lock `%s`!' % str ( self . lock ) )
def store ( self , msg , stuff_to_store , * args , * * kwargs ) : trajectory_name = kwargs [ 'trajectory_name' ] if trajectory_name not in self . references : self . references [ trajectory_name ] = [ ] self . references [ trajectory_name ] . append ( ( msg , cp . copy ( stuff_to_store ) , args , kwargs ) )
def store_references ( self , references ) : for trajectory_name in references : self . _storage_service . store ( pypetconstants . LIST , references [ trajectory_name ] , trajectory_name = trajectory_name ) self . _check_and_collect_garbage ( )
def parse_config ( init_func ) : @ functools . wraps ( init_func ) def new_func ( env , * args , * * kwargs ) : config_interpreter = ConfigInterpreter ( kwargs ) new_kwargs = config_interpreter . interpret ( ) init_func ( env , * args , * * new_kwargs ) config_interpreter . add_parameters ( env . traj ) return new_func
def _collect_section ( self , section ) : kwargs = { } try : if self . parser . has_section ( section ) : options = self . parser . options ( section ) for option in options : str_val = self . parser . get ( section , option ) val = ast . literal_eval ( str_val ) kwargs [ option ] = val return kwargs except : raise
def _collect_config ( self ) : kwargs = { } sections = ( 'storage_service' , 'trajectory' , 'environment' ) for section in sections : kwargs . update ( self . _collect_section ( section ) ) return kwargs
def interpret ( self ) : if self . config_file : new_kwargs = self . _collect_config ( ) for key in new_kwargs : if key not in self . kwargs : self . kwargs [ key ] = new_kwargs [ key ] if not use_simple_logging ( self . kwargs ) and 'log_config' not in self . kwargs : self . kwargs [ 'log_config' ] = self . config_file return self . kwargs
def add_parameters ( self , traj ) : if self . config_file : parameters = self . _collect_section ( 'parameters' ) for name in parameters : value = parameters [ name ] if not isinstance ( value , tuple ) : value = ( value , ) traj . f_add_parameter ( name , * value ) config = self . _collect_section ( 'config' ) for name in config : value = config [ name ] if not isinstance ( value , tuple ) : value = ( value , ) traj . f_add_config ( name , * value )
def _overview_group ( self ) : if self . _overview_group_ is None : self . _overview_group_ = self . _all_create_or_get_groups ( 'overview' ) [ 0 ] return self . _overview_group_
def _trj_load_exploration ( self , traj ) : if hasattr ( self . _overview_group , 'explorations' ) : explorations_table = self . _overview_group . _f_get_child ( 'explorations' ) for row in explorations_table . iterrows ( ) : param_name = row [ 'explorations' ] . decode ( 'utf-8' ) if param_name not in traj . _explored_parameters : traj . _explored_parameters [ param_name ] = None else : for what in ( 'parameters' , 'derived_parameters' ) : if hasattr ( self . _trajectory_group , what ) : parameters = self . _trajectory_group . _f_get_child ( what ) for group in parameters . _f_walk_groups ( ) : if self . _all_get_from_attrs ( group , HDF5StorageService . LENGTH ) : group_location = group . _v_pathname full_name = '.' . join ( group_location . split ( '/' ) [ 2 : ] ) traj . _explored_parameters [ full_name ] = None
def _trj_store_explorations ( self , traj ) : nexplored = len ( traj . _explored_parameters ) if nexplored > 0 : if hasattr ( self . _overview_group , 'explorations' ) : explorations_table = self . _overview_group . _f_get_child ( 'explorations' ) if len ( explorations_table ) != nexplored : self . _hdf5file . remove_node ( where = self . _overview_group , name = 'explorations' ) if not hasattr ( self . _overview_group , 'explorations' ) : explored_list = list ( traj . _explored_parameters . keys ( ) ) if explored_list : string_col = self . _all_get_table_col ( 'explorations' , explored_list , 'overview.explorations' ) else : string_col = pt . StringCol ( 1 ) description = { 'explorations' : string_col } explorations_table = self . _hdf5file . create_table ( where = self . _overview_group , name = 'explorations' , description = description ) rows = [ ( x . encode ( 'utf-8' ) , ) for x in explored_list ] if rows : explorations_table . append ( rows ) explorations_table . flush ( )
def _srvc_make_overview_tables ( self , tables_to_make , traj = None ) : for table_name in tables_to_make : paramdescriptiondict = { } expectedrows = 0 paramdescriptiondict [ 'location' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_LOCATION_LENGTH , pos = 0 ) paramdescriptiondict [ 'name' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH , pos = 1 ) paramdescriptiondict [ 'comment' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_COMMENT_LENGTH ) paramdescriptiondict [ 'value' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH , pos = 2 ) if table_name == 'config_overview' : if traj is not None : expectedrows = len ( traj . _config ) if table_name == 'parameters_overview' : if traj is not None : expectedrows = len ( traj . _parameters ) if table_name == 'explored_parameters_overview' : paramdescriptiondict [ 'range' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH ) paramdescriptiondict [ 'length' ] = pt . IntCol ( ) if traj is not None : expectedrows = len ( traj . _explored_parameters ) if table_name . endswith ( 'summary' ) : paramdescriptiondict [ 'hexdigest' ] = pt . StringCol ( 64 , pos = 10 ) if table_name == 'derived_parameters_overview' : expectedrows = self . _derived_parameters_per_run if traj is not None : expectedrows *= len ( traj ) expectedrows += len ( traj . _derived_parameters ) if table_name == 'results_overview' : expectedrows = self . _results_per_run if traj is not None : expectedrows *= len ( traj ) expectedrows += len ( traj . _results ) if expectedrows > 0 : paramtable = self . _all_get_or_create_table ( where = self . _overview_group , tablename = table_name , description = paramdescriptiondict , expectedrows = expectedrows ) else : paramtable = self . _all_get_or_create_table ( where = self . _overview_group , tablename = table_name , description = paramdescriptiondict ) paramtable . flush ( )
def _all_get_or_create_table ( self , where , tablename , description , expectedrows = None ) : where_node = self . _hdf5file . get_node ( where ) if not tablename in where_node : if not expectedrows is None : table = self . _hdf5file . create_table ( where = where_node , name = tablename , description = description , title = tablename , expectedrows = expectedrows , filters = self . _all_get_filters ( ) ) else : table = self . _hdf5file . create_table ( where = where_node , name = tablename , description = description , title = tablename , filters = self . _all_get_filters ( ) ) else : table = where_node . _f_get_child ( tablename ) return table
def _all_get_node_by_name ( self , name ) : path_name = name . replace ( '.' , '/' ) where = '/%s/%s' % ( self . _trajectory_name , path_name ) return self . _hdf5file . get_node ( where = where )
def _all_insert_into_row ( self , row , insert_dict ) : for key , val in insert_dict . items ( ) : try : row [ key ] = val except KeyError as ke : self . _logger . warning ( 'Could not write `%s` into a table, ' % key + repr ( ke ) )
def _all_create_or_get_group ( self , name , parent_hdf5_group = None ) : if not name in parent_hdf5_group : new_hdf5_group = self . _hdf5file . create_group ( where = parent_hdf5_group , name = name , title = name , filters = self . _all_get_filters ( ) ) return new_hdf5_group , True else : new_hdf5_group = parent_hdf5_group . _f_get_child ( name ) return new_hdf5_group , False
def _ann_store_annotations ( self , item_with_annotations , node , overwrite = False ) : if overwrite is True or overwrite == 'v_annotations' : annotated = self . _all_get_from_attrs ( node , HDF5StorageService . ANNOTATED ) if annotated : current_attrs = node . _v_attrs for attr_name in current_attrs . _v_attrnames : if attr_name . startswith ( HDF5StorageService . ANNOTATION_PREFIX ) : delattr ( current_attrs , attr_name ) delattr ( current_attrs , HDF5StorageService . ANNOTATED ) self . _hdf5file . flush ( ) if not item_with_annotations . v_annotations . f_is_empty ( ) : anno_dict = item_with_annotations . v_annotations . _dict current_attrs = node . _v_attrs changed = False for field_name in anno_dict : val = anno_dict [ field_name ] field_name_with_prefix = HDF5StorageService . ANNOTATION_PREFIX + field_name if field_name_with_prefix not in current_attrs : setattr ( current_attrs , field_name_with_prefix , val ) changed = True if changed : setattr ( current_attrs , HDF5StorageService . ANNOTATED , True ) self . _hdf5file . flush ( )
def _ann_load_annotations ( self , item_with_annotations , node ) : annotated = self . _all_get_from_attrs ( node , HDF5StorageService . ANNOTATED ) if annotated : annotations = item_with_annotations . v_annotations if not annotations . f_is_empty ( ) : raise TypeError ( 'Loading into non-empty annotations!' ) current_attrs = node . _v_attrs for attr_name in current_attrs . _v_attrnames : if attr_name . startswith ( HDF5StorageService . ANNOTATION_PREFIX ) : key = attr_name key = key . replace ( HDF5StorageService . ANNOTATION_PREFIX , '' ) data = getattr ( current_attrs , attr_name ) setattr ( annotations , key , data )
def _grp_load_group ( self , traj_group , load_data = pypetconstants . LOAD_DATA , with_links = True , recursive = False , max_depth = None , _traj = None , _as_new = False , _hdf5_group = None ) : if _hdf5_group is None : _hdf5_group = self . _all_get_node_by_name ( traj_group . v_full_name ) _traj = traj_group . v_root if recursive : parent_traj_node = traj_group . f_get_parent ( ) self . _tree_load_nodes_dfs ( parent_traj_node , load_data = load_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = 0 , trajectory = _traj , as_new = _as_new , hdf5_group = _hdf5_group ) else : if load_data == pypetconstants . LOAD_NOTHING : return elif load_data == pypetconstants . OVERWRITE_DATA : traj_group . v_annotations . f_empty ( ) traj_group . v_comment = '' self . _all_load_skeleton ( traj_group , _hdf5_group ) traj_group . _stored = not _as_new self . _node_processing_timer . signal_update ( )
def _all_load_skeleton ( self , traj_node , hdf5_group ) : if traj_node . v_annotations . f_is_empty ( ) : self . _ann_load_annotations ( traj_node , hdf5_group ) if traj_node . v_comment == '' : comment = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . COMMENT ) if comment is None : comment = '' traj_node . v_comment = comment
def _prm_write_shared_array ( self , key , data , hdf5_group , full_name , flag , * * kwargs ) : if flag == HDF5StorageService . ARRAY : self . _prm_write_into_array ( key , data , hdf5_group , full_name , * * kwargs ) elif flag in ( HDF5StorageService . CARRAY , HDF5StorageService . EARRAY , HDF5StorageService . VLARRAY ) : self . _prm_write_into_other_array ( key , data , hdf5_group , full_name , flag = flag , * * kwargs ) else : raise RuntimeError ( 'Flag `%s` of hdf5 data `%s` of `%s` not understood' % ( flag , key , full_name ) ) self . _hdf5file . flush ( )
def _prm_write_shared_table ( self , key , hdf5_group , fullname , * * kwargs ) : first_row = None description = None if 'first_row' in kwargs : first_row = kwargs . pop ( 'first_row' ) if not 'description' in kwargs : description = { } for colname in first_row : data = first_row [ colname ] column = self . _all_get_table_col ( key , [ data ] , fullname ) description [ colname ] = column if 'description' in kwargs : description = kwargs . pop ( 'description' ) if 'filters' in kwargs : filters = kwargs . pop ( 'filters' ) else : filters = self . _all_get_filters ( kwargs ) table = self . _hdf5file . create_table ( where = hdf5_group , name = key , description = description , filters = filters , * * kwargs ) table . flush ( ) if first_row is not None : row = table . row for key in description : row [ key ] = first_row [ key ] row . append ( ) table . flush ( )
def _lnk_delete_link ( self , link_name ) : translated_name = '/' + self . _trajectory_name + '/' + link_name . replace ( '.' , '/' ) link = self . _hdf5file . get_node ( where = translated_name ) link . _f_remove ( )
def _prm_make_description ( self , data , fullname ) : def _convert_lists_and_tuples ( series_of_data ) : """Converts lists and tuples to numpy arrays""" if isinstance ( series_of_data [ 0 ] , ( list , tuple ) ) : for idx , item in enumerate ( series_of_data ) : series_of_data [ idx ] = np . array ( item ) descriptiondict = { } original_data_type_dict = { } for key in data : val = data [ key ] self . _all_set_attributes_to_recall_natives ( val [ 0 ] , PTItemMock ( original_data_type_dict ) , HDF5StorageService . FORMATTED_COLUMN_PREFIX % key ) _convert_lists_and_tuples ( val ) col = self . _all_get_table_col ( key , val , fullname ) descriptiondict [ key ] = col return descriptiondict , original_data_type_dict
def _prm_get_longest_stringsize ( string_list ) : maxlength = 1 for stringar in string_list : if isinstance ( stringar , np . ndarray ) : if stringar . ndim > 0 : for string in stringar . ravel ( ) : maxlength = max ( len ( string ) , maxlength ) else : maxlength = max ( len ( stringar . tolist ( ) ) , maxlength ) else : maxlength = max ( len ( stringar ) , maxlength ) return int ( maxlength * 1.5 )
def make_set_name ( idx ) : GROUPSIZE = 1000 set_idx = idx // GROUPSIZE if set_idx >= 0 : return pypetconstants . FORMATTED_SET_NAME % set_idx else : return pypetconstants . SET_NAME_DUMMY
def _preset ( self , name , args , kwargs ) : if self . f_contains ( name , shortcuts = False ) : raise ValueError ( 'Parameter `%s` is already part of your trajectory, use the normal' 'accessing routine to change config.' % name ) else : self . _changed_default_parameters [ name ] = ( args , kwargs )
def _remove_exploration ( self ) : for param in self . _explored_parameters . values ( ) : if param . _stored : try : self . f_delete_item ( param ) except Exception : self . _logger . exception ( 'Could not delete expanded parameter `%s` ' 'from disk.' % param . v_full_name )
def _update_run_information ( self , run_information_dict ) : idx = run_information_dict [ 'idx' ] name = run_information_dict [ 'name' ] self . _run_information [ name ] = run_information_dict self . _updated_run_information . add ( idx )
def _add_run_info ( self , idx , name = '' , timestamp = 42.0 , finish_timestamp = 1.337 , runtime = 'forever and ever' , time = '>>Maybe time`s gone on strike' , completed = 0 , parameter_summary = 'Not yet my friend!' , short_environment_hexsha = 'N/A' ) : if idx in self . _single_run_ids : old_name = self . _single_run_ids [ idx ] del self . _single_run_ids [ old_name ] del self . _single_run_ids [ idx ] del self . _run_information [ old_name ] if name == '' : name = self . f_wildcard ( '$' , idx ) self . _single_run_ids [ name ] = idx self . _single_run_ids [ idx ] = name info_dict = { 'idx' : idx , 'timestamp' : timestamp , 'finish_timestamp' : finish_timestamp , 'runtime' : runtime , 'time' : time , 'completed' : completed , 'name' : name , 'parameter_summary' : parameter_summary , 'short_environment_hexsha' : short_environment_hexsha } self . _run_information [ name ] = info_dict self . _length = len ( self . _run_information )
def f_lock_parameters ( self ) : for par in self . _parameters . values ( ) : if not par . f_is_empty ( ) : par . f_lock ( )
def f_lock_derived_parameters ( self ) : for par in self . _derived_parameters . values ( ) : if not par . f_is_empty ( ) : par . f_lock ( )
def _make_reversed_wildcards ( self , old_length = - 1 ) : if len ( self . _reversed_wildcards ) > 0 : start = old_length else : start = - 1 for wildcards , func in self . _wildcard_functions . items ( ) : for irun in range ( start , len ( self ) ) : translated_name = func ( irun ) if not translated_name in self . _reversed_wildcards : self . _reversed_wildcards [ translated_name ] = ( [ ] , wildcards ) self . _reversed_wildcards [ translated_name ] [ 0 ] . append ( irun )
def _merge_single_runs ( self , other_trajectory , used_runs ) : count = len ( self ) run_indices = range ( len ( other_trajectory ) ) run_name_dict = OrderedDict ( ) to_store_groups_with_annotations = [ ] for idx in run_indices : if idx in used_runs : other_info_dict = other_trajectory . f_get_run_information ( idx ) time_ = other_info_dict [ 'time' ] timestamp = other_info_dict [ 'timestamp' ] completed = other_info_dict [ 'completed' ] short_environment_hexsha = other_info_dict [ 'short_environment_hexsha' ] finish_timestamp = other_info_dict [ 'finish_timestamp' ] runtime = other_info_dict [ 'runtime' ] new_idx = used_runs [ idx ] new_runname = self . f_wildcard ( '$' , new_idx ) run_name_dict [ idx ] = new_runname info_dict = dict ( idx = new_idx , time = time_ , timestamp = timestamp , completed = completed , short_environment_hexsha = short_environment_hexsha , finish_timestamp = finish_timestamp , runtime = runtime ) self . _add_run_info ( * * info_dict )
def _rename_full_name ( self , full_name , other_trajectory , used_runs = None , new_run_idx = None ) : split_name = full_name . split ( '.' ) for idx , name in enumerate ( split_name ) : if name in other_trajectory . _reversed_wildcards : run_indices , wildcards = other_trajectory . _reversed_wildcards [ name ] if new_run_idx is None : run_idx = None for run_jdx in run_indices : if run_jdx in used_runs : run_idx = used_runs [ run_jdx ] break elif run_jdx == - 1 : run_idx = - 1 break if run_idx is None : raise RuntimeError ( 'You shall not pass!' ) else : run_idx = new_run_idx new_name = self . f_wildcard ( wildcards [ 0 ] , run_idx ) split_name [ idx ] = new_name full_name = '.' . join ( split_name ) return full_name
def _make_single_run ( self ) : self . _is_run = False self . _new_nodes = OrderedDict ( ) self . _new_links = OrderedDict ( ) self . _is_run = True return self
def _set_start ( self ) : init_time = time . time ( ) formatted_time = datetime . datetime . fromtimestamp ( init_time ) . strftime ( '%Y_%m_%d_%Hh%Mm%Ss' ) run_info_dict = self . _run_information [ self . v_crun ] run_info_dict [ 'timestamp' ] = init_time run_info_dict [ 'time' ] = formatted_time if self . _environment_hexsha is not None : run_info_dict [ 'short_environment_hexsha' ] = self . _environment_hexsha [ 0 : 7 ]
def _set_finish ( self ) : run_info_dict = self . _run_information [ self . v_crun ] timestamp_run = run_info_dict [ 'timestamp' ] run_summary = self . _summarize_explored_parameters ( ) finish_timestamp_run = time . time ( ) findatetime = datetime . datetime . fromtimestamp ( finish_timestamp_run ) startdatetime = datetime . datetime . fromtimestamp ( timestamp_run ) runtime_run = str ( findatetime - startdatetime ) run_info_dict [ 'parameter_summary' ] = run_summary run_info_dict [ 'completed' ] = 1 run_info_dict [ 'finish_timestamp' ] = finish_timestamp_run run_info_dict [ 'runtime' ] = runtime_run
def _pool_single_run ( kwargs ) : wrap_mode = kwargs [ 'wrap_mode' ] traj = kwargs [ 'traj' ] traj . v_storage_service = _pool_single_run . storage_service if wrap_mode == pypetconstants . WRAP_MODE_LOCAL : traj . v_storage_service . free_references ( ) return _sigint_handling_single_run ( kwargs )
def _frozen_pool_single_run ( kwargs ) : idx = kwargs . pop ( 'idx' ) frozen_kwargs = _frozen_pool_single_run . kwargs frozen_kwargs . update ( kwargs ) traj = frozen_kwargs [ 'traj' ] traj . f_set_crun ( idx ) return _sigint_handling_single_run ( frozen_kwargs )
def _configure_pool ( kwargs ) : _pool_single_run . storage_service = kwargs [ 'storage_service' ] _configure_niceness ( kwargs ) _configure_logging ( kwargs , extract = False )
def _configure_frozen_pool ( kwargs ) : _frozen_pool_single_run . kwargs = kwargs _configure_niceness ( kwargs ) _configure_logging ( kwargs , extract = False ) traj = kwargs [ 'traj' ] traj . v_full_copy = kwargs [ 'full_copy' ]
def _process_single_run ( kwargs ) : _configure_niceness ( kwargs ) _configure_logging ( kwargs ) result_queue = kwargs [ 'result_queue' ] result = _sigint_handling_single_run ( kwargs ) result_queue . put ( result ) result_queue . close ( )
def _scoop_single_run ( kwargs ) : try : try : is_origin = scoop . IS_ORIGIN except AttributeError : is_origin = True if not is_origin : _configure_niceness ( kwargs ) _configure_logging ( kwargs ) return _single_run ( kwargs ) except Exception : scoop . logger . exception ( 'ERROR occurred during a single run!' ) raise
def _configure_niceness ( kwargs ) : niceness = kwargs [ 'niceness' ] if niceness is not None : try : try : current = os . nice ( 0 ) if niceness - current > 0 : os . nice ( niceness - current ) except AttributeError : psutil . Process ( ) . nice ( niceness ) except Exception as exc : sys . stderr . write ( 'Could not configure niceness because of: %s' % repr ( exc ) ) traceback . print_exc ( )
def _sigint_handling_single_run ( kwargs ) : try : graceful_exit = kwargs [ 'graceful_exit' ] if graceful_exit : sigint_handling . start ( ) if sigint_handling . hit : result = ( sigint_handling . SIGINT , None ) else : result = _single_run ( kwargs ) if sigint_handling . hit : result = ( sigint_handling . SIGINT , result ) return result return _single_run ( kwargs ) except : pypet_root_logger = logging . getLogger ( 'pypet' ) pypet_root_logger . exception ( 'ERROR occurred during a single run! ' ) raise
def _wrap_handling ( kwargs ) : _configure_logging ( kwargs , extract = False ) handler = kwargs [ 'handler' ] graceful_exit = kwargs [ 'graceful_exit' ] if graceful_exit : sigint_handling . start ( ) handler . run ( )
def f_supports ( self , data ) : dtype = type ( data ) if dtype is tuple or dtype is list : if len ( data ) == 0 : return False old_type = None for item in data : if not type ( item ) in pypetconstants . PARAMETER_SUPPORTED_DATA : return False if not old_type is None and old_type != type ( item ) : return False old_type = type ( item ) return True elif dtype is np . ndarray or dtype is np . matrix : if data . size == 0 : return False dtype = data . dtype if np . issubdtype ( dtype , np . str ) : dtype = np . str return dtype in pypetconstants . PARAMETER_SUPPORTED_DATA
def f_supports ( self , data ) : dtype = type ( data ) if dtype is tuple or dtype is list and len ( data ) == 0 : return True elif dtype is np . ndarray and data . size == 0 and data . ndim == 1 : return True else : return super ( ArrayParameter , self ) . f_supports ( data )
def _equal_values ( self , val1 , val2 ) : if self . _is_supported_matrix ( val1 ) : if self . _is_supported_matrix ( val2 ) : _ , _ , hash_tuple_1 = self . _serialize_matrix ( val1 ) _ , _ , hash_tuple_2 = self . _serialize_matrix ( val2 ) return hash ( hash_tuple_1 ) == hash ( hash_tuple_2 ) else : return False else : return super ( SparseParameter , self ) . _equal_values ( val1 , val2 )
def _is_supported_matrix ( data ) : return ( spsp . isspmatrix_csc ( data ) or spsp . isspmatrix_csr ( data ) or spsp . isspmatrix_bsr ( data ) or spsp . isspmatrix_dia ( data ) )
def f_translate_key ( self , key ) : if isinstance ( key , int ) : if key == 0 : key = self . v_name else : key = self . v_name + '_%d' % key return key
def f_remove ( self , * args ) : for arg in args : arg = self . f_translate_key ( arg ) if arg in self . _data : del self . _data [ arg ] else : raise AttributeError ( 'Your result `%s` does not contain %s.' % ( self . name_ , arg ) )
def _supports ( self , item ) : if SparseParameter . _is_supported_matrix ( item ) : return True else : return super ( SparseResult , self ) . _supports ( item )
def _store ( self ) : store_dict = { } for key , val in self . _data . items ( ) : store_dict [ key ] = pickle . dumps ( val , protocol = self . v_protocol ) store_dict [ PickleResult . PROTOCOL ] = self . v_protocol return store_dict
def main ( ) : folder = os . getcwd ( ) print ( 'Merging all files' ) merge_all_in_folder ( folder , delete_other_files = True , dynamic_imports = FunctionParameter , backup = False ) print ( 'Done' )
def create_session ( ) : ctx = saga . Context ( "UserPass" ) ctx . user_id = USER ctx . user_pass = PASSWORD session = saga . Session ( ) session . add_context ( ctx ) return session
def merge_trajectories ( session ) : jd = saga . job . Description ( ) jd . executable = 'python' jd . arguments = [ 'merge_trajs.py' ] jd . output = "mysagajob_merge.stdout" jd . error = "mysagajob_merge.stderr" jd . working_directory = WORKING_DIR js = saga . job . Service ( 'ssh://' + ADDRESS , session = session ) myjob = js . create_job ( jd ) print ( "\n...starting job...\n" ) myjob . run ( ) print ( "Job ID    : %s" % ( myjob . id ) ) print ( "Job State : %s" % ( myjob . state ) ) print ( "\n...waiting for job...\n" ) myjob . wait ( ) print ( "Job State : %s" % ( myjob . state ) ) print ( "Exitcode  : %s" % ( myjob . exit_code ) )
def start_jobs ( session ) : js = saga . job . Service ( 'ssh://' + ADDRESS , session = session ) batches = range ( 3 ) jobs = [ ] for batch in batches : print ( 'Starting batch %d' % batch ) jd = saga . job . Description ( ) jd . executable = 'python' jd . arguments = [ 'the_task.py --batch=' + str ( batch ) ] jd . output = "mysagajob.stdout" + str ( batch ) jd . error = "mysagajob.stderr" + str ( batch ) jd . working_directory = WORKING_DIR myjob = js . create_job ( jd ) print ( "Job ID    : %s" % ( myjob . id ) ) print ( "Job State : %s" % ( myjob . state ) ) print ( "\n...starting job...\n" ) myjob . run ( ) jobs . append ( myjob ) for myjob in jobs : print ( "Job ID    : %s" % ( myjob . id ) ) print ( "Job State : %s" % ( myjob . state ) ) print ( "\n...waiting for job...\n" ) myjob . wait ( ) print ( "Job State : %s" % ( myjob . state ) ) print ( "Exitcode  : %s" % ( myjob . exit_code ) )
def multiply ( traj ) : z = traj . x * traj . y traj . f_add_result ( 'z' , z = z , comment = 'I am the product of two reals!' )
def add_parameters ( traj ) : print ( 'Adding Parameters' ) traj . f_add_parameter ( 'neuron.V_init' , 0.0 , comment = 'The initial condition for the ' 'membrane potential' ) traj . f_add_parameter ( 'neuron.I' , 0.0 , comment = 'The externally applied current.' ) traj . f_add_parameter ( 'neuron.tau_V' , 10.0 , comment = 'The membrane time constant in milliseconds' ) traj . f_add_parameter ( 'neuron.tau_ref' , 5.0 , comment = 'The refractory period in milliseconds ' 'where the membrane potnetial ' 'is clamped.' ) traj . f_add_parameter ( 'simulation.duration' , 1000.0 , comment = 'The duration of the experiment in ' 'milliseconds.' ) traj . f_add_parameter ( 'simulation.dt' , 0.1 , comment = 'The step size of an Euler integration step.' )
def add_exploration ( traj ) : print ( 'Adding exploration of I and tau_ref' ) explore_dict = { 'neuron.I' : np . arange ( 0 , 1.01 , 0.01 ) . tolist ( ) , 'neuron.tau_ref' : [ 5.0 , 7.5 , 10.0 ] } explore_dict = cartesian_product ( explore_dict , ( 'neuron.tau_ref' , 'neuron.I' ) ) traj . f_explore ( explore_dict )
def make_filename ( traj ) : explored_parameters = traj . f_get_explored_parameters ( ) filename = '' for param in explored_parameters . values ( ) : short_name = param . v_name val = param . f_get ( ) filename += '%s_%s__' % ( short_name , str ( val ) ) return filename [ : - 2 ] + '.png'
def main ( ) : logger = logging . getLogger ( ) folder = os . path . join ( os . getcwd ( ) , 'experiments' , 'ca_patterns_pypet' ) if not os . path . isdir ( folder ) : os . makedirs ( folder ) filename = os . path . join ( folder , 'all_patterns.hdf5' ) env = Environment ( trajectory = 'cellular_automata' , multiproc = True , ncores = 4 , wrap_mode = 'QUEUE' , filename = filename , overwrite_file = True ) traj = env . traj traj . par . ncells = Parameter ( 'ncells' , 400 , 'Number of cells' ) traj . par . steps = Parameter ( 'steps' , 250 , 'Number of timesteps' ) traj . par . rule_number = Parameter ( 'rule_number' , 30 , 'The ca rule' ) traj . par . initial_name = Parameter ( 'initial_name' , 'random' , 'The type of initial state' ) traj . par . seed = Parameter ( 'seed' , 100042 , 'RNG Seed' ) exp_dict = { 'rule_number' : [ 10 , 30 , 90 , 110 , 184 ] , 'initial_name' : [ 'single' , 'random' ] , } exp_dict = cartesian_product ( exp_dict ) traj . f_explore ( exp_dict ) logger . info ( 'Starting Simulation' ) env . run ( wrap_automaton ) traj . f_load ( load_data = 2 ) logger . info ( 'Printing data' ) for idx , run_name in enumerate ( traj . f_iter_runs ( ) ) : filename = os . path . join ( folder , make_filename ( traj ) ) plot_pattern ( traj . crun . pattern , traj . rule_number , filename ) progressbar ( idx , len ( traj ) , logger = logger ) env . disable_logging ( )
def config_from_file ( filename , config = None ) : if config : try : with open ( filename , 'w' ) as fdesc : fdesc . write ( json . dumps ( config ) ) except IOError as error : logger . exception ( error ) return False return True else : if os . path . isfile ( filename ) : try : with open ( filename , 'r' ) as fdesc : return json . loads ( fdesc . read ( ) ) except IOError as error : return False else : return { }
def request_pin ( self ) : url = 'https://api.ecobee.com/authorize' params = { 'response_type' : 'ecobeePin' , 'client_id' : self . api_key , 'scope' : 'smartWrite' } try : request = requests . get ( url , params = params ) except RequestException : logger . warn ( "Error connecting to Ecobee.  Possible connectivity outage." "Could not request pin." ) return self . authorization_code = request . json ( ) [ 'code' ] self . pin = request . json ( ) [ 'ecobeePin' ] logger . error ( 'Please authorize your ecobee developer app with PIN code ' + self . pin + '\nGoto https://www.ecobee.com/consumerportal' '/index.html, click\nMy Apps, Add application, Enter Pin' ' and click Authorize.\nAfter authorizing, call request_' 'tokens() method.' )
def request_tokens ( self ) : url = 'https://api.ecobee.com/token' params = { 'grant_type' : 'ecobeePin' , 'code' : self . authorization_code , 'client_id' : self . api_key } try : request = requests . post ( url , params = params ) except RequestException : logger . warn ( "Error connecting to Ecobee.  Possible connectivity outage." "Could not request token." ) return if request . status_code == requests . codes . ok : self . access_token = request . json ( ) [ 'access_token' ] self . refresh_token = request . json ( ) [ 'refresh_token' ] self . write_tokens_to_file ( ) self . pin = None else : logger . warn ( 'Error while requesting tokens from ecobee.com.' ' Status code: ' + str ( request . status_code ) ) return
def refresh_tokens ( self ) : url = 'https://api.ecobee.com/token' params = { 'grant_type' : 'refresh_token' , 'refresh_token' : self . refresh_token , 'client_id' : self . api_key } request = requests . post ( url , params = params ) if request . status_code == requests . codes . ok : self . access_token = request . json ( ) [ 'access_token' ] self . refresh_token = request . json ( ) [ 'refresh_token' ] self . write_tokens_to_file ( ) return True else : self . request_pin ( )
def get_thermostats ( self ) : url = 'https://api.ecobee.com/1/thermostat' header = { 'Content-Type' : 'application/json;charset=UTF-8' , 'Authorization' : 'Bearer ' + self . access_token } params = { 'json' : ( '{"selection":{"selectionType":"registered",' '"includeRuntime":"true",' '"includeSensors":"true",' '"includeProgram":"true",' '"includeEquipmentStatus":"true",' '"includeEvents":"true",' '"includeWeather":"true",' '"includeSettings":"true"}}' ) } try : request = requests . get ( url , headers = header , params = params ) except RequestException : logger . warn ( "Error connecting to Ecobee.  Possible connectivity outage." ) return None if request . status_code == requests . codes . ok : self . authenticated = True self . thermostats = request . json ( ) [ 'thermostatList' ] return self . thermostats else : self . authenticated = False logger . info ( "Error connecting to Ecobee while attempting to get " "thermostat data.  Refreshing tokens and trying again." ) if self . refresh_tokens ( ) : return self . get_thermostats ( ) else : return None
def write_tokens_to_file ( self ) : config = dict ( ) config [ 'API_KEY' ] = self . api_key config [ 'ACCESS_TOKEN' ] = self . access_token config [ 'REFRESH_TOKEN' ] = self . refresh_token config [ 'AUTHORIZATION_CODE' ] = self . authorization_code if self . file_based_config : config_from_file ( self . config_filename , config ) else : self . config = config
def set_hvac_mode ( self , index , hvac_mode ) : body = { "selection" : { "selectionType" : "thermostats" , "selectionMatch" : self . thermostats [ index ] [ 'identifier' ] } , "thermostat" : { "settings" : { "hvacMode" : hvac_mode } } } log_msg_action = "set HVAC mode" return self . make_request ( body , log_msg_action )
def set_fan_min_on_time ( self , index , fan_min_on_time ) : body = { "selection" : { "selectionType" : "thermostats" , "selectionMatch" : self . thermostats [ index ] [ 'identifier' ] } , "thermostat" : { "settings" : { "fanMinOnTime" : fan_min_on_time } } } log_msg_action = "set fan minimum on time." return self . make_request ( body , log_msg_action )
def set_climate_hold ( self , index , climate , hold_type = "nextTransition" ) : body = { "selection" : { "selectionType" : "thermostats" , "selectionMatch" : self . thermostats [ index ] [ 'identifier' ] } , "functions" : [ { "type" : "setHold" , "params" : { "holdType" : hold_type , "holdClimateRef" : climate } } ] } log_msg_action = "set climate hold" return self . make_request ( body , log_msg_action )
def delete_vacation ( self , index , vacation ) : body = { "selection" : { "selectionType" : "thermostats" , "selectionMatch" : self . thermostats [ index ] [ 'identifier' ] } , "functions" : [ { "type" : "deleteVacation" , "params" : { "name" : vacation } } ] } log_msg_action = "delete a vacation" return self . make_request ( body , log_msg_action )
def resume_program ( self , index , resume_all = False ) : body = { "selection" : { "selectionType" : "thermostats" , "selectionMatch" : self . thermostats [ index ] [ 'identifier' ] } , "functions" : [ { "type" : "resumeProgram" , "params" : { "resumeAll" : resume_all } } ] } log_msg_action = "resume program" return self . make_request ( body , log_msg_action )
def send_message ( self , index , message = "Hello from python-ecobee!" ) : body = { "selection" : { "selectionType" : "thermostats" , "selectionMatch" : self . thermostats [ index ] [ 'identifier' ] } , "functions" : [ { "type" : "sendMessage" , "params" : { "text" : message [ 0 : 500 ] } } ] } log_msg_action = "send message" return self . make_request ( body , log_msg_action )
def dict_self ( self ) : return { k : v for k , v in self . __dict__ . items ( ) if k in FSM_ATTRS }
def reset ( self , iface = None , client_mac = None , xid = None , scriptfile = None ) : logger . debug ( 'Reseting attributes.' ) if iface is None : iface = conf . iface if client_mac is None : tempmac = get_if_raw_hwaddr ( iface ) if isinstance ( tempmac , tuple ) and len ( tempmac ) == 2 : mac = tempmac [ 1 ] else : mac = tempmac client_mac = str2mac ( mac ) self . client = DHCPCAP ( iface = iface , client_mac = client_mac , xid = xid ) if scriptfile is not None : self . script = ClientScript ( scriptfile ) else : self . script = None self . time_sent_request = None self . discover_attempts = 0 self . request_attempts = 0 self . current_state = STATE_PREINIT self . offers = list ( )
def get_timeout ( self , state , function ) : state = STATES2NAMES [ state ] for timeout_fn_t in self . timeout [ state ] : if timeout_fn_t [ 1 ] is not None and timeout_fn_t [ 1 ] . atmt_condname == function . atmt_condname : logger . debug ( 'Timeout for state %s, function %s, is %s' , state , function . atmt_condname , timeout_fn_t [ 0 ] ) return timeout_fn_t [ 0 ] return None
def set_timers ( self ) : logger . debug ( 'setting timeouts' ) self . set_timeout ( self . current_state , self . renewing_time_expires , self . client . lease . renewal_time ) self . set_timeout ( self . current_state , self . rebinding_time_expires , self . client . lease . rebinding_time )
def process_received_nak ( self , pkt ) : if isnak ( pkt ) : logger . info ( 'DHCPNAK of %s from %s' , self . client . client_ip , self . client . server_ip ) return True return False
def receive_offer ( self , pkt ) : logger . debug ( "C2. Received OFFER?, in SELECTING state." ) if isoffer ( pkt ) : logger . debug ( "C2: T, OFFER received" ) self . offers . append ( pkt ) if len ( self . offers ) >= MAX_OFFERS_COLLECTED : logger . debug ( "C2.5: T, raise REQUESTING." ) self . select_offer ( ) raise self . REQUESTING ( ) logger . debug ( "C2.5: F, raise SELECTING." ) raise self . SELECTING ( )
def receive_ack_requesting ( self , pkt ) : logger . debug ( "C3. Received ACK?, in REQUESTING state." ) if self . process_received_ack ( pkt ) : logger . debug ( "C3: T. Received ACK, in REQUESTING state, " "raise BOUND." ) raise self . BOUND ( )
def receive_nak_requesting ( self , pkt ) : logger . debug ( "C3.1. Received NAK?, in REQUESTING state." ) if self . process_received_nak ( pkt ) : logger . debug ( "C3.1: T. Received NAK, in REQUESTING state, " "raise INIT." ) raise self . INIT ( )
def receive_ack_renewing ( self , pkt ) : logger . debug ( "C3. Received ACK?, in RENEWING state." ) if self . process_received_ack ( pkt ) : logger . debug ( "C3: T. Received ACK, in RENEWING state, " "raise BOUND." ) raise self . BOUND ( )
def receive_nak_renewing ( self , pkt ) : logger . debug ( "C3.1. Received NAK?, in RENEWING state." ) if self . process_received_nak ( pkt ) : logger . debug ( "C3.1: T. Received NAK, in RENEWING state, " " raise INIT." ) raise self . INIT ( )
def receive_ack_rebinding ( self , pkt ) : logger . debug ( "C3. Received ACK?, in REBINDING state." ) if self . process_received_ack ( pkt ) : logger . debug ( "C3: T. Received ACK, in REBINDING state, " "raise BOUND." ) raise self . BOUND ( )
def receive_nak_rebinding ( self , pkt ) : logger . debug ( "C3.1. Received NAK?, in RENEWING state." ) if self . process_received_nak ( pkt ) : logger . debug ( "C3.1: T. Received NAK, in RENEWING state, " "raise INIT." ) raise self . INIT ( )
def set ( self , name , value ) : clone = self . _clone ( ) if django . VERSION [ 0 ] <= 1 and django . VERSION [ 1 ] <= 4 : value = value or None clone . _qsl = [ ( q , v ) for ( q , v ) in self . _qsl if q != name ] if value is not None : clone . _qsl . append ( ( name , value ) ) return clone
def add ( self , name , value ) : clone = self . _clone ( ) clone . _qsl = [ p for p in self . _qsl if not ( p [ 0 ] == name and p [ 1 ] == value ) ] clone . _qsl . append ( ( name , value , ) ) return clone
def remove ( self , name , value ) : clone = self . _clone ( ) clone . _qsl = [ qb for qb in self . _qsl if qb != ( name , str ( value ) ) ] return clone
def read_tdms ( tdms_file ) : tdms_file = nptdms . TdmsFile ( tdms_file ) ch_names = [ ] ch_data = [ ] for o in tdms_file . objects . values ( ) : if o . data is not None and len ( o . data ) : chn = o . path . split ( '/' ) [ - 1 ] . strip ( "'" ) if "unit_string" in o . properties : unit = o . properties [ "unit_string" ] ch_names . append ( "{} [{}]" . format ( chn , unit ) ) else : ch_names . append ( chn ) ch_data . append ( o . data ) return ch_names , ch_data
def tdms2fcs ( tdms_file ) : fcs_file = tdms_file [ : - 4 ] + "fcs" chn_names , data = read_tdms ( tdms_file ) chn_names , data = add_deformation ( chn_names , data ) fcswrite . write_fcs ( filename = fcs_file , chn_names = chn_names , data = np . array ( data ) . transpose ( ) )
def equal ( self , cwd ) : cmd = [ "diff" ] cmd . append ( "-q" ) cmd . append ( self . left . get_name ( ) ) cmd . append ( self . right . get_name ( ) ) try : Process ( cmd ) . run ( cwd = cwd , suppress_output = True ) except SubprocessError as e : if e . get_returncode ( ) == 1 : return False else : raise e return True
def _backup_file ( self , file , patch ) : dest_dir = self . quilt_pc + patch . get_name ( ) file_dir = file . get_directory ( ) if file_dir : #TODO get relative path dest_dir = dest_dir + file_dir backup = Backup ( ) backup . backup_file ( file , dest_dir , copy_empty = True )
def link ( self , link ) : if isinstance ( link , File ) : link = link . filename os . link ( self . filename , link )
def copy ( self , dest ) : if isinstance ( dest , File ) : dest_dir = dest . get_directory ( ) dest_dir . create ( ) dest = dest . filename elif isinstance ( dest , Directory ) : dest = dest . dirname shutil . copy2 ( self . filename , dest )
def apply_patch ( self , patch_name , force = False , quiet = False ) : self . _check ( ) patch = Patch ( patch_name ) patches = self . series . patches_until ( patch ) [ : ] applied = self . db . applied_patches ( ) for patch in applied : if patch in patches : patches . remove ( patch ) if not patches : raise AllPatchesApplied ( self . series , self . db . top_patch ( ) ) self . applying ( patch ) try : for cur_patch in patches : self . _apply_patch ( cur_patch , force , quiet ) finally : self . db . save ( ) self . applied ( self . db . top_patch ( ) )
def apply_next_patch ( self , force = False , quiet = False ) : self . _check ( ) top = self . db . top_patch ( ) if not top : patch = self . series . first_patch ( ) else : patch = self . series . patch_after ( top ) if not patch : raise AllPatchesApplied ( self . series , top ) self . applying ( patch ) self . _apply_patch ( patch , force , quiet ) self . db . save ( ) self . applied ( self . db . top_patch ( ) )
def apply_all ( self , force = False , quiet = False ) : self . _check ( ) top = self . db . top_patch ( ) if top : patches = self . series . patches_after ( top ) else : patches = self . series . patches ( ) if not patches : raise AllPatchesApplied ( self . series , top ) try : for patch in patches : self . applying ( patch ) self . _apply_patch ( patch , force , quiet ) finally : self . db . save ( ) self . applied ( self . db . top_patch ( ) )
def read ( self ) : self . patchlines = [ ] self . patch2line = dict ( ) if self . exists ( ) : with open ( self . series_file , "r" ) as f : for line in f : self . add_patch ( line )
def save ( self ) : with open ( self . series_file , "wb" ) as f : for patchline in self . patchlines : f . write ( _encode_str ( str ( patchline ) ) ) f . write ( b"\n" )
def add_patch ( self , patch ) : patchline = PatchLine ( patch ) patch = patchline . get_patch ( ) if patch : self . patch2line [ patch ] = patchline self . patchlines . append ( patchline )
def insert_patches ( self , patches ) : patchlines = [ ] for patch_name in patches : patchline = PatchLine ( patch_name ) patch = patchline . get_patch ( ) if patch : self . patch2line [ patch ] = patchline patchlines . append ( patchline ) patchlines . extend ( self . patchlines ) self . patchlines = patchlines
def add_patches ( self , patches , after = None ) : if after is None : self . insert_patches ( patches ) else : self . _check_patch ( after ) patchlines = self . _patchlines_before ( after ) patchlines . append ( self . patch2line [ after ] ) for patch in patches : patchline = PatchLine ( patch ) patchlines . append ( patchline ) self . patch2line [ patchline . get_patch ( ) ] = patchline patchlines . extend ( self . _patchlines_after ( after ) ) self . patchlines = patchlines
def remove_patch ( self , patch ) : self . _check_patch ( patch ) patchline = self . patch2line [ patch ] del self . patch2line [ patch ] self . patchlines . remove ( patchline )
def patches_after ( self , patch ) : return [ line . get_patch ( ) for line in self . _patchlines_after ( patch ) if line . get_patch ( ) ]
def patches_before ( self , patch ) : return [ line . get_patch ( ) for line in self . _patchlines_before ( patch ) if line . get_patch ( ) ]
def create ( self ) : if not os . path . exists ( self . dirname ) : os . makedirs ( self . dirname ) self . _create_version ( self . version_file )
def import_patches ( self , patches ) : dest_dir = self . quilt_patches patch_names = [ ] for patch in patches : patch_name = os . path . basename ( patch ) patch_file = File ( patch ) dest_file = dest_dir + File ( patch_name ) patch_file . copy ( dest_file ) patch_names . append ( patch_name ) self . _import_patches ( patch_names )
def way ( self , w ) : if w . id not in self . way_ids : return way_points = [ ] for n in w . nodes : try : way_points . append ( Point ( n . location . lon , n . location . lat ) ) except o . InvalidLocationError : logging . debug ( 'InvalidLocationError at way %s node %s' , w . id , n . ref ) self . ways [ w . id ] = Way ( w . id , way_points )
def missing_node_ids ( self ) : present_node_ids = self . nodes . keys ( ) for nid in self . node_ids : if nid not in present_node_ids : yield nid
def node ( self , n ) : if n . id not in self . node_ids : return try : self . nodes [ n . id ] = Node ( n . id , n . location . lon , n . location . lat , { t . k : t . v for t in n . tags } ) except o . InvalidLocationError : logging . debug ( 'InvalidLocationError at node %s' , n . id )
def build_route ( relation ) : if relation . tags . get ( 'type' ) != 'route' : return short_name = create_route_short_name ( relation ) color = relation . tags . get ( 'color' ) return Route ( relation . id , short_name , create_route_long_name ( relation , short_name ) , map_osm_route_type_to_gtfs ( relation . tags . get ( 'route' ) ) , 'https://www.openstreetmap.org/relation/{}' . format ( relation . id ) , color . strip ( '#' ) if color else '' , get_agency_id ( relation ) )
def create_route_long_name ( relation , short_name ) : if relation . tags . get ( 'from' ) and relation . tags . get ( 'to' ) : return "{0}-to-{1}" . format ( relation . tags . get ( 'from' ) , relation . tags . get ( 'to' ) ) name = relation . tags . get ( 'name' ) or relation . tags . get ( 'alt_name' ) or "OSM Route No. {}" . format ( relation . id ) if short_name and name . startswith ( short_name ) : return name [ len ( short_name ) : ] return name
def get_agency_id ( relation ) : op = relation . tags . get ( 'operator' ) if op : return int ( hashlib . sha256 ( op . encode ( 'utf-8' ) ) . hexdigest ( ) , 16 ) % 10 ** 8 return - 1
def process ( self ) : self . rh = RelationHandler ( ) self . rh . apply_file ( self . filename ) logging . debug ( 'Found %d public transport relations.' , len ( self . rh . relations ) ) node_ids , stop_node_ids , way_ids , reverse_map = self . __collect_ids ( ) self . nh = NodeHandler ( node_ids ) self . nh . apply_file ( self . filename , locations = True ) count = 0 for idx , missing_node_id in enumerate ( self . nh . missing_node_ids ) : count += 1 logging . warning ( '[no data] missing stop node. rel: https://osm.org/relation/%s node: https://osm.org/node/%s.' , reverse_map [ missing_node_id ] , missing_node_id ) if count : logging . warning ( '%d nodes that appear in relations are missing.' , count ) else : logging . debug ( 'Lucky you! All relation member nodes were found.' ) self . wh = WayHandler ( way_ids ) self . wh . apply_file ( self . filename , locations = True )
def relation ( self , rel ) : rel_type = rel . tags . get ( 'type' ) if any ( [ rel . deleted , not rel . visible , not self . is_new_version ( rel ) , rel_type not in [ 'route' , 'public_transport' ] ] ) : return route_tag = rel . tags . get ( 'route' ) if rel_type == 'route' and route_tag not in self . transit_route_types : return public_transport = rel . tags . get ( 'public_transport' ) if rel_type == 'public_transport' and public_transport != 'stop_area' : return self . relations [ rel . id ] = Relation ( rel . id , { 'type' : rel_type , 'public_transport' : public_transport , 'route' : route_tag , 'operator' : rel . tags . get ( 'operator' ) , 'color' : rel . tags . get ( 'color' ) , 'ref' : rel . tags . get ( 'ref' ) , 'from' : rel . tags . get ( 'from' ) , 'to' : rel . tags . get ( 'to' ) , 'name' : rel . tags . get ( 'name' ) , 'alt_name' : rel . tags . get ( 'alt_name' ) , 'url' : rel . tags . get ( 'url' ) , 'contact_website' : rel . tags . get ( 'contact:website' ) } , [ ( member . type , member . ref , member . role ) for member in rel . members ] ) self . versions [ rel . id ] = rel . version
def patch_agencies ( agencies ) : yield Agency ( - 1 , 'http://hiposfer.com' , 'Unknown agency' , 'Europe/Berlin' ) for agency_id , agency_url , agency_name , agency_timezone in agencies : if not agency_url : agency_url = 'http://hiposfer.com' if not agency_timezone : agency_timezone = 'Europe/Berlin' yield Agency ( agency_id , agency_url , agency_name , agency_timezone )
def _create_dummy_trip_stoptimes ( trip_id , stops , first_service_time ) : waiting = datetime . timedelta ( seconds = 30 ) arrival = first_service_time last_departure = first_service_time last_departure_hour = ( arrival + waiting ) . hour last_stop = None departure_hour = None arrival_hour = None for stop_sequence , stop in enumerate ( stops ) : arrival = last_departure + get_time_from_last_stop ( last_stop , stop ) departure = arrival + waiting if arrival . hour < last_departure_hour : diff = last_departure_hour arrival_hour = arrival . hour + diff departure_hour = departure . hour + diff last_departure_hour = departure . hour + diff else : arrival_hour = arrival . hour departure_hour = departure . hour last_departure_hour = departure . hour if departure . hour < arrival . hour : diff = last_departure_hour departure_hour = departure . hour + diff last_departure_hour = departure . hour + diff yield { 'trip_id' : trip_id , 'arrival_time' : '{:02}:{}' . format ( arrival_hour , arrival . strftime ( '%M:%S' ) ) , 'departure_time' : '{:02}:{}' . format ( departure_hour , departure . strftime ( '%M:%S' ) ) , 'stop_id' : stop . stop_id , 'stop_sequence' : stop_sequence } last_stop = stop last_departure = departure
def write_zipped ( self , filepath ) : with zipfile . ZipFile ( filepath , mode = 'w' , compression = zipfile . ZIP_DEFLATED ) as zfile : for name , buffer in self . _buffers . items ( ) : encoded_values = io . BytesIO ( buffer . getvalue ( ) . encode ( 'utf-8' ) ) zfile . writestr ( '{}.txt' . format ( name ) , encoded_values . getbuffer ( ) ) for name , path in self . _files . items ( ) : zfile . write ( path , arcname = name )
def write_unzipped ( self , destination ) : for name , buffer in self . _buffers . items ( ) : with open ( os . path . join ( destination , '{}.txt' . format ( name ) ) , 'w' , encoding = 'utf-8' ) as file : file . write ( buffer . getvalue ( ) ) for name , path in self . _files . items ( ) : shutil . copy ( path , os . path . join ( destination , name ) )
def build_agency ( relation , nodes ) : # op = relation . tags . get ( 'operator' ) agency_url = relation . tags . get ( 'url' ) or relation . tags . get ( 'contact_website' ) if not op : return agency_id = int ( hashlib . sha256 ( op . encode ( 'utf8' ) ) . hexdigest ( ) , 16 ) % 10 ** 8 return Agency ( agency_id , agency_url , op , '' )
def extract_stops ( relation , nodes , visited_stop_ids , stop_to_station_map ) : for member_type , member_id , member_role in relation . member_info : if member_id not in visited_stop_ids and member_id in nodes and member_role in ( 'stop' , 'halt' ) : location_type = '' visited_stop_ids . add ( member_id ) yield Stop ( member_id , nodes [ member_id ] . tags . get ( 'name' ) or "Unnamed {} stop." . format ( relation . tags . get ( 'route' ) ) , nodes [ member_id ] . lon if member_id in nodes else '' , nodes [ member_id ] . lat if member_id in nodes else '' , relation . id , _map_wheelchair ( nodes [ member_id ] . tags . get ( 'wheelchair' ) ) , location_type , stop_to_station_map . get ( member_id , '' ) )
def build_shape ( relation , nodes , ways ) : sequence_index = 0 for member_type , member_id , member_role in relation . member_info : if member_id in nodes : yield Shape ( relation . id , nodes [ member_id ] . lat , nodes [ member_id ] . lon , sequence_index ) sequence_index += 1 elif member_id in ways : continue else : pass
def get_supported_versions ( self ) : if not hasattr ( self , '_versions' ) : try : self . _versions = [ self . send_apdu ( INS_GET_VERSION ) . decode ( ) ] except exc . APDUError as e : self . _versions = [ 'v0' ] if e . code == 0x6d00 else [ ] return self . _versions
def send_apdu ( self , ins , p1 = 0 , p2 = 0 , data = b'' ) : if data is None : data = b'' elif isinstance ( data , int ) : data = int2byte ( data ) size = len ( data ) l0 = size >> 16 & 0xff l1 = size >> 8 & 0xff l2 = size & 0xff apdu_data = struct . pack ( 'B B B B B B B %is B B' % size , 0 , ins , p1 , p2 , l0 , l1 , l2 , data , 0x00 , 0x00 ) try : resp = self . _do_send_apdu ( apdu_data ) except Exception as e : raise exc . DeviceError ( e ) status = struct . unpack ( '>H' , resp [ - 2 : ] ) [ 0 ] data = resp [ : - 2 ] if status != APDU_OK : raise exc . APDUError ( status ) return data
def register ( devices , params , facet ) : for device in devices [ : ] : try : device . open ( ) except : devices . remove ( device ) sys . stderr . write ( '\nTouch the U2F device you wish to register...\n' ) try : while devices : removed = [ ] for device in devices : try : return u2f . register ( device , params , facet ) except exc . APDUError as e : if e . code == APDU_USE_NOT_SATISFIED : pass else : removed . append ( device ) except exc . DeviceError : removed . append ( device ) devices = [ d for d in devices if d not in removed ] for d in removed : d . close ( ) time . sleep ( 0.25 ) finally : for device in devices : device . close ( ) sys . stderr . write ( '\nUnable to register with any U2F device.\n' ) sys . exit ( 1 )
def u2str ( data ) : if isinstance ( data , dict ) : return { u2str ( k ) : u2str ( v ) for k , v in data . items ( ) } elif isinstance ( data , list ) : return [ u2str ( x ) for x in data ] elif isinstance ( data , text_type ) : return data . encode ( 'utf-8' ) else : return data
def wrap_function ( func = None , error_threshold = None , reraise_exception = True , save_current_stack_trace = True ) : # if func : return flawless . client . client . _wrap_function_with_error_decorator ( func = func , error_threshold = error_threshold , reraise_exception = reraise_exception , save_current_stack_trace = save_current_stack_trace ) else : return functools . partial ( flawless . client . client . _wrap_function_with_error_decorator , error_threshold = error_threshold , reraise_exception = reraise_exception , save_current_stack_trace = save_current_stack_trace )
def _get_entry ( self , entry , entry_tree ) : for e in entry_tree [ entry . filename ] : if entry == e : return e
def markdown_to_reST ( text ) : text = re . sub ( pattern = r"\n       (\w+) - (.+)\n" , repl = r"\n\n       *\g<1>* - \g<2>\n" , string = text ) text = re . sub ( pattern = r"\[([^\]]+)\]\([^)]+\)" , repl = r"\g<1>" , string = text ) text = re . sub ( pattern = r"\n(\d+). " , repl = r"\n\\\g<1>. " , string = text ) return text
def record_error ( hostname , exc_info , preceding_stack = None , error_threshold = None , additional_info = None ) : stack = [ ] exc_type , exc_value , sys_traceback = exc_info while sys_traceback is not None : stack . append ( sys_traceback ) sys_traceback = sys_traceback . tb_next stack_lines = [ ] for row in preceding_stack or [ ] : stack_lines . append ( api_ttypes . StackLine ( filename = os . path . abspath ( row [ 0 ] ) , line_number = row [ 1 ] , function_name = row [ 2 ] , text = row [ 3 ] ) ) for index , tb in enumerate ( stack ) : filename = tb . tb_frame . f_code . co_filename func_name = tb . tb_frame . f_code . co_name lineno = tb . tb_lineno line = linecache . getline ( filename , lineno , tb . tb_frame . f_globals ) frame_locals = None if index >= ( len ( stack ) - NUM_FRAMES_TO_SAVE ) : frame_locals = dict ( ( k , _myrepr ( k , v ) ) for k , v in list ( tb . tb_frame . f_locals . items ( ) ) [ : MAX_LOCALS ] if k != "self" ) if "self" in tb . tb_frame . f_locals and hasattr ( tb . tb_frame . f_locals [ "self" ] , "__dict__" ) : frame_locals . update ( dict ( ( "self." + k , _myrepr ( k , v ) ) for k , v in list ( tb . tb_frame . f_locals [ "self" ] . __dict__ . items ( ) ) [ : MAX_LOCALS ] if k != "self" ) ) stack_lines . append ( api_ttypes . StackLine ( filename = os . path . abspath ( filename ) , line_number = lineno , function_name = func_name , text = line , frame_locals = frame_locals ) ) key = CachedErrorInfo . get_hash_key ( stack_lines ) info = ERROR_CACHE . get ( key ) or CachedErrorInfo ( ) info . increment ( ) ERROR_CACHE [ key ] = info if info . should_report ( ) : error_count = info . mark_reported ( ) _send_request ( api_ttypes . RecordErrorRequest ( traceback = stack_lines , exception_message = repr ( exc_value ) , exception_type = exc_type . __module__ + "." + exc_type . __name__ , hostname = hostname , error_threshold = error_threshold , additional_info = additional_info , error_count = error_count , ) )
def url_to_image ( url ) : r = requests . get ( url ) image = StringIO ( r . content ) return image
def string_to_image ( image_string ) : image_filelike = StringIO ( image_string ) image = Image . open ( image_filelike ) return image
def _is_big_enough ( image , size ) : if ( size [ 0 ] > image . size [ 0 ] ) and ( size [ 1 ] > image . size [ 1 ] ) : raise ImageSizeError ( image . size , size )
def _width_is_big_enough ( image , width ) : if width > image . size [ 0 ] : raise ImageSizeError ( image . size [ 0 ] , width )
def _height_is_big_enough ( image , height ) : if height > image . size [ 1 ] : raise ImageSizeError ( image . size [ 1 ] , height )
def parse_category ( self , item , field_name , source_name ) : slug = category_map . get ( self . get_value ( item , source_name ) , None ) if not slug : return None try : return Category . objects . get ( slug = slug ) except Category . DoesNotExist : pass
def parse_totals ( self , item , field_name , source_name ) : val = self . get_value ( item , source_name ) try : return int ( val ) except : return 0
def get_items ( self ) : for event , item in ElementTree . iterparse ( self . source ) : if item . tag == self . item_tag_name : yield item item . clear ( )
def save_error ( self , data , exception_info ) : self . errors . append ( { 'data' : data , 'exception' : '' . join ( format_exception ( * exception_info ) ) , } )
def parse ( self ) : if not self . loaded : self . load ( self . source ) for item in self . get_items ( ) : data = self . parse_item ( item ) instance = self . get_instance ( data ) self . feed_instance ( data , instance ) try : self . save_item ( item , data , instance ) except Exception as e : self . save_error ( data , sys . exc_info ( ) ) self . unload ( )
def parse_item ( self , item ) : parsed_data = { } for field_name in self . fields : source_name = self . field_map . get ( field_name , field_name ) parse = getattr ( self , 'parse_%s' % field_name , None ) if parse : value = parse ( item , field_name , source_name ) else : value = self . get_value ( item , source_name ) parsed_data [ field_name ] = value return parsed_data
def get_instance ( self , data ) : unique_fields = self . unique_fields if not unique_fields : return self . model ( ) filter = dict ( [ ( f , data [ f ] ) for f in unique_fields ] ) try : instance = self . model . _default_manager . get ( * * filter ) except self . model . DoesNotExist : return self . model ( ) return instance
def save_item ( self , item , data , instance , commit = True ) : if commit : instance . save ( ) return instance
def load ( self , source ) : self . source = open ( self . source , 'rb' ) self . loaded = True
def get_items ( self ) : reader = csv . reader ( self . source ) headers = reader . next ( ) for row in reader : if not row : continue yield dict ( zip ( headers , row ) )
def allow_network_access ( self , value : bool ) : if self . _is_running : raise ValueError ( "Cannot change network access settings on a running sandbox" ) self . _allow_network_access = value
def get_enrollments_for_course_by_sis_id ( self , sis_course_id , params = { } ) : return self . get_enrollments_for_course ( self . _sis_id ( sis_course_id , sis_field = "course" ) , params )
def get_enrollments_for_section_by_sis_id ( self , sis_section_id , params = { } ) : return self . get_enrollments_for_section ( self . _sis_id ( sis_section_id , sis_field = "section" ) , params )
def get_roles_by_account_sis_id ( self , account_sis_id , params = { } ) : return self . get_roles_in_account ( self . _sis_id ( account_sis_id , sis_field = "account" ) , params )
def get_role_by_account_sis_id ( self , account_sis_id , role_id ) : return self . get_role ( self . _sis_id ( account_sis_id , sis_field = "account" ) , role_id )
def get_course_by_sis_id ( self , sis_course_id , params = { } ) : return self . get_course ( self . _sis_id ( sis_course_id , sis_field = "course" ) , params )
def get_courses_in_account_by_sis_id ( self , sis_account_id , params = { } ) : return self . get_courses_in_account ( self . _sis_id ( sis_account_id , sis_field = "account" ) , params )
def get_published_courses_in_account ( self , account_id , params = { } ) : params [ "published" ] = True return self . get_courses_in_account ( account_id , params )
def get_published_courses_in_account_by_sis_id ( self , sis_account_id , params = { } ) : return self . get_published_courses_in_account ( self . _sis_id ( sis_account_id , sis_field = "account" ) , params )
def get_users_for_course ( self , course_id , params = { } ) : url = COURSES_API . format ( course_id ) + "/users" data = self . _get_paged_resource ( url , params = params ) users = [ ] for datum in data : users . append ( CanvasUser ( data = datum ) ) return users
def get_users_for_sis_course_id ( self , sis_course_id , params = { } ) : return self . get_users_for_course ( self . _sis_id ( sis_course_id , sis_field = "course" ) , params )
def _next_page ( self , response ) : for link in response . getheader ( "link" , "" ) . split ( "," ) : try : ( url , rel ) = link . split ( ";" ) if "next" in rel : return url . lstrip ( "<" ) . rstrip ( ">" ) except Exception : return
def _get_resource ( self , url , params = None , data_key = None ) : if not params : params = { } self . _set_as_user ( params ) full_url = url + self . _params ( params ) return self . _get_resource_url ( full_url , True , data_key )
def _put_resource ( self , url , body ) : params = { } self . _set_as_user ( params ) headers = { 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } url = url + self . _params ( params ) response = DAO . putURL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 201 or response . status == 204 ) : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
def _post_resource ( self , url , body ) : params = { } self . _set_as_user ( params ) headers = { 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } url = url + self . _params ( params ) response = DAO . postURL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 204 ) : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
def _delete_resource ( self , url ) : params = { } self . _set_as_user ( params ) headers = { 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } url = url + self . _params ( params ) response = DAO . deleteURL ( url , headers ) if not ( response . status == 200 or response . status == 204 ) : raise DataFailureException ( url , response . status , response . data ) return response
def create_admin_by_sis_id ( self , sis_account_id , user_id , role ) : return self . create_admin ( self . _sis_id ( sis_account_id ) , user_id , role )
def delete_admin_by_sis_id ( self , sis_account_id , user_id , role ) : return self . delete_admin ( self . _sis_id ( sis_account_id ) , user_id , role )
def get_section_by_sis_id ( self , sis_section_id , params = { } ) : return self . get_section ( self . _sis_id ( sis_section_id , sis_field = "section" ) , params )
def get_sections_in_course_by_sis_id ( self , sis_course_id , params = { } ) : return self . get_sections_in_course ( self . _sis_id ( sis_course_id , sis_field = "course" ) , params )
def get_sections_with_students_in_course ( self , course_id , params = { } ) : include = params . get ( "include" , [ ] ) if "students" not in include : include . append ( "students" ) params [ "include" ] = include return self . get_sections_in_course ( course_id , params )
def get_sections_with_students_in_course_by_sis_id ( self , sis_course_id , params = { } ) : return self . get_sections_with_students_in_course ( self . _sis_id ( sis_course_id , sis_field = "course" ) , params )
def get_term_by_sis_id ( self , sis_term_id ) : for term in self . get_all_terms ( ) : if term . sis_term_id == sis_term_id : return term
def _build_archive ( self , dir_path ) : zip_path = os . path . join ( dir_path , "import.zip" ) archive = zipfile . ZipFile ( zip_path , "w" ) for filename in CSV_FILES : filepath = os . path . join ( dir_path , filename ) if os . path . exists ( filepath ) : archive . write ( filepath , filename , zipfile . ZIP_DEFLATED ) archive . close ( ) with open ( zip_path , "rb" ) as f : body = f . read ( ) return body
def get_report_data ( self , report ) : if report . report_id is None or report . status is None : raise ReportFailureException ( report ) interval = getattr ( settings , 'CANVAS_REPORT_POLLING_INTERVAL' , 5 ) while report . status != "complete" : if report . status == "error" : raise ReportFailureException ( report ) sleep ( interval ) report = self . get_report_status ( report ) if report . attachment is None or report . attachment . url is None : return data = self . _get_report_file ( report . attachment . url ) return data . split ( "\n" )
def empty_value ( self ) : edit_empty_value = self . config . get ( 'edit_empty_value' , False ) if edit_empty_value : return edit_empty_value else : return unicode ( inplace_settings . INPLACEEDIT_EDIT_EMPTY_VALUE )
def create_metrics ( self , metric_configs : Iterable [ MetricConfig ] ) -> Dict [ str , Metric ] : return self . registry . create_metrics ( metric_configs )
def _configure_registry ( self , include_process_stats : bool = False ) : if include_process_stats : self . registry . register_additional_collector ( ProcessCollector ( registry = None ) )
def create_metrics ( self , configs : Iterable [ MetricConfig ] ) -> Dict [ str , Metric ] : metrics : Dict [ str , Metric ] = { config . name : self . _register_metric ( config ) for config in configs } self . _metrics . update ( metrics ) return metrics
def get_metric ( self , name : str , labels : Union [ Dict [ str , str ] , None ] = None ) -> Metric : metric = self . _metrics [ name ] if labels : return metric . labels ( * * labels ) return metric
async def _handle_home ( self , request : Request ) -> Response : if self . description : title = f'{self.name} - {self.description}' else : title = self . name text = dedent ( ) return Response ( content_type = 'text/html' , text = text )
async def _handle_metrics ( self , request : Request ) -> Response : if self . _update_handler : await self . _update_handler ( self . registry . get_metrics ( ) ) response = Response ( body = self . registry . generate_metrics ( ) ) response . content_type = CONTENT_TYPE_LATEST return response
def info ( self ) : return itertools . chain ( self . pods , self . assumptions , self . warnings )
def results ( self ) : return ( pod for pod in self . pods if pod . primary or pod . title == 'Result' )
def vector ( members : Iterable [ T ] , meta : Optional [ IPersistentMap ] = None ) -> Vector [ T ] : return Vector ( pvector ( members ) , meta = meta )
def v ( * members : T , meta : Optional [ IPersistentMap ] = None ) -> Vector [ T ] : return Vector ( pvector ( members ) , meta = meta )
def eval_file ( filename : str , ctx : compiler . CompilerContext , module : types . ModuleType ) : last = None for form in reader . read_file ( filename , resolver = runtime . resolve_alias ) : last = compiler . compile_and_exec_form ( form , ctx , module ) return last
def eval_stream ( stream , ctx : compiler . CompilerContext , module : types . ModuleType ) : last = None for form in reader . read ( stream , resolver = runtime . resolve_alias ) : last = compiler . compile_and_exec_form ( form , ctx , module ) return last
def eval_str ( s : str , ctx : compiler . CompilerContext , module : types . ModuleType , eof : Any ) : last = eof for form in reader . read_str ( s , resolver = runtime . resolve_alias , eof = eof ) : last = compiler . compile_and_exec_form ( form , ctx , module ) return last
def run ( file_or_code , code , in_ns , use_var_indirection , warn_on_shadowed_name , warn_on_shadowed_var , warn_on_var_indirection , ) : basilisp . init ( ) ctx = compiler . CompilerContext ( filename = CLI_INPUT_FILE_PATH if code else ( STDIN_INPUT_FILE_PATH if file_or_code == STDIN_FILE_NAME else file_or_code ) , opts = { compiler . WARN_ON_SHADOWED_NAME : warn_on_shadowed_name , compiler . WARN_ON_SHADOWED_VAR : warn_on_shadowed_var , compiler . USE_VAR_INDIRECTION : use_var_indirection , compiler . WARN_ON_VAR_INDIRECTION : warn_on_var_indirection , } , ) eof = object ( ) with runtime . ns_bindings ( in_ns ) as ns : if code : print ( runtime . lrepr ( eval_str ( file_or_code , ctx , ns . module , eof ) ) ) elif file_or_code == STDIN_FILE_NAME : print ( runtime . lrepr ( eval_stream ( click . get_text_stream ( "stdin" ) , ctx , ns . module ) ) ) else : print ( runtime . lrepr ( eval_file ( file_or_code , ctx , ns . module ) ) )
def multifn ( dispatch : DispatchFunction , default = None ) -> MultiFunction [ T ] : name = sym . symbol ( dispatch . __qualname__ , ns = dispatch . __module__ ) return MultiFunction ( name , dispatch , default )
def __add_method ( m : lmap . Map , key : T , method : Method ) -> lmap . Map : return m . assoc ( key , method )
def __remove_method ( m : lmap . Map , key : T ) -> lmap . Map : return m . dissoc ( key )
def remove_method ( self , key : T ) -> Optional [ Method ] : method = self . methods . entry ( key , None ) if method : self . _methods . swap ( MultiFunction . __remove_method , key ) return method
def _is_macro ( v : Var ) -> bool : return ( Maybe ( v . meta ) . map ( lambda m : m . entry ( SYM_MACRO_META_KEY , None ) ) . or_else_get ( False ) )
def _clean_meta ( meta : Optional [ lmap . Map ] ) -> Optional [ lmap . Map ] : if meta is None : return None else : new_meta = meta . dissoc ( reader . READER_LINE_KW , reader . READER_COL_KW ) return None if len ( new_meta ) == 0 else new_meta
def __deftype_impls ( ctx : ParserContext , form : ISeq ) -> Tuple [ List [ DefTypeBase ] , List [ Method ] ] : current_interface_sym : Optional [ sym . Symbol ] = None current_interface : Optional [ DefTypeBase ] = None interfaces = [ ] methods : List [ Method ] = [ ] interface_methods : MutableMapping [ sym . Symbol , List [ Method ] ] = { } for elem in form : if isinstance ( elem , sym . Symbol ) : if current_interface is not None : if current_interface_sym in interface_methods : raise ParserException ( f"deftype* forms may only implement an interface once" , form = elem , ) assert ( current_interface_sym is not None ) , "Symbol must be defined with interface" interface_methods [ current_interface_sym ] = methods current_interface_sym = elem current_interface = _parse_ast ( ctx , elem ) methods = [ ] if not isinstance ( current_interface , ( MaybeClass , MaybeHostForm , VarRef ) ) : raise ParserException ( f"deftype* interface implementation must be an existing interface" , form = elem , ) interfaces . append ( current_interface ) elif isinstance ( elem , ISeq ) : if current_interface is None : raise ParserException ( f"deftype* method cannot be declared without interface" , form = elem ) methods . append ( __deftype_method ( ctx , elem , current_interface ) ) else : raise ParserException ( f"deftype* must consist of interface or protocol names and methods" , form = elem , ) if current_interface is not None : if len ( methods ) > 0 : if current_interface_sym in interface_methods : raise ParserException ( f"deftype* forms may only implement an interface once" , form = current_interface_sym , ) assert ( current_interface_sym is not None ) , "Symbol must be defined with interface" interface_methods [ current_interface_sym ] = methods else : raise ParserException ( f"deftype* may not declare interface without at least one method" , form = current_interface_sym , ) return interfaces , list ( chain . from_iterable ( interface_methods . values ( ) ) )
def _resolve_sym ( ctx : ParserContext , form : sym . Symbol ) -> Union [ MaybeClass , MaybeHostForm , VarRef ] : if form . ns is None and form . name . endswith ( "." ) : try : ns , name = form . name [ : - 1 ] . rsplit ( "." , maxsplit = 1 ) form = sym . symbol ( name , ns = ns ) except ValueError : form = sym . symbol ( form . name [ : - 1 ] ) if form . ns is not None : return __resolve_namespaced_symbol ( ctx , form ) else : return __resolve_bare_symbol ( ctx , form )
def _bootstrap_module ( gctx : GeneratorContext , optimizer : PythonASTOptimizer , mod : types . ModuleType , collect_bytecode : Optional [ BytecodeCollector ] = None , ) -> None : _incremental_compile_module ( optimizer , py_module_preamble ( gctx ) , mod , source_filename = gctx . filename , collect_bytecode = collect_bytecode , ) mod . __basilisp_bootstrapped__ = True
def sequence ( s : Iterable ) -> ISeq [ Any ] : try : i = iter ( s ) return _Sequence ( i , next ( i ) ) except StopIteration : return EMPTY
def fraction ( numerator : int , denominator : int ) -> Fraction : return Fraction ( numerator = numerator , denominator = denominator )
def get_handler ( level : str , fmt : str ) -> logging . Handler : handler : logging . Handler = logging . NullHandler ( ) if os . getenv ( "BASILISP_USE_DEV_LOGGER" ) == "true" : handler = logging . StreamHandler ( ) handler . setFormatter ( logging . Formatter ( fmt ) ) handler . setLevel ( level ) return handler
def map ( kvs : Mapping [ K , V ] , meta = None ) -> Map [ K , V ] : return Map ( pmap ( initial = kvs ) , meta = meta )
def partition ( coll , n : int ) : assert n > 0 start = 0 stop = n while stop <= len ( coll ) : yield tuple ( e for e in coll [ start : stop ] ) start += n stop += n if start < len ( coll ) < stop : stop = len ( coll ) yield tuple ( e for e in coll [ start : stop ] )
def _read_namespaced ( ctx : ReaderContext , allowed_suffix : Optional [ str ] = None ) -> Tuple [ Optional [ str ] , str ] : ns : List [ str ] = [ ] name : List [ str ] = [ ] reader = ctx . reader has_ns = False while True : token = reader . peek ( ) if token == "/" : reader . next_token ( ) if has_ns : raise SyntaxError ( "Found '/'; expected word character" ) elif len ( name ) == 0 : name . append ( "/" ) else : if "/" in name : raise SyntaxError ( "Found '/' after '/'" ) has_ns = True ns = name name = [ ] elif ns_name_chars . match ( token ) : reader . next_token ( ) name . append ( token ) elif allowed_suffix is not None and token == allowed_suffix : reader . next_token ( ) name . append ( token ) else : break ns_str = None if not has_ns else "" . join ( ns ) name_str = "" . join ( name ) if ns_str is None : if "/" in name_str and name_str != "/" : raise SyntaxError ( "'/' character disallowed in names" ) assert ns_str is None or len ( ns_str ) > 0 return ns_str , name_str
def _read_list ( ctx : ReaderContext ) -> llist . List : start = ctx . reader . advance ( ) assert start == "(" return _read_coll ( ctx , llist . list , ")" , "list" )
def _read_vector ( ctx : ReaderContext ) -> vector . Vector : start = ctx . reader . advance ( ) assert start == "[" return _read_coll ( ctx , vector . vector , "]" , "vector" )
def _read_set ( ctx : ReaderContext ) -> lset . Set : start = ctx . reader . advance ( ) assert start == "{" def set_if_valid ( s : Collection ) -> lset . Set : if len ( s ) != len ( set ( s ) ) : raise SyntaxError ( "Duplicated values in set" ) return lset . set ( s ) return _read_coll ( ctx , set_if_valid , "}" , "set" )
def _read_map ( ctx : ReaderContext ) -> lmap . Map : reader = ctx . reader start = reader . advance ( ) assert start == "{" d : MutableMapping [ Any , Any ] = { } while True : if reader . peek ( ) == "}" : reader . next_token ( ) break k = _read_next ( ctx ) if k is COMMENT : continue while True : if reader . peek ( ) == "}" : raise SyntaxError ( "Unexpected token '}'; expected map value" ) v = _read_next ( ctx ) if v is COMMENT : continue if k in d : raise SyntaxError ( f"Duplicate key '{k}' in map literal" ) break d [ k ] = v return lmap . map ( d )
def _read_kw ( ctx : ReaderContext ) -> keyword . Keyword : start = ctx . reader . advance ( ) assert start == ":" ns , name = _read_namespaced ( ctx ) if "." in name : raise SyntaxError ( "Found '.' in keyword name" ) return keyword . keyword ( name , ns = ns )
def _read_function ( ctx : ReaderContext ) -> llist . List : if ctx . is_in_anon_fn : raise SyntaxError ( f"Nested #() definitions not allowed" ) with ctx . in_anon_fn ( ) : form = _read_list ( ctx ) arg_set = set ( ) def arg_suffix ( arg_num ) : if arg_num is None : return "1" elif arg_num == "&" : return "rest" else : return arg_num def sym_replacement ( arg_num ) : suffix = arg_suffix ( arg_num ) return symbol . symbol ( f"arg-{suffix}" ) def identify_and_replace ( f ) : if isinstance ( f , symbol . Symbol ) : if f . ns is None : match = fn_macro_args . match ( f . name ) if match is not None : arg_num = match . group ( 2 ) suffix = arg_suffix ( arg_num ) arg_set . add ( suffix ) return sym_replacement ( arg_num ) return f body = walk . postwalk ( identify_and_replace , form ) if len ( form ) > 0 else None arg_list : List [ symbol . Symbol ] = [ ] numbered_args = sorted ( map ( int , filter ( lambda k : k != "rest" , arg_set ) ) ) if len ( numbered_args ) > 0 : max_arg = max ( numbered_args ) arg_list = [ sym_replacement ( str ( i ) ) for i in range ( 1 , max_arg + 1 ) ] if "rest" in arg_set : arg_list . append ( _AMPERSAND ) arg_list . append ( sym_replacement ( "rest" ) ) return llist . l ( _FN , vector . vector ( arg_list ) , body )
def _read_quoted ( ctx : ReaderContext ) -> llist . List : start = ctx . reader . advance ( ) assert start == "'" next_form = _read_next_consuming_comment ( ctx ) return llist . l ( _QUOTE , next_form )
def _read_syntax_quoted ( ctx : ReaderContext ) -> ReaderForm : start = ctx . reader . advance ( ) assert start == "`" with ctx . syntax_quoted ( ) : return _process_syntax_quoted_form ( ctx , _read_next_consuming_comment ( ctx ) )
def _read_deref ( ctx : ReaderContext ) -> LispForm : start = ctx . reader . advance ( ) assert start == "@" next_form = _read_next_consuming_comment ( ctx ) return llist . l ( _DEREF , next_form )
def _read_regex ( ctx : ReaderContext ) -> Pattern : s = _read_str ( ctx , allow_arbitrary_escapes = True ) try : return langutil . regex_from_str ( s ) except re . error : raise SyntaxError ( f"Unrecognized regex pattern syntax: {s}" )
def _read_next ( ctx : ReaderContext ) -> LispReaderForm : reader = ctx . reader token = reader . peek ( ) if token == "(" : return _read_list ( ctx ) elif token == "[" : return _read_vector ( ctx ) elif token == "{" : return _read_map ( ctx ) elif begin_num_chars . match ( token ) : return _read_num ( ctx ) elif whitespace_chars . match ( token ) : reader . next_token ( ) return _read_next ( ctx ) elif token == ":" : return _read_kw ( ctx ) elif token == '"' : return _read_str ( ctx ) elif token == "'" : return _read_quoted ( ctx ) elif token == "\\" : return _read_character ( ctx ) elif ns_name_chars . match ( token ) : return _read_sym ( ctx ) elif token == "#" : return _read_reader_macro ( ctx ) elif token == "^" : return _read_meta ( ctx ) elif token == ";" : return _read_comment ( ctx ) elif token == "`" : return _read_syntax_quoted ( ctx ) elif token == "~" : return _read_unquote ( ctx ) elif token == "@" : return _read_deref ( ctx ) elif token == "" : return ctx . eof else : raise SyntaxError ( "Unexpected token '{token}'" . format ( token = token ) )
def _basilisp_bytecode ( mtime : int , source_size : int , code : List [ types . CodeType ] ) -> bytes : data = bytearray ( MAGIC_NUMBER ) data . extend ( _w_long ( mtime ) ) data . extend ( _w_long ( source_size ) ) data . extend ( marshal . dumps ( code ) ) return data
def _exec_cached_module ( self , fullname : str , loader_state : Mapping [ str , str ] , path_stats : Mapping [ str , int ] , module : types . ModuleType , ) : filename = loader_state [ "filename" ] cache_filename = loader_state [ "cache_filename" ] with timed ( lambda duration : logger . debug ( f"Loaded cached Basilisp module '{fullname}' in {duration / 1000000}ms" ) ) : logger . debug ( f"Checking for cached Basilisp module '{fullname}''" ) cache_data = self . get_data ( cache_filename ) cached_code = _get_basilisp_bytecode ( fullname , path_stats [ "mtime" ] , path_stats [ "size" ] , cache_data ) compiler . compile_bytecode ( cached_code , compiler . GeneratorContext ( filename = filename ) , compiler . PythonASTOptimizer ( ) , module , )
def _exec_module ( self , fullname : str , loader_state : Mapping [ str , str ] , path_stats : Mapping [ str , int ] , module : types . ModuleType , ) : filename = loader_state [ "filename" ] cache_filename = loader_state [ "cache_filename" ] with timed ( lambda duration : logger . debug ( f"Loaded Basilisp module '{fullname}' in {duration / 1000000}ms" ) ) : all_bytecode = [ ] def add_bytecode ( bytecode : types . CodeType ) : all_bytecode . append ( bytecode ) logger . debug ( f"Reading and compiling Basilisp module '{fullname}'" ) forms = reader . read_file ( filename , resolver = runtime . resolve_alias ) compiler . compile_module ( forms , compiler . CompilerContext ( filename = filename ) , module , collect_bytecode = add_bytecode , ) cache_file_bytes = _basilisp_bytecode ( path_stats [ "mtime" ] , path_stats [ "size" ] , all_bytecode ) self . _cache_bytecode ( filename , cache_filename , cache_file_bytes )
def symbol ( name : str , ns : Optional [ str ] = None , meta = None ) -> Symbol : return Symbol ( name , ns = ns , meta = meta )
def complete ( text : str , kw_cache : atom . Atom [ "PMap[int, Keyword]" ] = __INTERN ) -> Iterable [ str ] : assert text . startswith ( ":" ) interns = kw_cache . deref ( ) text = text [ 1 : ] if "/" in text : prefix , suffix = text . split ( "/" , maxsplit = 1 ) results = filter ( lambda kw : ( kw . ns is not None and kw . ns == prefix ) and kw . name . startswith ( suffix ) , interns . itervalues ( ) , ) else : results = filter ( lambda kw : kw . name . startswith ( text ) or ( kw . ns is not None and kw . ns . startswith ( text ) ) , interns . itervalues ( ) , ) return map ( str , results )
def keyword ( name : str , ns : Optional [ str ] = None , kw_cache : atom . Atom [ "PMap[int, Keyword]" ] = __INTERN , ) -> Keyword : h = hash ( ( name , ns ) ) return kw_cache . swap ( __get_or_create , h , name , ns ) [ h ]
def _chain_py_ast ( * genned : GeneratedPyAST , ) -> Tuple [ PyASTStream , PyASTStream ] : deps = chain . from_iterable ( map ( lambda n : n . dependencies , genned ) ) nodes = map ( lambda n : n . node , genned ) return deps , nodes
def _simple_ast_generator ( gen_ast ) : @ wraps ( gen_ast ) def wrapped_ast_generator ( ctx : GeneratorContext , form : LispForm ) -> GeneratedPyAST : return GeneratedPyAST ( node = gen_ast ( ctx , form ) ) return wrapped_ast_generator
def _collection_ast ( ctx : GeneratorContext , form : Iterable [ Node ] ) -> Tuple [ PyASTStream , PyASTStream ] : return _chain_py_ast ( * map ( partial ( gen_py_ast , ctx ) , form ) )
def _clean_meta ( form : IMeta ) -> LispForm : assert form . meta is not None , "Form must have non-null 'meta' attribute" meta = form . meta . dissoc ( reader . READER_LINE_KW , reader . READER_COL_KW ) if len ( meta ) == 0 : return None return cast ( lmap . Map , meta )
def _is_redefable ( v : Var ) -> bool : return ( Maybe ( v . meta ) . map ( lambda m : m . get ( SYM_REDEF_META_KEY , None ) ) . or_else_get ( False ) )
def __should_warn_on_redef ( ctx : GeneratorContext , defsym : sym . Symbol , safe_name : str , def_meta : lmap . Map ) -> bool : no_warn_on_redef = def_meta . entry ( SYM_NO_WARN_ON_REDEF_META_KEY , False ) if no_warn_on_redef : return False elif safe_name in ctx . current_ns . module . __dict__ : return True elif defsym in ctx . current_ns . interns : var = ctx . current_ns . find ( defsym ) assert var is not None , f"Var {defsym} cannot be none here" if var . meta is not None and var . meta . entry ( SYM_REDEF_META_KEY ) : return False elif var . is_bound : return True else : return False else : return False
def _deftype_to_py_ast ( ctx : GeneratorContext , node : DefType ) -> GeneratedPyAST : assert node . op == NodeOp . DEFTYPE type_name = munge ( node . name ) ctx . symbol_table . new_symbol ( sym . symbol ( node . name ) , type_name , LocalType . DEFTYPE ) bases = [ ] for base in node . interfaces : base_node = gen_py_ast ( ctx , base ) assert ( count ( base_node . dependencies ) == 0 ) , "Class and host form nodes do not have dependencies" bases . append ( base_node . node ) decorator = ast . Call ( func = _ATTR_CLASS_DECORATOR_NAME , args = [ ] , keywords = [ ast . keyword ( arg = "cmp" , value = ast . NameConstant ( False ) ) , ast . keyword ( arg = "frozen" , value = ast . NameConstant ( node . is_frozen ) ) , ast . keyword ( arg = "slots" , value = ast . NameConstant ( True ) ) , ] , ) with ctx . new_symbol_table ( node . name ) : type_nodes = [ ] for field in node . fields : safe_field = munge ( field . name ) type_nodes . append ( ast . Assign ( targets = [ ast . Name ( id = safe_field , ctx = ast . Store ( ) ) ] , value = ast . Call ( func = _ATTRIB_FIELD_FN_NAME , args = [ ] , keywords = [ ] ) , ) ) ctx . symbol_table . new_symbol ( sym . symbol ( field . name ) , safe_field , field . local ) type_deps : List [ ast . AST ] = [ ] for method in node . methods : type_ast = __deftype_method_to_py_ast ( ctx , method ) type_nodes . append ( type_ast . node ) type_deps . extend ( type_ast . dependencies ) return GeneratedPyAST ( node = ast . Name ( id = type_name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( type_deps , [ ast . ClassDef ( name = type_name , bases = bases , keywords = [ ] , body = type_nodes , decorator_list = [ decorator ] , ) ] , ) ) , )
def _do_to_py_ast ( ctx : GeneratorContext , node : Do ) -> GeneratedPyAST : assert node . op == NodeOp . DO assert not node . is_body body_ast = GeneratedPyAST . reduce ( * map ( partial ( gen_py_ast , ctx ) , chain ( node . statements , [ node . ret ] ) ) ) fn_body_ast : List [ ast . AST ] = [ ] do_result_name = genname ( _DO_PREFIX ) fn_body_ast . extend ( map ( statementize , body_ast . dependencies ) ) fn_body_ast . append ( ast . Assign ( targets = [ ast . Name ( id = do_result_name , ctx = ast . Store ( ) ) ] , value = body_ast . node ) ) return GeneratedPyAST ( node = ast . Name ( id = do_result_name , ctx = ast . Load ( ) ) , dependencies = fn_body_ast )
def __fn_args_to_py_ast ( ctx : GeneratorContext , params : Iterable [ Binding ] , body : Do ) -> Tuple [ List [ ast . arg ] , Optional [ ast . arg ] , List [ ast . AST ] ] : fn_args , varg = [ ] , None fn_body_ast : List [ ast . AST ] = [ ] for binding in params : assert binding . init is None , ":fn nodes cannot have bindint :inits" assert varg is None , "Must have at most one variadic arg" arg_name = genname ( munge ( binding . name ) ) if not binding . is_variadic : fn_args . append ( ast . arg ( arg = arg_name , annotation = None ) ) ctx . symbol_table . new_symbol ( sym . symbol ( binding . name ) , arg_name , LocalType . ARG ) else : varg = ast . arg ( arg = arg_name , annotation = None ) safe_local = genname ( munge ( binding . name ) ) fn_body_ast . append ( ast . Assign ( targets = [ ast . Name ( id = safe_local , ctx = ast . Store ( ) ) ] , value = ast . Call ( func = _COLLECT_ARGS_FN_NAME , args = [ ast . Name ( id = arg_name , ctx = ast . Load ( ) ) ] , keywords = [ ] , ) , ) ) ctx . symbol_table . new_symbol ( sym . symbol ( binding . name ) , safe_local , LocalType . ARG ) body_ast = _synthetic_do_to_py_ast ( ctx , body ) fn_body_ast . extend ( map ( statementize , body_ast . dependencies ) ) fn_body_ast . append ( ast . Return ( value = body_ast . node ) ) return fn_args , varg , fn_body_ast
def __single_arity_fn_to_py_ast ( ctx : GeneratorContext , node : Fn , method : FnMethod , def_name : Optional [ str ] = None , meta_node : Optional [ MetaNode ] = None , ) -> GeneratedPyAST : assert node . op == NodeOp . FN assert method . op == NodeOp . FN_METHOD lisp_fn_name = node . local . name if node . local is not None else None py_fn_name = __fn_name ( lisp_fn_name ) if def_name is None else munge ( def_name ) py_fn_node = ast . AsyncFunctionDef if node . is_async else ast . FunctionDef with ctx . new_symbol_table ( py_fn_name ) , ctx . new_recur_point ( method . loop_id , RecurType . FN , is_variadic = node . is_variadic ) : if lisp_fn_name is not None : ctx . symbol_table . new_symbol ( sym . symbol ( lisp_fn_name ) , py_fn_name , LocalType . FN ) fn_args , varg , fn_body_ast = __fn_args_to_py_ast ( ctx , method . params , method . body ) meta_deps , meta_decorators = __fn_meta ( ctx , meta_node ) return GeneratedPyAST ( node = ast . Name ( id = py_fn_name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( meta_deps , [ py_fn_node ( name = py_fn_name , args = ast . arguments ( args = fn_args , kwarg = None , vararg = varg , kwonlyargs = [ ] , defaults = [ ] , kw_defaults = [ ] , ) , body = fn_body_ast , decorator_list = list ( chain ( meta_decorators , [ _BASILISP_FN_FN_NAME ] , [ _TRAMPOLINE_FN_NAME ] if ctx . recur_point . has_recur else [ ] , ) ) , returns = None , ) ] , ) ) , )
def __multi_arity_fn_to_py_ast ( ctx : GeneratorContext , node : Fn , methods : Collection [ FnMethod ] , def_name : Optional [ str ] = None , meta_node : Optional [ MetaNode ] = None , ) -> GeneratedPyAST : assert node . op == NodeOp . FN assert all ( [ method . op == NodeOp . FN_METHOD for method in methods ] ) lisp_fn_name = node . local . name if node . local is not None else None py_fn_name = __fn_name ( lisp_fn_name ) if def_name is None else munge ( def_name ) py_fn_node = ast . AsyncFunctionDef if node . is_async else ast . FunctionDef arity_to_name = { } rest_arity_name : Optional [ str ] = None fn_defs = [ ] for method in methods : arity_name = f"{py_fn_name}__arity{'_rest' if method.is_variadic else method.fixed_arity}" if method . is_variadic : rest_arity_name = arity_name else : arity_to_name [ method . fixed_arity ] = arity_name with ctx . new_symbol_table ( arity_name ) , ctx . new_recur_point ( method . loop_id , RecurType . FN , is_variadic = node . is_variadic ) : if lisp_fn_name is not None : ctx . symbol_table . new_symbol ( sym . symbol ( lisp_fn_name ) , py_fn_name , LocalType . FN ) fn_args , varg , fn_body_ast = __fn_args_to_py_ast ( ctx , method . params , method . body ) fn_defs . append ( py_fn_node ( name = arity_name , args = ast . arguments ( args = fn_args , kwarg = None , vararg = varg , kwonlyargs = [ ] , defaults = [ ] , kw_defaults = [ ] , ) , body = fn_body_ast , decorator_list = [ _TRAMPOLINE_FN_NAME ] if ctx . recur_point . has_recur else [ ] , returns = None , ) ) dispatch_fn_ast = __multi_arity_dispatch_fn ( ctx , py_fn_name , arity_to_name , default_name = rest_arity_name , max_fixed_arity = node . max_fixed_arity , meta_node = meta_node , is_async = node . is_async , ) return GeneratedPyAST ( node = dispatch_fn_ast . node , dependencies = list ( chain ( fn_defs , dispatch_fn_ast . dependencies ) ) , )
def _fn_to_py_ast ( ctx : GeneratorContext , node : Fn , def_name : Optional [ str ] = None , meta_node : Optional [ MetaNode ] = None , ) -> GeneratedPyAST : assert node . op == NodeOp . FN if len ( node . methods ) == 1 : return __single_arity_fn_to_py_ast ( ctx , node , next ( iter ( node . methods ) ) , def_name = def_name , meta_node = meta_node ) else : return __multi_arity_fn_to_py_ast ( ctx , node , node . methods , def_name = def_name , meta_node = meta_node )
def _import_to_py_ast ( ctx : GeneratorContext , node : Import ) -> GeneratedPyAST : assert node . op == NodeOp . IMPORT last = None deps : List [ ast . AST ] = [ ] for alias in node . aliases : safe_name = munge ( alias . name ) try : module = importlib . import_module ( safe_name ) if alias . alias is not None : ctx . add_import ( sym . symbol ( alias . name ) , module , sym . symbol ( alias . alias ) ) else : ctx . add_import ( sym . symbol ( alias . name ) , module ) except ModuleNotFoundError as e : raise ImportError ( f"Python module '{alias.name}' not found" , node . form , node ) from e py_import_alias = ( munge ( alias . alias ) if alias . alias is not None else safe_name . split ( "." , maxsplit = 1 ) [ 0 ] ) deps . append ( ast . Assign ( targets = [ ast . Name ( id = py_import_alias , ctx = ast . Store ( ) ) ] , value = ast . Call ( func = _load_attr ( "builtins.__import__" ) , args = [ ast . Str ( safe_name ) ] , keywords = [ ] , ) , ) ) last = ast . Name ( id = py_import_alias , ctx = ast . Load ( ) ) deps . append ( ast . Call ( func = _load_attr ( f"{_NS_VAR_VALUE}.add_import" ) , args = [ ast . Call ( func = _NEW_SYM_FN_NAME , args = [ ast . Str ( safe_name ) ] , keywords = [ ] ) , last , ] , keywords = [ ] , ) ) assert last is not None , "import* node must have at least one import" return GeneratedPyAST ( node = last , dependencies = deps )
def _invoke_to_py_ast ( ctx : GeneratorContext , node : Invoke ) -> GeneratedPyAST : assert node . op == NodeOp . INVOKE fn_ast = gen_py_ast ( ctx , node . fn ) args_deps , args_nodes = _collection_ast ( ctx , node . args ) return GeneratedPyAST ( node = ast . Call ( func = fn_ast . node , args = list ( args_nodes ) , keywords = [ ] ) , dependencies = list ( chain ( fn_ast . dependencies , args_deps ) ) , )
def _let_to_py_ast ( ctx : GeneratorContext , node : Let ) -> GeneratedPyAST : assert node . op == NodeOp . LET with ctx . new_symbol_table ( "let" ) : let_body_ast : List [ ast . AST ] = [ ] for binding in node . bindings : init_node = binding . init assert init_node is not None init_ast = gen_py_ast ( ctx , init_node ) binding_name = genname ( munge ( binding . name ) ) let_body_ast . extend ( init_ast . dependencies ) let_body_ast . append ( ast . Assign ( targets = [ ast . Name ( id = binding_name , ctx = ast . Store ( ) ) ] , value = init_ast . node , ) ) ctx . symbol_table . new_symbol ( sym . symbol ( binding . name ) , binding_name , LocalType . LET ) let_result_name = genname ( "let_result" ) body_ast = _synthetic_do_to_py_ast ( ctx , node . body ) let_body_ast . extend ( map ( statementize , body_ast . dependencies ) ) let_body_ast . append ( ast . Assign ( targets = [ ast . Name ( id = let_result_name , ctx = ast . Store ( ) ) ] , value = body_ast . node , ) ) return GeneratedPyAST ( node = ast . Name ( id = let_result_name , ctx = ast . Load ( ) ) , dependencies = let_body_ast )
def _loop_to_py_ast ( ctx : GeneratorContext , node : Loop ) -> GeneratedPyAST : assert node . op == NodeOp . LOOP with ctx . new_symbol_table ( "loop" ) : binding_names = [ ] init_bindings : List [ ast . AST ] = [ ] for binding in node . bindings : init_node = binding . init assert init_node is not None init_ast = gen_py_ast ( ctx , init_node ) init_bindings . extend ( init_ast . dependencies ) binding_name = genname ( munge ( binding . name ) ) binding_names . append ( binding_name ) init_bindings . append ( ast . Assign ( targets = [ ast . Name ( id = binding_name , ctx = ast . Store ( ) ) ] , value = init_ast . node , ) ) ctx . symbol_table . new_symbol ( sym . symbol ( binding . name ) , binding_name , LocalType . LOOP ) loop_result_name = genname ( "loop" ) with ctx . new_recur_point ( node . loop_id , RecurType . LOOP , binding_names = binding_names ) : loop_body_ast : List [ ast . AST ] = [ ] body_ast = _synthetic_do_to_py_ast ( ctx , node . body ) loop_body_ast . extend ( body_ast . dependencies ) loop_body_ast . append ( ast . Assign ( targets = [ ast . Name ( id = loop_result_name , ctx = ast . Store ( ) ) ] , value = body_ast . node , ) ) loop_body_ast . append ( ast . Break ( ) ) return GeneratedPyAST ( node = _load_attr ( loop_result_name ) , dependencies = list ( chain ( [ ast . Assign ( targets = [ ast . Name ( id = loop_result_name , ctx = ast . Store ( ) ) ] , value = ast . NameConstant ( None ) , ) ] , init_bindings , [ ast . While ( test = ast . NameConstant ( True ) , body = loop_body_ast , orelse = [ ] , ) ] , ) ) , )
def _quote_to_py_ast ( ctx : GeneratorContext , node : Quote ) -> GeneratedPyAST : assert node . op == NodeOp . QUOTE return _const_node_to_py_ast ( ctx , node . expr )
def __fn_recur_to_py_ast ( ctx : GeneratorContext , node : Recur ) -> GeneratedPyAST : assert node . op == NodeOp . RECUR assert ctx . recur_point . is_variadic is not None recur_nodes : List [ ast . AST ] = [ ] recur_deps : List [ ast . AST ] = [ ] for expr in node . exprs : expr_ast = gen_py_ast ( ctx , expr ) recur_nodes . append ( expr_ast . node ) recur_deps . extend ( expr_ast . dependencies ) return GeneratedPyAST ( node = ast . Call ( func = _TRAMPOLINE_ARGS_FN_NAME , args = list ( chain ( [ ast . NameConstant ( ctx . recur_point . is_variadic ) ] , recur_nodes ) ) , keywords = [ ] , ) , dependencies = recur_deps , )
def __deftype_method_recur_to_py_ast ( ctx : GeneratorContext , node : Recur ) -> GeneratedPyAST : assert node . op == NodeOp . RECUR recur_nodes : List [ ast . AST ] = [ ] recur_deps : List [ ast . AST ] = [ ] for expr in node . exprs : expr_ast = gen_py_ast ( ctx , expr ) recur_nodes . append ( expr_ast . node ) recur_deps . extend ( expr_ast . dependencies ) this_entry = ctx . symbol_table . find_symbol ( ctx . current_this ) assert this_entry is not None , "Field type local must have this" return GeneratedPyAST ( node = ast . Call ( func = _TRAMPOLINE_ARGS_FN_NAME , args = list ( chain ( [ ast . NameConstant ( ctx . recur_point . is_variadic ) , ast . Name ( id = this_entry . munged , ctx = ast . Load ( ) ) , ] , recur_nodes , ) ) , keywords = [ ] , ) , dependencies = recur_deps , )
def __loop_recur_to_py_ast ( ctx : GeneratorContext , node : Recur ) -> GeneratedPyAST : assert node . op == NodeOp . RECUR recur_deps : List [ ast . AST ] = [ ] recur_targets : List [ ast . Name ] = [ ] recur_exprs : List [ ast . AST ] = [ ] for name , expr in zip ( ctx . recur_point . binding_names , node . exprs ) : expr_ast = gen_py_ast ( ctx , expr ) recur_deps . extend ( expr_ast . dependencies ) recur_targets . append ( ast . Name ( id = name , ctx = ast . Store ( ) ) ) recur_exprs . append ( expr_ast . node ) if len ( recur_targets ) == 1 : assert len ( recur_exprs ) == 1 recur_deps . append ( ast . Assign ( targets = recur_targets , value = recur_exprs [ 0 ] ) ) else : recur_deps . append ( ast . Assign ( targets = [ ast . Tuple ( elts = recur_targets , ctx = ast . Store ( ) ) ] , value = ast . Tuple ( elts = recur_exprs , ctx = ast . Load ( ) ) , ) ) recur_deps . append ( ast . Continue ( ) ) return GeneratedPyAST ( node = ast . NameConstant ( None ) , dependencies = recur_deps )
def _set_bang_to_py_ast ( ctx : GeneratorContext , node : SetBang ) -> GeneratedPyAST : assert node . op == NodeOp . SET_BANG val_temp_name = genname ( "set_bang_val" ) val_ast = gen_py_ast ( ctx , node . val ) target = node . target assert isinstance ( target , ( HostField , Local , VarRef ) ) , f"invalid set! target type {type(target)}" if isinstance ( target , HostField ) : target_ast = _interop_prop_to_py_ast ( ctx , target , is_assigning = True ) elif isinstance ( target , VarRef ) : target_ast = _var_sym_to_py_ast ( ctx , target , is_assigning = True ) elif isinstance ( target , Local ) : target_ast = _local_sym_to_py_ast ( ctx , target , is_assigning = True ) else : raise GeneratorException ( f"invalid set! target type {type(target)}" , lisp_ast = target ) return GeneratedPyAST ( node = ast . Name ( id = val_temp_name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( val_ast . dependencies , [ ast . Assign ( targets = [ ast . Name ( id = val_temp_name , ctx = ast . Store ( ) ) ] , value = val_ast . node , ) ] , target_ast . dependencies , [ ast . Assign ( targets = [ target_ast . node ] , value = val_ast . node ) ] , ) ) , )
def _throw_to_py_ast ( ctx : GeneratorContext , node : Throw ) -> GeneratedPyAST : assert node . op == NodeOp . THROW throw_fn = genname ( _THROW_PREFIX ) exc_ast = gen_py_ast ( ctx , node . exception ) raise_body = ast . Raise ( exc = exc_ast . node , cause = None ) return GeneratedPyAST ( node = ast . Call ( func = ast . Name ( id = throw_fn , ctx = ast . Load ( ) ) , args = [ ] , keywords = [ ] ) , dependencies = [ ast . FunctionDef ( name = throw_fn , args = ast . arguments ( args = [ ] , kwarg = None , vararg = None , kwonlyargs = [ ] , defaults = [ ] , kw_defaults = [ ] , ) , body = list ( chain ( exc_ast . dependencies , [ raise_body ] ) ) , decorator_list = [ ] , returns = None , ) ] , )
def _try_to_py_ast ( ctx : GeneratorContext , node : Try ) -> GeneratedPyAST : assert node . op == NodeOp . TRY try_expr_name = genname ( "try_expr" ) body_ast = _synthetic_do_to_py_ast ( ctx , node . body ) catch_handlers = list ( map ( partial ( __catch_to_py_ast , ctx , try_expr_name = try_expr_name ) , node . catches ) ) finallys : List [ ast . AST ] = [ ] if node . finally_ is not None : finally_ast = _synthetic_do_to_py_ast ( ctx , node . finally_ ) finallys . extend ( map ( statementize , finally_ast . dependencies ) ) finallys . append ( statementize ( finally_ast . node ) ) return GeneratedPyAST ( node = ast . Name ( id = try_expr_name , ctx = ast . Load ( ) ) , dependencies = [ ast . Try ( body = list ( chain ( body_ast . dependencies , [ ast . Assign ( targets = [ ast . Name ( id = try_expr_name , ctx = ast . Store ( ) ) ] , value = body_ast . node , ) ] , ) ) , handlers = catch_handlers , orelse = [ ] , finalbody = finallys , ) ] , )
def _local_sym_to_py_ast ( ctx : GeneratorContext , node : Local , is_assigning : bool = False ) -> GeneratedPyAST : assert node . op == NodeOp . LOCAL sym_entry = ctx . symbol_table . find_symbol ( sym . symbol ( node . name ) ) assert sym_entry is not None if node . local == LocalType . FIELD : this_entry = ctx . symbol_table . find_symbol ( ctx . current_this ) assert this_entry is not None , "Field type local must have this" return GeneratedPyAST ( node = _load_attr ( f"{this_entry.munged}.{sym_entry.munged}" , ctx = ast . Store ( ) if is_assigning else ast . Load ( ) , ) ) else : return GeneratedPyAST ( node = ast . Name ( id = sym_entry . munged , ctx = ast . Store ( ) if is_assigning else ast . Load ( ) ) )
def __var_find_to_py_ast ( var_name : str , ns_name : str , py_var_ctx : ast . AST ) -> GeneratedPyAST : return GeneratedPyAST ( node = ast . Attribute ( value = ast . Call ( func = _FIND_VAR_FN_NAME , args = [ ast . Call ( func = _NEW_SYM_FN_NAME , args = [ ast . Str ( var_name ) ] , keywords = [ ast . keyword ( arg = "ns" , value = ast . Str ( ns_name ) ) ] , ) ] , keywords = [ ] , ) , attr = "value" , ctx = py_var_ctx , ) )
def _interop_call_to_py_ast ( ctx : GeneratorContext , node : HostCall ) -> GeneratedPyAST : assert node . op == NodeOp . HOST_CALL target_ast = gen_py_ast ( ctx , node . target ) args_deps , args_nodes = _collection_ast ( ctx , node . args ) return GeneratedPyAST ( node = ast . Call ( func = ast . Attribute ( value = target_ast . node , attr = munge ( node . method , allow_builtins = True ) , ctx = ast . Load ( ) , ) , args = list ( args_nodes ) , keywords = [ ] , ) , dependencies = list ( chain ( target_ast . dependencies , args_deps ) ) , )
def _interop_prop_to_py_ast ( ctx : GeneratorContext , node : HostField , is_assigning : bool = False ) -> GeneratedPyAST : assert node . op == NodeOp . HOST_FIELD target_ast = gen_py_ast ( ctx , node . target ) return GeneratedPyAST ( node = ast . Attribute ( value = target_ast . node , attr = munge ( node . field ) , ctx = ast . Store ( ) if is_assigning else ast . Load ( ) , ) , dependencies = target_ast . dependencies , )
def _with_meta_to_py_ast ( ctx : GeneratorContext , node : WithMeta , * * kwargs ) -> GeneratedPyAST : assert node . op == NodeOp . WITH_META handle_expr = _WITH_META_EXPR_HANDLER . get ( node . expr . op ) assert ( handle_expr is not None ) , "No expression handler for with-meta child node type" return handle_expr ( ctx , node . expr , meta_node = node . meta , * * kwargs )
def py_module_preamble ( ctx : GeneratorContext , ) -> GeneratedPyAST : preamble : List [ ast . AST ] = [ ] preamble . extend ( _module_imports ( ctx ) ) preamble . append ( _from_module_import ( ) ) preamble . append ( _ns_var ( ) ) return GeneratedPyAST ( node = ast . NameConstant ( None ) , dependencies = preamble )
def set ( members : Iterable [ T ] , meta = None ) -> Set [ T ] : return Set ( pset ( members ) , meta = meta )
def s ( * members : T , meta = None ) -> Set [ T ] : return Set ( pset ( members ) , meta = meta )
def visit_ExceptHandler ( self , node : ast . ExceptHandler ) -> Optional [ ast . AST ] : new_node = self . generic_visit ( node ) assert isinstance ( new_node , ast . ExceptHandler ) return ast . copy_location ( ast . ExceptHandler ( type = new_node . type , name = new_node . name , body = _filter_dead_code ( new_node . body ) , ) , new_node , )
def visit_FunctionDef ( self , node : ast . FunctionDef ) -> Optional [ ast . AST ] : new_node = self . generic_visit ( node ) assert isinstance ( new_node , ast . FunctionDef ) return ast . copy_location ( ast . FunctionDef ( name = new_node . name , args = new_node . args , body = _filter_dead_code ( new_node . body ) , decorator_list = new_node . decorator_list , returns = new_node . returns , ) , new_node , )
def visit_While ( self , node : ast . While ) -> Optional [ ast . AST ] : new_node = self . generic_visit ( node ) assert isinstance ( new_node , ast . While ) return ast . copy_location ( ast . While ( test = new_node . test , body = _filter_dead_code ( new_node . body ) , orelse = _filter_dead_code ( new_node . orelse ) , ) , new_node , )
def visit_Try ( self , node : ast . Try ) -> Optional [ ast . AST ] : new_node = self . generic_visit ( node ) assert isinstance ( new_node , ast . Try ) return ast . copy_location ( ast . Try ( body = _filter_dead_code ( new_node . body ) , handlers = new_node . handlers , orelse = _filter_dead_code ( new_node . orelse ) , finalbody = _filter_dead_code ( new_node . finalbody ) , ) , new_node , )
def nthrest ( coll , i : int ) : while True : if coll is None : return None if i == 0 : return coll i -= 1 coll = rest ( coll )
def nthnext ( coll , i : int ) -> Optional [ ISeq ] : while True : if coll is None : return None if i == 0 : return to_seq ( coll ) i -= 1 coll = next_ ( coll )
def to_seq ( o ) -> Optional [ ISeq ] : if o is None : return None if isinstance ( o , ISeq ) : return _seq_or_nil ( o ) if isinstance ( o , ISeqable ) : return _seq_or_nil ( o . seq ( ) ) return _seq_or_nil ( lseq . sequence ( o ) )
def concat ( * seqs ) -> ISeq : allseqs = lseq . sequence ( itertools . chain ( * filter ( None , map ( to_seq , seqs ) ) ) ) if allseqs is None : return lseq . EMPTY return allseqs
def partial ( f , * args ) : @ functools . wraps ( f ) def partial_f ( * inner_args ) : return f ( * itertools . chain ( args , inner_args ) ) return partial_f
def contains ( coll , k ) : if isinstance ( coll , IAssociative ) : return coll . contains ( k ) return k in coll
def get ( m , k , default = None ) : if isinstance ( m , IAssociative ) : return m . entry ( k , default = default ) try : return m [ k ] except ( KeyError , IndexError , TypeError ) as e : logger . debug ( "Ignored %s: %s" , type ( e ) . __name__ , e ) return default
def to_lisp ( o , keywordize_keys : bool = True ) : if not isinstance ( o , ( dict , frozenset , list , set , tuple ) ) : return o else : return _to_lisp_backup ( o , keywordize_keys = keywordize_keys )
def to_py ( o , keyword_fn : Callable [ [ kw . Keyword ] , Any ] = _kw_name ) : if isinstance ( o , ISeq ) : return _to_py_list ( o , keyword_fn = keyword_fn ) elif not isinstance ( o , ( IPersistentList , IPersistentMap , IPersistentSet , IPersistentVector ) ) : return o else : return _to_py_backup ( o , keyword_fn = keyword_fn )
def _collect_args ( args ) -> ISeq : if isinstance ( args , tuple ) : return llist . list ( args ) raise TypeError ( "Python variadic arguments should always be a tuple" )
def init_ns_var ( which_ns : str = CORE_NS , ns_var_name : str = NS_VAR_NAME ) -> Var : core_sym = sym . Symbol ( which_ns ) core_ns = Namespace . get_or_create ( core_sym ) ns_var = Var . intern ( core_sym , sym . Symbol ( ns_var_name ) , core_ns , dynamic = True ) logger . debug ( f"Created namespace variable {sym.symbol(ns_var_name, ns=which_ns)}" ) return ns_var
def set_current_ns ( ns_name : str , module : types . ModuleType = None , ns_var_name : str = NS_VAR_NAME , ns_var_ns : str = NS_VAR_NS , ) -> Var : symbol = sym . Symbol ( ns_name ) ns = Namespace . get_or_create ( symbol , module = module ) ns_var_sym = sym . Symbol ( ns_var_name , ns = ns_var_ns ) ns_var = Maybe ( Var . find ( ns_var_sym ) ) . or_else_raise ( lambda : RuntimeException ( f"Dynamic Var {sym.Symbol(ns_var_name, ns=ns_var_ns)} not bound!" ) ) ns_var . push_bindings ( ns ) logger . debug ( f"Setting {ns_var_sym} to {ns}" ) return ns_var
def get_current_ns ( ns_var_name : str = NS_VAR_NAME , ns_var_ns : str = NS_VAR_NS ) -> Namespace : ns_sym = sym . Symbol ( ns_var_name , ns = ns_var_ns ) ns : Namespace = Maybe ( Var . find ( ns_sym ) ) . map ( lambda v : v . value ) . or_else_raise ( lambda : RuntimeException ( f"Dynamic Var {ns_sym} not bound!" ) ) return ns
def resolve_alias ( s : sym . Symbol , ns : Optional [ Namespace ] = None ) -> sym . Symbol : if s in _SPECIAL_FORMS : return s ns = Maybe ( ns ) . or_else ( get_current_ns ) if s . ns is not None : aliased_ns = ns . get_alias ( sym . symbol ( s . ns ) ) if aliased_ns is not None : return sym . symbol ( s . name , aliased_ns . name ) else : return s else : which_var = ns . find ( sym . symbol ( s . name ) ) if which_var is not None : return sym . symbol ( which_var . name . name , which_var . ns . name ) else : return sym . symbol ( s . name , ns = ns . name )
def add_generated_python ( generated_python : str , var_name : str = _GENERATED_PYTHON_VAR_NAME , which_ns : Optional [ str ] = None , ) -> None : if which_ns is None : which_ns = get_current_ns ( ) . name ns_sym = sym . Symbol ( var_name , ns = which_ns ) v = Maybe ( Var . find ( ns_sym ) ) . or_else ( lambda : Var . intern ( sym . symbol ( which_ns ) , sym . symbol ( var_name ) , "" , dynamic = True , meta = lmap . map ( { _PRIVATE_META_KEY : True } ) , ) ) v . value = v . value + generated_python
def print_generated_python ( var_name : str = _PRINT_GENERATED_PY_VAR_NAME , core_ns_name : str = CORE_NS ) -> bool : ns_sym = sym . Symbol ( var_name , ns = core_ns_name ) return ( Maybe ( Var . find ( ns_sym ) ) . map ( lambda v : v . value ) . or_else_raise ( lambda : RuntimeException ( f"Dynamic Var {ns_sym} not bound!" ) ) )
def intern ( ns : sym . Symbol , name : sym . Symbol , val , dynamic : bool = False , meta = None ) -> "Var" : var_ns = Namespace . get_or_create ( ns ) var = var_ns . intern ( name , Var ( var_ns , name , dynamic = dynamic , meta = meta ) ) var . root = val return var
def intern_unbound ( ns : sym . Symbol , name : sym . Symbol , dynamic : bool = False , meta = None ) -> "Var" : var_ns = Namespace . get_or_create ( ns ) return var_ns . intern ( name , Var ( var_ns , name , dynamic = dynamic , meta = meta ) )
def add_alias ( self , alias : sym . Symbol , namespace : "Namespace" ) -> None : self . _aliases . swap ( lambda m : m . assoc ( alias , namespace ) )
def add_refer ( self , sym : sym . Symbol , var : Var ) -> None : if not var . is_private : self . _refers . swap ( lambda s : s . assoc ( sym , var ) )
def get_refer ( self , sym : sym . Symbol ) -> Optional [ Var ] : return self . refers . entry ( sym , None )
def __refer_all ( cls , refers : lmap . Map , other_ns_interns : lmap . Map ) -> lmap . Map : final_refers = refers for entry in other_ns_interns : s : sym . Symbol = entry . key var : Var = entry . value if not var . is_private : final_refers = final_refers . assoc ( s , var ) return final_refers
def refer_all ( self , other_ns : "Namespace" ) : self . _refers . swap ( Namespace . __refer_all , other_ns . interns )
def list ( members , meta = None ) -> List : return List ( plist ( iterable = members ) , meta = meta )
def l ( * members , meta = None ) -> List : return List ( plist ( iterable = members ) , meta = meta )
def change_style ( style , representer ) : def new_representer ( dumper , data ) : scalar = representer ( dumper , data ) scalar . style = style return scalar return new_representer
def delete ( self , * args ) : cache = get_cache ( ) key = self . get_cache_key ( * args ) if key in cache : del cache [ key ]
async def connect ( self ) : self . reader , self . writer = await asyncio . open_connection ( self . host , self . port , loop = self . loop ) self . welcome_msg = await self . reader . read ( self . buffer_size )
async def receive ( self ) : try : incomming = await self . reader . read ( self . buffer_size ) except OSError : return [ ] return _parse_receive ( incomming )
def dump ( ndb_model , fp , * * kwargs ) : for chunk in NdbEncoder ( * * kwargs ) . iterencode ( ndb_model ) : fp . write ( chunk )
def object_hook_handler ( self , val ) : return { k : self . decode_date ( v ) for k , v in val . iteritems ( ) }
def decode_date ( self , val ) : if isinstance ( val , basestring ) and val . count ( '-' ) == 2 and len ( val ) > 9 : try : dt = dateutil . parser . parse ( val ) if val . endswith ( ( '+00:00' , '-00:00' , 'Z' ) ) : dt = dt . replace ( tzinfo = None ) return dt except ( TypeError , ValueError ) : pass return val
def decode ( self , val ) : new_val = self . decode_date ( val ) if val != new_val : return new_val return json . JSONDecoder . decode ( self , val )
def default ( self , obj ) : obj_type = type ( obj ) if obj_type not in self . _ndb_type_encoding : if hasattr ( obj , '__metaclass__' ) : obj_type = obj . __metaclass__ else : for ndb_type in NDB_TYPES : if isinstance ( obj , ndb_type ) : obj_type = ndb_type break fn = self . _ndb_type_encoding . get ( obj_type ) if fn : return fn ( obj ) return json . JSONEncoder . default ( self , obj )
def validate_version ( ) : import leicacam version_string = leicacam . __version__ versions = version_string . split ( '.' , 3 ) try : for ver in versions : int ( ver ) except ValueError : print ( 'Only integers are allowed in release version, ' 'please adjust current version {}' . format ( version_string ) ) return None return version_string
def robust_topological_sort ( graph : Graph ) -> list : assert check_argument_types ( ) components = strongly_connected_components ( graph ) node_component = { } for component in components : for node in component : node_component [ node ] = component component_graph = { } for component in components : component_graph [ component ] = [ ] for node in graph : node_c = node_component [ node ] for successor in graph [ node ] : successor_c = node_component [ successor ] if node_c != successor_c : component_graph [ node_c ] . append ( successor_c ) return topological_sort ( component_graph )
def logger ( function ) : @ functools . wraps ( function ) def wrapper ( * args , * * kwargs ) : """Wrap function.""" sep = kwargs . get ( 'sep' , ' ' ) end = kwargs . get ( 'end' , '' ) out = sep . join ( [ repr ( x ) for x in args ] ) out = out + end _LOGGER . debug ( out ) return function ( * args , * * kwargs ) return wrapper
def connect ( self ) : self . socket = socket . socket ( ) self . socket . connect ( ( self . host , self . port ) ) self . socket . settimeout ( False ) sleep ( self . delay ) self . welcome_msg = self . socket . recv ( self . buffer_size )
def flush ( self ) : debug ( 'flushing incomming socket messages' ) try : while True : msg = self . socket . recv ( self . buffer_size ) debug ( b'< ' + msg ) except socket . error : pass
def receive ( self ) : try : incomming = self . socket . recv ( self . buffer_size ) except socket . error : return [ ] return _parse_receive ( incomming )
def enable ( self , slide = 0 , wellx = 1 , welly = 1 , fieldx = 1 , fieldy = 1 ) : cmd = [ ( 'cmd' , 'enable' ) , ( 'slide' , str ( slide ) ) , ( 'wellx' , str ( wellx ) ) , ( 'welly' , str ( welly ) ) , ( 'fieldx' , str ( fieldx ) ) , ( 'fieldy' , str ( fieldy ) ) , ( 'value' , 'true' ) ] self . send ( cmd ) return self . wait_for ( * cmd [ 0 ] )
def save_template ( self , filename = "{ScanningTemplate}leicacam.xml" ) : cmd = [ ( 'sys' , '0' ) , ( 'cmd' , 'save' ) , ( 'fil' , str ( filename ) ) ] self . send ( cmd ) return self . wait_for ( * cmd [ 0 ] )
def get_information ( self , about = 'stage' ) : cmd = [ ( 'cmd' , 'getinfo' ) , ( 'dev' , str ( about ) ) ] self . send ( cmd ) return self . wait_for ( * cmd [ 1 ] )
def locate_package_json ( ) : directory = settings . SYSTEMJS_PACKAGE_JSON_DIR if not directory : raise ImproperlyConfigured ( "Could not locate 'package.json'. Set SYSTEMJS_PACKAGE_JSON_DIR " "to the directory that holds 'package.json'." ) path = os . path . join ( directory , 'package.json' ) if not os . path . isfile ( path ) : raise ImproperlyConfigured ( "'package.json' does not exist, tried looking in %s" % path ) return path
def parse_package_json ( ) : with open ( locate_package_json ( ) ) as pjson : data = json . loads ( pjson . read ( ) ) return data
def _validate_yourls_response ( response , data ) : try : response . raise_for_status ( ) except HTTPError as http_exc : http_error_info = sys . exc_info ( ) reraise = False try : jsondata = response . json ( ) except ValueError : reraise = True else : logger . debug ( 'Received error {response} with JSON {json}' , response = response , json = jsondata ) _handle_api_error_with_json ( http_exc , jsondata , response ) if reraise : six . reraise ( * http_error_info ) else : jsondata = response . json ( ) logger . debug ( 'Received {response} with JSON {json}' , response = response , json = jsondata ) if { 'status' , 'code' , 'message' } <= set ( jsondata . keys ( ) ) : status = jsondata [ 'status' ] code = jsondata [ 'code' ] message = jsondata [ 'message' ] if status == 'fail' : if code == 'error:keyword' : raise YOURLSKeywordExistsError ( message , keyword = data [ 'keyword' ] ) elif code == 'error:url' : url = _json_to_shortened_url ( jsondata [ 'url' ] , jsondata [ 'shorturl' ] ) raise YOURLSURLExistsError ( message , url = url ) else : raise YOURLSAPIError ( message ) else : return jsondata else : return jsondata
def _interp_dep_vector ( wave , indep_vector ) : dep_vector_is_int = wave . dep_vector . dtype . name . startswith ( "int" ) dep_vector_is_complex = wave . dep_vector . dtype . name . startswith ( "complex" ) if ( wave . interp , wave . indep_scale ) == ( "CONTINUOUS" , "LOG" ) : wave_interp_func = scipy . interpolate . interp1d ( np . log10 ( wave . indep_vector ) , wave . dep_vector ) ret = wave_interp_func ( np . log10 ( indep_vector ) ) elif ( wave . interp , wave . indep_scale ) == ( "CONTINUOUS" , "LINEAR" ) : dep_vector = ( wave . dep_vector . astype ( np . float64 ) if not dep_vector_is_complex else wave . dep_vector ) wave_interp_func = scipy . interpolate . interp1d ( wave . indep_vector , dep_vector ) ret = wave_interp_func ( indep_vector ) else : wave_interp_func = scipy . interpolate . interp1d ( wave . indep_vector , wave . dep_vector , kind = "zero" ) ret = wave_interp_func ( indep_vector ) eq_comp = np . all ( np . isclose ( wave . indep_vector [ - 1 ] , indep_vector [ - 1 ] , FP_RTOL , FP_ATOL ) ) if eq_comp : ret [ - 1 ] = wave . dep_vector [ - 1 ] round_ret = np . round ( ret , 0 ) return ( round_ret . astype ( "int" ) if ( dep_vector_is_int and np . all ( np . isclose ( round_ret , ret , FP_RTOL , FP_ATOL ) ) ) else ret )
def _get_indep_vector ( wave_a , wave_b ) : exobj = pexdoc . exh . addex ( RuntimeError , "Independent variable ranges do not overlap" ) min_bound = max ( np . min ( wave_a . indep_vector ) , np . min ( wave_b . indep_vector ) ) max_bound = min ( np . max ( wave_a . indep_vector ) , np . max ( wave_b . indep_vector ) ) exobj ( bool ( min_bound > max_bound ) ) raw_range = np . unique ( np . concatenate ( ( wave_a . indep_vector , wave_b . indep_vector ) ) ) return raw_range [ np . logical_and ( min_bound <= raw_range , raw_range <= max_bound ) ]
def _verify_compatibility ( wave_a , wave_b , check_dep_units = True ) : exobj = pexdoc . exh . addex ( RuntimeError , "Waveforms are not compatible" ) ctuple = ( bool ( wave_a . indep_scale != wave_b . indep_scale ) , bool ( wave_a . dep_scale != wave_b . dep_scale ) , bool ( wave_a . indep_units != wave_b . indep_units ) , ( bool ( wave_a . dep_units != wave_b . dep_units ) if check_dep_units else False ) , bool ( wave_a . interp != wave_b . interp ) , ) exobj ( any ( ctuple ) )
def trace_pars ( mname ) : pickle_fname = os . path . join ( os . path . dirname ( __file__ ) , "{0}.pkl" . format ( mname ) ) ddir = os . path . dirname ( os . path . dirname ( __file__ ) ) moddb_fname = os . path . join ( ddir , "moddb.json" ) in_callables_fname = moddb_fname if os . path . exists ( moddb_fname ) else None out_callables_fname = os . path . join ( ddir , "{0}.json" . format ( mname ) ) noption = os . environ . get ( "NOPTION" , None ) exclude = [ "_pytest" , "execnet" ] partuple = collections . namedtuple ( "ParTuple" , [ "pickle_fname" , "in_callables_fname" , "out_callables_fname" , "noption" , "exclude" , ] , ) return partuple ( pickle_fname , in_callables_fname , out_callables_fname , noption , exclude )
def run_trace ( mname , fname , module_prefix , callable_names , no_print , module_exclude = None , callable_exclude = None , debug = False , ) : module_exclude = [ ] if module_exclude is None else module_exclude callable_exclude = [ ] if callable_exclude is None else callable_exclude par = trace_pars ( mname ) start_time = datetime . datetime . now ( ) with pexdoc . exdoc . ExDocCxt ( exclude = par . exclude + module_exclude , pickle_fname = par . pickle_fname , in_callables_fname = par . in_callables_fname , out_callables_fname = par . out_callables_fname , _no_print = no_print , ) as exdoc_obj : fname = os . path . realpath ( os . path . join ( os . path . dirname ( __file__ ) , ".." , ".." , "tests" , "test_{0}.py" . format ( fname ) , ) ) test_cmd = ( [ "--color=yes" ] + ( [ "-s" , "-vv" ] if debug else [ "-q" , "-q" , "-q" ] ) + [ "--disable-warnings" ] + [ "-x" ] + ( [ par . noption ] if par . noption else [ ] ) + [ "-m " + mname ] + [ fname ] ) with warnings . catch_warnings ( ) : warnings . filterwarnings ( "ignore" , category = PytestWarning ) if pytest . main ( test_cmd ) : raise RuntimeError ( "Tracing did not complete successfully" ) stop_time = datetime . datetime . now ( ) if not no_print : print ( "Auto-generation of exceptions documentation time: {0}" . format ( pmisc . elapsed_time_string ( start_time , stop_time ) ) ) for callable_name in callable_names : callable_name = module_prefix + callable_name print ( "\nCallable: {0}" . format ( callable_name ) ) print ( exdoc_obj . get_sphinx_doc ( callable_name , exclude = callable_exclude ) ) print ( "\n" ) return copy . copy ( exdoc_obj )
def flatten ( iterable , map2iter = None ) : if map2iter and isinstance ( iterable ) : iterable = map2iter ( iterable ) for item in iterable : if isinstance ( item , str ) or not isinstance ( item , abc . Iterable ) : yield item else : yield from flatten ( item , map2iter )
def printtsv ( table , sep = "\t" , file = sys . stdout ) : for record in table : print ( * record , sep = sep , file = file )
def mkdummy ( name , * * attrs ) : return type ( name , ( ) , dict ( __repr__ = ( lambda self : "<%s>" % name ) , * * attrs ) ) ( )
def from_str ( cls , human_readable_str , decimal = False , bits = False ) : divisor = 1000 if decimal else 1024 num = [ ] c = "" for c in human_readable_str : if c not in cls . digits : break num . append ( c ) num = "" . join ( num ) try : num = int ( num ) except ValueError : num = float ( num ) if bits : num /= 8 return cls ( round ( num * divisor ** cls . key [ c . lower ( ) ] ) )
def trace_module ( no_print = True ) : mname = "wave_core" fname = "peng" module_prefix = "peng.{0}.Waveform." . format ( mname ) callable_names = ( "__init__" , ) return docs . support . trace_support . run_trace ( mname , fname , module_prefix , callable_names , no_print )
def def_links ( mobj ) : fdict = json_load ( os . path . join ( "data" , "requirements.json" ) ) sdeps = sorted ( fdict . keys ( ) ) olines = [ ] for item in sdeps : olines . append ( ".. _{name}: {url}\n" . format ( name = fdict [ item ] [ "name" ] , url = fdict [ item ] [ "url" ] ) ) ret = [ ] for line in olines : wobj = textwrap . wrap ( line , width = LINE_WIDTH , subsequent_indent = "   " ) ret . append ( "\n" . join ( [ item for item in wobj ] ) ) mobj . out ( "\n" . join ( ret ) )
def make_common_entry ( plist , pyver , suffix , req_ver ) : prefix = "Python {pyver}.x{suffix}" . format ( pyver = pyver , suffix = suffix ) plist . append ( "{prefix}{ver}" . format ( prefix = prefix , ver = ops_to_words ( req_ver ) ) )
def make_multi_entry ( plist , pkg_pyvers , ver_dict ) : for pyver in pkg_pyvers : pver = pyver [ 2 ] + "." + pyver [ 3 : ] plist . append ( "Python {0}: {1}" . format ( pver , ops_to_words ( ver_dict [ pyver ] ) ) )
def ops_to_words ( item ) : unsupp_ops = [ "~=" , "===" ] supp_ops = [ ">=" , ">" , "==" , "<=" , "<" , "!=" ] tokens = sorted ( item . split ( "," ) , reverse = True ) actual_tokens = [ ] for req in tokens : for op in unsupp_ops : if req . startswith ( op ) : raise RuntimeError ( "Unsupported version specification: {0}" . format ( op ) ) for op in supp_ops : if req . startswith ( op ) : actual_tokens . append ( op ) break else : raise RuntimeError ( "Illegal comparison operator: {0}" . format ( op ) ) if len ( list ( set ( actual_tokens ) ) ) != len ( actual_tokens ) : raise RuntimeError ( "Multiple comparison operators of the same type" ) if "!=" in actual_tokens : return ( " and " . join ( [ op_to_words ( token ) for token in tokens [ : - 1 ] ] ) + " " + op_to_words ( tokens [ - 1 ] ) ) return " and " . join ( [ op_to_words ( token ) for token in tokens ] )
def _chunk_noise ( noise ) : data = zip ( noise [ "freq" ] , noise [ "nf" ] , np . abs ( noise [ "rc" ] ) , np . angle ( noise [ "rc" ] ) , noise [ "res" ] , ) for freq , nf , rcmag , rcangle , res in data : yield freq , nf , rcmag , rcangle , res
def _chunk_pars ( freq_vector , data_matrix , pformat ) : pformat = pformat . upper ( ) length = 4 for freq , data in zip ( freq_vector , data_matrix ) : data = data . flatten ( ) for index in range ( 0 , data . size , length ) : fpoint = [ freq ] if not index else [ None ] cdata = data [ index : index + length ] if pformat == "MA" : vector1 = np . abs ( cdata ) vector2 = np . rad2deg ( np . angle ( cdata ) ) elif pformat == "RI" : vector1 = np . real ( cdata ) vector2 = np . imag ( cdata ) else : vector1 = 20.0 * np . log10 ( np . abs ( cdata ) ) vector2 = np . rad2deg ( np . angle ( cdata ) ) sep_data = np . array ( [ ] ) for item1 , item2 in zip ( vector1 , vector2 ) : sep_data = np . concatenate ( ( sep_data , np . array ( [ item1 , item2 ] ) ) ) ret = np . concatenate ( ( np . array ( fpoint ) , sep_data ) ) yield ret
def _bound_waveform ( wave , indep_min , indep_max ) : indep_min , indep_max = _validate_min_max ( wave , indep_min , indep_max ) indep_vector = copy . copy ( wave . _indep_vector ) if ( isinstance ( indep_min , float ) or isinstance ( indep_max , float ) ) and indep_vector . dtype . name . startswith ( "int" ) : indep_vector = indep_vector . astype ( float ) min_pos = np . searchsorted ( indep_vector , indep_min ) if not np . isclose ( indep_min , indep_vector [ min_pos ] , FP_RTOL , FP_ATOL ) : indep_vector = np . insert ( indep_vector , min_pos , indep_min ) max_pos = np . searchsorted ( indep_vector , indep_max ) if not np . isclose ( indep_max , indep_vector [ max_pos ] , FP_RTOL , FP_ATOL ) : indep_vector = np . insert ( indep_vector , max_pos , indep_max ) dep_vector = _interp_dep_vector ( wave , indep_vector ) wave . _indep_vector = indep_vector [ min_pos : max_pos + 1 ] wave . _dep_vector = dep_vector [ min_pos : max_pos + 1 ]
def _build_units ( indep_units , dep_units , op ) : if ( not dep_units ) and ( not indep_units ) : return "" if dep_units and ( not indep_units ) : return dep_units if ( not dep_units ) and indep_units : return ( remove_extra_delims ( "1{0}({1})" . format ( op , indep_units ) ) if op == "/" else remove_extra_delims ( "({0})" . format ( indep_units ) ) ) return remove_extra_delims ( "({0}){1}({2})" . format ( dep_units , op , indep_units ) )
def _operation ( wave , desc , units , fpointer ) : ret = copy . copy ( wave ) ret . dep_units = units ret . dep_name = "{0}({1})" . format ( desc , ret . dep_name ) ret . _dep_vector = fpointer ( ret . _dep_vector ) return ret
def _running_area ( indep_vector , dep_vector ) : rect_height = np . minimum ( dep_vector [ : - 1 ] , dep_vector [ 1 : ] ) rect_base = np . diff ( indep_vector ) rect_area = np . multiply ( rect_height , rect_base ) triang_height = np . abs ( np . diff ( dep_vector ) ) triang_area = 0.5 * np . multiply ( triang_height , rect_base ) return np . cumsum ( np . concatenate ( ( np . array ( [ 0.0 ] ) , triang_area + rect_area ) ) )
def _validate_min_max ( wave , indep_min , indep_max ) : imin , imax = False , False if indep_min is None : indep_min = wave . _indep_vector [ 0 ] imin = True if indep_max is None : indep_max = wave . _indep_vector [ - 1 ] imax = True if imin and imax : return indep_min , indep_max exminmax = pexdoc . exh . addex ( RuntimeError , "Incongruent `indep_min` and `indep_max` arguments" ) exmin = pexdoc . exh . addai ( "indep_min" ) exmax = pexdoc . exh . addai ( "indep_max" ) exminmax ( bool ( indep_min >= indep_max ) ) exmin ( bool ( ( indep_min < wave . _indep_vector [ 0 ] ) and ( not np . isclose ( indep_min , wave . _indep_vector [ 0 ] , FP_RTOL , FP_ATOL ) ) ) ) exmax ( bool ( ( indep_max > wave . _indep_vector [ - 1 ] ) and ( not np . isclose ( indep_max , wave . _indep_vector [ - 1 ] , FP_RTOL , FP_ATOL ) ) ) ) return indep_min , indep_max
def get_short_desc ( long_desc ) : found = False olines = [ ] for line in [ item . rstrip ( ) for item in long_desc . split ( "\n" ) ] : if found and ( ( ( not line ) and ( not olines ) ) or ( line and olines ) ) : olines . append ( line ) elif found and olines and ( not line ) : return ( " " . join ( olines ) . split ( "." ) [ 0 ] ) . strip ( ) found = line == ".. [[[end]]]" if not found else found return ""
def render ( self , context ) : module_path = self . path . resolve ( context ) if not settings . SYSTEMJS_ENABLED : if settings . SYSTEMJS_DEFAULT_JS_EXTENSIONS : name , ext = posixpath . splitext ( module_path ) if not ext : module_path = '{}.js' . format ( module_path ) if settings . SYSTEMJS_SERVER_URL : tpl = """<script src="{url}{app}" type="text/javascript"></script>""" else : tpl = """<script type="text/javascript">System.import('{app}');</script>""" return tpl . format ( app = module_path , url = settings . SYSTEMJS_SERVER_URL ) rel_path = System . get_bundle_path ( module_path ) url = staticfiles_storage . url ( rel_path ) tag_attrs = { 'type' : 'text/javascript' } for key , value in self . tag_attrs . items ( ) : if not isinstance ( value , bool ) : value = value . resolve ( context ) tag_attrs [ key ] = value return """<script{attrs} src="{url}"></script>""" . format ( url = url , attrs = flatatt ( tag_attrs ) )
def _build_expr ( tokens , higher_oplevel = - 1 , ldelim = "(" , rdelim = ")" ) : if isinstance ( tokens , str ) : return tokens if len ( tokens ) == 2 : return "" . join ( tokens ) oplevel = _get_op_level ( tokens [ 1 ] ) stoken = "" for num , item in enumerate ( tokens ) : if num % 2 == 0 : stoken += _build_expr ( item , oplevel , ldelim = ldelim , rdelim = rdelim ) else : stoken += item if ( oplevel < higher_oplevel ) or ( ( oplevel == higher_oplevel ) and ( oplevel in _OP_PREC_PAR ) ) : stoken = ldelim + stoken + rdelim return stoken
def _next_rdelim ( items , pos ) : for num , item in enumerate ( items ) : if item > pos : break else : raise RuntimeError ( "Mismatched delimiters" ) del items [ num ] return item
def _get_functions ( expr , ldelim = "(" , rdelim = ")" ) : tpars = _pair_delims ( expr , ldelim = ldelim , rdelim = rdelim ) alphas = "abcdefghijklmnopqrstuvwxyz" "ABCDEFGHIJKLMNOPQRSTUVWXYZ" fchars = "abcdefghijklmnopqrstuvwxyz" "ABCDEFGHIJKLMNOPQRSTUVWXYZ" "0123456789" "_" tfuncs = [ ] for lnum , rnum in tpars : if lnum and expr [ lnum - 1 ] in fchars : for cnum , char in enumerate ( reversed ( expr [ : lnum ] ) ) : if char not in fchars : break else : cnum = lnum tfuncs . append ( { "fname" : expr [ lnum - cnum : lnum ] , "expr" : expr [ lnum + 1 : rnum ] , "start" : lnum - cnum , "stop" : rnum , } ) if expr [ lnum - cnum ] not in alphas : raise RuntimeError ( "Function name `{0}` is not valid" . format ( expr [ lnum - cnum : lnum ] ) ) return tfuncs
def _parse_expr ( text , ldelim = "(" , rdelim = ")" ) : var = pyparsing . Word ( pyparsing . alphas + "_" , pyparsing . alphanums + "_" ) point = pyparsing . Literal ( "." ) exp = pyparsing . CaselessLiteral ( "E" ) number = pyparsing . Combine ( pyparsing . Word ( "+-" + pyparsing . nums , pyparsing . nums ) + pyparsing . Optional ( point + pyparsing . Optional ( pyparsing . Word ( pyparsing . nums ) ) ) + pyparsing . Optional ( exp + pyparsing . Word ( "+-" + pyparsing . nums , pyparsing . nums ) ) ) atom = var | number oplist = [ ( pyparsing . Literal ( "**" ) , 2 , pyparsing . opAssoc . RIGHT ) , ( pyparsing . oneOf ( "+ - ~" ) , 1 , pyparsing . opAssoc . RIGHT ) , ( pyparsing . oneOf ( "* / // %" ) , 2 , pyparsing . opAssoc . LEFT ) , ( pyparsing . oneOf ( "+ -" ) , 2 , pyparsing . opAssoc . LEFT ) , ( pyparsing . oneOf ( "<< >>" ) , 2 , pyparsing . opAssoc . LEFT ) , ( pyparsing . Literal ( "&" ) , 2 , pyparsing . opAssoc . LEFT ) , ( pyparsing . Literal ( "^" ) , 2 , pyparsing . opAssoc . LEFT ) , ( pyparsing . Literal ( "|" ) , 2 , pyparsing . opAssoc . LEFT ) , ] expr = pyparsing . infixNotation ( atom , oplist , lpar = pyparsing . Suppress ( ldelim ) , rpar = pyparsing . Suppress ( rdelim ) ) return expr . parseString ( text ) [ 0 ]
def _remove_consecutive_delims ( expr , ldelim = "(" , rdelim = ")" ) : tpars = _pair_delims ( expr , ldelim = ldelim , rdelim = rdelim ) ddelim = [ ] for ctuple , ntuple in zip ( tpars , tpars [ 1 : ] ) : if ctuple == ( ntuple [ 0 ] - 1 , ntuple [ 1 ] + 1 ) : ddelim . extend ( ntuple ) ddelim . sort ( ) for num , item in enumerate ( ddelim ) : expr = expr [ : item - num ] + expr [ item - num + 1 : ] return expr
def needs_ext ( self ) : if settings . SYSTEMJS_DEFAULT_JS_EXTENSIONS : name , ext = posixpath . splitext ( self . app ) if not ext : return True return False
def bundle ( self ) : outfile , rel_path = self . get_paths ( ) options = self . opts if self . system . _has_jspm_log ( ) : self . command += ' --log {log}' options . setdefault ( 'log' , 'err' ) if options . get ( 'minify' ) : self . command += ' --minify' if options . get ( 'skip_source_maps' ) : self . command += ' --skip-source-maps' try : cmd = self . command . format ( app = self . app , outfile = outfile , * * options ) proc = subprocess . Popen ( cmd , shell = True , cwd = self . system . cwd , stdout = self . stdout , stdin = self . stdin , stderr = self . stderr ) result , err = proc . communicate ( ) if err and self . system . _has_jspm_log ( ) : fmt = 'Could not bundle \'%s\': \n%s' logger . warn ( fmt , self . app , err ) raise BundleError ( fmt % ( self . app , err ) ) if result . strip ( ) : logger . info ( result ) except ( IOError , OSError ) as e : if isinstance ( e , BundleError ) : raise raise BundleError ( 'Unable to apply %s (%r): %s' % ( self . __class__ . __name__ , cmd , e ) ) else : if not options . get ( 'sfx' ) : sourcemap = find_sourcemap_comment ( outfile ) with open ( outfile , 'a' ) as of : of . write ( "\nSystem.import('{app}{ext}');\n{sourcemap}" . format ( app = self . app , ext = '.js' if self . needs_ext ( ) else '' , sourcemap = sourcemap if sourcemap else '' , ) ) return rel_path
def parse_docstring ( doc ) : doc = inspect . cleandoc ( doc ) lines = doc . split ( '\n' ) section = None section_indent = None params = { } returns = None for line in lines : line = line . rstrip ( ) if len ( line ) == 0 : continue elif str ( line ) == 'Args:' : section = 'args' section_indent = None continue elif str ( line ) == 'Returns:' : section = 'return' section_indent = None continue if section is not None : stripped = line . lstrip ( ) margin = len ( line ) - len ( stripped ) if section_indent is None : section_indent = margin if margin != section_indent : continue if section == 'args' : param_name , type_info = parse_param ( stripped ) params [ param_name ] = type_info elif section == 'return' : returns = parse_return ( stripped ) return params , returns
def _split_line ( self , line ) : parts = shlex . split ( line , posix = self . posix_lex ) if not self . posix_lex : parts = [ self . _remove_quotes ( x ) for x in parts ] return parts
def _builtin_help ( self , args ) : if len ( args ) == 0 : return self . list_dir ( self . contexts [ - 1 ] ) if len ( args ) == 1 : func = self . find_function ( self . contexts [ - 1 ] , args [ 0 ] ) return annotate . get_help ( func ) help_text = "Too many arguments: " + str ( args ) + "\n" help_text += "Usage: help [function]" return help_text
def _extract_arg_value ( cls , arg_name , arg_type , remaining ) : next_arg = None should_consume = False if len ( remaining ) > 0 : next_arg = remaining [ 0 ] should_consume = True if next_arg == '--' : next_arg = None if arg_type == "bool" : if next_arg is None or next_arg . startswith ( '-' ) : next_arg = True should_consume = False else : if next_arg is None : raise ArgumentError ( "Could not find value for keyword argument" , argument = arg_name ) if should_consume : remaining . pop ( 0 ) return next_arg
def parse_param ( param , include_desc = False ) : param_def , _colon , desc = param . partition ( ':' ) if not include_desc : desc = None else : desc = desc . lstrip ( ) if _colon == "" : raise ValidationError ( "Invalid parameter declaration in docstring, missing colon" , declaration = param ) param_name , _space , param_type = param_def . partition ( ' ' ) if len ( param_type ) < 2 or param_type [ 0 ] != '(' or param_type [ - 1 ] != ')' : raise ValidationError ( "Invalid parameter type string not enclosed in ( ) characters" , param_string = param_def , type_string = param_type ) param_type = param_type [ 1 : - 1 ] return param_name , ParameterInfo ( param_type , [ ] , desc )
def _classify_section ( cls , section ) : name = section . lower ( ) if name in frozenset ( [ 'args' , 'arguments' , "params" , "parameters" ] ) : return cls . ARGS_SECTION if name in frozenset ( [ 'returns' , 'return' ] ) : return cls . RETURN_SECTION if name in frozenset ( [ 'main' ] ) : return cls . MAIN_SECTION return None
def _classify_line ( cls , line ) : line = line . rstrip ( ) if len ( line ) == 0 : return BlankLine ( '' ) if ' ' not in line and line . endswith ( ':' ) : name = line [ : - 1 ] return SectionHeader ( name ) if line . startswith ( '  ' ) : return ContinuationLine ( line . lstrip ( ) ) if line . startswith ( ' - ' ) : return ListItem ( '-' , line [ 3 : ] . lstrip ( ) ) if line . startswith ( '- ' ) : return ListItem ( '-' , line [ 2 : ] . lstrip ( ) ) return Line ( line )
def _join_paragraphs ( cls , lines , use_indent = False , leading_blanks = False , trailing_blanks = False ) : curr_para = [ ] paragraphs = [ ] for line in lines : if use_indent : if line . startswith ( ' ' ) : curr_para . append ( line . lstrip ( ) ) continue elif line == '' : continue else : if len ( curr_para ) > 0 : paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) curr_para = [ line . lstrip ( ) ] else : if len ( line ) != 0 : curr_para . append ( line ) else : paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) curr_para = [ ] if len ( curr_para ) > 0 : paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) return paragraphs
def split_type ( self , typename ) : name = self . _canonicalize_type ( typename ) if '(' not in name : return name , False , [ ] base , sub = name . split ( '(' ) if len ( sub ) == 0 or sub [ - 1 ] != ')' : raise ArgumentError ( "syntax error in complex type, no matching ) found" , passed_type = typename , basetype = base , subtype_string = sub ) sub = sub [ : - 1 ] subs = sub . split ( ',' ) return base , True , subs
def instantiate_type ( self , typename , base , subtypes ) : if base not in self . type_factories : raise ArgumentError ( "unknown complex base type specified" , passed_type = typename , base_type = base ) base_type = self . type_factories [ base ] #Make sure all of the subtypes are valid for sub_type in subtypes : try : self . get_type ( sub_type ) except KeyValueException as exc : raise ArgumentError ( "could not instantiate subtype for complex type" , passed_type = typename , sub_type = sub_type , error = exc ) typeobj = base_type . Build ( * subtypes , type_system = self ) self . inject_type ( typename , typeobj )
def short_description ( func ) : doc = inspect . getdoc ( func ) if doc is not None : doc = inspect . cleandoc ( doc ) lines = doc . splitlines ( ) return lines [ 0 ] return ""
def load ( ) : autodiscover_modules ( 'cron' ) if PROJECT_MODULE : if '.' in PROJECT_MODULE . __name__ : try : import_module ( '%s.cron' % '.' . join ( PROJECT_MODULE . __name__ . split ( '.' ) [ 0 : - 1 ] ) ) except ImportError as e : if 'No module named' not in str ( e ) : print ( e ) for cmd , app in get_commands ( ) . items ( ) : try : load_command_class ( app , cmd ) except django . core . exceptions . ImproperlyConfigured : pass
def install ( ) : load ( ) tab = crontab . CronTab ( user = True ) for task in registry : tab . new ( task . command , KRONOS_BREADCRUMB ) . setall ( task . schedule ) tab . write ( ) return len ( registry )
def uninstall ( ) : tab = crontab . CronTab ( user = True ) count = len ( list ( tab . find_comment ( KRONOS_BREADCRUMB ) ) ) tab . remove_all ( comment = KRONOS_BREADCRUMB ) tab . write ( ) return count
def kind ( self ) : optics = [ Equality , Isomorphism , Prism , Review , Lens , Traversal , Getter , Setter , Fold , ] for optic in optics : if self . _is_kind ( optic ) : return optic
def play ( ) : ai = { 'X' : player_move , 'O' : random_move } board = Board ( ) while not board . winner : x , y = ai [ board . player ] ( board ) board = board . make_move ( x , y ) print ( board , end = '\n\n' ) print ( board . winner )
def winner ( self ) : for potential_win in self . _potential_wins ( ) : if potential_win == tuple ( 'XXX' ) : return Outcome . win_for_crosses elif potential_win == tuple ( 'OOO' ) : return Outcome . win_for_naughts if self . _count ( ' ' ) == 0 : return Outcome . draw return Outcome . ongoing
def open_spider ( self , spider ) : self . ts = datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) . replace ( ':' , '-' )
def _upload_chunk ( self , spider ) : if not self . items : return f = self . _make_fileobj ( ) object_key = self . object_key_template . format ( * * self . _get_uri_params ( spider ) ) try : self . s3 . upload_fileobj ( f , self . bucket_name , object_key ) except ClientError : self . stats . inc_value ( 'pipeline/s3/fail' ) raise else : self . stats . inc_value ( 'pipeline/s3/success' ) finally : self . chunk_number += len ( self . items ) self . items = [ ]
def _make_fileobj ( self ) : bio = BytesIO ( ) f = gzip . GzipFile ( mode = 'wb' , fileobj = bio ) if self . use_gzip else bio exporter = JsonLinesItemExporter ( f ) exporter . start_exporting ( ) for item in self . items : exporter . export_item ( item ) exporter . finish_exporting ( ) if f is not bio : f . close ( ) bio . seek ( 0 ) return bio
def _call ( self , method , params = None , request_id = None ) : params = params or [ ] rid = request_id or self . _id_counter if request_id is None : self . _id_counter += 1 payload = { 'jsonrpc' : '2.0' , 'method' : method , 'params' : params , 'id' : rid } headers = { 'Content-Type' : 'application/json' } scheme = 'https' if self . tls else 'http' url = '{}://{}:{}' . format ( scheme , self . host , self . port ) try : response = self . session . post ( url , headers = headers , data = json . dumps ( payload ) ) response . raise_for_status ( ) except HTTPError : raise TransportError ( 'Got unsuccessful response from server (status code: {})' . format ( response . status_code ) , response = response ) try : response_data = response . json ( ) except ValueError as e : raise ProtocolError ( 'Unable to deserialize response body: {}' . format ( e ) , response = response ) if response_data . get ( 'error' ) : code = response_data [ 'error' ] . get ( 'code' , '' ) message = response_data [ 'error' ] . get ( 'message' , '' ) raise ProtocolError ( 'Error[{}] {}' . format ( code , message ) , response = response , data = response_data ) elif 'result' not in response_data : raise ProtocolError ( 'Response is empty (result field is missing)' , response = response , data = response_data ) return response_data [ 'result' ]
def is_hash256 ( s ) : if not s or not isinstance ( s , str ) : return False return re . match ( '^[0-9A-F]{64}$' , s . strip ( ) , re . IGNORECASE )
def is_hash160 ( s ) : if not s or not isinstance ( s , str ) : return False if not len ( s ) == 40 : return False for c in s : if ( c < '0' or c > '9' ) and ( c < 'A' or c > 'F' ) and ( c < 'a' or c > 'f' ) : return False return True
def encode_invocation_params ( params ) : final_params = [ ] for p in params : if isinstance ( p , bool ) : final_params . append ( { 'type' : ContractParameterTypes . BOOLEAN . value , 'value' : p } ) elif isinstance ( p , int ) : final_params . append ( { 'type' : ContractParameterTypes . INTEGER . value , 'value' : p } ) elif is_hash256 ( p ) : final_params . append ( { 'type' : ContractParameterTypes . HASH256 . value , 'value' : p } ) elif is_hash160 ( p ) : final_params . append ( { 'type' : ContractParameterTypes . HASH160 . value , 'value' : p } ) elif isinstance ( p , bytearray ) : final_params . append ( { 'type' : ContractParameterTypes . BYTE_ARRAY . value , 'value' : p } ) elif isinstance ( p , str ) : final_params . append ( { 'type' : ContractParameterTypes . STRING . value , 'value' : p } ) elif isinstance ( p , list ) : innerp = encode_invocation_params ( p ) final_params . append ( { 'type' : ContractParameterTypes . ARRAY . value , 'value' : innerp } ) return final_params
def decode_invocation_result ( result ) : if 'stack' not in result : return result result = copy . deepcopy ( result ) result [ 'stack' ] = _decode_invocation_result_stack ( result [ 'stack' ] ) return result
def connect ( cls , settings ) : server = serializer ( 'json' ) . loads ( settings [ 'kvs.perlsess' ] ) server . setdefault ( 'key_prefix' , 'perlsess::' ) server . setdefault ( 'codec' , 'storable' ) cls . cookie_name = server . pop ( 'cookie_name' , 'session_id' ) cls . client = KVS ( * * server )
def basic ( username , password ) : none ( ) _config . username = username _config . password = password
def api_key ( api_key ) : none ( ) _config . api_key_prefix [ "Authorization" ] = "api-key" _config . api_key [ "Authorization" ] = "key=" + b64encode ( api_key . encode ( ) ) . decode ( )
def _get_json_content_from_folder ( folder ) : for dirpath , dirnames , filenames in os . walk ( folder ) : for filename in filenames : if filename . lower ( ) . endswith ( ".json" ) : filepath = os . path . join ( dirpath , filename ) with open ( filepath , "rb" ) as file : yield json . loads ( file . read ( ) . decode ( "UTF-8" ) )
def get_schema ( self ) : path = os . path . join ( self . _get_schema_folder ( ) , self . _name + ".json" ) with open ( path , "rb" ) as file : schema = json . loads ( file . read ( ) . decode ( "UTF-8" ) ) return schema
def get_valid_examples ( self ) : path = os . path . join ( self . _get_schema_folder ( ) , "examples" , "valid" ) return list ( _get_json_content_from_folder ( path ) )
def get_invalid_examples ( self ) : path = os . path . join ( self . _get_schema_folder ( ) , "examples" , "invalid" ) return list ( _get_json_content_from_folder ( path ) )
def auth_user_get_url ( self , scope = None ) : if not self . client_id : raise AuthMissingError ( 'No client_id specified' ) return '{}?{}' . format ( self . auth_url_user , urllib . urlencode ( dict ( client_id = self . client_id , scope = ' ' . join ( scope or self . auth_scope ) , response_type = 'code' , redirect_uri = self . auth_redirect_uri ) ) )
def auth_user_process_url ( self , url ) : url = urlparse . urlparse ( url ) url_qs = dict ( it . chain . from_iterable ( urlparse . parse_qsl ( v ) for v in [ url . query , url . fragment ] ) ) if url_qs . get ( 'error' ) : raise APIAuthError ( '{} :: {}' . format ( url_qs [ 'error' ] , url_qs . get ( 'error_description' ) ) ) self . auth_code = url_qs [ 'code' ] return self . auth_code
def auth_get_token ( self , check_scope = True ) : res = self . auth_access_data_raw = self . _auth_token_request ( ) return self . _auth_token_process ( res , check_scope = check_scope )
def get_user_id ( self ) : if self . _user_id is None : self . _user_id = self . get_user_data ( ) [ 'id' ] return self . _user_id
def listdir ( self , folder_id = 'me/skydrive' , limit = None , offset = None ) : return self ( self . _api_url_join ( folder_id , 'files' ) , dict ( limit = limit , offset = offset ) )
def comment_add ( self , obj_id , message ) : return self ( self . _api_url_join ( obj_id , 'comments' ) , method = 'post' , data = dict ( message = message ) , auth_header = True )
def decode_obj ( obj , force = False ) : if isinstance ( obj , unicode ) : return obj elif isinstance ( obj , bytes ) : if force_encoding is not None : return obj . decode ( force_encoding ) if chardet : enc_guess = chardet . detect ( obj ) if enc_guess [ 'confidence' ] > 0.7 : return obj . decode ( enc_guess [ 'encoding' ] ) return obj . decode ( 'utf-8' ) else : return obj if not force else repr ( obj )
def set_drop_target ( obj , root , designer , inspector ) : if obj . _meta . container : dt = ToolBoxDropTarget ( obj , root , designer = designer , inspector = inspector ) obj . drop_target = dt for child in obj : set_drop_target ( child , root , designer , inspector )
def start_drag_opperation ( self , evt ) : ctrl = self . menu_ctrl_map [ evt . GetToolId ( ) ] ldata = wx . CustomDataObject ( "gui" ) ldata . SetData ( ctrl . _meta . name ) bmp = ctrl . _image . GetBitmap ( ) bdata = wx . BitmapDataObject ( bmp ) data = wx . DataObjectComposite ( ) data . Add ( ldata ) data . Add ( bdata ) dropSource = wx . DropSource ( self ) dropSource . SetData ( data ) if DEBUG : print ( "Begining DragDrop\n" ) result = dropSource . DoDragDrop ( wx . Drag_AllowMove ) if DEBUG : print ( "DragDrop completed: %d\n" % result ) if result == wx . DragMove : if DEBUG : print "dragmove!" self . Refresh ( )
def set_default_tlw ( self , tlw , designer , inspector ) : self . designer = designer self . inspector = inspector
def inspect ( obj ) : from gui . tools . inspector import InspectorTool inspector = InspectorTool ( ) inspector . show ( obj ) return inspector
def migrate_window ( bg ) : ret = { } for k , v in bg . items ( ) : if k == 'type' : v = WIN_MAP [ v ] . _meta . name elif k == 'menubar' : menus = v [ 'menus' ] v = [ migrate_control ( menu ) for menu in menus ] elif k == 'components' : v = [ migrate_control ( comp ) for comp in v ] else : k = SPEC_MAP [ 'Widget' ] . get ( k , k ) ret [ k ] = v return ret
def migrate_control ( comp ) : ret = { } for k , v in comp . items ( ) : if k == 'type' : v = CTRL_MAP [ v ] . _meta . name elif k == 'menubar' : pass elif k == 'components' : v = [ migrate_control ( comp ) for comp in v ] else : k = SPEC_MAP [ 'Widget' ] . get ( k , k ) if comp [ 'type' ] in SPEC_MAP : k = SPEC_MAP [ comp [ 'type' ] ] . get ( k , k ) if k == 'font' : v = migrate_font ( v ) ret [ k ] = v return ret
def migrate_font ( font ) : if 'faceName' in font : font [ 'face' ] = font . pop ( 'faceName' ) if 'family' in font and font [ 'family' ] == 'sansSerif' : font [ 'family' ] = 'sans serif' return font
def load_page ( self , location ) : if not location : self . wx_obj . SetPage ( "" ) else : self . wx_obj . LoadPage ( location )
def GetParam ( tag , param , default = __SENTINEL ) : if tag . HasParam ( param ) : return tag . GetParam ( param ) else : if default == __SENTINEL : raise KeyError else : return default
def send ( evt ) : msg = ctrl_input . value gui . alert ( msg , "Message" ) log ( msg ) ctrl_input . value = "" ctrl_input . set_focus ( )
def wellcome_tip ( wx_obj ) : msg = ( "Close the main window to exit & save.\n" "Drag & Drop / Click the controls from the ToolBox to create new ones.\n" "Left click on the created controls to select them.\n" "Double click to edit the default property.\n" "Right click to pop-up the context menu.\n" ) stt = STT . SuperToolTip ( msg ) stt . SetHeader ( "Welcome to gui2py designer!" ) stt . SetDrawHeaderLine ( True ) stt . ApplyStyle ( "Office 2007 Blue" ) stt . SetDropShadow ( True ) stt . SetHeaderBitmap ( images . designer . GetBitmap ( ) ) stt . SetEndDelay ( 15000 ) tip = CustomToolTipWindow ( wx_obj , stt ) tip . CalculateBestSize ( ) tip . CalculateBestPosition ( wx_obj ) tip . DropShadow ( stt . GetDropShadow ( ) ) if stt . GetUseFade ( ) : show = lambda : tip . StartAlpha ( True ) else : show = lambda : tip . Show ( ) wx . CallLater ( 1000 , show ) wx . CallLater ( 30000 , tip . Destroy )
def mouse_down ( self , evt ) : if DEBUG : print "down!" if ( not evt . ControlDown ( ) and not evt . ShiftDown ( ) ) or evt . AltDown ( ) : for obj in self . selection : if obj . sel_marker : obj . sel_marker . show ( False ) obj . sel_marker . destroy ( ) obj . sel_marker = None self . selection = [ ] wx_obj = evt . GetEventObject ( ) if wx_obj . Parent is None or evt . AltDown ( ) : if not evt . AltDown ( ) : evt . Skip ( ) self . current = wx_obj self . overlay = wx . Overlay ( ) self . pos = evt . GetPosition ( ) self . parent . wx_obj . CaptureMouse ( ) #if self.inspector and hasattr(wx_obj, "obj"): #self.dclick = False else : obj = wx_obj . obj self . overlay = None if DEBUG : print wx_obj sx , sy = wx_obj . ScreenToClient ( wx_obj . GetPositionTuple ( ) ) dx , dy = wx_obj . ScreenToClient ( wx . GetMousePosition ( ) ) self . pos = wx_obj . ScreenToClient ( wx . GetMousePosition ( ) ) self . start = ( sx - dx , sy - dy ) self . current = wx_obj if DEBUG : print "capture..." if not isinstance ( wx_obj , wx . Notebook ) : self . parent . wx_obj . CaptureMouse ( ) self . select ( obj , keep_selection = True )
def mouse_move ( self , evt ) : if DEBUG : print "move!" if self . current and not self . overlay : wx_obj = self . current sx , sy = self . start x , y = wx . GetMousePosition ( ) x , y = ( x + sx , y + sy ) if evt . ShiftDown ( ) : x = x / GRID_SIZE [ 0 ] * GRID_SIZE [ 0 ] y = y / GRID_SIZE [ 1 ] * GRID_SIZE [ 1 ] ox , oy = wx_obj . obj . pos dx , dy = ( x - ox ) , ( y - oy ) for obj in self . selection : x , y = obj . pos x = x + dx y = y + dy obj . pos = ( wx . Point ( x , y ) ) elif self . overlay : wx_obj = self . current pos = evt . GetPosition ( ) if evt . GetEventObject ( ) != wx_obj : pos = evt . GetEventObject ( ) . ClientToScreen ( pos ) pos = wx_obj . ScreenToClient ( pos ) rect = wx . RectPP ( self . pos , pos ) dc = wx . ClientDC ( wx_obj ) odc = wx . DCOverlay ( self . overlay , dc ) odc . Clear ( ) dc . SetPen ( wx . Pen ( "blue" , 2 ) ) if 'wxMac' in wx . PlatformInfo : dc . SetBrush ( wx . Brush ( wx . Colour ( 0xC0 , 0xC0 , 0xC0 , 0x80 ) ) ) else : dc . SetBrush ( wx . TRANSPARENT_BRUSH ) dc . DrawRectangleRect ( rect ) del odc
def key_press ( self , event ) : key = event . GetKeyCode ( ) if key in ( wx . WXK_LEFT , wx . WXK_UP , wx . WXK_RIGHT , wx . WXK_DOWN ) : for obj in self . selection : x , y = obj . pos if event . ShiftDown ( ) : if key == wx . WXK_LEFT : x = ( x - GRID_SIZE [ 0 ] ) / GRID_SIZE [ 0 ] * GRID_SIZE [ 0 ] elif key == wx . WXK_RIGHT : x = ( x + GRID_SIZE [ 0 ] ) / GRID_SIZE [ 0 ] * GRID_SIZE [ 0 ] elif key == wx . WXK_UP : y = ( y - GRID_SIZE [ 1 ] ) / GRID_SIZE [ 1 ] * GRID_SIZE [ 1 ] elif key == wx . WXK_DOWN : y = ( y + GRID_SIZE [ 1 ] ) / GRID_SIZE [ 1 ] * GRID_SIZE [ 1 ] else : if key == wx . WXK_LEFT : x = x - 1 elif key == wx . WXK_RIGHT : x = x + 1 elif key == wx . WXK_UP : y = y - 1 elif key == wx . WXK_DOWN : y = y + 1 obj . pos = ( x , y ) elif key == wx . WXK_DELETE : self . delete ( event ) elif key == wx . WXK_INSERT : self . duplicate ( event ) else : if DEBUG : print "KEY:" , key
def delete ( self , event ) : for obj in self . selection : if obj : if DEBUG : print "deleting" , obj . name obj . destroy ( ) self . selection = [ ] self . inspector . load_object ( )
def duplicate ( self , event ) : new_selection = [ ] for obj in self . selection : if obj : if DEBUG : print "duplicating" , obj . name obj . sel_marker . destroy ( ) obj . sel_marker = None obj2 = obj . duplicate ( ) obj2 . sel_marker = SelectionMarker ( obj2 ) obj2 . sel_marker . show ( True ) new_selection . append ( obj2 ) self . selection = new_selection self . inspector . load_object ( )
def refresh ( self ) : self . bmp = self . obj . snapshot ( ) self . Raise ( ) self . Show ( ) self . Refresh ( )
def CalculateBestPosition ( self , widget ) : if isinstance ( widget , wx . Frame ) : screen = wx . ClientDisplayRect ( ) [ 2 : ] left , top = widget . ClientToScreenXY ( 0 , 0 ) right , bottom = widget . ClientToScreenXY ( * widget . GetClientRect ( ) [ 2 : ] ) size = self . GetSize ( ) xpos = right ypos = bottom - size [ 1 ] self . SetPosition ( ( xpos , ypos ) ) else : STT . ToolTipWindow . CalculateBestPosition ( self , widget )
def GetPyData ( self , item ) : wx_data = self . GetItemData ( item ) py_data = self . _py_data_map . get ( wx_data ) return py_data
def SetPyData ( self , item , py_data ) : wx_data = wx . NewId ( ) self . SetItemData ( item , wx_data ) self . _py_data_map [ wx_data ] = py_data self . _wx_data_map [ py_data ] = wx_data return wx_data
def FindPyData ( self , start , py_data ) : wx_data = self . _wx_data_map [ py_data ] if wx . VERSION < ( 3 , 0 , 0 ) or 'classic' in wx . version ( ) : data = self . FindItemData ( start , wx_data ) else : data = self . FindItem ( start , wx_data ) return data
def DeleteItem ( self , item ) : wx_data = self . GetItemData ( item ) py_data = self . _py_data_map [ wx_data ] del self . _py_data_map [ wx_data ] del self . _wx_data_map [ py_data ] wx . ListCtrl . DeleteItem ( self , item )
def DeleteAllItems ( self ) : self . _py_data_map . clear ( ) self . _wx_data_map . clear ( ) wx . ListCtrl . DeleteAllItems ( self )
def delete ( self , a_position ) : key = self . wx_obj . GetPyData ( a_position ) del self . _items [ key ]
def clear_all ( self ) : self . clear ( ) for ch in reversed ( self . columns ) : del self [ ch . name ]
def clear ( self ) : dict . clear ( self ) self . _key = 0 if hasattr ( self . _list_view , "wx_obj" ) : self . _list_view . wx_obj . DeleteAllItems ( )
def _set_selection ( self , index , dummy = False ) : if index is None : self . wx_obj . SetSelection ( - 1 ) if hasattr ( self . wx_obj , "SetValue" ) : self . wx_obj . SetValue ( "" ) else : self . wx_obj . SetSelection ( index ) wx_event = ItemContainerControlSelectEvent ( self . _commandtype , index , self . wx_obj ) if hasattr ( self , "onchange" ) and self . onchange : event = FormEvent ( name = "change" , wx_event = wx_event ) self . onchange ( event )
def _get_string_selection ( self ) : if self . multiselect : return [ self . wx_obj . GetString ( i ) for i in self . wx_obj . GetSelections ( ) ] else : return self . wx_obj . GetStringSelection ( )
def set_data ( self , n , data ) : self . wx_obj . SetClientData ( n , data ) self . _items_dict [ data ] = self . get_string ( n )
def append ( self , a_string , data = None ) : self . wx_obj . Append ( a_string , data ) self . _items_dict [ data ] = a_string
def delete ( self , a_position ) : self . wx_obj . Delete ( a_position ) data = self . get_data ( ) if data in self . _items_dict : del self . _items_dict [ data ]
def represent ( obj , prefix , parent = "" , indent = 0 , context = False , max_cols = 80 ) : try : name = getattr ( obj , "name" , "" ) class_name = "%s.%s" % ( prefix , obj . __class__ . __name__ ) padding = len ( class_name ) + 1 + indent * 4 + ( 5 if context else 0 ) params = [ ] for ( k , spec ) in sorted ( obj . _meta . specs . items ( ) , key = get_sort_key ) : if k == "index" : continue if k == "parent" and parent != "" : v = parent else : v = getattr ( obj , k , "" ) if ( not isinstance ( spec , InternalSpec ) and v != spec . default and ( k != 'id' or v > 0 ) and isinstance ( v , ( basestring , int , long , float , bool , dict , list , decimal . Decimal , datetime . datetime , datetime . date , datetime . time , Font , Color ) ) and repr ( v ) != 'None' ) : v = repr ( v ) else : v = None if v is not None : params . append ( "%s=%s" % ( k , v ) ) param_lines = [ ] line = "" for param in params : if len ( line + param ) + 3 > max_cols - padding : param_lines . append ( line ) line = "" line += param + ", " param_lines . append ( line ) param_str = ( "\n%s" % ( " " * padding ) ) . join ( param_lines ) return "%s(%s)" % ( class_name , param_str ) except : raise return object . __repr__ ( obj )
def get ( obj_name , init = False ) : wx_parent = None if isinstance ( obj_name , basestring ) : obj_parent = COMPONENTS . get ( obj_name ) if not obj_parent : wx_parent = wx . FindWindowByName ( obj_name ) if wx_parent : obj_parent = getattr ( wx_parent , "obj" ) else : for obj in COMPONENTS . values ( ) : if obj . name == obj_name : obj_parent = obj else : obj_parent = obj_name return obj_parent or wx_parent
def duplicate ( self , new_parent = None ) : kwargs = { } for spec_name , spec in self . _meta . specs . items ( ) : value = getattr ( self , spec_name ) if isinstance ( value , Color ) : print "COLOR" , value , value . default if value . default : value = None if value is not None : kwargs [ spec_name ] = value del kwargs [ 'parent' ] new_id = wx . NewId ( ) kwargs [ 'id' ] = new_id kwargs [ 'name' ] = "%s_%s" % ( kwargs [ 'name' ] , new_id ) new_obj = self . __class__ ( new_parent or self . get_parent ( ) , * * kwargs ) for child in self : child . duplicate ( new_obj ) return new_obj
def _sizer_add ( self , child ) : if self . sizer : if DEBUG : print "adding to sizer:" , child . name border = None if not border : border = child . sizer_border flags = child . _sizer_flags if child . sizer_align : flags |= child . _sizer_align if child . sizer_expand : flags |= wx . EXPAND if 'grid' in self . sizer : self . _sizer . Add ( child . wx_obj , flag = flags , border = border , pos = ( child . sizer_row , child . sizer_col ) , span = ( child . sizer_rowspan , child . sizer_colspan ) ) else : self . _sizer . Add ( child . wx_obj , 0 , flags , border )
def set_parent ( self , new_parent , init = False ) : Component . set_parent ( self , new_parent , init ) if not init : if DEBUG : print "reparenting" , ctrl . name if hasattr ( self . wx_obj , "Reparent" ) : self . wx_obj . Reparent ( self . _parent . wx_obj )
def resize ( self , evt = None ) : if DEBUG : print "RESIZE!" , self . name , self . width , self . height if not isinstance ( self . wx_obj , wx . TopLevelWindow ) : if self . _left and self . _left [ - 1 ] == "%" or self . _top and self . _top [ - 1 ] == "%" : if DEBUG : print "MOVING" , self . name , self . _width self . _set_pos ( ( self . _left , self . _top ) ) if self . _width and self . _width [ - 1 ] == "%" or self . _height and self . _height [ - 1 ] == "%" : if DEBUG : print "RESIZING" , self . name , self . _width , self . _height self . _set_size ( ( self . _width , self . _height ) ) for child in self : if isinstance ( child , Control ) : child . resize ( evt ) if evt : evt . Skip ( )
def __tile_background ( self , dc ) : sz = self . wx_obj . GetClientSize ( ) bmp = self . _bitmap . get_bits ( ) w = bmp . GetWidth ( ) h = bmp . GetHeight ( ) if isinstance ( self , wx . ScrolledWindow ) : spx , spy = self . wx_obj . GetScrollPixelsPerUnit ( ) vsx , vsy = self . wx_obj . GetViewStart ( ) dx , dy = ( spx * vsx ) % w , ( spy * vsy ) % h else : dx , dy = ( w , h ) x = - dx while x < sz . width : y = - dy while y < sz . height : dc . DrawBitmap ( bmp , x , y ) y = y + h x = x + w
def __on_erase_background ( self , evt ) : if self . _bitmap : dc = evt . GetDC ( ) if not dc : dc = wx . ClientDC ( self ) r = self . wx_obj . GetUpdateRegion ( ) . GetBox ( ) dc . SetClippingRegion ( r . x , r . y , r . width , r . height ) if self . _background_tiling : self . __tile_background ( dc ) else : dc . DrawBitmapPoint ( self . _bitmap . get_bits ( ) , ( 0 , 0 ) )
def __on_paint ( self , event ) : dc = wx . GCDC ( wx . PaintDC ( self . wx_obj ) ) dc . SetFont ( self . wx_obj . GetFont ( ) ) dc . SetTextForeground ( self . wx_obj . GetForegroundColour ( ) ) dc . DrawText ( self . wx_obj . GetLabel ( ) , 0 , 0 )
def _get_column_headings ( self ) : headers = [ ctrl for ctrl in self if isinstance ( ctrl , GridColumn ) ] return sorted ( headers , key = lambda ch : ch . index )
def ResetView ( self , grid ) : grid . BeginBatch ( ) for current , new , delmsg , addmsg in [ ( self . _rows , self . GetNumberRows ( ) , gridlib . GRIDTABLE_NOTIFY_ROWS_DELETED , gridlib . GRIDTABLE_NOTIFY_ROWS_APPENDED ) , ( self . _cols , self . GetNumberCols ( ) , gridlib . GRIDTABLE_NOTIFY_COLS_DELETED , gridlib . GRIDTABLE_NOTIFY_COLS_APPENDED ) , ] : if new < current : msg = gridlib . GridTableMessage ( self , delmsg , new , current - new ) grid . ProcessTableMessage ( msg ) elif new > current : msg = gridlib . GridTableMessage ( self , addmsg , new - current ) grid . ProcessTableMessage ( msg ) self . UpdateValues ( grid ) grid . EndBatch ( ) self . _rows = self . GetNumberRows ( ) self . _cols = self . GetNumberCols ( ) self . _updateColAttrs ( grid ) grid . AdjustScrollbars ( ) grid . ForceRefresh ( )
def UpdateValues ( self , grid ) : msg = gridlib . GridTableMessage ( self , gridlib . GRIDTABLE_REQUEST_VIEW_GET_VALUES ) grid . ProcessTableMessage ( msg )
def _updateColAttrs ( self , grid ) : col = 0 for column in self . columns : attr = gridlib . GridCellAttr ( ) if False : attr . SetReadOnly ( ) if False : attr . SetRenderer ( renderer ) grid . SetColSize ( col , column . width ) grid . SetColAttr ( col , attr ) col += 1
def clear ( self ) : for i in range ( len ( self ) - 1 , - 1 , - 1 ) : del self [ i ] self . _key = 0 if hasattr ( self . _grid_view , "wx_obj" ) : self . _grid_view . wx_obj . ClearGrid ( )
def Create ( self , parent , id , evtHandler ) : self . _tc = wx . ComboBox ( parent , id , "" , ( 100 , 50 ) ) self . SetControl ( self . _tc ) self . _tc . PushEventHandler ( wx . EvtHandler ( ) ) self . _tc . Bind ( wx . EVT_COMBOBOX , self . OnChange )
def BeginEdit ( self , row , col , grid ) : self . startValue = grid . GetTable ( ) . GetValue ( row , col ) choices = grid . GetTable ( ) . columns [ col ] . _choices self . _tc . Clear ( ) self . _tc . AppendItems ( choices ) self . _tc . SetStringSelection ( self . startValue ) self . _tc . SetFocus ( )
def EndEdit ( self , row , col , grid , val = None ) : changed = False val = self . _tc . GetStringSelection ( ) print "val" , val , row , col , self . startValue if val != self . startValue : changed = True grid . GetTable ( ) . SetValue ( row , col , val ) self . startValue = '' self . _tc . SetStringSelection ( '' ) return changed
def IsAcceptedKey ( self , evt ) : return ( not ( evt . ControlDown ( ) or evt . AltDown ( ) ) and evt . GetKeyCode ( ) != wx . WXK_SHIFT )
def StartingKey ( self , evt ) : key = evt . GetKeyCode ( ) ch = None if key in [ wx . WXK_NUMPAD0 , wx . WXK_NUMPAD1 , wx . WXK_NUMPAD2 , wx . WXK_NUMPAD3 , wx . WXK_NUMPAD4 , wx . WXK_NUMPAD5 , wx . WXK_NUMPAD6 , wx . WXK_NUMPAD7 , wx . WXK_NUMPAD8 , wx . WXK_NUMPAD9 ] : ch = ch = chr ( ord ( '0' ) + key - wx . WXK_NUMPAD0 ) elif key < 256 and key >= 0 and chr ( key ) in string . printable : ch = chr ( key ) if not evt . ShiftDown ( ) : ch = ch . lower ( ) if ch is not None : self . _tc . SetStringSelection ( ch ) else : evt . Skip ( )
def Enable ( self , value ) : for i in range ( self . GetMenuItemCount ( ) ) : it = self . FindItemByPosition ( i ) it . Enable ( value )
def IsEnabled ( self , * args , * * kwargs ) : for i in range ( self . GetMenuItemCount ( ) ) : it = self . FindItemByPosition ( i ) if not it . IsEnabled ( ) : return False return True
def Enable ( self , value ) : for i in range ( self . GetMenuCount ( ) ) : self . EnableTop ( i , value )
def IsEnabled ( self , * args , * * kwargs ) : for i in range ( self . GetMenuCount ( ) ) : if not self . IsEnabledTop ( i ) : return False return True
def RemoveItem ( self , menu ) : menus = self . GetMenus ( ) menus = [ submenu for submenu in menus if submenu [ 0 ] != menu ] self . SetMenus ( menus )
def setObjectTag ( self , object , tag ) : object . _attributes = { } object . _name = tag . GetName ( ) . lower ( ) for name in self . attributes : object . _attributes [ "_%s" % name ] = tag . GetParam ( name ) if object . _attributes [ "_%s" % name ] == "" : object . _attributes [ "_%s" % name ] = None
def autosummary_table_visit_html ( self , node ) : try : tbody = node [ 0 ] [ 0 ] [ - 1 ] for row in tbody : col1_entry = row [ 0 ] par = col1_entry [ 0 ] for j , subnode in enumerate ( list ( par ) ) : if isinstance ( subnode , nodes . Text ) : new_text = unicode ( subnode . astext ( ) ) new_text = new_text . replace ( u" " , u"\u00a0" ) par [ j ] = nodes . Text ( new_text ) except IndexError : pass
def mangle_signature ( sig , max_chars = 30 ) : s = re . sub ( r"^\((.*)\)$" , r"\1" , sig ) . strip ( ) s = re . sub ( r"\\\\" , "" , s ) s = re . sub ( r"\\'" , "" , s ) s = re . sub ( r"'[^']*'" , "" , s ) args = [ ] opts = [ ] opt_re = re . compile ( r"^(.*, |)([a-zA-Z0-9_*]+)=" ) while s : m = opt_re . search ( s ) if not m : args = s . split ( ', ' ) break opts . insert ( 0 , m . group ( 2 ) ) s = m . group ( 1 ) [ : - 2 ] sig = limited_join ( ", " , args , max_chars = max_chars - 2 ) if opts : if not sig : sig = "[%s]" % limited_join ( ", " , opts , max_chars = max_chars - 4 ) elif len ( sig ) < max_chars - 4 - 2 - 3 : sig += "[, %s]" % limited_join ( ", " , opts , max_chars = max_chars - len ( sig ) - 4 - 2 ) return u"(%s)" % sig
def _import_by_name ( name ) : try : name_parts = name . split ( '.' ) modname = '.' . join ( name_parts [ : - 1 ] ) if modname : try : __import__ ( modname ) mod = sys . modules [ modname ] return getattr ( mod , name_parts [ - 1 ] ) , mod except ( ImportError , IndexError , AttributeError ) : pass last_j = 0 modname = None for j in reversed ( range ( 1 , len ( name_parts ) + 1 ) ) : last_j = j modname = '.' . join ( name_parts [ : j ] ) try : __import__ ( modname ) except : continue if modname in sys . modules : break if last_j < len ( name_parts ) : parent = None obj = sys . modules [ modname ] for obj_name in name_parts [ last_j : ] : parent = obj obj = getattr ( obj , obj_name ) return obj , parent else : return sys . modules [ modname ] , None except ( ValueError , ImportError , AttributeError , KeyError ) , e : raise ImportError ( * e . args )
def alert ( message , title = "" , parent = None , scrolled = False , icon = "exclamation" ) : if not scrolled : icons = { 'exclamation' : wx . ICON_EXCLAMATION , 'error' : wx . ICON_ERROR , 'question' : wx . ICON_QUESTION , 'info' : wx . ICON_INFORMATION } style = wx . OK | icons [ icon ] result = dialogs . messageDialog ( parent , message , title , style ) else : result = dialogs . scrolledMessageDialog ( parent , message , title )
def prompt ( message = "" , title = "" , default = "" , multiline = False , password = None , parent = None ) : if password : style = wx . TE_PASSWORD | wx . OK | wx . CANCEL result = dialogs . textEntryDialog ( parent , message , title , default , style ) elif multiline : style = wx . TE_MULTILINE | wx . OK | wx . CANCEL result = dialogs . textEntryDialog ( parent , message , title , default , style ) result . text = '\n' . join ( result . text . splitlines ( ) ) else : result = dialogs . textEntryDialog ( parent , message , title , default ) if result . accepted : return result . text
def select_font ( message = "" , title = "" , font = None , parent = None ) : if font is not None : wx_font = font . _get_wx_font ( ) else : wx_font = None font = Font ( ) result = dialogs . fontDialog ( parent , font = wx_font ) if result . accepted : font_data = result . fontData result . color = result . fontData . GetColour ( ) . Get ( ) wx_font = result . fontData . GetChosenFont ( ) font . set_wx_font ( wx_font ) wx_font = None return font
def select_color ( message = "" , title = "" , color = None , parent = None ) : result = dialogs . colorDialog ( parent , color = color ) return result . accepted and result . color
def choose_directory ( message = 'Choose a directory' , path = "" , parent = None ) : result = dialogs . directoryDialog ( parent , message , path ) return result . path
def find ( default = '' , whole_words = 0 , case_sensitive = 0 , parent = None ) : result = dialogs . findDialog ( parent , default , whole_words , case_sensitive ) return { 'text' : result . searchText , 'whole_words' : result . wholeWordsOnly , 'case_sensitive' : result . caseSensitive }
def clear ( self ) : dict . clear ( self ) self . _key = 0 if hasattr ( self . _tree_view , "wx_obj" ) : self . _tree_view . wx_obj . DeleteAllItems ( )
def set_has_children ( self , has_children = True ) : self . _tree_model . _tree_view . wx_obj . SetItemHasChildren ( self . wx_item , has_children )
def _set_icon ( self , icon = None ) : if icon is not None : try : wx_icon = wx . Icon ( icon , wx . BITMAP_TYPE_ICO ) self . wx_obj . SetIcon ( wx_icon ) except : pass
def show ( self , value = True , modal = None ) : self . wx_obj . Show ( value ) if modal : disabler = wx . WindowDisabler ( self . wx_obj ) eventloop = wx . EventLoop ( ) def on_close_modal ( evt ) : evt . Skip ( ) eventloop . Exit ( ) self . wx_obj . Bind ( wx . EVT_CLOSE , on_close_modal ) eventloop . Run ( ) del disabler
def resize ( self , evt = None ) : for child in self : if isinstance ( child , Control ) : child . resize ( evt ) if evt : evt . Skip ( )
def parse ( filename = "" ) : s = open ( filename ) . read ( ) ##s.decode("latin1").encode("utf8") import datetime , decimal rsrc = eval ( s ) return rsrc
def save ( filename , rsrc ) : s = pprint . pformat ( rsrc ) open ( filename , "w" ) . write ( s )
def build_window ( res ) : kwargs = dict ( res . items ( ) ) wintype = kwargs . pop ( 'type' ) menubar = kwargs . pop ( 'menubar' , None ) components = kwargs . pop ( 'components' ) panel = kwargs . pop ( 'panel' , { } ) from gui import registry import gui winclass = registry . WINDOWS [ wintype ] win = winclass ( * * kwargs ) if False and panel is not None : panel [ 'name' ] = 'panel' p = gui . Panel ( win , * * panel ) else : p = win if components : for comp in components : build_component ( comp , parent = p ) if menubar : mb = gui . MenuBar ( name = "menu" , parent = win ) for menu in menubar : build_component ( menu , parent = mb ) return win
def build_component ( res , parent = None ) : kwargs = dict ( res . items ( ) ) comtype = kwargs . pop ( 'type' ) if 'components' in res : components = kwargs . pop ( 'components' ) elif comtype == 'Menu' and 'items' in res : components = kwargs . pop ( 'items' ) else : components = [ ] from gui import registry if comtype in registry . CONTROLS : comclass = registry . CONTROLS [ comtype ] elif comtype in registry . MENU : comclass = registry . MENU [ comtype ] elif comtype in registry . MISC : comclass = registry . MISC [ comtype ] else : raise RuntimeError ( "%s not in registry" % comtype ) com = comclass ( parent = parent , * * kwargs ) for comp in components : build_component ( comp , parent = com ) return com
def convert ( self , name ) : new_name = PYTHONCARD_PROPERTY_MAP . get ( name ) if new_name : print "WARNING: property %s should be %s (%s)" % ( name , new_name , self . obj . name ) return new_name else : return name
def set_data ( data ) : try : if wx . TheClipboard . Open ( ) : if isinstance ( data , ( str , unicode ) ) : do = wx . TextDataObject ( ) do . SetText ( data ) wx . TheClipboard . SetData ( do ) elif isinstance ( data , wx . Bitmap ) : do = wx . BitmapDataObject ( ) do . SetBitmap ( data ) wx . TheClipboard . SetData ( do ) wx . TheClipboard . Close ( ) except : pass
def load_object ( self , obj = None ) : if obj : self . root_obj = obj else : obj = self . root_obj self . tree . DeleteAllItems ( ) self . root = self . tree . AddRoot ( "application" ) self . tree . SetItemText ( self . root , "App" , 1 ) self . tree . SetItemText ( self . root , "col 2 root" , 2 ) #self.tree.SetItemImage(self.root, fldridx, which = wx.TreeItemIcon_Normal) #self.tree.SetItemImage(self.root, fldropenidx, which = wx.TreeItemIcon_Expanded) self . build_tree ( self . root , obj ) self . tree . Expand ( self . root )
def inspect ( self , obj , context_menu = False , edit_prop = False , mouse_pos = None ) : child = self . tree . FindItem ( self . root , obj . name ) if DEBUG : print "inspect child" , child if child : self . tree . ScrollTo ( child ) self . tree . SetCurrentItem ( child ) self . tree . SelectItem ( child ) child . Selected = True self . activate_item ( child , edit_prop ) if context_menu : self . show_context_menu ( child , mouse_pos )
def activate_item ( self , child , edit_prop = False , select = False ) : d = self . tree . GetItemData ( child ) if d : o = d . GetData ( ) self . selected_obj = o callback = lambda o = o , * * kwargs : self . update ( o , * * kwargs ) self . propeditor . load_object ( o , callback ) if edit_prop : wx . CallAfter ( self . propeditor . edit ) if select and self . designer : self . designer . select ( o ) else : self . selected_obj = None
def update ( self , obj , * * kwargs ) : child = self . tree . FindItem ( self . root , kwargs [ 'name' ] ) if DEBUG : print "update child" , child , kwargs if child : self . tree . ScrollTo ( child ) self . tree . SetCurrentItem ( child ) self . tree . SelectItem ( child ) child . Selected = True self . tree . SetItemText ( child , obj . name , 0 )
def show_context_menu ( self , item , mouse_pos = None ) : if item : d = self . tree . GetItemData ( item ) if d : obj = d . GetData ( ) if obj : self . highlight ( obj . wx_obj ) self . obj = obj menu = wx . Menu ( ) id_del , id_dup , id_raise , id_lower = [ wx . NewId ( ) for i in range ( 4 ) ] menu . Append ( id_del , "Delete" ) menu . Append ( id_dup , "Duplicate" ) menu . Append ( id_raise , "Bring to Front" ) menu . Append ( id_lower , "Send to Back" ) sm = wx . Menu ( ) for ctrl in sorted ( obj . _meta . valid_children , key = lambda c : registry . ALL . index ( c . _meta . name ) ) : new_id = wx . NewId ( ) sm . Append ( new_id , ctrl . _meta . name ) self . Bind ( wx . EVT_MENU , lambda evt , ctrl = ctrl : self . add_child ( ctrl , mouse_pos ) , id = new_id ) menu . AppendMenu ( wx . NewId ( ) , "Add child" , sm ) self . Bind ( wx . EVT_MENU , self . delete , id = id_del ) self . Bind ( wx . EVT_MENU , self . duplicate , id = id_dup ) self . Bind ( wx . EVT_MENU , self . bring_to_front , id = id_raise ) self . Bind ( wx . EVT_MENU , self . send_to_back , id = id_lower ) self . PopupMenu ( menu ) menu . Destroy ( ) self . load_object ( self . root_obj )
def select_option ( self ) : if self . disabled : warn ( "Attempt to select disabled option: {}" . format ( self . value or self . text ) ) self . base . select_option ( )
def raise_server_error ( self ) : if self . server and self . server . error : try : if capybara . raise_server_errors : raise self . server . error finally : self . server . reset_error ( )
def __traceback ( self ) -> str : if not self . log_traceback : return "" exc_info = sys . exc_info ( ) stack = traceback . extract_stack ( ) exc_tb = traceback . extract_tb ( exc_info [ 2 ] ) full_tb = stack [ : 1 ] + exc_tb exc_line : typing . List [ str ] = traceback . format_exception_only ( * exc_info [ : 2 ] ) tb_text = "\nTraceback (most recent call last):\n" + "" . join ( traceback . format_list ( full_tb ) ) + "" . join ( exc_line ) return tb_text
def __get_obj_source ( self , instance : typing . Any , owner : typing . Optional [ type ] = None ) -> str : if self . log_object_repr : return f"{instance!r}" return f"<{owner.__name__ if owner is not None else instance.__class__.__name__}() at 0x{id(instance):X}>"
def logger ( self , logger : typing . Union [ logging . Logger , str , None ] ) -> None : if logger is None or isinstance ( logger , logging . Logger ) : self . __logger = logger else : self . __logger = logging . getLogger ( logger )
def channels ( self ) : if not self . _channels : self . _channels = self . _call_api ( 'channels.list' ) [ 'channels' ] return self . _channels
def users ( self ) : if not self . _users : self . _users = self . _call_api ( 'users.list' ) [ 'members' ] return self . _users
def channel_from_name ( self , name ) : try : channel = [ channel for channel in self . channels if channel [ 'name' ] == name ] [ 0 ] except IndexError : raise ValueError ( 'Unknown channel for name: "{}"' . format ( name ) ) return channel
def translate ( self , message ) : try : user_id = message . pop ( 'user' ) user = self . slack . user_from_id ( user_id ) message [ u'user' ] = user [ 'name' ] except ( KeyError , IndexError , ValueError ) : pass try : if type ( message [ 'channel' ] ) == str : channel_id = message . pop ( 'channel' ) else : channel_id = message . pop ( 'channel' ) [ 'id' ] self . slack . reload_channels ( ) channel = self . slack . channel_from_id ( channel_id ) message [ u'channel' ] = channel [ 'name' ] except ( KeyError , IndexError , ValueError ) : pass return message
def sendSlack ( self , message ) : channel = message . get ( 'channel' , 'general' ) self . sendMessage ( self . make_message ( message [ 'text' ] , channel ) )
def read_channel ( self ) : channel , message = self . protocol . channel_layer . receive_many ( [ u'slack.send' ] , block = False ) delay = 0.1 if channel : self . protocols [ 0 ] . sendSlack ( message ) reactor . callLater ( delay , self . read_channel )
def run ( self , args ) : args = self . parser . parse_args ( args ) if not args . token : raise ValueError ( 'Supply the slack token through --token or setting DJANGOBOT_TOKEN' ) sys . path . insert ( 0 , "." ) module_path , object_path = args . channel_layer . split ( ':' , 1 ) channel_layer = importlib . import_module ( module_path ) for part in object_path . split ( '.' ) : channel_layer = getattr ( channel_layer , part ) Client ( channel_layer = channel_layer , token = args . token , ) . run ( )
def dict_diff ( prv , nxt ) : keys = set ( prv . keys ( ) + nxt . keys ( ) ) result = { } for k in keys : if prv . get ( k ) != nxt . get ( k ) : result [ k ] = ( prv . get ( k ) , nxt . get ( k ) ) return result
def colorize ( msg , color ) : if DONT_COLORIZE : return msg else : return "{}{}{}" . format ( COLORS [ color ] , msg , COLORS [ "endc" ] )
def v2_playbook_on_task_start ( self , task , * * kwargs ) : self . last_task_name = task . get_name ( ) self . printed_last_task = False
def v2_runner_on_ok ( self , result , * * kwargs ) : failed = "failed" in result . _result unreachable = "unreachable" in result . _result if ( "print_action" in result . _task . tags or failed or unreachable or self . _display . verbosity > 1 ) : self . _print_task ( ) self . last_skipped = False msg = unicode ( result . _result . get ( "msg" , "" ) ) or unicode ( result . _result . get ( "reason" , "" ) ) or unicode ( result . _result . get ( "message" , "" ) ) stderr = [ result . _result . get ( "exception" , None ) , result . _result . get ( "module_stderr" , None ) , ] stderr = "\n" . join ( [ e for e in stderr if e ] ) . strip ( ) self . _print_host_or_item ( result . _host , result . _result . get ( "changed" , False ) , msg , result . _result . get ( "diff" , None ) , is_host = True , error = failed or unreachable , stdout = result . _result . get ( "module_stdout" , None ) , stderr = stderr . strip ( ) , ) if "results" in result . _result : for r in result . _result [ "results" ] : failed = "failed" in r stderr = [ r . get ( "exception" , None ) , r . get ( "module_stderr" , None ) ] stderr = "\n" . join ( [ e for e in stderr if e ] ) . strip ( ) self . _print_host_or_item ( r [ "item" ] , r . get ( "changed" , False ) , unicode ( r . get ( "msg" , "" ) ) , r . get ( "diff" , None ) , is_host = False , error = failed , stdout = r . get ( "module_stdout" , None ) , stderr = stderr . strip ( ) , ) else : self . last_skipped = True print ( "." , end = "" )
def v2_playbook_on_stats ( self , stats ) : print ( ) self . printed_last_task = False self . _print_task ( "STATS" ) hosts = sorted ( stats . processed . keys ( ) ) for host in hosts : s = stats . summarize ( host ) if s [ "failures" ] or s [ "unreachable" ] : color = "failed" elif s [ "changed" ] : color = "changed" else : color = "ok" msg = "{}    : ok={}\tchanged={}\tfailed={}\tunreachable={}" . format ( host , s [ "ok" ] , s [ "changed" ] , s [ "failures" ] , s [ "unreachable" ] ) print ( colorize ( msg , color ) )
def v2_runner_on_skipped ( self , result , * * kwargs ) : if self . _display . verbosity > 1 : self . _print_task ( ) self . last_skipped = False line_length = 120 spaces = " " * ( 31 - len ( result . _host . name ) - 4 ) line = "  * {}{}- {}" . format ( colorize ( result . _host . name , "not_so_bold" ) , spaces , colorize ( "skipped" , "skipped" ) , ) reason = result . _result . get ( "skipped_reason" , "" ) or result . _result . get ( "skip_reason" , "" ) if len ( reason ) < 50 : line += " -- {}" . format ( reason ) print ( "{} {}---------" . format ( line , "-" * ( line_length - len ( line ) ) ) ) else : print ( "{} {}" . format ( line , "-" * ( line_length - len ( line ) ) ) ) print ( self . _indent_text ( reason , 8 ) ) print ( reason )
def load_filters ( ) : all_filters = { } for m in JINJA_FILTERS : if hasattr ( m , "filters" ) : all_filters . update ( m . filters ( ) ) return all_filters
def get_authorization ( self ) : auth = self . authorization_class ( ) header = self . get_authorization_header ( ) if not header or not header . split : return auth header = header . split ( ) if len ( header ) > 1 and header [ 0 ] == 'Bearer' : auth . is_oauth = True access_token = header [ 1 ] self . validate_access_token ( access_token , auth ) if not auth . is_valid : auth . error = 'access_denied' return auth
def open ( self , bus ) : if self . _device is not None : self . close ( ) self . _device = open ( '/dev/i2c-{0}' . format ( bus ) , 'r+b' , buffering = 0 )
def read_byte ( self , addr ) : assert self . _device is not None , 'Bus must be opened before operations are made against it!' self . _select_device ( addr ) return ord ( self . _device . read ( 1 ) )
def read_bytes ( self , addr , number ) : assert self . _device is not None , 'Bus must be opened before operations are made against it!' self . _select_device ( addr ) return self . _device . read ( number )
def read_byte_data ( self , addr , cmd ) : assert self . _device is not None , 'Bus must be opened before operations are made against it!' reg = c_uint8 ( cmd ) result = c_uint8 ( ) request = make_i2c_rdwr_data ( [ ( addr , 0 , 1 , pointer ( reg ) ) , ( addr , I2C_M_RD , 1 , pointer ( result ) ) ] ) ioctl ( self . _device . fileno ( ) , I2C_RDWR , request ) return result . value
def write_quick ( self , addr ) : assert self . _device is not None , 'Bus must be opened before operations are made against it!' request = make_i2c_rdwr_data ( [ ( addr , 0 , 0 , None ) , ] ) ioctl ( self . _device . fileno ( ) , I2C_RDWR , request )
def write_byte ( self , addr , val ) : assert self . _device is not None , 'Bus must be opened before operations are made against it!' self . _select_device ( addr ) data = bytearray ( 1 ) data [ 0 ] = val & 0xFF self . _device . write ( data )
def write_bytes ( self , addr , buf ) : assert self . _device is not None , 'Bus must be opened before operations are made against it!' self . _select_device ( addr ) self . _device . write ( buf )
def write_byte_data ( self , addr , cmd , val ) : assert self . _device is not None , 'Bus must be opened before operations are made against it!' data = bytearray ( 2 ) data [ 0 ] = cmd & 0xFF data [ 1 ] = val & 0xFF self . _select_device ( addr ) self . _device . write ( data )
def write_i2c_block_data ( self , addr , cmd , vals ) : assert self . _device is not None , 'Bus must be opened before operations are made against it!' data = bytearray ( len ( vals ) + 1 ) data [ 0 ] = cmd & 0xFF data [ 1 : ] = vals [ 0 : ] self . _select_device ( addr ) self . _device . write ( data )
def datetime_created ( self ) : if self . info ( ) . get ( 'datetime_created' ) : return dateutil . parser . parse ( self . info ( ) [ 'datetime_created' ] )
def construct_from ( cls , group_info ) : group = cls ( group_info [ 'id' ] ) group . _info_cache = group_info return group
def _base_opration ( self , method ) : uuids = self . uuids ( ) while True : chunk = list ( islice ( uuids , 0 , self . chunk_size ) ) if not chunk : return rest_request ( method , self . storage_url , chunk )
def uuids ( self ) : for f in self . _seq : if isinstance ( f , File ) : yield f . uuid elif isinstance ( f , six . string_types ) : yield f else : raise ValueError ( 'Invalid type for sequence item: {0}' . format ( type ( f ) ) )
def _list ( api_list_class , arg_namespace , * * extra ) : if arg_namespace . starting_point : ordering_field = ( arg_namespace . ordering or '' ) . lstrip ( '-' ) if ordering_field in ( '' , 'datetime_uploaded' , 'datetime_created' ) : arg_namespace . starting_point = parser . parse ( arg_namespace . starting_point ) items = api_list_class ( starting_point = arg_namespace . starting_point , ordering = arg_namespace . ordering , limit = arg_namespace . limit , request_limit = arg_namespace . request_limit , * * extra ) items . constructor = lambda x : x try : pprint ( list ( items ) ) except ValueError as e : print ( e )
def bar ( iter_content , parts , title = '' ) : parts = max ( float ( parts ) , 1.0 ) cells = 10 progress = 0 step = cells / parts draw = lambda progress : sys . stdout . write ( '\r[{0:10}] {1:.2f}% {2}' . format ( '#' * int ( progress ) , progress * cells , title ) ) for chunk in iter_content : yield chunk progress += step draw ( progress ) sys . stdout . flush ( ) draw ( cells ) print ( '' )
def home_mode_set_state ( self , state , * * kwargs ) : if state not in ( HOME_MODE_ON , HOME_MODE_OFF ) : raise ValueError ( 'Invalid home mode state' ) api = self . _api_info [ 'home_mode' ] payload = dict ( { 'api' : api [ 'name' ] , 'method' : 'Switch' , 'version' : api [ 'version' ] , 'on' : state , '_sid' : self . _sid , } , * * kwargs ) response = self . _get_json_with_retry ( api [ 'url' ] , payload ) if response [ 'success' ] : return True return False
def home_mode_status ( self , * * kwargs ) : api = self . _api_info [ 'home_mode' ] payload = dict ( { 'api' : api [ 'name' ] , 'method' : 'GetInfo' , 'version' : api [ 'version' ] , '_sid' : self . _sid } , * * kwargs ) response = self . _get_json_with_retry ( api [ 'url' ] , payload ) return response [ 'data' ] [ 'on' ]
def camera_list ( self , * * kwargs ) : api = self . _api_info [ 'camera' ] payload = dict ( { '_sid' : self . _sid , 'api' : api [ 'name' ] , 'method' : 'List' , 'version' : api [ 'version' ] , } , * * kwargs ) response = self . _get_json_with_retry ( api [ 'url' ] , payload ) cameras = [ ] for data in response [ 'data' ] [ 'cameras' ] : cameras . append ( Camera ( data , self . _video_stream_url ) ) return cameras
def camera_info ( self , camera_ids , * * kwargs ) : api = self . _api_info [ 'camera' ] payload = dict ( { '_sid' : self . _sid , 'api' : api [ 'name' ] , 'method' : 'GetInfo' , 'version' : api [ 'version' ] , 'cameraIds' : ', ' . join ( str ( id ) for id in camera_ids ) , } , * * kwargs ) response = self . _get_json_with_retry ( api [ 'url' ] , payload ) cameras = [ ] for data in response [ 'data' ] [ 'cameras' ] : cameras . append ( Camera ( data , self . _video_stream_url ) ) return cameras
def camera_snapshot ( self , camera_id , * * kwargs ) : api = self . _api_info [ 'camera' ] payload = dict ( { '_sid' : self . _sid , 'api' : api [ 'name' ] , 'method' : 'GetSnapshot' , 'version' : api [ 'version' ] , 'cameraId' : camera_id , } , * * kwargs ) response = self . _get ( api [ 'url' ] , payload ) return response . content
def camera_event_motion_enum ( self , camera_id , * * kwargs ) : api = self . _api_info [ 'camera_event' ] payload = dict ( { '_sid' : self . _sid , 'api' : api [ 'name' ] , 'method' : 'MotionEnum' , 'version' : api [ 'version' ] , 'camId' : camera_id , } , * * kwargs ) response = self . _get_json_with_retry ( api [ 'url' ] , payload ) return MotionSetting ( camera_id , response [ 'data' ] [ 'MDParam' ] )
def camera_event_md_param_save ( self , camera_id , * * kwargs ) : api = self . _api_info [ 'camera_event' ] payload = dict ( { '_sid' : self . _sid , 'api' : api [ 'name' ] , 'method' : 'MDParamSave' , 'version' : api [ 'version' ] , 'camId' : camera_id , } , * * kwargs ) response = self . _get_json_with_retry ( api [ 'url' ] , payload ) return response [ 'data' ] [ 'camId' ]
def update ( self ) : cameras = self . _api . camera_list ( ) self . _cameras_by_id = { v . camera_id : v for i , v in enumerate ( cameras ) } motion_settings = [ ] for camera_id in self . _cameras_by_id . keys ( ) : motion_setting = self . _api . camera_event_motion_enum ( camera_id ) motion_settings . append ( motion_setting ) self . _motion_settings_by_id = { v . camera_id : v for i , v in enumerate ( motion_settings ) }
def set_home_mode ( self , state ) : state_parameter = HOME_MODE_OFF if state : state_parameter = HOME_MODE_ON return self . _api . home_mode_set_state ( state_parameter )
def is_last_li ( li , meta_data , current_numId ) : if not is_li ( li , meta_data ) : return False w_namespace = get_namespace ( li , 'w' ) next_el = li while True : if next_el is None : return True next_el = next_el . getnext ( ) if not is_li ( next_el , meta_data ) : continue new_numId = get_numId ( next_el , w_namespace ) if current_numId != new_numId : return True return False
def get_single_list_nodes_data ( li , meta_data ) : yield li w_namespace = get_namespace ( li , 'w' ) current_numId = get_numId ( li , w_namespace ) starting_ilvl = get_ilvl ( li , w_namespace ) el = li while True : el = el . getnext ( ) if el is None : break if not has_text ( el ) : continue if _is_top_level_upper_roman ( el , meta_data ) : break if ( is_li ( el , meta_data ) and ( starting_ilvl > get_ilvl ( el , w_namespace ) ) ) : break new_numId = get_numId ( el , w_namespace ) if new_numId is None or new_numId == - 1 : yield el continue if current_numId != new_numId : break if is_last_li ( el , meta_data , current_numId ) : yield el break yield el
def is_bold ( r ) : w_namespace = get_namespace ( r , 'w' ) rpr = r . find ( '%srPr' % w_namespace ) if rpr is None : return False bold = rpr . find ( '%sb' % w_namespace ) return style_is_false ( bold )
def build_tr ( tr , meta_data , row_spans ) : tr_el = etree . Element ( 'tr' ) w_namespace = get_namespace ( tr , 'w' ) visited_nodes = [ ] for el in tr : if el in visited_nodes : continue visited_nodes . append ( el ) if el . tag == '%stc' % w_namespace : v_merge = get_v_merge ( el ) if ( v_merge is not None and v_merge . get ( '%sval' % w_namespace ) != 'restart' ) : continue texts = [ ] for td_content in el : if td_content in visited_nodes : continue if is_li ( td_content , meta_data ) : li_nodes = get_single_list_nodes_data ( td_content , meta_data , ) list_el , list_visited_nodes = build_list ( li_nodes , meta_data , ) visited_nodes . extend ( list_visited_nodes ) texts . append ( etree . tostring ( list_el ) ) elif td_content . tag == '%stbl' % w_namespace : table_el , table_visited_nodes = build_table ( td_content , meta_data , ) visited_nodes . extend ( table_visited_nodes ) texts . append ( etree . tostring ( table_el ) ) elif td_content . tag == '%stcPr' % w_namespace : visited_nodes . append ( td_content ) continue else : text = get_element_content ( td_content , meta_data , is_td = True , ) texts . append ( text ) data = '<br />' . join ( t for t in texts if t is not None ) td_el = etree . XML ( '<td>%s</td>' % data ) colspan = get_grid_span ( el ) if colspan > 1 : td_el . set ( 'colspan' , '%d' % colspan ) v_merge = get_v_merge ( el ) if ( v_merge is not None and v_merge . get ( '%sval' % w_namespace ) == 'restart' ) : rowspan = next ( row_spans ) td_el . set ( 'rowspan' , '%d' % rowspan ) tr_el . append ( td_el ) return tr_el
def build_table ( table , meta_data ) : table_el = etree . Element ( 'table' ) w_namespace = get_namespace ( table , 'w' ) row_spans = get_rowspan_data ( table ) for el in table : if el . tag == '%str' % w_namespace : tr_el = build_tr ( el , meta_data , row_spans , ) table_el . append ( tr_el ) visited_nodes = list ( table . iter ( ) ) return table_el , visited_nodes
def get_t_tag_content ( t , parent , remove_bold , remove_italics , meta_data ) : if t is None or t . text is None : return '' text = cgi . escape ( t . text ) el_is_bold = not remove_bold and ( is_bold ( parent ) or is_underlined ( parent ) ) el_is_italics = not remove_italics and is_italics ( parent ) if el_is_bold : text = '<strong>%s</strong>' % text if el_is_italics : text = '<em>%s</em>' % text return text
def _strip_tag ( tree , tag ) : for el in tree . iter ( ) : if el . tag == tag : el . getparent ( ) . remove ( el )
def find ( dataset , url ) : fn = os . path . join ( DATASETS , dataset ) dn = os . path . dirname ( fn ) if not os . path . exists ( dn ) : print ( 'creating dataset directory: %s' , dn ) os . makedirs ( dn ) if not os . path . exists ( fn ) : if sys . version_info < ( 3 , ) : urllib . urlretrieve ( url , fn ) else : urllib . request . urlretrieve ( url , fn ) return fn
def load_mnist ( flatten = True , labels = False ) : fn = find ( 'mnist.pkl.gz' , 'http://deeplearning.net/data/mnist/mnist.pkl.gz' ) h = gzip . open ( fn , 'rb' ) if sys . version_info < ( 3 , ) : ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h ) else : ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h , encoding = 'bytes' ) h . close ( ) if not flatten : timg = timg . reshape ( ( - 1 , 28 , 28 , 1 ) ) vimg = vimg . reshape ( ( - 1 , 28 , 28 , 1 ) ) simg = simg . reshape ( ( - 1 , 28 , 28 , 1 ) ) if labels : return ( ( timg , tlab . astype ( 'i' ) ) , ( vimg , vlab . astype ( 'i' ) ) , ( simg , slab . astype ( 'i' ) ) ) return ( timg , ) , ( vimg , ) , ( simg , )
def load_cifar ( flatten = True , labels = False ) : def extract ( name ) : print ( 'extracting data from {}' . format ( name ) ) h = tar . extractfile ( name ) if sys . version_info < ( 3 , ) : d = pickle . load ( h ) else : d = pickle . load ( h , encoding = 'bytes' ) for k in list ( d ) : d [ k . decode ( 'utf8' ) ] = d [ k ] h . close ( ) img = d [ 'data' ] . reshape ( ( - 1 , 3 , 32 , 32 ) ) . transpose ( ( 0 , 2 , 3 , 1 ) ) . astype ( 'f' ) / 128 - 1 if flatten : img = img . reshape ( ( - 1 , 32 * 32 * 3 ) ) d [ 'data' ] = img return d fn = find ( 'cifar10.tar.gz' , 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz' ) tar = tarfile . open ( fn ) imgs = [ ] labs = [ ] for i in range ( 1 , 6 ) : d = extract ( 'cifar-10-batches-py/data_batch_{}' . format ( i ) ) imgs . extend ( d [ 'data' ] ) labs . extend ( d [ 'labels' ] ) timg = np . asarray ( imgs [ : 40000 ] ) tlab = np . asarray ( labs [ : 40000 ] , 'i' ) vimg = np . asarray ( imgs [ 40000 : ] ) vlab = np . asarray ( labs [ 40000 : ] , 'i' ) d = extract ( 'cifar-10-batches-py/test_batch' ) simg = d [ 'data' ] slab = d [ 'labels' ] tar . close ( ) if labels : return ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) return ( timg , ) , ( vimg , ) , ( simg , )
def plot_layers ( weights , tied_weights = False , channels = 1 ) : if hasattr ( weights [ 0 ] , 'get_value' ) : weights = [ w . get_value ( ) for w in weights ] k = min ( len ( weights ) , 9 ) imgs = np . eye ( weights [ 0 ] . shape [ 0 ] ) for i , weight in enumerate ( weights [ : - 1 ] ) : imgs = np . dot ( weight . T , imgs ) plot_images ( imgs , 100 + 10 * k + i + 1 , channels = channels , title = 'Layer {}' . format ( i + 1 ) ) weight = weights [ - 1 ] n = weight . shape [ 1 ] / channels if int ( np . sqrt ( n ) ) ** 2 != n : return if tied_weights : imgs = np . dot ( weight . T , imgs ) plot_images ( imgs , 100 + 10 * k + k , channels = channels , title = 'Layer {}' . format ( k ) ) else : plot_images ( weight , 100 + 10 * k + k , channels = channels , title = 'Decoding weights' )
def plot_filters ( filters ) : imgs = filters . get_value ( ) N , channels , x , y = imgs . shape n = int ( np . sqrt ( N ) ) assert n * n == N , 'filters must contain a square number of rows!' assert channels == 1 or channels == 3 , 'can only plot grayscale or rgb filters!' img = np . zeros ( ( ( y + 1 ) * n - 1 , ( x + 1 ) * n - 1 , channels ) , dtype = imgs [ 0 ] . dtype ) for i , pix in enumerate ( imgs ) : r , c = divmod ( i , n ) img [ r * ( y + 1 ) : ( r + 1 ) * ( y + 1 ) - 1 , c * ( x + 1 ) : ( c + 1 ) * ( x + 1 ) - 1 ] = pix . transpose ( ( 1 , 2 , 0 ) ) img -= img . min ( ) img /= img . max ( ) ax = plt . gcf ( ) . add_subplot ( 111 ) ax . xaxis . set_visible ( False ) ax . yaxis . set_visible ( False ) ax . set_frame_on ( False ) ax . imshow ( img . squeeze ( ) , cmap = plt . cm . gray )
def batches ( dataset ) : seq_lengths = dataset . variables [ 'seqLengths' ] . data seq_begins = np . concatenate ( ( [ 0 ] , np . cumsum ( seq_lengths ) [ : - 1 ] ) ) def sample ( ) : chosen = np . random . choice ( list ( range ( len ( seq_lengths ) ) ) , BATCH_SIZE , replace = False ) return batch_at ( dataset . variables [ 'inputs' ] . data , dataset . variables [ 'targetClasses' ] . data , seq_begins [ chosen ] , seq_lengths [ chosen ] ) return sample
def variables ( self ) : result = [ self . _target ] if self . _weights is not None : result . append ( self . _weights ) return result
def reservoir ( xs , n , rng ) : pool = [ ] for i , x in enumerate ( xs ) : if len ( pool ) < n : pool . append ( x / np . linalg . norm ( x ) ) continue j = rng . randint ( i + 1 ) if j < n : pool [ j ] = x / np . linalg . norm ( x ) L = len ( pool ) S = np . std ( pool , axis = 0 ) while len ( pool ) < n : x = pool [ rng . randint ( L ) ] pool . append ( x + S * rng . randn ( * x . shape ) ) return np . array ( pool , dtype = pool [ 0 ] . dtype )
def inputs ( self ) : return [ l . input for l in self . layers if isinstance ( l , layers . Input ) ]
def variables ( self ) : result = self . inputs seen = set ( i . name for i in result ) for loss in self . losses : for v in loss . variables : if v . name not in seen : result . append ( v ) seen . add ( v . name ) return result
def output_size ( self ) : shape = self . output_shape if shape is None : raise util . ConfigurationError ( 'undefined output size for layer "{}"' . format ( self . name ) ) return shape [ - 1 ]
def resolve_outputs ( self ) : input_shape = None for i , shape in enumerate ( self . _input_shapes . values ( ) ) : if i == 0 : input_shape = shape if len ( input_shape ) != len ( shape ) or any ( a is not None and b is not None and a != b for a , b in zip ( input_shape [ : - 1 ] , shape [ : - 1 ] ) ) : raise util . ConfigurationError ( 'layer "{}" incompatible input shapes {}' . format ( self . name , self . _input_shapes ) ) size = self . kwargs . get ( 'size' ) shape = self . kwargs . get ( 'shape' ) if shape is not None : pass elif size is not None : shape = tuple ( input_shape [ : - 1 ] ) + ( size , ) else : raise util . ConfigurationError ( 'layer "{}" does not specify a size' . format ( self . name ) ) self . _output_shapes [ 'out' ] = shape
def log ( self ) : inputs = ', ' . join ( '"{0}" {1}' . format ( * ns ) for ns in self . _input_shapes . items ( ) ) util . log ( 'layer {0.__class__.__name__} "{0.name}" {0.output_shape} {1} from {2}' , self , getattr ( self . activate , 'name' , self . activate ) , inputs ) util . log ( 'learnable parameters: {}' , self . log_params ( ) )
def log_params ( self ) : total = 0 for p in self . params : shape = p . get_value ( ) . shape util . log ( 'parameter "{}" {}' , p . name , shape ) total += np . prod ( shape ) return total
def _fmt ( self , string ) : if '{' not in string : string = '{}.' + string return string . format ( self . name )
def get_all_intervals ( self ) : ints = sorted ( self . get_intervals ( True ) ) if self . tier_type == 'IntervalTier' : if not ints : ints . append ( ( self . xmin , self . xmax , '' ) ) else : if ints [ 0 ] [ 0 ] > self . xmin : ints . insert ( 0 , ( self . xmin , ints [ 0 ] [ 0 ] , '' ) ) if ints [ - 1 ] [ 1 ] < self . xmax : ints . append ( ( ints [ - 1 ] [ 1 ] , self . xmax , '' ) ) p = ints [ - 1 ] for index , i in reversed ( list ( enumerate ( ints [ : - 1 ] , 1 ) ) ) : if p [ 0 ] - i [ 1 ] != 0 : ints . insert ( index , ( i [ 1 ] , p [ 0 ] , '' ) ) p = i return ints
def main ( ) : import optparse import sys import codecs import locale import six from . algorithm import get_display parser = optparse . OptionParser ( ) parser . add_option ( '-e' , '--encoding' , dest = 'encoding' , default = 'utf-8' , type = 'string' , help = 'Text encoding (default: utf-8)' ) parser . add_option ( '-u' , '--upper-is-rtl' , dest = 'upper_is_rtl' , default = False , action = 'store_true' , help = "Treat upper case chars as strong 'R' " 'for debugging (default: False).' ) parser . add_option ( '-d' , '--debug' , dest = 'debug' , default = False , action = 'store_true' , help = "Output to stderr steps taken with the algorithm" ) parser . add_option ( '-b' , '--base-dir' , dest = 'base_dir' , default = None , type = 'string' , help = "Override base direction [L|R]" ) options , rest = parser . parse_args ( ) if options . base_dir and options . base_dir not in 'LR' : parser . error ( 'option -b can be L or R' ) if six . PY2 : sys . stdout = codecs . getwriter ( locale . getpreferredencoding ( ) ) ( sys . stdout ) if rest : lines = rest else : lines = sys . stdin for line in lines : display = get_display ( line , options . encoding , options . upper_is_rtl , options . base_dir , options . debug ) if not isinstance ( display , six . text_type ) : display = display . decode ( options . encoding ) six . print_ ( display , end = '' )
def debug_storage ( storage , base_info = False , chars = True , runs = False ) : import codecs import locale import sys if six . PY2 : stderr = codecs . getwriter ( locale . getpreferredencoding ( ) ) ( sys . stderr ) else : stderr = sys . stderr caller = inspect . stack ( ) [ 1 ] [ 3 ] stderr . write ( 'in %s\n' % caller ) if base_info : stderr . write ( u'  base level  : %d\n' % storage [ 'base_level' ] ) stderr . write ( u'  base dir    : %s\n' % storage [ 'base_dir' ] ) if runs : stderr . write ( u'  runs        : %s\n' % list ( storage [ 'runs' ] ) ) if chars : output = u'  Chars       : ' for _ch in storage [ 'chars' ] : if _ch != '\n' : output += _ch [ 'ch' ] else : output += 'C' stderr . write ( output + u'\n' ) output = u'  Res. levels : %s\n' % u'' . join ( [ six . text_type ( _ch [ 'level' ] ) for _ch in storage [ 'chars' ] ] ) stderr . write ( output ) _types = [ _ch [ 'type' ] . ljust ( 3 ) for _ch in storage [ 'chars' ] ] for i in range ( 3 ) : if i : output = u'                %s\n' else : output = u'  Res. types  : %s\n' stderr . write ( output % u'' . join ( [ _t [ i ] for _t in _types ] ) )
def reorder_resolved_levels ( storage , debug ) : should_reset = True chars = storage [ 'chars' ] for _ch in chars [ : : - 1 ] : if _ch [ 'orig' ] in ( 'B' , 'S' ) : _ch [ 'level' ] = storage [ 'base_level' ] should_reset = True elif should_reset and _ch [ 'orig' ] in ( 'BN' , 'WS' ) : _ch [ 'level' ] = storage [ 'base_level' ] else : should_reset = False max_len = len ( chars ) line_start = line_end = 0 highest_level = 0 lowest_odd_level = EXPLICIT_LEVEL_LIMIT for idx in range ( max_len ) : _ch = chars [ idx ] char_level = _ch [ 'level' ] if char_level > highest_level : highest_level = char_level if char_level % 2 and char_level < lowest_odd_level : lowest_odd_level = char_level if _ch [ 'orig' ] == 'B' or idx == max_len - 1 : line_end = idx if _ch [ 'orig' ] == 'B' : line_end -= 1 reverse_contiguous_sequence ( chars , line_start , line_end , highest_level , lowest_odd_level ) line_start = idx + 1 highest_level = 0 lowest_odd_level = EXPLICIT_LEVEL_LIMIT if debug : debug_storage ( storage )
def process ( self , context ) : import os from maya import cmds current_file = cmds . file ( sceneName = True , query = True ) normalised = os . path . normpath ( current_file ) context . set_data ( 'currentFile' , value = normalised ) context . set_data ( 'current_file' , value = normalised )
def _add ( object , name , value ) : self . __added__ . append ( name ) setattr ( object , name , value )
def cli ( args ) : import argparse parser = argparse . ArgumentParser ( ) parser . add_argument ( "--convert" , help = "Path to compiled Python module, e.g. my_ui.py" ) parser . add_argument ( "--compile" , help = "Accept raw .ui file and compile with native " "PySide2 compiler." ) parser . add_argument ( "--stdout" , help = "Write to stdout instead of file" , action = "store_true" ) parser . add_argument ( "--stdin" , help = "Read from stdin instead of file" , action = "store_true" ) args = parser . parse_args ( args ) if args . stdout : raise NotImplementedError ( "--stdout" ) if args . stdin : raise NotImplementedError ( "--stdin" ) if args . compile : raise NotImplementedError ( "--compile" ) if args . convert : sys . stdout . write ( "#\n" "#\n" ) # # with open ( args . convert ) as f : lines = convert ( f . readlines ( ) ) backup = "%s_backup%s" % os . path . splitext ( args . convert ) sys . stdout . write ( "Creating \"%s\"..\n" % backup ) shutil . copy ( args . convert , backup ) # # with open ( args . convert , "w" ) as f : f . write ( "" . join ( lines ) ) sys . stdout . write ( "Successfully converted \"%s\"\n" % args . convert )
def _discover_gui ( ) : guis = reversed ( pyblish . api . registered_guis ( ) ) for gui in guis : try : gui = __import__ ( gui ) . show except ( ImportError , AttributeError ) : continue else : return gui
def get_single_axis_values ( self , axis , dataset ) : data_index = getattr ( self , '%s_data_index' % axis ) return [ p [ data_index ] for p in dataset [ 'data' ] ]
def __draw_constant_line ( self , value_label_style ) : value , label , style = value_label_style start = self . transform_output_coordinates ( ( 0 , value ) ) [ 1 ] stop = self . graph_width path = etree . SubElement ( self . graph , 'path' , { 'd' : 'M 0 %(start)s h%(stop)s' % locals ( ) , 'class' : 'constantLine' } ) if style : path . set ( 'style' , style ) text = etree . SubElement ( self . graph , 'text' , { 'x' : str ( 2 ) , 'y' : str ( start - 2 ) , 'class' : 'constantLine' } ) text . text = label
def load_transform_parameters ( self ) : x_min , x_max , x_div = self . x_range ( ) y_min , y_max , y_div = self . y_range ( ) x_step = ( float ( self . graph_width ) - self . font_size * 2 ) / ( x_max - x_min ) y_step = ( float ( self . graph_height ) - self . font_size * 2 ) / ( y_max - y_min ) self . __transform_parameters = dict ( locals ( ) ) del self . __transform_parameters [ 'self' ]
def add_popup ( self , x , y , label ) : txt_width = len ( label ) * self . font_size * 0.6 + 10 tx = x + [ 5 , - 5 ] [ int ( x + txt_width > self . width ) ] anchor = [ 'start' , 'end' ] [ x + txt_width > self . width ] style = 'fill: #000; text-anchor: %s;' % anchor id = 'label-%s' % self . _w3c_name ( label ) attrs = { 'x' : str ( tx ) , 'y' : str ( y - self . font_size ) , 'visibility' : 'hidden' , 'style' : style , 'text' : label , 'id' : id , } etree . SubElement ( self . foreground , 'text' , attrs ) vis_tmpl = ( "document.getElementById('{id}').setAttribute('visibility', {val})" ) attrs = { 'cx' : str ( x ) , 'cy' : str ( y ) , 'r' : str ( 10 ) , 'style' : 'opacity: 0;' , 'onmouseover' : vis_tmpl . format ( val = 'visible' , id = id ) , 'onmouseout' : vis_tmpl . format ( val = 'hidden' , id = id ) , } etree . SubElement ( self . foreground , 'circle' , attrs )
def make_datapoint_text ( self , x , y , value , style = None ) : if not self . show_data_values : return e = etree . SubElement ( self . foreground , 'text' , { 'x' : str ( x ) , 'y' : str ( y ) , 'class' : 'dataPointLabel' , 'style' : '%(style)s stroke: #fff; stroke-width: 2;' % vars ( ) , } ) e . text = str ( value ) e = etree . SubElement ( self . foreground , 'text' , { 'x' : str ( x ) , 'y' : str ( y ) , 'class' : 'dataPointLabel' } ) e . text = str ( value ) if style : e . set ( 'style' , style )
def draw_x_labels ( self ) : if self . show_x_labels : labels = self . get_x_labels ( ) count = len ( labels ) labels = enumerate ( iter ( labels ) ) start = int ( not self . step_include_first_x_label ) labels = itertools . islice ( labels , start , None , self . step_x_labels ) list ( map ( self . draw_x_label , labels ) ) self . draw_x_guidelines ( self . field_width ( ) , count )
def draw_y_labels ( self ) : if not self . show_y_labels : return labels = self . get_y_labels ( ) count = len ( labels ) labels = enumerate ( iter ( labels ) ) start = int ( not self . step_include_first_y_label ) labels = itertools . islice ( labels , start , None , self . step_y_labels ) list ( map ( self . draw_y_label , labels ) ) self . draw_y_guidelines ( self . field_height ( ) , count )
def draw_x_guidelines ( self , label_height , count ) : if not self . show_x_guidelines : return for count in range ( 1 , count ) : move = 'M {start} 0 v{stop}' . format ( start = label_height * count , stop = self . graph_height , ) path = { 'd' : move , 'class' : 'guideLines' } etree . SubElement ( self . graph , 'path' , path )
def draw_y_guidelines ( self , label_height , count ) : if not self . show_y_guidelines : return for count in range ( 1 , count ) : move = 'M 0 {start} h{stop}' . format ( start = self . graph_height - label_height * count , stop = self . graph_width , ) path = { 'd' : move , 'class' : 'guideLines' } etree . SubElement ( self . graph , 'path' , path )
def draw_titles ( self ) : if self . show_graph_title : self . draw_graph_title ( ) if self . show_graph_subtitle : self . draw_graph_subtitle ( ) if self . show_x_title : self . draw_x_title ( ) if self . show_y_title : self . draw_y_title ( )
def render_inline_styles ( self ) : if not self . css_inline : return styles = self . parse_css ( ) for node in self . root . xpath ( '//*[@class]' ) : cl = '.' + node . attrib [ 'class' ] if cl not in styles : continue style = styles [ cl ] if 'style' in node . attrib : style += node . attrib [ 'style' ] node . attrib [ 'style' ] = style
def start_svg ( self ) : SVG_NAMESPACE = 'http://www.w3.org/2000/svg' SVG = '{%s}' % SVG_NAMESPACE NSMAP = { None : SVG_NAMESPACE , 'xlink' : 'http://www.w3.org/1999/xlink' , 'a3' : 'http://ns.adobe.com/AdobeSVGViewerExtensions/3.0/' , } root_attrs = self . _get_root_attributes ( ) self . root = etree . Element ( SVG + "svg" , attrib = root_attrs , nsmap = NSMAP ) if hasattr ( self , 'style_sheet_href' ) : pi = etree . ProcessingInstruction ( 'xml-stylesheet' , 'href="%s" type="text/css"' % self . style_sheet_href ) self . root . addprevious ( pi ) comment_strings = ( ' Created with SVG.Graph ' , ' SVG.Graph by Jason R. Coombs ' , ' Based on SVG::Graph by Sean E. Russel ' , ' Based on Perl SVG:TT:Graph by Leo Lapworth & Stephan Morgan ' , ' ' + '/' * 66 , ) list ( map ( self . root . append , map ( etree . Comment , comment_strings ) ) ) defs = etree . SubElement ( self . root , 'defs' ) self . add_defs ( defs ) if not hasattr ( self , 'style_sheet_href' ) and not self . css_inline : self . root . append ( etree . Comment ( ' include default stylesheet if none specified ' ) ) style = etree . SubElement ( defs , 'style' , type = 'text/css' ) style . text = self . get_stylesheet ( ) . cssText self . root . append ( etree . Comment ( 'SVG Background' ) ) etree . SubElement ( self . root , 'rect' , { 'width' : str ( self . width ) , 'height' : str ( self . height ) , 'x' : '0' , 'y' : '0' , 'class' : 'svgBackground' } )
def get_stylesheet_resources ( self ) : class_vars = class_dict ( self ) loader = functools . partial ( self . load_resource_stylesheet , subs = class_vars ) sheets = list ( map ( loader , self . stylesheet_names ) ) return sheets
def send_validation_email ( self ) : if self . email_verified : raise ValueError ( _ ( 'Cannot validate already active user.' ) ) site = Site . objects . get_current ( ) self . validation_notification ( user = self , site = site ) . notify ( )
def send_password_reset ( self ) : site = Site . objects . get_current ( ) self . password_reset_notification ( user = self , site = site ) . notify ( )
def allow_request ( self , request , view ) : if request . method != 'POST' : return True return super ( PostRequestThrottleMixin , self ) . allow_request ( request , view )
def client ( self ) : cls = self . __class__ if cls . _client is None : kwargs = { } if self . tls_config : kwargs [ 'tls' ] = docker . tls . TLSConfig ( * * self . tls_config ) kwargs . update ( kwargs_from_env ( ) ) client = docker . APIClient ( version = 'auto' , * * kwargs ) cls . _client = client return cls . _client
def poll ( self ) : service = yield self . get_service ( ) if not service : self . log . warn ( "Docker service not found" ) return 0 task_filter = { 'service' : service [ 'Spec' ] [ 'Name' ] } tasks = yield self . docker ( 'tasks' , task_filter ) running_task = None for task in tasks : task_state = task [ 'Status' ] [ 'State' ] self . log . debug ( "Task %s of Docker service %s status: %s" , task [ 'ID' ] [ : 7 ] , self . service_id [ : 7 ] , pformat ( task_state ) , ) if task_state == 'running' : running_task = task if running_task is not None : return None else : return 1
def filter_queryset ( self , value , queryset ) : return super ( UniqueEmailValidator , self ) . filter_queryset ( value . lower ( ) , queryset , )
def update ( self , instance , validated_data ) : if not instance . check_password ( validated_data [ 'old_password' ] ) : msg = _ ( 'Invalid password.' ) raise serializers . ValidationError ( { 'old_password' : msg } ) instance . set_password ( validated_data [ 'new_password' ] ) instance . save ( ) return instance
def update ( self , instance , validated_data ) : instance . set_password ( validated_data [ 'new_password' ] ) instance . save ( ) return instance
def delete ( self , request , * args , * * kwargs ) : auth = get_authorization_header ( request ) . split ( ) if not auth or auth [ 0 ] . lower ( ) != b'token' : return response . Response ( status = status . HTTP_400_BAD_REQUEST ) if len ( auth ) == 1 : msg = 'Invalid token header. No credentials provided.' return response . Response ( msg , status = status . HTTP_400_BAD_REQUEST ) elif len ( auth ) > 2 : msg = 'Invalid token header. Token string should not contain spaces.' return response . Response ( msg , status = status . HTTP_400_BAD_REQUEST ) try : token = self . model . objects . get ( key = auth [ 1 ] ) except self . model . DoesNotExist : pass else : token . delete ( ) signals . user_logged_out . send ( type ( self ) , user = token . user , request = request , ) return response . Response ( status = status . HTTP_204_NO_CONTENT )
def initial ( self , request , * args , * * kwargs ) : email = request . data . get ( 'email' ) if request . user . is_authenticated ( ) and email != request . user . email : raise PermissionDenied ( ) return super ( ResendConfirmationEmail , self ) . initial ( request , * args , * * kwargs )
def post ( self , request , * args , * * kwargs ) : serializer = self . serializer_class ( data = request . data ) if not serializer . is_valid ( ) : return response . Response ( serializer . errors , status = status . HTTP_400_BAD_REQUEST , ) serializer . user . send_validation_email ( ) msg = _ ( 'Email confirmation sent.' ) return response . Response ( msg , status = status . HTTP_204_NO_CONTENT )
def update_expiry ( self , commit = True ) : self . expires = update_expiry ( self . created ) if commit : self . save ( )
def password_reset_email_context ( notification ) : return { 'protocol' : 'https' , 'uid' : notification . user . generate_uid ( ) , 'token' : notification . user . generate_token ( ) , 'site' : notification . site , }
def email_handler ( notification , email_context ) : incuna_mail . send ( to = notification . user . email , subject = notification . email_subject , template_name = notification . text_email_template , html_template_name = notification . html_email_template , context = email_context ( notification ) , headers = getattr ( notification , 'headers' , { } ) , )
def password_reset_email_handler ( notification ) : base_subject = _ ( '{domain} password reset' ) . format ( domain = notification . site . domain ) subject = getattr ( settings , 'DUM_PASSWORD_RESET_SUBJECT' , base_subject ) notification . email_subject = subject email_handler ( notification , password_reset_email_context )
def validation_email_handler ( notification ) : base_subject = _ ( '{domain} account validate' ) . format ( domain = notification . site . domain ) subject = getattr ( settings , 'DUM_VALIDATE_EMAIL_SUBJECT' , base_subject ) notification . email_subject = subject email_handler ( notification , validation_email_context )
def authenticate_credentials ( self , key ) : user , token = super ( TokenAuthentication , self ) . authenticate_credentials ( key ) if token . expires < timezone . now ( ) : msg = _ ( 'Token has expired.' ) raise exceptions . AuthenticationFailed ( msg ) token . update_expiry ( ) return ( user , token )
def notebook_show ( obj , doc , comm ) : target = obj . ref [ 'id' ] load_mime = 'application/vnd.holoviews_load.v0+json' exec_mime = 'application/vnd.holoviews_exec.v0+json' bokeh_script , bokeh_div , _ = bokeh . embed . notebook . notebook_content ( obj , comm . id ) publish_display_data ( data = { 'text/html' : encode_utf8 ( bokeh_div ) } ) JS = '\n' . join ( [ PYVIZ_PROXY , JupyterCommManager . js_manager ] ) publish_display_data ( data = { load_mime : JS , 'application/javascript' : JS } ) msg_handler = bokeh_msg_handler . format ( plot_id = target ) comm_js = comm . js_template . format ( plot_id = target , comm_id = comm . id , msg_handler = msg_handler ) bokeh_js = '\n' . join ( [ comm_js , bokeh_script ] ) publish_display_data ( data = { exec_mime : '' , 'text/html' : '' , 'application/javascript' : bokeh_js } , metadata = { exec_mime : { 'id' : target } } )
def process_hv_plots ( widgets , plots ) : bokeh_plots = [ ] for plot in plots : if hasattr ( plot , '_update_callbacks' ) : for subplot in plot . traverse ( lambda x : x ) : subplot . comm = widgets . server_comm for cb in subplot . callbacks : for c in cb . callbacks : c . code = c . code . replace ( plot . id , widgets . plot_id ) plot = plot . state bokeh_plots . append ( plot ) return bokeh_plots
def widget ( self , param_name ) : if param_name not in self . _widgets : self . _widgets [ param_name ] = self . _make_widget ( param_name ) return self . _widgets [ param_name ]
def render_function ( obj , view ) : try : import holoviews as hv except : hv = None if hv and isinstance ( obj , hv . core . Dimensioned ) : renderer = hv . renderer ( 'bokeh' ) if not view . _notebook : renderer = renderer . instance ( mode = 'server' ) plot = renderer . get_plot ( obj , doc = view . _document ) if view . _notebook : plot . comm = view . _comm plot . document = view . _document return plot . state return obj
def TextWidget ( * args , * * kw ) : kw [ 'value' ] = str ( kw [ 'value' ] ) kw . pop ( 'options' , None ) return TextInput ( * args , * * kw )
def ping ( self , params = None ) : try : self . transport . perform_request ( 'HEAD' , '/' , params = params ) except TransportError : raise gen . Return ( False ) raise gen . Return ( True )
def bytes_to_readable ( num ) : if num < 512 : return "0 Kb" elif num < 1024 : return "1 Kb" for unit in [ '' , 'Kb' , 'Mb' , 'Gb' , 'Tb' , 'Pb' , 'Eb' , 'Zb' ] : if abs ( num ) < 1024.0 : return "%3.1f%s" % ( num , unit ) num /= 1024.0 return "%.1f%s" % ( num , 'Yb' )
def cpu_total_load ( self ) : system_load = self . cpu_system_load user_load = self . cpu_user_load other_load = self . cpu_other_load if system_load is not None and user_load is not None and other_load is not None : return system_load + user_load + other_load
def memory_size ( self , human_readable = True ) : if self . _data is not None : return_data = int ( self . _data [ "memory" ] [ "memory_size" ] ) * 1024 if human_readable : return SynoFormatHelper . bytes_to_readable ( return_data ) else : return return_data
def network_up ( self , human_readable = True ) : network = self . _get_network ( "total" ) if network is not None : return_data = int ( network [ "tx" ] ) if human_readable : return SynoFormatHelper . bytes_to_readable ( return_data ) else : return return_data
def volumes ( self ) : if self . _data is not None : volumes = [ ] for volume in self . _data [ "volumes" ] : volumes . append ( volume [ "id" ] ) return volumes
def _get_volume ( self , volume_id ) : if self . _data is not None : for volume in self . _data [ "volumes" ] : if volume [ "id" ] == volume_id : return volume
def volume_size_total ( self , volume , human_readable = True ) : volume = self . _get_volume ( volume ) if volume is not None : return_data = int ( volume [ "size" ] [ "total" ] ) if human_readable : return SynoFormatHelper . bytes_to_readable ( return_data ) else : return return_data
def volume_percentage_used ( self , volume ) : volume = self . _get_volume ( volume ) if volume is not None : total = int ( volume [ "size" ] [ "total" ] ) used = int ( volume [ "size" ] [ "used" ] ) if used is not None and used > 0 and total is not None and total > 0 : return round ( ( float ( used ) / float ( total ) ) * 100.0 , 1 )
def volume_disk_temp_avg ( self , volume ) : volume = self . _get_volume ( volume ) if volume is not None : vol_disks = volume [ "disks" ] if vol_disks is not None : total_temp = 0 total_disks = 0 for vol_disk in vol_disks : disk_temp = self . disk_temp ( vol_disk ) if disk_temp is not None : total_disks += 1 total_temp += disk_temp if total_temp > 0 and total_disks > 0 : return round ( total_temp / total_disks , 0 )
def volume_disk_temp_max ( self , volume ) : volume = self . _get_volume ( volume ) if volume is not None : vol_disks = volume [ "disks" ] if vol_disks is not None : max_temp = 0 for vol_disk in vol_disks : disk_temp = self . disk_temp ( vol_disk ) if disk_temp is not None and disk_temp > max_temp : max_temp = disk_temp return max_temp
def _get_disk ( self , disk_id ) : if self . _data is not None : for disk in self . _data [ "disks" ] : if disk [ "id" ] == disk_id : return disk
def _login ( self ) : api_path = "%s/auth.cgi?api=SYNO.API.Auth&version=2" % ( self . base_url , ) login_path = "method=login&%s" % ( self . _encode_credentials ( ) ) url = "%s&%s&session=Core&format=cookie" % ( api_path , login_path ) result = self . _execute_get_url ( url , False ) if result is not None : self . access_token = result [ "data" ] [ "sid" ] self . _debuglog ( "Authentication Succesfull, token: " + str ( self . access_token ) ) return True else : self . _debuglog ( "Authentication Failed" ) return False
def _get_url ( self , url , retry_on_error = True ) : if self . access_token is None or self . _session is None or self . _session_error : self . access_token = None self . _session_error = False if self . _session is not None : self . _session = None self . _debuglog ( "Creating New Session" ) self . _session = requests . Session ( ) if self . _use_https : self . _session . verify = False if self . _login ( ) is False : self . _session_error = True self . _debuglog ( "Login Failed, unable to process request" ) return response = self . _execute_get_url ( url ) if ( self . _session_error or response is None ) and retry_on_error : self . _debuglog ( "Error occured, retrying..." ) self . _get_url ( url , False ) return response
def _execute_get_url ( self , request_url , append_sid = True ) : self . _debuglog ( "Requesting URL: '" + request_url + "'" ) if append_sid : self . _debuglog ( "Appending access_token (SID: " + self . access_token + ") to url" ) request_url = "%s&_sid=%s" % ( request_url , self . access_token ) try : resp = self . _session . get ( request_url ) self . _debuglog ( "Request executed: " + str ( resp . status_code ) ) if resp . status_code == 200 : json_data = json . loads ( resp . text ) if json_data [ "success" ] : self . _debuglog ( "Succesfull returning data" ) self . _debuglog ( str ( json_data ) ) return json_data else : if json_data [ "error" ] [ "code" ] in { 105 , 106 , 107 , 119 } : self . _debuglog ( "Session error: " + str ( json_data [ "error" ] [ "code" ] ) ) self . _session_error = True else : self . _debuglog ( "Failed: " + resp . text ) else : return None except : return None
def update ( self ) : if self . _utilisation is not None : api = "SYNO.Core.System.Utilization" url = "%s/entry.cgi?api=%s&version=1&method=get&_sid=%s" % ( self . base_url , api , self . access_token ) self . _utilisation . update ( self . _get_url ( url ) ) if self . _storage is not None : api = "SYNO.Storage.CGI.Storage" url = "%s/entry.cgi?api=%s&version=1&method=load_info&_sid=%s" % ( self . base_url , api , self . access_token ) self . _storage . update ( self . _get_url ( url ) )
def utilisation ( self ) : if self . _utilisation is None : api = "SYNO.Core.System.Utilization" url = "%s/entry.cgi?api=%s&version=1&method=get" % ( self . base_url , api ) self . _utilisation = SynoUtilization ( self . _get_url ( url ) ) return self . _utilisation
def storage ( self ) : if self . _storage is None : api = "SYNO.Storage.CGI.Storage" url = "%s/entry.cgi?api=%s&version=1&method=load_info" % ( self . base_url , api ) self . _storage = SynoStorage ( self . _get_url ( url ) ) return self . _storage
def for_request ( request , body = None ) : tenant , jwt_data = Tenant . objects . for_request ( request , body ) webhook_sender_id = jwt_data . get ( 'sub' ) sender_data = None if body and 'item' in body : if 'sender' in body [ 'item' ] : sender_data = body [ 'item' ] [ 'sender' ] elif 'message' in body [ 'item' ] and 'from' in body [ 'item' ] [ 'message' ] : sender_data = body [ 'item' ] [ 'message' ] [ 'from' ] if sender_data is None : if webhook_sender_id is None : raise BadTenantError ( 'Cannot identify sender in tenant' ) sender_data = { 'id' : webhook_sender_id } return Context ( tenant = tenant , sender = HipchatUser ( id = sender_data . get ( 'id' ) , name = sender_data . get ( 'name' ) , mention_name = sender_data . get ( 'mention_name' ) , ) , signed_request = request . GET . get ( 'signed_request' ) , context = jwt_data . get ( 'context' ) or { } , )
def tenant_token ( self ) : rv = getattr ( self , '_tenant_token' , None ) if rv is None : rv = self . _tenant_token = self . tenant . get_token ( ) return rv
def build_attrs ( self , extra_attrs = None , * * kwargs ) : self . attrs = self . widget . build_attrs ( extra_attrs = None , * * kwargs ) return self . attrs
def get_global_settings ( self ) : return dict ( ( key , getattr ( global_settings , key ) ) for key in dir ( global_settings ) if key . isupper ( ) )
def do_GET ( self ) : parsed_url = urlparse ( self . path ) if parsed_url [ 2 ] == "/" + SERVER_REDIRECT_PATH : parsed_query = parse_qs ( parsed_url [ 4 ] ) if "code" not in parsed_query : self . send_response ( 200 ) self . send_header ( "Content-Type" , "text/plain" ) self . end_headers ( ) self . wfile . write ( "No code found, try again!" . encode ( "utf-8" ) ) return self . server . response_code = parsed_query [ "code" ] [ 0 ] self . send_response ( 200 ) self . send_header ( "Content-Type" , "text/plain" ) self . end_headers ( ) self . wfile . write ( "Thank you for using OAuth2Util. The authorization was successful, " "you can now close this window." . encode ( "utf-8" ) ) elif parsed_url [ 2 ] == "/" + SERVER_LINK_PATH : self . send_response ( 200 ) self . send_header ( "Content-Type" , "text/html" ) self . end_headers ( ) self . wfile . write ( "<html><body>Hey there!<br/>Click <a href=\"{0}\">here</a> to claim your prize.</body></html>" . format ( self . server . authorize_url ) . encode ( "utf-8" ) ) else : self . send_response ( 404 ) self . send_header ( "Content-Type" , "text/plain" ) self . end_headers ( ) self . wfile . write ( "404 not found" . encode ( "utf-8" ) )
def _get_value ( self , key , func = None , split_val = None , as_boolean = False , exception_default = None ) : try : if as_boolean : return self . config . getboolean ( key [ 0 ] , key [ 1 ] ) value = self . config . get ( key [ 0 ] , key [ 1 ] ) if split_val is not None : value = value . split ( split_val ) if func is not None : return func ( value ) return value except ( KeyError , configparser . NoSectionError , configparser . NoOptionError ) as e : if exception_default is not None : return exception_default raise KeyError ( e )
def _change_value ( self , key , value ) : if not self . config . has_section ( key [ 0 ] ) : self . config . add_section ( key [ 0 ] ) self . config . set ( key [ 0 ] , key [ 1 ] , str ( value ) ) with open ( self . configfile , "w" ) as f : self . config . write ( f )
def _migrate_config ( self , oldname = DEFAULT_CONFIG , newname = DEFAULT_CONFIG ) : self . _log ( "Your OAuth2Util config file is in an old format and needs " "to be changed. I tried as best as I could to migrate it." , logging . WARNING ) with open ( oldname , "r" ) as old : with open ( newname , "w" ) as new : new . write ( "[app]\n" ) new . write ( old . read ( ) )
def _start_webserver ( self , authorize_url = None ) : server_address = ( SERVER_URL , SERVER_PORT ) self . server = HTTPServer ( server_address , OAuth2UtilRequestHandler ) self . server . response_code = None self . server . authorize_url = authorize_url t = Thread ( target = self . server . serve_forever ) t . daemon = True t . start ( )
def _wait_for_response ( self ) : while not self . server . response_code : time . sleep ( 2 ) time . sleep ( 5 ) self . server . shutdown ( )
def _get_new_access_information ( self ) : if not self . r . has_oauth_app_info : self . _log ( 'Cannot obtain authorize url from PRAW. Please check your configuration.' , logging . ERROR ) raise AttributeError ( 'Reddit Session invalid, please check your designated config file.' ) url = self . r . get_authorize_url ( 'UsingOAuth2Util' , self . _get_value ( CONFIGKEY_SCOPE , set , split_val = ',' ) , self . _get_value ( CONFIGKEY_REFRESHABLE , as_boolean = True ) ) self . _start_webserver ( url ) if not self . _get_value ( CONFIGKEY_SERVER_MODE , as_boolean = True ) : webbrowser . open ( url ) else : print ( "Webserver is waiting for you :D. Please open {0}:{1}/{2} " "in your browser" . format ( SERVER_URL , SERVER_PORT , SERVER_LINK_PATH ) ) self . _wait_for_response ( ) try : access_information = self . r . get_access_information ( self . server . response_code ) except praw . errors . OAuthException : self . _log ( "Can not authenticate, maybe the app infos (e.g. secret) are wrong." , logging . ERROR ) raise self . _change_value ( CONFIGKEY_TOKEN , access_information [ "access_token" ] ) self . _change_value ( CONFIGKEY_REFRESH_TOKEN , access_information [ "refresh_token" ] ) self . _change_value ( CONFIGKEY_VALID_UNTIL , time . time ( ) + TOKEN_VALID_DURATION )
def _check_token_present ( self ) : try : self . _get_value ( CONFIGKEY_TOKEN ) self . _get_value ( CONFIGKEY_REFRESH_TOKEN ) self . _get_value ( CONFIGKEY_REFRESHABLE ) except KeyError : self . _log ( "Request new Token (CTP)" ) self . _get_new_access_information ( )
def set_access_credentials ( self , _retry = 0 ) : if _retry >= 5 : raise ConnectionAbortedError ( 'Reddit is not accessible right now, cannot refresh OAuth2 tokens.' ) self . _check_token_present ( ) try : self . r . set_access_credentials ( self . _get_value ( CONFIGKEY_SCOPE , set , split_val = "," ) , self . _get_value ( CONFIGKEY_TOKEN ) , self . _get_value ( CONFIGKEY_REFRESH_TOKEN ) ) except ( praw . errors . OAuthInvalidToken , praw . errors . HTTPException ) as e : self . _log ( "Request new Token (SAC)" ) self . _get_new_access_information ( )
def fix_schema ( prefix , schema ) : schema_dict = extract_schema ( schema ) snake_case_organization = schema_dict [ 'vendor' ] . replace ( '.' , '_' ) . lower ( ) snake_case_name = re . sub ( '([^A-Z_])([A-Z])' , '\g<1>_\g<2>' , schema_dict [ 'name' ] ) . lower ( ) model = schema_dict [ 'version' ] . split ( '-' ) [ 0 ] return "{}_{}_{}_{}" . format ( prefix , snake_case_organization , snake_case_name , model )
def transform ( line , known_fields = ENRICHED_EVENT_FIELD_TYPES , add_geolocation_data = True ) : return jsonify_good_event ( line . split ( '\t' ) , known_fields , add_geolocation_data )
def jsonify_good_event ( event , known_fields = ENRICHED_EVENT_FIELD_TYPES , add_geolocation_data = True ) : if len ( event ) != len ( known_fields ) : raise SnowplowEventTransformationException ( [ "Expected {} fields, received {} fields." . format ( len ( known_fields ) , len ( event ) ) ] ) else : output = { } errors = [ ] if add_geolocation_data and event [ LATITUDE_INDEX ] != '' and event [ LONGITUDE_INDEX ] != '' : output [ 'geo_location' ] = event [ LATITUDE_INDEX ] + ',' + event [ LONGITUDE_INDEX ] for i in range ( len ( event ) ) : key = known_fields [ i ] [ 0 ] if event [ i ] != '' : try : kvpairs = known_fields [ i ] [ 1 ] ( key , event [ i ] ) for kvpair in kvpairs : output [ kvpair [ 0 ] ] = kvpair [ 1 ] except SnowplowEventTransformationException as sete : errors += sete . error_messages except Exception as e : errors += [ "Unexpected exception parsing field with key {} and value {}: {}" . format ( known_fields [ i ] [ 0 ] , event [ i ] , repr ( e ) ) ] if errors : raise SnowplowEventTransformationException ( errors ) else : return output
def print_context ( self , context ) : text = [ CONTEXT_TITLE ] for i , context_scope in enumerate ( context ) : dump1 = linebreaksbr ( pformat_django_context_html ( context_scope ) ) dump2 = pformat_dict_summary_html ( context_scope ) if len ( context_scope ) <= 3 and dump1 . count ( '<br />' ) > 20 : ( dump1 , dump2 ) = ( dump2 , dump1 ) text . append ( CONTEXT_BLOCK . format ( style = PRE_STYLE , num = i , dump1 = dump1 , dump2 = dump2 ) ) return u'' . join ( text )
def print_variables ( self , context ) : text = [ ] for name , expr in self . variables : data = '' try : if isinstance ( expr . var , Variable ) : data = expr . var . resolve ( context ) else : data = expr . resolve ( context ) except VariableDoesNotExist as e : keys = [ ] for scope in context : keys += scope . keys ( ) keys = sorted ( set ( keys ) ) return ERROR_TYPE_BLOCK . format ( style = PRE_ALERT_STYLE , error = escape ( u"Variable '{0}' not found!  Available context variables are:\n\n{1}" . format ( expr , u', ' . join ( keys ) ) ) ) else : textdata = linebreaksbr ( pformat_django_context_html ( data ) ) if isinstance ( data , SHORT_NAME_TYPES ) : text . append ( BASIC_TYPE_BLOCK . format ( style = PRE_STYLE , name = name , value = textdata ) ) else : text . append ( OBJECT_TYPE_BLOCK . format ( style = PRE_STYLE , name = name , type = data . __class__ . __name__ , value = textdata ) ) return u'' . join ( text )
def pformat_sql_html ( sql ) : sql = escape ( sql ) sql = RE_SQL_NL . sub ( u'<br>\n\\1' , sql ) sql = RE_SQL . sub ( u'<strong>\\1</strong>' , sql ) return sql
def pformat_dict_summary_html ( dict ) : if not dict : return '   {}' html = [ ] for key , value in sorted ( six . iteritems ( dict ) ) : if not isinstance ( value , DICT_EXPANDED_TYPES ) : value = '...' html . append ( _format_dict_item ( key , value ) ) return mark_safe ( u'<br/>' . join ( html ) )
def _format ( self , object , stream , indent , allowance , context , level ) : try : PrettyPrinter . _format ( self , object , stream , indent , allowance , context , level ) except Exception as e : stream . write ( _format_exception ( e ) )
def get_organisation_information ( self , query_params = None ) : return self . fetch_json ( uri_path = self . base_uri , query_params = query_params or { } )
def get_list_information ( self , query_params = None ) : return self . fetch_json ( uri_path = self . base_uri , query_params = query_params or { } )
def add_card ( self , query_params = None ) : card_json = self . fetch_json ( uri_path = self . base_uri + '/cards' , http_method = 'POST' , query_params = query_params or { } ) return self . create_card ( card_json )
def get_label_information ( self , query_params = None ) : return self . fetch_json ( uri_path = self . base_uri , query_params = query_params or { } )
def _update_label_name ( self , name ) : label_json = self . fetch_json ( uri_path = self . base_uri , http_method = 'PUT' , query_params = { 'name' : name } ) return self . create_label ( label_json )
def _update_label_dict ( self , query_params = { } ) : label_json = self . fetch_json ( uri_path = self . base_uri , http_method = 'PUT' , query_params = query_params ) return self . create_label ( label_json )
def get_card_information ( self , query_params = None ) : return self . fetch_json ( uri_path = self . base_uri , query_params = query_params or { } )
def add_comment ( self , comment_text ) : return self . fetch_json ( uri_path = self . base_uri + '/actions/comments' , http_method = 'POST' , query_params = { 'text' : comment_text } )
def add_attachment ( self , filename , open_file ) : fields = { 'api_key' : self . client . api_key , 'token' : self . client . user_auth_token } content_type , body = self . encode_multipart_formdata ( fields = fields , filename = filename , file_values = open_file ) return self . fetch_json ( uri_path = self . base_uri + '/attachments' , http_method = 'POST' , body = body , headers = { 'Content-Type' : content_type } , )
def add_checklist ( self , query_params = None ) : checklist_json = self . fetch_json ( uri_path = self . base_uri + '/checklists' , http_method = 'POST' , query_params = query_params or { } ) return self . create_checklist ( checklist_json )
def _add_label_from_dict ( self , query_params = None ) : return self . fetch_json ( uri_path = self . base_uri + '/labels' , http_method = 'POST' , query_params = query_params or { } )
def _add_label_from_class ( self , label = None ) : return self . fetch_json ( uri_path = self . base_uri + '/idLabels' , http_method = 'POST' , query_params = { 'value' : label . id } )
def add_member ( self , member_id ) : members = self . fetch_json ( uri_path = self . base_uri + '/idMembers' , http_method = 'POST' , query_params = { 'value' : member_id } ) members_list = [ ] for member_json in members : members_list . append ( self . create_member ( member_json ) ) return members_list
def create_checklist_item ( self , card_id , checklist_id , checklistitem_json , * * kwargs ) : return self . client . create_checklist_item ( card_id , checklist_id , checklistitem_json , * * kwargs )
def get_board_information ( self , query_params = None ) : return self . fetch_json ( uri_path = '/boards/' + self . id , query_params = query_params or { } )
def get_checklists ( self ) : checklists = self . getChecklistsJson ( self . base_uri ) checklists_list = [ ] for checklist_json in checklists : checklists_list . append ( self . createChecklist ( checklist_json ) ) return checklists_list
def update_board ( self , query_params = None ) : board_json = self . fetch_json ( uri_path = self . base_uri , http_method = 'PUT' , query_params = query_params or { } ) return self . create_board ( board_json )
def add_list ( self , query_params = None ) : list_json = self . fetch_json ( uri_path = self . base_uri + '/lists' , http_method = 'POST' , query_params = query_params or { } ) return self . create_list ( list_json )
def add_label ( self , query_params = None ) : list_json = self . fetch_json ( uri_path = self . base_uri + '/labels' , http_method = 'POST' , query_params = query_params or { } ) return self . create_label ( list_json )
def get_checklist_information ( self , query_params = None ) : return self . fetch_json ( uri_path = self . base_uri , query_params = query_params or { } )
def get_card ( self ) : card_id = self . get_checklist_information ( ) . get ( 'idCard' , None ) if card_id : return self . client . get_card ( card_id )
def get_item_objects ( self , query_params = None ) : card = self . get_card ( ) checklistitems_list = [ ] for checklistitem_json in self . get_items ( query_params ) : checklistitems_list . append ( self . create_checklist_item ( card . id , self . id , checklistitem_json ) ) return checklistitems_list
def update_checklist ( self , name ) : checklist_json = self . fetch_json ( uri_path = self . base_uri , http_method = 'PUT' , query_params = { 'name' : name } ) return self . create_checklist ( checklist_json )
def remove_item ( self , item_id ) : return self . fetch_json ( uri_path = self . base_uri + '/checkItems/' + item_id , http_method = 'DELETE' )
def update_name ( self , name ) : checklistitem_json = self . fetch_json ( uri_path = self . base_uri + '/name' , http_method = 'PUT' , query_params = { 'value' : name } ) return self . create_checklist_item ( self . idCard , self . idChecklist , checklistitem_json )
def update_state ( self , state ) : checklistitem_json = self . fetch_json ( uri_path = self . base_uri + '/state' , http_method = 'PUT' , query_params = { 'value' : 'complete' if state else 'incomplete' } ) return self . create_checklist_item ( self . idCard , self . idChecklist , checklistitem_json )
def add_authorisation ( self , query_params ) : query_params [ 'key' ] = self . api_key if self . user_auth_token : query_params [ 'token' ] = self . user_auth_token return query_params
def check_errors ( self , uri , response ) : if response . status == 401 : raise trolly . Unauthorised ( uri , response ) if response . status != 200 : raise trolly . ResourceUnavailable ( uri , response )
def build_uri ( self , path , query_params ) : url = 'https://api.trello.com/1' + self . clean_path ( path ) url += '?' + urlencode ( query_params ) return url
def create_checklist_item ( self , card_id , checklist_id , checklistitem_json ) : return trolly . checklist . ChecklistItem ( trello_client = self , card_id = card_id , checklist_id = checklist_id , checklistitem_id = checklistitem_json [ 'id' ] . encode ( 'utf-8' ) , name = checklistitem_json [ 'name' ] . encode ( 'utf-8' ) , state = checklistitem_json [ 'state' ] . encode ( 'utf-8' ) )
def set_password ( self , service , username , password ) : assoc = self . _generate_assoc ( service , username ) password_encrypted = self . encrypt ( password . encode ( 'utf-8' ) , assoc ) password_base64 = '\n' + encodebytes ( password_encrypted ) . decode ( ) self . _write_config_value ( service , username , password_base64 )
def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] cli = CommandLineTool ( ) try : return cli . run ( argv ) except KeyboardInterrupt : print ( 'Canceled' ) return 3
def _create_cipher ( self , password , salt , nonce = None ) : from argon2 . low_level import hash_secret_raw , Type from Crypto . Cipher import AES aesmode = self . _get_mode ( self . aesmode ) if aesmode is None : raise ValueError ( 'invalid AES mode: %s' % self . aesmode ) key = hash_secret_raw ( secret = password . encode ( self . password_encoding ) , salt = salt , time_cost = self . time_cost , memory_cost = self . memory_cost , parallelism = self . parallelism , hash_len = 16 , type = Type . ID ) return AES . new ( key , aesmode , nonce )
def _get_mode ( mode = None ) : from Crypto . Cipher import AES AESModeMap = { 'CCM' : AES . MODE_CCM , 'EAX' : AES . MODE_EAX , 'GCM' : AES . MODE_GCM , 'OCB' : AES . MODE_OCB , } if mode is None : return AESModeMap . keys ( ) return AESModeMap . get ( mode )
def connectToBroker ( self , protocol ) : self . protocol = protocol self . protocol . onPublish = self . onPublish self . protocol . onDisconnection = self . onDisconnection self . protocol . setWindowSize ( 3 ) try : yield self . protocol . connect ( "TwistedMQTT-subs" , keepalive = 60 ) yield self . subscribe ( ) except Exception as e : log . error ( "Connecting to {broker} raised {excp!s}" , broker = BROKER , excp = e ) else : log . info ( "Connected and subscribed to {broker}" , broker = BROKER )
def onPublish ( self , topic , payload , qos , dup , retain , msgId ) : log . debug ( "msg={payload}" , payload = payload )
def connectToBroker ( self , protocol ) : self . protocol = protocol self . protocol . onPublish = self . onPublish self . protocol . onDisconnection = self . onDisconnection self . protocol . setWindowSize ( 3 ) self . task = task . LoopingCall ( self . publish ) self . task . start ( 5.0 , now = False ) try : yield self . protocol . connect ( "TwistedMQTT-pubsubs" , keepalive = 60 ) yield self . subscribe ( ) except Exception as e : log . error ( "Connecting to {broker} raised {excp!s}" , broker = BROKER , excp = e ) else : log . info ( "Connected and subscribed to {broker}" , broker = BROKER )
def makeId ( self ) : self . id = ( self . id + 1 ) % 65536 self . id = self . id or 1 return self . id
def connect ( self , request ) : state = self . __class__ . __name__ return defer . fail ( MQTTStateError ( "Unexpected connect() operation" , state ) )
def handleCONNACK ( self , response ) : state = self . __class__ . __name__ log . error ( "Unexpected {packet:7} packet received in {log_source}" , packet = "CONNACK" )
def encode ( self ) : header = bytearray ( 2 ) header [ 0 ] = 0xE0 self . encoded = header return str ( header ) if PY2 else bytes ( header )
def decode ( self , packet ) : self . encoded = packet lenLen = 1 while packet [ lenLen ] & 0x80 : lenLen += 1 packet_remaining = packet [ lenLen + 1 : ] version_str , packet_remaining = decodeString ( packet_remaining ) version_id = int ( packet_remaining [ 0 ] ) if version_id == v31 [ 'level' ] : self . version = v31 else : self . version = v311 flags = packet_remaining [ 1 ] self . cleanStart = ( flags & 0x02 ) != 0 willFlag = ( flags & 0x04 ) != 0 willQoS = ( flags >> 3 ) & 0x03 willRetain = ( flags & 0x20 ) != 0 userFlag = ( flags & 0x80 ) != 0 passFlag = ( flags & 0x40 ) != 0 packet_remaining = packet_remaining [ 2 : ] self . keepalive = decode16Int ( packet_remaining ) packet_remaining = packet_remaining [ 2 : ] self . clientId , packet_remaining = decodeString ( packet_remaining ) if willFlag : self . willRetain = willRetain self . willQoS = willQoS self . willTopic , packet_remaining = decodeString ( packet_remaining ) self . willMessage , packet_remaining = decodeString ( packet_remaining ) if userFlag : self . username , packet_remaining = decodeString ( packet_remaining ) if passFlag : l = decode16Int ( packet_remaining ) self . password = packet_remaining [ 2 : 2 + l ]
def encode ( self ) : header = bytearray ( 1 ) varHeader = bytearray ( 2 ) header [ 0 ] = 0x20 varHeader [ 0 ] = self . session varHeader [ 1 ] = self . resultCode header . extend ( encodeLength ( len ( varHeader ) ) ) header . extend ( varHeader ) self . encoded = header return str ( header ) if PY2 else bytes ( header )
def decode ( self , packet ) : self . encoded = packet lenLen = 1 while packet [ lenLen ] & 0x80 : lenLen += 1 packet_remaining = packet [ lenLen + 1 : ] self . session = ( packet_remaining [ 0 ] & 0x01 ) == 0x01 self . resultCode = int ( packet_remaining [ 1 ] )
def decode ( self , packet ) : self . encoded = packet lenLen = 1 while packet [ lenLen ] & 0x80 : lenLen += 1 packet_remaining = packet [ lenLen + 1 : ] self . msgId = decode16Int ( packet_remaining [ 0 : 2 ] ) self . topics = [ ] packet_remaining = packet_remaining [ 2 : ] while len ( packet_remaining ) : topic , packet_remaining = decodeString ( packet_remaining ) qos = int ( packet_remaining [ 0 ] ) & 0x03 self . topics . append ( ( topic , qos ) ) packet_remaining = packet_remaining [ 1 : ]
def encode ( self ) : header = bytearray ( 1 ) payload = bytearray ( ) varHeader = encode16Int ( self . msgId ) header [ 0 ] = 0x90 for code in self . granted : payload . append ( code [ 0 ] | ( 0x80 if code [ 1 ] == True else 0x00 ) ) header . extend ( encodeLength ( len ( varHeader ) + len ( payload ) ) ) header . extend ( varHeader ) header . extend ( payload ) self . encoded = header return str ( header ) if PY2 else bytes ( header )
def decode ( self , packet ) : self . encoded = packet lenLen = 1 while packet [ lenLen ] & 0x80 : lenLen += 1 packet_remaining = packet [ lenLen + 1 : ] self . msgId = decode16Int ( packet_remaining [ 0 : 2 ] ) self . topics = [ ] packet_remaining = packet_remaining [ 2 : ] while len ( packet_remaining ) : l = decode16Int ( packet_remaining [ 0 : 2 ] ) topic = packet_remaining [ 2 : 2 + l ] . decode ( encoding = 'utf-8' ) self . topics . append ( topic ) packet_remaining = packet_remaining [ 2 + l : ]
def encode ( self ) : header = bytearray ( 1 ) varHeader = encode16Int ( self . msgId ) header [ 0 ] = 0xB0 header . extend ( encodeLength ( len ( varHeader ) ) ) header . extend ( varHeader ) self . encoded = header return str ( header ) if PY2 else bytes ( header )
def decode ( self , packet ) : self . encoded = packet lenLen = 1 while packet [ lenLen ] & 0x80 : lenLen += 1 packet_remaining = packet [ lenLen + 1 : ] self . dup = ( packet [ 0 ] & 0x08 ) == 0x08 self . qos = ( packet [ 0 ] & 0x06 ) >> 1 self . retain = ( packet [ 0 ] & 0x01 ) == 0x01 self . topic , _ = decodeString ( packet_remaining ) topicLen = decode16Int ( packet_remaining ) if self . qos : self . msgId = decode16Int ( packet_remaining [ topicLen + 2 : topicLen + 4 ] ) self . payload = packet_remaining [ topicLen + 4 : ] else : self . msgId = None self . payload = packet_remaining [ topicLen + 2 : ]
def decode ( self , packet ) : self . encoded = packet lenLen = 1 while packet [ lenLen ] & 0x80 : lenLen += 1 packet_remaining = packet [ lenLen + 1 : ] self . msgId = decode16Int ( packet_remaining ) self . dup = ( packet [ 0 ] & 0x08 ) == 0x08
def refresh ( self ) : if self . comm . rank == 0 : self . _blocks = self . list_blocks ( ) else : self . _blocks = None self . _blocks = self . comm . bcast ( self . _blocks )
def get_total_time_span ( d ) : tmax = 0 for di in d . values ( ) : if di . uTime . max ( ) > tmax : tmax = di . uTime . max ( ) return tmax
def get_defined_srms ( srm_file ) : srms = read_table ( srm_file ) return np . asanyarray ( srms . index . unique ( ) )
def read_configuration ( config = 'DEFAULT' ) : _ , conf = read_latoolscfg ( ) if config == 'DEFAULT' : config = conf [ 'DEFAULT' ] [ 'config' ] conf = dict ( conf [ config ] ) conf [ 'config' ] = config return conf
def print_all ( ) : _ , conf = read_latoolscfg ( ) default = conf [ 'DEFAULT' ] [ 'config' ] pstr = '\nCurrently defined LAtools configurations:\n\n' for s in conf . sections ( ) : if s == default : pstr += s + ' [DEFAULT]\n' elif s == 'REPRODUCE' : pstr += s + ' [DO NOT ALTER]\n' else : pstr += s + '\n' for k , v in conf [ s ] . items ( ) : if k != 'config' : if v [ : 9 ] == 'resources' : v = pkgrs . resource_filename ( 'latools' , v ) pstr += '   ' + k + ': ' + v + '\n' pstr += '\n' print ( pstr ) return
def change_default ( config ) : config_file , cf = read_latoolscfg ( ) if config not in cf . sections ( ) : raise ValueError ( "\n'{:s}' is not a defined configuration." . format ( config ) ) if config == 'REPRODUCE' : pstr = ( 'Are you SURE you want to set REPRODUCE as your default configuration?\n' + '     ... this is an odd thing to be doing.' ) else : pstr = ( 'Are you sure you want to change the default configuration from {:s}' . format ( cf [ 'DEFAULT' ] [ 'config' ] ) + 'to {:s}?' . format ( config ) ) response = input ( pstr + '\n> [N/y]: ' ) if response . lower ( ) == 'y' : cf . set ( 'DEFAULT' , 'config' , config ) with open ( config_file , 'w' ) as f : cf . write ( f ) print ( '  Default changed!' ) else : print ( '  Done nothing.' )
def autorange_plot ( self , analyte = 'total_counts' , gwin = 7 , swin = None , win = 20 , on_mult = [ 1.5 , 1. ] , off_mult = [ 1. , 1.5 ] , transform = 'log' ) : if analyte is None : sig = self . data [ 'total_counts' ] elif analyte == 'total_counts' : sig = self . data [ 'total_counts' ] elif analyte in self . analytes : sig = self . focus [ analyte ] else : raise ValueError ( 'Invalid analyte.' ) if transform == 'log' : sig = np . log10 ( sig ) fig , axs = plot . autorange_plot ( t = self . Time , sig = sig , gwin = gwin , swin = swin , win = win , on_mult = on_mult , off_mult = off_mult ) return fig , axs
def rangecalc ( x , y = None , pad = 0.05 ) : mn = np . nanmin ( [ np . nanmin ( x ) , np . nanmin ( y ) ] ) mx = np . nanmax ( [ np . nanmax ( x ) , np . nanmax ( y ) ] ) rn = mx - mn return ( mn - pad * rn , mx + pad * rn )
def rangecalcx ( x , pad = 0.05 ) : mn = np . nanmin ( x ) mx = np . nanmax ( x ) rn = mx - mn return ( mn - pad * rn , mx + pad * rn )
def gen_keywords ( * args : Union [ ANSIColors , ANSIStyles ] , * * kwargs : Union [ ANSIColors , ANSIStyles ] ) -> tuple : fields : tuple = tuple ( ) values : tuple = tuple ( ) for tpl in args : fields += tpl . _fields values += tpl for prefix , tpl in kwargs . items ( ) : fields += tuple ( map ( lambda x : '_' . join ( [ prefix , x ] ) , tpl . _fields ) ) values += tpl return namedtuple ( 'ANSISequences' , fields ) ( * values )
def dedup ( stack : tuple ) -> tuple : reducer = lambda x , y : x if y in x else x + ( y , ) return reduce ( reducer , stack , tuple ( ) )
def stderr ( a ) : return np . nanstd ( a ) / np . sqrt ( sum ( np . isfinite ( a ) ) )
def filter_nremoved ( self , filt = True , quiet = False ) : rminfo = { } for n in self . subsets [ 'All_Samples' ] : s = self . data [ n ] rminfo [ n ] = s . filt_nremoved ( filt ) if not quiet : maxL = max ( [ len ( s ) for s in rminfo . keys ( ) ] ) print ( '{string:{number}s}' . format ( string = 'Sample ' , number = maxL + 3 ) + '{total:4s}' . format ( total = 'tot' ) + '{removed:4s}' . format ( removed = 'flt' ) + '{percent:4s}' . format ( percent = '%rm' ) ) for k , ( ntot , nfilt , pcrm ) in rminfo . items ( ) : print ( '{string:{number}s}' . format ( string = k , number = maxL + 3 ) + '{total:4.0f}' . format ( total = ntot ) + '{removed:4.0f}' . format ( removed = nfilt ) + '{percent:4.0f}' . format ( percent = pcrm ) ) return rminfo
def getstats ( self , save = True , filename = None , samples = None , subset = None , ablation_time = False ) : slst = [ ] if samples is not None : subset = self . make_subset ( samples ) samples = self . _get_samples ( subset ) for s in self . stats_calced : for nm in [ n for n in samples if self . srm_identifier not in n ] : if self . stats [ nm ] [ s ] . ndim == 2 : reps = np . arange ( self . stats [ nm ] [ s ] . shape [ - 1 ] ) ss = np . array ( [ s ] * reps . size ) nms = np . array ( [ nm ] * reps . size ) stdf = pd . DataFrame ( self . stats [ nm ] [ s ] . T , columns = self . stats [ nm ] [ 'analytes' ] , index = [ ss , nms , reps ] ) stdf . index . set_names ( [ 'statistic' , 'sample' , 'rep' ] , inplace = True ) else : stdf = pd . DataFrame ( self . stats [ nm ] [ s ] , index = self . stats [ nm ] [ 'analytes' ] , columns = [ [ s ] , [ nm ] ] ) . T stdf . index . set_names ( [ 'statistic' , 'sample' ] , inplace = True ) slst . append ( stdf ) out = pd . concat ( slst ) if ablation_time : ats = self . ablation_times ( samples = samples , subset = subset ) ats [ 'statistic' ] = 'nanmean' ats . set_index ( 'statistic' , append = True , inplace = True ) ats = ats . reorder_levels ( [ 'statistic' , 'sample' , 'rep' ] ) out = out . join ( ats ) out . drop ( self . internal_standard , 1 , inplace = True ) if save : if filename is None : filename = 'stat_export.csv' out . to_csv ( self . export_dir + '/' + filename ) self . stats_df = out return out
def _minimal_export_traces ( self , outdir = None , analytes = None , samples = None , subset = 'All_Analyses' ) : if analytes is None : analytes = self . analytes elif isinstance ( analytes , str ) : analytes = [ analytes ] if samples is not None : subset = self . make_subset ( samples ) samples = self . _get_samples ( subset ) focus_stage = 'rawdata' if not os . path . isdir ( outdir ) : os . mkdir ( outdir ) for s in samples : d = self . data [ s ] . data [ focus_stage ] out = Bunch ( ) for a in analytes : out [ a ] = d [ a ] out = pd . DataFrame ( out , index = self . data [ s ] . Time ) out . index . name = 'Time' d = dateutil . parser . parse ( self . data [ s ] . meta [ 'date' ] ) header = [ % ( time . strftime ( '%Y:%m:%d %H:%M:%S' ) ) , , , '#' , % ( s ) , + d . strftime ( '%Y-%m-%d %H:%M:%S' ) ] header = '\n' . join ( header ) + '\n' csv = out . to_csv ( ) with open ( '%s/%s.csv' % ( outdir , s ) , 'w' ) as f : f . write ( header ) f . write ( csv ) return
def save_log ( self , directory = None , logname = None , header = None ) : if directory is None : directory = self . export_dir if not os . path . isdir ( directory ) : directory = os . path . dirname ( directory ) if logname is None : logname = 'analysis.lalog' if header is None : header = self . _log_header ( ) loc = logging . write_logfile ( self . log , header , os . path . join ( directory , logname ) ) return loc
def calc_windows ( fn , s , min_points ) : max_points = np . sum ( ~ np . isnan ( s ) ) n_points = max_points - min_points out = np . full ( ( n_points , s . size ) , np . nan ) ind = ~ np . isnan ( s ) s = s [ ind ] for i , w in enumerate ( range ( min_points , s . size ) ) : r = rolling_window ( s , w , pad = np . nan ) out [ i , ind ] = np . apply_along_axis ( fn , 1 , r ) return out
def calc_window_mean_std ( s , min_points , ind = None ) : max_points = np . sum ( ~ np . isnan ( s ) ) n_points = max_points - min_points mean = np . full ( ( n_points , s . size ) , np . nan ) std = np . full ( ( n_points , s . size ) , np . nan ) if ind is None : ind = ~ np . isnan ( s ) else : ind = ind & ~ np . isnan ( s ) s = s [ ind ] for i , w in enumerate ( range ( min_points , s . size ) ) : r = rolling_window ( s , w , pad = np . nan ) mean [ i , ind ] = r . sum ( 1 ) / w std [ i , ind ] = ( ( ( r - mean [ i , ind ] [ : , np . newaxis ] ) ** 2 ) . sum ( 1 ) / ( w - 1 ) ) ** 0.5 return mean , std
def bayes_scale ( s ) : if sum ( ~ np . isnan ( s ) ) > 1 : bm , bv , bs = bayes_mvs ( s [ ~ np . isnan ( s ) ] ) return ( s - bm . statistic ) / bs . statistic else : return np . full ( s . shape , np . nan )
def median_scaler ( s ) : if sum ( ~ np . isnan ( s ) ) > 2 : ss = s [ ~ np . isnan ( s ) ] median = np . median ( ss ) IQR = np . diff ( np . percentile ( ss , [ 25 , 75 ] ) ) return ( s - median ) / IQR else : return np . full ( s . shape , np . nan )
def clear ( self ) : self . components = { } self . info = { } self . params = { } self . switches = { } self . keys = { } self . index = { } self . sets = { } self . maxset = - 1 self . n = 0 for a in self . analytes : self . switches [ a ] = { } return
def clean ( self ) : for f in sorted ( self . components . keys ( ) ) : unused = not any ( self . switches [ a ] [ f ] for a in self . analytes ) if unused : self . remove ( f )
def get_info ( self ) : out = '' for k in sorted ( self . components . keys ( ) ) : out += '{:s}: {:s}' . format ( k , self . info [ k ] ) + '\n' return ( out )
def _log ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : a = func ( self , * args , * * kwargs ) self . log . append ( func . __name__ + ' :: args={} kwargs={}' . format ( args , kwargs ) ) return a return wrapper
def autologin ( function , timeout = TIMEOUT ) : @ wraps ( function ) async def wrapper ( self , * args , * * kwargs ) : """Wrap a function with timeout.""" try : async with async_timeout . timeout ( timeout ) : return await function ( self , * args , * * kwargs ) except ( asyncio . TimeoutError , ClientError , Error ) : pass _LOGGER . debug ( "autologin" ) try : async with async_timeout . timeout ( timeout ) : await self . login ( ) return await function ( self , * args , * * kwargs ) except ( asyncio . TimeoutError , ClientError , Error ) : raise Error ( str ( function ) ) return wrapper
async def get_information ( ) : jar = aiohttp . CookieJar ( unsafe = True ) websession = aiohttp . ClientSession ( cookie_jar = jar ) modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) await modem . login ( password = sys . argv [ 2 ] ) result = await modem . information ( ) for sms in result . sms : pprint . pprint ( sms ) await modem . logout ( ) await websession . close ( )
async def send_message ( ) : jar = aiohttp . CookieJar ( unsafe = True ) websession = aiohttp . ClientSession ( cookie_jar = jar ) modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) await modem . login ( password = sys . argv [ 2 ] ) await modem . sms ( phone = sys . argv [ 3 ] , message = sys . argv [ 4 ] ) await modem . logout ( ) await websession . close ( )
async def get_information ( ) : jar = aiohttp . CookieJar ( unsafe = True ) websession = aiohttp . ClientSession ( cookie_jar = jar ) try : modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) await modem . login ( password = sys . argv [ 2 ] ) result = await modem . information ( ) print ( "upstream: {}" . format ( result . upstream ) ) print ( "serial_number: {}" . format ( result . serial_number ) ) print ( "wire_connected: {}" . format ( result . wire_connected ) ) print ( "mobile_connected: {}" . format ( result . mobile_connected ) ) print ( "connection_text: {}" . format ( result . connection_text ) ) print ( "connection_type: {}" . format ( result . connection_type ) ) print ( "current_nw_service_type: {}" . format ( result . current_nw_service_type ) ) print ( "current_ps_service_type: {}" . format ( result . current_ps_service_type ) ) print ( "register_network_display: {}" . format ( result . register_network_display ) ) print ( "roaming: {}" . format ( result . roaming ) ) print ( "radio_quality: {}" . format ( result . radio_quality ) ) print ( "rx_level: {}" . format ( result . rx_level ) ) print ( "tx_level: {}" . format ( result . tx_level ) ) print ( "current_band: {}" . format ( result . current_band ) ) print ( "cell_id: {}" . format ( result . cell_id ) ) await modem . logout ( ) except eternalegypt . Error : print ( "Could not login" ) await websession . close ( )
async def set_failover_mode ( mode ) : jar = aiohttp . CookieJar ( unsafe = True ) websession = aiohttp . ClientSession ( cookie_jar = jar ) try : modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) await modem . login ( password = sys . argv [ 2 ] ) await modem . set_failover_mode ( mode ) await modem . logout ( ) except eternalegypt . Error : print ( "Could not login" ) await websession . close ( )
def nbviewer_link ( url ) : if six . PY2 : from urlparse import urlparse as urlsplit else : from urllib . parse import urlsplit info = urlsplit ( url ) domain = info . netloc url_type = 'github' if domain == 'github.com' else 'url' return 'https://nbviewer.jupyter.org/%s%s' % ( url_type , info . path )
def thumbnail_div ( self ) : return self . THUMBNAIL_TEMPLATE . format ( snippet = self . get_description ( ) [ 1 ] , thumbnail = self . thumb_file , ref_name = self . reference )
def code_div ( self ) : code_example = self . code_example if code_example is None : return None return self . CODE_TEMPLATE . format ( snippet = self . get_description ( ) [ 1 ] , code = code_example , ref_name = self . reference )
def code_example ( self ) : if self . _code_example is not None : return self . _code_example return getattr ( self . nb . metadata , 'code_example' , None )
def supplementary_files ( self ) : if self . _supplementary_files is not None : return self . _supplementary_files return getattr ( self . nb . metadata , 'supplementary_files' , None )
def other_supplementary_files ( self ) : if self . _other_supplementary_files is not None : return self . _other_supplementary_files return getattr ( self . nb . metadata , 'other_supplementary_files' , None )
def url ( self ) : if self . _url is not None : url = self . _url else : url = getattr ( self . nb . metadata , 'url' , None ) if url is not None : return nbviewer_link ( url )
def get_out_file ( self , ending = 'rst' ) : return os . path . splitext ( self . outfile ) [ 0 ] + os . path . extsep + ending
def create_py ( self , nb , force = False ) : if list ( map ( int , re . findall ( '\d+' , nbconvert . __version__ ) ) ) >= [ 4 , 2 ] : py_file = os . path . basename ( self . py_file ) else : py_file = self . py_file try : level = logger . logger . level except AttributeError : level = logger . level spr . call ( [ 'jupyter' , 'nbconvert' , '--to=python' , '--output=' + py_file , '--log-level=%s' % level , self . outfile ] ) with open ( self . py_file ) as f : py_content = f . read ( ) py_content = re . sub ( '^\s*get_ipython\(\).magic.*' , , py_content , flags = re . MULTILINE ) with open ( self . py_file , 'w' ) as f : f . write ( py_content )
def data_download ( self , files ) : if len ( files ) > 1 : return self . DATA_DOWNLOAD % ( ( '\n\n' + ' ' * 8 ) + ( '\n' + ' ' * 8 ) . join ( '* :download:`%s`' % f for f in files ) ) return self . DATA_DOWNLOAD % ':download:`%s`' % files [ 0 ]
def create_thumb ( self ) : thumbnail_figure = self . copy_thumbnail_figure ( ) if thumbnail_figure is not None : if isinstance ( thumbnail_figure , six . string_types ) : pic = thumbnail_figure else : pic = self . pictures [ thumbnail_figure ] self . save_thumbnail ( pic ) else : for pic in self . pictures [ : : - 1 ] : if pic . endswith ( 'png' ) : self . save_thumbnail ( pic ) return
def get_description ( self ) : def split_header ( s , get_header = True ) : s = s . lstrip ( ) . rstrip ( ) parts = s . splitlines ( ) if parts [ 0 ] . startswith ( '#' ) : if get_header : header = re . sub ( '#+\s*' , '' , parts . pop ( 0 ) ) if not parts : return header , '' else : header = '' rest = '\n' . join ( parts ) . lstrip ( ) . split ( '\n\n' ) desc = rest [ 0 ] . replace ( '\n' , ' ' ) return header , desc else : if get_header : if parts [ 0 ] . startswith ( ( '=' , '-' ) ) : parts = parts [ 1 : ] header = parts . pop ( 0 ) if parts and parts [ 0 ] . startswith ( ( '=' , '-' ) ) : parts . pop ( 0 ) if not parts : return header , '' else : header = '' rest = '\n' . join ( parts ) . lstrip ( ) . split ( '\n\n' ) desc = rest [ 0 ] . replace ( '\n' , ' ' ) return header , desc first_cell = self . nb [ 'cells' ] [ 0 ] if not first_cell [ 'cell_type' ] == 'markdown' : return '' , '' header , desc = split_header ( first_cell [ 'source' ] ) if not desc and len ( self . nb [ 'cells' ] ) > 1 : second_cell = self . nb [ 'cells' ] [ 1 ] if second_cell [ 'cell_type' ] == 'markdown' : _ , desc = split_header ( second_cell [ 'source' ] , False ) return header , desc
def save_thumbnail ( self , image_path ) : thumb_dir = os . path . join ( os . path . dirname ( image_path ) , 'thumb' ) create_dirs ( thumb_dir ) thumb_file = os . path . join ( thumb_dir , '%s_thumb.png' % self . reference ) if os . path . exists ( image_path ) : logger . info ( 'Scaling %s to thumbnail %s' , image_path , thumb_file ) self . scale_image ( image_path , thumb_file , 400 , 280 ) self . thumb_file = thumb_file
def copy_thumbnail_figure ( self ) : ret = None if self . _thumbnail_figure is not None : if not isstring ( self . _thumbnail_figure ) : ret = self . _thumbnail_figure else : ret = osp . join ( osp . dirname ( self . outfile ) , osp . basename ( self . _thumbnail_figure ) ) copyfile ( self . _thumbnail_figure , ret ) return ret elif hasattr ( self . nb . metadata , 'thumbnail_figure' ) : if not isstring ( self . nb . metadata . thumbnail_figure ) : ret = self . nb . metadata . thumbnail_figure else : ret = osp . join ( osp . dirname ( self . outfile ) , 'images' , osp . basename ( self . nb . metadata . thumbnail_figure ) ) copyfile ( osp . join ( osp . dirname ( self . infile ) , self . nb . metadata . thumbnail_figure ) , ret ) return ret
def get_db_change_languages ( self , field_name , db_table_fields ) : for lang_code , lang_name in get_languages ( ) : if get_real_fieldname ( field_name , lang_code ) not in db_table_fields : yield lang_code for db_table_field in db_table_fields : pattern = re . compile ( '^%s_(?P<lang>\w{2})$' % field_name ) m = pattern . match ( db_table_field ) if not m : continue lang = m . group ( 'lang' ) yield lang
def pre_save ( self , model_instance , add ) : file = getattr ( model_instance , self . attname ) if file and not file . _committed : image_file = file if self . resize_source_to : file . seek ( 0 ) image_file = processors . process ( file , self . resize_source_to ) image_file = post_processors . process ( image_file , self . resize_source_to ) filename = str ( shortuuid . uuid ( ) ) + os . path . splitext ( file . name ) [ 1 ] file . save ( filename , image_file , save = False ) return file
def _refresh_cache ( self ) : self . _thumbnails = { } metadatas = self . metadata_backend . get_thumbnails ( self . source_image . name ) for metadata in metadatas : self . _thumbnails [ metadata . size ] = Thumbnail ( metadata = metadata , storage = self . storage )
def all ( self ) : if self . _thumbnails is not None : return self . _thumbnails self . _refresh_cache ( ) return self . _thumbnails
def create ( self , size ) : thumbnail = images . create ( self . source_image . name , size , self . metadata_backend , self . storage ) return thumbnail
def delete ( self , size ) : images . delete ( self . source_image . name , size , self . metadata_backend , self . storage ) del ( self . _thumbnails [ size ] )
def get ( source_name , size , metadata_backend = None , storage_backend = None ) : if storage_backend is None : storage_backend = backends . storage . get_backend ( ) if metadata_backend is None : metadata_backend = backends . metadata . get_backend ( ) metadata = metadata_backend . get_thumbnail ( source_name , size ) if metadata is None : return None else : return Thumbnail ( metadata = metadata , storage = storage_backend )
def delete ( source_name , size , metadata_backend = None , storage_backend = None ) : if storage_backend is None : storage_backend = backends . storage . get_backend ( ) if metadata_backend is None : metadata_backend = backends . metadata . get_backend ( ) storage_backend . delete ( get_thumbnail_name ( source_name , size ) ) metadata_backend . delete_thumbnail ( source_name , size )
def jsonex_api ( f ) : @ wraps ( f ) def wrapper ( * args , * * kwargs ) : try : code , res = 200 , f ( * args , * * kwargs ) except HTTPException as e : code , res = e . code , { 'error' : e } except Exception as e : code , res = 500 , { 'error' : e } logger . exception ( 'Method error' ) response = make_response ( jsonex_dumps ( res ) , code ) response . headers [ 'Content-Type' ] = 'application/json' return response return wrapper
def estimate_tx_gas_with_web3 ( self , safe_address : str , to : str , value : int , data : bytes ) -> int : return self . ethereum_client . estimate_gas ( safe_address , to , value , data , block_identifier = 'pending' )
def has_bad_headers ( self , default_from = None ) : sender = self . sender or default_from reply_to = self . reply_to or '' for val in [ self . subject , sender , reply_to ] + self . recipients : for c in '\r\n' : if c in val : return True return False
def from_module ( module_name ) : d = importlib . import_module ( module_name ) config = { } for key in dir ( d ) : if key . isupper ( ) : config [ key ] = getattr ( d , key ) return Config ( config )
def register_resources ( self , * * resources ) : for key , resource in resources . items ( ) : if key in self . _resources : raise AlreadyExistsException ( 'A Service for {} is already registered.' . format ( key ) ) self . _init_resource ( key , resource )
def require ( self , key ) : value = self . get ( key ) if not value : raise ValueError ( '"{}" is empty.' . format ( key ) ) return value
def _exit ( self , obj , type , value , traceback ) : if type is None : try : obj . next ( ) except StopIteration : return else : raise RuntimeError ( '{} yielded more than once.' . format ( obj ) ) else : try : obj . throw ( type , value , traceback ) raise RuntimeError ( '{} did not close after throw()' . format ( obj ) ) except StopIteration as exc : return exc is not value except : # if sys . exc_info ( ) [ 1 ] is not value : raise
def samefile ( path1 , path2 ) : info1 = fs . getfileinfo ( path1 ) info2 = fs . getfileinfo ( path2 ) return ( info1 . dwVolumeSerialNumber == info2 . dwVolumeSerialNumber and info1 . nFileIndexHigh == info2 . nFileIndexHigh and info1 . nFileIndexLow == info2 . nFileIndexLow )
def create ( source , link_name ) : success = False if not os . path . isdir ( source ) : raise Exception ( "%s is not a directory" % source ) if os . path . exists ( link_name ) : raise Exception ( "%s: junction link name already exists" % link_name ) link_name = os . path . abspath ( link_name ) os . mkdir ( link_name ) hlink = CreateFile ( link_name , fs . GENERIC_WRITE , fs . FILE_SHARE_READ | fs . FILE_SHARE_WRITE , None , fs . OPEN_EXISTING , fs . FILE_FLAG_OPEN_REPARSE_POINT | fs . FILE_FLAG_BACKUP_SEMANTICS , None ) try : if hlink == fs . INVALID_HANDLE_VALUE : raise WinError ( ) srcvolpath = unparsed_convert ( source ) ( junctioninfo , infolen ) = new_junction_reparse_buffer ( srcvolpath ) dummy = DWORD ( 0 ) res = DeviceIoControl ( hlink , FSCTL_SET_REPARSE_POINT , byref ( junctioninfo ) , infolen , None , 0 , byref ( dummy ) , None ) if res == 0 : raise WinError ( ) success = True finally : if hlink != fs . INVALID_HANDLE_VALUE : CloseHandle ( hlink ) if not success : os . rmdir ( link_name )
def initialize_logger ( args ) : global log_filename log_filename = os . path . join ( os . getcwd ( ) , "jacquard.log" ) if args . log_file : _validate_log_file ( args . log_file ) log_filename = args . log_file logging . basicConfig ( format = _FILE_LOG_FORMAT , level = "DEBUG" , datefmt = _DATE_FORMAT , filename = log_filename ) global _verbose if args . verbose : _verbose = args . verbose start_time = datetime . now ( ) . strftime ( _DATE_FORMAT ) global _logging_dict _logging_dict = { 'user' : getpass . getuser ( ) , 'host' : socket . gethostname ( ) , 'start_time' : start_time , 'tool' : args . subparser_name }
def error ( self , message ) : message = self . _remessage_invalid_subparser ( message ) raise utils . UsageError ( message )
def read ( * paths ) : with open ( os . path . join ( * paths ) , 'r' ) as filename : return filename . read ( )
def prefix_line_terminator ( self , data ) : for t in self . LINE_TERMINATORS : if data . startswith ( t ) : return t return None
def suffix_line_terminator ( self , data ) : for t in self . LINE_TERMINATORS : if data . endswith ( t ) : return t return None
def tail ( self , lines = 10 ) : self . file . seek ( 0 , SEEK_END ) for i in range ( lines ) : if self . seek_previous_line ( ) == - 1 : break data = self . file . read ( ) for t in self . LINE_TERMINATORS : if data . endswith ( t ) : data = data [ : - len ( t ) ] break if data : return self . splitlines ( data ) else : return [ ]
def head ( self , lines = 10 ) : self . file . seek ( 0 ) for i in range ( lines ) : if self . seek_next_line ( ) == - 1 : break end_pos = self . file . tell ( ) self . file . seek ( 0 ) data = self . file . read ( end_pos ) for t in self . LINE_TERMINATORS : if data . endswith ( t ) : data = data [ : - len ( t ) ] break if data : return self . splitlines ( data ) else : return [ ]
def format_tags ( self ) : tags = VcfRecord . _EMPTY_SET if self . sample_tag_values : first_sample = list ( self . sample_tag_values . keys ( ) ) [ 0 ] tags = set ( self . sample_tag_values [ first_sample ] . keys ( ) ) return tags
def _join_info_fields ( self ) : if self . info_dict : info_fields = [ ] if len ( self . info_dict ) > 1 : self . info_dict . pop ( "." , None ) for field , value in self . info_dict . items ( ) : if field == value : info_fields . append ( value ) else : info_fields . append ( "=" . join ( [ field , value ] ) ) self . info = ";" . join ( info_fields ) else : self . info = "."
def _format_field ( self ) : format_field = "." if self . sample_tag_values : first_sample = list ( self . sample_tag_values . keys ( ) ) [ 0 ] tag_names = self . sample_tag_values [ first_sample ] . keys ( ) if tag_names : format_field = ":" . join ( tag_names ) return format_field
def text ( self ) : stringifier = [ self . chrom , self . pos , self . vcf_id , self . ref , self . alt , self . qual , self . filter , self . info , self . _format_field ( ) ] for sample in self . sample_tag_values : stringifier . append ( self . _sample_field ( sample ) ) return "\t" . join ( stringifier ) + "\n"
def add_or_replace_filter ( self , new_filter ) : if self . filter . lower ( ) in self . _FILTERS_TO_REPLACE : self . filter = new_filter elif new_filter not in self . filter . split ( ";" ) : self . filter = ";" . join ( [ self . filter , new_filter ] )
def add_product_error ( self , product , error ) : self . add_error ( self . field_name ( product ) , error )
def model_fields_form_factory ( model ) : fields = model . _meta . get_fields ( ) choices = [ ] for field in fields : if hasattr ( field , "verbose_name" ) : choices . append ( ( field . name , field . verbose_name ) ) class ModelFieldsForm ( forms . Form ) : fields = forms . MultipleChoiceField ( choices = choices , required = False , ) return ModelFieldsForm
def items_pending_or_purchased ( self ) : status = [ commerce . Cart . STATUS_PAID , commerce . Cart . STATUS_ACTIVE ] return self . _items ( status )
def iter_osm_notes ( feed_limit = 25 , interval = 60 , parse_timestamps = True ) : last_seen_guid = None while True : u = urllib2 . urlopen ( 'https://www.openstreetmap.org/api/0.6/notes/feed?limit=%d' % feed_limit ) tree = etree . parse ( u ) new_notes = [ ] for note_item in tree . xpath ( '/rss/channel/item' ) : title = note_item . xpath ( 'title' ) [ 0 ] . text if title . startswith ( 'new note (' ) : action = 'create' elif title . startswith ( 'new comment (' ) : action = 'comment' elif title . startswith ( 'closed note (' ) : action = 'close' guid = note_item . xpath ( 'link' ) [ 0 ] . text if last_seen_guid == guid : break elif last_seen_guid == None : last_seen_guid = guid else : note_id = int ( guid . split ( '/' ) [ - 1 ] . split ( '#c' ) [ 0 ] ) new_notes . append ( ( action , get_note ( note_id , parse_timestamps ) ) ) for note in reversed ( new_notes ) : yield note yield model . Finished ( None , None ) time . sleep ( interval )
def passes_filter ( self , user ) : cls = type ( self . condition ) qs = cls . objects . filter ( pk = self . condition . id ) return self . condition in self . pre_filter ( qs , user )
def apply_voucher ( self , voucher_code ) : voucher = inventory . Voucher . objects . get ( code = voucher_code . upper ( ) ) if voucher in self . cart . vouchers . all ( ) : return self . _test_voucher ( voucher ) self . cart . vouchers . add ( voucher )
def _recalculate_discounts ( self ) : commerce . DiscountItem . objects . filter ( cart = self . cart ) . delete ( ) product_items = self . cart . productitem_set . all ( ) . select_related ( "product" , "product__category" ) . order_by ( "-product__price" ) products = [ i . product for i in product_items ] discounts = DiscountController . available_discounts ( self . cart . user , [ ] , products , ) for item in product_items : self . _add_discount ( item . product , item . quantity , discounts )
def rows ( self , content_type ) : for row in self . _data : yield [ self . cell_text ( content_type , i , cell ) for i , cell in enumerate ( row ) ]
def get_form ( self , request ) : if self . form_type is not None : form = self . form_type ( request . GET ) form . is_valid ( ) else : form = None return form
def reports_list ( request ) : reports = [ ] for report in get_all_reports ( ) : reports . append ( { "name" : report . __name__ , "url" : reverse ( report ) , "description" : report . __doc__ , } ) reports . sort ( key = lambda report : report [ "name" ] ) ctx = { "reports" : reports , } return render ( request , "registrasion/reports_list.html" , ctx )
def sales_payment_summary ( ) : def value_or_zero ( aggregate , key ) : return aggregate [ key ] or 0 def sum_amount ( payment_set ) : a = payment_set . values ( "amount" ) . aggregate ( total = Sum ( "amount" ) ) return value_or_zero ( a , "total" ) headings = [ "Category" , "Total" ] data = [ ] sales = commerce . LineItem . objects . filter ( invoice__status = commerce . Invoice . STATUS_PAID , ) . values ( "price" , "quantity" ) . aggregate ( total = Sum ( F ( "price" ) * F ( "quantity" ) , output_field = CURRENCY ( ) ) , ) sales = value_or_zero ( sales , "total" ) all_payments = sum_amount ( commerce . PaymentBase . objects . all ( ) ) all_credit_notes = 0 - sum_amount ( commerce . CreditNote . objects . all ( ) ) unclaimed_credit_notes = 0 - sum_amount ( commerce . CreditNote . unclaimed ( ) ) claimed_credit_notes = sum_amount ( commerce . CreditNoteApplication . objects . all ( ) ) refunded_credit_notes = 0 - sum_amount ( commerce . CreditNote . refunded ( ) ) data . append ( [ "Items on paid invoices" , sales ] ) data . append ( [ "All payments" , all_payments ] ) data . append ( [ "Sales - Payments " , sales - all_payments ] ) data . append ( [ "All credit notes" , all_credit_notes ] ) data . append ( [ "Credit notes paid on invoices" , claimed_credit_notes ] ) data . append ( [ "Credit notes refunded" , refunded_credit_notes ] ) data . append ( [ "Unclaimed credit notes" , unclaimed_credit_notes ] ) data . append ( [ "Credit notes - (claimed credit notes + unclaimed credit notes)" , all_credit_notes - claimed_credit_notes - refunded_credit_notes - unclaimed_credit_notes ] ) return ListReport ( "Sales and Payments Summary" , headings , data )
def payments ( ) : payments = commerce . PaymentBase . objects . all ( ) return QuerysetReport ( "Payments" , [ "invoice__id" , "id" , "reference" , "amount" ] , payments , link_view = views . invoice , )
def credit_note_refunds ( ) : notes_refunded = commerce . CreditNote . refunded ( ) return QuerysetReport ( "Credit note refunds" , [ "id" , "creditnoterefund__reference" , "amount" ] , notes_refunded , link_view = views . credit_note , )
def discount_status ( request , form ) : discounts = form . cleaned_data [ "discount" ] items = commerce . DiscountItem . objects . filter ( Q ( discount__in = discounts ) , ) . select_related ( "cart" , "product" , "product__category" ) items = group_by_cart_status ( items , [ "discount" ] , [ "discount" , "discount__description" ] , ) headings = [ "Discount" , "Paid" , "Reserved" , "Unreserved" , "Refunded" , ] data = [ ] for item in items : data . append ( [ item [ "discount__description" ] , item [ "total_paid" ] , item [ "total_reserved" ] , item [ "total_unreserved" ] , item [ "total_refunded" ] , ] ) return ListReport ( "Usage by item" , headings , data )
def credit_notes ( request , form ) : notes = commerce . CreditNote . objects . all ( ) . select_related ( "creditnoterefund" , "creditnoteapplication" , "invoice" , "invoice__user__attendee__attendeeprofilebase" , ) return QuerysetReport ( "Credit Notes" , [ "id" , "invoice__user__attendee__attendeeprofilebase__invoice_recipient" , "status" , "value" ] , notes , headings = [ "id" , "Owner" , "Status" , "Value" ] , link_view = views . credit_note , )
def invoices ( request , form ) : invoices = commerce . Invoice . objects . all ( ) . order_by ( "status" , "id" ) return QuerysetReport ( "Invoices" , [ "id" , "recipient" , "value" , "get_status_display" ] , invoices , headings = [ "id" , "Recipient" , "Value" , "Status" ] , link_view = views . invoice , )
def attendee_list ( request ) : attendees = people . Attendee . objects . select_related ( "attendeeprofilebase" , "user" , ) profiles = AttendeeProfile . objects . filter ( attendee__in = attendees ) . select_related ( "attendee" , "attendee__user" , ) profiles_by_attendee = dict ( ( i . attendee , i ) for i in profiles ) attendees = attendees . annotate ( has_registered = Count ( Q ( user__invoice__status = commerce . Invoice . STATUS_PAID ) ) , ) headings = [ "User ID" , "Name" , "Email" , "Has registered" , ] data = [ ] for a in attendees : data . append ( [ a . user . id , ( profiles_by_attendee [ a ] . attendee_name ( ) if a in profiles_by_attendee else "" ) , a . user . email , a . has_registered > 0 , ] ) data . sort ( key = lambda a : ( - a [ 3 ] , a [ 0 ] ) ) return AttendeeListReport ( "Attendees" , headings , data , link_view = attendee )
def speaker_registrations ( request , form ) : kinds = form . cleaned_data [ "kind" ] presentations = schedule_models . Presentation . objects . filter ( proposal_base__kind__in = kinds , ) . exclude ( cancelled = True , ) users = User . objects . filter ( Q ( speaker_profile__presentations__in = presentations ) | Q ( speaker_profile__copresentations__in = presentations ) ) paid_carts = commerce . Cart . objects . filter ( status = commerce . Cart . STATUS_PAID ) paid_carts = Case ( When ( cart__in = paid_carts , then = Value ( 1 ) ) , default = Value ( 0 ) , output_field = models . IntegerField ( ) , ) users = users . annotate ( paid_carts = Sum ( paid_carts ) ) users = users . order_by ( "paid_carts" ) return QuerysetReport ( "Speaker Registration Status" , [ "id" , "speaker_profile__name" , "email" , "paid_carts" ] , users , link_view = attendee , ) return [ ]
def missing_categories ( context ) : user = user_for_context ( context ) categories_available = set ( CategoryController . available_categories ( user ) ) items = ItemController ( user ) . items_pending_or_purchased ( ) categories_held = set ( ) for product , quantity in items : categories_held . add ( product . category ) return categories_available - categories_held
def voucher_code ( request ) : VOUCHERS_FORM_PREFIX = "vouchers" v = _handle_voucher ( request , VOUCHERS_FORM_PREFIX ) voucher_form , voucher_handled = v if voucher_handled : messages . success ( request , "Your voucher code was accepted." ) return redirect ( "dashboard" ) data = { "voucher_form" : voucher_form , } return render ( request , "registrasion/voucher_code.html" , data )
def amend_registration ( request , user_id ) : user = User . objects . get ( id = int ( user_id ) ) current_cart = CartController . for_user ( user ) items = commerce . ProductItem . objects . filter ( cart = current_cart . cart , ) . select_related ( "product" ) initial = [ { "product" : i . product , "quantity" : i . quantity } for i in items ] StaffProductsFormSet = forms . staff_products_formset_factory ( user ) formset = StaffProductsFormSet ( request . POST or None , initial = initial , prefix = "products" , ) for item , form in zip ( items , formset ) : queryset = inventory . Product . objects . filter ( id = item . product . id ) form . fields [ "product" ] . queryset = queryset voucher_form = forms . VoucherForm ( request . POST or None , prefix = "voucher" , ) if request . POST and formset . is_valid ( ) : pq = [ ( f . cleaned_data [ "product" ] , f . cleaned_data [ "quantity" ] ) for f in formset if "product" in f . cleaned_data and f . cleaned_data [ "product" ] is not None ] try : current_cart . set_quantities ( pq ) return redirect ( amend_registration , user_id ) except ValidationError as ve : for ve_field in ve . error_list : product , message = ve_field . message for form in formset : if "product" not in form . cleaned_data : continue if form . cleaned_data [ "product" ] == product : form . add_error ( "quantity" , message ) if request . POST and voucher_form . has_changed ( ) and voucher_form . is_valid ( ) : try : current_cart . apply_voucher ( voucher_form . cleaned_data [ "voucher" ] ) return redirect ( amend_registration , user_id ) except ValidationError as ve : voucher_form . add_error ( None , ve ) ic = ItemController ( user ) data = { "user" : user , "paid" : ic . items_purchased ( ) , "cancelled" : ic . items_released ( ) , "form" : formset , "voucher_form" : voucher_form , } return render ( request , "registrasion/amend_registration.html" , data )
def extend_reservation ( request , user_id , days = 7 ) : user = User . objects . get ( id = int ( user_id ) ) cart = CartController . for_user ( user ) cart . extend_reservation ( datetime . timedelta ( days = days ) ) return redirect ( request . META [ "HTTP_REFERER" ] )
def invoice_mailout ( request ) : category = request . GET . getlist ( "category" , [ ] ) product = request . GET . getlist ( "product" , [ ] ) status = request . GET . get ( "status" ) form = forms . InvoiceEmailForm ( request . POST or None , category = category , product = product , status = status , ) emails = [ ] if form . is_valid ( ) : emails = [ ] for invoice in form . cleaned_data [ "invoice" ] : from_email = form . cleaned_data [ "from_email" ] subject = form . cleaned_data [ "subject" ] body = Template ( form . cleaned_data [ "body" ] ) . render ( Context ( { "invoice" : invoice , "user" : invoice . user , } ) ) recipient_list = [ invoice . user . email ] emails . append ( Email ( subject , body , from_email , recipient_list ) ) if form . cleaned_data [ "action" ] == forms . InvoiceEmailForm . ACTION_SEND : send_mass_mail ( emails ) messages . info ( request , "The e-mails have been sent." ) data = { "form" : form , "emails" : emails , } return render ( request , "registrasion/invoice_mailout.html" , data )
def render_badge ( user ) : data = { "user" : user , } t = loader . get_template ( 'registrasion/badge.svg' ) return t . render ( data )
def _generate_from_cart ( cls , cart ) : cart . refresh_from_db ( ) product_items = commerce . ProductItem . objects . filter ( cart = cart ) product_items = product_items . select_related ( "product" , "product__category" , ) product_items = product_items . order_by ( "product__category__order" , "product__order" ) if len ( product_items ) == 0 : raise ValidationError ( "Your cart is empty." ) discount_items = commerce . DiscountItem . objects . filter ( cart = cart ) discount_items = discount_items . select_related ( "discount" , "product" , "product__category" , ) def format_product ( product ) : return "%s - %s" % ( product . category . name , product . name ) def format_discount ( discount , product ) : description = discount . description return "%s (%s)" % ( description , format_product ( product ) ) line_items = [ ] for item in product_items : product = item . product line_item = commerce . LineItem ( description = format_product ( product ) , quantity = item . quantity , price = product . price , product = product , ) line_items . append ( line_item ) for item in discount_items : line_item = commerce . LineItem ( description = format_discount ( item . discount , item . product ) , quantity = item . quantity , price = cls . resolve_discount_value ( item ) * - 1 , product = item . product , ) line_items . append ( line_item ) min_due_time = cart . reservation_duration + cart . time_last_updated return cls . _generate ( cart . user , cart , min_due_time , line_items )
def _apply_credit_notes ( cls , invoice ) : invoices = commerce . Invoice . objects . filter ( user = invoice . user , status = commerce . Invoice . STATUS_UNPAID , ) if invoices . count ( ) > 1 : return notes = commerce . CreditNote . unclaimed ( ) . filter ( invoice__user = invoice . user ) for note in notes : try : CreditNoteController ( note ) . apply_to_invoice ( invoice ) except ValidationError : break invoice . refresh_from_db ( )
def _refresh ( self ) : self . invoice . refresh_from_db ( ) if self . invoice . cart : self . invoice . cart . refresh_from_db ( )
def void ( self ) : if self . invoice . total_payments ( ) > 0 : raise ValidationError ( "Invoices with payments must be refunded." ) elif self . invoice . is_refunded : raise ValidationError ( "Refunded invoices may not be voided." ) if self . invoice . is_paid : self . _release_cart ( ) self . _mark_void ( )
def update ( self , data ) : fields = [ 'id' , 'status' , 'type' , 'persistence' , 'date_start' , 'date_finish' , 'date_created' , 'date_modified' , 'checksum' , 'processor_name' , 'input' , 'input_schema' , 'output' , 'output_schema' , 'static' , 'static_schema' , 'var' , 'var_template' , ] self . annotation = { } for f in fields : setattr ( self , f , data [ f ] ) self . name = data [ 'static' ] [ 'name' ] if 'name' in data [ 'static' ] else '' self . annotation . update ( self . _flatten_field ( data [ 'input' ] , data [ 'input_schema' ] , 'input' ) ) self . annotation . update ( self . _flatten_field ( data [ 'output' ] , data [ 'output_schema' ] , 'output' ) ) self . annotation . update ( self . _flatten_field ( data [ 'static' ] , data [ 'static_schema' ] , 'static' ) ) self . annotation . update ( self . _flatten_field ( data [ 'var' ] , data [ 'var_template' ] , 'var' ) )
def _flatten_field ( self , field , schema , path ) : flat = { } for field_schema , fields , path in iterate_schema ( field , schema , path ) : name = field_schema [ 'name' ] typ = field_schema [ 'type' ] label = field_schema [ 'label' ] value = fields [ name ] if name in fields else None flat [ path ] = { 'name' : name , 'value' : value , 'type' : typ , 'label' : label } return flat
def print_downloads ( self ) : for path , ann in self . annotation . items ( ) : if path . startswith ( 'output' ) and ann [ 'type' ] == 'basic:file:' : print ( "{}: {}" . format ( path , ann [ 'value' ] [ 'file' ] ) )
def data ( self , * * query ) : objects = self . cache [ 'objects' ] data = self . api . data . get ( * * query ) [ 'objects' ] data_objects = [ ] for d in data : _id = d [ 'id' ] if _id in objects : objects [ _id ] . update ( d ) else : objects [ _id ] = GenData ( d , self ) data_objects . append ( objects [ _id ] ) for d in data_objects : count += 1 while True : ref_annotation = { } remove_annotation = [ ] for path , ann in d . annotation . items ( ) : if ann [ 'type' ] . startswith ( 'data:' ) : _id = ann [ 'value' ] if _id not in objects : try : d_tmp = self . api . data ( _id ) . get ( ) except slumber . exceptions . HttpClientError as ex : if ex . response . status_code == 404 : continue else : raise ex objects [ _id ] = GenData ( d_tmp , self ) annotation = objects [ _id ] . annotation ref_annotation . update ( { path + '.' + k : v for k , v in annotation . items ( ) } ) remove_annotation . append ( path ) if ref_annotation : d . annotation . update ( ref_annotation ) for path in remove_annotation : del d . annotation [ path ] else : break return data_objects
def rundata ( self , strjson ) : d = json . loads ( strjson ) return self . api . data . post ( d )
def get_subclasses ( c ) : subclasses = c . __subclasses__ ( ) for d in list ( subclasses ) : subclasses . extend ( get_subclasses ( d ) ) return subclasses
def get_repo_and_project ( self ) : app = self . app repo = app . data . apply ( 'github-repo' , app . args . github_repo , app . prompt_repo , on_load = app . github . get_repo , on_save = lambda r : r . id ) assert repo , "repository not found." project = app . data . apply ( 'asana-project' , app . args . asana_project , app . prompt_project , on_load = app . asana . projects . find_by_id , on_save = lambda p : p [ 'id' ] ) assert project , "project not found." first_issue = app . data . apply ( 'first-issue' , app . args . first_issue , "set the first issue to sync with [1 for new repos]" , on_save = int ) assert first_issue assert first_issue >= 0 , "issue must be positive" app . sync_data ( ) return repo , project
def get_variant_phenotypes_with_suggested_changes ( variant_id_list ) : variants = civic . get_variants_by_ids ( variant_id_list ) evidence = list ( ) for variant in variants : evidence . extend ( variant . evidence ) for e in evidence : suggested_changes_url = f'https://civicdb.org/api/evidence_items/{e.id}/suggested_changes' resp = requests . get ( suggested_changes_url ) resp . raise_for_status ( ) suggested_changes = dict ( ) for suggested_change in resp . json ( ) : pheno_changes = suggested_change [ 'suggested_changes' ] . get ( 'phenotype_ids' , None ) if pheno_changes is None : continue a , b = pheno_changes added = set ( b ) - set ( a ) deleted = set ( a ) - set ( b ) rid = suggested_change [ 'id' ] suggested_changes [ rid ] = { 'added' : added , 'deleted' : deleted } yield e , { 'suggested_changes' : suggested_changes , 'current' : set ( [ x . id for x in e . phenotypes ] ) }
def get_variant_phenotypes_with_suggested_changes_merged ( variant_id_list ) : for evidence , phenotype_status in get_variant_phenotypes_with_suggested_changes ( variant_id_list ) : final = phenotype_status [ 'current' ] for rid in sorted ( phenotype_status [ 'suggested_changes' ] ) : changes = phenotype_status [ 'suggested_changes' ] [ rid ] final = final - changes [ 'deleted' ] final = final | changes [ 'added' ] if final : yield evidence , final
def update ( self , allow_partial = True , force = False , * * kwargs ) : if kwargs : self . __init__ ( partial = allow_partial , force = force , * * kwargs ) return not self . _partial if not force and CACHE . get ( hash ( self ) ) : cached = CACHE [ hash ( self ) ] for field in self . _SIMPLE_FIELDS | self . _COMPLEX_FIELDS : v = getattr ( cached , field ) setattr ( self , field , v ) self . _partial = False logging . info ( f'Loading {str(self)} from cache' ) return True resp_dict = element_lookup_by_id ( self . type , self . id ) self . __init__ ( partial = False , * * resp_dict ) return True
def uniqify ( cls , seq ) : seen = set ( ) seen_add = seen . add return [ x for x in seq if x not in seen and not seen_add ( x ) ]
def authenticate ( self ) : if self . oauth : return False self . settings . apply ( 'api-asana' , self . args . asana_api , "enter asana api key" ) self . settings . apply ( 'api-github' , self . args . github_api , "enter github.com token" ) logging . debug ( "authenticating asana api." ) self . asana = Client . basic_auth ( self . settings [ 'api-asana' ] ) self . asana_errors = asana_errors self . asana_me = self . asana . users . me ( ) logging . debug ( "authenticating github api" ) self . github = Github ( self . settings [ 'api-github' ] ) self . github_user = self . github . get_user ( ) self . oauth = True
def _list_select ( cls , lst , prompt , offset = 0 ) : inp = raw_input ( "select %s: " % prompt ) assert inp , "value required." try : return lst [ int ( inp ) + offset ] except ValueError : return inp except IndexError : assert False , "bad value."
def move_saved_issue_data ( self , issue , ns , other_ns ) : if isinstance ( issue , int ) : issue_number = str ( issue ) elif isinstance ( issue , basestring ) : issue_number = issue else : issue_number = issue . number issue_data_key = self . _issue_data_key ( ns ) other_issue_data_key = self . _issue_data_key ( other_ns ) issue_data = self . data . get ( issue_data_key , { } ) other_issue_data = self . data . get ( other_issue_data_key , { } ) _id = issue_data . pop ( issue_number , None ) if _id : other_issue_data [ issue_number ] = _id self . data [ other_issue_data_key ] = other_issue_data self . data [ issue_data_key ] = issue_data
def get_asana_task ( self , asana_task_id ) : try : return self . asana . tasks . find_by_id ( asana_task_id ) except asana_errors . NotFoundError : return None except asana_errors . ForbiddenError : return None
def transport_task ( func ) : def wrapped_func ( * args , * * kwargs ) : tries = 0 while True : try : try : return func ( * args , * * kwargs ) except ( asana_errors . InvalidRequestError , asana_errors . NotFoundError ) , exc : logging . warn ( "warning: invalid request: %r" , exc ) except asana_errors . ForbiddenError , exc : logging . warn ( "forbidden error: %r" , exc ) except asana_errors . NotFoundError , exc : logging . warn ( "not found error: %r" , exc ) return None except asana_errors . RetryableAsanaError , retry_exc : tries += 1 logging . warn ( "retry exception %r on try %d" , retry_exc , tries ) if tries >= 3 : raise except Exception , exc : logging . exception ( "Exception in transport." ) return return wrapped_func
def flush ( callback = None ) : while True : if shutdown_event . is_set ( ) : return if callable ( callback ) : callback ( ) try : item = queue . get ( timeout = 1 ) queue . put ( item ) except Queue . Empty : return
def format_task_numbers_with_links ( tasks ) : project_id = data . get ( 'asana-project' , None ) def _task_format ( task_id ) : if project_id : asana_url = tool . ToolApp . make_asana_url ( project_id , task_id ) return "[#%d](%s)" % ( task_id , asana_url ) else : return "#%d" % task_id return "\n" . join ( [ _task_format ( tid ) for tid in tasks ] )
def create_missing_task ( self , asana_workspace_id , name , assignee , projects , completed , issue_number , issue_html_url , issue_state , issue_body , tasks , labels , label_tag_map ) : task = self . asana . tasks . create_in_workspace ( asana_workspace_id , { 'name' : name , 'notes' : issue_body , 'assignee' : assignee , 'projects' : projects , 'completed' : completed , } ) task_id = task [ 'id' ] put ( "create_story" , task_id = task_id , text = "Git Issue #%d: \n" "%s" % ( issue_number , issue_html_url , ) ) put ( "apply_tasks_to_issue" , tasks = [ task_id ] , issue_number = issue_number , issue_body = issue_body , ) put_setting ( "save_issue_data_task" , issue = issue_number , task_id = task_id , namespace = issue_state ) tasks . append ( task_id ) put ( "sync_tags" , tasks = tasks , labels = labels , label_tag_map = label_tag_map )
def apply_tasks_to_issue ( self , tasks , issue_number , issue_body ) : issue_body = issue_body task_numbers = format_task_numbers_with_links ( tasks ) if task_numbers : new_body = ASANA_SECTION_RE . sub ( '' , issue_body ) new_body = new_body + % task_numbers put ( "issue_edit" , issue_number = issue_number , body = new_body ) return new_body return issue_body
def data_types ( self ) : data = self . gencloud . project_data ( self . id ) return sorted ( set ( d . type for d in data ) )
def data ( self , * * query ) : data = self . gencloud . project_data ( self . id ) query [ 'case_ids__contains' ] = self . id ids = set ( d [ 'id' ] for d in self . gencloud . api . dataid . get ( * * query ) [ 'objects' ] ) return [ d for d in data if d . id in ids ]
def initPort ( self ) : try : self . m_ser = serial . Serial ( port = self . m_ttyport , baudrate = self . m_baudrate , timeout = 0 , parity = serial . PARITY_EVEN , stopbits = serial . STOPBITS_ONE , bytesize = serial . SEVENBITS , rtscts = False ) ekm_log ( "Pyserial version = " + serial . VERSION ) ekm_log ( "Port = " + self . m_ttyport ) ekm_log ( "Rate = " + str ( self . m_baudrate ) ) time . sleep ( self . m_init_wait ) return True except : ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) return False
def combineAB ( self ) : v4definition_meter = V4Meter ( ) v4definition_meter . makeAB ( ) defv4 = v4definition_meter . getReadBuffer ( ) v3definition_meter = V3Meter ( ) v3definition_meter . makeReturnFormat ( ) defv3 = v3definition_meter . getReadBuffer ( ) for fld in defv3 : if fld not in self . m_all_fields : compare_fld = fld . upper ( ) if not "RESERVED" in compare_fld and not "CRC" in compare_fld : self . m_all_fields [ fld ] = defv3 [ fld ] for fld in defv4 : if fld not in self . m_all_fields : compare_fld = fld . upper ( ) if not "RESERVED" in compare_fld and not "CRC" in compare_fld : self . m_all_fields [ fld ] = defv4 [ fld ] pass
def updateObservers ( self ) : for observer in self . m_observers : try : observer . update ( self . m_req ) except : ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) )
def initLcdLookup ( self ) : self . m_lcd_lookup [ "kWh_Tot" ] = LCDItems . kWh_Tot self . m_lcd_lookup [ "Rev_kWh_Tot" ] = LCDItems . Rev_kWh_Tot self . m_lcd_lookup [ "RMS_Volts_Ln_1" ] = LCDItems . RMS_Volts_Ln_1 self . m_lcd_lookup [ "RMS_Volts_Ln_2" ] = LCDItems . RMS_Volts_Ln_2 self . m_lcd_lookup [ "RMS_Volts_Ln_3" ] = LCDItems . RMS_Volts_Ln_3 self . m_lcd_lookup [ "Amps_Ln_1" ] = LCDItems . Amps_Ln_1 self . m_lcd_lookup [ "Amps_Ln_2" ] = LCDItems . Amps_Ln_2 self . m_lcd_lookup [ "Amps_Ln_3" ] = LCDItems . Amps_Ln_3 self . m_lcd_lookup [ "RMS_Watts_Ln_1" ] = LCDItems . RMS_Watts_Ln_1 self . m_lcd_lookup [ "RMS_Watts_Ln_2" ] = LCDItems . RMS_Watts_Ln_2 self . m_lcd_lookup [ "RMS_Watts_Ln_3" ] = LCDItems . RMS_Watts_Ln_3 self . m_lcd_lookup [ "RMS_Watts_Tot" ] = LCDItems . RMS_Watts_Tot self . m_lcd_lookup [ "Power_Factor_Ln_1" ] = LCDItems . Power_Factor_Ln_1 self . m_lcd_lookup [ "Power_Factor_Ln_2" ] = LCDItems . Power_Factor_Ln_2 self . m_lcd_lookup [ "Power_Factor_Ln_3" ] = LCDItems . Power_Factor_Ln_3 self . m_lcd_lookup [ "kWh_Tariff_1" ] = LCDItems . kWh_Tariff_1 self . m_lcd_lookup [ "kWh_Tariff_2" ] = LCDItems . kWh_Tariff_2 self . m_lcd_lookup [ "kWh_Tariff_3" ] = LCDItems . kWh_Tariff_3 self . m_lcd_lookup [ "kWh_Tariff_4" ] = LCDItems . kWh_Tariff_4 self . m_lcd_lookup [ "Rev_kWh_Tariff_1" ] = LCDItems . Rev_kWh_Tariff_1 self . m_lcd_lookup [ "Rev_kWh_Tariff_2" ] = LCDItems . Rev_kWh_Tariff_2 self . m_lcd_lookup [ "Rev_kWh_Tariff_3" ] = LCDItems . Rev_kWh_Tariff_3 self . m_lcd_lookup [ "Rev_kWh_Tariff_4" ] = LCDItems . Rev_kWh_Tariff_4 self . m_lcd_lookup [ "Reactive_Pwr_Ln_1" ] = LCDItems . Reactive_Pwr_Ln_1 self . m_lcd_lookup [ "Reactive_Pwr_Ln_2" ] = LCDItems . Reactive_Pwr_Ln_2 self . m_lcd_lookup [ "Reactive_Pwr_Ln_3" ] = LCDItems . Reactive_Pwr_Ln_3 self . m_lcd_lookup [ "Reactive_Pwr_Tot" ] = LCDItems . Reactive_Pwr_Tot self . m_lcd_lookup [ "Line_Freq" ] = LCDItems . Line_Freq self . m_lcd_lookup [ "Pulse_Cnt_1" ] = LCDItems . Pulse_Cnt_1 self . m_lcd_lookup [ "Pulse_Cnt_2" ] = LCDItems . Pulse_Cnt_2 self . m_lcd_lookup [ "Pulse_Cnt_3" ] = LCDItems . Pulse_Cnt_3 self . m_lcd_lookup [ "kWh_Ln_1" ] = LCDItems . kWh_Ln_1 self . m_lcd_lookup [ "Rev_kWh_Ln_1" ] = LCDItems . Rev_kWh_Ln_1 self . m_lcd_lookup [ "kWh_Ln_2" ] = LCDItems . kWh_Ln_2 self . m_lcd_lookup [ "Rev_kWh_Ln_2" ] = LCDItems . Rev_kWh_Ln_2 self . m_lcd_lookup [ "kWh_Ln_3" ] = LCDItems . kWh_Ln_3 self . m_lcd_lookup [ "Rev_kWh_Ln_3" ] = LCDItems . Rev_kWh_Ln_3 self . m_lcd_lookup [ "Reactive_Energy_Tot" ] = LCDItems . Reactive_Energy_Tot self . m_lcd_lookup [ "Max_Demand_Rst" ] = LCDItems . Max_Demand_Rst self . m_lcd_lookup [ "Rev_kWh_Rst" ] = LCDItems . Rev_kWh_Rst self . m_lcd_lookup [ "State_Inputs" ] = LCDItems . State_Inputs self . m_lcd_lookup [ "Max_Demand" ] = LCDItems . Max_Demand
def makeAB ( self ) : for fld in self . m_blk_a : compare_fld = fld . upper ( ) if not "RESERVED" in compare_fld and not "CRC" in compare_fld : self . m_req [ fld ] = self . m_blk_a [ fld ] for fld in self . m_blk_b : compare_fld = fld . upper ( ) if not "RESERVED" in compare_fld and not "CRC" in compare_fld : self . m_req [ fld ] = self . m_blk_b [ fld ] pass
def serialPostEnd ( self ) : ekm_log ( "Termination string sent (" + self . m_context + ")" ) try : self . m_serial_port . write ( "0142300375" . decode ( "hex" ) ) except : ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) pass
def apply_tasks_to_issue ( self , issue , tasks , issue_body = None ) : issue_body = issue_body or issue . body task_numbers = transport . format_task_numbers_with_links ( tasks ) if task_numbers : new_body = transport . ASANA_SECTION_RE . sub ( '' , issue_body ) new_body = new_body + % task_numbers transport . issue_edit ( issue , body = new_body ) return new_body return issue_body
def statistics ( self , elapsed , result ) : return "\n" . join ( ( self . timing ( elapsed ) , self . result_summary ( result ) ) )
def color ( self , color , text ) : return "{escape}{text}{reset}" . format ( escape = self . ANSI [ color ] , text = text , reset = self . ANSI [ "reset" ] , )
def show ( self , text ) : self . stream . write ( text ) self . stream . flush ( )
def result_summary ( self , result ) : return "{} examples, {} errors, {} failures\n" . format ( result . testsRun , len ( result . errors ) , len ( result . failures ) , )
def parse ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] if not argv or argv [ 0 ] not in { "run" , "transform" } : argv = [ "run" ] + argv arguments = _clean ( _parser . parse_args ( argv ) ) return arguments
def setup ( config ) : formatter = config . Formatter ( ) if config . verbose : formatter = result . Verbose ( formatter ) if config . color : formatter = result . Colored ( formatter ) current_result = result . ExampleResult ( formatter ) ivoire . current_result = ivoire . _manager . result = current_result
def run ( config ) : setup ( config ) if config . exitfirst : ivoire . current_result . failfast = True ivoire . current_result . startTestRun ( ) for spec in config . specs : try : load_by_name ( spec ) except Exception : ivoire . current_result . addError ( _ExampleNotRunning ( ) , sys . exc_info ( ) ) ivoire . current_result . stopTestRun ( ) sys . exit ( not ivoire . current_result . wasSuccessful ( ) )
def transform ( config ) : if transform_possible : ExampleLoader . register ( ) args , sys . argv [ 1 : ] = sys . argv [ 1 : ] , config . args try : return runpy . run_path ( config . runner , run_name = "__main__" ) finally : sys . argv [ 1 : ] = args
def takes_only_self ( self ) : return ast . arguments ( args = [ ast . arg ( arg = "self" ) ] , defaults = [ ] , kw_defaults = [ ] , kwonlyargs = [ ] , )
def register ( cls ) : cls . _finder = FileFinder . path_hook ( ( cls , [ cls . suffix ] ) ) sys . path_hooks . append ( cls . _finder )
def source_to_code ( self , source_bytes , source_path ) : node = ast . parse ( source_bytes ) transformed = ExampleTransformer ( ) . transform ( node ) return compile ( transformed , source_path , "exec" , dont_inherit = True )
def apply_argument_parser ( argumentsParser , options = None ) : if options is not None : args = argumentsParser . parse_args ( options ) else : args = argumentsParser . parse_args ( ) return args
def load_by_name ( name ) : if os . path . exists ( name ) : load_from_path ( name ) else : __import__ ( name )
def load_from_path ( path ) : if os . path . isdir ( path ) : paths = discover ( path ) else : paths = [ path ] for path in paths : name = os . path . basename ( os . path . splitext ( path ) [ 0 ] ) imp . load_source ( name , path )
def delimit ( values , delimiter = ', ' ) : toks = [ ] if not values : return toks if not isinstance ( delimiter , ( list , tuple ) ) : delimiter = [ delimiter ] last = len ( values ) - 1 for i , value in enumerate ( values ) : toks . append ( value ) if i < last : toks . extend ( delimiter ) return toks
def exists ( value ) : if not isinstance ( value , Token ) : raise TypeError ( 'value must be a token' ) if not hasattr ( value , 'identifier' ) : raise TypeError ( 'value must support an identifier' ) if not value . identifier : value = value . __class__ ( * * value . __dict__ ) value . identifier = 'v' ident = Identifier ( value . identifier ) return Query ( [ OptionalMatch ( value ) , Return ( Predicate ( ident , 'IS NOT NULL' ) ) , Limit ( 1 ) , ] )
def get ( value ) : if not isinstance ( value , Token ) : raise TypeError ( 'value must be a token' ) if not hasattr ( value , 'identifier' ) : raise TypeError ( 'value must support an identifier' ) if not value . identifier : value = value . __class__ ( * * value . __dict__ ) value . identifier = 'v' ident = Identifier ( value . identifier ) return Query ( [ Match ( value ) , Return ( ident ) ] )
def check ( self ) : if self . closed : raise ValueError ( "Cannot check a closed state" ) self . _maybeReset ( ) if self . url is None : return False return self . _maybeCheck ( )
def wrapHeart ( service ) : master = taservice . MultiService ( ) service . setServiceParent ( master ) maybeAddHeart ( master ) return master
def freeze_from_checkpoint ( input_checkpoint , output_file_path , output_node_names ) : check_input_checkpoint ( input_checkpoint ) output_node_names = output_node_names_string_as_list ( output_node_names ) with tf . Session ( ) as sess : restore_from_checkpoint ( sess , input_checkpoint ) freeze_graph . freeze_graph_with_def_protos ( input_graph_def = sess . graph_def , input_saver_def = None , input_checkpoint = input_checkpoint , output_node_names = ',' . join ( output_node_names ) , restore_op_name = 'save/restore_all' , filename_tensor_name = 'save/Const:0' , output_graph = output_file_path , clear_devices = True , initializer_nodes = '' )
def freeze ( sess , output_file_path , output_node_names ) : with TemporaryDirectory ( ) as temp_dir_name : checkpoint_path = os . path . join ( temp_dir_name , 'model.ckpt' ) tf . train . Saver ( ) . save ( sess , checkpoint_path ) freeze_from_checkpoint ( checkpoint_path , output_file_path , output_node_names )
def save_graph_only ( sess , output_file_path , output_node_names , as_text = False ) : for node in sess . graph_def . node : node . device = '' graph_def = graph_util . extract_sub_graph ( sess . graph_def , output_node_names ) output_dir , output_filename = os . path . split ( output_file_path ) graph_io . write_graph ( graph_def , output_dir , output_filename , as_text = as_text )
def save_graph_only_from_checkpoint ( input_checkpoint , output_file_path , output_node_names , as_text = False ) : check_input_checkpoint ( input_checkpoint ) output_node_names = output_node_names_string_as_list ( output_node_names ) with tf . Session ( ) as sess : restore_from_checkpoint ( sess , input_checkpoint ) save_graph_only ( sess , output_file_path , output_node_names , as_text = as_text )
def save_weights ( sess , output_path , conv_var_names = None , conv_transpose_var_names = None ) : if not conv_var_names : conv_var_names = [ ] if not conv_transpose_var_names : conv_transpose_var_names = [ ] for var in tf . trainable_variables ( ) : filename = '{}-{}' . format ( output_path , var . name . replace ( ':' , '-' ) . replace ( '/' , '-' ) ) if var . name in conv_var_names : var = tf . transpose ( var , perm = [ 3 , 0 , 1 , 2 ] ) elif var . name in conv_transpose_var_names : var = tf . transpose ( var , perm = [ 3 , 1 , 0 , 2 ] ) value = sess . run ( var ) with open ( filename , 'w' ) as file_ : value . tofile ( file_ )
def save_weights_from_checkpoint ( input_checkpoint , output_path , conv_var_names = None , conv_transpose_var_names = None ) : check_input_checkpoint ( input_checkpoint ) with tf . Session ( ) as sess : restore_from_checkpoint ( sess , input_checkpoint ) save_weights ( sess , output_path , conv_var_names = conv_var_names , conv_transpose_var_names = conv_transpose_var_names )
def restore_from_checkpoint ( sess , input_checkpoint ) : saver = tf . train . import_meta_graph ( '{}.meta' . format ( input_checkpoint ) ) saver . restore ( sess , input_checkpoint ) return saver
def render_tag ( self , context , * tag_args , * * tag_kwargs ) : raise NotImplementedError ( "{0}.render_tag() is not implemented!" . format ( self . __class__ . __name__ ) )
def validate_args ( cls , tag_name , * args , * * kwargs ) : if cls . min_args is not None and len ( args ) < cls . min_args : if cls . min_args == 1 : raise TemplateSyntaxError ( "'{0}' tag requires at least {1} argument" . format ( tag_name , cls . min_args ) ) else : raise TemplateSyntaxError ( "'{0}' tag requires at least {1} arguments" . format ( tag_name , cls . min_args ) ) if cls . max_args is not None and len ( args ) > cls . max_args : if cls . max_args == 0 : if cls . allowed_kwargs : raise TemplateSyntaxError ( "'{0}' tag only allows keywords arguments, for example {1}=\"...\"." . format ( tag_name , cls . allowed_kwargs [ 0 ] ) ) else : raise TemplateSyntaxError ( "'{0}' tag doesn't support any arguments" . format ( tag_name ) ) elif cls . max_args == 1 : raise TemplateSyntaxError ( "'{0}' tag only allows {1} argument." . format ( tag_name , cls . max_args ) ) else : raise TemplateSyntaxError ( "'{0}' tag only allows {1} arguments." . format ( tag_name , cls . max_args ) )
def get_context_data ( self , parent_context , * tag_args , * * tag_kwargs ) : raise NotImplementedError ( "{0}.get_context_data() is not implemented." . format ( self . __class__ . __name__ ) )
def render_tag ( self , context , * tag_args , * * tag_kwargs ) : if self . as_var : context [ self . as_var ] = self . get_value ( context , * tag_args , * * tag_kwargs ) return u''
def parse ( cls , parser , token ) : bits , as_var = parse_as_var ( parser , token ) tag_name , args , kwargs = parse_token_kwargs ( parser , bits , ( 'template' , ) + cls . allowed_kwargs , compile_args = cls . compile_args , compile_kwargs = cls . compile_kwargs ) cls . validate_args ( tag_name , * args ) return cls ( tag_name , as_var , * args , * * kwargs )
def render_tag ( self , context , * tag_args , * * tag_kwargs ) : if self . as_var : return BaseAssignmentNode . render_tag ( self , context , * tag_args , * * tag_kwargs ) else : return BaseInclusionNode . render_tag ( self , context , * tag_args , * * tag_kwargs )
def caffe_to_tensorflow_session ( caffe_def_path , caffemodel_path , inputs , graph_name = 'Graph' , conversion_out_dir_path = None , use_padding_same = False ) : try : from caffeflow import convert except ImportError : raise Exception ( "caffeflow package needs to be installed to freeze Caffe models. Check out the README file." ) with ( dummy_context_mgr ( conversion_out_dir_path ) or util . TemporaryDirectory ( ) ) as dir_path : params_values_output_path = os . path . join ( dir_path , 'params_values.npy' ) network_output_path = os . path . join ( dir_path , 'network.py' ) convert . convert ( caffe_def_path , caffemodel_path , params_values_output_path , network_output_path , False , use_padding_same = use_padding_same ) network_module = imp . load_source ( 'module.name' , network_output_path ) network_class = getattr ( network_module , graph_name ) network = network_class ( inputs ) sess = tf . Session ( ) network . load ( params_values_output_path , sess ) return sess
def freeze ( caffe_def_path , caffemodel_path , inputs , output_file_path , output_node_names , graph_name = 'Graph' , conversion_out_dir_path = None , checkpoint_out_path = None , use_padding_same = False ) : with caffe_to_tensorflow_session ( caffe_def_path , caffemodel_path , inputs , graph_name = graph_name , conversion_out_dir_path = conversion_out_dir_path , use_padding_same = use_padding_same ) as sess : saver = tf . train . Saver ( ) with ( dummy_context_mgr ( checkpoint_out_path ) or util . TemporaryDirectory ( ) ) as temp_dir_path : checkpoint_path = checkpoint_out_path or os . path . join ( temp_dir_path , 'pose.ckpt' ) saver . save ( sess , checkpoint_path ) output_node_names = util . output_node_names_string_as_list ( output_node_names ) tf_freeze . freeze_from_checkpoint ( checkpoint_path , output_file_path , output_node_names )
def save_graph_only ( caffe_def_path , caffemodel_path , inputs , output_file_path , output_node_names , graph_name = 'Graph' , use_padding_same = False ) : with caffe_to_tensorflow_session ( caffe_def_path , caffemodel_path , inputs , graph_name = graph_name , use_padding_same = use_padding_same ) as sess : tf_freeze . save_graph_only ( sess , output_file_path , output_node_names )
def save_weights ( caffe_def_path , caffemodel_path , inputs , output_path , graph_name = 'Graph' , conv_var_names = None , conv_transpose_var_names = None , use_padding_same = False ) : with caffe_to_tensorflow_session ( caffe_def_path , caffemodel_path , inputs , graph_name = graph_name , use_padding_same = use_padding_same ) as sess : tf_freeze . save_weights ( sess , output_path , conv_var_names = conv_var_names , conv_transpose_var_names = conv_transpose_var_names )
def descendant ( self , chain_path ) : public_child = self . hdkeychain chain_step_bytes = 4 max_bits_per_step = 2 ** 31 chain_steps = [ int ( chain_path [ i : i + chain_step_bytes * 2 ] , 16 ) % max_bits_per_step for i in range ( 0 , len ( chain_path ) , chain_step_bytes * 2 ) ] for step in chain_steps : public_child = public_child . get_child ( step ) return PublicKeychain ( public_child )
def object_iter ( obj , parent = None , parent_key = None , idx = None , siblings = None ) : obj_node = Node ( value = obj , parent = parent , parent_key = parent_key , siblings = siblings , idx = idx ) if isinstance ( obj , list ) : _siblings = len ( obj ) for i , elem in enumerate ( obj ) : for node in object_iter ( elem , obj_node , None , i + 1 , _siblings ) : yield node elif isinstance ( obj , collections . Mapping ) : for key in obj : for node in object_iter ( obj [ key ] , obj_node , key ) : yield node yield obj_node
def parse ( self , selector ) : log . debug ( self . obj ) tokens = lex ( selector ) if self . peek ( tokens , 'operator' ) == '*' : self . match ( tokens , 'operator' ) results = list ( object_iter ( self . obj ) ) else : results = self . selector_production ( tokens ) results = [ node . value for node in results ] if len ( results ) == 1 : return results [ 0 ] elif not len ( results ) : return None return results
def selector_production ( self , tokens ) : validators = [ ] if self . peek ( tokens , 'type' ) : type_ = self . match ( tokens , 'type' ) validators . append ( self . type_production ( type_ ) ) if self . peek ( tokens , 'identifier' ) : key = self . match ( tokens , 'identifier' ) validators . append ( self . key_production ( key ) ) if self . peek ( tokens , 'pclass' ) : pclass = self . match ( tokens , 'pclass' ) validators . append ( self . pclass_production ( pclass ) ) if self . peek ( tokens , 'nth_func' ) : nth_func = self . match ( tokens , 'nth_func' ) validators . append ( self . nth_child_production ( nth_func , tokens ) ) if self . peek ( tokens , 'pclass_func' ) : pclass_func = self . match ( tokens , 'pclass_func' ) validators . append ( self . pclass_func_production ( pclass_func , tokens ) ) if not len ( validators ) : raise SelectorSyntaxError ( 'no selector recognized.' ) results = self . _match_nodes ( validators , self . obj ) if self . peek ( tokens , 'operator' ) : operator = self . match ( tokens , 'operator' ) rvals = self . selector_production ( tokens ) if operator == ',' : results . extend ( rvals ) elif operator == '>' : results = self . parents ( results , rvals ) elif operator == '~' : results = self . siblings ( results , rvals ) elif operator == ' ' : results = self . ancestors ( results , rvals ) else : raise SelectorSyntaxError ( "unrecognized operator '%s'" % operator ) else : if len ( tokens ) : rvals = self . selector_production ( tokens ) results = self . ancestors ( results , rvals ) return results
def parents ( self , lhs , rhs ) : return [ node for node in rhs if node . parent in lhs ]
def ancestors ( self , lhs , rhs ) : def _search ( node ) : if node in lhs : return True if not node . parent : return False return _search ( node . parent ) return [ node for node in rhs if _search ( node ) ]
def siblings ( self , lhs , rhs ) : parents = [ node . parent for node in lhs ] return [ node for node in rhs if node . parent in parents ]
def nth_child_production ( self , lexeme , tokens ) : args = self . match ( tokens , 'expr' ) pat = self . nth_child_pat . match ( args ) if pat . group ( 5 ) : a = 2 b = 1 if pat . group ( 5 ) == 'odd' else 0 elif pat . group ( 6 ) : a = 0 b = int ( pat . group ( 6 ) ) else : sign = pat . group ( 1 ) if pat . group ( 1 ) else '+' coef = pat . group ( 2 ) if pat . group ( 2 ) else '1' a = eval ( sign + coef ) b = eval ( pat . group ( 3 ) + pat . group ( 4 ) ) if pat . group ( 3 ) else 0 reverse = False if lexeme == 'nth-last-child' : reverse = True def validate ( node ) : """This crazy function taken from jsonselect.js:444.""" if not node . siblings : return False idx = node . idx - 1 tot = node . siblings if reverse : idx = tot - idx else : idx += 1 if a == 0 : m = b == idx else : mod = ( idx - b ) % a m = not mod and ( idx * a + b ) >= 0 return m return validate
def getBody ( self , url , method = 'GET' , headers = { } , data = None , socket = None ) : if not 'User-Agent' in headers : headers [ 'User-Agent' ] = [ 'Tensor HTTP checker' ] return self . request ( url , method , headers , data , socket )
def expire ( self , age ) : now = time . time ( ) cache = self . _acquire_cache ( ) expired = [ k for k , v in cache . items ( ) if ( now - v [ 0 ] ) > age ] for k in expired : if k in cache : del cache [ k ] if k in self . store : del self . store [ k ] self . _write_cache ( cache )
def set ( self , k , v ) : self . store [ k ] = ( time . time ( ) , v ) self . _persist ( )
def get ( self , k ) : if self . _changed ( ) : self . _read ( ) if k in self . store : return tuple ( self . store [ k ] ) else : return None
def contains ( self , k ) : if self . _changed ( ) : self . _read ( ) return k in self . store . keys ( )
def rendered_content ( self ) : template = self . resolve_template ( self . template_name ) if django . VERSION [ 1 ] < 8 : if template . name . endswith ( '.min' ) : return super ( MinifiedJsTemplateResponse , self ) . rendered_content else : if template . template . name . endswith ( '.min' ) : return super ( MinifiedJsTemplateResponse , self ) . rendered_content content = super ( MinifiedJsTemplateResponse , self ) . rendered_content content = jsmin . jsmin ( content ) return content
def get ( self , max_lines = None ) : rows = [ ] self . get_fn ( lambda row : rows . append ( row ) , max_lines = max_lines ) return rows
def engine ( self ) : if not hasattr ( self , '_engine' ) : from cryptography . fernet import Fernet from cryptography . hazmat . backends import default_backend from cryptography . hazmat . primitives import hashes digest = hashes . Hash ( hashes . SHA256 ( ) , backend = default_backend ( ) ) digest . update ( current_app . config [ 'SECRET_KEY' ] . encode ( 'utf8' ) ) fernet_key = urlsafe_b64encode ( digest . finalize ( ) ) self . _engine = Fernet ( fernet_key ) return self . _engine
def create_token ( self , obj_id , extra_data ) : return self . engine . encrypt ( super ( EncryptedTokenMixIn , self ) . create_token ( obj_id , extra_data ) )
def compat_validate_token ( cls , * args , * * kwargs ) : data = None for algorithm in SUPPORTED_DIGEST_ALGORITHMS : data = cls ( algorithm_name = algorithm ) . validate_token ( * args , * * kwargs ) if not data : continue return data
def create_token ( cls , obj_id , data , expires_at = None ) : if expires_at : s = TimedSecretLinkSerializer ( expires_at = expires_at ) else : s = SecretLinkSerializer ( ) return s . create_token ( obj_id , data )
def Counter32 ( a , b , delta ) : if b < a : c = 4294967295 - a return ( c + b ) / float ( delta ) return ( b - a ) / float ( delta )
def Counter64 ( a , b , delta ) : if b < a : c = 18446744073709551615 - a return ( c + b ) / float ( delta ) return ( b - a ) / float ( delta )
def average_duration ( total_duration , visits ) : if not visits : seconds = 0 else : seconds = int ( round ( total_duration / Decimal ( visits ) ) ) duration = timedelta ( seconds = seconds ) return str ( duration )
def setupSources ( self , config ) : sources = config . get ( 'sources' , [ ] ) for source in sources : src = self . createSource ( source ) self . setupTriggers ( source , src ) self . sources . append ( src )
def validate_expires_at ( form , field ) : if form . accept . data : if not field . data or datetime . utcnow ( ) . date ( ) >= field . data : raise validators . StopValidation ( _ ( "Please provide a future date." ) ) if not field . data or datetime . utcnow ( ) . date ( ) + timedelta ( days = 365 ) < field . data : raise validators . StopValidation ( _ ( "Please provide a date no more than 1 year into the future." ) )
def validate_accept ( form , field ) : if field . data and form . reject . data : raise validators . ValidationError ( _ ( "Both reject and accept cannot be set at the same time." ) )
def validate_reject ( form , field ) : if field . data and form . accept . data : raise validators . ValidationError ( _ ( "Both reject and accept cannot be set at the same time." ) )
def verify_token ( ) : try : from . models import SecretLink token = request . args [ 'token' ] if token and SecretLink . validate_token ( token , { } ) : session [ 'accessrequests-secret-token' ] = token except KeyError : pass
def init_app ( self , app ) : app . before_request ( verify_token ) self . init_config ( app ) state = _AppState ( app = app ) app . extensions [ 'zenodo-accessrequests' ] = state
def name ( self ) : if ( self . device_type and self . device_type . code in ( DeviceType . MOBILE , DeviceType . TABLET ) ) : return self . device else : return self . browser
def _warn_node ( self , msg , * args , * * kwargs ) : if not msg . startswith ( 'nonlocal image URI found:' ) : _warn_node_old ( self , msg , * args , * * kwargs )
def connect_receivers ( ) : request_created . connect ( send_email_validation ) request_confirmed . connect ( send_confirmed_notifications ) request_rejected . connect ( send_reject_notification ) request_accepted . connect ( create_secret_link ) request_accepted . connect ( send_accept_notification )
def create_secret_link ( request , message = None , expires_at = None ) : pid , record = get_record ( request . recid ) if not record : raise RecordNotFound ( request . recid ) description = render_template ( "zenodo_accessrequests/link_description.tpl" , request = request , record = record , pid = pid , expires_at = expires_at , message = message , ) request . create_secret_link ( record [ "title" ] , description = description , expires_at = expires_at )
def send_accept_notification ( request , message = None , expires_at = None ) : pid , record = get_record ( request . recid ) _send_notification ( request . sender_email , _ ( "Access request accepted" ) , "zenodo_accessrequests/emails/accepted.tpl" , request = request , record = record , pid = pid , record_link = request . link . get_absolute_url ( 'invenio_records_ui.recid' ) , message = message , expires_at = expires_at , )
def send_confirmed_notifications ( request ) : pid , record = get_record ( request . recid ) if record is None : current_app . logger . error ( "Cannot retrieve record %s. Emails not sent" % request . recid ) return title = _ ( "Access request: %(record)s" , record = record [ "title" ] ) _send_notification ( request . receiver . email , title , "zenodo_accessrequests/emails/new_request.tpl" , request = request , record = record , pid = pid , ) _send_notification ( request . sender_email , title , "zenodo_accessrequests/emails/confirmation.tpl" , request = request , record = record , pid = pid , )
def send_email_validation ( request ) : token = EmailConfirmationSerializer ( ) . create_token ( request . id , dict ( email = request . sender_email ) ) pid , record = get_record ( request . recid ) _send_notification ( request . sender_email , _ ( "Access request verification" ) , "zenodo_accessrequests/emails/validate_email.tpl" , request = request , record = record , pid = pid , days = timedelta ( seconds = current_app . config [ "ACCESSREQUESTS_CONFIRMLINK_EXPIRES_IN" ] ) . days , confirm_link = url_for ( "invenio_records_ui.recid_access_request_email_confirm" , pid_value = request . recid , token = token , _external = True , ) )
def send_reject_notification ( request , message = None ) : pid , record = get_record ( request . recid ) _send_notification ( request . sender_email , _ ( "Access request rejected" ) , "zenodo_accessrequests/emails/rejected.tpl" , request = request , record = record , pid = pid , message = message , )
def _send_notification ( to , subject , template , * * ctx ) : msg = Message ( subject , sender = current_app . config . get ( 'SUPPORT_EMAIL' ) , recipients = [ to ] ) msg . body = render_template ( template , * * ctx ) send_email . delay ( msg . __dict__ )
def create ( cls , title , owner , extra_data , description = "" , expires_at = None ) : if isinstance ( expires_at , date ) : expires_at = datetime . combine ( expires_at , datetime . min . time ( ) ) with db . session . begin_nested ( ) : obj = cls ( owner = owner , title = title , description = description , expires_at = expires_at , token = '' , ) db . session . add ( obj ) with db . session . begin_nested ( ) : obj . token = SecretLinkFactory . create_token ( obj . id , extra_data , expires_at = expires_at ) . decode ( 'utf8' ) link_created . send ( obj ) return obj
def revoke ( self ) : if self . revoked_at is None : with db . session . begin_nested ( ) : self . revoked_at = datetime . utcnow ( ) link_revoked . send ( self ) return True return False
def get_by_receiver ( cls , request_id , user ) : return cls . query . filter_by ( id = request_id , receiver_user_id = user . id ) . first ( )
def confirm_email ( self ) : with db . session . begin_nested ( ) : if self . status != RequestStatus . EMAIL_VALIDATION : raise InvalidRequestStateError ( RequestStatus . EMAIL_VALIDATION ) self . status = RequestStatus . PENDING request_confirmed . send ( self )
def create_secret_link ( self , title , description = None , expires_at = None ) : self . link = SecretLink . create ( title , self . receiver , extra_data = dict ( recid = self . recid ) , description = description , expires_at = expires_at , ) return self . link
def is_embargoed ( record ) : return record . get ( 'access_right' ) == 'embargoed' and record . get ( 'embargo_date' ) and record . get ( 'embargo_date' ) > datetime . utcnow ( ) . date ( )
def access_request ( pid , record , template , * * kwargs ) : recid = int ( pid . pid_value ) datastore = LocalProxy ( lambda : current_app . extensions [ 'security' ] . datastore ) if record . get ( 'access_right' ) != 'restricted' or not record . get ( 'access_conditions' ) : abort ( 404 ) owners = record . get ( 'owners' , [ ] ) record_owners = [ datastore . find_user ( id = owner_id ) for owner_id in owners ] if not record_owners : abort ( 404 ) sender = None initialdata = dict ( ) if current_user . is_authenticated : sender = current_user initialdata [ 'email' ] = current_user . email if current_user . profile : initialdata [ 'full_name' ] = current_user . profile . full_name form = AccessRequestForm ( formdata = request . form , * * initialdata ) if form . validate_on_submit ( ) : accreq = AccessRequest . create ( recid = recid , receiver = record_owners [ 0 ] , sender_full_name = form . data [ 'full_name' ] , sender_email = form . data [ 'email' ] , justification = form . data [ 'justification' ] , sender = sender ) db . session . commit ( ) if accreq . status == RequestStatus . EMAIL_VALIDATION : flash ( _ ( "Email confirmation needed: We have sent you an email to " "verify your address. Please check the email and follow the " "instructions to complete the access request." ) , category = 'info' ) else : flash ( _ ( "Access request submitted." ) , category = 'info' ) return redirect ( url_for ( 'invenio_records_ui.recid' , pid_value = recid ) ) return render_template ( template , pid = pid , record = record , form = form , owners = record_owners , )
def confirm ( pid , record , template , * * kwargs ) : recid = int ( pid . pid_value ) token = request . view_args [ 'token' ] data = EmailConfirmationSerializer . compat_validate_token ( token ) if data is None : flash ( _ ( "Invalid confirmation link." ) , category = 'danger' ) return redirect ( url_for ( "invenio_records_ui.recid" , pid_value = recid ) ) r = AccessRequest . query . get ( data [ 'id' ] ) if not r : abort ( 404 ) if r . status != RequestStatus . EMAIL_VALIDATION : abort ( 404 ) r . confirm_email ( ) db . session . commit ( ) flash ( _ ( "Email validated and access request submitted." ) , category = 'info' ) return redirect ( url_for ( "invenio_records_ui.recid" , pid_value = recid ) )
def _get_endpoint ( self ) : return SSHCommandClientEndpoint . newConnection ( reactor , b'/bin/cat' , self . username , self . hostname , port = self . port , keys = self . keys , password = self . password , knownHosts = self . knownHosts )
def reverse ( self , col ) : if col in self . options : if self . is_selected ( col ) : return col if not self . asc else '-{0}' . format ( col ) else : return col return None
def selected ( self ) : if self . _selected : return self . _selected if self . asc else "-{0}" . format ( self . _selected ) return None
def items ( self ) : if self . asc is not None : if self . _selected and self . asc : return self . query . order_by ( self . _selected ) elif self . _selected and not self . asc : return self . query . order_by ( desc ( self . _selected ) ) return self . query
def records ( ) : import uuid from invenio_records . api import Record from invenio_pidstore . models import PersistentIdentifier , PIDStatus create_test_user ( ) indexer = RecordIndexer ( ) with db . session . begin_nested ( ) : rec_uuid = uuid . uuid4 ( ) pid1 = PersistentIdentifier . create ( 'recid' , '1' , object_type = 'rec' , object_uuid = rec_uuid , status = PIDStatus . REGISTERED ) Record . create ( { 'title' : 'Registered' , 'description' : 'This is an awesome description' , 'control_number' : '1' , 'access_right' : 'restricted' , 'access_conditions' : 'fuu' , 'owners' : [ 1 , 2 ] , 'recid' : 1 } , id_ = rec_uuid ) indexer . index_by_id ( pid1 . object_uuid ) db . session . commit ( ) sleep ( 3 )
def _init_ssh ( self ) : self . ssh_host = self . config . get ( 'ssh_host' , self . hostname ) self . known_hosts = self . config . get ( 'ssh_knownhosts_file' , self . tensor . config . get ( 'ssh_knownhosts_file' , None ) ) self . ssh_keyfile = self . config . get ( 'ssh_keyfile' , self . tensor . config . get ( 'ssh_keyfile' , None ) ) self . ssh_key = self . config . get ( 'ssh_key' , self . tensor . config . get ( 'ssh_key' , None ) ) self . ssh_keypass = self . config . get ( 'ssh_keypass' , self . tensor . config . get ( 'ssh_keypass' , None ) ) self . ssh_user = self . config . get ( 'ssh_username' , self . tensor . config . get ( 'ssh_username' , None ) ) self . ssh_password = self . config . get ( 'ssh_password' , self . tensor . config . get ( 'ssh_password' , None ) ) self . ssh_port = self . config . get ( 'ssh_port' , self . tensor . config . get ( 'ssh_port' , 22 ) ) if not ( self . ssh_key or self . ssh_keyfile or self . ssh_password ) : raise Exception ( "To use SSH you must specify *one* of ssh_key," " ssh_keyfile or ssh_password for this source" " check or globally" ) if not self . ssh_user : raise Exception ( "ssh_username must be set" ) self . ssh_keydb = [ ] cHash = hashlib . sha1 ( ':' . join ( ( self . ssh_host , self . ssh_user , str ( self . ssh_port ) , str ( self . ssh_password ) , str ( self . ssh_key ) , str ( self . ssh_keyfile ) ) ) . encode ( ) ) . hexdigest ( ) if cHash in self . tensor . hostConnectorCache : self . ssh_client = self . tensor . hostConnectorCache . get ( cHash ) self . ssh_connector = False else : self . ssh_connector = True self . ssh_client = ssh . SSHClient ( self . ssh_host , self . ssh_user , self . ssh_port , password = self . ssh_password , knownhosts = self . known_hosts ) if self . ssh_keyfile : self . ssh_client . addKeyFile ( self . ssh_keyfile , self . ssh_keypass ) if self . ssh_key : self . ssh_client . addKeyString ( self . ssh_key , self . ssh_keypass ) self . tensor . hostConnectorCache [ cHash ] = self . ssh_client
def startTimer ( self ) : self . td = self . t . start ( self . inter ) if self . use_ssh and self . ssh_connector : self . ssh_client . connect ( )
def createEvent ( self , state , description , metric , prefix = None , hostname = None , aggregation = None , evtime = None ) : if prefix : service_name = self . service + "." + prefix else : service_name = self . service return Event ( state , service_name , description , metric , self . ttl , hostname = hostname or self . hostname , aggregation = aggregation , evtime = evtime , tags = self . tags , attributes = self . attributes )
def createLog ( self , type , data , evtime = None , hostname = None ) : return Event ( None , type , data , 0 , self . ttl , hostname = hostname or self . hostname , evtime = evtime , tags = self . tags , type = 'log' )
def index ( ) : query = request . args . get ( 'query' , '' ) order = request . args . get ( 'sort' , '-created' ) try : page = int ( request . args . get ( 'page' , 1 ) ) per_page = int ( request . args . get ( 'per_page' , 20 ) ) except ( TypeError , ValueError ) : abort ( 404 ) form = DeleteForm ( request . form ) if form . validate_on_submit ( ) : link = SecretLink . query_by_owner ( current_user ) . filter_by ( id = form . link . data ) . first ( ) if link . revoke ( ) : flash ( _ ( "Shared link revoked." ) , category = 'success' ) db . session . commit ( ) links = SecretLink . query_by_owner ( current_user ) . filter ( SecretLink . revoked_at . is_ ( None ) ) if query : lquery = "%{0}%" . format ( query ) links = links . filter ( SecretLink . title . like ( lquery ) | SecretLink . description . like ( lquery ) ) ordering = QueryOrdering ( links , [ 'title' , 'created' , 'expires_at' ] , order ) links = ordering . items ( ) requests = AccessRequest . query_by_receiver ( current_user ) . filter_by ( status = RequestStatus . PENDING ) . order_by ( 'created' ) return render_template ( "zenodo_accessrequests/settings/index.html" , links_pagination = links . paginate ( page , per_page = per_page ) , requests = requests , query = query , order = ordering , get_record = get_record , form = DeleteForm ( ) , )
def createClient ( self ) : server = self . config . get ( 'server' , 'localhost' ) port = self . config . get ( 'port' , 5555 ) failover = self . config . get ( 'failover' , False ) self . factory = riemann . RiemannClientFactory ( server , failover = failover ) if failover : initial = random . choice ( server ) else : initial = server log . msg ( 'Connecting to Riemann on %s:%s' % ( initial , port ) ) if self . tls : if SSL : self . connector = reactor . connectSSL ( initial , port , self . factory , ClientTLSContext ( self . key , self . cert ) ) else : log . msg ( '[FATAL] SSL support not available!' ' Please install PyOpenSSL. Exiting now' ) reactor . stop ( ) else : self . connector = reactor . connectTCP ( initial , port , self . factory ) d = defer . Deferred ( ) def cb ( ) : if hasattr ( self . factory , 'proto' ) and self . factory . proto : self . t . start ( self . inter ) d . callback ( None ) else : reactor . callLater ( 0.01 , cb ) cb ( ) return d
def stop ( self ) : self . t . stop ( ) self . factory . stopTrying ( ) self . connector . disconnect ( )
def tick ( self ) : if self . factory . proto : if ( self . pressure < 0 ) or ( self . factory . proto . pressure <= self . pressure ) : self . emptyQueue ( ) elif self . expire : for i , e in enumerate ( self . events ) : if ( time . time ( ) - e . time ) > e . ttl : self . events . pop ( i )
def emptyQueue ( self ) : if self . events : if self . queueDepth and ( len ( self . events ) > self . queueDepth ) : events = self . events [ : self . queueDepth ] self . events = self . events [ self . queueDepth : ] else : events = self . events self . events = [ ] if self . allow_nan : self . factory . proto . sendEvents ( events ) else : self . factory . proto . sendEvents ( [ e for e in events if e . metric is not None ] )
def createClient ( self ) : server = self . config . get ( 'server' , '127.0.0.1' ) port = self . config . get ( 'port' , 5555 ) def connect ( ip ) : self . protocol = riemann . RiemannUDP ( ip , port ) self . endpoint = reactor . listenUDP ( 0 , self . protocol ) d = reactor . resolve ( server ) d . addCallback ( connect ) return d
def createClient ( self ) : server = self . config . get ( 'server' , 'localhost' ) port = int ( self . config . get ( 'port' , 9200 ) ) self . client = elasticsearch . ElasticSearch ( self . url , self . user , self . password , self . index ) self . t . start ( self . inter )
def tick ( self ) : if self . events : if self . queueDepth and ( len ( self . events ) > self . queueDepth ) : events = self . events [ : self . queueDepth ] self . events = self . events [ self . queueDepth : ] else : events = self . events self . events = [ ] try : result = yield self . sendEvents ( events ) if result . get ( 'errors' , False ) : log . msg ( repr ( result ) ) self . events . extend ( events ) except Exception as e : log . msg ( 'Could not connect to elasticsearch ' + str ( e ) ) self . events . extend ( events )
def encodeEvent ( self , event ) : pbevent = proto_pb2 . Event ( time = int ( event . time ) , state = event . state , service = event . service , host = event . hostname , description = event . description , tags = event . tags , ttl = event . ttl , ) if event . metric is not None : if isinstance ( event . metric , int ) : pbevent . metric_sint64 = event . metric pbevent . metric_f = float ( event . metric ) else : pbevent . metric_d = float ( event . metric ) pbevent . metric_f = float ( event . metric ) if event . attributes is not None : for key , value in event . attributes . items ( ) : attribute = pbevent . attributes . add ( ) attribute . key , attribute . value = key , value return pbevent
def encodeMessage ( self , events ) : message = proto_pb2 . Msg ( events = [ self . encodeEvent ( e ) for e in events if e . _type == 'riemann' ] ) return message . SerializeToString ( )
def decodeMessage ( self , data ) : message = proto_pb2 . Msg ( ) message . ParseFromString ( data ) return message
def sendEvents ( self , events ) : self . pressure += 1 self . sendString ( self . encodeMessage ( events ) )
def generate ( ctx , url , * args , * * kwargs ) : file_previews = ctx . obj [ 'file_previews' ] options = { } metadata = kwargs [ 'metadata' ] width = kwargs [ 'width' ] height = kwargs [ 'height' ] output_format = kwargs [ 'format' ] if metadata : options [ 'metadata' ] = metadata . split ( ',' ) if width : options . setdefault ( 'size' , { } ) options [ 'size' ] [ 'width' ] = width if height : options . setdefault ( 'size' , { } ) options [ 'size' ] [ 'height' ] = height if output_format : options [ 'format' ] = output_format results = file_previews . generate ( url , * * options ) click . echo ( results )
def retrieve ( ctx , preview_id , * args , * * kwargs ) : file_previews = ctx . obj [ 'file_previews' ] results = file_previews . retrieve ( preview_id ) click . echo ( results )
def message_loop ( self , t_q , r_q ) : t_msg = { } while t_msg . get ( "state" , "" ) != "__DIE__" : try : t_msg = t_q . get ( True , self . cycle_sleep ) self . task = t_msg . get ( "task" , "" ) if self . task != "" : self . task . task_start = time . time ( ) self . r_q_send ( { "w_id" : self . w_id , "task" : self . task , "state" : "__ACK__" } ) self . cycle_sleep = self . task . worker_loop_delay self . task . result = self . task . run ( ) self . task . task_stop = time . time ( ) self . r_q_send ( { "w_id" : self . w_id , "task" : self . task , "state" : "__FINISHED__" } ) self . task = None except Empty : pass except Full : time . sleep ( 0.1 ) except : if self . task is not None : self . task . task_stop = time . time ( ) tb_str = "" . join ( tb . format_exception ( * ( sys . exc_info ( ) ) ) ) self . r_q_send ( { "w_id" : self . w_id , "task" : self . task , "error" : tb_str , "state" : "__ERROR__" , } ) return
def log_time ( self ) : if self . hot_loop and self . time_delta >= self . log_interval : return True return False
def log_message ( self ) : time_delta = deepcopy ( self . time_delta ) total_work_time = self . worker_count * time_delta time_worked = sum ( self . exec_times ) pct_busy = time_worked / total_work_time * 100.0 min_task_time = min ( self . exec_times ) avg_task_time = sum ( self . exec_times ) / len ( self . exec_times ) max_task_time = max ( self . exec_times ) min_queue_time = min ( self . queue_times ) avg_queue_time = sum ( self . queue_times ) / len ( self . queue_times ) max_queue_time = max ( self . queue_times ) time_delta = self . time_delta total_tasks = len ( self . exec_times ) avg_task_rate = total_tasks / time_delta self . reset ( ) task_msg = """Ran {0} tasks, {1} tasks/s; {2} workers {3}% busy""" . format ( total_tasks , round ( avg_task_rate , 1 ) , self . worker_count , round ( pct_busy , 1 ) ) task_mam = """     Task run times: {0}/{1}/{2} (min/avg/max)""" . format ( round ( min_task_time , 3 ) , round ( avg_task_time , 3 ) , round ( max_task_time , 3 ) ) queue_mam = """     Time in queue: {0}/{1}/{2} (min/avg/max)""" . format ( round ( min_queue_time , 6 ) , round ( avg_queue_time , 6 ) , round ( max_queue_time , 6 ) ) return """{0}\n{1}\n{2}""" . format ( task_msg , task_mam , queue_mam )
def _postConstruction ( self ) : self . setWindowTitle ( 'Filesystem Browser' ) self . _filesystemWidget . sortByColumn ( 0 , QtCore . Qt . AscendingOrder ) self . _bookmarksWidget . hide ( ) self . _acceptButton . setDefault ( True ) self . _acceptButton . setDisabled ( True ) self . _acceptButton . clicked . connect ( self . accept ) self . _cancelButton . clicked . connect ( self . reject ) self . _configureShortcuts ( ) self . setLocation ( self . _root ) self . _filesystemWidget . horizontalHeader ( ) . setResizeMode ( QtGui . QHeaderView . ResizeToContents ) self . _filesystemWidget . horizontalHeader ( ) . setResizeMode ( 0 , QtGui . QHeaderView . Stretch ) self . _upButton . clicked . connect ( self . _onNavigateUpButtonClicked ) self . _locationWidget . currentIndexChanged . connect ( self . _onNavigate ) self . _filesystemWidget . activated . connect ( self . _onActivateItem ) selectionModel = self . _filesystemWidget . selectionModel ( ) selectionModel . currentRowChanged . connect ( self . _onSelectItem )
def _configureShortcuts ( self ) : self . _upShortcut = QtGui . QShortcut ( QtGui . QKeySequence ( 'Backspace' ) , self ) self . _upShortcut . setAutoRepeat ( False ) self . _upShortcut . activated . connect ( self . _onNavigateUpButtonClicked )
def _onActivateItem ( self , index ) : item = self . _filesystemWidget . model ( ) . item ( index ) if not isinstance ( item , riffle . model . File ) : self . _acceptButton . setDisabled ( True ) self . setLocation ( item . path , interactive = True )
def _onSelectItem ( self , selection , previousSelection ) : self . _acceptButton . setEnabled ( True ) del self . _selected [ : ] item = self . _filesystemWidget . model ( ) . item ( selection ) self . _selected . append ( item . path )
def _onNavigate ( self , index ) : if index > 0 : self . setLocation ( self . _locationWidget . itemData ( index ) , interactive = True )
def _segmentPath ( self , path ) : parts = [ ] model = self . _filesystemWidget . model ( ) remainder = path while True : if remainder == model . root . path : break if remainder : parts . append ( remainder ) head , tail = os . path . split ( remainder ) if head == remainder : break remainder = head parts . append ( model . root . path ) return parts
def finalize_options ( self ) : self . resource_source_path = os . path . join ( RESOURCE_PATH , 'resource.qrc' ) self . resource_target_path = RESOURCE_TARGET_PATH
def addChild ( self , item ) : if item . parent and item . parent != self : item . parent . removeChild ( item ) self . children . append ( item ) item . parent = self
def _fetchChildren ( self ) : children = [ ] for entry in QDir . drives ( ) : path = os . path . normpath ( entry . canonicalFilePath ( ) ) children . append ( Mount ( path ) ) return children
def _fetchChildren ( self ) : children = [ ] paths = [ ] for name in os . listdir ( self . path ) : paths . append ( os . path . normpath ( os . path . join ( self . path , name ) ) ) collections , remainder = clique . assemble ( paths , [ clique . PATTERNS [ 'frames' ] ] ) for path in remainder : try : child = ItemFactory ( path ) except ValueError : pass else : children . append ( child ) for collection in collections : children . append ( Collection ( collection ) ) return children
def _fetchChildren ( self ) : children = [ ] for path in self . _collection : try : child = ItemFactory ( path ) except ValueError : pass else : children . append ( child ) return children
def rowCount ( self , parent ) : if parent . column ( ) > 0 : return 0 if parent . isValid ( ) : item = parent . internalPointer ( ) else : item = self . root return len ( item . children )
def index ( self , row , column , parent ) : if not self . hasIndex ( row , column , parent ) : return QModelIndex ( ) if not parent . isValid ( ) : item = self . root else : item = parent . internalPointer ( ) try : child = item . children [ row ] except IndexError : return QModelIndex ( ) else : return self . createIndex ( row , column , child )
def pathIndex ( self , path ) : if path == self . root . path : return QModelIndex ( ) if not path . startswith ( self . root . path ) : return QModelIndex ( ) parts = [ ] while True : if path == self . root . path : break head , tail = os . path . split ( path ) if head == path : if path : parts . append ( path ) break parts . append ( tail ) path = head parts . reverse ( ) if parts : item = self . root count = 0 for count , part in enumerate ( parts ) : matched = False for child in item . children : if child . name == part : item = child matched = True break if not matched : break if count + 1 == len ( parts ) : return self . createIndex ( item . row , 0 , item ) return QModelIndex ( )
def parent ( self , index ) : if not index . isValid ( ) : return QModelIndex ( ) item = index . internalPointer ( ) if not item : return QModelIndex ( ) parent = item . parent if not parent or parent == self . root : return QModelIndex ( ) return self . createIndex ( parent . row , 0 , parent )
def data ( self , index , role ) : if not index . isValid ( ) : return None column = index . column ( ) item = index . internalPointer ( ) if role == self . ITEM_ROLE : return item elif role == Qt . DisplayRole : if column == 0 : return item . name elif column == 1 : if item . size : return item . size elif column == 2 : return item . type elif column == 3 : if item . modified is not None : return item . modified . strftime ( '%c' ) elif role == Qt . DecorationRole : if column == 0 : return self . iconFactory . icon ( item ) elif role == Qt . TextAlignmentRole : if column == 1 : return Qt . AlignRight else : return Qt . AlignLeft return None
def headerData ( self , section , orientation , role ) : if orientation == Qt . Horizontal : if section < len ( self . columns ) : column = self . columns [ section ] if role == Qt . DisplayRole : return column return None
def canFetchMore ( self , index ) : if not index . isValid ( ) : item = self . root else : item = index . internalPointer ( ) return item . canFetchMore ( )
def fetchMore ( self , index ) : if not index . isValid ( ) : item = self . root else : item = index . internalPointer ( ) if item . canFetchMore ( ) : startIndex = len ( item . children ) additionalChildren = item . fetchChildren ( ) endIndex = startIndex + len ( additionalChildren ) - 1 if endIndex >= startIndex : self . beginInsertRows ( index , startIndex , endIndex ) for newChild in additionalChildren : item . addChild ( newChild ) self . endInsertRows ( )
def lessThan ( self , left , right ) : sourceModel = self . sourceModel ( ) if sourceModel : leftItem = sourceModel . item ( left ) rightItem = sourceModel . item ( right ) if ( isinstance ( leftItem , Directory ) and not isinstance ( rightItem , Directory ) ) : return self . sortOrder ( ) == Qt . AscendingOrder elif ( not isinstance ( leftItem , Directory ) and isinstance ( rightItem , Directory ) ) : return self . sortOrder ( ) == Qt . DescendingOrder return super ( FilesystemSortProxy , self ) . lessThan ( left , right )
def pathIndex ( self , path ) : sourceModel = self . sourceModel ( ) if not sourceModel : return QModelIndex ( ) return self . mapFromSource ( sourceModel . pathIndex ( path ) )
def item ( self , index ) : sourceModel = self . sourceModel ( ) if not sourceModel : return None return sourceModel . item ( self . mapToSource ( index ) )
def icon ( self , index ) : sourceModel = self . sourceModel ( ) if not sourceModel : return None return sourceModel . icon ( self . mapToSource ( index ) )
def hasChildren ( self , index ) : sourceModel = self . sourceModel ( ) if not sourceModel : return False return sourceModel . hasChildren ( self . mapToSource ( index ) )
def canFetchMore ( self , index ) : sourceModel = self . sourceModel ( ) if not sourceModel : return False return sourceModel . canFetchMore ( self . mapToSource ( index ) )
def fetchMore ( self , index ) : sourceModel = self . sourceModel ( ) if not sourceModel : return False return sourceModel . fetchMore ( self . mapToSource ( index ) )
def type ( self , item ) : iconType = IconType . Unknown if isinstance ( item , riffle . model . Computer ) : iconType = IconType . Computer elif isinstance ( item , riffle . model . Mount ) : iconType = IconType . Mount elif isinstance ( item , riffle . model . Directory ) : iconType = IconType . Directory elif isinstance ( item , riffle . model . File ) : iconType = IconType . File elif isinstance ( item , riffle . model . Collection ) : iconType = IconType . Collection return iconType
def _get_max_fd ( self ) : limits = resource . getrlimit ( resource . RLIMIT_NOFILE ) result = limits [ 1 ] if result == resource . RLIM_INFINITY : result = maxfd return result
def _close_fd ( self , fd ) : try : os . close ( fd ) except OSError , exc : if exc . errno != errno . EBADF : msg = "Failed to close file descriptor {}: {}" . format ( fd , exc ) raise Error ( msg )
def _close_open_fds ( self ) : maxfd = self . _get_max_fd ( ) for fd in reversed ( range ( maxfd ) ) : if fd not in self . exclude_fds : self . _close_fd ( fd )
def _redirect ( self , stream , target ) : if target is None : target_fd = os . open ( os . devnull , os . O_RDWR ) else : target_fd = target . fileno ( ) os . dup2 ( target_fd , stream . fileno ( ) )
def is_valid_s3_url ( url ) : if url . startswith ( 'source:' ) : return True scheme , netloc , path , _ , _ , _ = urlparse ( url ) port_except = RemotePortValidationError ( 'Port value %s is not a valid s3 location' % url ) if len ( scheme ) < 2 : raise port_except if 's3' in scheme or 's3' in netloc or 's3' in path : return True else : raise port_except
def _get_template_abs_path ( filename ) : if os . path . isabs ( filename ) and os . path . isfile ( filename ) : return filename else : return os . path . join ( os . getcwd ( ) , filename )
def list ( self , s3_folder = '' , full_key_data = False ) : if not s3_folder . startswith ( '/' ) : s3_folder = '/' + s3_folder s3_prefix = self . prefix + s3_folder bucket_data = self . client . list_objects ( Bucket = self . bucket , Prefix = s3_prefix ) if full_key_data : return bucket_data [ 'Contents' ] else : return [ k [ 'Key' ] for k in bucket_data [ 'Contents' ] ]
def _build_worklfow_json ( self ) : wf_json = { 'tasks' : [ ] , 'name' : 'cloud-harness_%s' % str ( uuid . uuid4 ( ) ) } task_def = json . loads ( self . task_template . json ( ) ) d = { "name" : task_def [ 'name' ] , "outputs" : [ ] , "inputs" : [ ] , "taskType" : task_def [ 'taskType' ] } for port in self . task_template . input_ports : port_value = port . value if port_value is False : port_value = 'false' if port_value is True : port_value = 'true' d [ 'inputs' ] . append ( { "name" : port . _name , "value" : port_value } ) for port in self . task_template . output_ports : d [ 'outputs' ] . append ( { "name" : port . _name } ) wf_json [ 'tasks' ] . append ( d ) for port in self . task_template . output_ports : if hasattr ( port , 'stageToS3' ) and port . stageToS3 : save_location = '{customer_storage}/{run_name}/{port}' . format ( customer_storage = self . storage . location , run_name = self . task_template . run_name , port = port . name ) new_task = dict ( * * self . STAGE_TO_S3 ) new_task [ 'inputs' ] = [ { 'name' : 'data' , 'source' : '%s:%s' % ( task_def [ 'name' ] , port . _name ) } , { 'name' : 'destination' , 'value' : save_location } ] wf_json [ 'tasks' ] . append ( new_task ) return wf_json
def execute ( self , override_wf_json = None ) : r = self . gbdx . post ( self . URL , json = self . json if override_wf_json is None else override_wf_json ) try : r . raise_for_status ( ) except : print ( "GBDX API Status Code: %s" % r . status_code ) print ( "GBDX API Response: %s" % r . text ) self . id = None return self . id = r . json ( ) [ 'id' ] self . _refresh_status ( )
def archive ( folder , dry_run = False ) : for f in folder : if not os . path . exists ( f ) : bail ( 'folder does not exist: ' + f ) _archive_safe ( folder , PROJ_ARCHIVE , dry_run = dry_run )
def _mkdir ( p ) : isdir = os . path . isdir stack = [ os . path . abspath ( p ) ] while not isdir ( stack [ - 1 ] ) : parent_dir = os . path . dirname ( stack [ - 1 ] ) stack . append ( parent_dir ) while stack : p = stack . pop ( ) if not isdir ( p ) : os . mkdir ( p )
def list ( pattern = ( ) ) : globs = [ '*{0}*' . format ( p ) for p in pattern ] + [ '*' ] matches = [ ] offset = len ( PROJ_ARCHIVE ) + 1 for suffix in globs : glob_pattern = os . path . join ( PROJ_ARCHIVE , '*' , '*' , suffix ) matches . append ( set ( f [ offset : ] for f in glob . glob ( glob_pattern ) ) ) matches = reduce ( lambda x , y : x . intersection ( y ) , matches ) for m in sorted ( matches ) : print ( m )
def restore ( folder ) : if os . path . isdir ( folder ) : bail ( 'a folder of the same name already exists!' ) pattern = os . path . join ( PROJ_ARCHIVE , '*' , '*' , folder ) matches = glob . glob ( pattern ) if not matches : bail ( 'no project matches: ' + folder ) if len ( matches ) > 1 : print ( 'Warning: multiple matches, picking the most recent' , file = sys . stderr ) source = sorted ( matches ) [ - 1 ] print ( source , '-->' , folder ) shutil . move ( source , '.' )
def __validate_storage_path ( cls , path , projects_allowed = True ) : if not path or not isinstance ( path , str ) or path [ 0 ] != '/' or path == '/' : raise StorageArgumentException ( 'The path must be a string, start with a slash (/), and be longer' ' than 1 character.' ) if not projects_allowed and len ( [ elem for elem in path . split ( '/' ) if elem ] ) == 1 : raise StorageArgumentException ( 'This method does not accept projects in the path.' )
def new ( cls , access_token , environment = 'prod' ) : return cls ( storage_client = StorageClient . new ( access_token , environment = environment ) )
def emit ( self , record ) : msg = self . format ( record ) if not isinstance ( msg , dict ) : msg = json . loads ( msg ) self . collection . insert ( msg )
def sort ( self , f = lambda d : d [ "t" ] ) : list . sort ( self , key = f ) return self
def sum ( self ) : raw = self . raw ( ) s = 0 for i in range ( len ( raw ) ) : s += raw [ i ] [ "d" ] return s
def rfxcom ( device ) : if device is None : device = app . config . get ( 'DEVICE' ) if device is None : print ( "The serial device needs to be passed in as --device or " "set in the config as DEVICE." ) return rfxcom_collect ( device )
def create_user ( username ) : password = prompt_pass ( "Enter password" ) user = User ( username = username , password = password ) db . session . add ( user ) db . session . commit ( )
def refresh ( self ) : self . metadata = self . db . read ( self . path ) . json ( )
def streams ( self ) : result = self . db . read ( self . path , { "q" : "ls" } ) if result is None or result . json ( ) is None : return [ ] streams = [ ] for s in result . json ( ) : strm = self [ s [ "name" ] ] strm . metadata = s streams . append ( strm ) return streams
def users ( self ) : result = self . db . read ( "" , { "q" : "ls" } ) if result is None or result . json ( ) is None : return [ ] users = [ ] for u in result . json ( ) : usr = self ( u [ "name" ] ) usr . metadata = u users . append ( usr ) return users
def connectordb ( self ) : if self . __cdb is None : logging . debug ( "Logger: Connecting to " + self . serverurl ) self . __cdb = ConnectorDB ( self . apikey , url = self . serverurl ) return self . __cdb
def sync ( self ) : logging . debug ( "Logger: Syncing..." ) failed = False try : cdb = self . connectordb cdb . ping ( ) with self . synclock : c = self . database . cursor ( ) for stream in self . streams : s = cdb [ stream ] c . execute ( "SELECT * FROM cache WHERE stream=? ORDER BY timestamp ASC;" , ( stream , ) ) datapointArray = [ ] for dp in c . fetchall ( ) : datapointArray . append ( { "t" : dp [ 1 ] , "d" : json . loads ( dp [ 2 ] ) } ) if len ( s ) > 0 : newtime = s [ - 1 ] [ "t" ] while ( len ( datapointArray ) > 0 and datapointArray [ 0 ] [ "t" ] < newtime ) : logging . debug ( "Datapoint exists with older timestamp. Removing the datapoint." ) datapointArray = datapointArray [ 1 : ] if len ( datapointArray ) > 0 : logging . debug ( "%s: syncing %i datapoints" % ( stream , len ( datapointArray ) ) ) while ( len ( datapointArray ) > DATAPOINT_INSERT_LIMIT ) : s . insert_array ( datapointArray [ : DATAPOINT_INSERT_LIMIT ] ) datapointArray = datapointArray [ DATAPOINT_INSERT_LIMIT : ] c . execute ( "DELETE FROM cache WHERE stream=? AND timestamp <?" , ( stream , datapointArray [ 0 ] [ "t" ] ) ) s . insert_array ( datapointArray ) c . execute ( "DELETE FROM cache WHERE stream=? AND timestamp <=?" , ( stream , datapointArray [ - 1 ] [ "t" ] ) ) self . lastsynctime = time . time ( ) if self . onsync is not None : self . onsync ( ) except Exception as e : falied = True reraise = self . syncraise if self . onsyncfail is not None : reraise = self . onsyncfail ( e ) if reraise : raise
def stop ( self ) : with self . synclock : if self . syncthread is not None : self . syncthread . cancel ( ) self . syncthread = None
def read ( * paths ) : filename = os . path . join ( * paths ) with codecs . open ( filename , mode = 'r' , encoding = 'utf-8' ) as handle : return handle . read ( )
def download_url_job ( job , url , name = None , s3_key_path = None , cghub_key_path = None ) : work_dir = job . fileStore . getLocalTempDir ( ) fpath = download_url ( job = job , url = url , work_dir = work_dir , name = name , s3_key_path = s3_key_path , cghub_key_path = cghub_key_path ) return job . fileStore . writeGlobalFile ( fpath )
def s3am_upload_job ( job , file_id , file_name , s3_dir , s3_key_path = None ) : work_dir = job . fileStore . getLocalTempDir ( ) fpath = job . fileStore . readGlobalFile ( file_id , os . path . join ( work_dir , file_name ) ) s3am_upload ( job = job , fpath = fpath , s3_dir = s3_dir , num_cores = job . cores , s3_key_path = s3_key_path )
def labels ( ontology , output , ols_base ) : for label in get_labels ( ontology = ontology , ols_base = ols_base ) : click . echo ( label , file = output )
def tree ( ontology , output , ols_base ) : for parent , child in get_hierarchy ( ontology = ontology , ols_base = ols_base ) : click . echo ( '{}\t{}' . format ( parent , child ) , file = output )
def get_mean_insert_size ( work_dir , bam_name ) : cmd = "docker run --log-driver=none --rm -v {}:/data quay.io/ucsc_cgl/samtools " "view -f66 {}" . format ( work_dir , os . path . join ( work_dir , bam_name ) ) process = subprocess . Popen ( args = cmd , shell = True , stdout = subprocess . PIPE ) b_sum = 0.0 b_count = 0.0 while True : line = process . stdout . readline ( ) if not line : break tmp = line . split ( "\t" ) if abs ( long ( tmp [ 8 ] ) ) < 10000 : b_sum += abs ( long ( tmp [ 8 ] ) ) b_count += 1 process . wait ( ) try : mean = b_sum / b_count except ZeroDivisionError : mean = 150 print "Using insert size: %d" % mean return int ( mean )
def device ( self ) : splitted_path = self . path . split ( "/" ) return Device ( self . db , splitted_path [ 0 ] + "/" + splitted_path [ 1 ] )
def __get_empty_config ( self ) : self . _generate_config ( ) path = self . _get_config_path ( ) with open ( path , 'r' ) as readable : contents = readable . read ( ) os . remove ( path ) return contents
def _create_pipeline_command ( self , args , workdir_path , config_path ) : return ( [ self . _name , 'run' , os . path . join ( workdir_path , 'jobStore' ) , '--config' , config_path , '--workDir' , workdir_path , '--retryCount' , '1' ] + ( [ '--restart' ] if args . restart else [ ] ) )
def delete ( self , path ) : return self . handleresult ( self . r . delete ( urljoin ( self . url + CRUD_PATH , path ) ) )
def subscribe ( self , stream , callback , transform = "" ) : return self . ws . subscribe ( stream , callback , transform )
def devices ( self ) : result = self . db . read ( self . path , { "q" : "ls" } ) if result is None or result . json ( ) is None : return [ ] devices = [ ] for d in result . json ( ) : dev = self [ d [ "name" ] ] dev . metadata = d devices . append ( dev ) return devices
def send ( self , cmd ) : with self . ws_sendlock : self . ws . send ( json . dumps ( cmd ) )
def subscribe ( self , stream , callback , transform = "" ) : if self . status == "disconnected" or self . status == "disconnecting" or self . status == "connecting" : self . connect ( ) if self . status is not "connected" : return False logging . debug ( "Subscribing to %s" , stream ) self . send ( { "cmd" : "subscribe" , "arg" : stream , "transform" : transform } ) with self . subscription_lock : self . subscriptions [ stream + ":" + transform ] = callback return True
def __reconnect ( self ) : self . status = "reconnecting" if self . disconnected_time - self . connected_time > 15 * 60 : self . reconnect_time = self . reconnect_time_starting_seconds else : self . reconnect_time *= self . reconnect_time_backoff_multiplier if self . reconnect_time > self . reconnect_time_max_seconds : self . reconnect_time = self . reconnect_time_max_seconds self . reconnect_time *= 1 + random . uniform ( - 0.2 , 0.2 ) if self . reconnect_time < self . reconnect_time_starting_seconds : self . reconnect_time = self . reconnect_time_starting_seconds logging . warn ( "ConnectorDB:WS: Attempting to reconnect in %fs" , self . reconnect_time ) self . reconnector = threading . Timer ( self . reconnect_time , self . __reconnect_fnc ) self . reconnector . daemon = True self . reconnector . start ( )
def __on_open ( self , ws ) : logging . debug ( "ConnectorDB: Websocket opened" ) self . reconnect_time /= self . reconnect_time_backoff_multiplier self . status = "connected" self . lastpingtime = time . time ( ) self . __ensure_ping ( ) self . connected_time = time . time ( ) self . ws_openlock . release ( )
def __on_close ( self , ws ) : if self . status == "disconnected" : return logging . debug ( "ConnectorDB:WS: Websocket closed" ) if self . pingtimer is not None : self . pingtimer . cancel ( ) self . disconnected_time = time . time ( ) if self . status == "disconnecting" : self . status = "disconnected" elif self . status == "connected" : self . __reconnect ( )
def __on_error ( self , ws , err ) : logging . debug ( "ConnectorDB:WS: Connection Error" ) if self . status == "connecting" : self . status = "errored" self . ws_openlock . release ( )
def __on_message ( self , ws , msg ) : msg = json . loads ( msg ) logging . debug ( "ConnectorDB:WS: Msg '%s'" , msg [ "stream" ] ) stream_key = msg [ "stream" ] + ":" if "transform" in msg : stream_key += msg [ "transform" ] self . subscription_lock . acquire ( ) if stream_key in self . subscriptions : subscription_function = self . subscriptions [ stream_key ] self . subscription_lock . release ( ) fresult = subscription_function ( msg [ "stream" ] , msg [ "data" ] ) if fresult is True : fresult = msg [ "data" ] if fresult is not False and fresult is not None and msg [ "stream" ] . endswith ( "/downlink" ) and msg [ "stream" ] . count ( "/" ) == 3 : self . insert ( msg [ "stream" ] [ : - 9 ] , fresult ) else : self . subscription_lock . release ( ) logging . warn ( "ConnectorDB:WS: Msg '%s' not subscribed! Subscriptions: %s" , msg [ "stream" ] , list ( self . subscriptions . keys ( ) ) )
def write_config ( configuration ) : with open ( CONFIG_PATH , 'w' ) as f : json . dump ( configuration , f , indent = 2 , sort_keys = True )
def check ( self ) : status = _checkContainerStatus ( self . sparkContainerID , self . hdfsContainerID , sparkNoun = 'worker' , hdfsNoun = 'datanode' ) return status
def base_tokenizer ( fp ) : if isinstance ( fp , StringIO ) : template_file = fp size = template_file . len else : #empty file check if os . fstat ( fp . fileno ( ) ) . st_size == 0 : yield TOKEN_EOF , 'EOF' , 0 , 0 return template_file = mmap . mmap ( fp . fileno ( ) , 0 , access = mmap . ACCESS_READ ) size = template_file . size ( ) lineno = 0 while 1 : lineno += 1 pos = 1 if template_file . tell ( ) == size : yield TOKEN_EOF , 'EOF' , lineno , 0 break line = template_file . readline ( ) . decode ( 'utf-8' ) line = line . replace ( '\r\n' , '' ) line = line . replace ( '\n' , '' ) if re_comment . match ( line ) : continue last_text = deque ( ) while line : line_len = len ( line ) for token in tokens : m = token . regex . match ( line ) if m : if last_text : yield TOKEN_TEXT , '' . join ( last_text ) , lineno , pos pos += len ( last_text ) last_text . clear ( ) offset , value = m . end ( ) , m . group ( ) line = line [ offset : ] yield token , value , lineno , pos pos += offset break if line_len == len ( line ) : last_text . append ( line [ 0 ] ) line = line [ 1 : ] if last_text : yield TOKEN_TEXT , '' . join ( last_text ) , lineno , pos pos += len ( last_text ) last_text . clear ( ) yield TOKEN_NEWLINE , '\n' , lineno , pos template_file . close ( )
def fitness ( self ) : if len ( self . __members ) != 0 : if self . __num_processes > 1 : members = [ m . get ( ) for m in self . __members ] else : members = self . __members return sum ( m . fitness_score for m in members ) / len ( members ) else : return None
def ave_cost_fn_val ( self ) : if len ( self . __members ) != 0 : if self . __num_processes > 1 : members = [ m . get ( ) for m in self . __members ] else : members = self . __members return sum ( m . cost_fn_val for m in members ) / len ( members ) else : return None
def med_cost_fn_val ( self ) : if len ( self . __members ) != 0 : if self . __num_processes > 1 : members = [ m . get ( ) for m in self . __members ] else : members = self . __members return median ( [ m . cost_fn_val for m in members ] ) else : return None
def parameters ( self ) : if len ( self . __members ) != 0 : if self . __num_processes > 1 : members = [ m . get ( ) for m in self . __members ] else : members = self . __members params = { } for p in self . __parameters : params [ p . name ] = sum ( m . parameters [ p . name ] for m in members ) / len ( members ) return params else : return None
def members ( self ) : if self . __num_processes > 1 : return [ m . get ( ) for m in self . __members ] else : return self . __members
def get_environ_vars ( self ) : for key , val in os . environ . items ( ) : if _environ_prefix_re . search ( key ) : yield ( _environ_prefix_re . sub ( "" , key ) . lower ( ) , val )
def _transform_result ( typ , result ) : if issubclass ( typ , bytes ) : return tostring ( result , encoding = 'utf-8' ) elif issubclass ( typ , unicode ) : return tostring ( result , encoding = 'unicode' ) else : return result
def is_single_class ( ) : ret = False counts = get_counts ( ) if counts [ "classes" ] < 1 and counts [ "modules" ] < 1 : ret = counts [ "tests" ] > 0 else : ret = counts [ "classes" ] <= 1 and counts [ "modules" ] <= 1 return ret
def is_single_module ( ) : ret = False counts = get_counts ( ) if counts [ "modules" ] == 1 : ret = True elif counts [ "modules" ] < 1 : ret = is_single_class ( ) return ret
def validate_params ( request ) : if 'params' in request : correct_params = isinstance ( request [ 'params' ] , ( list , dict ) ) error = 'Incorrect parameter values' assert correct_params , error
def validate_id ( request ) : if 'id' in request : correct_id = isinstance ( request [ 'id' ] , ( string_types , int , None ) , ) error = 'Incorrect identifier' assert correct_id , error
def _escape_argspec ( obj , iterable , escape ) : for key , value in iterable : if hasattr ( value , '__html__' ) or isinstance ( value , string_types ) : obj [ key ] = escape ( value ) return obj
def sub_symbols ( pattern , code , symbol ) : return pattern . replace ( '¤¤',  c de). r e place(' ¤ ', s y bol) 
def amount_converter ( obj ) : if isinstance ( obj , Decimal ) : return obj elif isinstance ( obj , ( str , int , float ) ) : return Decimal ( str ( obj ) ) else : raise ValueError ( 'do not know how to convert: {}' . format ( type ( obj ) ) )
def exception ( self ) : buf = traceback . format_exception_only ( self . exc_type , self . exc_value ) rv = '' . join ( buf ) . strip ( ) return rv . decode ( 'utf-8' , 'replace' ) if PY2 else rv
def render_summary ( self , include_title = True ) : title = '' frames = [ ] classes = [ 'traceback' ] if not self . frames : classes . append ( 'noframe-traceback' ) if include_title : if self . is_syntax_error : title = u'Syntax Error' else : title = u'Traceback <em>(most recent call last)</em>:' for frame in self . frames : frames . append ( u'<li%s>%s' % ( frame . info and u' title="%s"' % escape ( frame . info ) or u'' , frame . render ( ) ) ) if self . is_syntax_error : description_wrapper = u'<pre class=syntaxerror>%s</pre>' else : description_wrapper = u'<blockquote>%s</blockquote>' return SUMMARY_HTML % { 'classes' : u' ' . join ( classes ) , 'title' : title and u'<h3>%s</h3>' % title or u'' , 'frames' : u'\n' . join ( frames ) , 'description' : description_wrapper % escape ( self . exception ) }
def generate_plaintext_traceback ( self ) : yield u'Traceback (most recent call last):' for frame in self . frames : yield u'  File "%s", line %s, in %s' % ( frame . filename , frame . lineno , frame . function_name ) yield u'    ' + frame . current_line . strip ( ) yield self . exception
def get_annotated_lines ( self ) : lines = [ Line ( idx + 1 , x ) for idx , x in enumerate ( self . sourcelines ) ] if hasattr ( self . code , 'co_firstlineno' ) : lineno = self . code . co_firstlineno - 1 while lineno > 0 : if _funcdef_re . match ( lines [ lineno ] . code ) : break lineno -= 1 try : offset = len ( inspect . getblock ( [ x . code + '\n' for x in lines [ lineno : ] ] ) ) except TokenError : offset = 0 for line in lines [ lineno : lineno + offset ] : line . in_frame = True try : lines [ self . lineno - 1 ] . current = True except IndexError : pass return lines
def render_source ( self ) : return SOURCE_TABLE_HTML % u'\n' . join ( line . render ( ) for line in self . get_annotated_lines ( ) )
def _get_content_type ( url , session ) : scheme , netloc , path , query , fragment = urllib_parse . urlsplit ( url ) if scheme not in ( 'http' , 'https' ) : return '' resp = session . head ( url , allow_redirects = True ) resp . raise_for_status ( ) return resp . headers . get ( "Content-Type" , "" )
def links ( self ) : for anchor in self . parsed . findall ( ".//a" ) : if anchor . get ( "href" ) : href = anchor . get ( "href" ) url = self . clean_link ( urllib_parse . urljoin ( self . base_url , href ) ) internal = None if self . api_version and self . api_version >= 2 : internal = bool ( anchor . get ( "rel" ) and "internal" in anchor . get ( "rel" ) . split ( ) ) yield Link ( url , self , internal = internal )
def find_data_files ( self , package , src_dir ) : globs = ( self . package_data . get ( '' , [ ] ) + self . package_data . get ( package , [ ] ) ) files = self . manifest_files . get ( package , [ ] ) [ : ] for pattern in globs : files . extend ( glob ( os . path . join ( src_dir , convert_path ( pattern ) ) ) ) return self . exclude_data_files ( package , src_dir , files )
def check_package ( self , package , package_dir ) : try : return self . packages_checked [ package ] except KeyError : pass init_py = orig . build_py . check_package ( self , package , package_dir ) self . packages_checked [ package ] = init_py if not init_py or not self . distribution . namespace_packages : return init_py for pkg in self . distribution . namespace_packages : if pkg == package or pkg . startswith ( package + '.' ) : break else : return init_py f = open ( init_py , 'rbU' ) if 'declare_namespace' . encode ( ) not in f . read ( ) : from distutils . errors import DistutilsError raise DistutilsError ( "Namespace package problem: %s is a namespace package, but " "its\n__init__.py does not call declare_namespace()! Please " 'fix it.\n(See the setuptools manual under ' '"Namespace Packages" for details.)\n"' % ( package , ) ) f . close ( ) return init_py
def exclude_data_files ( self , package , src_dir , files ) : globs = ( self . exclude_package_data . get ( '' , [ ] ) + self . exclude_package_data . get ( package , [ ] ) ) bad = [ ] for pattern in globs : bad . extend ( fnmatch . filter ( files , os . path . join ( src_dir , convert_path ( pattern ) ) ) ) bad = dict . fromkeys ( bad ) seen = { } return [ f for f in files if f not in bad and f not in seen and seen . setdefault ( f , 1 ) ]
def ignore_comments ( iterator ) : for line in iterator : line = COMMENT_RE . sub ( '' , line ) line = line . strip ( ) if line : yield line
def skip_regex ( lines , options ) : skip_regex = options . skip_requirements_regex if options else None if skip_regex : lines = filterfalse ( re . compile ( skip_regex ) . search , lines ) return lines
def compile ( marker ) : try : return _cache [ marker ] except KeyError : pass if not marker . strip ( ) : def marker_fn ( environment = None , override = None ) : """""" return True else : compiled_marker = compile_marker ( parse_marker ( marker ) ) def marker_fn ( environment = None , override = None ) : """override updates environment""" if override is None : override = { } if environment is None : environment = default_environment ( ) environment . update ( override ) return eval ( compiled_marker , environment ) marker_fn . __doc__ = marker _cache [ marker ] = marker_fn return _cache [ marker ]
def visit ( self , node ) : if not isinstance ( node , self . ALLOWED ) : raise SyntaxError ( 'Not allowed in environment markers.\n%s\n%s' % ( self . statement , ( ' ' * node . col_offset ) + '^' ) ) return ast . NodeTransformer . visit ( self , node )
def visit_Attribute ( self , node ) : new_node = ast . Name ( "%s.%s" % ( node . value . id , node . attr ) , node . ctx ) return ast . copy_location ( new_node , node )
def push ( self ) : self . _refcnt += 1 _app_ctx_stack . push ( self ) appcontext_pushed . send ( self . app )
def pop ( self , exc = None ) : self . _refcnt -= 1 if self . _refcnt <= 0 : if exc is None : exc = sys . exc_info ( ) [ 1 ] self . app . do_teardown_appcontext ( exc ) rv = _app_ctx_stack . pop ( ) assert rv is self , 'Popped wrong app context.  (%r instead of %r)' % ( rv , self ) appcontext_popped . send ( self . app )
def push ( self ) : top = _request_ctx_stack . top if top is not None and top . preserved : top . pop ( top . _preserved_exc ) app_ctx = _app_ctx_stack . top if app_ctx is None or app_ctx . app != self . app : app_ctx = self . app . app_context ( ) app_ctx . push ( ) self . _implicit_app_ctx_stack . append ( app_ctx ) else : self . _implicit_app_ctx_stack . append ( None ) _request_ctx_stack . push ( self ) self . session = self . app . open_session ( self . request ) if self . session is None : self . session = self . app . make_null_session ( )
def dist_in_usersite ( dist ) : norm_path = normalize_path ( dist_location ( dist ) ) return norm_path . startswith ( normalize_path ( user_site ) )
def dist_is_editable ( dist ) : from pip import FrozenRequirement req = FrozenRequirement . from_dist ( dist , [ ] ) return req . editable
def run ( self , options , args ) : shells = COMPLETION_SCRIPTS . keys ( ) shell_options = [ '--' + shell for shell in sorted ( shells ) ] if options . shell in shells : script = COMPLETION_SCRIPTS . get ( options . shell , '' ) print ( BASE_COMPLETION % { 'script' : script , 'shell' : options . shell } ) else : sys . stderr . write ( 'ERROR: You must pass %s\n' % ' or ' . join ( shell_options ) )
def root_is_purelib ( name , wheeldir ) : name_folded = name . replace ( "-" , "_" ) for item in os . listdir ( wheeldir ) : match = dist_info_re . match ( item ) if match and match . group ( 'name' ) == name_folded : with open ( os . path . join ( wheeldir , item , 'WHEEL' ) ) as wheel : for line in wheel : line = line . lower ( ) . rstrip ( ) if line == "root-is-purelib: true" : return True return False
def iter_symbols ( code ) : for name in code . co_names : yield name for const in code . co_consts : if isinstance ( const , basestring ) : yield const elif isinstance ( const , CodeType ) : for name in iter_symbols ( const ) : yield name
def ensure_fresh_rates ( func ) : def wrapper ( self , * args , * * kwargs ) : if self . last_updated + timedelta ( minutes = 5 ) < zulu . now ( ) : self . refresh ( ) return func ( self , * args , * * kwargs ) return wrapper
def write_delete_marker_file ( directory ) : filepath = os . path . join ( directory , PIP_DELETE_MARKER_FILENAME ) with open ( filepath , 'w' ) as marker_fp : marker_fp . write ( DELETE_MARKER_MESSAGE )
def running_under_virtualenv ( ) : if hasattr ( sys , 'real_prefix' ) : return True elif sys . prefix != getattr ( sys , "base_prefix" , sys . prefix ) : return True return False
def __get_username ( ) : if WINDOWS : return getpass . getuser ( ) import pwd return pwd . getpwuid ( os . geteuid ( ) ) . pw_name
def distutils_scheme ( dist_name , user = False , home = None , root = None , isolated = False ) : from distutils . dist import Distribution scheme = { } if isolated : extra_dist_args = { "script_args" : [ "--no-user-cfg" ] } else : extra_dist_args = { } dist_args = { 'name' : dist_name } dist_args . update ( extra_dist_args ) d = Distribution ( dist_args ) d . parse_config_files ( ) i = d . get_command_obj ( 'install' , create = True ) i . user = user or i . user i . home = home or i . home i . root = root or i . root i . finalize_options ( ) for key in SCHEME_KEYS : scheme [ key ] = getattr ( i , 'install_' + key ) if i . install_lib is not None : scheme . update ( dict ( purelib = i . install_lib , platlib = i . install_lib ) ) if running_under_virtualenv ( ) : scheme [ 'headers' ] = os . path . join ( sys . prefix , 'include' , 'site' , 'python' + sys . version [ : 3 ] , dist_name , ) if root is not None : scheme [ "headers" ] = os . path . join ( root , os . path . abspath ( scheme [ "headers" ] ) [ 1 : ] , ) return scheme
def install_script ( self , dist , script_name , script_text , dev_path = None ) : spec = str ( dist . as_requirement ( ) ) is_script = is_python_script ( script_text , script_name ) if is_script : script_text = ( ScriptWriter . get_header ( script_text ) + self . _load_template ( dev_path ) % locals ( ) ) self . write_script ( script_name , _to_ascii ( script_text ) , 'b' )
def install_site_py ( self ) : if self . sitepy_installed : return sitepy = os . path . join ( self . install_dir , "site.py" ) source = resource_string ( "setuptools" , "site-patch.py" ) current = "" if os . path . exists ( sitepy ) : log . debug ( "Checking existing site.py in %s" , self . install_dir ) f = open ( sitepy , 'rb' ) current = f . read ( ) if PY3 : current = current . decode ( ) f . close ( ) if not current . startswith ( 'def __boot():' ) : raise DistutilsError ( "%s is not a setuptools-generated site.py; please" " remove it." % sitepy ) if current != source : log . info ( "Creating %s" , sitepy ) if not self . dry_run : ensure_directory ( sitepy ) f = open ( sitepy , 'wb' ) f . write ( source ) f . close ( ) self . byte_compile ( [ sitepy ] ) self . sitepy_installed = True
def save ( self ) : if not self . dirty : return data = '\n' . join ( map ( self . make_relative , self . paths ) ) if data : log . debug ( "Saving %s" , self . filename ) data = ( "import sys; sys.__plen = len(sys.path)\n" "%s\n" "import sys; new=sys.path[sys.__plen:];" " del sys.path[sys.__plen:];" " p=getattr(sys,'__egginsert',0); sys.path[p:p]=new;" " sys.__egginsert = p+len(new)\n" ) % data if os . path . islink ( self . filename ) : os . unlink ( self . filename ) f = open ( self . filename , 'wt' ) f . write ( data ) f . close ( ) elif os . path . exists ( self . filename ) : log . debug ( "Deleting empty %s" , self . filename ) os . unlink ( self . filename ) self . dirty = False
def add_filters ( self , filterer , filters ) : for f in filters : try : filterer . addFilter ( self . config [ 'filters' ] [ f ] ) except StandardError as e : raise ValueError ( 'Unable to add filter %r: %s' % ( f , e ) )
def configure_handler ( self , config ) : formatter = config . pop ( 'formatter' , None ) if formatter : try : formatter = self . config [ 'formatters' ] [ formatter ] except StandardError as e : raise ValueError ( 'Unable to set formatter ' '%r: %s' % ( formatter , e ) ) level = config . pop ( 'level' , None ) filters = config . pop ( 'filters' , None ) if '()' in config : c = config . pop ( '()' ) if not hasattr ( c , '__call__' ) and hasattr ( types , 'ClassType' ) and type ( c ) != types . ClassType : c = self . resolve ( c ) factory = c else : klass = self . resolve ( config . pop ( 'class' ) ) if issubclass ( klass , logging . handlers . MemoryHandler ) and 'target' in config : try : config [ 'target' ] = self . config [ 'handlers' ] [ config [ 'target' ] ] except StandardError as e : raise ValueError ( 'Unable to set target handler ' '%r: %s' % ( config [ 'target' ] , e ) ) elif issubclass ( klass , logging . handlers . SMTPHandler ) and 'mailhost' in config : config [ 'mailhost' ] = self . as_tuple ( config [ 'mailhost' ] ) elif issubclass ( klass , logging . handlers . SysLogHandler ) and 'address' in config : config [ 'address' ] = self . as_tuple ( config [ 'address' ] ) factory = klass kwargs = dict ( ( k , config [ k ] ) for k in config if valid_ident ( k ) ) try : result = factory ( * * kwargs ) except TypeError as te : if "'stream'" not in str ( te ) : raise #(e.g. by Django) kwargs [ 'strm' ] = kwargs . pop ( 'stream' ) result = factory ( * * kwargs ) if formatter : result . setFormatter ( formatter ) if level is not None : result . setLevel ( _checkLevel ( level ) ) if filters : self . add_filters ( result , filters ) return result
def add_handlers ( self , logger , handlers ) : for h in handlers : try : logger . addHandler ( self . config [ 'handlers' ] [ h ] ) except StandardError as e : raise ValueError ( 'Unable to add handler %r: %s' % ( h , e ) )
def common_logger_config ( self , logger , config , incremental = False ) : level = config . get ( 'level' , None ) if level is not None : logger . setLevel ( _checkLevel ( level ) ) if not incremental : for h in logger . handlers [ : ] : logger . removeHandler ( h ) handlers = config . get ( 'handlers' , None ) if handlers : self . add_handlers ( logger , handlers ) filters = config . get ( 'filters' , None ) if filters : self . add_filters ( logger , filters )
def _execfile ( filename , globals , locals = None ) : mode = 'rb' with open ( filename , mode ) as stream : script = stream . read ( ) if sys . version_info [ : 2 ] < ( 2 , 7 ) or sys . version_info [ : 2 ] >= ( 3 , 0 ) and sys . version_info [ : 2 ] < ( 3 , 2 ) : script = script . replace ( b'\r\n' , b'\n' ) script = script . replace ( b'\r' , b'\n' ) if locals is None : locals = globals code = compile ( script , filename , 'exec' ) exec ( code , globals , locals )
def override_temp ( replacement ) : if not os . path . isdir ( replacement ) : os . makedirs ( replacement ) saved = tempfile . tempdir tempfile . tempdir = replacement try : yield finally : tempfile . tempdir = saved
def run_setup ( setup_script , args ) : setup_dir = os . path . abspath ( os . path . dirname ( setup_script ) ) with setup_context ( setup_dir ) : try : sys . argv [ : ] = [ setup_script ] + list ( args ) sys . path . insert ( 0 , setup_dir ) working_set . __init__ ( ) working_set . callbacks . append ( lambda dist : dist . activate ( ) ) def runner ( ) : ns = dict ( __file__ = setup_script , __name__ = '__main__' ) _execfile ( setup_script , ns ) DirectorySandbox ( setup_dir ) . run ( runner ) except SystemExit as v : if v . args and v . args [ 0 ] : raise
def getitem ( self , obj , argument ) : try : return obj [ argument ] except ( TypeError , LookupError ) : if isinstance ( argument , string_types ) : try : attr = str ( argument ) except Exception : pass else : try : return getattr ( obj , attr ) except AttributeError : pass return self . undefined ( obj = obj , name = argument )
def find_eggs_in_zip ( importer , path_item , only = False ) : if importer . archive . endswith ( '.whl' ) : return metadata = EggMetadata ( importer ) if metadata . has_metadata ( 'PKG-INFO' ) : yield Distribution . from_filename ( path_item , metadata = metadata ) if only : return for subitem in metadata . resource_listdir ( '/' ) : if subitem . endswith ( '.egg' ) : subpath = os . path . join ( path_item , subitem ) for dist in find_eggs_in_zip ( zipimport . zipimporter ( subpath ) , subpath ) : yield dist
def find_on_path ( importer , path_item , only = False ) : path_item = _normalize_cached ( path_item ) if os . path . isdir ( path_item ) and os . access ( path_item , os . R_OK ) : if path_item . lower ( ) . endswith ( '.egg' ) : yield Distribution . from_filename ( path_item , metadata = PathMetadata ( path_item , os . path . join ( path_item , 'EGG-INFO' ) ) ) else : for entry in os . listdir ( path_item ) : lower = entry . lower ( ) if lower . endswith ( '.egg-info' ) or lower . endswith ( '.dist-info' ) : fullpath = os . path . join ( path_item , entry ) if os . path . isdir ( fullpath ) : metadata = PathMetadata ( path_item , fullpath ) else : metadata = FileMetadata ( fullpath ) yield Distribution . from_location ( path_item , entry , metadata , precedence = DEVELOP_DIST ) elif not only and lower . endswith ( '.egg' ) : dists = find_distributions ( os . path . join ( path_item , entry ) ) for dist in dists : yield dist elif not only and lower . endswith ( '.egg-link' ) : with open ( os . path . join ( path_item , entry ) ) as entry_file : entry_lines = entry_file . readlines ( ) for line in entry_lines : if not line . strip ( ) : continue path = os . path . join ( path_item , line . rstrip ( ) ) dists = find_distributions ( path ) for item in dists : yield item break
def declare_namespace ( packageName ) : imp . acquire_lock ( ) try : if packageName in _namespace_packages : return path , parent = sys . path , None if '.' in packageName : parent = '.' . join ( packageName . split ( '.' ) [ : - 1 ] ) declare_namespace ( parent ) if parent not in _namespace_packages : __import__ ( parent ) try : path = sys . modules [ parent ] . __path__ except AttributeError : raise TypeError ( "Not a package:" , parent ) _namespace_packages . setdefault ( parent , [ ] ) . append ( packageName ) _namespace_packages . setdefault ( packageName , [ ] ) for path_item in path : _handle_ns ( packageName , path_item ) finally : imp . release_lock ( )
def _get_mro ( cls ) : if not isinstance ( cls , type ) : class cls ( cls , object ) : pass return cls . __mro__ [ 1 : ] return cls . __mro__
def _find_adapter ( registry , ob ) : for t in _get_mro ( getattr ( ob , '__class__' , type ( ob ) ) ) : if t in registry : return registry [ t ]
def ensure_directory ( path ) : dirname = os . path . dirname ( path ) if not os . path . isdir ( dirname ) : os . makedirs ( dirname )
def insert_on ( self , path , loc = None ) : loc = loc or self . location if not loc : return nloc = _normalize_cached ( loc ) bdir = os . path . dirname ( nloc ) npath = [ ( p and _normalize_cached ( p ) or p ) for p in path ] for p , item in enumerate ( npath ) : if item == nloc : break elif item == bdir and self . precedence == EGG_DIST : if path is sys . path : self . check_version_conflict ( ) path . insert ( p , loc ) npath . insert ( p , nloc ) break else : if path is sys . path : self . check_version_conflict ( ) path . append ( loc ) return while True : try : np = npath . index ( nloc , p + 1 ) except ValueError : break else : del npath [ np ] , path [ np ] p = np return
def parse_pattern ( pattern ) : if isinstance ( pattern , NumberPattern ) : return pattern def _match_number ( pattern ) : rv = number_re . search ( pattern ) if rv is None : raise ValueError ( 'Invalid number pattern %r' % pattern ) return rv . groups ( ) pos_pattern = pattern if ';' in pattern : pos_pattern , neg_pattern = pattern . split ( ';' , 1 ) pos_prefix , number , pos_suffix = _match_number ( pos_pattern ) neg_prefix , _ , neg_suffix = _match_number ( neg_pattern ) else : pos_prefix , number , pos_suffix = _match_number ( pos_pattern ) neg_prefix = '-' + pos_prefix neg_suffix = pos_suffix if 'E' in number : number , exp = number . split ( 'E' , 1 ) else : exp = None if '@' in number : if '.' in number and '0' in number : raise ValueError ( 'Significant digit patterns can not contain ' '"@" or "0"' ) if '.' in number : integer , fraction = number . rsplit ( '.' , 1 ) else : integer = number fraction = '' def parse_precision ( p ) : """Calculate the min and max allowed digits""" min = max = 0 for c in p : if c in '@0' : min += 1 max += 1 elif c == '#' : max += 1 elif c == ',' : continue else : break return min , max int_prec = parse_precision ( integer ) frac_prec = parse_precision ( fraction ) if exp : exp_plus = exp . startswith ( '+' ) exp = exp . lstrip ( '+' ) exp_prec = parse_precision ( exp ) else : exp_plus = None exp_prec = None grouping = babel . numbers . parse_grouping ( integer ) return NumberPattern ( pattern , ( pos_prefix , neg_prefix ) , ( pos_suffix , neg_suffix ) , grouping , int_prec , frac_prec , exp_prec , exp_plus )
def get_decimal_quantum ( precision ) : assert isinstance ( precision , ( int , decimal . Decimal ) ) return decimal . Decimal ( 10 ) ** ( - precision )
def scientific_notation_elements ( self , value , locale ) : exp = value . adjusted ( ) value = value * get_decimal_quantum ( exp ) assert value . adjusted ( ) == 0 lead_shift = max ( [ 1 , min ( self . int_prec ) ] ) - 1 exp = exp - lead_shift value = value * get_decimal_quantum ( - lead_shift ) exp_sign = '' if exp < 0 : exp_sign = babel . numbers . get_minus_sign_symbol ( locale ) elif self . exp_plus : exp_sign = babel . numbers . get_plus_sign_symbol ( locale ) exp = abs ( exp ) return value , exp , exp_sign
def total_seconds ( td ) : if hasattr ( td , 'total_seconds' ) : return td . total_seconds ( ) ms = td . microseconds secs = ( td . seconds + td . days * 24 * 3600 ) return ( ms + secs * 10 ** 6 ) / 10 ** 6
def check_extras ( dist , attr , value ) : try : for k , v in value . items ( ) : if ':' in k : k , m = k . split ( ':' , 1 ) if pkg_resources . invalid_marker ( m ) : raise DistutilsSetupError ( "Invalid environment marker: " + m ) list ( pkg_resources . parse_requirements ( v ) ) except ( TypeError , ValueError , AttributeError ) : raise DistutilsSetupError ( "'extras_require' must be a dictionary whose values are " "strings or lists of strings containing valid project/version " "requirement specifiers." )
def check_requirements ( dist , attr , value ) : try : list ( pkg_resources . parse_requirements ( value ) ) except ( TypeError , ValueError ) as error : tmpl = ( "{attr!r} must be a string or list of strings " "containing valid project/version requirement specifiers; {error}" ) raise DistutilsSetupError ( tmpl . format ( attr = attr , error = error ) )
def fetch_build_egg ( self , req ) : try : cmd = self . _egg_fetcher cmd . package_index . to_scan = [ ] except AttributeError : from setuptools . command . easy_install import easy_install dist = self . __class__ ( { 'script_args' : [ 'easy_install' ] } ) dist . parse_config_files ( ) opts = dist . get_option_dict ( 'easy_install' ) keep = ( 'find_links' , 'site_dirs' , 'index_url' , 'optimize' , 'site_dirs' , 'allow_hosts' ) for key in list ( opts ) : if key not in keep : del opts [ key ] if self . dependency_links : links = self . dependency_links [ : ] if 'find_links' in opts : links = opts [ 'find_links' ] [ 1 ] . split ( ) + links opts [ 'find_links' ] = ( 'setup' , links ) install_dir = self . get_egg_cache_dir ( ) cmd = easy_install ( dist , args = [ "x" ] , install_dir = install_dir , exclude_scripts = True , always_copy = False , build_directory = None , editable = False , upgrade = False , multi_version = True , no_report = True , user = False ) cmd . ensure_finalized ( ) self . _egg_fetcher = cmd return cmd . easy_install ( req )
def do_dice_roll ( ) : options = get_options ( ) dice = Dice ( options . sides ) rolls = [ dice . roll ( ) for n in range ( options . number ) ] for roll in rolls : print ( 'rolled' , roll ) if options . number > 1 : print ( 'total' , sum ( rolls ) )
def price_converter ( obj ) : if isinstance ( obj , str ) : obj = PriceClass . parse ( obj ) return obj
def get_method ( self , args ) : try : method = self . app [ args [ 'method' ] ] except KeyError : method_not_found ( args [ 'id' ] ) else : return method
def apply ( self , method , args ) : try : params = args [ 'params' ] if isinstance ( params , dict ) : result = method ( * * params ) else : result = method ( * params ) except Exception as error : server_error ( args [ 'id' ] , error ) else : return result
def blueprint ( self ) : if self . url_rule and '.' in self . url_rule . endpoint : return self . url_rule . endpoint . rsplit ( '.' , 1 ) [ 0 ]
def cleanup_files ( self ) : logger . debug ( 'Cleaning up...' ) with indent_log ( ) : for req in self . reqs_to_cleanup : req . remove_temporary_source ( )
def _get_all_ns_packages ( self ) : nsp = set ( ) for pkg in self . distribution . namespace_packages or [ ] : pkg = pkg . split ( '.' ) while pkg : nsp . add ( '.' . join ( pkg ) ) pkg . pop ( ) return sorted ( nsp )
def default ( self , obj ) : if isinstance ( obj , models . Model ) : return self . encode ( model_to_dict ( obj ) ) elif isinstance ( obj , models . query . QuerySet ) : return serializers . serialize ( 'json' , obj ) else : return super ( JsonResponseEncoder , self ) . default ( obj )
def tokenize_annotated ( doc , annotation ) : tokens = tokenize ( doc , include_hrefs = False ) for tok in tokens : tok . annotation = annotation return tokens
def copy_annotations ( src , dest ) : assert len ( src ) == len ( dest ) for src_tok , dest_tok in zip ( src , dest ) : dest_tok . annotation = src_tok . annotation
def fixup_chunks ( chunks ) : tag_accum = [ ] cur_word = None result = [ ] for chunk in chunks : if isinstance ( chunk , tuple ) : if chunk [ 0 ] == 'img' : src = chunk [ 1 ] tag , trailing_whitespace = split_trailing_whitespace ( chunk [ 2 ] ) cur_word = tag_token ( 'img' , src , html_repr = tag , pre_tags = tag_accum , trailing_whitespace = trailing_whitespace ) tag_accum = [ ] result . append ( cur_word ) elif chunk [ 0 ] == 'href' : href = chunk [ 1 ] cur_word = href_token ( href , pre_tags = tag_accum , trailing_whitespace = " " ) tag_accum = [ ] result . append ( cur_word ) continue if is_word ( chunk ) : chunk , trailing_whitespace = split_trailing_whitespace ( chunk ) cur_word = token ( chunk , pre_tags = tag_accum , trailing_whitespace = trailing_whitespace ) tag_accum = [ ] result . append ( cur_word ) elif is_start_tag ( chunk ) : tag_accum . append ( chunk ) elif is_end_tag ( chunk ) : if tag_accum : tag_accum . append ( chunk ) else : assert cur_word , ( "Weird state, cur_word=%r, result=%r, chunks=%r of %r" % ( cur_word , result , chunk , chunks ) ) cur_word . post_tags . append ( chunk ) else : assert ( 0 ) if not result : return [ token ( '' , pre_tags = tag_accum ) ] else : result [ - 1 ] . post_tags . extend ( tag_accum ) return result
def start_tag ( el ) : return '<%s%s>' % ( el . tag , '' . join ( [ ' %s="%s"' % ( name , html_escape ( value , True ) ) for name , value in el . attrib . items ( ) ] ) )
def _fixup_ins_del_tags ( doc ) : for tag in [ 'ins' , 'del' ] : for el in doc . xpath ( 'descendant-or-self::%s' % tag ) : if not _contains_block_level_tag ( el ) : continue _move_el_inside_block ( el , tag = tag ) el . drop_tag ( )
def cache_url ( self , * * kwargs ) : query = { 'Operation' : self . Operation , 'Service' : "AWSECommerceService" , 'Version' : self . Version , } query . update ( kwargs ) service_domain = SERVICE_DOMAINS [ self . Region ] [ 0 ] return "http://" + service_domain + "/onca/xml?" + _quote_query ( query )
def document_fromstring ( html , guess_charset = True , parser = None ) : if not isinstance ( html , _strings ) : raise TypeError ( 'string required' ) if parser is None : parser = html_parser return parser . parse ( html , useChardet = guess_charset ) . getroot ( )
def export ( self , location ) : url , rev = self . get_url_rev ( ) rev_options = get_rev_options ( url , rev ) logger . info ( 'Exporting svn repository %s to %s' , url , location ) with indent_log ( ) : if os . path . exists ( location ) : rmtree ( location ) self . run_command ( [ 'export' ] + rev_options + [ url , location ] , show_stdout = False )
def get_revision ( self , location ) : revision = 0 for base , dirs , files in os . walk ( location ) : if self . dirname not in dirs : dirs [ : ] = [ ] continue dirs . remove ( self . dirname ) entries_fn = os . path . join ( base , self . dirname , 'entries' ) if not os . path . exists ( entries_fn ) : continue dirurl , localrev = self . _get_svn_url_rev ( base ) if base == location : base_url = dirurl + '/' elif not dirurl or not dirurl . startswith ( base_url ) : dirs [ : ] = [ ] continue revision = max ( revision , localrev ) return revision
def unique ( iterable ) : seen = set ( ) for value in iterable : if not value in seen : seen . add ( value ) yield value
def handle_requires ( metadata , pkg_info , key ) : may_requires = defaultdict ( list ) for value in pkg_info . get_all ( key ) : extra_match = EXTRA_RE . search ( value ) if extra_match : groupdict = extra_match . groupdict ( ) condition = groupdict [ 'condition' ] extra = groupdict [ 'extra' ] package = groupdict [ 'package' ] if condition . endswith ( ' and ' ) : condition = condition [ : - 5 ] else : condition , extra = None , None package = value key = MayRequiresKey ( condition , extra ) may_requires [ key ] . append ( package ) if may_requires : metadata [ 'run_requires' ] = [ ] for key , value in may_requires . items ( ) : may_requirement = { 'requires' : value } if key . extra : may_requirement [ 'extra' ] = key . extra if key . condition : may_requirement [ 'environment' ] = key . condition metadata [ 'run_requires' ] . append ( may_requirement ) if not 'extras' in metadata : metadata [ 'extras' ] = [ ] metadata [ 'extras' ] . extend ( [ key . extra for key in may_requires . keys ( ) if key . extra ] )
def requires_to_requires_dist ( requirement ) : requires_dist = [ ] for op , ver in requirement . specs : requires_dist . append ( op + ver ) if not requires_dist : return '' return " (%s)" % ',' . join ( requires_dist )
def modules ( self ) : sys . path . insert ( 0 , self . basedir ) for p in self . paths ( ) : try : module_name = self . module_path ( p ) logger . debug ( "Importing {} from path {}" . format ( module_name , p ) ) m = importlib . import_module ( module_name ) yield m except Exception as e : logger . warning ( 'Caught exception while importing {}: {}' . format ( p , e ) ) logger . warning ( e , exc_info = True ) error_info = getattr ( self , 'error_info' , None ) if not error_info : exc_info = sys . exc_info ( ) #raise e.__class__, e, exc_info[2] #self.error_info = (e, exc_info) self . error_info = exc_info continue sys . path . pop ( 0 )
def classes ( self ) : for module in self . modules ( ) : cs = inspect . getmembers ( module , inspect . isclass ) class_name = getattr ( self , 'class_name' , '' ) class_regex = '' if class_name : if class_name . startswith ( "*" ) : class_name = class_name . strip ( "*" ) class_regex = re . compile ( r'.*?{}' . format ( class_name ) , re . I ) else : class_regex = re . compile ( r'^{}' . format ( class_name ) , re . I ) for c_name , c in cs : can_yield = True if class_regex and not class_regex . match ( c_name ) : #if class_name and class_name not in c_name: can_yield = False if can_yield and issubclass ( c , unittest . TestCase ) : if c is not unittest . TestCase : logger . debug ( 'class: {} matches {}' . format ( c_name , class_name ) ) yield c
def method_names ( self ) : for c in self . classes ( ) : #ms = inspect.getmembers(c, inspect.ismethod) ms = inspect . getmembers ( c , lambda f : inspect . ismethod ( f ) or inspect . isfunction ( f ) ) method_name = getattr ( self , 'method_name' , '' ) method_regex = '' if method_name : if method_name . startswith ( self . method_prefix ) : method_regex = re . compile ( r'^{}' . format ( method_name ) , flags = re . I ) else : if method_name . startswith ( "*" ) : method_name = method_name . strip ( "*" ) method_regex = re . compile ( r'^{}[_]{{0,1}}.*?{}' . format ( self . method_prefix , method_name ) , flags = re . I ) else : method_regex = re . compile ( r'^{}[_]{{0,1}}{}' . format ( self . method_prefix , method_name ) , flags = re . I ) for m_name , m in ms : if not m_name . startswith ( self . method_prefix ) : continue can_yield = True if method_regex and not method_regex . match ( m_name ) : can_yield = False if can_yield : logger . debug ( 'method: {} matches {}' . format ( m_name , method_name ) ) yield c , m_name
def _dump_arg_defaults ( kwargs ) : if current_app : kwargs . setdefault ( 'cls' , current_app . json_encoder ) if not current_app . config [ 'JSON_AS_ASCII' ] : kwargs . setdefault ( 'ensure_ascii' , False ) kwargs . setdefault ( 'sort_keys' , current_app . config [ 'JSON_SORT_KEYS' ] ) else : kwargs . setdefault ( 'sort_keys' , True ) kwargs . setdefault ( 'cls' , JSONEncoder )
def _load_arg_defaults ( kwargs ) : if current_app : kwargs . setdefault ( 'cls' , current_app . json_decoder ) else : kwargs . setdefault ( 'cls' , JSONDecoder )
def get_dist ( self ) : egg_info = self . egg_info_path ( '' ) . rstrip ( '/' ) base_dir = os . path . dirname ( egg_info ) metadata = pkg_resources . PathMetadata ( base_dir , egg_info ) dist_name = os . path . splitext ( os . path . basename ( egg_info ) ) [ 0 ] return pkg_resources . Distribution ( os . path . dirname ( egg_info ) , project_name = dist_name , metadata = metadata )
def to_text ( s , blank_if_none = True ) : if s is None : if blank_if_none : return "" else : return None elif isinstance ( s , text_type ) : return s else : return text_type ( s )
def find_ca_bundle ( ) : if os . name == 'nt' : return get_win_certfile ( ) else : for cert_path in cert_paths : if os . path . isfile ( cert_path ) : return cert_path try : return pkg_resources . resource_filename ( 'certifi' , 'cacert.pem' ) except ( ImportError , ResolutionError , ExtractionError ) : return None
def parse ( doc , treebuilder = "etree" , encoding = None , namespaceHTMLElements = True ) : tb = treebuilders . getTreeBuilder ( treebuilder ) p = HTMLParser ( tb , namespaceHTMLElements = namespaceHTMLElements ) return p . parse ( doc , encoding = encoding )
def bind ( self ) : HTTPServer . __init__ ( self , ( self . host , self . port ) , HTTPRequestHandler ) self . port = self . server_port
def report ( self ) : print ( self . report_message . format ( service = self . service , host = self . host , port = self . port , ) ) sys . stdout . flush ( )
def load_bytecode ( self , f ) : magic = f . read ( len ( bc_magic ) ) if magic != bc_magic : self . reset ( ) return checksum = pickle . load ( f ) if self . checksum != checksum : self . reset ( ) return self . code = marshal_load ( f )
def get_impl_ver ( ) : impl_ver = sysconfig . get_config_var ( "py_version_nodot" ) if not impl_ver : impl_ver = '' . join ( map ( str , sys . version_info [ : 2 ] ) ) return impl_ver
def distros_for_location ( location , basename , metadata = None ) : if basename . endswith ( '.egg.zip' ) : basename = basename [ : - 4 ] if basename . endswith ( '.egg' ) and '-' in basename : return [ Distribution . from_location ( location , basename , metadata ) ] if basename . endswith ( '.exe' ) : win_base , py_ver , platform = parse_bdist_wininst ( basename ) if win_base is not None : return interpret_distro_name ( location , win_base , metadata , py_ver , BINARY_DIST , platform ) # for ext in EXTENSIONS : if basename . endswith ( ext ) : basename = basename [ : - len ( ext ) ] return interpret_distro_name ( location , basename , metadata ) return [ ]
def find_external_links ( url , page ) : for match in REL . finditer ( page ) : tag , rel = match . groups ( ) rels = set ( map ( str . strip , rel . lower ( ) . split ( ',' ) ) ) if 'homepage' in rels or 'download' in rels : for match in HREF . finditer ( tag ) : yield urljoin ( url , htmldecode ( match . group ( 1 ) ) ) for tag in ( "<th>Home Page" , "<th>Download URL" ) : pos = page . find ( tag ) if pos != - 1 : match = HREF . search ( page , pos ) if match : yield urljoin ( url , htmldecode ( match . group ( 1 ) ) )
def local_open ( url ) : scheme , server , path , param , query , frag = urlparse ( url ) filename = url2pathname ( path ) if os . path . isfile ( filename ) : return urllib2 . urlopen ( url ) elif path . endswith ( '/' ) and os . path . isdir ( filename ) : files = [ ] for f in os . listdir ( filename ) : if f == 'index.html' : with open ( os . path . join ( filename , f ) , 'r' ) as fp : body = fp . read ( ) break elif os . path . isdir ( os . path . join ( filename , f ) ) : f += '/' files . append ( "<a href=%r>%s</a>" % ( f , f ) ) else : body = ( "<html><head><title>%s</title>" % url ) + "</head><body>%s</body></html>" % '\n' . join ( files ) status , message = 200 , "OK" else : status , message , body = 404 , "Path not found" , "Not found" headers = { 'content-type' : 'text/html' } return HTTPError ( url , status , message , headers , StringIO ( body ) )
def process_url ( self , url , retrieve = False ) : if url in self . scanned_urls and not retrieve : return self . scanned_urls [ url ] = True if not URL_SCHEME ( url ) : self . process_filename ( url ) return else : dists = list ( distros_for_url ( url ) ) if dists : if not self . url_ok ( url ) : return self . debug ( "Found link: %s" , url ) if dists or not retrieve or url in self . fetched_urls : list ( map ( self . add , dists ) ) return if not self . url_ok ( url ) : self . fetched_urls [ url ] = True return self . info ( "Reading %s" , url ) self . fetched_urls [ url ] = True f = self . open_url ( url , "Download error on %s: %%s -- Some packages may not be found!" % url ) if f is None : return self . fetched_urls [ f . url ] = True if 'html' not in f . headers . get ( 'content-type' , '' ) . lower ( ) : f . close ( ) return base = f . url page = f . read ( ) if not isinstance ( page , str ) : if isinstance ( f , HTTPError ) : charset = 'latin-1' else : charset = f . headers . get_param ( 'charset' ) or 'latin-1' page = page . decode ( charset , "ignore" ) f . close ( ) for match in HREF . finditer ( page ) : link = urljoin ( base , htmldecode ( match . group ( 1 ) ) ) self . process_url ( link ) if url . startswith ( self . index_url ) and getattr ( f , 'code' , None ) != 404 : page = self . process_index ( url , page )
def _init_pathinfo ( ) : d = set ( ) for dir in sys . path : try : if os . path . isdir ( dir ) : dir , dircase = makepath ( dir ) d . add ( dircase ) except TypeError : continue return d
def setcopyright ( ) : builtins . copyright = _Printer ( "copyright" , sys . copyright ) if _is_jython : builtins . credits = _Printer ( "credits" , "Jython is maintained by the Jython developers (www.jython.org)." ) elif _is_pypy : builtins . credits = _Printer ( "credits" , "PyPy is maintained by the PyPy developers: http://pypy.org/" ) else : builtins . credits = _Printer ( "credits" , ) here = os . path . dirname ( os . __file__ ) builtins . license = _Printer ( "license" , "See http://www.python.org/%.3s/license.html" % sys . version , [ "LICENSE.txt" , "LICENSE" ] , [ os . path . join ( here , os . pardir ) , here , os . curdir ] )
def have_pyrex ( ) : pyrex_impls = 'Cython.Distutils.build_ext' , 'Pyrex.Distutils.build_ext' for pyrex_impl in pyrex_impls : try : __import__ ( pyrex_impl , fromlist = [ 'build_ext' ] ) . build_ext return True except Exception : pass return False
def debug_application ( self , environ , start_response ) : app_iter = None try : app_iter = self . app ( environ , start_response ) for item in app_iter : yield item if hasattr ( app_iter , 'close' ) : app_iter . close ( ) except Exception : if hasattr ( app_iter , 'close' ) : app_iter . close ( ) traceback = get_current_traceback ( skip = 1 , show_hidden_frames = self . show_hidden_frames , ignore_system_exceptions = True ) for frame in traceback . frames : self . frames [ frame . id ] = frame self . tracebacks [ traceback . id ] = traceback try : start_response ( '500 INTERNAL SERVER ERROR' , [ ( 'Content-Type' , 'text/html; charset=utf-8' ) , ( 'X-XSS-Protection' , '0' ) , ] ) except Exception : environ [ 'wsgi.errors' ] . write ( 'Debugging middleware caught exception in streamed ' 'response at a point where response headers were already ' 'sent.\n' ) else : yield traceback . render_full ( evalex = self . evalex , secret = self . secret ) . encode ( 'utf-8' , 'replace' ) traceback . log ( environ [ 'wsgi.errors' ] )
def get_resource ( self , request , filename ) : filename = join ( dirname ( __file__ ) , 'shared' , basename ( filename ) ) if isfile ( filename ) : mimetype = mimetypes . guess_type ( filename ) [ 0 ] or 'application/octet-stream' f = open ( filename , 'rb' ) try : return Response ( f . read ( ) , mimetype = mimetype ) finally : f . close ( ) return Response ( 'Not Found' , status = 404 )
def user_agent ( ) : data = { "installer" : { "name" : "pip" , "version" : pip . __version__ } , "python" : platform . python_version ( ) , "implementation" : { "name" : platform . python_implementation ( ) , } , } if data [ "implementation" ] [ "name" ] == 'CPython' : data [ "implementation" ] [ "version" ] = platform . python_version ( ) elif data [ "implementation" ] [ "name" ] == 'PyPy' : if sys . pypy_version_info . releaselevel == 'final' : pypy_version_info = sys . pypy_version_info [ : 3 ] else : pypy_version_info = sys . pypy_version_info data [ "implementation" ] [ "version" ] = "." . join ( [ str ( x ) for x in pypy_version_info ] ) elif data [ "implementation" ] [ "name" ] == 'Jython' : data [ "implementation" ] [ "version" ] = platform . python_version ( ) elif data [ "implementation" ] [ "name" ] == 'IronPython' : data [ "implementation" ] [ "version" ] = platform . python_version ( ) if sys . platform . startswith ( "linux" ) : distro = dict ( filter ( lambda x : x [ 1 ] , zip ( [ "name" , "version" , "id" ] , platform . linux_distribution ( ) ) , ) ) libc = dict ( filter ( lambda x : x [ 1 ] , zip ( [ "lib" , "version" ] , platform . libc_ver ( ) ) , ) ) if libc : distro [ "libc" ] = libc if distro : data [ "distro" ] = distro if sys . platform . startswith ( "darwin" ) and platform . mac_ver ( ) [ 0 ] : data [ "distro" ] = { "name" : "OS X" , "version" : platform . mac_ver ( ) [ 0 ] } if platform . system ( ) : data . setdefault ( "system" , { } ) [ "name" ] = platform . system ( ) if platform . release ( ) : data . setdefault ( "system" , { } ) [ "release" ] = platform . release ( ) if platform . machine ( ) : data [ "cpu" ] = platform . machine ( ) return "{data[installer][name]}/{data[installer][version]} {json}" . format ( data = data , json = json . dumps ( data , separators = ( "," , ":" ) , sort_keys = True ) , )
def is_url ( name ) : if ':' not in name : return False scheme = name . split ( ':' , 1 ) [ 0 ] . lower ( ) return scheme in [ 'http' , 'https' , 'file' , 'ftp' ] + vcs . all_schemes
def _download_http_url ( link , session , temp_dir ) : target_url = link . url . split ( '#' , 1 ) [ 0 ] try : resp = session . get ( target_url , headers = { "Accept-Encoding" : "identity" } , stream = True , ) resp . raise_for_status ( ) except requests . HTTPError as exc : logger . critical ( "HTTP error %s while getting %s" , exc . response . status_code , link , ) raise content_type = resp . headers . get ( 'content-type' , '' ) filename = link . filename content_disposition = resp . headers . get ( 'content-disposition' ) if content_disposition : type , params = cgi . parse_header ( content_disposition ) filename = params . get ( 'filename' ) or filename ext = splitext ( filename ) [ 1 ] if not ext : ext = mimetypes . guess_extension ( content_type ) if ext : filename += ext if not ext and link . url != resp . url : ext = os . path . splitext ( resp . url ) [ 1 ] if ext : filename += ext file_path = os . path . join ( temp_dir , filename ) with open ( file_path , 'wb' ) as content_file : _download_url ( resp , link , content_file ) return file_path , content_type
def currencyFormat ( _context , code , symbol , format , currency_digits = True , decimal_quantization = True , name = '' ) : _context . action ( discriminator = ( 'currency' , name , code ) , callable = _register_currency , args = ( name , code , symbol , format , currency_digits , decimal_quantization ) )
def exchange ( _context , component , backend , base , name = '' ) : _context . action ( discriminator = ( 'currency' , 'exchange' , component ) , callable = _register_exchange , args = ( name , component , backend , base ) )
def print_results ( distributions , list_all_files ) : results_printed = False for dist in distributions : results_printed = True logger . info ( "---" ) logger . info ( "Metadata-Version: %s" % dist . get ( 'metadata-version' ) ) logger . info ( "Name: %s" % dist [ 'name' ] ) logger . info ( "Version: %s" % dist [ 'version' ] ) logger . info ( "Summary: %s" % dist . get ( 'summary' ) ) logger . info ( "Home-page: %s" % dist . get ( 'home-page' ) ) logger . info ( "Author: %s" % dist . get ( 'author' ) ) logger . info ( "Author-email: %s" % dist . get ( 'author-email' ) ) logger . info ( "License: %s" % dist . get ( 'license' ) ) logger . info ( "Location: %s" % dist [ 'location' ] ) logger . info ( "Requires: %s" % ', ' . join ( dist [ 'requires' ] ) ) if list_all_files : logger . info ( "Files:" ) if dist [ 'files' ] is not None : for line in dist [ 'files' ] : logger . info ( "  %s" % line . strip ( ) ) else : logger . info ( "Cannot locate installed-files.txt" ) if 'entry_points' in dist : logger . info ( "Entry-points:" ) for line in dist [ 'entry_points' ] : logger . info ( "  %s" % line . strip ( ) ) return results_printed
def _decode ( self , data , decode_content , flush_decoder ) : try : if decode_content and self . _decoder : data = self . _decoder . decompress ( data ) except ( IOError , zlib . error ) as e : content_encoding = self . headers . get ( 'content-encoding' , '' ) . lower ( ) raise DecodeError ( "Received response with content-encoding: %s, but " "failed to decode it." % content_encoding , e ) if flush_decoder and decode_content and self . _decoder : buf = self . _decoder . decompress ( binary_type ( ) ) data += buf + self . _decoder . flush ( ) return data
def _render ( template , context , app ) : rv = template . render ( context ) template_rendered . send ( app , template = template , context = context ) return rv
def parse_version ( version ) : global parse_version try : from pkg_resources import parse_version except ImportError : from distutils . version import LooseVersion as parse_version return parse_version ( version )
def is_declared ( self , name ) : if name in self . declared_locally or name in self . declared_parameter : return True return name in self . declared
def visit_Name ( self , node ) : if node . ctx == 'store' : self . identifiers . declared_locally . add ( node . name ) elif node . ctx == 'param' : self . identifiers . declared_parameter . add ( node . name ) elif node . ctx == 'load' and not self . identifiers . is_declared ( node . name ) : self . identifiers . undeclared . add ( node . name )
def visit_FromImport ( self , node , frame ) : self . newline ( node ) self . write ( 'included_template = environment.get_template(' ) self . visit ( node . template , frame ) self . write ( ', %r).' % self . name ) if node . with_context : self . write ( 'make_module(context.parent, True)' ) else : self . write ( 'module' ) var_names = [ ] discarded_names = [ ] for name in node . names : if isinstance ( name , tuple ) : name , alias = name else : alias = name self . writeline ( 'l_%s = getattr(included_template, ' '%r, missing)' % ( alias , name ) ) self . writeline ( 'if l_%s is missing:' % alias ) self . indent ( ) self . writeline ( 'l_%s = environment.undefined(%r %% ' 'included_template.__name__, ' 'name=%r)' % ( alias , 'the template %%r (imported on %s) does ' 'not export the requested name %s' % ( self . position ( node ) , repr ( name ) ) , name ) ) self . outdent ( ) if frame . toplevel : var_names . append ( alias ) if not alias . startswith ( '_' ) : discarded_names . append ( alias ) frame . assigned_names . add ( alias ) if var_names : if len ( var_names ) == 1 : name = var_names [ 0 ] self . writeline ( 'context.vars[%r] = l_%s' % ( name , name ) ) else : self . writeline ( 'context.vars.update({%s})' % ', ' . join ( '%r: l_%s' % ( name , name ) for name in var_names ) ) if discarded_names : if len ( discarded_names ) == 1 : self . writeline ( 'context.exported_vars.discard(%r)' % discarded_names [ 0 ] ) else : self . writeline ( 'context.exported_vars.difference_' 'update((%s))' % ', ' . join ( imap ( repr , discarded_names ) ) )
def populate_requirement_set ( requirement_set , args , options , finder , session , name , wheel_cache ) : for req in args : requirement_set . add_requirement ( InstallRequirement . from_line ( req , None , isolated = options . isolated_mode , wheel_cache = wheel_cache ) ) for req in options . editables : requirement_set . add_requirement ( InstallRequirement . from_editable ( req , default_vcs = options . default_vcs , isolated = options . isolated_mode , wheel_cache = wheel_cache ) ) found_req_in_file = False for filename in options . requirements : for req in parse_requirements ( filename , finder = finder , options = options , session = session , wheel_cache = wheel_cache ) : found_req_in_file = True requirement_set . add_requirement ( req ) if not ( args or options . editables or found_req_in_file ) : opts = { 'name' : name } if options . find_links : msg = ( 'You must give at least one requirement to ' '%(name)s (maybe you meant "pip %(name)s ' '%(links)s"?)' % dict ( opts , links = ' ' . join ( options . find_links ) ) ) else : msg = ( 'You must give at least one requirement ' 'to %(name)s (see "pip help %(name)s")' % opts ) logger . warning ( msg )
def export ( self , location ) : temp_dir = tempfile . mkdtemp ( '-export' , 'pip-' ) self . unpack ( temp_dir ) if os . path . exists ( location ) : rmtree ( location ) try : self . run_command ( [ 'export' , location ] , cwd = temp_dir , show_stdout = False ) finally : rmtree ( temp_dir )
def verify_signature ( self , key , value , sig ) : return constant_time_compare ( sig , self . get_signature ( key , value ) )
def get_signature ( self , value ) : value = want_bytes ( value ) key = self . derive_key ( ) sig = self . algorithm . get_signature ( key , value ) return base64_encode ( sig )
def sign ( self , value ) : return value + want_bytes ( self . sep ) + self . get_signature ( value )
def verify_signature ( self , value , sig ) : key = self . derive_key ( ) try : sig = base64_decode ( sig ) except Exception : return False return self . algorithm . verify_signature ( key , value , sig )
def unsign ( self , signed_value ) : signed_value = want_bytes ( signed_value ) sep = want_bytes ( self . sep ) if sep not in signed_value : raise BadSignature ( 'No %r found in value' % self . sep ) value , sig = signed_value . rsplit ( sep , 1 ) if self . verify_signature ( value , sig ) : return value raise BadSignature ( 'Signature %r does not match' % sig , payload = value )
def sign ( self , value ) : value = want_bytes ( value ) timestamp = base64_encode ( int_to_bytes ( self . get_timestamp ( ) ) ) sep = want_bytes ( self . sep ) value = value + sep + timestamp return value + sep + self . get_signature ( value )
def _all_dirs ( base_path ) : for root , dirs , files in os . walk ( base_path , followlinks = True ) : for dir in dirs : yield os . path . relpath ( os . path . join ( root , dir ) , base_path )
def install_scripts ( distributions ) : try : from setuptools . command import easy_install import pkg_resources except ImportError : raise RuntimeError ( "'wheel install_scripts' needs setuptools." ) for dist in distributions : pkg_resources_dist = pkg_resources . get_distribution ( dist ) install = wheel . paths . get_install_command ( dist ) command = easy_install . easy_install ( install . distribution ) command . args = [ 'wheel' ] command . finalize_options ( ) command . install_egg_scripts ( pkg_resources_dist )
def get_node ( self , ID ) : node = super ( Graph , self ) . get_node ( ID ) if node is not None : return node for graph in self . all_graphs : for each_node in graph . nodes : if each_node . ID == ID : return each_node else : return None
def _directed_changed ( self , new ) : if new : conn = "->" else : conn = "--" for edge in [ e for g in self . all_graphs for e in g . edges ] : edge . conn = conn
def _on_edges ( self , object , name , old , new ) : if name == "edges_items" : edges = new . added elif name == "edges" : edges = new else : edges = [ ] all_nodes = [ n for g in self . all_graphs for n in g . nodes ] for each_edge in edges : if each_edge . tail_node not in all_nodes : object . nodes . append ( each_edge . tail_node ) if each_edge . head_node not in all_nodes : object . nodes . append ( each_edge . head_node ) each_edge . _nodes = all_nodes
def _component_changed ( self , old , new ) : canvas = self . canvas if old is not None : canvas . remove ( old ) if new is not None : canvas . add ( new )
def _diagram_canvas_changed ( self , new ) : logger . debug ( "Diagram canvas changed!" ) canvas = self . diagram_canvas for tool in self . tools : if canvas is not None : print "Adding tool: %s" % tool canvas . tools . append ( tool ( canvas ) )
def clear_canvas ( self ) : logger . debug ( "Clearing the diagram canvas!" ) old_canvas = self . diagram_canvas new_canvas = Canvas ( ) new_canvas . copy_traits ( old_canvas , [ "bgcolor" , "draw_axes" ] ) self . diagram_canvas = new_canvas self . viewport . component = new_canvas self . viewport . request_redraw ( ) return
def _domain_model_changed_for_diagram ( self , obj , name , old , new ) : if old is not None : self . unmap_model ( old ) if new is not None : self . map_model ( new )
def map_model ( self , new ) : logger . debug ( "Mapping the domain model!" ) dot = Dot ( ) self . diagram . clear_canvas ( ) for node_mapping in self . nodes : ct = node_mapping . containment_trait logger . debug ( "Mapping elements contained by the '%s' trait" % ct ) if hasattr ( new , ct ) : elements = getattr ( new , ct ) logger . debug ( "%d element(s) found" % len ( elements ) ) for element in elements : pydot_node = Node ( str ( id ( element ) ) ) dot_attrs = node_mapping . dot_node if dot_attrs is not None : self . _style_node ( pydot_node , dot_attrs ) dot . add_node ( pydot_node ) new . on_trait_change ( self . map_element , ct + "_items" ) logger . debug ( "Retrieving xdot data and forming pydot graph!" ) xdot = graph_from_dot_data ( dot . create ( self . program , "xdot" ) ) parser = XDotParser ( ) for node in xdot . get_node_list ( ) : diagram_node = parser . parse_node ( node ) logger . debug ( "Parsed node [%s] and received diagram node [%s]" % ( node , diagram_node ) ) if diagram_node is not None : for node_mapping in self . nodes : ct = node_mapping . containment_trait for element in getattr ( new , ct ) : if str ( id ( element ) ) == diagram_node . dot_node . get_name ( ) : logger . debug ( "Referencing element [%s] from diagram node [%s]" % ( element , diagram_node ) ) diagram_node . element = element break if isinstance ( diagram_node . element , node_mapping . element ) : for tool in node_mapping . tools : logger . debug ( "Adding tool [%s] to diagram node [%s]" % ( tool , diagram_node ) ) diagram_node . tools . append ( tool ( diagram_node ) ) else : if diagram_node . element is None : logger . warning ( "Diagram node not referenced to element" ) self . diagram . diagram_canvas . add ( diagram_node ) del parser
def unmap_model ( self , old ) : for node_mapping in self . nodes : ct = node_mapping . containment_trait if hasattr ( old , ct ) : old_elements = getattr ( old , ct ) for old_element in old_elements : old . on_trait_change ( self . map_element , ct + "_items" , remove = True )
def map_element ( self , obj , name , event ) : canvas = self . diagram . diagram_canvas parser = XDotParser ( ) for element in event . added : logger . debug ( "Mapping new element [%s] to diagram node" % element ) for node_mapping in self . nodes : ct = name [ : - 6 ] #strip '_items' if node_mapping . containment_trait == ct : dot_attrs = node_mapping . dot_node dot = Dot ( ) graph_node = Node ( str ( id ( element ) ) ) self . _style_node ( graph_node , dot_attrs ) dot . add_node ( graph_node ) xdot = graph_from_dot_data ( dot . create ( self . program , "xdot" ) ) diagram_nodes = parser . parse_nodes ( xdot ) #.get_node_list()) for dn in diagram_nodes : if dn is not None : dn . element = element for tool in node_mapping . tools : dn . tools . append ( tool ( dn ) ) canvas . add ( dn ) canvas . request_redraw ( ) for element in event . removed : logger . debug ( "Unmapping element [%s] from diagram" % element ) for component in canvas . components : if element == component . element : canvas . remove ( component ) canvas . request_redraw ( ) break
def parse_xdot_data ( self , data ) : parser = self . parser if data : return parser . parseString ( data ) else : return [ ]
def proc_font ( self , tokens ) : size = int ( tokens [ "s" ] ) self . pen . font = "%s %d" % ( tokens [ "b" ] , size ) return [ ]
def _proc_ellipse ( self , tokens , filled ) : component = Ellipse ( pen = self . pen , x_origin = tokens [ "x0" ] , y_origin = tokens [ "y0" ] , e_width = tokens [ "w" ] , e_height = tokens [ "h" ] , filled = filled ) return component
def _proc_polygon ( self , tokens , filled ) : pts = [ ( p [ "x" ] , p [ "y" ] ) for p in tokens [ "points" ] ] component = Polygon ( pen = self . pen , points = pts , filled = filled ) return component
def proc_polyline ( self , tokens ) : pts = [ ( p [ "x" ] , p [ "y" ] ) for p in tokens [ "points" ] ] component = Polyline ( pen = self . pen , points = pts ) return component
def proc_text ( self , tokens ) : component = Text ( pen = self . pen , text_x = tokens [ "x" ] , text_y = tokens [ "y" ] , justify = tokens [ "j" ] , text_w = tokens [ "w" ] , text = tokens [ "b" ] ) return component
def proc_image ( self , tokens ) : print "IMAGE:" , tokens , tokens . asList ( ) , tokens . keys ( ) raise NotImplementedError
def render_grid_file ( context , f ) : f . seek ( 0 ) response = context . response if __debug__ : response . headers [ 'Grid-ID' ] = str ( f . _id ) log . debug ( "Serving GridFS file." , extra = dict ( identifier = str ( f . _id ) , filename = f . filename , length = f . length , mimetype = f . content_type ) ) response . conditional_response = True response . accept_ranges = 'bytes' response . content_type = f . content_type response . content_length = f . length response . content_md5 = response . etag = f . md5 response . last_modified = f . metadata . get ( 'modified' , None ) response . content_disposition = 'attachment; filename=' + f . name if context . request . if_range . match_response ( response ) : response . body_file = f else : response . app_iter = iter ( f ) return True
def save ( self , obj ) : fd = None try : fd = open ( self . dot_file . absolute_path , "wb" ) obj . save_dot ( fd ) finally : if fd is not None : fd . close ( ) return
def load ( self ) : fd = None try : obj = parse_dot_file ( self . dot_file . absolute_path ) finally : if fd is not None : fd . close ( ) return obj
def is_in ( self , point_x , point_y ) : x = self . x_origin y = self . y_origin a = self . e_width b = self . e_height #/2 return ( ( point_x - x ) ** 2 / ( a ** 2 ) ) + ( ( point_y - y ) ** 2 / ( b ** 2 ) ) < 1.0
def _draw_bounds ( self , gc ) : dx , dy = self . bounds x , y = self . position gc . rect ( x , y , dx , dy ) gc . stroke_path ( )
def perform ( self , event ) : wizard = NewDotGraphWizard ( parent = self . window . control , window = self . window , title = "New Graph" ) if wizard . open ( ) == OK : wizard . finished = True
def start ( self , context ) : if __debug__ : log . info ( "Connecting SQLAlchemy database layer." , extra = dict ( uri = redact_uri ( self . uri ) , config = self . config , alias = self . alias , ) ) engine = self . engine = create_engine ( self . uri , * * self . config ) self . Session = scoped_session ( sessionmaker ( bind = engine ) ) engine . connect ( ) . close ( ) context . db [ self . alias ] = engine
def _parse_dot_code_fired ( self ) : parser = GodotDataParser ( ) graph = parser . parse_dot_data ( self . dot_code ) if graph is not None : self . model = graph
def new_model ( self , info ) : if info . initialized : retval = confirm ( parent = info . ui . control , message = "Replace existing graph?" , title = "New Graph" , default = YES ) if retval == YES : self . model = Graph ( )
def open_file ( self , info ) : if not info . initialized : return dlg = FileDialog ( action = "open" , wildcard = "Graphviz Files (*.dot, *.xdot, *.txt)|" "*.dot;*.xdot;*.txt|Dot Files (*.dot)|*.dot|" "All Files (*.*)|*.*|" ) if dlg . open ( ) == OK : parser = GodotDataParser ( ) model = parser . parse_dot_file ( dlg . path ) if model is not None : self . model = model else : print "error parsing: %s" % dlg . path self . save_file = dlg . path del dlg
def save ( self , info ) : save_file = self . save_file if not isfile ( save_file ) : self . save_as ( info ) else : fd = None try : fd = open ( save_file , "wb" ) dot_code = str ( self . model ) fd . write ( dot_code ) finally : if fd is not None : fd . close ( )
def save_as ( self , info ) : if not info . initialized : return dlg = FileDialog ( action = "save as" , wildcard = "Graphviz Files (*.dot, *.xdot, *.txt)|" "*.dot;*.xdot;*.txt|Dot Files (*.dot)|*.dot|" "All Files (*.*)|*.*|" ) if dlg . open ( ) == OK : fd = None try : fd = open ( dlg . path , "wb" ) dot_code = str ( self . model ) fd . write ( dot_code ) self . save_file = dlg . path except : error ( parent = info . ui . control , title = "Save Error" , message = "An error was encountered when saving\nto %s" % self . file ) finally : if fd is not None : fd . close ( ) del dlg
def configure_graph ( self , info ) : if info . initialized : self . model . edit_traits ( parent = info . ui . control , kind = "live" , view = attr_view )
def configure_nodes ( self , info ) : if info . initialized : self . model . edit_traits ( parent = info . ui . control , kind = "live" , view = nodes_view )
def configure_edges ( self , info ) : if info . initialized : self . model . edit_traits ( parent = info . ui . control , kind = "live" , view = edges_view )
def about_godot ( self , info ) : if info . initialized : self . edit_traits ( parent = info . ui . control , kind = "livemodal" , view = about_view )
def add_node ( self , info ) : if not info . initialized : return graph = self . _request_graph ( info . ui . control ) if graph is None : return IDs = [ v . ID for v in graph . nodes ] node = Node ( ID = make_unique_name ( "node" , IDs ) ) graph . nodes . append ( node ) retval = node . edit_traits ( parent = info . ui . control , kind = "livemodal" ) if not retval . result : graph . nodes . remove ( node )
def add_edge ( self , info ) : if not info . initialized : return graph = self . _request_graph ( info . ui . control ) if graph is None : return n_nodes = len ( graph . nodes ) IDs = [ v . ID for v in graph . nodes ] if n_nodes == 0 : tail_node = Node ( ID = make_unique_name ( "node" , IDs ) ) head_name = make_unique_name ( "node" , IDs + [ tail_node . ID ] ) head_node = Node ( ID = head_name ) elif n_nodes == 1 : tail_node = graph . nodes [ 0 ] head_node = Node ( ID = make_unique_name ( "node" , IDs ) ) else : tail_node = graph . nodes [ 0 ] head_node = graph . nodes [ 1 ] edge = Edge ( tail_node , head_node , _nodes = graph . nodes ) retval = edge . edit_traits ( parent = info . ui . control , kind = "livemodal" ) if retval . result : graph . edges . append ( edge )
def add_subgraph ( self , info ) : if not info . initialized : return graph = self . _request_graph ( info . ui . control ) if graph is not None : subgraph = Subgraph ( ) #root=graph, parent=graph) retval = subgraph . edit_traits ( parent = info . ui . control , kind = "livemodal" ) if retval . result : graph . subgraphs . append ( subgraph )
def add_cluster ( self , info ) : if not info . initialized : return graph = self . _request_graph ( info . ui . control ) if graph is not None : cluster = Cluster ( ) #root=graph, parent=graph) retval = cluster . edit_traits ( parent = info . ui . control , kind = "livemodal" ) if retval . result : graph . clusters . append ( cluster )
def godot_options ( self , info ) : if info . initialized : self . edit_traits ( parent = info . ui . control , kind = "livemodal" , view = "options_view" )
def configure_dot_code ( self , info ) : if not info . initialized : return self . dot_code = str ( self . model ) retval = self . edit_traits ( parent = info . ui . control , kind = "livemodal" , view = "dot_code_view" )
def on_exit ( self , info ) : if self . prompt_on_exit : retval = confirm ( parent = info . ui . control , message = "Exit Godot?" , title = "Confirm exit" , default = YES ) if retval == YES : self . _on_close ( info ) else : self . _on_close ( info )
def save_to_file_like ( self , flo , format = None , * * kwargs ) : format = self . format if format is None else format save = getattr ( self , "save_%s" % format , None ) if save is None : raise ValueError ( "Unknown format '%s'." % format ) save ( flo , * * kwargs )
def save_to_file ( self , filename , format = None , * * kwargs ) : if format is None : format = format_from_extension ( filename ) with file ( filename , 'wb' ) as fp : self . save_to_file_like ( fp , format , * * kwargs )
def add_node ( self , node_or_ID , * * kwds ) : if not isinstance ( node_or_ID , Node ) : nodeID = str ( node_or_ID ) if nodeID in self . nodes : node = self . nodes [ self . nodes . index ( nodeID ) ] else : if self . default_node is not None : node = self . default_node . clone_traits ( copy = "deep" ) node . ID = nodeID else : node = Node ( nodeID ) self . nodes . append ( node ) else : node = node_or_ID if node in self . nodes : node = self . nodes [ self . nodes . index ( node_or_ID ) ] else : self . nodes . append ( node ) node . set ( * * kwds ) return node
def delete_node ( self , node_or_ID ) : if isinstance ( node_or_ID , Node ) : node = node_or_ID else : node = self . get_node ( node_or_ID ) if node is None : raise ValueError ( "Node %s does not exists" % node_or_ID ) self . nodes . remove ( node )
def get_node ( self , ID ) : for node in self . nodes : if node . ID == str ( ID ) : return node return None
def delete_edge ( self , tail_node_or_ID , head_node_or_ID ) : if isinstance ( tail_node_or_ID , Node ) : tail_node = tail_node_or_ID else : tail_node = self . get_node ( tail_node_or_ID ) if isinstance ( head_node_or_ID , Node ) : head_node = head_node_or_ID else : head_node = self . get_node ( head_node_or_ID ) if ( tail_node is None ) or ( head_node is None ) : return None for i , edge in enumerate ( self . edges ) : if ( edge . tail_node == tail_node ) and ( edge . head_node == head_node ) : edge = self . edges . pop ( i ) return edge return None
def add_edge ( self , tail_node_or_ID , head_node_or_ID , * * kwds ) : tail_node = self . add_node ( tail_node_or_ID ) head_node = self . add_node ( head_node_or_ID ) if "directed" in self . trait_names ( ) : directed = self . directed else : directed = False if self . default_edge is not None : edge = self . default_edge . clone_traits ( copy = "deep" ) edge . tail_node = tail_node edge . head_node = head_node edge . conn = "->" if directed else "--" edge . set ( * * kwds ) else : edge = Edge ( tail_node , head_node , directed , * * kwds ) if "strict" in self . trait_names ( ) : if not self . strict : self . edges . append ( edge ) else : self . edges . append ( edge ) else : self . edges . append ( edge )
def add_subgraph ( self , subgraph_or_ID ) : if not isinstance ( subgraph_or_ID , ( godot . subgraph . Subgraph , godot . cluster . Cluster ) ) : subgraphID = str ( subgraph_or_ID ) if subgraph_or_ID . startswith ( "cluster" ) : subgraph = godot . cluster . Cluster ( ID = subgraphID ) else : subgraph = godot . subgraph . Subgraph ( ID = subgraphID ) else : subgraph = subgraph_or_ID subgraph . default_node = self . default_node subgraph . default_edge = self . default_edge if isinstance ( subgraph , godot . subgraph . Subgraph ) : self . subgraphs . append ( subgraph ) elif isinstance ( subgraph , godot . cluster . Cluster ) : self . clusters . append ( subgraph ) else : raise return subgraph
def _program_changed ( self , new ) : progs = self . progs if not progs . has_key ( prog ) : logger . warning ( 'GraphViz\'s executable "%s" not found' % prog ) if not os . path . exists ( progs [ prog ] ) or not os . path . isfile ( progs [ prog ] ) : logger . warning ( "GraphViz's executable '%s' is not a " "file or doesn't exist" % progs [ prog ] )
def _set_node_lists ( self , new ) : for edge in self . edges : edge . _nodes = self . nodes
def parse_dot_file ( filename ) : parser = GodotDataParser ( ) graph = parser . parse_dot_file ( filename ) del parser return graph
def parse_dot_file ( self , file_or_filename ) : if isinstance ( file_or_filename , basestring ) : file = None try : file = open ( file_or_filename , "rb" ) data = file . read ( ) except : print "Could not open %s." % file_or_filename return None finally : if file is not None : file . close ( ) else : file = file_or_filename data = file . read ( ) return self . parse_dot_data ( data )
def build_top_graph ( self , tokens ) : strict = tokens [ 0 ] == 'strict' graphtype = tokens [ 1 ] directed = graphtype == 'digraph' graphname = tokens [ 2 ] graph = Graph ( ID = graphname , strict = strict , directed = directed ) self . graph = self . build_graph ( graph , tokens [ 3 ] )
def build_graph ( self , graph , tokens ) : subgraph = None for element in tokens : cmd = element [ 0 ] if cmd == ADD_NODE : cmd , nodename , opts = element graph . add_node ( nodename , * * opts ) elif cmd == ADD_EDGE : cmd , src , dest , opts = element srcport = destport = "" if isinstance ( src , tuple ) : srcport = src [ 1 ] src = src [ 0 ] if isinstance ( dest , tuple ) : destport = dest [ 1 ] dest = dest [ 0 ] graph . add_edge ( src , dest , tailport = srcport , headport = destport , * * opts ) elif cmd in [ ADD_GRAPH_TO_NODE_EDGE , ADD_GRAPH_TO_GRAPH_EDGE , ADD_NODE_TO_GRAPH_EDGE ] : cmd , src , dest , opts = element srcport = destport = "" if isinstance ( src , tuple ) : srcport = src [ 1 ] if isinstance ( dest , tuple ) : destport = dest [ 1 ] if not ( cmd == ADD_NODE_TO_GRAPH_EDGE ) : if cmd == ADD_GRAPH_TO_NODE_EDGE : src = subgraph else : src = prev_subgraph dest = subgraph else : dest = subgraph src_is_graph = isinstance ( src , ( Subgraph , Cluster ) ) dst_is_graph = isinstance ( dst , ( Subgraph , Cluster ) ) if src_is_graph : src_nodes = src . nodes else : src_nodes = [ src ] if dst_is_graph : dst_nodes = dst . nodes else : dst_nodes = [ dst ] for src_node in src_nodes : for dst_node in dst_nodes : graph . add_edge ( from_node = src_node , to_node = dst_node , tailport = srcport , headport = destport , * * kwds ) elif cmd == SET_GRAPH_ATTR : graph . set ( * * element [ 1 ] ) elif cmd == SET_DEF_NODE_ATTR : graph . default_node . set ( * * element [ 1 ] ) elif cmd == SET_DEF_EDGE_ATTR : graph . default_edge . set ( * * element [ 1 ] ) elif cmd == SET_DEF_GRAPH_ATTR : graph . default_graph . set ( * * element [ 1 ] ) elif cmd == ADD_SUBGRAPH : cmd , name , elements = element if subgraph : prev_subgraph = subgraph if name . startswith ( "cluster" ) : cluster = Cluster ( ID = name ) cluster = self . build_graph ( cluster , elements ) graph . add_cluster ( cluster ) else : subgraph = Subgraph ( ID = name ) subgraph = self . build_graph ( subgraph , elements ) graph . add_subgraph ( subgraph ) return graph
def format_duration ( seconds ) : units , divider = get_time_units_and_multiplier ( seconds ) seconds *= divider return "%.3f %s" % ( seconds , units )
def on_path ( self , new ) : self . name = basename ( new ) self . graph = self . editor_input . load ( )
def get_children ( self , object ) : children = [ ] children . extend ( object . subgraphs ) children . extend ( object . clusters ) children . extend ( object . nodes ) children . extend ( object . edges ) return children
def append_child ( self , object , child ) : if isinstance ( child , Subgraph ) : object . subgraphs . append ( child ) elif isinstance ( child , Cluster ) : object . clusters . append ( child ) elif isinstance ( child , Node ) : object . nodes . append ( child ) elif isinstance ( child , Edge ) : object . edges . append ( child ) else : pass
def insert_child ( self , object , index , child ) : if isinstance ( child , Subgraph ) : object . subgraphs . insert ( index , child ) elif isinstance ( child , Cluster ) : object . clusters . insert ( index , child ) elif isinstance ( child , Node ) : object . nodes . insert ( index , child ) elif isinstance ( child , Edge ) : object . edges . insert ( index , child ) else : pass
def delete_child ( self , object , index ) : if isinstance ( child , Subgraph ) : object . subgraphs . pop ( index ) elif isinstance ( child , Cluster ) : object . clusters . pop ( index ) elif isinstance ( child , Node ) : object . nodes . pop ( index ) elif isinstance ( child , Edge ) : object . edges . pop ( index ) else : pass
def get_label ( self , object ) : label = self . label if label [ : 1 ] == '=' : return label [ 1 : ] label = xgetattr ( object , label , '' ) if self . formatter is None : return label return self . formatter ( object , label )
def set_label ( self , object , label ) : label_name = self . label if label_name [ : 1 ] != '=' : xsetattr ( object , label_name , label )
def _add_listeners ( self ) : object = self . value canvas = self . factory . canvas if canvas is not None : for name in canvas . node_children : object . on_trait_change ( self . _nodes_replaced , name ) object . on_trait_change ( self . _nodes_changed , name + "_items" ) for name in canvas . edge_children : object . on_trait_change ( self . _edges_replaced , name ) object . on_trait_change ( self . _edges_changed , name + "_items" ) else : raise ValueError ( "Graph canvas not set for graph editor." )
def _nodes_replaced ( self , object , name , old , new ) : self . _delete_nodes ( old ) self . _add_nodes ( new )
def _nodes_changed ( self , object , name , undefined , event ) : self . _delete_nodes ( event . removed ) self . _add_nodes ( event . added )
def _delete_nodes ( self , features ) : graph = self . _graph if graph is not None : for feature in features : graph . delete_node ( id ( feature ) ) graph . arrange_all ( )
def _edges_replaced ( self , object , name , old , new ) : self . _delete_edges ( old ) self . _add_edges ( new )
def _edges_changed ( self , object , name , undefined , event ) : self . _delete_edges ( event . removed ) self . _add_edges ( event . added )
def _delete_edges ( self , features ) : graph = self . _graph if graph is not None : for feature in features : for graph_edge in self . factory . edges : if feature . __class__ in graph_edge . edge_for : tail_feature = getattr ( feature , graph_edge . tail_name ) head_feature = getattr ( feature , graph_edge . head_name ) graph . delete_edge ( id ( tail_feature ) , id ( head_feature ) ) graph . arrange_all ( )
def arrange_all ( self ) : import godot . dot_data_parser import godot . graph graph = godot . graph . Graph ( ID = "g" , directed = True ) self . conn = "->" graph . edges . append ( self ) xdot_data = graph . create ( format = "xdot" ) parser = godot . dot_data_parser . GodotDataParser ( ) ndata = xdot_data . replace ( '\\\n' , '' ) tokens = parser . dotparser . parseString ( ndata ) [ 0 ] for element in tokens [ 3 ] : cmd = element [ 0 ] if cmd == "add_edge" : cmd , src , dest , opts = element self . set ( * * opts )
def _parse_xdot_directive ( self , name , new ) : parser = XdotAttrParser ( ) components = parser . parse_xdot_data ( new ) x1 = min ( [ c . x for c in components ] ) y1 = min ( [ c . y for c in components ] ) print "X1/Y1:" , name , x1 , y1 for c in components : if isinstance ( c , Ellipse ) : component . x_origin -= x1 component . y_origin -= y1 elif isinstance ( c , ( Polygon , BSpline ) ) : print "Points:" , c . points c . points = [ ( t [ 0 ] - x1 , t [ 1 ] - y1 ) for t in c . points ] print "Points:" , c . points elif isinstance ( c , Text ) : c . text_x , c . text_y = c . x - x1 , c . y - y1 container = Container ( auto_size = True , position = [ x1 , y1 ] , bgcolor = "yellow" ) container . add ( * components ) if name == "_draw_" : self . drawing = container elif name == "_hdraw_" : self . arrowhead_drawing = container else : raise
def _on_drawing ( self , object , name , old , new ) : attrs = [ "drawing" , "arrowhead_drawing" ] others = [ getattr ( self , a ) for a in attrs if ( a != name ) and ( getattr ( self , a ) is not None ) ] x , y = self . component . position print "POS:" , x , y , self . component . position abs_x = [ d . x + x for d in others ] abs_y = [ d . y + y for d in others ] print "ABS:" , abs_x , abs_y x1 = min ( abs_x + [ new . x ] ) y1 = min ( abs_y + [ new . y ] ) print "DRAW:" , new . position new . position = [ new . x - x1 , new . y - y1 ] print "DRAW:" , new . position if old is not None : self . component . remove ( old ) if new is not None : self . component . add ( new ) print "POS NEW:" , self . component . position self . component . position = [ x1 , y1 ] print "POS NEW:" , self . component . position self . component . request_redraw ( ) print "POS NEW:" , self . component . position
def node_factory ( * * row_factory_kw ) : if "__table_editor__" in row_factory_kw : graph = row_factory_kw [ "__table_editor__" ] . object ID = make_unique_name ( "n" , [ node . ID for node in graph . nodes ] ) del row_factory_kw [ "__table_editor__" ] return godot . node . Node ( ID ) else : return godot . node . Node ( uuid . uuid4 ( ) . hex [ : 6 ] )
def edge_factory ( * * row_factory_kw ) : if "__table_editor__" in row_factory_kw : table_editor = row_factory_kw [ "__table_editor__" ] graph = table_editor . object ID = make_unique_name ( "node" , [ node . ID for node in graph . nodes ] ) n_nodes = len ( graph . nodes ) IDs = [ v . ID for v in graph . nodes ] if n_nodes == 0 : tail_node = godot . Node ( ID = make_unique_name ( "n" , IDs ) ) head_node = godot . Node ( ID = make_unique_name ( "n" , IDs ) ) elif n_nodes == 1 : tail_node = graph . nodes [ 0 ] head_node = godot . Node ( ID = make_unique_name ( "n" , IDs ) ) else : tail_node = graph . nodes [ 0 ] head_node = graph . nodes [ 1 ] return godot . edge . Edge ( tail_node , head_node , _nodes = graph . nodes ) else : return None
def start ( self , context ) : self . config [ 'alias' ] = self . alias safe_config = dict ( self . config ) del safe_config [ 'host' ] log . info ( "Connecting MongoEngine database layer." , extra = dict ( uri = redact_uri ( self . config [ 'host' ] ) , config = self . config , ) ) self . connection = connect ( * * self . config )
def prepare ( self , context ) : context . db [ self . alias ] = MongoEngineProxy ( self . connection )
def arrange_all ( self ) : import godot . dot_data_parser import godot . graph graph = godot . graph . Graph ( ID = "g" ) graph . add_node ( self ) print "GRAPH DOT:\n" , str ( graph ) xdot_data = graph . create ( format = "xdot" ) print "XDOT DATA:\n" , xdot_data parser = godot . dot_data_parser . GodotDataParser ( ) flat_data = xdot_data . replace ( '\\\n' , '' ) tokens = parser . dotparser . parseString ( flat_data ) [ 0 ] for element in tokens [ 3 ] : print "TOK:" , element cmd = element [ 0 ] if cmd == 'add_node' : cmd , nodename , opts = element assert nodename == self . ID print "OPTIONS:" , opts self . set ( * * opts )
def parse_xdot_drawing_directive ( self , new ) : components = XdotAttrParser ( ) . parse_xdot_data ( new ) max_x = max ( [ c . bounds [ 0 ] for c in components ] + [ 1 ] ) max_y = max ( [ c . bounds [ 1 ] for c in components ] + [ 1 ] ) pos_x = min ( [ c . x for c in components ] ) pos_y = min ( [ c . y for c in components ] ) move_to_origin ( components ) container = Container ( auto_size = True , position = [ pos_x - self . pos [ 0 ] , pos_y - self . pos [ 1 ] ] , bgcolor = "blue" ) container . add ( * components ) self . drawing = container
def _drawing_changed ( self , old , new ) : if old is not None : self . component . remove ( old ) if new is not None : self . component . add ( new ) w , h = self . component . bounds self . component . position = [ self . pos [ 0 ] - ( w / 2 ) , self . pos [ 1 ] - ( h / 2 ) ] self . component . request_redraw ( )
def _on_position_change ( self , new ) : w , h = self . component . bounds self . pos = tuple ( [ new [ 0 ] + ( w / 2 ) , new [ 1 ] + ( h / 2 ) ] )
def _pos_changed ( self , new ) : w , h = self . component . bounds self . component . position = [ new [ 0 ] - ( w / 2 ) , new [ 1 ] - ( h / 2 ) ] self . component . request_redraw ( )
def highlight_info ( ctx , style ) : click . secho ( "The following styles are available to choose from:" , fg = "green" ) click . echo ( list ( pygments . styles . get_all_styles ( ) ) ) click . echo ( ) click . secho ( f'The following CSS for the "{style}" style can be customized:' , fg = "green" ) click . echo ( pygments . formatters . HtmlFormatter ( style = style ) . get_style_defs ( ) )
def _draw_mainlayer ( self , gc , view_bounds = None , mode = "default" ) : gc . save_state ( ) try : if len ( self . points ) >= 2 : gc . set_fill_color ( self . pen . fill_color_ ) gc . set_stroke_color ( self . pen . color_ ) gc . set_line_width ( self . pen . line_width ) gc . begin_path ( ) gc . lines ( self . points ) gc . close_path ( ) if self . filled : gc . draw_path ( self . inside_rule_ ) else : gc . stroke_path ( ) finally : gc . restore_state ( )
def is_in ( self , point_x , point_y ) : point_array = array ( ( ( point_x , point_y ) , ) ) vertices = array ( self . points ) winding = self . inside_rule == "winding" result = points_in_polygon ( point_array , vertices , winding ) return result [ 0 ]
def _draw_mainlayer ( self , gc , view_bounds = None , mode = "default" ) : if not self . points : return gc . save_state ( ) try : gc . set_fill_color ( self . pen . fill_color_ ) gc . set_line_width ( self . pen . line_width ) gc . set_stroke_color ( self . pen . color_ ) gc . begin_path ( ) start_x , start_y = self . points [ 0 ] gc . move_to ( start_x , start_y ) for triple in nsplit ( self . points [ 1 : ] , 3 ) : x1 , y1 = triple [ 0 ] x2 , y2 = triple [ 1 ] end_x , end_y = triple [ 2 ] gc . curve_to ( x1 , y1 , x2 , y2 , end_x , end_y ) gc . move_to ( end_x , end_y ) gc . stroke_path ( ) finally : gc . restore_state ( )
def _connect ( self , context ) : if __debug__ : log . info ( "Connecting " + self . engine . partition ( ':' ) [ 0 ] + " database layer." , extra = dict ( uri = redact_uri ( self . uri , self . protect ) , config = self . config , alias = self . alias , ) ) self . connection = context . db [ self . alias ] = self . _connector ( self . uri , * * self . config )
def _handle_event ( self , event , * args , * * kw ) : for engine in self . engines . values ( ) : if hasattr ( engine , event ) : getattr ( engine , event ) ( * args , * * kw )
def get_full_page_url ( self , page_number , scheme = None ) : args = dict ( request . view_args , _external = True , ) if scheme is not None : args [ '_scheme' ] = scheme if page_number != 1 : args [ 'page' ] = page_number return url_for ( request . endpoint , * * args )
def render_prev_next_links ( self , scheme = None ) : output = '' if self . has_prev : output += '<link rel="prev" href="{}" />\n' . format ( self . get_full_page_url ( self . prev , scheme = scheme ) ) if self . has_next : output += '<link rel="next" href="{}" />\n' . format ( self . get_full_page_url ( self . next , scheme = scheme ) ) return Markup ( output )
def render_seo_links ( self , scheme = None ) : out = self . render_prev_next_links ( scheme = scheme ) if self . total_pages == 1 : out += self . render_canonical_link ( scheme = scheme ) return out
def _content_type_matches ( candidate , pattern ) : def _wildcard_compare ( type_spec , type_pattern ) : return type_pattern == '*' or type_spec == type_pattern return ( _wildcard_compare ( candidate . content_type , pattern . content_type ) and _wildcard_compare ( candidate . content_subtype , pattern . content_subtype ) )
def ensure_dir ( path ) : try : log . info ( 'Ensuring directory exists: %s' % path ) os . makedirs ( path ) except OSError : if not os . path . isdir ( path ) : raise
def list_dataset_uris ( cls , base_uri , config_path ) : storage_account_name = generous_parse_uri ( base_uri ) . netloc blobservice = get_blob_service ( storage_account_name , config_path ) containers = blobservice . list_containers ( include_metadata = True ) uri_list = [ ] for c in containers : admin_metadata = c . metadata uri = cls . generate_uri ( admin_metadata [ 'name' ] , admin_metadata [ 'uuid' ] , base_uri ) uri_list . append ( uri ) return uri_list
def list_overlay_names ( self ) : overlay_names = [ ] for blob in self . _blobservice . list_blobs ( self . uuid , prefix = self . overlays_key_prefix ) : overlay_file = blob . name . rsplit ( '/' , 1 ) [ - 1 ] overlay_name , ext = overlay_file . split ( '.' ) overlay_names . append ( overlay_name ) return overlay_names
def iter_item_handles ( self ) : blob_generator = self . _blobservice . list_blobs ( self . uuid , include = 'metadata' ) for blob in blob_generator : if 'type' in blob . metadata : if blob . metadata [ 'type' ] == 'item' : handle = blob . metadata [ 'relpath' ] yield handle
def luhn_check ( card_number ) : sum = 0 num_digits = len ( card_number ) oddeven = num_digits & 1 for count in range ( 0 , num_digits ) : digit = int ( card_number [ count ] ) if not ( ( count & 1 ) ^ oddeven ) : digit *= 2 if digit > 9 : digit -= 9 sum += digit return ( sum % 10 ) == 0
def remove_namespaces ( root ) : for elem in root . getiterator ( ) : if not hasattr ( elem . tag , 'find' ) : continue i = elem . tag . find ( '}' ) if i >= 0 : elem . tag = elem . tag [ i + 1 : ] objectify . deannotate ( root , cleanup_namespaces = True )
def merge ( self , new_dict ) : actions = new_dict . pop ( "actions" ) for action in actions : self . add_action ( action ) self . __dict__ . update ( new_dict )
def execute_actions ( self , cwd ) : self . _execute_globals ( cwd ) for action in self . actions : logger . info ( "executing {}" . format ( action ) ) p = subprocess . Popen ( action , shell = True , cwd = cwd ) p . wait ( )
def add_details ( self , message ) : msg = message try : from flask import request url = request . url method = request . method endpoint = request . endpoint form_dict = dict ( request . form ) for key in form_dict : if key . lower ( ) in _error_reporting_obscured_fields : form_dict [ key ] = '******' elif len ( form_dict [ key ] ) == 1 : form_dict [ key ] = form_dict [ key ] [ 0 ] form = pprint . pformat ( form_dict ) . replace ( '\n' , '\n          ' ) msg = '%s\nRequest:\n\nurl:      %s\nmethod:   %s\nendpoint: %s\nform:     %s\n' % ( msg , url , method , endpoint , form ) except Exception : traceback . print_exc ( ) try : from flask import session from flask . json import JSONEncoder session_str = json . dumps ( dict ( * * session ) , indent = 2 , cls = JSONEncoder ) msg = '%s\nSession:\n\n%s\n' % ( msg , session_str ) except Exception : traceback . print_exc ( ) return msg
def get_context ( self , value ) : context = super ( RenditionAwareStructBlock , self ) . get_context ( value ) context [ 'image_rendition' ] = self . rendition . image_rendition or 'original' return context
def set ( self , k , v ) : k = k . lstrip ( '/' ) url = '{}/{}' . format ( self . endpoint , k ) r = requests . put ( url , data = str ( v ) ) if r . status_code != 200 or r . json ( ) is not True : raise KVStoreError ( 'PUT returned {}' . format ( r . status_code ) )
def get ( self , k , wait = False , wait_index = False , timeout = '5m' ) : k = k . lstrip ( '/' ) url = '{}/{}' . format ( self . endpoint , k ) params = { } if wait : params [ 'index' ] = wait_index params [ 'wait' ] = timeout r = requests . get ( url , params = params ) if r . status_code == 404 : raise KeyDoesNotExist ( "Key " + k + " does not exist" ) if r . status_code != 200 : raise KVStoreError ( 'GET returned {}' . format ( r . status_code ) ) try : return base64 . b64decode ( r . json ( ) [ 0 ] [ 'Value' ] ) except TypeError as e : return ""
def recurse ( self , k , wait = False , wait_index = None , timeout = '5m' ) : k = k . lstrip ( '/' ) url = '{}/{}' . format ( self . endpoint , k ) params = { } params [ 'recurse' ] = 'true' if wait : params [ 'wait' ] = timeout if not wait_index : params [ 'index' ] = self . index ( k , recursive = True ) else : params [ 'index' ] = wait_index r = requests . get ( url , params = params ) if r . status_code == 404 : raise KeyDoesNotExist ( "Key " + k + " does not exist" ) if r . status_code != 200 : raise KVStoreError ( 'GET returned {}' . format ( r . status_code ) ) entries = { } for e in r . json ( ) : if e [ 'Value' ] : entries [ e [ 'Key' ] ] = base64 . b64decode ( e [ 'Value' ] ) else : entries [ e [ 'Key' ] ] = '' return entries
def delete ( self , k , recursive = False ) : k = k . lstrip ( '/' ) url = '{}/{}' . format ( self . endpoint , k ) params = { } if recursive : params [ 'recurse' ] = '' r = requests . delete ( url , params = params ) if r . status_code != 200 : raise KVStoreError ( 'DELETE returned {}' . format ( r . status_code ) )
def add_months ( months , timestamp = datetime . datetime . utcnow ( ) ) : month = timestamp . month new_month = month + months years = 0 while new_month < 1 : new_month += 12 years -= 1 while new_month > 12 : new_month -= 12 years += 1 year = timestamp . year + years try : return datetime . datetime ( year , new_month , timestamp . day , timestamp . hour , timestamp . minute , timestamp . second ) except ValueError : if months > 0 : new_month += 1 if new_month > 12 : new_month -= 12 year += 1 return datetime . datetime ( year , new_month , 1 , timestamp . hour , timestamp . minute , timestamp . second ) else : new_day = calendar . monthrange ( year , new_month ) [ 1 ] return datetime . datetime ( year , new_month , new_day , timestamp . hour , timestamp . minute , timestamp . second )
def add_months_to_date ( months , date ) : month = date . month new_month = month + months years = 0 while new_month < 1 : new_month += 12 years -= 1 while new_month > 12 : new_month -= 12 years += 1 year = date . year + years try : return datetime . date ( year , new_month , date . day ) except ValueError : if months > 0 : new_month += 1 if new_month > 12 : new_month -= 12 year += 1 return datetime . datetime ( year , new_month , 1 ) else : new_day = calendar . monthrange ( year , new_month ) [ 1 ] return datetime . datetime ( year , new_month , new_day )
def is_christmas_period ( ) : now = datetime . date . today ( ) if now . month != 12 : return False if now . day < 15 : return False if now . day > 27 : return False return True
def from_csv ( self , label_column = 'labels' ) : df = pd . read_csv ( self . path , header = 0 ) X = df . loc [ : , df . columns != label_column ] . to_dict ( 'records' ) X = map_dict_list ( X , if_func = lambda k , v : v and math . isfinite ( v ) ) y = list ( df [ label_column ] . values ) return X , y
def from_json ( self ) : with gzip . open ( '%s.gz' % self . path , 'rt' ) if self . gz else open ( self . path ) as file : return list ( map ( list , zip ( * json . load ( file ) ) ) ) [ : : - 1 ]
def restore_data ( self , data_dict ) : session [ self . _base_key ] = data_dict self . _data_dict = session [ self . _base_key ]
def _verify_block ( self , block_type , block ) : if block_type in self . _registry : raise AlreadyRegistered ( "A block has already been registered to the {} `block_type` " "in the registry. Either unregister that block before trying " "to register this block under a different `block_type`" . format ( block_type ) ) if not isinstance ( block , Block ) : raise InvalidBlock ( "The block you tried register to {} is invalid. Only " "instances of `wagtail.wagtailcore.blocks.Block` may be " "registered with the the block_registry." . format ( block_type ) )
def register_block ( self , block_type , block ) : self . _verify_block ( block_type , block ) self . _registry [ block_type ] = block
def connect ( self ) : SCOPES = 'https://www.googleapis.com/auth/drive' store = file . Storage ( 'drive_credentials.json' ) creds = store . get ( ) if not creds or creds . invalid : try : flow = client . flow_from_clientsecrets ( 'client_secret.json' , SCOPES ) except InvalidClientSecretsError : log . error ( 'ERROR: Could not find client_secret.json in current directory, please obtain it from the API console.' ) return creds = tools . run_flow ( flow , store ) self . connection = build ( 'drive' , 'v3' , http = creds . authorize ( Http ( ) ) ) response = self . connection . files ( ) . list ( q = "name='Music' and mimeType='application/vnd.google-apps.folder' and trashed=false" ) . execute ( ) try : folder_id = response . get ( 'files' , [ ] ) [ 0 ] [ 'id' ] except IndexError : log . warning ( 'Music folder is missing. Creating it.' ) folder_metadata = { 'name' : 'Music' , 'mimeType' : 'application/vnd.google-apps.folder' } folder = self . connection . files ( ) . create ( body = folder_metadata , fields = 'id' ) . execute ( )
def connect ( self ) : if self . music_folder is None : music_folder = os . path . join ( os . path . expanduser ( '~' ) , 'Music' ) if not os . path . exists ( music_folder ) : os . makedirs ( music_folder ) self . music_folder = music_folder
def write_sky_params_to_file ( self ) : inp_file = self . sky_file + '_params.txt' lg . info ( 'Writing Inputs to file : ' + inp_file ) f = open ( inp_file , 'w' ) f . write ( 'verbose= ' + str ( self . verbose ) + '\n' ) f . write ( 'band_count= ' + str ( self . num_bands ) + '\n' ) f . write ( 'band_centres_data= ' ) f . write ( "," . join ( [ str ( wave ) for wave in self . wavelengths ] ) + '\n' ) f . write ( 'partition= ' + self . partition + '\n' ) f . write ( 'vn= ' + str ( self . vn ) + '\n' ) f . write ( 'hn= ' + str ( self . hn ) + '\n' ) f . write ( 'rdif= ' + str ( self . sky_r_dif ) + '\n' ) f . write ( 'theta_points= ' ) f . write ( "," . join ( [ str ( theta ) for theta in self . theta_points ] ) + '\n' ) f . write ( 'type= ' + self . sky_type + '\n' ) f . write ( 'azimuth= ' + str ( self . sky_azimuth ) + '\n' ) f . write ( 'zenith= ' + str ( self . sky_zenith ) + '\n' ) f . write ( 'sky_save_fp= ' + inp_file . strip ( '_params.txt' ) + '\n' ) f . write ( 'sky_image_save_fp= ' + self . sky_file + '.ppm' + '\n' ) f . write ( 'sky_image_size= 256' + '\n' ) if self . sky_type == 'hlideal' : f . write ( 'C= ' + str ( self . sky_c ) + '\n' ) f . write ( 'rdif= ' + str ( self . sky_r_dif ) + '\n' ) f . flush ( ) f . close ( )
def write_surf_params_to_file ( self ) : inp_file = self . water_surface_file + '_params.txt' lg . info ( 'Writing Inputs to file : ' + inp_file ) if self . surf_state == 'flat' : lg . info ( 'Surface Type is :: flat' ) f = open ( inp_file , 'w' ) f . write ( 'verbose= ' + str ( self . verbose ) + '\n' ) f . write ( 'band_count= ' + str ( self . num_bands ) + '\n' ) f . write ( 'band_centres_data= ' ) f . write ( "," . join ( [ str ( wave ) for wave in self . wavelengths ] ) + '\n' ) f . write ( 'partition= ' + self . partition + '\n' ) f . write ( 'vn= ' + str ( self . vn ) + '\n' ) f . write ( 'hn= ' + str ( self . hn ) + '\n' ) f . write ( 'theta_points= ' ) f . write ( "," . join ( [ str ( theta ) for theta in self . theta_points ] ) + '\n' ) f . write ( 'type= ' + self . iface_type + '\n' ) f . write ( 'refrac_index_0= ' + str ( self . iface_0_ri ) + '\n' ) f . write ( 'refrac_index_1= ' + str ( self . iface_1_ri ) + '\n' ) f . write ( 'wind_speed= ' + str ( self . wind_speed ) + '\n' ) f . write ( 'wind_direc= ' + str ( self . wind_direc ) + '\n' ) f . write ( 'crosswind_vertices= ' + str ( self . crosswind_vertices ) + '\n' ) f . write ( 'upwind_vertices= ' + str ( self . upwind_vertices ) + '\n' ) f . write ( 'surface_size= ' + str ( self . surface_size ) + '\n' ) f . write ( 'surface_radius=' + str ( self . surface_radius ) + '\n' ) f . write ( 'target_size= ' + str ( self . target_size ) + '\n' ) f . write ( 'rays_per_quad= ' + str ( self . rays_per_quad ) + '\n' ) f . write ( 'surface_count= ' + str ( self . surface_count ) + '\n' ) f . write ( 'azimuthally_average= ' + str ( self . azimuthally_average ) + '\n' ) f . write ( 'surface_save_fp= ' + inp_file . strip ( '_params.txt' ) + '\n' ) f . flush ( ) f . close ( )
def write_phase_params_to_file ( self ) : inp_file = os . path . join ( os . path . join ( self . input_path , 'phase_files' ) , self . phase_function_file ) + '_params.txt' lg . info ( 'Writing Inputs to file : ' + inp_file ) if self . iop_type == 'isotropic' or 'isotropic_integ' or 'petzold' or 'pure_water ' : lg . info ( 'Iop type is :: ' + self . iop_type ) f = open ( inp_file , 'w' ) f . write ( 'verbose = ' + str ( self . verbose ) + '\n' ) f . write ( 'band_count = ' + str ( self . num_bands ) + '\n' ) f . write ( 'band_centres_data = ' ) f . write ( "," . join ( [ str ( wave ) for wave in self . wavelengths ] ) + '\n' ) f . write ( 'partition = ' + self . partition + '\n' ) f . write ( 'vn = ' + str ( self . vn ) + '\n' ) f . write ( 'hn = ' + str ( self . hn ) + '\n' ) f . write ( 'theta_points = ' ) f . write ( "," . join ( [ str ( theta ) for theta in self . theta_points ] ) + '\n' ) f . write ( 'type = ' + self . iop_type + '\n' ) f . write ( 'phase_func_save_fp = ' + inp_file . strip ( '_params.txt' ) + '\n' ) f . flush ( ) f . close ( )
def update_filenames ( self ) : self . sky_file = os . path . abspath ( os . path . join ( os . path . join ( self . input_path , 'sky_files' ) , 'sky_' + self . sky_state + '_z' + str ( self . sky_zenith ) + '_a' + str ( self . sky_azimuth ) + '_' + str ( self . num_bands ) + '_' + self . ds_code ) )
def string_to_float_list ( string_var ) : try : return [ float ( s ) for s in string_var . strip ( '[' ) . strip ( ']' ) . split ( ', ' ) ] except : return [ float ( s ) for s in string_var . strip ( '[' ) . strip ( ']' ) . split ( ',' ) ]
def set_handler ( self , signals , handler = signal . SIG_DFL ) : for sig in signals : self . log . debug ( "Creating handler for signal: {0}" . format ( sig ) ) signal . signal ( sig , handler )
def pseudo_handler ( self , signum , frame ) : self . log . warn ( "Received sigal {0} but system is already busy processing a previous signal, current frame: {1}" . format ( signum , str ( frame ) ) )
def default_handler ( self , signum , frame ) : self . log . debug ( "Signal handler called with signal: {0}" . format ( signum ) ) if signum in self . restart_signals : self . set_handler ( self . handled_signals , self . pseudo_handler ) self . _cleanup ( ) os . execl ( 'python' , 'python' , * sys . argv ) elif signum in self . abort_signals : self . abort ( signum ) elif signum in self . pause_signals : self . pause ( signum ) elif signum in self . resume_signals : self . resume ( signum ) elif signum in self . status_signals : self . status ( signum ) elif signum in self . error_signals : self . log . error ( 'Signal handler received error signal from an external process, aborting' ) self . abort ( signum ) else : self . log . error ( "Unhandled signal received: {0}" . format ( signum ) ) raise
def status ( self , signum ) : self . log . debug ( 'Signal handler got status signal' ) new_status_callbacks = [ ] for status_call in self . status_callbacks : try : self . log . debug ( "Calling {0}({1},{2})" . format ( status_call [ 'function' ] . __name__ , status_call [ 'args' ] , status_call [ 'kwargs' ] ) ) except AttributeError : self . log . debug ( "Calling unbound function/method {0}" . format ( str ( status_call ) ) ) apply ( status_call [ 'function' ] , status_call [ 'args' ] , status_call [ 'kwargs' ] ) if status_call [ 'persistent' ] : new_status_callbacks . append ( status_call ) self . status_callbacks = new_status_callbacks self . _resume ( signum )
def _unreg_event ( self , event_list , event ) : try : self . log . debug ( "Removing event {0}({1},{2})" . format ( event [ 'function' ] . __name__ , event [ 'args' ] , event [ 'kwargs' ] ) ) except AttributeError : self . log . debug ( "Removing event {0}" . format ( str ( event ) ) ) try : event_list . remove ( event ) except ValueError : try : self . log . warn ( "Unable to remove event {0}({1},{2}) , not found in list: {3}" . format ( event [ 'function' ] . __name__ , event [ 'args' ] , event [ 'kwargs' ] , event_list ) ) except AttributeError : self . log . debug ( "Unable to remove event {0}" . format ( str ( event ) ) ) raise KeyError ( 'Unable to unregister the specified event from the signals specified' )
def __sig_from_partial ( self , inst ) : self . pargl = list ( inst . pargl ) self . kargl = list ( inst . kargl ) self . def_argv = inst . def_argv . copy ( ) self . var_pargs = inst . var_pargs self . var_kargs = inst . var_kargs
def vlq2int ( data ) : byte = ord ( data . read ( 1 ) ) value = byte & 0x7F shift = 1 while byte & 0x80 != 0 : byte = ord ( data . read ( 1 ) ) value = ( ( byte & 0x7F ) << shift * 7 ) | value shift += 1 return value
def _parse_header ( self ) : header = OrderedDict ( ) user_data_header = self . archive . header [ 'user_data_header' ] [ 'content' ] if re . search ( r'StarCraft II replay' , user_data_header ) : user_data_header = StringIO . StringIO ( user_data_header ) user_data_header . seek ( 30 ) header . update ( read_table ( user_data_header , [ 'release_flag' , 'major_version' , 'minor_version' , 'maintenance_version' , 'build_number' , 'unknown' , 'unknown' , 'duration' ] ) ) header [ 'version' ] = '%s.%s.%s.%s' % ( header [ 'major_version' ] , header [ 'minor_version' ] , header [ 'maintenance_version' ] , header [ 'build_number' ] ) if not header [ 'release_flag' ] : header [ 'version' ] += ' (dev)' header [ 'duration' ] /= 16 else : raise ValueError ( "The given file is not a StarCraft II replay." ) return header
def get_duration ( self , seconds ) : duration = "" minutes , seconds = divmod ( seconds , 60 ) if minutes >= 60 : hours , minutes = divmod ( minutes , 60 ) duration = "%sh " % hours duration += "%sm %ss" % ( minutes , seconds ) return duration
def print_details ( self ) : print 'Map      ' , self . map print 'Duration ' , self . duration print 'Version  ' , self . version print 'Team  Player       Race       Color' print '-----------------------------------' for player in self . players : print '{team:<5} {name:12} {race:10} {color}' . format ( * * player )
def data ( self ) : self . batch_name_value = self . ui . batch_name_value . text ( ) self . saa_values = self . ui . saa_values . text ( ) self . sza_values = self . ui . sza_values . text ( ) self . p_values = self . ui . p_values . text ( ) self . x_value = self . ui . x_value . text ( ) self . y_value = self . ui . y_value . text ( ) self . g_value = self . ui . g_value . text ( ) self . s_value = self . ui . s_value . text ( ) self . z_value = self . ui . z_value . text ( ) self . wavelength_values = self . ui . wavelength_values . text ( ) self . verbose_value = self . ui . verbose_value . text ( ) self . phytoplankton_path = self . ui . phyto_path . text ( ) self . bottom_path = self . ui . bottom_path . text ( ) self . executive_path = self . ui . exec_path . text ( ) self . nb_cpu = self . ui . nb_cpu . currentText ( ) self . report_parameter_value = str ( self . ui . report_parameter_value . text ( ) )
def search_file_result ( self ) : if self . ui . tabWidget . currentIndex ( ) == TabWidget . NORMAL_MODE : self . result_file = self . file_dialog . getOpenFileName ( caption = str ( "Open Report File" ) , directory = "./outputs" ) if not self . result_file == '' : self . ui . show_all_curves . setDisabled ( False ) self . ui . show_grid . setDisabled ( False ) self . data_processing ( ) self . display_the_graphic ( self . num_line , self . wavelength , self . data_wanted , self . information ) self . authorized_display = True
def write_to_file ( self ) : bt = BatchFile ( self . batch_name_value , self . p_values , self . x_value , self . y_value , self . g_value , self . s_value , self . z_value , self . wavelength_values , self . verbose_value , self . phytoplankton_path , self . bottom_path , self . nb_cpu , self . executive_path , self . saa_values , self . sza_values , self . report_parameter_value ) bt . write_batch_to_file ( str ( self . batch_name_value + "_batch.txt" ) )
def data_processing ( self ) : the_file_name = str ( self . result_file ) the_file = open ( the_file_name , 'r' ) lines = the_file . readlines ( ) lines_array = [ ] for line in lines : line = line . split ( ',' ) lines_array . append ( line ) labels_line = lines_array [ 0 ] cell_labels_line = 0 flag = True try : while flag : if "wave length (nm)" in labels_line [ cell_labels_line ] : index = labels_line . index ( labels_line [ cell_labels_line ] ) flag = False else : cell_labels_line += 1 except IndexError : raise sys . exit ( "Warning : There is no value named 'wavelength' in the file used to plot curves. " "So, I can't separate data to plot curves and data about tests linking with these curves." ) self . information = [ ] data_wavelength = [ ] self . num_line = 0 for line in lines_array : cell_line = 0 self . information . append ( [ ] ) data_wavelength . append ( [ ] ) while cell_line < len ( line ) : if cell_line < index : self . information [ self . num_line ] . append ( line [ cell_line ] ) elif cell_line > index : data_wavelength [ self . num_line ] . append ( line [ cell_line ] ) cell_line += 1 self . num_line += 1 line_wavelength = 0 for row_data_wavelength in data_wavelength : row_data_wavelength = [ float ( item . strip ( '\n' ) . strip ( '\"' ) ) for item in row_data_wavelength ] data_wavelength [ line_wavelength ] = row_data_wavelength line_wavelength += 1 self . wavelength = data_wavelength [ 0 ] self . data_wanted = data_wavelength [ 1 : ] the_file . close ( )
def display_error_message ( self ) : self . ui . error_label . setScaledContents ( True ) self . ui . error_text_label . show ( ) self . ui . error_text_label . setStyleSheet ( 'color: red' )
def hide_error_message ( self ) : self . ui . error_label . setScaledContents ( False ) self . ui . error_text_label . hide ( )
def run ( self ) : print ( 'Executing planarrad' ) if self . ui . tabWidget . currentIndex ( ) == TabWidget . NORMAL_MODE : self . data ( ) self . check_values ( ) if self . without_error == False : self . display_error_message ( ) elif self . without_error == True : self . is_running = True self . hide_error_message ( ) self . write_to_file ( ) os . chdir ( './' ) self . progress_bar ( ) this_dir = os . path . dirname ( os . path . realpath ( __file__ ) ) . rstrip ( 'gui/' ) batch_file = os . path . join ( this_dir , "inputs/batch_files/" + str ( self . batch_name_value ) + "_batch.txt" ) print ( batch_file ) self . p = subprocess . Popen ( [ "./planarrad.py -i " + batch_file ] , shell = True ) if self . ui . progressBar . value ( ) == 100 : self . display_the_graphic ( self . num_line , self . wavelength , self . data_wanted , self . information )
def cancel_planarrad ( self ) : if ( self . is_running == True ) & ( self . ui . tabWidget . currentIndex ( ) == TabWidget . NORMAL_MODE ) : cancel = QtGui . QMessageBox . question ( self . ui . cancel , 'Cancel PlanarRad' , "Are you sure to cancel ?" , QtGui . QMessageBox . Yes , QtGui . QMessageBox . No ) if cancel == QtGui . QMessageBox . Yes : self . is_running = False os . kill ( self . p . pid , signal . SIGTERM ) print ( "Necessary to check if cancel_planarrad works well !" ) self . ui . progressBar . reset ( ) else : pass
def quit ( self ) : if self . is_running == True : warning_planarrad_running = QtGui . QMessageBox . warning ( self . ui . quit , 'Warning !' , "PlanarRad is running. Stop it before quit !" , QtGui . QMessageBox . Ok ) else : quit = QtGui . QMessageBox . question ( self . ui . quit , 'Quit PlanarRad' , "Are you sure to quit ?" , QtGui . QMessageBox . Yes , QtGui . QMessageBox . No ) if quit == QtGui . QMessageBox . Yes : QtGui . qApp . quit ( )
def open_log_file ( self ) : f = open ( os . path . expanduser ( '~/.planarradpy/log/libplanarradpy.log' ) ) self . uiLog . textEdit . setPlainText ( str ( f . read ( ) ) ) self . log_window . show ( )
def open_documentation ( self ) : window = Window ( ) html = QtCore . QUrl . fromLocalFile ( os . path . join ( os . getcwd ( ) , './docs/_build/html/index.html' ) ) #open('./docs/_build/html/index.html').read() #window.show() window . view . load ( html ) window . show ( ) window . exec_ ( )
def prerequisite_actions ( self ) : self . hide_error_message ( ) self . ui . show_all_curves . setDisabled ( True ) self . ui . sens . setDisabled ( True ) self . ui . show_grid . setDisabled ( True ) pathname = os . path . dirname ( sys . argv [ 0 ] ) path = os . path . abspath ( pathname ) self . verbose_value = self . ui . verbose_value . setText ( "6" ) self . report_parameter_value = self . ui . report_parameter_value . setText ( "Rrs" ) self . ui . progressBar . reset ( )
def click ( self , event ) : if event . button == 3 : if self . ui . tabWidget . currentIndex ( ) == TabWidget . NORMAL_MODE : self . pos = QtGui . QCursor ( ) . pos ( ) self . graphic_context_menu ( self . pos )
def mouse_move ( self , event ) : if ( self . ui . tabWidget . currentIndex ( ) == TabWidget . NORMAL_MODE ) : self . posX = event . xdata self . posY = event . ydata self . graphic_target ( self . posX , self . posY )
def graphic_target ( self , x , y ) : if self . authorized_display == True : try : self . display_the_graphic ( self . num_line , self . wavelength , self . data_wanted , self . information ) self . ui . mouse_coordinate . setText ( "(%0.3f, %0.3f)" % ( x , y ) ) except : pass
def sign ( self , privkey ) : if self . v : raise InvalidSignature ( "already signed" ) if privkey in ( 0 , '' , '\x00' * 32 ) : raise InvalidSignature ( "Zero privkey cannot sign" ) rawhash = sha3 ( rlp . encode ( self , self . __class__ . exclude ( [ 'v' , 'r' , 's' ] ) ) ) if len ( privkey ) == 64 : privkey = encode_privkey ( privkey , 'bin' ) pk = PrivateKey ( privkey , raw = True ) signature = pk . ecdsa_recoverable_serialize ( pk . ecdsa_sign_recoverable ( rawhash , raw = True ) ) signature = signature [ 0 ] + chr ( signature [ 1 ] ) self . v = ord ( signature [ 64 ] ) + 27 self . r = big_endian_to_int ( signature [ 0 : 32 ] ) self . s = big_endian_to_int ( signature [ 32 : 64 ] ) self . _sender = None return self
def hash ( self ) : if self . sender is None : raise MissingSignatureError ( ) class HashSerializable ( rlp . Serializable ) : fields = [ ( field , sedes ) for field , sedes in self . fields if field not in ( 'v' , 'r' , 's' ) ] + [ ( '_sender' , binary ) ] _sedes = None return sha3 ( rlp . encode ( self , HashSerializable ) )
def check ( self ) : if not self . is_valid : return True test = ( self . has_quorum , self . has_quorum_possible , self . has_noquorum ) assert 1 == len ( [ x for x in test if x is not None ] ) return True
def validate_votes ( self , validators_H , validators_prevH ) : assert self . sender def check ( lockset , validators ) : if not lockset . num_eligible_votes == len ( validators ) : raise InvalidProposalError ( 'lockset num_eligible_votes mismatch' ) for v in lockset : if v . sender not in validators : raise InvalidProposalError ( 'invalid signer' ) if self . round_lockset : check ( self . round_lockset , validators_H ) check ( self . signing_lockset , validators_prevH ) return True
def validate_votes ( self , validators_H ) : assert self . sender if not self . round_lockset . num_eligible_votes == len ( validators_H ) : raise InvalidProposalError ( 'round_lockset num_eligible_votes mismatch' ) for v in self . round_lockset : if v . sender not in validators_H : raise InvalidProposalError ( 'invalid signer' )
def issue_funds ( ctx , amount = 'uint256' , rtgs_hash = 'bytes32' , returns = STATUS ) : ctx . accounts [ ctx . msg_sender ] += amount ctx . issued_amounts [ ctx . msg_sender ] += amount ctx . Issuance ( ctx . msg_sender , rtgs_hash , amount ) return OK
def last_lock ( self ) : rs = list ( self . rounds ) assert len ( rs ) < 2 or rs [ 0 ] > rs [ 1 ] for r in self . rounds : if self . rounds [ r ] . lock is not None : return self . rounds [ r ] . lock
def last_voted_blockproposal ( self ) : for r in self . rounds : if isinstance ( self . rounds [ r ] . proposal , BlockProposal ) : assert isinstance ( self . rounds [ r ] . lock , Vote ) if self . rounds [ r ] . proposal . blockhash == self . rounds [ r ] . lock . blockhash : return self . rounds [ r ] . proposal
def last_valid_lockset ( self ) : for r in self . rounds : ls = self . rounds [ r ] . lockset if ls . is_valid : return ls return None
def get_timeout ( self ) : if self . timeout_time is not None or self . proposal : return now = self . cm . chainservice . now round_timeout = ConsensusManager . round_timeout round_timeout_factor = ConsensusManager . round_timeout_factor delay = round_timeout * round_timeout_factor ** self . round self . timeout_time = now + delay return delay
def on_proposal ( self , proposal , proto ) : assert isinstance ( proto , HDCProtocol ) assert isinstance ( proposal , Proposal ) if proposal . height >= self . cm . height : assert proposal . lockset . is_valid self . last_active_protocol = proto
def mk_privkeys ( num ) : privkeys = [ ] assert num <= num_colors for i in range ( num ) : j = 0 while True : k = sha3 ( str ( j ) ) a = privtoaddr ( k ) an = big_endian_to_int ( a ) if an % num_colors == i : break j += 1 privkeys . append ( k ) return privkeys
def delay ( self , sender , receiver , packet , add_delay = 0 ) : bw = min ( sender . ul_bandwidth , receiver . dl_bandwidth ) delay = sender . base_latency + receiver . base_latency delay += len ( packet ) / bw delay += add_delay return delay
def deliver ( self , sender , receiver , packet ) : to = ConsensusManager . round_timeout assert to > 0 print "in slow transport deliver" super ( SlowTransport , self ) . deliver ( sender , receiver , packet , add_delay = to )
def chain_nac_proxy ( chain , sender , contract_address , value = 0 ) : klass = registry [ contract_address ] . im_self assert issubclass ( klass , NativeABIContract ) def mk_method ( method ) : def m ( s , * args ) : data = abi_encode_args ( method , args ) block = chain . head_candidate output = test_call ( block , sender , contract_address , data ) if output is not None : return abi_decode_return_vals ( method , output ) return m class cproxy ( object ) : pass for m in klass . _abi_methods ( ) : setattr ( cproxy , m . __func__ . func_name , mk_method ( m ) ) return cproxy ( )
def address_to_native_contract_class ( self , address ) : assert isinstance ( address , bytes ) and len ( address ) == 20 assert self . is_instance_address ( address ) nca = self . native_contract_address_prefix + address [ - 4 : ] return self . native_contracts [ nca ]
def update ( self , data ) : if data not in self . filter : self . filter . append ( data ) if len ( self . filter ) > self . max_items : self . filter . pop ( 0 ) return True else : self . filter . append ( self . filter . pop ( 0 ) ) return False
def on_receive_transactions ( self , proto , transactions ) : log . debug ( '----------------------------------' ) log . debug ( 'remote_transactions_received' , count = len ( transactions ) , remote_id = proto ) def _add_txs ( ) : for tx in transactions : self . add_transaction ( tx , origin = proto ) gevent . spawn ( _add_txs )
def img_from_vgg ( x ) : x = x . transpose ( ( 1 , 2 , 0 ) ) x [ : , : , 0 ] += 103.939 x [ : , : , 1 ] += 116.779 x [ : , : , 2 ] += 123.68 x = x [ : , : , : : - 1 ] return x
def img_to_vgg ( x ) : x = x [ : , : , : : - 1 ] x [ : , : , 0 ] -= 103.939 x [ : , : , 1 ] -= 116.779 x [ : , : , 2 ] -= 123.68 x = x . transpose ( ( 2 , 0 , 1 ) ) return x
def get_f_layer ( self , layer_name ) : inputs = [ self . net_input ] if self . learning_phase is not None : inputs . append ( K . learning_phase ( ) ) return K . function ( inputs , [ self . get_layer_output ( layer_name ) ] )
def get_layer_output ( self , name ) : if not name in self . _f_layer_outputs : layer = self . net . get_layer ( name ) self . _f_layer_outputs [ name ] = layer . output return self . _f_layer_outputs [ name ]
def get_features ( self , x , layers ) : if not layers : return None inputs = [ self . net . input ] if self . learning_phase is not None : inputs . append ( self . learning_phase ) f = K . function ( inputs , [ self . get_layer_output ( layer_name ) for layer_name in layers ] ) feature_outputs = f ( [ x ] ) features = dict ( zip ( layers , feature_outputs ) ) return features
def fix_compile ( remove_flags ) : import distutils . ccompiler def _fix_compile ( self , sources , output_dir = None , macros = None , include_dirs = None , debug = 0 , extra_preargs = None , extra_postargs = None , depends = None ) : for flag in remove_flags : if flag in self . compiler_so : self . compiler_so . remove ( flag ) macros , objects , extra_postargs , pp_opts , build = self . _setup_compile ( output_dir , macros , include_dirs , sources , depends , extra_postargs ) cc_args = self . _get_cc_args ( pp_opts , debug , extra_preargs ) for obj in objects : try : src , ext = build [ obj ] except KeyError : continue self . _compile ( obj , src , ext , cc_args , extra_postargs , pp_opts ) return objects distutils . ccompiler . CCompiler . compile = _fix_compile
def do_table ( self , line ) : if len ( line ) > 0 : if line . strip ( ) . lower ( ) == "on" : log . write ( "Table ON" ) self . table_output = True return elif line . strip ( ) . lower ( ) == "off" : log . write ( "Table OFF" ) self . table_output = False return log . write ( "Table output: {}" . format ( "ON" if self . table_output else "OFF" ) )
def float_with_multiplier ( string ) : match = re_float_with_multiplier . search ( string ) if not match or not match . group ( 'num' ) : raise ValueError ( 'String "{}" is not numeric!' . format ( string ) ) num = float ( match . group ( 'num' ) ) multi = match . group ( 'multi' ) if multi : try : num *= multipliers [ multi ] except KeyError : raise ValueError ( 'Unknown multiplier: {}' . format ( multi ) ) return num
def specific_gains ( string ) : if not string : return { } gains = { } for gain in string . split ( ',' ) : amp_name , value = gain . split ( '=' ) gains [ amp_name . strip ( ) ] = float ( value . strip ( ) ) return gains
def device_settings ( string ) : if not string : return { } settings = { } for setting in string . split ( ',' ) : setting_name , value = setting . split ( '=' ) settings [ setting_name . strip ( ) ] = value . strip ( ) return settings
def wrap ( text , indent = '    ' ) : wrapper = textwrap . TextWrapper ( width = int ( os . environ . get ( 'COLUMNS' , 80 ) ) , initial_indent = indent , subsequent_indent = indent ) return '\n' . join ( wrapper . wrap ( text ) )
def detect_devices ( soapy_args = '' ) : devices = simplesoapy . detect_devices ( soapy_args , as_string = True ) text = [ ] text . append ( 'Detected SoapySDR devices:' ) if devices : for i , d in enumerate ( devices ) : text . append ( '  {}' . format ( d ) ) else : text . append ( '  No devices found!' ) return ( devices , '\n' . join ( text ) )
def set_center_freq ( self , center_freq ) : psd_state = { 'repeats' : 0 , 'freq_array' : self . _base_freq_array + self . _lnb_lo + center_freq , 'pwr_array' : None , 'update_lock' : threading . Lock ( ) , 'futures' : [ ] , } return psd_state
def result ( self , psd_state ) : freq_array = numpy . fft . fftshift ( psd_state [ 'freq_array' ] ) pwr_array = numpy . fft . fftshift ( psd_state [ 'pwr_array' ] ) if self . _crop_factor : crop_bins_half = round ( ( self . _crop_factor * self . _bins ) / 2 ) freq_array = freq_array [ crop_bins_half : - crop_bins_half ] pwr_array = pwr_array [ crop_bins_half : - crop_bins_half ] if psd_state [ 'repeats' ] > 1 : pwr_array = pwr_array / psd_state [ 'repeats' ] if self . _log_scale : pwr_array = 10 * numpy . log10 ( pwr_array ) return ( freq_array , pwr_array )
def wait_for_result ( self , psd_state ) : if len ( psd_state [ 'futures' ] ) > 1 : concurrent . futures . wait ( psd_state [ 'futures' ] ) elif psd_state [ 'futures' ] : psd_state [ 'futures' ] [ 0 ] . result ( ) return self . result ( psd_state )
def update ( self , psd_state , samples_array ) : freq_array , pwr_array = simplespectral . welch ( samples_array , self . _sample_rate , nperseg = self . _bins , window = self . _fft_window , noverlap = self . _fft_overlap_bins , detrend = self . _detrend ) if self . _remove_dc : pwr_array [ 0 ] = ( pwr_array [ 1 ] + pwr_array [ - 1 ] ) / 2 with psd_state [ 'update_lock' ] : psd_state [ 'repeats' ] += 1 if psd_state [ 'pwr_array' ] is None : psd_state [ 'pwr_array' ] = pwr_array else : psd_state [ 'pwr_array' ] += pwr_array
def read ( self , f ) : magic = f . read ( len ( self . magic ) ) if not magic : return None if magic != self . magic : raise ValueError ( 'Magic bytes not found! Read data: {}' . format ( magic ) ) header = self . header . _make ( self . header_struct . unpack ( f . read ( self . header_struct . size ) ) ) pwr_array = numpy . fromstring ( f . read ( header . size ) , dtype = 'float32' ) return ( header , pwr_array )
def write ( self , f , time_start , time_stop , start , stop , step , samples , pwr_array ) : f . write ( self . magic ) f . write ( self . header_struct . pack ( self . version , time_start , time_stop , start , stop , step , samples , pwr_array . nbytes ) ) #pwr_array.tofile(f) f . write ( pwr_array . tobytes ( ) ) f . flush ( )
def write ( self , psd_data_or_future , time_start , time_stop , samples ) : try : f_array , pwr_array = psd_data_or_future . result ( ) except AttributeError : f_array , pwr_array = psd_data_or_future try : step = f_array [ 1 ] - f_array [ 0 ] self . formatter . write ( self . output , time_start . timestamp ( ) , time_stop . timestamp ( ) , f_array [ 0 ] , f_array [ - 1 ] + step , step , samples , pwr_array ) except Exception as e : logging . exception ( 'Error writing to output file: {}' . format ( e ) )
def write ( self , psd_data_or_future , time_start , time_stop , samples ) : try : f_array , pwr_array = psd_data_or_future . result ( ) except AttributeError : f_array , pwr_array = psd_data_or_future self . output . write ( ) self . output . write ( . format ( time_start ) ) self . output . write ( . format ( time_stop ) ) self . output . write ( '#\n' ) self . output . write ( ) for f , pwr in zip ( f_array , pwr_array ) : self . output . write ( '{} {}\n' . format ( f , pwr ) ) self . output . write ( '\n' ) self . output . flush ( )
def write ( self , psd_data_or_future , time_start , time_stop , samples ) : try : f_array , pwr_array = psd_data_or_future . result ( ) except AttributeError : f_array , pwr_array = psd_data_or_future try : step = f_array [ 1 ] - f_array [ 0 ] row = [ time_stop . strftime ( '%Y-%m-%d' ) , time_stop . strftime ( '%H:%M:%S' ) , f_array [ 0 ] , f_array [ - 1 ] + step , step , samples ] row += list ( pwr_array ) self . output . write ( '{}\n' . format ( ', ' . join ( str ( x ) for x in row ) ) ) self . output . flush ( ) except Exception as e : logging . exception ( 'Error writing to output file:' )
def time_to_repeats ( self , bins , integration_time ) : return math . ceil ( ( self . device . sample_rate * integration_time ) / bins )
def freq_plan ( self , min_freq , max_freq , bins , overlap = 0 , quiet = False ) : bin_size = self . bins_to_bin_size ( bins ) bins_crop = round ( ( 1 - overlap ) * bins ) sample_rate_crop = ( 1 - overlap ) * self . device . sample_rate freq_range = max_freq - min_freq hopping = True if freq_range >= sample_rate_crop else False hop_size = self . nearest_freq ( sample_rate_crop , bin_size ) hops = math . ceil ( freq_range / hop_size ) if hopping else 1 min_center_freq = min_freq + ( hop_size / 2 ) if hopping else min_freq + ( freq_range / 2 ) max_center_freq = min_center_freq + ( ( hops - 1 ) * hop_size ) freq_list = [ min_center_freq + ( i * hop_size ) for i in range ( hops ) ] if not quiet : logger . info ( 'overlap: {:.5f}' . format ( overlap ) ) logger . info ( 'bin_size: {:.2f} Hz' . format ( bin_size ) ) logger . info ( 'bins: {}' . format ( bins ) ) logger . info ( 'bins (after crop): {}' . format ( bins_crop ) ) logger . info ( 'sample_rate: {:.3f} MHz' . format ( self . device . sample_rate / 1e6 ) ) logger . info ( 'sample_rate (after crop): {:.3f} MHz' . format ( sample_rate_crop / 1e6 ) ) logger . info ( 'freq_range: {:.3f} MHz' . format ( freq_range / 1e6 ) ) logger . info ( 'hopping: {}' . format ( 'YES' if hopping else 'NO' ) ) logger . info ( 'hop_size: {:.3f} MHz' . format ( hop_size / 1e6 ) ) logger . info ( 'hops: {}' . format ( hops ) ) logger . info ( 'min_center_freq: {:.3f} MHz' . format ( min_center_freq / 1e6 ) ) logger . info ( 'max_center_freq: {:.3f} MHz' . format ( max_center_freq / 1e6 ) ) logger . info ( 'min_freq (after crop): {:.3f} MHz' . format ( ( min_center_freq - ( hop_size / 2 ) ) / 1e6 ) ) logger . info ( 'max_freq (after crop): {:.3f} MHz' . format ( ( max_center_freq + ( hop_size / 2 ) ) / 1e6 ) ) logger . debug ( 'Frequency hops table:' ) logger . debug ( '  {:8s}      {:8s}      {:8s}' . format ( 'Min:' , 'Center:' , 'Max:' ) ) for f in freq_list : logger . debug ( '  {:8.3f} MHz  {:8.3f} MHz  {:8.3f} MHz' . format ( ( f - ( self . device . sample_rate / 2 ) ) / 1e6 , f / 1e6 , ( f + ( self . device . sample_rate / 2 ) ) / 1e6 , ) ) return freq_list
def create_buffer ( self , bins , repeats , base_buffer_size , max_buffer_size = 0 ) : samples = bins * repeats buffer_repeats = 1 buffer_size = math . ceil ( samples / base_buffer_size ) * base_buffer_size if not max_buffer_size : max_buffer_size = ( 100 * 1024 ** 2 ) / 8 if max_buffer_size > 0 : max_buffer_size = math . ceil ( max_buffer_size / base_buffer_size ) * base_buffer_size if buffer_size > max_buffer_size : logger . warning ( 'Required buffer size ({}) will be shrinked to max_buffer_size ({})!' . format ( buffer_size , max_buffer_size ) ) buffer_repeats = math . ceil ( buffer_size / max_buffer_size ) buffer_size = max_buffer_size logger . info ( 'repeats: {}' . format ( repeats ) ) logger . info ( 'samples: {} (time: {:.5f} s)' . format ( samples , samples / self . device . sample_rate ) ) if max_buffer_size > 0 : logger . info ( 'max_buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( max_buffer_size , max_buffer_size / bins , max_buffer_size / self . device . sample_rate ) ) else : logger . info ( 'max_buffer_size (samples): UNLIMITED' ) logger . info ( 'buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( buffer_size , buffer_size / bins , buffer_size / self . device . sample_rate ) ) logger . info ( 'buffer_repeats: {}' . format ( buffer_repeats ) ) return ( buffer_repeats , zeros ( buffer_size , numpy . complex64 ) )
def setup ( self , bins , repeats , base_buffer_size = 0 , max_buffer_size = 0 , fft_window = 'hann' , fft_overlap = 0.5 , crop_factor = 0 , log_scale = True , remove_dc = False , detrend = None , lnb_lo = 0 , tune_delay = 0 , reset_stream = False , max_threads = 0 , max_queue_size = 0 ) : if self . device . is_streaming : self . device . stop_stream ( ) base_buffer = self . device . start_stream ( buffer_size = base_buffer_size ) self . _bins = bins self . _repeats = repeats self . _base_buffer_size = len ( base_buffer ) self . _max_buffer_size = max_buffer_size self . _buffer_repeats , self . _buffer = self . create_buffer ( bins , repeats , self . _base_buffer_size , self . _max_buffer_size ) self . _tune_delay = tune_delay self . _reset_stream = reset_stream self . _psd = psd . PSD ( bins , self . device . sample_rate , fft_window = fft_window , fft_overlap = fft_overlap , crop_factor = crop_factor , log_scale = log_scale , remove_dc = remove_dc , detrend = detrend , lnb_lo = lnb_lo , max_threads = max_threads , max_queue_size = max_queue_size ) self . _writer = writer . formats [ self . _output_format ] ( self . _output )
def stop ( self ) : if not self . device . is_streaming : return self . device . stop_stream ( ) self . _writer . close ( ) self . _bins = None self . _repeats = None self . _base_buffer_size = None self . _max_buffer_size = None self . _buffer_repeats = None self . _buffer = None self . _tune_delay = None self . _reset_stream = None self . _psd = None self . _writer = None
def psd ( self , freq ) : if not self . device . is_streaming : raise RuntimeError ( 'Streaming is not initialized, you must run setup() first!' ) logger . debug ( '  Frequency hop: {:.2f} Hz' . format ( freq ) ) t_freq = time . time ( ) if self . device . freq != freq : if self . _reset_stream : self . device . device . deactivateStream ( self . device . stream ) self . device . freq = freq if self . _reset_stream : self . device . device . activateStream ( self . device . stream ) if self . _tune_delay : t_delay = time . time ( ) while True : self . device . read_stream ( ) t_delay_end = time . time ( ) if t_delay_end - t_delay >= self . _tune_delay : break logger . debug ( '    Tune delay: {:.3f} s' . format ( t_delay_end - t_delay ) ) else : logger . debug ( '    Same frequency as before, tuning skipped' ) psd_state = self . _psd . set_center_freq ( freq ) t_freq_end = time . time ( ) logger . debug ( '    Tune time: {:.3f} s' . format ( t_freq_end - t_freq ) ) for repeat in range ( self . _buffer_repeats ) : logger . debug ( '    Repeat: {}' . format ( repeat + 1 ) ) t_acq = time . time ( ) acq_time_start = datetime . datetime . utcnow ( ) self . device . read_stream_into_buffer ( self . _buffer ) acq_time_stop = datetime . datetime . utcnow ( ) t_acq_end = time . time ( ) logger . debug ( '      Acquisition time: {:.3f} s' . format ( t_acq_end - t_acq ) ) self . _psd . update_async ( psd_state , numpy . copy ( self . _buffer ) ) t_final = time . time ( ) if _shutdown : break psd_future = self . _psd . result_async ( psd_state ) logger . debug ( '    Total hop time: {:.3f} s' . format ( t_final - t_freq ) ) return ( psd_future , acq_time_start , acq_time_stop )
def sweep ( self , min_freq , max_freq , bins , repeats , runs = 0 , time_limit = 0 , overlap = 0 , fft_window = 'hann' , fft_overlap = 0.5 , crop = False , log_scale = True , remove_dc = False , detrend = None , lnb_lo = 0 , tune_delay = 0 , reset_stream = False , base_buffer_size = 0 , max_buffer_size = 0 , max_threads = 0 , max_queue_size = 0 ) : self . setup ( bins , repeats , base_buffer_size , max_buffer_size , fft_window = fft_window , fft_overlap = fft_overlap , crop_factor = overlap if crop else 0 , log_scale = log_scale , remove_dc = remove_dc , detrend = detrend , lnb_lo = lnb_lo , tune_delay = tune_delay , reset_stream = reset_stream , max_threads = max_threads , max_queue_size = max_queue_size ) try : freq_list = self . freq_plan ( min_freq - lnb_lo , max_freq - lnb_lo , bins , overlap ) t_start = time . time ( ) run = 0 while not _shutdown and ( runs == 0 or run < runs ) : run += 1 t_run_start = time . time ( ) logger . debug ( 'Run: {}' . format ( run ) ) for freq in freq_list : psd_future , acq_time_start , acq_time_stop = self . psd ( freq ) self . _writer . write_async ( psd_future , acq_time_start , acq_time_stop , len ( self . _buffer ) * self . _buffer_repeats ) if _shutdown : break write_next_future = self . _writer . write_next_async ( ) t_run = time . time ( ) logger . debug ( '  Total run time: {:.3f} s' . format ( t_run - t_run_start ) ) if time_limit and ( time . time ( ) - t_start ) >= time_limit : logger . info ( 'Time limit of {} s exceeded, completed {} runs' . format ( time_limit , run ) ) break write_next_future . result ( ) logging . debug ( 'Number of USB buffer overflow errors: {}' . format ( self . device . buffer_overflow_count ) ) logging . debug ( 'PSD worker threads: {}' . format ( self . _psd . _executor . _max_workers ) ) logging . debug ( 'Max. PSD queue size: {} / {}' . format ( self . _psd . _executor . max_queue_size_reached , self . _psd . _executor . max_queue_size ) ) logging . debug ( 'Writer worker threads: {}' . format ( self . _writer . _executor . _max_workers ) ) logging . debug ( 'Max. Writer queue size: {} / {}' . format ( self . _writer . _executor . max_queue_size_reached , self . _writer . _executor . max_queue_size ) ) finally : self . stop ( ) t_stop = time . time ( ) logger . info ( 'Total time: {:.3f} s' . format ( t_stop - t_start ) )
def run_cmake ( arg = "" ) : if ds . find_executable ( 'cmake' ) is None : print "CMake  is required to build zql" print "Please install cmake version >= 2.8 and re-run setup" sys . exit ( - 1 ) print "Configuring zql build with CMake.... " cmake_args = arg try : build_dir = op . join ( op . split ( __file__ ) [ 0 ] , 'build' ) dd . mkpath ( build_dir ) os . chdir ( "build" ) ds . spawn ( [ 'cmake' , '..' ] + cmake_args . split ( ) ) ds . spawn ( [ 'make' , 'clean' ] ) ds . spawn ( [ 'make' ] ) os . chdir ( ".." ) except ds . DistutilsExecError : print "Error while running cmake" print "run 'setup.py build --help' for build options" print "You may also try editing the settings in CMakeLists.txt file and re-running setup" sys . exit ( - 1 )
def bring_gpio_interrupt_into_userspace ( ) : try : with open ( GPIO_INTERRUPT_DEVICE_VALUE ) : return except IOError : with open ( GPIO_EXPORT_FILE , 'w' ) as export_file : export_file . write ( str ( GPIO_INTERRUPT_PIN ) ) wait_until_file_exists ( GPIO_INTERRUPT_DEVICE_VALUE )
def gpio_interrupts_enable ( self ) : try : bring_gpio_interrupt_into_userspace ( ) set_gpio_interrupt_edge ( ) except Timeout as e : raise InterruptEnableException ( "There was an error bringing gpio%d into userspace. %s" % ( GPIO_INTERRUPT_PIN , e . message ) )
def has_errors ( self , form ) : return any ( [ fieldname_error for fieldname_error in form . errors . keys ( ) if fieldname_error in self ] )
def get_form_kwargs ( self ) : kwargs = super ( FormContainersMixin , self ) . get_form_kwargs ( ) kwargs . update ( { 'pack' : "foundation-{}" . format ( self . kwargs . get ( 'foundation_version' ) ) } ) return kwargs
def publish ( self ) : return self . _publish ( self . args , self . server , self . URI )
def get ( data ) : crc = 0 for byte in array ( 'B' , data ) : crc = ( VProCRC . CRC_TABLE [ ( crc >> 8 ) ^ byte ] ^ ( ( crc & 0xFF ) << 8 ) ) return crc
def _unpack_storm_date ( date ) : year = ( date & 0x7f ) + 2000 day = ( date >> 7 ) & 0x01f month = ( date >> 12 ) & 0x0f return "%s-%s-%s" % ( year , month , day )
def _use_rev_b_archive ( self , records , offset ) : if type ( self . _ARCHIVE_REV_B ) is bool : return self . _ARCHIVE_REV_B data = ArchiveBStruct . unpack_from ( records , offset ) if data [ 'RecType' ] == 0 : log . info ( 'detected archive rev. B' ) self . _ARCHIVE_REV_B = True else : log . info ( 'detected archive rev. A' ) self . _ARCHIVE_REV_B = False return self . _ARCHIVE_REV_B
def _wakeup ( self ) : log . info ( "send: WAKEUP" ) for i in xrange ( 3 ) : self . port . write ( '\n' ) ack = self . port . read ( len ( self . WAKE_ACK ) ) log_raw ( 'read' , ack ) if ack == self . WAKE_ACK : return raise NoDeviceException ( 'Can not access weather station' )
def _dmpaft_cmd ( self , time_fields ) : records = [ ] tbuf = struct . pack ( '2H' , * time_fields ) self . _cmd ( 'DMPAFT' ) crc = VProCRC . get ( tbuf ) crc = struct . pack ( '>H' , crc ) log_raw ( 'send' , tbuf + crc ) self . port . write ( tbuf + crc ) ack = self . port . read ( len ( self . ACK ) ) log_raw ( 'read' , ack ) if ack != self . ACK : return raw = self . port . read ( DmpStruct . size ) log_raw ( 'read' , raw ) if not VProCRC . verify ( raw ) : log_raw ( 'send ESC' , self . ESC ) self . port . write ( self . ESC ) return log_raw ( 'send ACK' , self . ACK ) self . port . write ( self . ACK ) dmp = DmpStruct . unpack ( raw ) log . info ( 'reading %d pages, start offset %d' % ( dmp [ 'Pages' ] , dmp [ 'Offset' ] ) ) for i in xrange ( dmp [ 'Pages' ] ) : raw = self . port . read ( DmpPageStruct . size ) log_raw ( 'read' , raw ) if not VProCRC . verify ( raw ) : log_raw ( 'send ESC' , self . ESC ) self . port . write ( self . ESC ) return log_raw ( 'send ACK' , self . ACK ) self . port . write ( self . ACK ) page = DmpPageStruct . unpack ( raw ) offset = 0 if i == 0 : offset = dmp [ 'Offset' ] * ArchiveAStruct . size while offset < ArchiveAStruct . size * 5 : log . info ( 'page %d, reading record at offset %d' % ( page [ 'Index' ] , offset ) ) if self . _use_rev_b_archive ( page [ 'Records' ] , offset ) : a = ArchiveBStruct . unpack_from ( page [ 'Records' ] , offset ) else : a = ArchiveAStruct . unpack_from ( page [ 'Records' ] , offset ) if a [ 'DateStamp' ] != 0xffff and a [ 'TimeStamp' ] != 0xffff : records . append ( a ) offset += ArchiveAStruct . size log . info ( 'read all pages' ) return records
def weather_update ( station , pub_sites , interval ) : station . parse ( ) if station . fields [ 'TempOut' ] > 200 : raise NoSensorException ( 'Out of range temperature value: %.1f, check sensors' % ( station . fields [ 'TempOut' ] , ) ) gust , gust_dir = WindGust . get ( station , interval ) for ps in pub_sites : try : ps . set ( pressure = station . fields [ 'Pressure' ] , dewpoint = station . fields [ 'DewPoint' ] , humidity = station . fields [ 'HumOut' ] , tempf = station . fields [ 'TempOut' ] , rainin = station . fields [ 'RainRate' ] , rainday = station . fields [ 'RainDay' ] , dateutc = station . fields [ 'DateStampUtc' ] , windspeed = station . fields [ 'WindSpeed10Min' ] , winddir = station . fields [ 'WindDir' ] , windgust = gust , windgustdir = gust_dir , ) ps . publish ( ) except ( Exception ) as e : log . warn ( 'publisher %s: %s' % ( ps . __class__ . __name__ , e ) )
def init_log ( quiet , debug ) : from logging . handlers import SysLogHandler fmt = logging . Formatter ( os . path . basename ( sys . argv [ 0 ] ) + ".%(name)s %(levelname)s - %(message)s" ) facility = SysLogHandler . LOG_DAEMON syslog = SysLogHandler ( address = '/dev/log' , facility = facility ) syslog . setFormatter ( fmt ) log . addHandler ( syslog ) if not quiet : console = logging . StreamHandler ( ) console . setFormatter ( fmt ) log . addHandler ( console ) log . setLevel ( logging . INFO ) if debug : log . setLevel ( logging . DEBUG )
def get_pub_services ( opts ) : sites = [ ] for p_key in vars ( opts ) . keys ( ) : args = getattr ( opts , p_key ) if p_key in PUB_SERVICES and args : if isinstance ( args , tuple ) : ps = PUB_SERVICES [ p_key ] ( * args ) else : ps = PUB_SERVICES [ p_key ] ( args ) sites . append ( ps ) return sites
def get_options ( parser ) : pub_g = optparse . OptionGroup ( parser , "Publication Services" , , ) pub_g . add_option ( '-w' , '--wundergound' , nargs = 2 , type = 'string' , dest = 'wug' , help = 'Weather Underground service; WUG=[SID(station ID), PASSWORD]' ) pub_g . add_option ( '-p' , '--pws' , nargs = 2 , type = 'string' , dest = 'pws' , help = 'PWS service; PWS=[SID(station ID), PASSWORD]' ) pub_g . add_option ( '-f' , '--file' , nargs = 1 , type = 'string' , dest = 'file' , help = 'Local file; FILE=[FILE_NAME]' ) parser . add_option_group ( pub_g ) parser . add_option ( '-d' , '--debug' , dest = 'debug' , action = "store_true" , default = False , help = 'enable verbose debug logging' ) parser . add_option ( '-q' , '--quiet' , dest = 'quiet' , action = "store_true" , default = False , help = 'disable all console logging' ) parser . add_option ( '-t' , '--tty' , dest = 'tty' , default = '/dev/ttyS0' , help = 'set serial port device [/dev/ttyS0]' ) parser . add_option ( '-n' , '--interval' , dest = 'interval' , default = 60 , type = 'int' , help = 'polling/update interval in seconds [60]' ) return parser . parse_args ( )
def set ( self , * * kw ) : self . args = kw log . debug ( self . args )
def publish ( self ) : with open ( self . file_name , 'w' ) as fh : for k , v in self . args . iteritems ( ) : buf = StringIO . StringIO ( ) buf . write ( k ) self . _append_vals ( buf , v ) fh . write ( buf . getvalue ( ) + '\n' ) buf . close ( )
def init_app ( self , app ) : if not hasattr ( app , "extensions" ) : app . extensions = { } app . extensions [ "allows" ] = self @ app . before_request def start_context ( * a , * * k ) : self . overrides . push ( Override ( ) ) self . additional . push ( Additional ( ) ) @ app . after_request def cleanup ( response ) : self . clear_all_overrides ( ) self . clear_all_additional ( ) return response
def unduplicate_field_names ( field_names ) : res = [ ] for k in field_names : if k in res : i = 1 while k + '_' + str ( i ) in res : i += 1 k += '_' + str ( i ) res . append ( k ) return res
def get_dataframe ( self ) : if pd is None : raise ImportError ( "Try installing Pandas first." ) frame = pd . DataFrame ( self [ : ] , columns = ( self and self . keys ) or [ ] ) return frame
def get_widgets_sorted ( self ) : result = [ ] for widget_name , widget in self . get_widgets ( ) . items ( ) : result . append ( ( widget_name , widget , widget . position ) ) result . sort ( key = lambda x : x [ 2 ] ) return result
def unregister_widget ( self , widget_cls ) : if widget_cls . __name__ in self . widgets : del self . widgets [ widget_cls ( ) . get_name ( ) ]
def get_last_update ( self ) : instance , created = models . DashboardWidgetLastUpdate . objects . get_or_create ( widget_name = self . get_name ( ) ) return instance
def save_setting ( self , setting_name , value ) : setting = self . get_setting ( setting_name ) if setting is None : setting = models . DashboardWidgetSettings . objects . create ( widget_name = self . get_name ( ) , setting_name = setting_name , value = value ) setting . value = value setting . save ( ) return setting
def _format_axes ( axes , shape ) : if isinstance ( axes , int ) : axes = ( axes , ) elif isinstance ( axes , list ) or hasattr ( axes , '__iter__' ) : axes = tuple ( axes ) if not isinstance ( axes , tuple ) : raise ValueError ( "axes argument %s in the constructor not specified correctly" % str ( axes ) ) if min ( axes ) < 0 or max ( axes ) > len ( shape ) - 1 : raise ValueError ( "invalid key axes %s given shape %s" % ( str ( axes ) , str ( shape ) ) ) return axes
def _wrap ( func , shape , context = None , axis = ( 0 , ) , dtype = None , npartitions = None ) : if isinstance ( shape , int ) : shape = ( shape , ) key_shape , value_shape = get_kv_shape ( shape , ConstructSpark . _format_axes ( axis , shape ) ) split = len ( key_shape ) rdd = context . parallelize ( list ( product ( * [ arange ( x ) for x in key_shape ] ) ) , npartitions ) rdd = rdd . map ( lambda x : ( x , func ( value_shape , dtype , order = 'C' ) ) ) return BoltArraySpark ( rdd , shape = shape , split = split , dtype = dtype )
def first ( self ) : from bolt . local . array import BoltArrayLocal rdd = self . _rdd if self . _ordered else self . _rdd . sortByKey ( ) return BoltArrayLocal ( rdd . values ( ) . first ( ) )
def zip_with_index ( rdd ) : starts = [ 0 ] if rdd . getNumPartitions ( ) > 1 : nums = rdd . mapPartitions ( lambda it : [ sum ( 1 for _ in it ) ] ) . collect ( ) count = sum ( nums ) for i in range ( len ( nums ) - 1 ) : starts . append ( starts [ - 1 ] + nums [ i ] ) else : count = rdd . count ( ) def func ( k , it ) : for i , v in enumerate ( it , starts [ k ] ) : yield v , i return count , rdd . mapPartitionsWithIndex ( func )
def wrapped ( f ) : import inspect def extract ( func ) : append = "" args = inspect . getargspec ( func ) for i , a in enumerate ( args . args ) : if i < ( len ( args ) - len ( args . defaults ) ) : append += str ( a ) + ", " else : default = args . defaults [ i - len ( args . defaults ) ] if hasattr ( default , "__name__" ) : default = default . __name__ else : default = str ( default ) append += str ( a ) + "=" + default + ", " append = append [ : - 2 ] + ")" return append doc = f . __doc__ + "\n" doc += "    local -> array(" + extract ( getattr ( ConstructLocal , f . __name__ ) ) + "\n" doc += "    spark -> array(" + extract ( getattr ( ConstructSpark , f . __name__ ) ) + "\n" f . __doc__ = doc return f
def plotcdf ( x , xmin , alpha ) : x = sort ( x ) n = len ( x ) xcdf = arange ( n , 0 , - 1 , dtype = 'float' ) / float ( n ) q = x [ x >= xmin ] fcdf = ( q / xmin ) ** ( 1 - alpha ) nc = xcdf [ argmax ( x >= xmin ) ] fcdf_norm = nc * fcdf loglog ( x , xcdf ) loglog ( q , fcdf_norm )
def plotpdf ( x , xmin , alpha , nbins = 30 , dolog = False ) : x = sort ( x ) n = len ( x ) if dolog : hb = hist ( x , bins = logspace ( log10 ( min ( x ) ) , log10 ( max ( x ) ) , nbins ) , log = True ) alpha += 1 else : hb = hist ( x , bins = linspace ( ( min ( x ) ) , ( max ( x ) ) , nbins ) ) h , b = hb [ 0 ] , hb [ 1 ] b = b [ 1 : ] q = x [ x >= xmin ] px = ( alpha - 1 ) / xmin * ( q / xmin ) ** ( - alpha ) arg = argmin ( abs ( b - xmin ) ) norm = mean ( h [ b > xmin ] / ( ( alpha - 1 ) / xmin * ( b [ b > xmin ] / xmin ) ** ( - alpha ) ) ) px = px * norm loglog ( q , px ) gca ( ) . set_xlim ( min ( x ) , max ( x ) )
def discrete_max_likelihood_arg ( data , xmin , alpharange = ( 1.5 , 3.5 ) , n_alpha = 201 ) : likelihoods = discrete_likelihood_vector ( data , xmin , alpharange = alpharange , n_alpha = n_alpha ) Largmax = np . argmax ( likelihoods ) return Largmax
def discrete_max_likelihood ( data , xmin , alpharange = ( 1.5 , 3.5 ) , n_alpha = 201 ) : likelihoods = discrete_likelihood_vector ( data , xmin , alpharange = alpharange , n_alpha = n_alpha ) Lmax = np . max ( likelihoods ) return Lmax
def most_likely_alpha ( data , xmin , alpharange = ( 1.5 , 3.5 ) , n_alpha = 201 ) : alpha_vector = np . linspace ( alpharange [ 0 ] , alpharange [ 1 ] , n_alpha ) return alpha_vector [ discrete_max_likelihood_arg ( data , xmin , alpharange = alpharange , n_alpha = n_alpha ) ]
def plotcdf ( self , x = None , xmin = None , alpha = None , pointcolor = 'k' , dolog = True , zoom = True , pointmarker = '+' , * * kwargs ) : if x is None : x = self . data if xmin is None : xmin = self . _xmin if alpha is None : alpha = self . _alpha x = np . sort ( x ) n = len ( x ) xcdf = np . arange ( n , 0 , - 1 , dtype = 'float' ) / float ( n ) q = x [ x >= xmin ] fcdf = ( q / xmin ) ** ( 1 - alpha ) nc = xcdf [ argmax ( x >= xmin ) ] fcdf_norm = nc * fcdf D_location = argmax ( xcdf [ x >= xmin ] - fcdf_norm ) pylab . vlines ( q [ D_location ] , xcdf [ x >= xmin ] [ D_location ] , fcdf_norm [ D_location ] , color = 'm' , linewidth = 2 , zorder = 2 ) pylab . plot ( [ q [ D_location ] ] * 2 , [ xcdf [ x >= xmin ] [ D_location ] , fcdf_norm [ D_location ] ] , color = 'm' , marker = 's' , zorder = 3 ) #plotx = pylab.linspace(q.min(),q.max(),1000) #ploty = (plotx/xmin)**(1-alpha) * nc if dolog : pylab . loglog ( x , xcdf , marker = pointmarker , color = pointcolor , * * kwargs ) pylab . loglog ( q , fcdf_norm , 'r' , * * kwargs ) else : pylab . semilogx ( x , xcdf , marker = pointmarker , color = pointcolor , * * kwargs ) pylab . semilogx ( q , fcdf_norm , 'r' , * * kwargs ) if zoom : pylab . axis ( [ xmin , x . max ( ) , xcdf . min ( ) , nc ] )
def plot_lognormal_pdf ( self , * * kwargs ) : if not hasattr ( self , 'lognormal_dist' ) : return normalized_pdf = self . lognormal_dist . pdf ( self . data ) / self . lognormal_dist . pdf ( self . data ) . max ( ) minY , maxY = pylab . gca ( ) . get_ylim ( ) pylab . plot ( self . data , normalized_pdf * maxY , '.' , * * kwargs )
def plot_lognormal_cdf ( self , * * kwargs ) : if not hasattr ( self , 'lognormal_dist' ) : return x = np . sort ( self . data ) n = len ( x ) xcdf = np . arange ( n , 0 , - 1 , dtype = 'float' ) / float ( n ) lcdf = self . lognormal_dist . sf ( x ) D_location = argmax ( xcdf - lcdf ) pylab . vlines ( x [ D_location ] , xcdf [ D_location ] , lcdf [ D_location ] , color = 'm' , linewidth = 2 ) pylab . plot ( x , lcdf , ',' , * * kwargs )
def hash_sha256 ( self ) : fp_plain = hashlib . sha256 ( self . _decoded_key ) . digest ( ) return ( b"SHA256:" + base64 . b64encode ( fp_plain ) . replace ( b"=" , b"" ) ) . decode ( "utf-8" )
def hash_sha512 ( self ) : fp_plain = hashlib . sha512 ( self . _decoded_key ) . digest ( ) return ( b"SHA512:" + base64 . b64encode ( fp_plain ) . replace ( b"=" , b"" ) ) . decode ( "utf-8" )
def _parse_long ( cls , data ) : if sys . version < '3' : ret = long ( 0 ) for byte in data : ret = ( ret << 8 ) + ord ( byte ) else : ret = 0 for byte in data : ret = ( ret << 8 ) + byte return ret
def decode_key ( cls , pubkey_content ) : try : decoded_key = base64 . b64decode ( pubkey_content . encode ( "ascii" ) ) except ( TypeError , binascii . Error ) : raise MalformedDataError ( "Unable to decode the key" ) return decoded_key
def parse_options ( self , options ) : quote_open = False parsed_options = { } def parse_add_single_option ( opt ) : """Parses and validates a single option, and adds it to parsed_options field.""" if "=" in opt : opt_name , opt_value = opt . split ( "=" , 1 ) opt_value = opt_value . replace ( '"' , '' ) else : opt_name = opt opt_value = True if " " in opt_name or not self . OPTION_NAME_RE . match ( opt_name ) : raise InvalidOptionNameError ( "%s is not valid option name." % opt_name ) if self . strict_mode : for valid_opt_name , value_required in self . OPTIONS_SPEC : if opt_name . lower ( ) == valid_opt_name : if value_required and opt_value is True : raise MissingMandatoryOptionValueError ( "%s is missing mandatory value." % opt_name ) break else : raise UnknownOptionNameError ( "%s is unrecognized option name." % opt_name ) if opt_name not in parsed_options : parsed_options [ opt_name ] = [ ] parsed_options [ opt_name ] . append ( opt_value ) start_of_current_opt = 0 i = 1 for i , character in enumerate ( options ) : if character == '"' : quote_open = not quote_open if quote_open : continue if character == "," : opt = options [ start_of_current_opt : i ] parse_add_single_option ( opt ) start_of_current_opt = i + 1 if start_of_current_opt + 1 != i : opt = options [ start_of_current_opt : ] parse_add_single_option ( opt ) if quote_open : raise InvalidOptionsError ( "Unbalanced quotes." ) return parsed_options
def _process_ssh_rsa ( self , data ) : current_position , raw_e = self . _unpack_by_int ( data , 0 ) current_position , raw_n = self . _unpack_by_int ( data , current_position ) unpacked_e = self . _parse_long ( raw_e ) unpacked_n = self . _parse_long ( raw_n ) self . rsa = RSAPublicNumbers ( unpacked_e , unpacked_n ) . public_key ( default_backend ( ) ) self . bits = self . rsa . key_size if self . strict_mode : min_length = self . RSA_MIN_LENGTH_STRICT max_length = self . RSA_MAX_LENGTH_STRICT else : min_length = self . RSA_MIN_LENGTH_LOOSE max_length = self . RSA_MAX_LENGTH_LOOSE if self . bits < min_length : raise TooShortKeyError ( "%s key data can not be shorter than %s bits (was %s)" % ( self . key_type , min_length , self . bits ) ) if self . bits > max_length : raise TooLongKeyError ( "%s key data can not be longer than %s bits (was %s)" % ( self . key_type , max_length , self . bits ) ) return current_position
def _process_ssh_dss ( self , data ) : data_fields = { } current_position = 0 for item in ( "p" , "q" , "g" , "y" ) : current_position , value = self . _unpack_by_int ( data , current_position ) data_fields [ item ] = self . _parse_long ( value ) q_bits = self . _bits_in_number ( data_fields [ "q" ] ) p_bits = self . _bits_in_number ( data_fields [ "p" ] ) if q_bits != self . DSA_N_LENGTH : raise InvalidKeyError ( "Incorrect DSA key parameters: bits(p)=%s, q=%s" % ( self . bits , q_bits ) ) if self . strict_mode : min_length = self . DSA_MIN_LENGTH_STRICT max_length = self . DSA_MAX_LENGTH_STRICT else : min_length = self . DSA_MIN_LENGTH_LOOSE max_length = self . DSA_MAX_LENGTH_LOOSE if p_bits < min_length : raise TooShortKeyError ( "%s key can not be shorter than %s bits (was %s)" % ( self . key_type , min_length , p_bits ) ) if p_bits > max_length : raise TooLongKeyError ( "%s key data can not be longer than %s bits (was %s)" % ( self . key_type , max_length , p_bits ) ) dsa_parameters = DSAParameterNumbers ( data_fields [ "p" ] , data_fields [ "q" ] , data_fields [ "g" ] ) self . dsa = DSAPublicNumbers ( data_fields [ "y" ] , dsa_parameters ) . public_key ( default_backend ( ) ) self . bits = self . dsa . key_size return current_position
def _process_ecdsa_sha ( self , data ) : current_position , curve_information = self . _unpack_by_int ( data , 0 ) if curve_information not in self . ECDSA_CURVE_DATA : raise NotImplementedError ( "Invalid curve type: %s" % curve_information ) curve , hash_algorithm = self . ECDSA_CURVE_DATA [ curve_information ] current_position , key_data = self . _unpack_by_int ( data , current_position ) try : ecdsa_key = ecdsa . VerifyingKey . from_string ( key_data [ 1 : ] , curve , hash_algorithm ) except AssertionError : raise InvalidKeyError ( "Invalid ecdsa key" ) self . bits = int ( curve_information . replace ( b"nistp" , b"" ) ) self . ecdsa = ecdsa_key return current_position
def main ( properties = properties , options = options , * * custom_options ) : return init ( * * dict ( options , * * custom_options ) ) ( * * properties )
def _create_file ( ) : f = wave . open ( 'audio.wav' , mode = 'wb' ) f . setnchannels ( 2 ) p = pyaudio . PyAudio ( ) f . setsampwidth ( p . get_sample_size ( pyaudio . paInt16 ) ) f . setframerate ( p . get_default_input_device_info ( ) [ 'defaultSampleRate' ] ) try : yield f finally : f . close ( )
def djfrontend_jquery_datatables_css ( version = None ) : if version is None : if not getattr ( settings , 'DJFRONTEND_JQUERY_DATATABLES_CSS' , False ) : version = getattr ( settings , 'DJFRONTEND_JQUERY_DATATABLES_VERSION' , DJFRONTEND_JQUERY_DATATABLES_VERSION_DEFAULT ) else : version = getattr ( settings , 'DJFRONTEND_JQUERY_DATATABLES_CSS' , DJFRONTEND_JQUERY_DATATABLES_VERSION_DEFAULT ) return format_html ( '<link rel="stylesheet" href="{static}djfrontend/css/jquery/jquery.dataTables/{v}/jquery.dataTables{min}.css">' , static = _static_url , v = version , min = _min )
def djfrontend_jquery_datatables_themeroller ( version = None ) : if version is None : if not getattr ( settings , 'DJFRONTEND_JQUERY_DATATABLES_THEMEROLLER' , False ) : version = getattr ( settings , 'DJFRONTEND_JQUERY_DATATABLES_VERSION' , DJFRONTEND_JQUERY_DATATABLES_VERSION_DEFAULT ) else : version = getattr ( settings , 'DJFRONTEND_JQUERY_DATATABLES_THEMEROLLER' , DJFRONTEND_JQUERY_DATATABLES_VERSION_DEFAULT ) return format_html ( '<link rel="stylesheet" href="href="{static}djfrontend/css/jquery/jquery.dataTables/{v}/jquery.dataTables_themeroller.min.css">' , static = _static_url , v = version )
def calc_expiry_time ( minutes_valid ) : return ( timezone . now ( ) + datetime . timedelta ( minutes = minutes_valid + 1 ) ) . replace ( second = 0 , microsecond = 0 )
def get_user_token ( user , purpose , minutes_valid ) : token = '' . join ( dumps ( [ user . get_username ( ) , get_auth_hash ( user , purpose ) , ] ) . encode ( 'base64' ) . split ( '\n' ) ) return { 'id' : get_meteor_id ( user ) , 'token' : token , 'tokenExpires' : calc_expiry_time ( minutes_valid ) , }
def serialize ( self , obj , * args , * * kwargs ) : data = super ( Users , self ) . serialize ( obj , * args , * * kwargs ) profile = data . pop ( 'fields' ) profile . setdefault ( 'name' , obj . get_full_name ( ) ) fields = data [ 'fields' ] = { 'username' : obj . get_username ( ) , 'emails' : [ ] , 'profile' : profile , 'permissions' : sorted ( self . model . get_all_permissions ( obj ) ) , } for sensitive in [ 'password' , 'user_permissions_ids' , 'is_active' , 'is_staff' , 'is_superuser' , 'groups_ids' , ] : profile . pop ( sensitive , None ) try : fields [ 'createdAt' ] = profile . pop ( 'date_joined' ) except KeyError : date_joined = getattr ( obj , 'get_date_joined' , lambda : getattr ( obj , 'date_joined' , None ) ) ( ) if date_joined : fields [ 'createdAt' ] = date_joined try : email = profile . pop ( 'email' ) except KeyError : email = getattr ( obj , 'get_email' , lambda : getattr ( obj , 'email' , None ) ) ( ) if email : fields [ 'emails' ] . append ( { 'address' : email , 'verified' : True } ) return data
def deserialize_profile ( profile , key_prefix = '' , pop = False ) : result = { } if pop : getter = profile . pop else : getter = profile . get def prefixed ( name ) : """Return name prefixed by `key_prefix`.""" return '%s%s' % ( key_prefix , name ) for key in profile . keys ( ) : val = getter ( key ) if key == prefixed ( 'name' ) : result [ 'full_name' ] = val else : raise MeteorError ( 400 , 'Bad profile key: %r' % key ) return result
def update ( self , selector , update , options = None ) : del options user = get_object ( self . model , selector [ '_id' ] , pk = this . user_id , ) profile_update = self . deserialize_profile ( update [ '$set' ] , key_prefix = 'profile.' , pop = True , ) if len ( update [ '$set' ] ) != 0 : raise MeteorError ( 400 , 'Invalid update fields: %r' ) for key , val in profile_update . items ( ) : setattr ( user , key , val ) user . save ( )
def auth_failed ( * * credentials ) : if credentials : user_login_failed . send_robust ( sender = __name__ , credentials = auth . _clean_credentials ( credentials ) , ) raise MeteorError ( 403 , 'Authentication failed.' )
def validated_user ( cls , token , purpose , minutes_valid ) : try : username , auth_hash = loads ( token . decode ( 'base64' ) ) except ( ValueError , Error ) : cls . auth_failed ( token = token ) try : user = cls . user_model . objects . get ( * * { cls . user_model . USERNAME_FIELD : username , 'is_active' : True , } ) user . backend = 'django.contrib.auth.backends.ModelBackend' except cls . user_model . DoesNotExist : cls . auth_failed ( username = username , token = token ) if auth_hash not in iter_auth_hashes ( user , purpose , minutes_valid ) : cls . auth_failed ( username = username , token = token ) return user
def check_secure ( ) : if this . request . is_secure ( ) : return True elif this . request . META [ 'REMOTE_ADDR' ] in [ 'localhost' , '127.0.0.1' , ] : return True raise MeteorError ( 403 , 'Authentication refused without SSL.' )
def get_username ( self , user ) : if isinstance ( user , basestring ) : return user elif isinstance ( user , dict ) and len ( user ) == 1 : [ ( key , val ) ] = user . items ( ) if key == 'username' or ( key == self . user_model . USERNAME_FIELD ) : return val elif key in ( 'email' , 'emails.address' ) : email_field = getattr ( self . user_model , 'EMAIL_FIELD' , 'email' ) if self . user_model . USERNAME_FIELD == email_field : return val return self . user_model . objects . values_list ( self . user_model . USERNAME_FIELD , flat = True , ) . get ( * * { email_field : val } ) elif key in ( 'id' , 'pk' ) : return self . user_model . objects . values_list ( self . user_model . USERNAME_FIELD , flat = True , ) . get ( pk = val , ) else : raise MeteorError ( 400 , 'Invalid user lookup: %r' % key ) else : raise MeteorError ( 400 , 'Invalid user expression: %r' % user )
def create_user ( self , params ) : receivers = create_user . send ( sender = __name__ , request = this . request , params = params , ) if len ( receivers ) == 0 : raise NotImplementedError ( 'Handler for `create_user` not registered.' ) user = receivers [ 0 ] [ 1 ] user = auth . authenticate ( username = user . get_username ( ) , password = params [ 'password' ] , ) self . do_login ( user ) return get_user_token ( user = user , purpose = HashPurpose . RESUME_LOGIN , minutes_valid = HASH_MINUTES_VALID [ HashPurpose . RESUME_LOGIN ] , )
def do_login ( self , user ) : this . user_id = user . pk this . user_ddp_id = get_meteor_id ( user ) this . user_sub_id = meteor_random_id ( ) API . do_sub ( this . user_sub_id , 'LoggedInUser' , silent = True ) self . update_subs ( user . pk ) user_logged_in . send ( sender = user . __class__ , request = this . request , user = user , )
def do_logout ( self ) : API . do_unsub ( this . user_sub_id , silent = True ) del this . user_sub_id self . update_subs ( None ) user_logged_out . send ( sender = self . user_model , request = this . request , user = this . user , ) this . user_id = None this . user_ddp_id = None
def login ( self , params ) : if 'password' in params : return self . login_with_password ( params ) elif 'resume' in params : return self . login_with_resume_token ( params ) else : self . auth_failed ( * * params )
def login_with_password ( self , params ) : self . check_secure ( ) username = self . get_username ( params [ 'user' ] ) password = self . get_password ( params [ 'password' ] ) user = auth . authenticate ( username = username , password = password ) if user is not None : if user . is_active : self . do_login ( user ) return get_user_token ( user = user , purpose = HashPurpose . RESUME_LOGIN , minutes_valid = HASH_MINUTES_VALID [ HashPurpose . RESUME_LOGIN ] , ) self . auth_failed ( )
def forgot_password ( self , params ) : username = self . get_username ( params ) try : user = self . user_model . objects . get ( * * { self . user_model . USERNAME_FIELD : username , } ) except self . user_model . DoesNotExist : self . auth_failed ( ) minutes_valid = HASH_MINUTES_VALID [ HashPurpose . PASSWORD_RESET ] token = get_user_token ( user = user , purpose = HashPurpose . PASSWORD_RESET , minutes_valid = minutes_valid , ) forgot_password . send ( sender = __name__ , user = user , token = token , request = this . request , expiry_date = calc_expiry_time ( minutes_valid ) , )
def reset_password ( self , token , new_password ) : user = self . validated_user ( token , purpose = HashPurpose . PASSWORD_RESET , minutes_valid = HASH_MINUTES_VALID [ HashPurpose . PASSWORD_RESET ] , ) user . set_password ( new_password ) user . save ( ) self . do_login ( user ) return { "userId" : this . user_ddp_id }
def read ( path , default = None , encoding = 'utf8' ) : if not path : return default try : with io . open ( path , mode = 'r' , encoding = encoding ) as contents : return contents . read ( ) except IOError : if default is not None : return default raise
def get_meteor_id ( obj_or_model , obj_pk = None ) : if obj_or_model is None : return None meta = obj_or_model . _meta model = meta . model if model is ObjectMapping : raise TypeError ( "Can't map ObjectMapping instances through self." ) if isinstance ( obj_or_model , model ) : if isinstance ( meta . pk , AleaIdField ) : return obj_or_model . pk if obj_pk is None : obj_pk = str ( obj_or_model . pk ) alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique ] if len ( alea_unique_fields ) == 1 : aid = alea_unique_fields [ 0 ] . attname if isinstance ( obj_or_model , model ) : val = getattr ( obj_or_model , aid ) elif obj_pk is None : val = None else : val = model . objects . values_list ( aid , flat = True ) . get ( pk = obj_pk , ) if val : return val if obj_pk is None : return None content_type = ContentType . objects . get_for_model ( model ) try : return ObjectMapping . objects . values_list ( 'meteor_id' , flat = True , ) . get ( content_type = content_type , object_id = obj_pk , ) except ObjectDoesNotExist : return ObjectMapping . objects . create ( content_type = content_type , object_id = obj_pk , meteor_id = meteor_random_id ( '/collection/%s' % meta ) , ) . meteor_id
def get_meteor_ids ( model , object_ids ) : meta = model . _meta result = collections . OrderedDict ( ( str ( obj_pk ) , None ) for obj_pk in object_ids ) if isinstance ( meta . pk , AleaIdField ) : return collections . OrderedDict ( ( obj_pk , obj_pk ) for obj_pk in object_ids ) alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique and not field . null ] if len ( alea_unique_fields ) == 1 : aid = alea_unique_fields [ 0 ] . name query = model . objects . filter ( pk__in = object_ids , ) . values_list ( 'pk' , aid ) else : content_type = ContentType . objects . get_for_model ( model ) query = ObjectMapping . objects . filter ( content_type = content_type , object_id__in = list ( result ) ) . values_list ( 'object_id' , 'meteor_id' ) for obj_pk , meteor_id in query : result [ str ( obj_pk ) ] = meteor_id for obj_pk , meteor_id in result . items ( ) : if meteor_id is None : result [ obj_pk ] = get_meteor_id ( model , obj_pk ) return result
def get_object_id ( model , meteor_id ) : if meteor_id is None : return None meta = model . _meta if model is ObjectMapping : raise TypeError ( "Can't map ObjectMapping instances through self." ) if isinstance ( meta . pk , AleaIdField ) : return meteor_id alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique ] if len ( alea_unique_fields ) == 1 : val = model . objects . values_list ( 'pk' , flat = True , ) . get ( * * { alea_unique_fields [ 0 ] . attname : meteor_id , } ) if val : return val content_type = ContentType . objects . get_for_model ( model ) return ObjectMapping . objects . filter ( content_type = content_type , meteor_id = meteor_id , ) . values_list ( 'object_id' , flat = True ) . get ( )
def get_object_ids ( model , meteor_ids ) : if model is ObjectMapping : raise TypeError ( "Can't map ObjectMapping instances through self." ) meta = model . _meta alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique and not field . null ] result = collections . OrderedDict ( ( str ( meteor_id ) , None ) for meteor_id in meteor_ids ) if len ( alea_unique_fields ) == 1 : aid = alea_unique_fields [ 0 ] . name query = model . objects . filter ( * * { '%s__in' % aid : meteor_ids , } ) . values_list ( aid , 'pk' ) else : content_type = ContentType . objects . get_for_model ( model ) query = ObjectMapping . objects . filter ( content_type = content_type , meteor_id__in = meteor_ids , ) . values_list ( 'meteor_id' , 'object_id' ) for meteor_id , object_id in query : result [ meteor_id ] = object_id return result
def get_object ( model , meteor_id , * args , * * kwargs ) : meta = model . _meta if isinstance ( meta . pk , AleaIdField ) : return model . objects . filter ( * args , * * kwargs ) . get ( pk = meteor_id ) alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique and not field . null ] if len ( alea_unique_fields ) == 1 : return model . objects . filter ( * args , * * kwargs ) . get ( * * { alea_unique_fields [ 0 ] . name : meteor_id , } ) return model . objects . filter ( * args , * * kwargs ) . get ( pk = get_object_id ( model , meteor_id ) , )
def get_pk_value_on_save ( self , instance ) : value = super ( AleaIdField , self ) . get_pk_value_on_save ( instance ) if not value : value = self . get_seeded_value ( instance ) return value
def pre_save ( self , model_instance , add ) : value = super ( AleaIdField , self ) . pre_save ( model_instance , add ) if ( not value ) and self . default in ( meteor_random_id , NOT_PROVIDED ) : value = self . get_seeded_value ( model_instance ) setattr ( model_instance , self . attname , value ) return value
def set_default_forwards ( app_name , operation , apps , schema_editor ) : model = apps . get_model ( app_name , operation . model_name ) for obj_pk in model . objects . values_list ( 'pk' , flat = True ) : model . objects . filter ( pk = obj_pk ) . update ( * * { operation . name : get_meteor_id ( model , obj_pk ) , } )
def set_default_reverse ( app_name , operation , apps , schema_editor ) : model = apps . get_model ( app_name , operation . model_name ) for obj_pk in model . objects . values_list ( 'pk' , flat = True ) : get_meteor_id ( model , obj_pk )
def database_forwards ( self , app_label , schema_editor , from_state , to_state ) : self . truncate ( app_label , schema_editor , self . truncate_forwards )
def database_backwards ( self , app_label , schema_editor , from_state , to_state ) : self . truncate ( app_label , schema_editor , self . truncate_backwards )
def initialize_options ( self ) : setuptools . command . build_py . build_py . initialize_options ( self ) self . meteor = 'meteor' self . meteor_debug = False self . build_lib = None self . package_dir = None self . meteor_builds = [ ] self . no_prune_npm = None self . inplace = True
def finalize_options ( self ) : self . set_undefined_options ( 'build' , ( 'build_lib' , 'build_lib' ) , ) self . set_undefined_options ( 'build_py' , ( 'package_dir' , 'package_dir' ) , ) setuptools . command . build_py . build_py . finalize_options ( self )
def path_to_dir ( * path_args ) : return os . path . join ( * list ( path_args [ : - 1 ] ) + path_args [ - 1 ] . split ( posixpath . sep ) )
def seed ( self , values ) : if not values : seed_ids = [ int , str , random , self , values , self . __class__ ] random . shuffle ( seed_ids ) values = list ( map ( id , seed_ids ) ) + [ time . time ( ) , os . urandom ( 512 ) ] mash = Mash ( ) self . c = 1 self . s0 = mash ( ' ' ) self . s1 = mash ( ' ' ) self . s2 = mash ( ' ' ) for val in values : self . s0 -= mash ( val ) if self . s0 < 0 : self . s0 += 1 self . s1 -= mash ( val ) if self . s1 < 0 : self . s1 += 1 self . s2 -= mash ( val ) if self . s2 < 0 : self . s2 += 1
def state ( self ) : return { 'c' : self . c , 's0' : self . s0 , 's1' : self . s1 , 's2' : self . s2 }
def random_string ( self , length , alphabet ) : return '' . join ( self . choice ( alphabet ) for n in range ( length ) )
def api_endpoints ( obj ) : for name in dir ( obj ) : attr = getattr ( obj , name ) api_path = getattr ( attr , 'api_path' , None ) if api_path : yield ( '%s%s' % ( obj . api_path_prefix , api_path ) , attr , ) for api_provider in obj . api_providers : for api_path , attr in api_endpoints ( api_provider ) : yield ( api_path , attr )
def clear_api_path_map_cache ( self ) : self . _api_path_cache = None for api_provider in self . api_providers : if six . get_method_self ( api_provider . clear_api_path_map_cache , ) is not None : api_provider . clear_api_path_map_cache ( )
def dprint ( name , val ) : from pprint import pformat print ( '% 5s: %s' % ( name , '\n       ' . join ( pformat ( val , indent = 4 , width = 75 , ) . split ( '\n' ) ) , ) , )
def validate_kwargs ( func , kwargs ) : func_name = func . __name__ argspec = inspect . getargspec ( func ) all_args = argspec . args [ : ] defaults = list ( argspec . defaults or [ ] ) if inspect . ismethod ( func ) and all_args [ : 1 ] == [ 'self' ] : all_args [ : 1 ] = [ ] if defaults : required = all_args [ : - len ( defaults ) ] else : required = all_args [ : ] trans = { arg : arg . endswith ( '_' ) and arg [ : - 1 ] or arg for arg in all_args } for key in list ( kwargs ) : key_adj = '%s_' % key if key_adj in all_args : kwargs [ key_adj ] = kwargs . pop ( key ) supplied = sorted ( kwargs ) missing = [ trans . get ( arg , arg ) for arg in required if arg not in supplied ] if missing : raise MeteorError ( 400 , func . err , 'Missing required arguments to %s: %s' % ( func_name , ' ' . join ( missing ) , ) , ) extra = [ arg for arg in supplied if arg not in all_args ] if extra : raise MeteorError ( 400 , func . err , 'Unknown arguments to %s: %s' % ( func_name , ' ' . join ( extra ) ) , )
def on_open ( self ) : this . request = WSGIRequest ( self . ws . environ ) this . ws = self this . send = self . send this . reply = self . reply self . logger = self . ws . logger self . remote_ids = collections . defaultdict ( set ) self . _tx_buffer = { } self . _tx_buffer_id_gen = itertools . cycle ( irange ( sys . maxint ) ) self . _tx_next_id_gen = itertools . cycle ( irange ( sys . maxint ) ) self . _tx_next_id = next ( self . _tx_next_id_gen ) this . remote_addr = self . remote_addr = '{0[REMOTE_ADDR]}:{0[REMOTE_PORT]}' . format ( self . ws . environ , ) this . subs = { } safe_call ( self . logger . info , '+ %s OPEN' , self ) self . send ( 'o' ) self . send ( 'a["{\\"server_id\\":\\"0\\"}"]' )
def on_close ( self , * args , * * kwargs ) : if self . connection is not None : del self . pgworker . connections [ self . connection . pk ] self . connection . delete ( ) self . connection = None signals . request_finished . send ( sender = self . __class__ ) safe_call ( self . logger . info , '- %s %s' , self , args or 'CLOSE' )
def on_message ( self , message ) : if self . ws . closed : return None try : safe_call ( self . logger . debug , '< %s %r' , self , message ) for data in self . ddp_frames_from_message ( message ) : self . process_ddp ( data ) signals . request_finished . send ( sender = self . __class__ ) except geventwebsocket . WebSocketError : self . ws . close ( )
def ddp_frames_from_message ( self , message ) : try : msgs = ejson . loads ( message ) except ValueError : self . reply ( 'error' , error = 400 , reason = 'Data is not valid EJSON' , ) raise StopIteration if not isinstance ( msgs , list ) : self . reply ( 'error' , error = 400 , reason = 'Invalid EJSON messages' , ) raise StopIteration while msgs : raw = msgs . pop ( 0 ) try : data = ejson . loads ( raw ) except ( TypeError , ValueError ) : data = None if not isinstance ( data , dict ) : self . reply ( 'error' , error = 400 , reason = 'Invalid SockJS DDP payload' , offendingMessage = raw , ) yield data if msgs : gevent . sleep ( )
def process_ddp ( self , data ) : msg_id = data . get ( 'id' , None ) try : msg = data . pop ( 'msg' ) except KeyError : self . reply ( 'error' , reason = 'Bad request' , offendingMessage = data , ) return try : self . dispatch ( msg , data ) except Exception as err : kwargs = { 'msg' : { 'method' : 'result' } . get ( msg , 'error' ) , } if msg_id is not None : kwargs [ 'id' ] = msg_id if isinstance ( err , MeteorError ) : error = err . as_dict ( ) else : error = { 'error' : 500 , 'reason' : 'Internal server error' , } if kwargs [ 'msg' ] == 'error' : kwargs . update ( error ) else : kwargs [ 'error' ] = error if not isinstance ( err , MeteorError ) : stack , _ = safe_call ( self . logger . error , '%r %r' , msg , data , exc_info = 1 , ) if stack is not None : traceback . print_exc ( file = sys . stderr ) sys . stderr . write ( 'Additionally, while handling the above error the ' 'following error was encountered:\n' ) sys . stderr . write ( stack ) elif settings . DEBUG : print ( 'ERROR: %s' % err ) dprint ( 'msg' , msg ) dprint ( 'data' , data ) error . setdefault ( 'details' , traceback . format_exc ( ) ) print ( error [ 'details' ] ) self . reply ( * * kwargs ) if msg_id and msg == 'method' : self . reply ( 'updated' , methods = [ msg_id ] )
def dispatch ( self , msg , kwargs ) : if self . connection is None and msg != 'connect' : self . reply ( 'error' , reason = 'Must connect first' ) return if msg == 'method' : if ( 'method' not in kwargs ) or ( 'id' not in kwargs ) : self . reply ( 'error' , error = 400 , reason = 'Malformed method invocation' , ) return try : handler = getattr ( self , 'recv_%s' % msg ) except ( AttributeError , UnicodeEncodeError ) : raise MeteorError ( 404 , 'Method not found' ) validate_kwargs ( handler , kwargs ) handler ( * * kwargs )
def recv_connect ( self , version = None , support = None , session = None ) : del session if self . connection is not None : raise MeteorError ( 400 , 'Session already established.' , self . connection . connection_id , ) elif None in ( version , support ) or version not in self . versions : self . reply ( 'failed' , version = self . versions [ 0 ] ) elif version not in support : raise MeteorError ( 400 , 'Client version/support mismatch.' ) else : from dddp . models import Connection cur = connection . cursor ( ) cur . execute ( 'SELECT pg_backend_pid()' ) ( backend_pid , ) = cur . fetchone ( ) this . version = version this . support = support self . connection = Connection . objects . create ( server_addr = '%d:%s' % ( backend_pid , self . ws . handler . socket . getsockname ( ) , ) , remote_addr = self . remote_addr , version = version , ) self . pgworker . connections [ self . connection . pk ] = self atexit . register ( self . on_close , 'Shutting down.' ) self . reply ( 'connected' , session = self . connection . connection_id )
def recv_ping ( self , id_ = None ) : if id_ is None : self . reply ( 'pong' ) else : self . reply ( 'pong' , id = id_ )
def recv_sub ( self , id_ , name , params ) : self . api . sub ( id_ , name , * params )
def recv_unsub ( self , id_ = None ) : if id_ : self . api . unsub ( id_ ) else : self . reply ( 'nosub' )
def recv_method ( self , method , params , id_ , randomSeed = None ) : if randomSeed is not None : this . random_streams . random_seed = randomSeed this . alea_random = alea . Alea ( randomSeed ) self . api . method ( method , params , id_ ) self . reply ( 'updated' , methods = [ id_ ] )
def ddpp_sockjs_info ( environ , start_response ) : import random import ejson start_response ( '200 OK' , [ ( 'Content-Type' , 'application/json; charset=UTF-8' ) , ] + common_headers ( environ ) , ) yield ejson . dumps ( collections . OrderedDict ( [ ( 'websocket' , True ) , ( 'origins' , [ '*:*' , ] ) , ( 'cookie_needed' , False ) , ( 'entropy' , random . getrandbits ( 32 ) ) , ] ) )
def serve ( listen , verbosity = 1 , debug_port = 0 , * * ssl_args ) : launcher = DDPLauncher ( debug = verbosity == 3 , verbosity = verbosity ) if debug_port : launcher . servers . append ( launcher . get_backdoor_server ( 'localhost:%d' % debug_port ) ) launcher . add_web_servers ( listen , * * ssl_args ) sigmap = { val : name for name , val in vars ( signal ) . items ( ) if name . startswith ( 'SIG' ) } def sighandler ( signum = None , frame = None ) : """Signal handler""" launcher . logger . info ( 'Received signal %s in frame %r' , sigmap . get ( signum , signum ) , frame , ) launcher . stop ( ) for signum in [ signal . SIGINT , signal . SIGQUIT ] : gevent . signal ( signum , sighandler ) launcher . run ( )
def main ( ) : parser = argparse . ArgumentParser ( description = __doc__ ) django = parser . add_argument_group ( 'Django Options' ) django . add_argument ( '--verbosity' , '-v' , metavar = 'VERBOSITY' , dest = 'verbosity' , type = int , default = 1 , ) django . add_argument ( '--debug-port' , metavar = 'DEBUG_PORT' , dest = 'debug_port' , type = int , default = 0 , ) django . add_argument ( '--settings' , metavar = 'SETTINGS' , dest = 'settings' , help = "The Python path to a settings module, e.g. " "\"myproject.settings.main\". If this isn't provided, the " "DJANGO_SETTINGS_MODULE environment variable will be used." , ) http = parser . add_argument_group ( 'HTTP Options' ) http . add_argument ( 'listen' , metavar = 'address[:port]' , nargs = '*' , type = addr , help = 'Listening address for HTTP(s) server.' , ) ssl = parser . add_argument_group ( 'SSL Options' ) ssl . add_argument ( '--ssl-version' , metavar = 'SSL_VERSION' , dest = 'ssl_version' , help = "SSL version to use (see stdlib ssl module's) [3]" , choices = [ '1' , '2' , '3' ] , default = '3' ) ssl . add_argument ( '--certfile' , metavar = 'FILE' , dest = 'certfile' , help = "SSL certificate file [None]" ) ssl . add_argument ( '--ciphers' , metavar = 'CIPHERS' , dest = 'ciphers' , help = "Ciphers to use (see stdlib ssl module's) [TLSv1]" ) ssl . add_argument ( '--ca-certs' , metavar = 'FILE' , dest = 'ca_certs' , help = "CA certificates file [None]" ) ssl . add_argument ( '--keyfile' , metavar = 'FILE' , dest = 'keyfile' , help = "SSL key file [None]" ) namespace = parser . parse_args ( ) if namespace . settings : os . environ [ 'DJANGO_SETTINGS_MODULE' ] = namespace . settings serve ( namespace . listen or [ Addr ( 'localhost' , 8000 ) ] , debug_port = namespace . debug_port , keyfile = namespace . keyfile , certfile = namespace . certfile , verbosity = namespace . verbosity , )
def print ( self , msg , * args , * * kwargs ) : if self . verbosity >= 1 : print ( msg , * args , * * kwargs )
def stop ( self ) : self . logger . debug ( 'PostgresGreenlet stop' ) self . _stop_event . set ( ) for server in self . servers + [ DDPLauncher . pgworker ] : self . logger . debug ( 'Stopping %s' , server ) server . stop ( ) gevent . joinall ( self . threads + [ DDPLauncher . pgworker ] ) self . threads = [ ]
def run ( self ) : self . logger . debug ( 'PostgresGreenlet run' ) self . start ( ) self . _stop_event . wait ( ) gevent . joinall ( self . threads + [ DDPLauncher . pgworker ] ) self . threads = [ ]
def _run ( self ) : conn_params = self . connection . get_connection_params ( ) conn_params . update ( async = True , application_name = '{} pid={} django-ddp' . format ( socket . gethostname ( ) , os . getpid ( ) , ) [ : 64 ] , ) conn = None while conn is None : try : conn = psycopg2 . connect ( * * conn_params ) except psycopg2 . OperationalError as err : msg = ( '%s' % err ) . strip ( ) msg_prefix = 'invalid connection option "' if not msg . startswith ( msg_prefix ) : raise key = msg [ len ( msg_prefix ) : - 1 ] self . logger . warning ( 'Ignoring unknown settings.DATABASES[%r] option: %s=%r' , self . connection . alias , key , conn_params . pop ( key ) , ) self . poll ( conn ) import logging logging . getLogger ( 'dddp' ) . info ( '=> Started PostgresGreenlet.' ) cur = conn . cursor ( ) cur . execute ( 'LISTEN "ddp";' ) while not self . _stop_event . is_set ( ) : try : self . select_greenlet = gevent . spawn ( gevent . select . select , [ conn ] , [ ] , [ ] , timeout = None , ) self . select_greenlet . get ( ) except gevent . GreenletExit : self . _stop_event . set ( ) finally : self . select_greenlet = None self . poll ( conn ) self . poll ( conn ) cur . close ( ) self . poll ( conn ) conn . close ( )
def poll ( self , conn ) : while 1 : state = conn . poll ( ) if state == psycopg2 . extensions . POLL_OK : while conn . notifies : notify = conn . notifies . pop ( ) self . logger . info ( "Got NOTIFY (pid=%d, payload=%r)" , notify . pid , notify . payload , ) hdr , chunk = notify . payload . split ( '|' , 1 ) header = ejson . loads ( hdr ) uuid = header [ 'uuid' ] size , chunks = self . chunks . setdefault ( uuid , [ 0 , { } ] ) if header [ 'fin' ] : size = self . chunks [ uuid ] [ 0 ] = header [ 'seq' ] chunks [ header [ 'seq' ] ] = chunk if len ( chunks ) != size : continue data = '' . join ( chunk for _ , chunk in sorted ( chunks . items ( ) ) ) del self . chunks [ uuid ] data = ejson . loads ( data ) sender = data . pop ( '_sender' , None ) tx_id = data . pop ( '_tx_id' , None ) for connection_id in data . pop ( '_connection_ids' ) : try : websocket = self . connections [ connection_id ] except KeyError : continue if connection_id == sender : websocket . send ( data , tx_id = tx_id ) else : websocket . send ( data ) break elif state == psycopg2 . extensions . POLL_WRITE : gevent . select . select ( [ ] , [ conn . fileno ( ) ] , [ ] ) elif state == psycopg2 . extensions . POLL_READ : gevent . select . select ( [ conn . fileno ( ) ] , [ ] , [ ] ) else : self . logger . warn ( 'POLL_ERR: %s' , state )
def greenify ( ) : if _GREEN : return _GREEN [ True ] = True from gevent . monkey import patch_all , saved if ( 'threading' in sys . modules ) and ( 'threading' not in saved ) : import warnings warnings . warn ( 'threading module loaded before patching!' ) patch_all ( ) try : import psycopg2 del psycopg2 except ImportError : from psycopg2cffi import compat compat . register ( ) from psycogreen . gevent import patch_psycopg patch_psycopg ( )
def meteor_random_id ( name = None , length = 17 ) : if name is None : stream = THREAD_LOCAL . alea_random else : stream = THREAD_LOCAL . random_streams [ name ] return stream . random_string ( length , METEOR_ID_CHARS )
def autodiscover ( ) : from django . utils . module_loading import autodiscover_modules from dddp . api import API autodiscover_modules ( 'ddp' , register_to = API ) return API
def as_dict ( self , * * kwargs ) : error , reason , details , err_kwargs = self . args result = { key : val for key , val in { 'error' : error , 'reason' : reason , 'details' : details , } . items ( ) if val is not None } result . update ( err_kwargs ) result . update ( kwargs ) return result
def get ( self , name , factory , * factory_args , * * factory_kwargs ) : update_thread_local = getattr ( factory , 'update_thread_local' , True ) if ( not update_thread_local ) or ( name not in self . __dict__ ) : obj = factory ( * factory_args , * * factory_kwargs ) if update_thread_local : setattr ( self , name , obj ) return obj return getattr ( self , name )
def emit ( self , record ) : if getattr ( this , 'subs' , { } ) . get ( LOGS_NAME , False ) : self . format ( record ) this . send ( { 'msg' : ADDED , 'collection' : LOGS_NAME , 'id' : meteor_random_id ( '/collection/%s' % LOGS_NAME ) , 'fields' : { attr : { 'args' : lambda args : [ repr ( arg ) for arg in args ] , 'created' : datetime . datetime . fromtimestamp , 'exc_info' : stacklines_or_none , } . get ( attr , lambda val : val ) ( getattr ( record , attr , None ) ) for attr in ( 'args' , 'asctime' , 'created' , 'exc_info' , 'filename' , 'funcName' , 'levelname' , 'levelno' , 'lineno' , 'module' , 'msecs' , 'message' , 'name' , 'pathname' , 'process' , 'processName' , 'relativeCreated' , 'thread' , 'threadName' , ) } , } )
def send_message ( self , message , * * kwargs ) : from . . libs . gcm import gcm_send_message data = kwargs . pop ( "extra" , { } ) if message is not None : data [ "message" ] = message return gcm_send_message ( registration_id = self . registration_id , data = data , * * kwargs )
def gcm_send_message ( registration_id , data , encoding = 'utf-8' , * * kwargs ) : messenger = GCMMessenger ( registration_id , data , encoding = encoding , * * kwargs ) return messenger . send_plain ( )
def gcm_send_bulk_message ( registration_ids , data , encoding = 'utf-8' , * * kwargs ) : messenger = GCMMessenger ( registration_ids , data , encoding = encoding , * * kwargs ) return messenger . send_bulk ( )
def send_json ( self , ids = None ) : items = ids or self . _registration_id values = { "registration_ids" : items } if self . _data is not None : values [ "data" ] = self . _data for key , val in self . _kwargs . items ( ) : if val : values [ key ] = val data = json . dumps ( values , separators = ( "," , ":" ) , sort_keys = True ) . encode ( self . encoding ) result = json . loads ( self . _send ( data , "application/json" ) ) if ( "failure" in result ) and ( result [ "failure" ] ) : unregistered = [ ] throw_error = False for index , error in enumerate ( result . get ( "results" , [ ] ) ) : error = error . get ( "error" , "" ) if error in ( "NotRegistered" , "InvalidRegistration" ) : unregistered . append ( items [ index ] ) elif error != "" : throw_error = True self . deactivate_unregistered_devices ( unregistered ) if throw_error : raise GCMPushError ( result ) return result
def _send ( self , data , content_type ) : headers = { "Content-Type" : content_type , "Authorization" : "key=%s" % ( self . api_key ) , "Content-Length" : str ( len ( data ) ) } request = Request ( self . api_url , data , headers ) return urlopen ( request ) . read ( ) . decode ( self . encoding )
def get_model ( module_location ) : if not isinstance ( module_location , ( str , unicode ) ) : raise ValueError ( "The value provided should either be a string or " "unicode instance. The value '%s' provided was %s " "rather." % ( module_location , type ( module_location ) ) ) try : name_split = module_location . split ( "." ) class_name = name_split . pop ( - 1 ) if not len ( name_split ) : raise ValueError ( "The value should provide the module location " "joined by '.' e.g. for model named 'test' in " "/app/module.py, The value should be 'app.module.test'" ) module_location = "." . join ( name_split ) module = importlib . import_module ( module_location ) cls = getattr ( module , class_name ) return cls except AttributeError : pass
def _fetch ( self , endpoint_name , * * params ) : params [ 'api_key' ] = self . api_key resp = requests . get ( self . _endpoint ( endpoint_name ) , params = params ) resp . raise_for_status ( ) data = resp . json ( ) self . _check_or_raise ( data . get ( 'meta' , { } ) ) return data
def video ( request , video_id ) : api = Api ( ) api . authenticate ( ) availability = api . check_upload_status ( video_id ) if availability is not True : video = Video . objects . filter ( video_id = video_id ) . get ( ) state = availability [ "upload_state" ] if state == "failed" or state == "rejected" : return render_to_response ( "django_youtube/video_failed.html" , { "video" : video , "video_id" : video_id , "message" : _ ( "Invalid video." ) , "availability" : availability } , context_instance = RequestContext ( request ) ) else : return render_to_response ( "django_youtube/video_unavailable.html" , { "video" : video , "video_id" : video_id , "message" : _ ( "This video is currently being processed" ) , "availability" : availability } , context_instance = RequestContext ( request ) ) video_params = _video_params ( request , video_id ) return render_to_response ( "django_youtube/video.html" , video_params , context_instance = RequestContext ( request ) )
def newick ( self ) : label = self . name or '' if self . _length : label += ':' + self . _length descendants = ',' . join ( [ n . newick for n in self . descendants ] ) if descendants : descendants = '(' + descendants + ')' return descendants + label
def remove_internal_names ( self ) : self . visit ( lambda n : setattr ( n , 'name' , None ) , lambda n : not n . is_leaf )
def remove_leaf_names ( self ) : self . visit ( lambda n : setattr ( n , 'name' , None ) , lambda n : n . is_leaf )
def auth_required ( realm , auth_func ) : def auth_decorator ( func ) : def inner ( self , * args , * * kw ) : if self . get_authenticated_user ( auth_func , realm ) : return func ( self , * args , * * kw ) return inner return auth_decorator
def require_setting ( self , name , feature = "this feature" ) : if name not in self . settings : raise Exception ( "You must define the '%s' setting in your " "application to use %s" % ( name , feature ) )
def get_cookie ( self , name , default = None ) : assert self . cookie_monster , 'Cookie Monster not set' return self . cookie_monster . get_cookie ( name , default )
def clear_cookie ( self , name , path = "/" , domain = None ) : assert self . cookie_monster , 'Cookie Monster not set' #, path=path, domain=domain) self . cookie_monster . delete_cookie ( name )
def get_authenticated_user ( self , callback ) : oauth_ns = "" for name , values in self . request . arguments . iteritems ( ) : if name . startswith ( "openid.ns." ) and values [ - 1 ] == u"http://specs.openid.net/extensions/oauth/1.0" : oauth_ns = name [ 10 : ] break token = self . get_argument ( "openid." + oauth_ns + ".request_token" , "" ) if token : http = httpclient . AsyncHTTPClient ( ) token = dict ( key = token , secret = "" ) http . fetch ( self . _oauth_access_token_url ( token ) , self . async_callback ( self . _on_access_token , callback ) ) else : OpenIdMixin . get_authenticated_user ( self , callback )
def add ( self , name , value ) : norm_name = HTTPHeaders . _normalize_name ( name ) self . _last_key = norm_name if norm_name in self : dict . __setitem__ ( self , norm_name , self [ norm_name ] + ',' + value ) self . _as_list [ norm_name ] . append ( value ) else : self [ norm_name ] = value
def get_list ( self , name ) : norm_name = HTTPHeaders . _normalize_name ( name ) return self . _as_list . get ( norm_name , [ ] )
def selectPolicy ( self , origin , request_method = None ) : ret_origin = None policyname = None if self . matchstrategy in ( "firstmatch" , "verbmatch" ) : for pol in self . activepolicies : policy = self . policies [ pol ] ret_origin = None policyname = policy . name if policyname == "deny" : break if self . matchstrategy == "verbmatch" : if policy . methods != "*" and not CORS . matchlist ( request_method , policy . methods , case_sensitive = True ) : continue if origin and policy . match : if CORS . matchlist ( origin , policy . match ) : ret_origin = origin elif policy . origin == "copy" : ret_origin = origin elif policy . origin : ret_origin = policy . origin if ret_origin : break return policyname , ret_origin
def get_data_from_user ( msg_type ) : data = { } for k , v in CONFIG [ msg_type ] [ "settings" ] . items ( ) : data [ k ] = input ( v + ": " ) return data
def get_auth_from_user ( msg_type ) : auth = [ ] for k , v in CONFIG [ msg_type ] [ "auth" ] . items ( ) : auth . append ( ( k , getpass ( v + ": " ) ) ) return OrderedDict ( auth )
def _construct_message ( self ) : self . message [ "text" ] = "" if self . from_ : self . message [ "text" ] += "From: " + self . from_ + "\n" if self . subject : self . message [ "text" ] += "Subject: " + self . subject + "\n" self . message [ "text" ] += self . body self . _add_attachments ( )
def send ( self , encoding = "json" ) : self . _construct_message ( ) if self . verbose : print ( "Debugging info" "\n--------------" "\n{} Message created." . format ( timestamp ( ) ) ) if encoding == "json" : resp = requests . post ( self . url , json = self . message ) elif encoding == "url" : resp = requests . post ( self . url , data = self . message ) try : resp . raise_for_status ( ) if resp . history and resp . history [ 0 ] . status_code >= 300 : raise MessageSendError ( "HTTP Redirect: Possibly Invalid authentication" ) elif "invalid_auth" in resp . text : raise MessageSendError ( "Invalid Auth: Possibly Bad Auth Token" ) except ( requests . exceptions . HTTPError , MessageSendError ) as e : raise MessageSendError ( e ) if self . verbose : print ( timestamp ( ) , type ( self ) . __name__ , " info:" , self . __str__ ( indentation = "\n * " ) , "\n * HTTP status code:" , resp . status_code , ) print ( "Message sent." )
def validate_input ( msg_type , attr , value ) : try : valid = { "Email" : validate_email , "Twilio" : validate_twilio , "SlackWebhook" : validate_slackwebhook , "SlackPost" : validate_slackpost , "TelegramBot" : validate_telegrambot , "WhatsApp" : validate_whatsapp , } [ msg_type ] ( attr , value ) except KeyError : return 1 else : return 0
def validate_twilio ( attr , value ) : if attr in ( "from_" , "to" ) : check_valid ( "Twilio" , attr , value , validus . isphone , "phone number" ) elif attr in ( "attachments" ) : check_valid ( "Twilio" , attr , value , validus . isurl , "url" )
def validate_slackpost ( attr , value ) : if attr in ( "channel" , "credentials" ) : if not isinstance ( value , str ) : raise InvalidMessageInputError ( "SlackPost" , attr , value , "string" ) elif attr in ( "attachments" ) : check_valid ( "SlackPost" , attr , value , validus . isurl , "url" )
def validate_whatsapp ( attr , value ) : if attr in ( "from_" , "to" ) : if value is not None and "whatsapp:" in value : value = value . split ( "whatsapp:+" ) [ - 1 ] check_valid ( "WhatsApp" , attr , value , validus . isint , "phone number starting with the '+' symbol" , ) elif attr in ( "attachments" ) : check_valid ( "WhatsApp" , attr , value , validus . isurl , "url" )
def add_message ( self , msg ) : try : self . _coro . send ( msg ) except AttributeError : raise UnsupportedMessageTypeError ( msg . __class__ . __name__ )
def get_body_from_file ( kwds ) : if kwds [ "file" ] and os . path . isfile ( kwds [ "file" ] ) : kwds [ "body" ] = open ( kwds [ "file" ] , "r" ) . read ( ) kwds [ "file" ] = None
def trim_args ( kwds ) : reject_key = ( "type" , "types" , "configure" ) reject_val = ( None , ( ) ) kwargs = { k : v for k , v in kwds . items ( ) if k not in reject_key and v not in reject_val } for k , v in kwargs . items ( ) : if k in ( "to" , "cc" , "bcc" , "attachments" ) : kwargs [ k ] = list ( kwargs [ k ] ) return kwargs
def send_message ( msg_type , kwds ) : if kwds [ "file" ] : get_body_from_file ( kwds ) kwargs = trim_args ( kwds ) send ( msg_type , send_async = False , * * kwargs )
def get_chat_id ( self , username ) : if username is not None : chats = requests . get ( self . base_url + "/getUpdates" ) . json ( ) user = username . split ( "@" ) [ - 1 ] for chat in chats [ "result" ] : if chat [ "message" ] [ "from" ] [ "username" ] == user : return chat [ "message" ] [ "from" ] [ "id" ]
def _construct_message ( self ) : self . message [ "chat_id" ] = self . chat_id self . message [ "text" ] = "" if self . from_ : self . message [ "text" ] += "From: " + self . from_ + "\n" if self . subject : self . message [ "text" ] += "Subject: " + self . subject + "\n" self . message [ "text" ] += self . body self . message . update ( self . params )
def _send_content ( self , method = "/sendMessage" ) : url = self . base_url + method try : resp = requests . post ( url , json = self . message ) resp . raise_for_status ( ) except requests . exceptions . HTTPError as e : raise MessageSendError ( e ) if self . verbose : if method == "/sendMessage" : content_type = "Message body" elif method == "/sendDocument" : content_type = "Attachment: " + self . message [ "document" ] print ( timestamp ( ) , content_type , "sent." )
def send ( self ) : self . _construct_message ( ) if self . verbose : print ( "Debugging info" "\n--------------" "\n{} Message created." . format ( timestamp ( ) ) ) self . _send_content ( "/sendMessage" ) if self . attachments : if isinstance ( self . attachments , str ) : self . attachments = [ self . attachments ] for a in self . attachments : self . message [ "document" ] = a self . _send_content ( method = "/sendDocument" ) if self . verbose : print ( timestamp ( ) , type ( self ) . __name__ + " info:" , self . __str__ ( indentation = "\n * " ) , ) print ( "Message sent." )
def get_server ( address = None ) : if address : domain = address . split ( "@" ) [ 1 ] try : return SMTP_SERVERS [ domain ] except KeyError : return ( "smtp." + domain , 465 ) return ( None , None )
def _generate_email ( self ) : self . message = MIMEMultipart ( ) self . _add_header ( ) self . _add_body ( ) self . _add_attachments ( )
def _add_header ( self ) : self . message [ "From" ] = self . from_ self . message [ "Subject" ] = self . subject if self . to : self . message [ "To" ] = self . list_to_string ( self . to ) if self . cc : self . message [ "Cc" ] = self . list_to_string ( self . cc ) if self . bcc : self . message [ "Bcc" ] = self . list_to_string ( self . bcc )
def _add_body ( self ) : if self . body : b = MIMEText ( "text" , "plain" ) b . set_payload ( self . body ) self . message . attach ( b )
def _add_attachments ( self ) : num_attached = 0 if self . attachments : if isinstance ( self . attachments , str ) : self . attachments = [ self . attachments ] for item in self . attachments : doc = MIMEApplication ( open ( item , "rb" ) . read ( ) ) doc . add_header ( "Content-Disposition" , "attachment" , filename = item ) self . message . attach ( doc ) num_attached += 1 return num_attached
def _get_session ( self ) : if self . port in ( 465 , "465" ) : session = self . _get_ssl ( ) elif self . port in ( 587 , "587" ) : session = self . _get_tls ( ) try : session . login ( self . from_ , self . _auth ) except SMTPResponseException as e : raise MessageSendError ( e . smtp_error . decode ( "unicode_escape" ) ) return session
def _get_ssl ( self ) : return smtplib . SMTP_SSL ( self . server , self . port , context = ssl . create_default_context ( ) )
def _get_tls ( self ) : session = smtplib . SMTP ( self . server , self . port ) session . ehlo ( ) session . starttls ( context = ssl . create_default_context ( ) ) session . ehlo ( ) return session
def delete ( self , filename = None ) : if self . tags is not None : if filename is None : filename = self . filename else : warnings . warn ( "delete(filename=...) is deprecated, reload the file" , DeprecationWarning ) return self . tags . delete ( filename )
def save ( self , filename = None , * * kwargs ) : if filename is None : filename = self . filename else : warnings . warn ( "save(filename=...) is deprecated, reload the file" , DeprecationWarning ) if self . tags is not None : return self . tags . save ( filename , * * kwargs ) else : raise ValueError ( "no tags in file" )
def unload ( self ) : if self . _handle != - 1 : lib . UnloadImage ( self . _handle ) self . _handle = - 1
def clear ( self ) : for i in list ( self . _internal ) : self . _internal . remove ( i )
def read ( self ) : self . __fileobj . seek ( self . data_offset ) self . data = self . __fileobj . read ( self . data_size )
def delete ( self ) : delete_bytes ( self . __fileobj , self . size , self . offset ) if self . parent_chunk is not None : self . parent_chunk . resize ( self . parent_chunk . data_size - self . size )
def resize ( self , data_size ) : self . __fileobj . seek ( self . offset + 4 ) self . __fileobj . write ( pack ( '>I' , data_size ) ) if self . parent_chunk is not None : size_diff = self . data_size - data_size self . parent_chunk . resize ( self . parent_chunk . data_size - size_diff ) self . data_size = data_size self . size = data_size + self . HEADER_SIZE
def insert_chunk ( self , id_ ) : if not isinstance ( id_ , text_type ) : id_ = id_ . decode ( 'ascii' ) if not is_valid_chunk_id ( id_ ) : raise KeyError ( "AIFF key must be four ASCII characters." ) self . __fileobj . seek ( self . __next_offset ) self . __fileobj . write ( pack ( '>4si' , id_ . ljust ( 4 ) . encode ( 'ascii' ) , 0 ) ) self . __fileobj . seek ( self . __next_offset ) chunk = IFFChunk ( self . __fileobj , self [ u'FORM' ] ) self [ u'FORM' ] . resize ( self [ u'FORM' ] . data_size + chunk . size ) self . __chunks [ id_ ] = chunk self . __next_offset = chunk . offset + chunk . size
def save ( self , filename = None , v2_version = 4 , v23_sep = '/' ) : framedata = self . _prepare_framedata ( v2_version , v23_sep ) framesize = len ( framedata ) if filename is None : filename = self . filename fileobj = open ( filename , 'rb+' ) iff_file = IFFFile ( fileobj ) try : if u'ID3' not in iff_file : iff_file . insert_chunk ( u'ID3' ) chunk = iff_file [ u'ID3' ] fileobj . seek ( chunk . data_offset ) header = fileobj . read ( 10 ) header = self . _prepare_id3_header ( header , framesize , v2_version ) header , new_size , _ = header data = header + framedata + ( b'\x00' * ( new_size - framesize ) ) new_size += 10 if new_size > chunk . size : insert_at = chunk . offset + chunk . size insert_size = new_size - chunk . size + new_size % 2 insert_bytes ( fileobj , insert_size , insert_at ) chunk . resize ( new_size ) fileobj . seek ( chunk . data_offset ) fileobj . write ( data ) finally : fileobj . close ( )
def delete ( self , filename = None ) : if filename is None : filename = self . filename delete ( filename ) self . clear ( )
def load ( self , filename , * * kwargs ) : self . filename = filename try : self . tags = _IFFID3 ( filename , * * kwargs ) except ID3Error : self . tags = None try : fileobj = open ( filename , "rb" ) self . info = AIFFInfo ( fileobj ) finally : fileobj . close ( )
def parse_file ( self , filename ) : self . reset ( ) self . filename = filename fileinput . close ( ) self . format = None self . lineno = 0 self . lines = [ ] for line in fileinput . input ( filename ) : if line [ - 1 ] == '\012' : line = line [ 0 : - 1 ] if self . format == None : self . process_normal_line ( line ) else : if self . format . end . match ( line ) : self . lines . append ( line ) self . add_block_lines ( ) elif self . format . column . match ( line ) : self . lines . append ( line ) else : self . add_block_lines ( ) self . process_normal_line ( line ) self . add_block_lines ( )
def process_normal_line ( self , line ) : for f in re_source_block_formats : if f . start . match ( line ) : self . add_block_lines ( ) self . format = f self . lineno = fileinput . filelineno ( ) self . lines . append ( line )
def add_block_lines ( self ) : if self . lines != [ ] : block = SourceBlock ( self , self . filename , self . lineno , self . lines ) self . blocks . append ( block ) self . format = None self . lines = [ ]
def make_html_words ( self , words ) : line = "" if words : line = html_quote ( words [ 0 ] ) for w in words [ 1 : ] : line = line + " " + html_quote ( w ) return line
def make_html_word ( self , word ) : m = re_crossref . match ( word ) if m : try : name = m . group ( 1 ) rest = m . group ( 2 ) block = self . identifiers [ name ] url = self . make_block_url ( block ) return '<a href="' + url + '">' + name + '</a>' + rest except : sys . stderr . write ( "WARNING: undefined cross reference '" + name + "'.\n" ) return '?' + name + '?' + rest m = re_italic . match ( word ) if m : name = m . group ( 1 ) rest = m . group ( 3 ) return '<i>' + name + '</i>' + rest m = re_bold . match ( word ) if m : name = m . group ( 1 ) rest = m . group ( 3 ) return '<b>' + name + '</b>' + rest return html_quote ( word )
def make_html_para ( self , words ) : line = "" if words : line = self . make_html_word ( words [ 0 ] ) for word in words [ 1 : ] : line = line + " " + self . make_html_word ( word ) line = re . sub ( r"(^|\W)`(.*?)'(\W|$)" , r'\1&lsquo;\2&rsquo;\3' , line ) line = string . replace ( line , "~" , "&nbsp;" ) return para_header + line + para_footer
def make_html_code ( self , lines ) : line = code_header + '\n' for l in lines : line = line + html_quote ( l ) + '\n' return line + code_footer
def make_html_items ( self , items ) : lines = [ ] for item in items : if item . lines : lines . append ( self . make_html_code ( item . lines ) ) else : lines . append ( self . make_html_para ( item . words ) ) return string . join ( lines , '\n' )
def save ( self , filename ) : values = [ ] items = sorted ( self . items ( ) , key = MP4Tags . __get_sort_stats ) for key , value in items : info = self . __atoms . get ( key [ : 4 ] , ( None , type ( self ) . __render_text ) ) try : values . append ( info [ 1 ] ( self , key , value , * info [ 2 : ] ) ) except ( TypeError , ValueError ) as s : reraise ( MP4MetadataValueError , s , sys . exc_info ( ) [ 2 ] ) data = Atom . render ( b"ilst" , b"" . join ( values ) ) fileobj = open ( filename , "rb+" ) try : atoms = Atoms ( fileobj ) try : path = atoms . path ( b"moov" , b"udta" , b"meta" , b"ilst" ) except KeyError : self . __save_new ( fileobj , atoms , data ) else : self . __save_existing ( fileobj , atoms , path , data ) finally : fileobj . close ( )
def __update_parents ( self , fileobj , path , delta ) : for atom in path : fileobj . seek ( atom . offset ) size = cdata . uint_be ( fileobj . read ( 4 ) ) if size == 1 : size = cdata . ulonglong_be ( fileobj . read ( 12 ) [ 4 : ] ) fileobj . seek ( atom . offset + 8 ) fileobj . write ( cdata . to_ulonglong_be ( size + delta ) ) else : fileobj . seek ( atom . offset ) fileobj . write ( cdata . to_uint_be ( size + delta ) )
def load ( self , filename ) : self . filename = filename fileobj = open ( filename , "rb" ) try : data = _APEv2Data ( fileobj ) finally : fileobj . close ( ) if data . tag : self . clear ( ) self . __casemap . clear ( ) self . __parse_tag ( data . tag , data . items ) else : raise APENoHeaderError ( "No APE tag found" )
def delete ( self , filename = None ) : filename = filename or self . filename fileobj = open ( filename , "r+b" ) try : data = _APEv2Data ( fileobj ) if data . start is not None and data . size is not None : delete_bytes ( fileobj , data . end - data . start , data . start ) finally : fileobj . close ( ) self . clear ( )
def size ( self ) : header_size = 27 for datum in self . packets : quot , rem = divmod ( len ( datum ) , 255 ) header_size += quot + 1 if not self . complete and rem == 0 : header_size -= 1 header_size += sum ( map ( len , self . packets ) ) return header_size
def load ( self , filename ) : self . filename = filename fileobj = open ( filename , "rb" ) try : try : self . info = self . _Info ( fileobj ) self . tags = self . _Tags ( fileobj , self . info ) self . info . _post_tags ( fileobj ) except error as e : reraise ( self . _Error , e , sys . exc_info ( ) [ 2 ] ) except EOFError : raise self . _Error ( "no appropriate stream found" ) finally : fileobj . close ( )
def set_section ( self , section_name ) : if not self . sections . has_key ( section_name ) : section = DocSection ( section_name ) self . sections [ section_name ] = section self . section = section else : self . section = self . sections [ section_name ]
def add_markup ( self ) : if self . markup and self . markup_lines : marks = self . markup_lines if len ( marks ) > 0 and not string . strip ( marks [ - 1 ] ) : self . markup_lines = marks [ : - 1 ] m = DocMarkup ( self . markup , self . markup_lines ) self . markups . append ( m ) self . markup = None self . markup_lines = [ ]
def get_markup ( self , tag_name ) : for m in self . markups : if m . tag == string . lower ( tag_name ) : return m return None
def utf8 ( data ) : if isinstance ( data , bytes ) : return data . decode ( "utf-8" , "replace" ) . encode ( "utf-8" ) elif isinstance ( data , text_type ) : return data . encode ( "utf-8" ) else : raise TypeError ( "only unicode/bytes types can be converted to UTF-8" )
def delete ( self ) : cset = ChangeSet ( connection = self . connection , hosted_zone_id = self . zone_id ) cset . add_change ( 'DELETE' , self ) return self . connection . _change_resource_record_sets ( cset )
def save ( self ) : cset = ChangeSet ( connection = self . connection , hosted_zone_id = self . zone_id ) cset . add_change ( 'DELETE' , self ) cset . add_change ( 'CREATE' , self ) retval = self . connection . _change_resource_record_sets ( cset ) for key , val in self . _initial_vals . items ( ) : self . _initial_vals [ key ] = getattr ( self , key ) return retval
def ParseID3v1 ( data ) : try : data = data [ data . index ( b'TAG' ) : ] except ValueError : return None if 128 < len ( data ) or len ( data ) < 124 : return None unpack_fmt = "3s30s30s30s%ds29sBB" % ( len ( data ) - 124 ) try : tag , title , artist , album , year , comment , track , genre = unpack ( unpack_fmt , data ) except StructError : return None if tag != b"TAG" : return None def fix ( data ) : return data . split ( b'\x00' ) [ 0 ] . strip ( ) . decode ( 'latin1' ) title , artist , album , year , comment = map ( fix , [ title , artist , album , year , comment ] ) frames = { } if title : frames [ 'TIT2' ] = TIT2 ( encoding = 0 , text = title ) if artist : frames [ 'TPE1' ] = TPE1 ( encoding = 0 , text = [ artist ] ) if album : frames [ 'TALB' ] = TALB ( encoding = 0 , text = album ) if year : frames [ 'TDRC' ] = TDRC ( encoding = 0 , text = year ) if comment : frames [ 'COMM' ] = COMM ( encoding = 0 , lang = 'eng' , desc = "ID3v1 Comment" , text = comment ) if track and ( ( track != 32 ) or ( data [ - 3 ] == b'\x00' [ 0 ] ) ) : frames [ 'TRCK' ] = TRCK ( encoding = 0 , text = str ( track ) ) if genre != 255 : frames [ 'TCON' ] = TCON ( encoding = 0 , text = str ( genre ) ) return frames
def MakeID3v1 ( id3 ) : v1 = { } for v2id , name in { "TIT2" : "title" , "TPE1" : "artist" , "TALB" : "album" } . items ( ) : if v2id in id3 : text = id3 [ v2id ] . text [ 0 ] . encode ( 'latin1' , 'replace' ) [ : 30 ] else : text = b'' v1 [ name ] = text + ( b'\x00' * ( 30 - len ( text ) ) ) if "COMM" in id3 : cmnt = id3 [ "COMM" ] . text [ 0 ] . encode ( 'latin1' , 'replace' ) [ : 28 ] else : cmnt = b'' v1 [ 'comment' ] = cmnt + ( b'\x00' * ( 29 - len ( cmnt ) ) ) if "TRCK" in id3 : try : v1 [ "track" ] = chr_ ( + id3 [ "TRCK" ] ) except ValueError : v1 [ "track" ] = b'\x00' else : v1 [ "track" ] = b'\x00' if "TCON" in id3 : try : genre = id3 [ "TCON" ] . genres [ 0 ] except IndexError : pass else : if genre in TCON . GENRES : v1 [ "genre" ] = chr_ ( TCON . GENRES . index ( genre ) ) if "genre" not in v1 : v1 [ "genre" ] = b"\xff" if "TDRC" in id3 : year = text_type ( id3 [ "TDRC" ] ) . encode ( 'latin1' , 'replace' ) elif "TYER" in id3 : year = text_type ( id3 [ "TYER" ] ) . encode ( 'latin1' , 'replace' ) else : year = b'' v1 [ 'year' ] = ( year + b'\x00\x00\x00\x00' ) [ : 4 ] return ( b'TAG' + v1 [ 'title' ] + v1 [ 'artist' ] + v1 [ 'album' ] + v1 [ 'year' ] + v1 [ 'comment' ] + v1 [ 'track' ] + v1 [ 'genre' ] )
def __fullread ( self , size ) : try : if size < 0 : raise ValueError ( 'Requested bytes (%s) less than zero' % size ) if size > self . __filesize : raise EOFError ( 'Requested %#x of %#x (%s)' % ( int ( size ) , int ( self . __filesize ) , self . filename ) ) except AttributeError : pass data = self . _fileobj . read ( size ) if len ( data ) != size : raise EOFError self . __readbytes += size return data
def delall ( self , key ) : if key in self : del ( self [ key ] ) else : key = key + ":" for k in self . keys ( ) : if k . startswith ( key ) : del ( self [ k ] )
def loaded_frame ( self , tag ) : if len ( type ( tag ) . __name__ ) == 3 : tag = type ( tag ) . __base__ ( tag ) self [ tag . HashKey ] = tag
def __update_common ( self ) : if "TCON" in self : self [ "TCON" ] . genres = self [ "TCON" ] . genres if self . version < self . _V23 : pics = self . getall ( "APIC" ) mimes = { "PNG" : "image/png" , "JPG" : "image/jpeg" } self . delall ( "APIC" ) for pic in pics : newpic = APIC ( encoding = pic . encoding , mime = mimes . get ( pic . mime , pic . mime ) , type = pic . type , desc = pic . desc , data = pic . data ) self . add ( newpic ) self . delall ( "LINK" )
def unload ( self ) : if self . _handle != - 1 : lib . UnloadSound ( self . _handle ) self . _handle = - 1
def adobe_glyph_values ( ) : lines = string . split ( adobe_glyph_list , '\n' ) glyphs = [ ] values = [ ] for line in lines : if line : fields = string . split ( line , ';' ) subfields = string . split ( fields [ 1 ] , ' ' ) if len ( subfields ) == 1 : glyphs . append ( fields [ 0 ] ) values . append ( fields [ 1 ] ) return glyphs , values
def filter_glyph_names ( alist , filter ) : count = 0 extras = [ ] for name in alist : try : filtered_index = filter . index ( name ) except : extras . append ( name ) return extras
def dump_encoding ( file , encoding_name , encoding_list ) : write = file . write write ( "  /* the following are indices into the SID name table */\n" ) write ( "  static const unsigned short  " + encoding_name + "[" + repr ( len ( encoding_list ) ) + "] =\n" ) write ( "  {\n" ) line = "    " comma = "" col = 0 for value in encoding_list : line += comma line += "%3d" % value comma = "," col += 1 if col == 16 : col = 0 comma = ",\n    " write ( line + "\n  };\n\n\n" )
def dump_array ( the_array , write , array_name ) : write ( "  static const unsigned char  " + array_name + "[" + repr ( len ( the_array ) ) + "L] =\n" ) write ( "  {\n" ) line = "" comma = "    " col = 0 for value in the_array : line += comma line += "%3d" % ord ( value ) comma = "," col += 1 if col == 16 : col = 0 comma = ",\n    " if len ( line ) > 1024 : write ( line ) line = "" write ( line + "\n  };\n\n\n" )
def file_exists ( pathname ) : result = 1 try : file = open ( pathname , "r" ) file . close ( ) except : result = None sys . stderr . write ( pathname + " couldn't be accessed\n" ) return result
def make_file_list ( args = None ) : file_list = [ ] if not args : args = sys . argv [ 1 : ] for pathname in args : if string . find ( pathname , '*' ) >= 0 : newpath = glob . glob ( pathname ) newpath . sort ( ) else : newpath = [ pathname ] file_list . extend ( newpath ) if len ( file_list ) == 0 : file_list = None else : file_list = filter ( file_exists , file_list ) return file_list
def writeblocks ( blocks ) : data = [ ] codes = [ [ block . code , block . write ( ) ] for block in blocks ] codes [ - 1 ] [ 0 ] |= 128 for code , datum in codes : byte = chr_ ( code ) if len ( datum ) > 2 ** 24 : raise error ( "block is too long to write" ) length = struct . pack ( ">I" , len ( datum ) ) [ - 3 : ] data . append ( byte + length + datum ) return b"" . join ( data )
def load ( self , filename ) : self . metadata_blocks = [ ] self . tags = None self . cuesheet = None self . seektable = None self . filename = filename fileobj = StrictFileObject ( open ( filename , "rb" ) ) try : self . __check_header ( fileobj ) while self . __read_metadata_block ( fileobj ) : pass finally : fileobj . close ( ) try : self . metadata_blocks [ 0 ] . length except ( AttributeError , IndexError ) : raise FLACNoHeaderError ( "Stream info block not found" )
def init_logs ( ) : start_time = dt . fromtimestamp ( time . time ( ) ) . strftime ( '%Y%m%d_%H%M' ) logname = os . path . join ( os . path . expanduser ( "~" ) + "/nanoGUI_" + start_time + ".log" ) handlers = [ logging . FileHandler ( logname ) ] logging . basicConfig ( format = '%(asctime)s %(message)s' , handlers = handlers , level = logging . INFO ) logging . info ( 'NanoGUI {} started with NanoPlot {}' . format ( __version__ , nanoplot . __version__ ) ) logging . info ( 'Python version is: {}' . format ( sys . version . replace ( '\n' , ' ' ) ) ) return logname
def alias_item ( self , alias ) : ident = self . alias [ alias ] return self . items [ ident ]
def initialize_bars ( self , sender = None , * * kwargs ) : for bar in self . bars . values ( ) : for initializer in bar . initializers : initializer ( self )
def bind_bar ( self , sender = None , * * kwargs ) : bar = kwargs . pop ( 'bar' ) self . bars [ bar . name ] = bar
def validate ( metric_class ) : if not hasattr ( metric_class , 'label' ) : raise ImproperlyConfigured ( "No 'label' attribute found for metric %s." % metric_class . __name__ ) if not hasattr ( metric_class , 'widget' ) : raise ImproperlyConfigured ( "No 'widget' attribute found for metric %s." % metric_class . __name__ )
def calculate_statistics ( stat , frequencies ) : stats = ensure_list ( stat ) frequencies = ensure_list ( frequencies ) for stat in stats : for f in frequencies : print "Calculating %s (%s)..." % ( stat . __name__ , settings . STATISTIC_FREQUENCY_DICT [ f ] ) stat . calculate ( f )
def handle ( self , * args , * * kwargs ) : frequency = kwargs [ 'frequency' ] frequencies = settings . STATISTIC_FREQUENCY_ALL if frequency == 'a' else ( frequency . split ( ',' ) if ',' in frequency else [ frequency ] ) if kwargs [ 'list' ] : maintenance . list_statistics ( ) elif kwargs [ 'calculate' ] : maintenance . calculate_statistics ( maintenance . get_statistic_by_name ( kwargs [ 'calculate' ] ) , frequencies ) elif kwargs [ 'reset' ] : maintenance . reset_statistics ( maintenance . get_statistic_by_name ( kwargs [ 'reset' ] ) , frequencies , kwargs [ 'reset_cumulative' ] ) elif kwargs [ 'recalculate' ] : maintenance . reset_statistics ( maintenance . get_statistic_by_name ( kwargs [ 'recalculate' ] ) , frequencies , kwargs [ 'reset_cumulative' ] , True )
def get_GET_array ( request , var_name , fail_silently = True ) : vals = request . GET . getlist ( var_name ) if not vals : if fail_silently : return [ ] else : raise Exception , _ ( "No array called '%(varname)s' in GET variables" ) % { 'varname' : var_name } return vals
def get_GET_bool ( request , var_name , default = True ) : val = request . GET . get ( var_name , default ) if isinstance ( val , str ) or isinstance ( val , unicode ) : val = True if val [ 0 ] == 't' else False return val
def get_next_colour ( ) : colour = settings . GECKOBOARD_COLOURS [ get_next_colour . cur_colour ] get_next_colour . cur_colour += 1 if get_next_colour . cur_colour >= len ( settings . GECKOBOARD_COLOURS ) : get_next_colour . cur_colour = 0 return colour
def geckoboard_number_widget ( request ) : params = get_gecko_params ( request , days_back = 7 ) metric = Metric . objects . get ( uid = params [ 'uid' ] ) try : latest_stat = metric . statistics . filter ( frequency = params [ 'frequency' ] ) . order_by ( '-date_time' ) [ 0 ] except IndexError : return ( 0 , 0 ) try : prev_stat = metric . statistics . filter ( frequency = params [ 'frequency' ] , date_time__lte = latest_stat . date_time - timedelta ( days = params [ 'days_back' ] ) ) . order_by ( '-date_time' ) [ 0 ] except IndexError : return ( latest_stat . cumulative_count , 0 ) if params [ 'cumulative' ] else ( latest_stat . count , 0 ) return ( latest_stat . cumulative_count , prev_stat . cumulative_count ) if params [ 'cumulative' ] else ( latest_stat . count , prev_stat . count )
def geckoboard_line_chart ( request ) : params = get_gecko_params ( request , cumulative = False , days_back = 7 ) metric = Metric . objects . get ( uid = params [ 'uid' ] ) start_date = datetime . now ( ) - timedelta ( days = params [ 'days_back' ] ) stats = [ s for s in metric . statistics . filter ( frequency = params [ 'frequency' ] , date_time__gte = start_date ) . order_by ( 'date_time' ) ] if len ( stats ) == 0 : raise Exception , _ ( "No statistics for metric %(metric)s." ) % { 'metric' : params [ 'uid' ] } dates = [ stats [ 0 ] . date_time ] if len ( stats ) >= 3 : mid = len ( stats ) / 2 if not mid : mid = 1 dates . extend ( [ stats [ mid ] . date_time , stats [ - 1 ] . date_time ] ) elif len ( stats ) == 2 : dates . extend ( [ stats [ - 1 ] . date_time ] ) return ( [ s . count for s in stats ] , dates , metric . title , )
def geckoboard_geckometer ( request ) : params = get_gecko_params ( request , cumulative = True ) metric = Metric . objects . get ( uid = params [ 'uid' ] ) return ( metric . latest_count ( frequency = params [ 'frequency' ] , count = not params [ 'cumulative' ] , cumulative = params [ 'cumulative' ] ) , params [ 'min' ] , params [ 'max' ] )
def geckoboard_funnel ( request , frequency = settings . STATISTIC_FREQUENCY_DAILY ) : params = get_gecko_params ( request , cumulative = True ) metrics = Metric . objects . filter ( uid__in = params [ 'uids' ] ) items = [ ( metric . latest_count ( frequency = params [ 'frequency' ] , count = not params [ 'cumulative' ] , cumulative = params [ 'cumulative' ] ) , metric . title ) for metric in metrics ] return { 'items' : items , 'type' : params [ 'type' ] , 'percentage' : params [ 'percentage' ] , 'sort' : params [ 'sort' ] , }
def get_active_stats ( self ) : stats = [ ] for gadget in self . _registry . values ( ) : for s in gadget . stats : if s not in stats : stats . append ( s ) return stats
def get_context_data ( self , * * kwargs ) : #max_columns, max_rows = self.get_max_dimension() context = { 'gadgets' : self . _registry , 'columns' : self . columns , 'rows' : self . rows , 'column_ratio' : 100 - self . columns * 2 , 'row_ratio' : 100 - self . rows * 2 , } context . update ( kwargs ) return context
def error ( self , message , code = 1 ) : print >> sys . stderr , message sys . exit ( code )
def valid ( schema = None ) : def dec ( fun ) : @ wraps ( fun ) def d_func ( self , ctx , data , * a , * * kw ) : try : validate ( data [ 'params' ] , schema ) except ValidationError as err : raise InvalidParams ( err ) except SchemaError as err : raise InternalError ( err ) return fun ( self , ctx , data [ 'params' ] , * a , * * kw ) return d_func return dec
def long_input ( prompt = 'Multi-line input\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxlines = None , maxlength = None ) : lines = [ ] print ( prompt ) lnum = 1 try : while True : if maxlines : if lnum > maxlines : break else : if maxlength : lines . append ( string_input ( '' ) [ : maxlength ] ) else : lines . append ( string_input ( '' ) ) lnum += 1 else : if maxlength : lines . append ( string_input ( '' ) [ : maxlength ] ) else : lines . append ( string_input ( '' ) ) except EOFError : pass finally : return '\n' . join ( lines )
def list_input ( prompt = 'List input - enter each item on a seperate line\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxitems = None , maxlength = None ) : lines = [ ] print ( prompt ) inum = 1 try : while True : if maxitems : if inum > maxitems : break else : if maxlength : lines . append ( string_input ( '' ) [ : maxlength ] ) else : lines . append ( string_input ( '' ) ) inum += 1 else : if maxlength : lines . append ( string_input ( '' ) [ : maxlength ] ) else : lines . append ( string_input ( '' ) ) except EOFError : pass finally : return lines
def outfile_input ( extension = None ) : fileok = False while not fileok : filename = string_input ( 'File name? ' ) if extension : if not filename . endswith ( extension ) : if extension . startswith ( '.' ) : filename = filename + extension else : filename = filename + '.' + extension if os . path . isfile ( filename ) : choice = choice_input ( prompt = filename + ' already exists. Overwrite?' , options = [ 'y' , 'n' ] ) if choice == 'y' : try : nowtime = time . time ( ) with open ( filename , 'a' ) as f : os . utime ( filename , ( nowtime , nowtime ) ) fileok = True except IOError : print ( 'Write permission denied on ' + filename + '. Try again.' ) except PermissionError : print ( 'Write permission denied on ' + filename + '. Try again.' ) except FileNotFoundError : print ( filename + ': directory not found. Try again.' ) else : choice = choice_input ( prompt = filename + ' does not exist. Create it?' , options = [ 'y' , 'n' ] ) if choice == 'y' : try : nowtime = time . time ( ) with open ( filename , 'w' ) as f : os . utime ( filename , ( nowtime , nowtime ) ) fileok = True except IOError : print ( 'Write permission denied on ' + filename + '. Try again.' ) except PermissionError : print ( 'Write permission denied on ' + filename + '. Try again.' ) except FileNotFoundError : print ( filename + ': directory not found. Try again.' ) return filename
def winner ( self ) : hmScore = self . home_score ( ) awScore = self . away_score ( ) if hmScore > awScore : return self . home ( ) elif hmScore < awScore : return self . away ( ) else : return None
def standings ( self ) : doc = self . get_sub_doc ( 'standings' ) east_table = doc ( 'table#divs_standings_E' ) east_df = pd . DataFrame ( sportsref . utils . parse_table ( east_table ) ) east_df . sort_values ( 'wins' , ascending = False , inplace = True ) east_df [ 'seed' ] = range ( 1 , len ( east_df ) + 1 ) east_df [ 'conference' ] = 'E' west_table = doc ( 'table#divs_standings_W' ) west_df = sportsref . utils . parse_table ( west_table ) west_df . sort_values ( 'wins' , ascending = False , inplace = True ) west_df [ 'seed' ] = range ( 1 , len ( west_df ) + 1 ) west_df [ 'conference' ] = 'W' full_df = pd . concat ( [ east_df , west_df ] , axis = 0 ) . reset_index ( drop = True ) full_df [ 'team_id' ] = full_df . team_id . str . extract ( r'(\w+)\W*\(\d+\)' , expand = False ) full_df [ 'gb' ] = [ gb if isinstance ( gb , int ) or isinstance ( gb , float ) else 0 for gb in full_df [ 'gb' ] ] full_df = full_df . drop ( 'has_class_full_table' , axis = 1 ) expanded_table = doc ( 'table#expanded_standings' ) expanded_df = sportsref . utils . parse_table ( expanded_table ) full_df = pd . merge ( full_df , expanded_df , on = 'team_id' ) return full_df
def roy_voting ( self ) : url = '{}/awards/awards_{}.html' . format ( sportsref . nba . BASE_URL , self . yr ) doc = pq ( sportsref . utils . get_html ( url ) ) table = doc ( 'table#roy' ) df = sportsref . utils . parse_table ( table ) return df
def linescore ( self ) : doc = self . get_main_doc ( ) table = doc ( 'table#line_score' ) columns = [ th . text ( ) for th in table ( 'tr.thead' ) . items ( 'th' ) ] columns [ 0 ] = 'team_id' data = [ [ sportsref . utils . flatten_links ( td ) for td in tr ( 'td' ) . items ( ) ] for tr in table ( 'tr.thead' ) . next_all ( 'tr' ) . items ( ) ] return pd . DataFrame ( data , index = [ 'away' , 'home' ] , columns = columns , dtype = 'float' )
def get_class_instance_key ( cls , args , kwargs ) : l = [ id ( cls ) ] for arg in args : l . append ( id ( arg ) ) l . extend ( ( k , id ( v ) ) for k , v in kwargs . items ( ) ) return tuple ( sorted ( l ) )
def stats_per_game ( self , kind = 'R' , summary = False ) : return self . _get_stats_table ( 'per_game' , kind = kind , summary = summary )
def stats_totals ( self , kind = 'R' , summary = False ) : return self . _get_stats_table ( 'totals' , kind = kind , summary = summary )
def stats_per36 ( self , kind = 'R' , summary = False ) : return self . _get_stats_table ( 'per_minute' , kind = kind , summary = summary )
def stats_per100 ( self , kind = 'R' , summary = False ) : return self . _get_stats_table ( 'per_poss' , kind = kind , summary = summary )
def stats_advanced ( self , kind = 'R' , summary = False ) : return self . _get_stats_table ( 'advanced' , kind = kind , summary = summary )
def stats_shooting ( self , kind = 'R' , summary = False ) : return self . _get_stats_table ( 'shooting' , kind = kind , summary = summary )
def stats_pbp ( self , kind = 'R' , summary = False ) : return self . _get_stats_table ( 'advanced_pbp' , kind = kind , summary = summary )
def GamePlayFinder ( * * kwargs ) : querystring = _kwargs_to_qs ( * * kwargs ) url = '{}?{}' . format ( GPF_URL , querystring ) if kwargs . get ( 'verbose' , False ) : print ( url ) html = utils . get_html ( url ) doc = pq ( html ) table = doc ( 'table#all_plays' ) plays = utils . parse_table ( table ) if 'score' in plays . columns : oScore , dScore = zip ( * plays . score . apply ( lambda s : s . split ( '-' ) ) ) plays [ 'teamScore' ] = oScore plays [ 'oppScore' ] = dScore if 'description' in plays . columns : plays = pbp . expand_details ( plays , detailCol = 'description' ) return plays
def get ( self ) : self . write ( "Memory Session Object Demo:" ) if "sv" in self . session : current_value = self . session [ "sv" ] self . write ( "current sv value is %s, and system will delete this value.<br/>" % self . session [ "sv" ] ) self . session . delete ( "sv" ) if "sv" not in self . session : self . write ( "current sv value is empty" ) else : self . write ( "Session data not found" )
def PlayerSeasonFinder ( * * kwargs ) : if 'offset' not in kwargs : kwargs [ 'offset' ] = 0 playerSeasons = [ ] while True : querystring = _kwargs_to_qs ( * * kwargs ) url = '{}?{}' . format ( PSF_URL , querystring ) if kwargs . get ( 'verbose' , False ) : print ( url ) html = utils . get_html ( url ) doc = pq ( html ) table = doc ( 'table#results' ) df = utils . parse_table ( table ) if df . empty : break thisSeason = list ( zip ( df . player_id , df . year ) ) playerSeasons . extend ( thisSeason ) if doc ( '*:contains("Next Page")' ) : kwargs [ 'offset' ] += 100 else : break return playerSeasons
def wait ( self ) : with self . cvar : self . count . value += 1 self . cvar . notify_all ( ) while self . count . value < self . n_procs : self . cvar . wait ( )
def wait ( self ) : self . barrier_A . wait ( ) self . barrier_A , self . barrier_B = self . barrier_B , self . barrier_A self . barrier_A . reset ( )
def close ( self ) : self . read_queue . put ( QueueClosed ) self . write_queue . put ( QueueClosed )
def _read_varint ( self ) : buff = self . _fd . read ( 1 ) if buff == b'' : return 0 while ( bytearray ( buff ) [ - 1 ] & 0x80 ) >> 7 == 1 : new_byte = self . _fd . read ( 1 ) if new_byte == b'' : raise EOFError ( 'unexpected EOF.' ) buff += new_byte varint , _ = decodeVarint ( buff , 0 ) return varint
def close ( self ) : self . flush ( ) if self . _myfd is not None : self . _myfd . close ( ) self . _myfd = None
def flush ( self ) : if not self . is_output ( ) : return count = len ( self . _write_buff ) if count == 0 : return encodeVarint ( self . _fd . write , count , True ) for obj in self . _write_buff : obj_str = obj . SerializeToString ( ) encodeVarint ( self . _fd . write , len ( obj_str ) , True ) self . _fd . write ( obj_str ) self . _write_buff = [ ]
def get_game_dir ( self , username = False ) : if not self . common and not username : raise RuntimeError ( "Can't determine this game's directory without username" ) if self . common : subdir = "common" else : subdir = "username" subsubdir = self . dir if WIN32 or CYGWIN : subsubdir = subsubdir . lower ( ) return os . path . join ( subdir , subsubdir )
def with_ignored_exceptions ( self , * ignored_exceptions ) : for exception in ignored_exceptions : self . _ignored_exceptions = self . _ignored_exceptions + ( exception , ) return self
def _send ( self , message , read_reply = False ) : sock = None for tries in range ( 0 , 3 ) : try : sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) sock . connect ( ( self . _host , self . PORT ) ) break except ( ConnectionError , BrokenPipeError ) : if tries == 3 : print ( "socket connect failed." ) return sleep ( 0.1 ) sock . send ( codecs . decode ( message , 'hex_codec' ) ) if read_reply : sleep ( 0.1 ) reply = '' tries = 0 max_tries = 20 while len ( reply ) < len ( message ) and tries < max_tries : try : reply += codecs . encode ( sock . recv ( self . BUFFERSIZE ) , 'hex' ) . decode ( "utf-8" ) except ( ConnectionError , BrokenPipeError ) : pass tries += 1 sock . close ( ) if tries >= max_tries : return return reply sock . close ( )
def power_off ( self ) : status = self . status ( ) if status [ 'power' ] : self . _send ( self . CMD_POWERSAVE + self . CMD_OFF )
def power_on ( self ) : status = self . status ( ) if not status [ 'power' ] : self . _send ( self . CMD_ON , read_reply = True ) sleep ( 0.5 )
def set_volume ( self , volume ) : if 0 <= volume <= 200 : volume = format ( volume , "02x" ) self . _send ( self . CMD_VOLUME + volume )
def select_source ( self , source ) : status = self . status ( ) if status [ 'power' ] : if status [ 'source' ] != source : if source in self . SOURCES : self . _send ( self . CMD_SOURCE + self . SOURCES [ source ] , read_reply = True )
def exec_command ( self , domain , function , operator , value = None ) : if operator in CMDS [ domain ] [ function ] [ 'supported_operators' ] : if operator is '=' and value is None : raise ValueError ( 'No value provided' ) if value is None : cmd = '' . join ( [ CMDS [ domain ] [ function ] [ 'cmd' ] , operator ] ) else : cmd = '' . join ( [ CMDS [ domain ] [ function ] [ 'cmd' ] , operator , str ( value ) ] ) else : raise ValueError ( 'Invalid operator provided %s' % operator ) if self . _open_connection ( ) : self . telnet . write ( ( '' . join ( [ '\r' , cmd , '\n' ] ) . encode ( ) ) ) loop = 3 while loop : msg = self . telnet . read_until ( '\n' . encode ( ) , self . timeout ) if msg == "" : loop -= 1 continue msg = msg . decode ( ) . strip ( '\r\n' ) #print("NAD reponded with '%s'" % msg) if msg . strip ( ) . split ( '=' ) [ 0 ] . lower ( ) == '.' . join ( [ domain , function ] ) . lower ( ) : return msg . strip ( ) . split ( '=' ) [ 1 ] raise RuntimeError ( 'Failed to read response' ) raise RuntimeError ( 'Failed to open connection' )
def _crc ( plaintext ) : if not isinstance ( plaintext , six . binary_type ) : plaintext = six . b ( plaintext ) return ( zlib . crc32 ( plaintext ) % 2147483647 ) & 0xffffffff
def missing_schema ( self , html , song_name ) : #html=self.get_html_response(url) soup = BeautifulSoup ( html ) name = ' ' . join ( song_name ) print '%s not found' % name print "But you can download any of the following songs :" a_list = soup . findAll ( 'a' , 'touch' ) for x in xrange ( len ( a_list ) - 1 ) : r = a_list [ x ] p = str ( r ) q = re . sub ( r'<a.*/>|<span.*">|</span>|</a>|<a.*html">|<font.*">|</font>' , '' , p ) print q
def list_of_all_href ( self , html ) : soup = BeautifulSoup ( html ) links = [ ] a_list = soup . findAll ( 'a' , 'touch' ) for x in xrange ( len ( a_list ) - 1 ) : link = a_list [ x ] . get ( 'href' ) name = a_list [ x ] name = str ( name ) name = re . sub ( r'<a.*/>|<span.*">|</span>|</a>|<a.*html">|<font.*">|</font>' , '' , name ) name = re . sub ( r'^[0-9]+\.' , '' , name ) links . append ( [ link , name ] ) #quit() return links
def check_if_song_name ( self , html ) : soup = BeautifulSoup ( html ) a_list = soup . findAll ( 'a' , 'touch' ) #print a_list text = [ str ( x ) for x in a_list ] text = '' . join ( text ) text = text . lower ( ) string1 = 'download in 48 kbps' string2 = 'download in 128 kbps' string3 = 'download in 320 kbps' href = '' if string3 in text : #print 'Downloading in 320 kbps' href = a_list [ 2 ] . get ( 'href' ) elif string2 in text : #print 'Downloading in 128 kbps' href = a_list [ 1 ] . get ( 'href' ) elif string1 in text : #print 'Downloading in 48 kbps'	 href = a_list [ 0 ] . get ( 'href' ) else : return ( True , 'nothing' ) return ( False , href )
def google_url ( self , song_name , website ) : name = '+' . join ( song_name ) prefix = 'https://www.google.co.in/search?q=' website = website . split ( " " ) suffix = '+' . join ( website ) url = prefix + name + suffix #print url return url
def get_html_response ( self , url ) : print "Downloading page %s .." % url try : response = requests . get ( url , timeout = 50 ) except requests . exceptions . SSLError : try : response = requests . get ( url , verify = False , timeout = 50 ) except requests . exceptions . RequestException as e : print e quit ( ) except requests . exceptions . RequestException as e : print e quit ( ) return response . content
def file_download_using_requests ( self , url ) : file_name = url . split ( '/' ) [ - 1 ] if os . path . exists ( os . path . join ( os . getcwd ( ) , file_name ) ) : print 'File already exists' return #print 'Downloading file %s '%file_name #print 'Downloading from %s'%url try : r = requests . get ( url , stream = True , timeout = 200 ) except requests . exceptions . SSLError : try : response = requests . get ( url , stream = True , verify = False , timeout = 200 ) except requests . exceptions . RequestException as e : print e quit ( ) except requests . exceptions . RequestException as e : print e quit ( ) chunk_size = 1024 total_size = int ( r . headers [ 'Content-Length' ] ) total_chunks = total_size / chunk_size file_iterable = r . iter_content ( chunk_size = chunk_size ) tqdm_iter = tqdm ( iterable = file_iterable , total = total_chunks , unit = 'KB' , leave = False ) with open ( file_name , 'wb' ) as f : for data in tqdm_iter : f . write ( data ) #total_size=float(r.headers['Content-Length'])/(1024*1024) print 'Downloaded file %s ' % file_name
def file_download_using_wget ( self , url ) : file_name = url . split ( '/' ) [ - 1 ] print 'Downloading file %s ' % file_name command = 'wget -c --read-timeout=50 --tries=3 -q --show-progress --no-check-certificate ' url = '"' + url + '"' command = command + url os . system ( command )
def main ( ) : #print VERSION from commands . download import Download options = docopt ( __doc__ , version = VERSION ) #print "You reached here" #print options print "working." p = Download ( options ) p . run ( )
def findStationCodesByCity ( city_name , token ) : req = requests . get ( API_ENDPOINT_SEARCH , params = { 'token' : token , 'keyword' : city_name } ) if req . status_code == 200 and req . json ( ) [ "status" ] == "ok" : return [ result [ "uid" ] for result in req . json ( ) [ "data" ] ] else : return [ ]
def get_location_observation ( lat , lng , token ) : req = requests . get ( API_ENDPOINT_GEO % ( lat , lng ) , params = { 'token' : token } ) if req . status_code == 200 and req . json ( ) [ "status" ] == "ok" : return parse_observation_response ( req . json ( ) [ "data" ] ) return { }
def parse_observation_response ( json ) : logging . debug ( json ) iaqi = json [ 'iaqi' ] result = { 'idx' : json [ 'idx' ] , 'city' : json . get ( 'city' , '' ) , 'aqi' : json [ 'aqi' ] , 'dominentpol' : json . get ( "dominentpol" , '' ) , 'time' : json [ 'time' ] [ 's' ] , 'iaqi' : [ { 'p' : item , 'v' : iaqi [ item ] [ 'v' ] } for item in iaqi ] } return result
def compilers ( self ) : return [ self . environment . compilers . get ( e ) for e in self . compiler_extensions ]
def mimetype ( self ) : return ( self . environment . mimetypes . get ( self . format_extension ) or self . compiler_mimetype or 'application/octet-stream' )
def compiler_mimetype ( self ) : for compiler in reversed ( self . compilers ) : if compiler . result_mimetype : return compiler . result_mimetype return None
def compiler_format_extension ( self ) : for extension , mimetype in self . environment . mimetypes . items ( ) : if mimetype == self . compiler_mimetype : return extension return None
def register ( self , mimetype , processor ) : if mimetype not in self or processor not in self [ mimetype ] : self . setdefault ( mimetype , [ ] ) . append ( processor )
def register_defaults ( self ) : self . mimetypes . register_defaults ( ) self . preprocessors . register_defaults ( ) self . postprocessors . register_defaults ( )
def table ( name , auth = None , eager = True ) : auth = auth or [ ] dynamodb = boto . connect_dynamodb ( * auth ) table = dynamodb . get_table ( name ) return Table ( table = table , eager = eager )
def tables ( auth = None , eager = True ) : auth = auth or [ ] dynamodb = boto . connect_dynamodb ( * auth ) return [ table ( t , auth , eager = eager ) for t in dynamodb . list_tables ( ) ]
def metadata_id ( item ) : if Crates . metadata_category ( item ) == CATEGORY_CRATES : return str ( item [ 'id' ] ) else : ts = item [ 'fetched_on' ] ts = str_to_datetime ( ts ) return str ( ts . timestamp ( ) )
def __fetch_crate_owner_team ( self , crate_id ) : raw_owner_team = self . client . crate_attribute ( crate_id , 'owner_team' ) owner_team = json . loads ( raw_owner_team ) return owner_team
def __fetch_crate_owner_user ( self , crate_id ) : raw_owner_user = self . client . crate_attribute ( crate_id , 'owner_user' ) owner_user = json . loads ( raw_owner_user ) return owner_user
def __fetch_crate_versions ( self , crate_id ) : raw_versions = self . client . crate_attribute ( crate_id , "versions" ) version_downloads = json . loads ( raw_versions ) return version_downloads
def __fetch_crate_version_downloads ( self , crate_id ) : raw_version_downloads = self . client . crate_attribute ( crate_id , "downloads" ) version_downloads = json . loads ( raw_version_downloads ) return version_downloads
def summary ( self ) : path = urijoin ( CRATES_API_URL , CATEGORY_SUMMARY ) raw_content = self . fetch ( path ) return raw_content
def crates ( self , from_page = 1 ) : path = urijoin ( CRATES_API_URL , CATEGORY_CRATES ) raw_crates = self . __fetch_items ( path , from_page ) return raw_crates
def crate ( self , crate_id ) : path = urijoin ( CRATES_API_URL , CATEGORY_CRATES , crate_id ) raw_crate = self . fetch ( path ) return raw_crate
def __fetch_items ( self , path , page = 1 ) : fetch_data = True parsed_crates = 0 total_crates = 0 while fetch_data : logger . debug ( "Fetching page: %i" , page ) try : payload = { 'sort' : 'alphabetical' , 'page' : page } raw_content = self . fetch ( path , payload = payload ) content = json . loads ( raw_content ) parsed_crates += len ( content [ 'crates' ] ) if not total_crates : total_crates = content [ 'meta' ] [ 'total' ] except requests . exceptions . HTTPError as e : logger . error ( "HTTP exception raised - %s" , e . response . text ) raise e yield raw_content page += 1 if parsed_crates >= total_crates : fetch_data = False
def fetch ( self , url , payload = None ) : response = super ( ) . fetch ( url , payload = payload ) return response . text
def get_questions ( self , offset = None ) : page = KitsuneClient . FIRST_PAGE if offset : page += int ( offset / KitsuneClient . ITEMS_PER_PAGE ) while True : api_questions_url = urijoin ( self . base_url , '/question' ) + '/' params = { "page" : page , "ordering" : "updated" } questions = self . fetch ( api_questions_url , params ) yield questions questions_json = json . loads ( questions ) next_uri = questions_json [ 'next' ] if not next_uri : break page += 1
def fetch ( self , url , params ) : logger . debug ( "Kitsune client calls API: %s params: %s" , url , str ( params ) ) response = super ( ) . fetch ( url , payload = params ) return response . text
def get_items ( self , category = CATEGORY_EVENT , offset = REMO_DEFAULT_OFFSET ) : more = True next_uri = None page = ReMoClient . FIRST_PAGE page += int ( offset / ReMoClient . ITEMS_PER_PAGE ) if category == CATEGORY_EVENT : api = self . api_events_url elif category == CATEGORY_ACTIVITY : api = self . api_activities_url elif category == CATEGORY_USER : api = self . api_users_url else : raise ValueError ( category + ' not supported in ReMo' ) while more : params = { "page" : page , "orderby" : "ASC" } logger . debug ( "ReMo client calls APIv2: %s params: %s" , api , str ( params ) ) raw_items = self . fetch ( api , payload = params ) yield raw_items items_data = json . loads ( raw_items ) next_uri = items_data [ 'next' ] if not next_uri : more = False else : parsed_uri = urllib . parse . urlparse ( next_uri ) parsed_params = urllib . parse . parse_qs ( parsed_uri . query ) page = parsed_params [ 'page' ] [ 0 ]
def io_priority ( self ) : return ( self . _iocb . aio_reqprio if self . _iocb . u . c . flags & libaio . IOCB_FLAG_IOPRIO else None )
def get_cells ( self ) : logger . info ( "Retrieving all cells spreadsheet data ..." ) logger . debug ( "MozillaClub client calls API: %s" , self . base_url ) raw_cells = self . fetch ( self . base_url ) return raw_cells . text
def parse ( self ) : nevents_wrong = 0 feed_json = json . loads ( self . feed ) if 'entry' not in feed_json [ 'feed' ] : return self . cells = feed_json [ 'feed' ] [ 'entry' ] self . ncell = 0 event_fields = self . __get_event_fields ( ) while self . ncell < len ( self . cells ) : event = self . __get_next_event ( event_fields ) if event [ 'Date of Event' ] is None or event [ 'Club Name' ] is None : logger . warning ( "Wrong event data: %s" , event ) nevents_wrong += 1 continue yield event logger . info ( "Total number of wrong events: %i" , nevents_wrong )
def get_data_files ( dirname ) : flist = [ ] for dirpath , _dirnames , filenames in os . walk ( dirname ) : for fname in filenames : flist . append ( osp . join ( dirpath , fname ) ) return flist
def export_formats ( self , pid_type ) : if pid_type not in self . _export_formats : fmts = self . app . config . get ( 'RECORDS_UI_EXPORT_FORMATS' , { } ) . get ( pid_type , { } ) self . _export_formats [ pid_type ] = sorted ( [ ( k , v ) for k , v in fmts . items ( ) if v ] , key = lambda x : x [ 1 ] [ 'order' ] , ) return self . _export_formats [ pid_type ]
def permission_factory ( self ) : if self . _permission_factory is None : imp = self . app . config [ 'RECORDS_UI_DEFAULT_PERMISSION_FACTORY' ] self . _permission_factory = obj_or_import_string ( imp ) return self . _permission_factory
def records ( ) : import uuid from invenio_records . api import Record from invenio_pidstore . models import PersistentIdentifier , PIDStatus with db . session . begin_nested ( ) : pid1 = PersistentIdentifier . create ( 'recid' , '1' , object_type = 'rec' , object_uuid = rec1_uuid , status = PIDStatus . REGISTERED ) Record . create ( { 'title' : 'Registered ' , 'authors' : [ { 'name' : 'Ellis Jonathan' } , { 'name' : 'Higgs Peter' } , ] , 'access' : 'open' , 'keywords' : [ 'CERN' , 'higgs' ] , } , id_ = rec1_uuid ) PersistentIdentifier . create ( 'recid' , '2' , object_type = 'rec' , object_uuid = rec2_uuid , status = PIDStatus . REGISTERED ) Record . create ( { 'title' : 'Registered ' , 'authors' : [ { 'name' : 'Ellis Jonathan' } , { 'name' : 'Higgs Peter' } , ] , 'access' : 'closed' , 'keywords' : [ 'CERN' , 'higgs' ] , } , id_ = rec2_uuid ) rec3_uuid = uuid . uuid4 ( ) pid = PersistentIdentifier . create ( 'recid' , '3' , object_type = 'rec' , object_uuid = rec3_uuid , status = PIDStatus . REGISTERED ) pid . delete ( ) Record . create ( { 'title' : 'Live ' } , id_ = rec3_uuid ) PersistentIdentifier . create ( 'recid' , '4' , status = PIDStatus . DELETED ) PersistentIdentifier . create ( 'recid' , '5' , status = PIDStatus . REGISTERED ) pid = PersistentIdentifier . create ( 'recid' , '6' , status = PIDStatus . REGISTERED ) pid . redirect ( pid1 ) doi = PersistentIdentifier . create ( 'doi' , '10.1234/foo' , status = PIDStatus . REGISTERED ) pid = PersistentIdentifier . create ( 'recid' , '7' , status = PIDStatus . REGISTERED ) pid . redirect ( doi ) PersistentIdentifier . create ( 'recid' , '8' , status = PIDStatus . RESERVED ) db . session . commit ( )
def time_callable ( self , name , target , rate = None , args = ( ) , kwargs = { } ) : assert callable ( target ) if rate is None : rate = self . _rate else : assert_sample_rate ( rate ) start_time = time ( ) result = target ( * args , * * kwargs ) self . since ( name , start_time , rate ) return result
def increment ( self , name , count = 1 , rate = 1 ) : if self . _should_send_metric ( name , rate ) : self . _request ( Counter ( self . _create_metric_name_for_request ( name ) , int ( count ) , rate ) . to_request ( ) )
def timing ( self , name , milliseconds , rate = 1 ) : if self . _should_send_metric ( name , rate ) : milliseconds = int ( milliseconds ) self . _request ( Timer ( self . _create_metric_name_for_request ( name ) , milliseconds , rate ) . to_request ( ) )
def timing_since ( self , name , start_time , rate = 1 ) : duration = 0 if isinstance ( start_time , datetime ) : duration = ( datetime . now ( start_time . tzinfo ) - start_time ) . total_seconds ( ) * 1000 elif is_numeric ( start_time ) : assert start_time > 0 duration = ( time ( ) - start_time ) * 1000 else : raise ValueError ( "start time should be a timestamp or a datetime" ) self . timing ( name , duration , rate )
def gauge ( self , name , value , rate = 1 ) : if self . _should_send_metric ( name , rate ) : if not is_numeric ( value ) : value = float ( value ) self . _request ( Gauge ( self . _create_metric_name_for_request ( name ) , value , rate ) . to_request ( ) )
def gauge_delta ( self , name , delta , rate = 1 ) : if self . _should_send_metric ( name , rate ) : if not is_numeric ( delta ) : delta = float ( delta ) self . _request ( GaugeDelta ( self . _create_metric_name_for_request ( name ) , delta , rate ) . to_request ( ) )
def set ( self , name , value , rate = 1 ) : if self . _should_send_metric ( name , rate ) : value = str ( value ) self . _request ( Set ( self . _create_metric_name_for_request ( name ) , value , rate ) . to_request ( ) )
def _request ( self , data ) : data = bytearray ( "{}\n" . format ( data ) . encode ( ) ) self . _prepare_batches_for_storage ( len ( data ) ) self . _batches [ - 1 ] . extend ( data )
def batch_client ( self , size = 512 ) : batch_client = BatchClient ( self . host , self . port , self . prefix , size ) self . _configure_client ( batch_client ) return batch_client
def unit_client ( self ) : client = Client ( self . host , self . port , self . prefix ) self . _configure_client ( client ) return client
def flush ( self ) : address = self . remote_address while len ( self . _batches ) > 0 : self . _socket . sendto ( self . _batches [ 0 ] , address ) self . _batches . popleft ( ) return self
def my_permission_factory ( record , * args , * * kwargs ) : def can ( self ) : rec = Record . get_record ( record . id ) return rec . get ( 'access' , '' ) == 'open' return type ( 'MyPermissionChecker' , ( ) , { 'can' : can } ) ( )
def batch_client ( self , size = 512 ) : batch_client = TCPBatchClient ( self . host , self . port , self . prefix , size ) self . _configure_client ( batch_client ) return batch_client
def flush ( self ) : while len ( self . _batches ) > 0 : self . _socket . sendall ( self . _batches [ 0 ] ) self . _batches . popleft ( ) return self
def unit_client ( self ) : client = TCPClient ( self . host , self . port , self . prefix ) self . _configure_client ( client ) return client
def convertAsOpenMath ( term , converter ) : if hasattr ( term , "_ishelper" ) and term . _ishelper or isinstance ( term , om . OMAny ) : return interpretAsOpenMath ( term ) if converter is not None : try : _converted = converter . to_openmath ( term ) except Exception as e : _converted = None if isinstance ( _converted , om . OMAny ) : return _converted return interpretAsOpenMath ( term )
def to_python ( self , omobj ) : if omobj . __class__ in self . _omclass_to_py : return self . _omclass_to_py [ omobj . __class__ ] ( omobj ) elif isinstance ( omobj , om . OMSymbol ) : return self . _lookup_to_python ( omobj . cdbase , omobj . cd , omobj . name ) elif isinstance ( omobj , om . OMApplication ) : elem = self . to_python ( omobj . elem ) arguments = [ self . to_python ( x ) for x in omobj . arguments ] return elem ( * arguments ) raise ValueError ( 'Cannot convert object of class %s to Python.' % omobj . __class__ . __name__ )
def to_openmath ( self , obj ) : for cl , conv in reversed ( self . _conv_to_om ) : if cl is None or isinstance ( obj , cl ) : try : return conv ( obj ) except CannotConvertError : continue if hasattr ( obj , '__openmath__' ) : return obj . __openmath__ ( ) raise ValueError ( 'Cannot convert %r to OpenMath.' % obj )
def init_app ( self , app ) : app . config . setdefault ( 'REDIS_URLS' , { 'main' : 'redis://localhost:6379/0' , 'admin' : 'redis://localhost:6379/1' , } ) app . before_request ( self . before_request ) self . app = app
def valid_choices ( choices ) : for key , value in choices : if isinstance ( value , ( list , tuple ) ) : for key , _ in value : yield key else : yield key
def split_model_kwargs ( kw ) : from collections import defaultdict model_fields = { } fields_agrs = defaultdict ( lambda : { } ) for key in kw . keys ( ) : if '__' in key : field , _ , subfield = key . partition ( '__' ) fields_agrs [ field ] [ subfield ] = kw [ key ] else : model_fields [ key ] = kw [ key ] return model_fields , fields_agrs
def any_form_default ( form_cls , * * kwargs ) : form_data = { } form_files = { } form_fields , fields_args = split_model_kwargs ( kwargs ) for name , field in form_cls . base_fields . iteritems ( ) : if name in form_fields : form_data [ name ] = kwargs [ name ] else : form_data [ name ] = any_form_field ( field , * * fields_args [ name ] ) return form_data , form_files
def field_choices_attibute ( function ) : def _wrapper ( field , * * kwargs ) : if hasattr ( field . widget , 'choices' ) : return random . choice ( list ( valid_choices ( field . widget . choices ) ) ) return function ( field , * * kwargs ) return _wrapper
def model_choice_field_data ( field , * * kwargs ) : data = list ( field . queryset [ : 10 ] ) if data : return random . choice ( data ) else : raise TypeError ( 'No %s available in queryset' % field . queryset . model )
def tag ( version = __version__ ) : build = local ( "git tag {0}" . format ( version ) ) if build . succeeded : local ( "git push --tags" )
def any_field_blank ( function ) : def wrapper ( field , * * kwargs ) : if kwargs . get ( 'isnull' , False ) : return None if field . blank and random . random < 0.1 : return None return function ( field , * * kwargs ) return wrapper
def any_file_field ( field , * * kwargs ) : def get_some_file ( path ) : subdirs , files = field . storage . listdir ( path ) if files : result_file = random . choice ( files ) instance = field . storage . open ( "%s/%s" % ( path , result_file ) ) . file return FieldFile ( instance , field , result_file ) for subdir in subdirs : result = get_some_file ( "%s/%s" % ( path , subdir ) ) if result : return result result = get_some_file ( field . upload_to ) if result is None and not field . null : raise TypeError ( "Can't found file in %s for non nullable FileField" % field . upload_to ) return result
def any_filepath_field ( field , * * kwargs ) : def get_some_file ( path ) : subdirs , files = [ ] , [ ] for entry in os . listdir ( path ) : entry_path = os . path . join ( path , entry ) if os . path . isdir ( entry_path ) : subdirs . append ( entry_path ) else : if not field . match or re . match ( field . match , entry ) : files . append ( entry_path ) if files : return random . choice ( files ) if field . recursive : for subdir in subdirs : result = get_some_file ( subdir ) if result : return result result = get_some_file ( field . path ) if result is None and not field . null : raise TypeError ( "Can't found file in %s for non nullable FilePathField" % field . path ) return result
def decode ( data ) : data = bytearray ( data ) result = bytearray ( ) pos = 0 while pos < len ( data ) : header_byte = data [ pos ] if header_byte > 127 : header_byte -= 256 pos += 1 if 0 <= header_byte <= 127 : result . extend ( data [ pos : pos + header_byte + 1 ] ) pos += header_byte + 1 elif header_byte == - 128 : pass else : result . extend ( [ data [ pos ] ] * ( 1 - header_byte ) ) pos += 1 return bytes ( result )
def encode ( data ) : if len ( data ) == 0 : return data if len ( data ) == 1 : return b'\x00' + data data = bytearray ( data ) result = bytearray ( ) buf = bytearray ( ) pos = 0 repeat_count = 0 MAX_LENGTH = 127 state = 'RAW' def finish_raw ( ) : if len ( buf ) == 0 : return result . append ( len ( buf ) - 1 ) result . extend ( buf ) buf [ : ] = bytearray ( ) def finish_rle ( ) : result . append ( 256 - ( repeat_count - 1 ) ) result . append ( data [ pos ] ) while pos < len ( data ) - 1 : current_byte = data [ pos ] if data [ pos ] == data [ pos + 1 ] : if state == 'RAW' : finish_raw ( ) state = 'RLE' repeat_count = 1 elif state == 'RLE' : if repeat_count == MAX_LENGTH : finish_rle ( ) repeat_count = 0 repeat_count += 1 else : if state == 'RLE' : repeat_count += 1 finish_rle ( ) state = 'RAW' repeat_count = 0 elif state == 'RAW' : if len ( buf ) == MAX_LENGTH : finish_raw ( ) buf . append ( current_byte ) pos += 1 if state == 'RAW' : buf . append ( data [ pos ] ) finish_raw ( ) else : repeat_count += 1 finish_rle ( ) return bytes ( result )
def add ( self , name , path ) : if not ( os . path . exists ( path ) ) : raise ValueError ( "Workspace path `%s` doesn't exists." % path ) if ( self . exists ( name ) ) : raise ValueError ( "Workspace `%s` already exists." % name ) self . config [ "workspaces" ] [ name ] = { "path" : path , "repositories" : { } } self . config . write ( )
def remove ( self , name ) : if not ( self . exists ( name ) ) : raise ValueError ( "Workspace `%s` doesn't exists." % name ) self . config [ "workspaces" ] . pop ( name , 0 ) self . config . write ( )
def list ( self ) : ws_list = { } for key , value in self . config [ "workspaces" ] . items ( ) : ws_list [ key ] = dict ( { "name" : key } , * * value ) return ws_list
def repository_exists ( self , workspace , repo ) : if not self . exists ( workspace ) : return False workspaces = self . list ( ) return repo in workspaces [ workspace ] [ "repositories" ]
def sync ( self , ws_name ) : path = self . config [ "workspaces" ] [ ws_name ] [ "path" ] repositories = self . config [ "workspaces" ] [ ws_name ] [ "repositories" ] logger = logging . getLogger ( __name__ ) color = Color ( ) for r in os . listdir ( path ) : try : repo = Repository ( os . path . join ( path , r ) ) except RepositoryError : continue else : repositories [ r ] = repo . path for repo_name , path in repositories . items ( ) : logger . info ( color . colored ( " - %s" % repo_name , "blue" ) ) self . config [ "workspaces" ] [ ws_name ] [ "repositories" ] self . config . write ( )
def clone ( url , path ) : adapter = None if url [ : 4 ] == "git@" or url [ - 4 : ] == ".git" : adapter = Git ( path ) if url [ : 6 ] == "svn://" : adapter = Svn ( path ) if url [ : 6 ] == "bzr://" : adapter = Bzr ( path ) if url [ : 9 ] == "ssh://hg@" : adapter = Hg ( path ) if adapter is None : raise RepositoryAdapterNotFound ( "Can't find adapter for `%s` repository url" % url ) return adapter . clone ( url )
def check_version ( ) : import requests r = requests . get ( 'https://pypi.python.org/pypi/ndio/json' ) . json ( ) r = r [ 'info' ] [ 'version' ] if r != version : print ( "A newer version of ndio is available. " + "'pip install -U ndio' to update." ) return r
def execute ( self , args ) : if args . name is not None : self . print_workspace ( args . name ) elif args . all is not None : self . print_all ( )
def print_update ( self , repo_name , repo_path ) : color = Color ( ) self . logger . info ( color . colored ( "=> [%s] %s" % ( repo_name , repo_path ) , "green" ) ) try : repo = Repository ( repo_path ) repo . update ( ) except RepositoryError as e : self . logger . error ( e ) pass print ( "\n" )
def set_console_handler ( self , debug = False ) : console = logging . StreamHandler ( ) console . setFormatter ( Formatter ( LFORMAT ) ) if not debug : console . setLevel ( logging . INFO ) self . addHandler ( console )
def execute ( self , command , path = None ) : logger = logging . getLogger ( __name__ ) self . check_executable ( ) logger . debug ( "Executing command `%s` (cwd: %s)" % ( command , path ) ) process = subprocess . Popen ( command , shell = True , cwd = path , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) stdout , stderr = process . communicate ( ) exit_code = process . wait ( ) if stdout : logger . info ( stdout . decode ( "utf-8" ) ) if stderr : if exit_code != 0 : logger . error ( stderr . decode ( "utf-8" ) ) else : logger . info ( stderr . decode ( "utf-8" ) ) return process
def print_workspace ( self , name ) : path_list = find_path ( name , self . config ) if len ( path_list ) == 0 : self . logger . error ( "No matches for `%s`" % name ) return False for name , path in path_list . items ( ) : self . print_status ( name , path )
def print_status ( self , repo_name , repo_path ) : color = Color ( ) self . logger . info ( color . colored ( "=> [%s] %s" % ( repo_name , repo_path ) , "green" ) ) try : repo = Repository ( repo_path ) repo . status ( ) except RepositoryError as e : self . logger . error ( e ) pass print ( "\n" )
def _post_cutout_no_chunking_blosc ( self , token , channel , x_start , y_start , z_start , data , resolution ) : data = numpy . expand_dims ( data , axis = 0 ) blosc_data = blosc . pack_array ( data ) url = self . url ( "{}/{}/blosc/{}/{},{}/{},{}/{},{}/0,0/" . format ( token , channel , resolution , x_start , x_start + data . shape [ 3 ] , y_start , y_start + data . shape [ 2 ] , z_start , z_start + data . shape [ 1 ] ) ) req = self . remote_utils . post_url ( url , data = blosc_data , headers = { 'Content-Type' : 'application/octet-stream' } ) if req . status_code is not 200 : raise RemoteDataUploadError ( req . text ) else : return True
def clone ( self , url ) : return self . execute ( "%s branch %s %s" % ( self . executable , url , self . path ) )
def get_version ( ) : requirement = pkg_resources . Requirement . parse ( "yoda" ) provider = pkg_resources . get_provider ( requirement ) return provider . version
def mix_and_match ( name , greeting = 'Hello' , yell = False ) : say = '%s, %s' % ( greeting , name ) if yell : print '%s!' % say . upper ( ) else : print '%s.' % say
def option_decorator ( name , greeting , yell ) : say = '%s, %s' % ( greeting , name ) if yell : print '%s!' % say . upper ( ) else : print '%s.' % say
def parse ( self ) : parser = self . subparser . add_parser ( "show" , help = "Show workspace details" , description = "Show workspace details." ) group = parser . add_mutually_exclusive_group ( required = True ) group . add_argument ( '--all' , action = 'store_true' , help = "All workspaces" ) group . add_argument ( 'name' , type = str , help = "Workspace name" , nargs = '?' )
def execute ( self , args ) : if args . name is not None : self . show_workspace ( slashes2dash ( args . name ) ) elif args . all is not None : self . show_all ( )
def show_workspace ( self , name ) : if not self . workspace . exists ( name ) : raise ValueError ( "Workspace `%s` doesn't exists." % name ) color = Color ( ) workspaces = self . workspace . list ( ) self . logger . info ( "<== %s workspace ==>" % color . colored ( name , "green" ) ) self . logger . info ( "\tPath: %s" % workspaces [ name ] [ "path" ] ) self . logger . info ( "\tNumber of repositories: %s" % color . colored ( len ( workspaces [ name ] [ "repositories" ] ) , "yellow" ) ) repo_colored = color . colored ( "Repositories" , "blue" ) path_colored = color . colored ( "Path" , "blue" ) trepositories = PrettyTable ( [ repo_colored , path_colored , color . colored ( "+" , "blue" ) ] ) trepositories . align [ repo_colored ] = "l" trepositories . align [ path_colored ] = "l" for repo_name in workspaces [ name ] [ "repositories" ] : fullname = "%s/%s" % ( name , repo_name ) fullpath = find_path ( fullname , self . config ) [ fullname ] try : repo = Repository ( fullpath ) repo_scm = repo . get_scm ( ) except RepositoryAdapterNotFound : repo_scm = None trepositories . add_row ( [ color . colored ( repo_name , "cyan" ) , fullpath , repo_scm ] ) self . logger . info ( trepositories )
def show_all ( self ) : for ws in self . workspace . list ( ) . keys ( ) : self . show_workspace ( ws ) print ( "\n\n" )
def RAMON ( typ ) : if six . PY2 : lookup = [ str , unicode ] elif six . PY3 : lookup = [ str ] if type ( typ ) is int : return _ramon_types [ typ ] elif type ( typ ) in lookup : return _ramon_types [ _types [ typ ] ]
def nd_json ( self , dataset , project , channel_list , metadata ) : nd_dict = { } nd_dict [ 'dataset' ] = self . dataset_dict ( * dataset ) nd_dict [ 'project' ] = self . project_dict ( * project ) nd_dict [ 'metadata' ] = metadata nd_dict [ 'channels' ] = { } for channel_name , value in channel_list . items ( ) : nd_dict [ 'channels' ] [ channel_name ] = self . channel_dict ( * value ) return json . dumps ( nd_dict , sort_keys = True , indent = 4 )
def dataset_dict ( self , dataset_name , imagesize , voxelres , offset , timerange , scalinglevels , scaling ) : dataset_dict = { } dataset_dict [ 'dataset_name' ] = dataset_name dataset_dict [ 'imagesize' ] = imagesize dataset_dict [ 'voxelres' ] = voxelres if offset is not None : dataset_dict [ 'offset' ] = offset if timerange is not None : dataset_dict [ 'timerange' ] = timerange if scalinglevels is not None : dataset_dict [ 'scalinglevels' ] = scalinglevels if scaling is not None : dataset_dict [ 'scaling' ] = scaling return dataset_dict
def channel_dict ( self , channel_name , datatype , channel_type , data_url , file_format , file_type , exceptions , resolution , windowrange , readonly ) : channel_dict = { } channel_dict [ 'channel_name' ] = channel_name channel_dict [ 'datatype' ] = datatype channel_dict [ 'channel_type' ] = channel_type if exceptions is not None : channel_dict [ 'exceptions' ] = exceptions if resolution is not None : channel_dict [ 'resolution' ] = resolution if windowrange is not None : channel_dict [ 'windowrange' ] = windowrange if readonly is not None : channel_dict [ 'readonly' ] = readonly channel_dict [ 'data_url' ] = data_url channel_dict [ 'file_format' ] = file_format channel_dict [ 'file_type' ] = file_type return channel_dict
def project_dict ( self , project_name , token_name , public ) : project_dict = { } project_dict [ 'project_name' ] = project_name if token_name is not None : if token_name == '' : project_dict [ 'token_name' ] = project_name else : project_dict [ 'token_name' ] = token_name else : project_dict [ 'token_name' ] = project_name if public is not None : project_dict [ 'public' ] = public return project_dict
def identify_imagesize ( self , image_type , image_path = '/tmp/img.' ) : dims = ( ) try : if ( image_type . lower ( ) == 'png' ) : dims = np . shape ( ndpng . load ( '{}{}' . format ( image_path , image_type ) ) ) elif ( image_type . lower ( ) == 'tif' or image_type . lower ( ) == 'tiff' ) : dims = np . shape ( ndtiff . load ( '{}{}' . format ( image_path , image_type ) ) ) else : raise ValueError ( "Unsupported image type." ) except : raise OSError ( 'The file was not accessible at {}{}' . format ( image_path , image_type ) ) return dims [ : : - 1 ]
def put_data ( self , data ) : URLPath = self . oo . url ( "autoIngest/" ) try : response = requests . post ( URLPath , data = json . dumps ( data ) , verify = False ) assert ( response . status_code == 200 ) print ( "From ndio: {}" . format ( response . content ) ) except : raise OSError ( . format ( response . status_code ) )
def find_path ( name , config , wsonly = False ) : workspace = Workspace ( config ) config = config [ "workspaces" ] path_list = { } if name . find ( '/' ) != - 1 : wsonly = False try : ws , repo = name . split ( '/' ) except ValueError : raise ValueError ( "There is too many / in `name` argument. " "Argument syntax: `workspace/repository`." ) if ( workspace . exists ( ws ) ) : if ( repo in config [ ws ] [ "repositories" ] ) : path_name = "%s/%s" % ( ws , repo ) path_list [ path_name ] = config [ ws ] [ "repositories" ] [ repo ] for ws_name , ws in sorted ( config . items ( ) ) : if ( name == ws_name ) : if wsonly is True : return { ws_name : ws [ "path" ] } repositories = sorted ( config [ ws_name ] [ "repositories" ] . items ( ) ) for name , path in repositories : path_list [ "%s/%s" % ( ws_name , name ) ] = path break for repo_name , repo_path in sorted ( ws [ "repositories" ] . items ( ) ) : if ( repo_name == name ) : path_list [ "%s/%s" % ( ws_name , repo_name ) ] = repo_path return path_list
def nvim_io_recover ( self , io : NvimIORecover [ A ] ) -> NvimIO [ B ] : return eval_step ( self . vim ) ( io . map ( lambda a : a ) )
def ugettext ( message , context = None ) : stripped = strip_whitespace ( message ) message = add_context ( context , stripped ) if context else stripped ret = django_ugettext ( message ) return stripped if ret == message else ret
def ungettext ( singular , plural , number , context = None ) : singular_stripped = strip_whitespace ( singular ) plural_stripped = strip_whitespace ( plural ) if context : singular = add_context ( context , singular_stripped ) plural = add_context ( context , plural_stripped ) else : singular = singular_stripped plural = plural_stripped ret = django_nugettext ( singular , plural , number ) if ret == singular : return singular_stripped elif ret == plural : return plural_stripped return ret
def install_jinja_translations ( ) : class Translation ( object ) : ugettext = staticmethod ( ugettext ) ungettext = staticmethod ( ungettext ) import jingo jingo . env . install_gettext_translations ( Translation )
def exclusive_ns ( guard : StateGuard [ A ] , desc : str , thunk : Callable [ ... , NS [ A , B ] ] , * a : Any ) -> Do : yield guard . acquire ( ) log . debug2 ( lambda : f'exclusive: {desc}' ) state , response = yield N . ensure_failure ( thunk ( * a ) . run ( guard . state ) , guard . release ) yield N . delay ( lambda v : unsafe_update_state ( guard , state ) ) yield guard . release ( ) log . debug2 ( lambda : f'release: {desc}' ) yield N . pure ( response )
def _percent ( data , part , total ) : try : return round ( 100 * float ( data [ part ] ) / float ( data [ total ] ) , 1 ) except ZeroDivisionError : return 0
def _get_cache_stats ( server_name = None ) : server_info = { } for svr in mc_client . get_stats ( ) : svr_info = svr [ 0 ] . split ( ' ' ) svr_name = svr_info [ 0 ] svr_stats = svr [ 1 ] svr_stats [ 'bytes_percent' ] = _percent ( svr_stats , 'bytes' , 'limit_maxbytes' ) svr_stats [ 'get_hit_rate' ] = _percent ( svr_stats , 'get_hits' , 'cmd_get' ) svr_stats [ 'get_miss_rate' ] = _percent ( svr_stats , 'get_misses' , 'cmd_get' ) if server_name and server_name == svr_name : return svr_stats server_info [ svr_name ] = svr_stats return server_info
def _get_cache_slabs ( server_name = None ) : server_info = { } for svr in mc_client . get_slabs ( ) : svr_info = svr [ 0 ] . split ( ' ' ) svr_name = svr_info [ 0 ] if server_name and server_name == svr_name : return svr [ 1 ] server_info [ svr_name ] = svr [ 1 ] return server_info
def _context_data ( data , request = None ) : try : return dict ( site . each_context ( request ) . items ( ) + data . items ( ) ) except AttributeError : return data
def server_status ( request ) : data = { 'cache_stats' : _get_cache_stats ( ) , 'can_get_slabs' : hasattr ( mc_client , 'get_slabs' ) , } return render_to_response ( 'memcache_admin/server_status.html' , data , RequestContext ( request ) )
def dashboard ( request ) : if not isinstance ( mc_client , dict ) : cache_stats = _get_cache_stats ( ) else : cache_stats = None if cache_stats : data = _context_data ( { 'title' : _ ( 'Memcache Dashboard' ) , 'cache_stats' : cache_stats , 'can_get_slabs' : hasattr ( mc_client , 'get_slabs' ) , 'REFRESH_RATE' : SETTINGS [ 'REFRESH_RATE' ] , } , request ) template = 'memcache_admin/dashboard.html' else : data = _context_data ( { 'title' : _ ( 'Memcache Dashboard - Error' ) , 'error_message' : _ ( 'Unable to connect to a memcache server.' ) , } , request ) template = 'memcache_admin/dashboard_error.html' return render_to_response ( template , data , RequestContext ( request ) )
def stats ( request , server_name ) : server_name = server_name . strip ( '/' ) data = _context_data ( { 'title' : _ ( 'Memcache Statistics for %s' ) % server_name , 'cache_stats' : _get_cache_stats ( server_name ) , } , request ) return render_to_response ( 'memcache_admin/stats.html' , data , RequestContext ( request ) )
def slabs ( request , server_name ) : data = _context_data ( { 'title' : _ ( 'Memcache Slabs for %s' ) % server_name , 'cache_slabs' : _get_cache_slabs ( server_name ) , } , request ) return render_to_response ( 'memcache_admin/slabs.html' , data , RequestContext ( request ) )
def human_bytes ( value ) : value = float ( value ) if value >= 1073741824 : gigabytes = value / 1073741824 size = '%.2f GB' % gigabytes elif value >= 1048576 : megabytes = value / 1048576 size = '%.2f MB' % megabytes elif value >= 1024 : kilobytes = value / 1024 size = '%.2f KB' % kilobytes else : size = '%.2f B' % value return size
def add ( self , * * kwargs ) : for key in kwargs : if type ( kwargs [ key ] ) == str : self . _children [ key ] = Directory ( kwargs [ key ] ) else : self . _children [ key ] = kwargs [ key ] self . _children [ key ] . _env = self self . _children [ key ] . apply_config ( ConfigApplicator ( self . config ) ) self . _children [ key ] . prepare ( )
def apply_config ( self , applicator ) : if type ( self . _fpath ) == str : self . _fpath = applicator . apply ( self . _fpath )
def path ( self ) : if self . _parent : return os . path . join ( self . _parent . path , self . _fpath ) else : return self . _fpath
def read ( self ) : with open ( self . path ) as f : d = f . read ( ) return d
def configure ( self ) : handler = logging . FileHandler ( self . path , delay = True ) if self . _format : handler . setFormatter ( logging . Formatter ( self . _format ) ) if type ( self . _formatter ) == str : if self . _env and self . _env . config . logging . dict_config . formatters [ self . _formatter ] : d = self . _env . config . logging . dict_config . formatters [ self . _formatter ] . to_dict ( ) handler . setFormatter ( logging . Formatter ( * * d ) ) elif type ( self . _formatter ) == dict : handler . setFormatter ( logging . Formatter ( * * self . _formatter ) ) if len ( self . _loggers ) : for name in self . _loggers : logging . getLogger ( name ) . addHandler ( handler ) else : logging . getLogger ( ) . addHandler ( handler )
def apply_config ( self , applicator ) : if type ( self . _path ) == str : self . _path = applicator . apply ( self . _path ) for key in self . _children : self . _children [ key ] . apply_config ( applicator )
def path ( self ) : p = '' if self . _parent and self . _parent . path : p = os . path . join ( p , self . _parent . path ) if self . _base : p = os . path . join ( p , self . _base ) if self . _path : p = os . path . join ( p , self . _path ) return p
def remove ( self , recursive = True , ignore_error = True ) : try : if recursive or self . _cleanup == 'recursive' : shutil . rmtree ( self . path ) else : os . rmdir ( self . path ) except Exception as e : if not ignore_error : raise e
def path_to ( self , path ) : return os . path . join ( self . path , str ( path ) )
def list ( self ) : return [ File ( f , parent = self ) for f in os . listdir ( self . path ) ]
def write ( self , filename , data , mode = 'w' ) : with open ( self . path_to ( str ( filename ) ) , mode ) as f : f . write ( data )
def read ( self , filename ) : with open ( self . path_to ( str ( filename ) ) ) as f : d = f . read ( ) return d
def add ( self , * args , * * kwargs ) : for key in kwargs : if isinstance ( kwargs [ key ] , str ) : self . _children [ key ] = File ( kwargs [ key ] ) else : self . _children [ key ] = kwargs [ key ] self . _children [ key ] . _parent = self self . _children [ key ] . _env = self . _env added = [ ] for arg in args : if isinstance ( arg , File ) : self . _children [ arg . name ] = arg self . _children [ arg . name ] . _parent = self self . _children [ arg . name ] . _env = self . _env elif isinstance ( arg , str ) : f = File ( arg ) added . append ( f ) self . _children [ arg ] = f self . _children [ arg ] . _parent = self self . _children [ arg ] . _env = self . _env else : raise TypeError ( type ( arg ) ) if len ( added ) == 1 : return added [ 0 ] if len ( args ) == 1 : return args [ 0 ]
def save ( self ) : with open ( self . path , 'w' ) as f : f . write ( yaml . dump ( dict ( self . d ) ) )
def load ( self ) : if os . path . exists ( self . path ) : with open ( self . path , 'r' ) as f : self . d = yaml . safe_load ( f . read ( ) . replace ( '\t' , ' ' * 4 ) )
def cleanup ( self ) : if os . path . exists ( self . path ) : os . remove ( self . path )
def _get_value ( self ) : if self . _path : try : container , last = self . _resolve_path ( ) return container [ last ] except KeyError : return None except IndexError : return None else : return self . _data
def load ( self , reload = False ) : if reload or not self . _loaded : if self . _defaults_file and type ( self . _defaults_file ) == str : self . _defaults_file = File ( self . _defaults_file , parent = self . _parent ) defaults = { } if self . _defaults_file : defaults = yaml . safe_load ( self . _defaults_file . read ( ) . replace ( '\t' , '    ' ) ) data = { } if self . exists : data = yaml . safe_load ( self . read ( ) . replace ( '\t' , '    ' ) ) self . _defaults = defaults self . _data = copy . deepcopy ( self . _defaults ) self . update ( data = data ) if self . _apply_env : self . update ( ConfigEnv ( self . _env_prefix ) ) self . _loaded = True return self
def apply_to_str ( self , obj ) : toks = re . split ( '({config:|})' , obj ) newtoks = [ ] try : while len ( toks ) : tok = toks . pop ( 0 ) if tok == '{config:' : var = toks . pop ( 0 ) val = self . config [ var ] if type ( val ) == ConfigNode and val == None : raise KeyError ( "No such config variable '{}'" . format ( var ) ) newtoks . append ( str ( val ) ) toks . pop ( 0 ) else : newtoks . append ( tok ) return '' . join ( newtoks ) except IndexError : pass return obj
def process_input ( self ) : try : pyngus . read_socket_input ( self . connection , self . socket ) except Exception as e : LOG . error ( "Exception on socket read: %s" , str ( e ) ) self . connection . close_input ( ) self . connection . close ( ) self . connection . process ( time . time ( ) )
def send_output ( self ) : try : pyngus . write_socket_output ( self . connection , self . socket ) except Exception as e : LOG . error ( "Exception on socket write: %s" , str ( e ) ) self . connection . close_output ( ) self . connection . close ( ) self . connection . process ( time . time ( ) )
def _send_request ( self ) : msg = Message ( ) msg . subject = "An RPC call!" msg . address = self . _to msg . reply_to = self . _reply_to msg . body = self . _method msg . correlation_id = 5 print ( "sending RPC call request: %s" % str ( self . _method ) ) self . _sender . send ( msg , self )
def configure ( self , target_address , source_address , handler , properties ) : self . _handler = handler self . _properties = properties dynamic_props = None if properties : dynamic_props = properties . get ( "dynamic-node-properties" ) mode = _dist_modes . get ( properties . get ( "distribution-mode" ) ) if mode is not None : self . _pn_link . source . distribution_mode = mode mode = _snd_settle_modes . get ( properties . get ( "snd-settle-mode" ) ) if mode is not None : self . _pn_link . snd_settle_mode = mode mode = _rcv_settle_modes . get ( properties . get ( "rcv-settle-mode" ) ) if mode is not None : self . _pn_link . rcv_settle_mode = mode if target_address is None : if not self . _pn_link . is_sender : raise Exception ( "Dynamic target not allowed" ) self . _pn_link . target . dynamic = True if dynamic_props : self . _pn_link . target . properties . clear ( ) self . _pn_link . target . properties . put_dict ( dynamic_props ) elif target_address : self . _pn_link . target . address = target_address if source_address is None : if not self . _pn_link . is_receiver : raise Exception ( "Dynamic source not allowed" ) self . _pn_link . source . dynamic = True if dynamic_props : self . _pn_link . source . properties . clear ( ) self . _pn_link . source . properties . put_dict ( dynamic_props ) elif source_address : self . _pn_link . source . address = source_address
def source_address ( self ) : if self . _pn_link . is_sender : return self . _pn_link . source . address else : return self . _pn_link . remote_source . address
def target_address ( self ) : if self . _pn_link . is_receiver : return self . _pn_link . target . address else : return self . _pn_link . remote_target . address
def _session_closed ( self ) : if self . _endpoint_state & proton . Endpoint . REMOTE_ACTIVE : self . _process_remote_state ( ) elif self . _endpoint_state & proton . Endpoint . REMOTE_UNINIT : self . _failed = True self . _link_failed ( "Parent session closed." )
def reject ( self , pn_condition = None ) : self . _pn_link . source . type = proton . Terminus . UNSPECIFIED super ( SenderLink , self ) . reject ( pn_condition )
def _process_delivery ( self , pn_delivery ) : if pn_delivery . tag in self . _send_requests : if pn_delivery . settled or pn_delivery . remote_state : outcome = pn_delivery . remote_state state = SenderLink . _DISPOSITION_STATE_MAP . get ( outcome , self . UNKNOWN ) pn_disposition = pn_delivery . remote info = { } if state == SenderLink . REJECTED : if pn_disposition . condition : info [ "condition" ] = pn_disposition . condition elif state == SenderLink . MODIFIED : info [ "delivery-failed" ] = pn_disposition . failed info [ "undeliverable-here" ] = pn_disposition . undeliverable annotations = pn_disposition . annotations if annotations : info [ "message-annotations" ] = annotations send_req = self . _send_requests . pop ( pn_delivery . tag ) send_req . destroy ( state , info ) pn_delivery . settle ( ) elif pn_delivery . writable : if self . _pending_sends : tag = self . _pending_sends . popleft ( ) send_req = self . _send_requests [ tag ] self . _write_msg ( pn_delivery , send_req ) else : LOG . debug ( "Delivery ignored, tag=%s" , str ( pn_delivery . tag ) ) pn_delivery . settle ( )
def reject ( self , pn_condition = None ) : self . _pn_link . target . type = proton . Terminus . UNSPECIFIED super ( ReceiverLink , self ) . reject ( pn_condition )
def _process_delivery ( self , pn_delivery ) : if pn_delivery . readable and not pn_delivery . partial : data = self . _pn_link . recv ( pn_delivery . pending ) msg = proton . Message ( ) msg . decode ( data ) self . _pn_link . advance ( ) if self . _handler : handle = "rmsg-%s:%x" % ( self . _name , self . _next_handle ) self . _next_handle += 1 self . _unsettled_deliveries [ handle ] = pn_delivery with self . _callback_lock : self . _handler . message_received ( self , msg , handle ) else : pn_delivery . settle ( )
def new_sender ( self , name ) : pn_link = self . _pn_session . sender ( name ) return self . request_sender ( pn_link )
def request_sender ( self , pn_link ) : sl = SenderLink ( self . _connection , pn_link ) self . _links . add ( sl ) return sl
def new_receiver ( self , name ) : pn_link = self . _pn_session . receiver ( name ) return self . request_receiver ( pn_link )
def request_receiver ( self , pn_link ) : rl = ReceiverLink ( self . _connection , pn_link ) self . _links . add ( rl ) return rl
def link_destroyed ( self , link ) : self . _links . discard ( link ) if not self . _links : LOG . debug ( "destroying unneeded session" ) self . _pn_session . close ( ) self . _pn_session . free ( ) self . _pn_session = None self . _connection = None
def _ep_need_close ( self ) : LOG . debug ( "Session %s close requested - closing..." , self . _name ) links = self . _links . copy ( ) for link in links : link . _session_closed ( )
def extendMarkdown ( self , md , md_globals ) : mark_tag = SimpleTagPattern ( MARK_RE , 'mark' ) md . inlinePatterns . add ( 'mark' , mark_tag , '_begin' )
def receiver_remote_closed ( self , receiver_link , pn_condition ) : LOG . debug ( "receiver_remote_closed condition=%s" , pn_condition ) receiver_link . close ( ) self . done = True
def receiver_failed ( self , receiver_link , error ) : LOG . warn ( "receiver_failed error=%s" , error ) receiver_link . close ( ) self . done = True
def get_host_port ( server_address ) : regex = re . compile ( r"^amqp://([a-zA-Z0-9.]+)(:([\d]+))?$" ) x = regex . match ( server_address ) if not x : raise Exception ( "Bad address syntax: %s" % server_address ) matches = x . groups ( ) host = matches [ 0 ] port = int ( matches [ 2 ] ) if matches [ 2 ] else None return host , port
def connect_socket ( host , port , blocking = True ) : addr = socket . getaddrinfo ( host , port , socket . AF_INET , socket . SOCK_STREAM ) if not addr : raise Exception ( "Could not translate address '%s:%s'" % ( host , str ( port ) ) ) my_socket = socket . socket ( addr [ 0 ] [ 0 ] , addr [ 0 ] [ 1 ] , addr [ 0 ] [ 2 ] ) if not blocking : my_socket . setblocking ( 0 ) try : my_socket . connect ( addr [ 0 ] [ 4 ] ) except socket . error as e : if e . errno != errno . EINPROGRESS : raise return my_socket
def server_socket ( host , port , backlog = 10 ) : addr = socket . getaddrinfo ( host , port , socket . AF_INET , socket . SOCK_STREAM ) if not addr : raise Exception ( "Could not translate address '%s:%s'" % ( host , str ( port ) ) ) my_socket = socket . socket ( addr [ 0 ] [ 0 ] , addr [ 0 ] [ 1 ] , addr [ 0 ] [ 2 ] ) my_socket . setblocking ( 0 ) try : my_socket . bind ( addr [ 0 ] [ 4 ] ) my_socket . listen ( backlog ) except socket . error as e : if e . errno != errno . EINPROGRESS : raise return my_socket
def process ( self , now ) : if self . _pn_connection is None : LOG . error ( "Connection.process() called on destroyed connection!" ) return 0 if self . _pn_connection . state & proton . Endpoint . LOCAL_UNINIT : return 0 if self . _pn_sasl and not self . _sasl_done : if ( _PROTON_VERSION < ( 0 , 10 ) ) : if self . _pn_sasl . state not in ( proton . SASL . STATE_PASS , proton . SASL . STATE_FAIL ) : LOG . debug ( "SASL in progress. State=%s" , str ( self . _pn_sasl . state ) ) if self . _handler : with self . _callback_lock : self . _handler . sasl_step ( self , self . _pn_sasl ) return self . _next_deadline self . _sasl_done = True if self . _handler : with self . _callback_lock : self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) else : if self . _pn_sasl . outcome is not None : self . _sasl_done = True if self . _handler : with self . _callback_lock : self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) timer_deadline = self . _expire_timers ( now ) transport_deadline = self . _pn_transport . tick ( now ) if timer_deadline and transport_deadline : self . _next_deadline = min ( timer_deadline , transport_deadline ) else : self . _next_deadline = timer_deadline or transport_deadline pn_event = self . _pn_collector . peek ( ) while pn_event : if _Link . _handle_proton_event ( pn_event , self ) : pass elif self . _handle_proton_event ( pn_event ) : pass elif _SessionProxy . _handle_proton_event ( pn_event , self ) : pass self . _pn_collector . pop ( ) pn_event = self . _pn_collector . peek ( ) if self . _error : if self . _handler : self . _next_deadline = now with self . _callback_lock : self . _handler . connection_failed ( self , self . _error ) elif ( self . _endpoint_state == self . _CLOSED and self . _read_done and self . _write_done ) : if self . _handler : with self . _callback_lock : self . _handler . connection_closed ( self ) return self . _next_deadline
def output_data ( self ) : c = self . has_output if c <= 0 : return None try : buf = self . _pn_transport . peek ( c ) except Exception as e : self . _connection_failed ( str ( e ) ) return None return buf
def create_sender ( self , source_address , target_address = None , event_handler = None , name = None , properties = None ) : ident = name or str ( source_address ) if ident in self . _sender_links : raise KeyError ( "Sender %s already exists!" % ident ) session = _SessionProxy ( "session-%s" % ident , self ) session . open ( ) sl = session . new_sender ( ident ) sl . configure ( target_address , source_address , event_handler , properties ) self . _sender_links [ ident ] = sl return sl
def reject_sender ( self , link_handle , pn_condition = None ) : link = self . _sender_links . get ( link_handle ) if not link : raise Exception ( "Invalid link_handle: %s" % link_handle ) link . reject ( pn_condition ) link . destroy ( )
def create_receiver ( self , target_address , source_address = None , event_handler = None , name = None , properties = None ) : ident = name or str ( target_address ) if ident in self . _receiver_links : raise KeyError ( "Receiver %s already exists!" % ident ) session = _SessionProxy ( "session-%s" % ident , self ) session . open ( ) rl = session . new_receiver ( ident ) rl . configure ( target_address , source_address , event_handler , properties ) self . _receiver_links [ ident ] = rl return rl
def _connection_failed ( self , error = "Error not specified!" ) : if not self . _error : LOG . error ( "Connection failed: %s" , str ( error ) ) self . _error = error
def _ep_active ( self ) : LOG . debug ( "Connection is up" ) if self . _handler : with self . _callback_lock : self . _handler . connection_active ( self )
def _ep_need_close ( self ) : LOG . debug ( "Connection remotely closed" ) if self . _handler : cond = self . _pn_connection . remote_condition with self . _callback_lock : self . _handler . connection_remote_closed ( self , cond )
def _ep_error ( self , error ) : super ( Connection , self ) . _ep_error ( error ) self . _connection_failed ( "Protocol error occurred." )
def _get_color_string ( self ) : s = '' if self . color_type == 'd' : if self . name is "black" : s = '%.3f G' % 0 else : s = '%.3f %.3f %.3f RG' % ( self . red / 255.0 , self . green / 255.0 , self . blue / 255.0 ) elif self . color_type == 'f' or self . color_type == 't' : if self . name is "black" : s = '%.3f g' % 0 else : s = '%.3f %.3f %.3f rg' % ( self . red / 255.0 , self . green / 255.0 , self . blue / 255.0 ) return s
def _set_font ( self , family = None , style = None , size = None ) : if style is not None : if 'B' in style : family += '_bold' if 'I' in style : family += '_italic' self . _set_family ( family ) self . _get_diffs ( ) self . _set_style ( style ) self . _set_metrics ( ) self . _set_size ( size ) self . _set_font_key ( )
def _string_width ( self , s ) : s = str ( s ) w = 0 for char in s : char = ord ( char ) w += self . character_widths [ char ] return w * self . font_size / 1000.0
def get_ttf ( self ) : font_dict = { } families = [ ] rootdirlist = string . split ( self . search_path , os . pathsep ) #for rootdir in rootdirlist: for dirName , subdirList , filelist in itertools . chain . from_iterable ( os . walk ( path ) for path in rootdirlist ) : for item in filelist : root , ext = os . path . splitext ( item ) if ext == '.ttf' : if root [ 0 ] . lower ( ) in english : source = os . path . join ( dirName , item ) name = root . lower ( ) . replace ( '_' , ' ' ) if ' bold' in name : name = name . replace ( ' bold' , '_bold' ) if ' italic' in name : name = name . replace ( ' italic' , '_italic' ) elif 'bold' in name : name = name . replace ( 'bold' , '_bold' ) if 'italic' in name : name = name . replace ( 'italic' , '_italic' ) elif ' italic' in name : name = name . replace ( ' italic' , '_italic' ) elif 'italic' in name : name = name . replace ( 'italic' , '_italic' ) elif 'oblique' in name : name = name . replace ( 'oblique' , '_italic' ) else : families . append ( name ) font_dict [ name ] = source else : source = os . path . join ( dirName , item ) name = root . lower ( ) . replace ( '_' , ' ' ) font_dict [ name ] = source families . append ( name ) self . font_dict = font_dict self . families = families
def _put_stream ( self , stream ) : self . _out ( 'stream' ) self . _out ( stream ) self . _out ( 'endstream' )
def set_font_size ( self , size ) : if self . font . font_size == size : pass else : self . font . _set_size ( size )
def add_pie_chart ( self , data , cursor , width , height , title = None , data_type = "raw" , fill_colors = None , labels = False , background = None , legend = None ) : save_draw_color = self . draw_color save_fill_color = self . fill_color chart = PDFPieChart ( self . session , self . page , data , cursor , width , height , title , data_type , fill_colors , labels , background , legend ) self . set_draw_color ( save_draw_color ) self . set_fill_color ( save_fill_color )
def _output ( self ) : self . session . _out ( '<</Type /XObject' ) self . session . _out ( '/Subtype /Image' ) self . session . _out ( '/Width %s' % self . width ) self . session . _out ( '/Height %s' % self . height ) if self . colorspace is 'Indexed' : self . session . _out ( '/ColorSpace [/Indexed /DeviceRGB %s %s 0 R' % ( self . pal , self . number + 1 ) ) else : self . session . _out ( '/ColorSpace /%s' % self . colorspace ) if self . colorspace is 'DeviceCMYK' : self . session . _out ( '/Decode [1 0 1 0 1 0 1 0]' ) self . session . _out ( '/BitsPerComponent %s' % self . bits_per_component ) if self . filter : self . session . _out ( '/Filter /%s' % self . filter ) if self . decode : self . session . _out ( '/DecodeParms << %s >>' % self . decode ) if self . transparent : self . session . _out ( '/Mask [%s]' % self . transparent_string ) if self . soft_mask : self . session . _out ( '/SMask %s 0 R' % ( self . number + 1 ) ) self . session . _out ( '/Length %s >>' % self . size ) self . session . _put_stream ( self . image_data ) self . session . _out ( 'endobj' ) if self . colorspace is 'Indexed' : self . session . _out ( '<<%s /Length %s >>' % ( self . palette_filter , self . palette_length ) ) self . session . _put_stream ( self . palette ) self . session . _out ( 'endobj' ) if isinstance ( self . soft_mask , PDFImage ) : obj = self . session . _add_object ( ) self . soft_mask . _set_number ( obj . id ) self . soft_mask . _output ( )
def absolute_position ( self , x , y ) : ( a , b , c , d , e , f ) = self . _currentMatrix xp = a * x + c * y + e yp = b * x + d * y + f return xp , yp
def _set_font ( self , family = None , style = None , size = None ) : self . _set_family ( family ) self . _set_style ( style ) self . _set_size ( size ) self . _set_font_key ( ) self . _set_name ( ) self . _set_character_widths ( )
def _string_width ( self , s ) : s = str ( s ) w = 0 for i in s : w += self . character_widths [ i ] return w * self . font_size / 1000.0
def set_display_mode ( self , zoom = 'fullpage' , layout = 'continuous' ) : self . zoom_options = [ "fullpage" , "fullwidth" , "real" , "default" ] self . layout_options = [ "single" , "continuous" , "two" , "default" ] if zoom in self . zoom_options or ( isinstance ( zoom , int ) and 0 < zoom <= 100 ) : self . zoom_mode = zoom else : raise Exception ( 'Incorrect zoom display mode: ' + zoom ) if layout in self . layout_options : self . layout_mode = layout else : raise Exception ( 'Incorrect layout display mode: ' + layout )
def close ( self ) : self . document . _set_page_numbers ( ) self . _put_header ( ) self . _put_pages ( ) self . _put_resources ( ) self . _put_information ( ) self . _put_catalog ( ) self . _put_trailer ( ) if hasattr ( self . destination , "write" ) : output = self . _output_to_io ( ) elif self . destination == 'string' : output = self . _output_to_string ( ) else : self . _output_to_file ( ) output = None return output
def _put_header ( self ) : self . session . _out ( '%%PDF-%s' % self . pdf_version ) if self . session . compression : self . session . buffer += '%' + chr ( 235 ) + chr ( 236 ) + chr ( 237 ) + chr ( 238 ) + "\n"
def _put_resource_dict ( self ) : self . session . _add_object ( 2 ) self . session . _out ( '<<' ) self . session . _out ( '/ProcSet [/PDF /Text /ImageB /ImageC /ImageI]' ) self . session . _out ( '/Font <<' ) for font in self . document . fonts : self . session . _out ( '/F%s %s 0 R' % ( font . index , font . number ) ) self . session . _out ( '>>' ) if self . document . images : self . session . _out ( '/XObject <<' ) for image in self . document . images : self . session . _out ( '/I%s %s 0 R' % ( image . index , image . number ) ) self . session . _out ( '>>' ) self . session . _out ( '>>' ) self . session . _out ( 'endobj' )
def _put_information ( self ) : self . session . _add_object ( ) self . session . _out ( '<<' ) self . session . _out ( '/Producer ' + self . _text_to_string ( ) ) if self . title : self . session . _out ( '/Title ' + self . _text_to_string ( self . title ) ) if self . subject : self . session . _out ( '/Subject ' + self . _text_to_string ( self . subject ) ) if self . author : self . session . _out ( '/Author ' + self . _text_to_string ( self . author ) ) if self . keywords : self . session . _out ( '/Keywords ' + self . _text_to_string ( self . keywords ) ) if self . creator : self . session . _out ( '/Creator ' + self . _text_to_string ( self . creator ) ) self . session . _out ( '/CreationDate ' + self . _text_to_string ( 'D:' + datetime . now ( ) . strftime ( '%Y%m%d%H%M%S' ) ) ) self . session . _out ( '>>' ) self . session . _out ( 'endobj' )
def x_fit ( self , test_length ) : if ( self . x + test_length ) >= self . xmax : return False else : return True
def y_fit ( self , test_length ) : if ( self . y + test_length ) >= self . ymax : return False else : return True
def x_is_greater_than ( self , test_ordinate ) : self . _is_coordinate ( test_ordinate ) if self . x > test_ordinate . x : return True else : return False
def y_is_greater_than ( self , test_ordinate ) : self . _is_coordinate ( test_ordinate ) if self . y > test_ordinate . y : return True else : return False
def copy ( self ) : new_cursor = self . __class__ ( self . x , self . y ) new_cursor . set_bounds ( self . xmin , self . ymin , self . xmax , self . ymax , self . ymaxmax ) new_cursor . set_deltas ( self . dx , self . dy ) return new_cursor
def x_plus ( self , dx = None ) : if dx is None : self . x += self . dx else : self . x = self . x + dx
def y_plus ( self , dy = None ) : if dy is None : self . y += self . dy else : self . y = self . y + dy
def _draw ( self ) : self . _compile ( ) self . rows [ 0 ] . _advance_first_row ( ) self . _set_borders ( ) self . _draw_fill ( ) self . _draw_borders ( ) self . _draw_text ( ) self . _set_final_cursor ( )
def setup ( app ) : app . setup_extension ( 'sphinx.ext.todo' ) app . setup_extension ( 'sphinx.ext.mathjax' ) app . setup_extension ( "sphinx.ext.intersphinx" ) app . config . intersphinx_mapping . update ( { 'https://docs.python.org/' : None } ) app . config . intersphinx_mapping . update ( { sage_doc_url + doc + "/" : None for doc in sage_documents } ) app . config . intersphinx_mapping . update ( { sage_doc_url + "reference/" + module : None for module in sage_modules } ) app . setup_extension ( "sphinx.ext.extlinks" ) app . config . extlinks . update ( { 'python' : ( 'https://docs.python.org/release/' + pythonversion + '/%s' , '' ) , 'trac' : ( 'https://trac.sagemath.org/%s' , 'trac ticket #' ) , 'wikipedia' : ( 'https://en.wikipedia.org/wiki/%s' , 'Wikipedia article ' ) , 'arxiv' : ( 'http://arxiv.org/abs/%s' , 'Arxiv ' ) , 'oeis' : ( 'https://oeis.org/%s' , 'OEIS sequence ' ) , 'doi' : ( 'https://dx.doi.org/%s' , 'doi:' ) , 'pari' : ( 'http://pari.math.u-bordeaux.fr/dochtml/help/%s' , 'pari:' ) , 'mathscinet' : ( 'http://www.ams.org/mathscinet-getitem?mr=%s' , 'MathSciNet ' ) } ) app . config . html_theme = 'sage'
def duration ( self ) : ecc = self . ecc if not np . isnan ( self . ecc ) else np . sqrt ( self . ecw ** 2 + self . esw ** 2 ) esw = self . esw if not np . isnan ( self . esw ) else ecc * np . sin ( self . w ) aRs = ( ( G * self . rhos * ( 1. + self . MpMs ) * ( self . per * DAYSEC ) ** 2. ) / ( 3. * np . pi ) ) ** ( 1. / 3. ) inc = np . arccos ( self . bcirc / aRs ) becc = self . bcirc * ( 1 - ecc ** 2 ) / ( 1 - esw ) tdur = self . per / 2. / np . pi * np . arcsin ( ( ( 1. + self . RpRs ) ** 2 - becc ** 2 ) ** 0.5 / ( np . sin ( inc ) * aRs ) ) tdur *= np . sqrt ( 1. - ecc ** 2. ) / ( 1. - esw ) return tdur
def update ( self , * * kwargs ) : if kwargs . get ( 'verify_kwargs' , True ) : valid = [ y [ 0 ] for x in [ TRANSIT , LIMBDARK , SETTINGS ] for y in x . _fields_ ] valid += [ 'b' , 'times' ] for k in kwargs . keys ( ) : if k not in valid : raise Exception ( "Invalid kwarg '%s'." % k ) if ( 'q1' in kwargs . keys ( ) ) and ( 'q2' in kwargs . keys ( ) ) : kwargs . update ( { 'ldmodel' : KIPPING } ) elif ( 'c1' in kwargs . keys ( ) ) and ( 'c2' in kwargs . keys ( ) ) and ( 'c3' in kwargs . keys ( ) ) and ( 'c4' in kwargs . keys ( ) ) : kwargs . update ( { 'ldmodel' : NONLINEAR } ) self . limbdark . update ( * * kwargs ) self . transit . update ( * * kwargs ) self . settings . update ( * * kwargs )
def Compute ( self ) : err = _Compute ( self . transit , self . limbdark , self . settings , self . arrays ) if err != _ERR_NONE : RaiseError ( err )
def Bin ( self ) : err = _Bin ( self . transit , self . limbdark , self . settings , self . arrays ) if err != _ERR_NONE : RaiseError ( err )
def Free ( self ) : if self . arrays . _calloc : _dbl_free ( self . arrays . _time ) _dbl_free ( self . arrays . _flux ) _dbl_free ( self . arrays . _bflx ) _dbl_free ( self . arrays . _M ) _dbl_free ( self . arrays . _E ) _dbl_free ( self . arrays . _f ) _dbl_free ( self . arrays . _r ) _dbl_free ( self . arrays . _x ) _dbl_free ( self . arrays . _y ) _dbl_free ( self . arrays . _z ) self . arrays . _calloc = 0 if self . arrays . _balloc : _dbl_free ( self . arrays . _b ) self . arrays . _balloc = 0 if self . arrays . _ialloc : _dbl_free ( self . arrays . _iarr ) self . arrays . _ialloc = 0
def list_extensions_gen ( self ) : code , message = self . command ( "LIST EXTENSIONS" ) if code != 202 : raise NNTPReplyError ( code , message ) for line in self . info_gen ( code , message ) : yield line . strip ( )
def xpat_gen ( self , header , msgid_range , * pattern ) : args = " " . join ( [ header , utils . unparse_msgid_range ( msgid_range ) ] + list ( pattern ) ) code , message = self . command ( "XPAT" , args ) if code != 221 : raise NNTPReplyError ( code , message ) for line in self . info_gen ( code , message ) : yield line . strip ( )
def xfeature_compress_gzip ( self , terminator = False ) : args = "TERMINATOR" if terminator else None code , message = self . command ( "XFEATURE COMPRESS GZIP" , args ) if code != 290 : raise NNTPReplyError ( code , message ) return True
def _api_post ( self , url , * * kwargs ) : response = self . session . post ( url = url , headers = self . _get_api_headers ( ) , * * kwargs ) if not response . ok : raise ServerException ( '{0}: {1}' . format ( response . status_code , response . text or response . reason ) ) return response . json ( )
def _api_delete ( self , url , * * kwargs ) : response = self . session . delete ( url = url , headers = self . _get_api_headers ( ) , * * kwargs ) if not response . ok : raise ServerException ( '{0}: {1}' . format ( response . status_code , response . text or response . reason ) ) return response
def _api_get ( self , url , * * kwargs ) : response = self . session . get ( url = url , headers = self . _get_api_headers ( ) , * * kwargs ) if not response . ok : raise ServerException ( '{0}: {1}' . format ( response . status_code , response . text or response . reason ) ) return response . json ( )
def _create_scheduled_query ( self , query , change , scope_unit , scope_count ) : query_data = { 'scheduled_query' : { 'name' : 'ForAnomalyReport' , 'query' : query , 'threshold_type' : '%' , 'threshold_value' : change , 'time_period' : scope_unit . title ( ) , 'time_value' : scope_count , } } query_url = 'https://logentries.com/rest/{account_id}/api/scheduled_queries' return self . _api_post ( url = query_url . format ( account_id = self . account_id ) , data = json . dumps ( query_data , sort_keys = True ) )
def do_POST ( self ) : self . send_response ( urllib2 . httplib . OK ) self . end_headers ( ) content_length = int ( self . headers [ 'Content-Length' ] ) body = self . rfile . read ( content_length ) print ( "Client: {0}" . format ( str ( self . client_address ) ) ) print ( "headers: {0}" . format ( self . headers ) ) print ( "path: {0}" . format ( self . path ) ) print ( "body: {0}" . format ( body ) )
def defaults_docstring ( defaults , header = None , indent = None , footer = None ) : if indent is None : indent = '' if header is None : header = '' if footer is None : footer = '' width = 60 hbar = '\n' s = hbar + ( header ) + hbar for key , value , desc in defaults : if isinstance ( value , basestring ) : value = "'" + value + "'" if hasattr ( value , '__call__' ) : value = "<" + value . __name__ + ">" s += indent + '%-12s\n' % ( "%s :" % key ) s += indent + indent + ( indent + 23 * ' ' ) . join ( desc . split ( '\n' ) ) s += ' [%s]\n\n' % str ( value ) s += hbar s += footer return s
def defaults_decorator ( defaults ) : def decorator ( func ) : kwargs = dict ( header = 'Keyword arguments\n-----------------\n' , indent = '  ' , footer = '\n' ) doc = defaults_docstring ( defaults , * * kwargs ) if func . __doc__ is None : func . __doc__ = '' func . __doc__ += doc return func return decorator
def _load ( self , * * kwargs ) : defaults = dict ( [ ( d [ 0 ] , d [ 1 ] ) for d in self . defaults ] ) for k in kwargs : if k not in defaults : msg = "Unrecognized attribute of %s: %s" % ( self . __class__ . __name__ , k ) raise AttributeError ( msg ) defaults . update ( kwargs ) self . __dict__ . update ( defaults ) self . check_type ( self . __dict__ [ 'default' ] ) self . set ( * * defaults )
def defaults_docstring ( cls , header = None , indent = None , footer = None ) : return defaults_docstring ( cls . defaults , header = header , indent = indent , footer = footer )
def set_errors ( self , errors ) : if errors is None : self . __errors__ = None return self . __errors__ = [ asscalar ( e ) for e in errors ]
def load_and_parse ( self ) : f = open ( self . file_path , "r" ) metrics_json = f . read ( ) self . metrics = json . loads ( metrics_json )
def extract_dictionary ( self , metrics ) : new_metrics = { } for m in metrics : metric = self . extract_fields ( m ) new_metrics [ m [ 'name' ] ] = metric return new_metrics
def filter ( self ) : if self . filter_expression is not None : new_metrics = [ ] metrics = self . metrics [ 'result' ] for m in metrics : if self . filter_expression . search ( m [ 'name' ] ) : new_metrics . append ( m ) else : new_metrics = self . metrics [ 'result' ] self . metrics = self . extract_dictionary ( new_metrics )
def get_arguments ( self ) : ApiCli . get_arguments ( self ) if self . args . hostGroupId is not None : self . hostGroupId = self . args . hostGroupId self . path = "v1/hostgroup/{0}" . format ( str ( self . hostGroupId ) )
def get_arguments ( self ) : ApiCli . get_arguments ( self ) if self . args . tenant_id is not None : self . _tenant_id = self . args . tenant_id if self . args . fingerprint_fields is not None : self . _fingerprint_fields = self . args . fingerprint_fields if self . args . title is not None : self . _title = self . args . title if self . args . source is not None : self . _source = self . args . source if self . args . severity is not None : self . _severity = self . args . severity if self . args . message is not None : self . _message = self . args . message event = { } if self . _title is not None : event [ 'title' ] = self . _title if self . _severity is not None : event [ 'severity' ] = self . _severity if self . _message is not None : event [ 'message' ] = self . _message if self . _source is not None : if 'source' not in event : event [ 'source' ] = { } if len ( self . _source ) >= 1 : event [ 'source' ] [ 'ref' ] = self . _source [ 0 ] if len ( self . _source ) >= 2 : event [ 'source' ] [ 'type' ] = self . _source [ 1 ] self . _process_properties ( self . args . properties ) if self . _properties is not None : event [ 'properties' ] = self . _properties if self . _fingerprint_fields is not None : event [ 'fingerprintFields' ] = self . _fingerprint_fields self . data = json . dumps ( event , sort_keys = True ) self . headers = { 'Content-Type' : 'application/json' }
def _call_api ( self ) : sockobj = socket ( AF_INET , SOCK_STREAM ) sockobj . connect ( ( self . rpc_host , self . rpc_port ) ) self . get_json ( ) message = [ self . rpc_message . encode ( 'utf-8' ) ] for line in message : sockobj . send ( line ) data = sockobj . recv ( self . MAX_LINE ) print ( data ) self . rpc_data . append ( data ) sockobj . close ( )
def get_arguments ( self ) : HostgroupModify . get_arguments ( self ) if self . args . host_group_id is not None : self . host_group_id = self . args . host_group_id self . path = "v1/hostgroup/" + str ( self . host_group_id )
def identifier ( self , text ) : self . _attempting ( text ) return concatenation ( [ alternation ( [ self . alpha_character , "_" ] ) , zero_or_more ( alternation ( [ self . alpha_character , "_" , self . digit ] ) ) ] , ignore_whitespace = False ) ( text ) . compressed ( TokenType . identifier )
def operator ( self , text ) : self . _attempting ( text ) return alternation ( [ "|" , "." , "," , "-" ] ) ( text ) . retyped ( TokenType . operator )
def op_mult ( self , text ) : self . _attempting ( text ) return terminal ( "*" ) ( text ) . retyped ( TokenType . op_mult )
def op_add ( self , text ) : self . _attempting ( text ) return terminal ( "+" ) ( text ) . retyped ( TokenType . op_add )
def get_arguments ( self ) : ApiCli . get_arguments ( self ) if self . args . pluginName is not None : self . pluginName = self . args . pluginName self . path = "v1/plugins/{0}/components" . format ( self . pluginName )
def _get_environment ( self ) : if 'TSP_EMAIL' in os . environ : self . _email = os . environ [ 'TSP_EMAIL' ] if 'TSP_API_TOKEN' in os . environ : self . _api_token = os . environ [ 'TSP_API_TOKEN' ] if 'TSP_API_HOST' in os . environ : self . _api_host = os . environ [ 'TSP_API_HOST' ] else : self . _api_host = 'api.truesight.bmc.com'
def _call_api ( self ) : self . _url = self . form_url ( ) if self . _headers is not None : logging . debug ( self . _headers ) if self . _data is not None : logging . debug ( self . _data ) if len ( self . _get_url_parameters ( ) ) > 0 : logging . debug ( self . _get_url_parameters ( ) ) result = self . _methods [ self . _method ] ( ) if not self . good_response ( result . status_code ) : logging . error ( self . _url ) logging . error ( self . _method ) if self . _data is not None : logging . error ( self . _data ) logging . error ( result ) self . _api_result = result
def get_arguments ( self ) : if self . args . file_name is not None : self . file_name = self . args . file_name
def execute ( self ) : self . add_arguments ( ) self . _parse_args ( ) self . get_arguments ( ) if self . _validate_arguments ( ) : self . _plot_data ( ) else : print ( self . _message )
def get_remote_file_size ( self , url ) : try : req = urllib . request . urlopen ( url ) return int ( req . getheader ( 'Content-Length' ) . strip ( ) ) except urllib . error . HTTPError as error : logger . error ( 'Error retrieving size of the remote file %s' % error ) print ( 'Error retrieving size of the remote file %s' % error ) self . connect_earthexplorer ( ) self . get_remote_file_size ( url )
def download ( self , bands = None , download_dir = None , metadata = False ) : if not download_dir : download_dir = DOWNLOAD_DIR if bands is None : bands = list ( range ( 1 , 12 ) ) + [ 'BQA' ] else : self . validate_bands ( bands ) pattern = re . compile ( '^[^\s]+_(.+)\.tiff?' , re . I ) band_list = [ 'B%i' % ( i , ) if isinstance ( i , int ) else i for i in bands ] image_list = [ ] self . connect_earthexplorer ( ) tgzname = self . sceneInfo . name + '.tgz' dest_dir = check_create_folder ( join ( download_dir , self . sceneInfo . name ) ) downloaded = self . download_file ( self . url , dest_dir , tgzname ) logger . debug ( 'Status downloaded %s' % downloaded ) print ( '\n Status downloaded %s' % downloaded ) if downloaded [ 'sucess' ] : print ( '\n Downloaded sucess' ) logger . debug ( 'Downloaded sucess of scene: %s' % self . sceneInfo . name ) try : tar = tarfile . open ( downloaded [ 'file_path' ] , 'r' ) folder_path = join ( download_dir , self . sceneInfo . name ) tar . extractall ( folder_path ) remove ( downloaded [ 'file_path' ] ) images_path = listdir ( folder_path ) for image_path in images_path : matched = pattern . match ( image_path ) file_path = join ( folder_path , image_path ) if matched and matched . group ( 1 ) in band_list : image_list . append ( [ file_path , getsize ( file_path ) ] ) elif matched : remove ( file_path ) except tarfile . ReadError as error : print ( '\nError when extracting files. %s' % error ) logger . error ( 'Error when extracting files. %s' % error ) return image_list else : logger . debug ( 'Info downloaded: %s' % downloaded ) print ( '\n Info downloaded: %s' % downloaded ) return downloaded
def validate_bands ( bands ) : if not isinstance ( bands , list ) : raise TypeError ( 'Parameter bands must be a "list"' ) valid_bands = list ( range ( 1 , 12 ) ) + [ 'BQA' ] for band in bands : if band not in valid_bands : raise InvalidBandError ( '%s is not a valid band' % band )
def connect_earthexplorer ( self ) : logger . info ( "Establishing connection to Earthexplorer" ) print ( "\n Establishing connection to Earthexplorer" ) try : opener = urllib . request . build_opener ( urllib . request . HTTPCookieProcessor ( ) ) urllib . request . install_opener ( opener ) params = urllib . parse . urlencode ( dict ( username = self . user , password = self . password ) ) params = params . encode ( 'utf-8' ) f = opener . open ( "https://ers.cr.usgs.gov/login" , params ) data = f . read ( ) . decode ( 'utf-8' ) f . close ( ) if data . find ( 'You must sign in as a registered user to download data or place orders for USGS EROS products' ) > 0 : print ( "\n Authentification failed" ) logger . error ( "Authentification failed" ) raise AutenticationUSGSFailed ( 'Authentification USGS failed' ) print ( 'User %s connected with USGS' % self . user ) logger . debug ( 'User %s connected with USGS' % self . user ) return except Exception as e : print ( '\nError when trying to connect USGS: %s' % e ) raise logger . error ( 'Error when trying to connect USGS: %s' % e )
def get_arguments ( self ) : ApiCli . get_arguments ( self ) if self . args . metric_name is not None : self . _metric_name = self . args . metric_name self . path = "v1/metrics/{0}" . format ( self . _metric_name )
def normalize ( self , dt , is_dst = False ) : if dt . tzinfo is None : raise ValueError ( 'Naive time - no tzinfo set' ) return dt . replace ( tzinfo = self )
def get_arguments ( self ) : ApiCli . get_arguments ( self ) if self . args . hostGroupId is not None : self . hostGroupId = self . args . hostGroupId if self . args . force is not None : self . force = self . args . force if self . force : self . url_parameters = { "forceRemove" : True } self . path = "v1/hostgroup/{0}" . format ( str ( self . hostGroupId ) )
def get_arguments ( self ) : ApiCli . get_arguments ( self ) self . _actions = self . args . actions if self . args . actions is not None else None self . _alarm_name = self . args . alarm_name if self . args . alarm_name is not None else None self . _metric = self . args . metric if self . args . metric is not None else None self . _aggregate = self . args . aggregate if self . args . aggregate is not None else None self . _operation = self . args . operation if self . args . operation is not None else None self . _threshold = self . args . threshold if self . args . threshold is not None else None self . _trigger_interval = self . args . trigger_interval if self . args . trigger_interval is not None else None self . _host_group_id = self . args . host_group_id if self . args . host_group_id is not None else None self . _note = self . args . note if self . args . note is not None else None self . _per_host_notify = self . args . per_host_notify if self . args . per_host_notify is not None else None self . _is_disabled = self . args . is_disabled if self . args . is_disabled is not None else None self . _notify_clear = self . args . notify_clear if self . args . notify_clear is not None else None self . _notify_set = self . args . notify_set if self . args . notify_set is not None else None self . _timeout_interval = self . args . timeout_interval if self . args . timeout_interval is not None else None
def _dump_text ( self ) : results = self . _relay_output [ 'result' ] for l in results : dt = time . strftime ( "%Y-%m-%dT%H:%M:%SZ" , time . gmtime ( int ( l [ 1 ] [ 'ts' ] ) ) ) print ( "{0} {1} {2} {3}" . format ( l [ 0 ] , dt , l [ 1 ] [ 'type' ] , l [ 1 ] [ 'msg' ] ) )
def _handle_results ( self ) : if self . _api_result . status_code == requests . codes . ok : self . _relay_output = json . loads ( self . _api_result . text ) if self . _raw : self . _dump_json ( ) else : self . _dump_text ( )
def get_arguments ( self ) : PluginBase . get_arguments ( self ) if self . args . organizationName is not None : self . organizationName = self . args . organizationName if self . args . repositoryName is not None : self . repositoryName = self . args . repositoryName self . path = "v1/plugins/private/{0}/{1}/{2}" . format ( self . pluginName , self . organizationName , self . repositoryName )
def get_arguments ( self ) : AlarmModify . get_arguments ( self ) self . _alarm_id = self . args . alarm_id if self . args . alarm_id is not None else None self . get_api_parameters ( )
def _filter ( self ) : if self . _metrics or self . _control or self . _plugins : relays = self . _relays [ 'result' ] [ 'relays' ] for relay in relays : if self . _metrics : del relays [ relay ] [ 'metrics' ] if self . _control : del relays [ relay ] [ 'control' ] if self . _plugins : if 'plugins' in relays [ relay ] : del relays [ relay ] [ 'plugins' ]
def _handle_results ( self ) : if self . _api_result . status_code == requests . codes . ok : self . _relays = json . loads ( self . _api_result . text ) self . _filter ( ) self . _dump_json ( )
def fromlist ( cls , files , equal = False , offensive = False , lang = None ) : self = cls . __new__ ( cls ) self . files = fortunes = [ ] count = 0 for file in files : fortune = load_fortune ( file , offensive = offensive , lang = lang ) if fortune is None : logger . warn ( "Can't load: %s" , file ) continue count += 1 if equal else fortune . size fortunes . append ( ( fortune , count ) ) if not fortunes : raise ValueError ( 'All fortune files specified are invalid' ) self . count = count self . keys = [ i [ 1 ] for i in self . files ] return self
def set_chance ( cls , files , equal = False , offensive = False , lang = None ) : self = cls . __new__ ( cls ) total = 0. file = [ ] leftover = [ ] for name , chance in files : if total >= 1 : break fortune = load_fortune ( name , offensive = offensive , lang = lang ) if fortune is None or not fortune . size : continue if chance : file . append ( ( fortune , chance ) ) total += chance else : leftover . append ( fortune ) if leftover and total < 1 : left = 1 - total if equal : perfile = left / len ( leftover ) for fortune in leftover : file . append ( ( fortune , perfile ) ) else : entries = sum ( map ( attrgetter ( 'size' ) , leftover ) ) logger . debug ( '%d entries left' , entries ) for fortune in leftover : chance = left * fortune . size / entries file . append ( ( fortune , chance ) ) self . count = count = 65536 bound = 0 self . files = fortunes = [ ] for file , chance in file : bound += int ( chance * count ) fortunes . append ( ( file , bound ) ) self . keys = [ i [ 1 ] for i in self . files ] return self
def grammar ( self , text ) : self . _attempting ( text ) return concatenation ( [ zero_or_more ( self . comment , ignore_whitespace = True ) , self . rule , zero_or_more ( alternation ( [ self . comment , self . rule , ] ) , ignore_whitespace = True ) , ] , ignore_whitespace = True ) ( text ) . retyped ( TokenType . grammar )
def rule ( self , text ) : self . _attempting ( text ) return concatenation ( [ self . identifier , "=" , self . expression , ";" , ] , ignore_whitespace = True ) ( text ) . retyped ( TokenType . rule )
def special_handling ( self , text ) : self . _attempting ( text ) return concatenation ( [ "?" , self . identifier , "?" , ] , ignore_whitespace = True ) ( text ) . retyped ( TokenType . special_handling )
def number ( self , text ) : self . _attempting ( text ) return concatenation ( [ exclusion ( self . digit , "0" ) , zero_or_more ( self . digit , ignore_whitespace = False ) , ] , ignore_whitespace = False ) ( text ) . compressed ( TokenType . number )
def get_arguments ( self ) : ApiCli . get_arguments ( self ) if self . args . metricName is not None : self . metricName = self . args . metricName if self . args . measurement is not None : self . measurement = self . args . measurement if self . args . source is not None : self . source = self . args . source else : self . source = socket . gethostname ( ) if self . args . timestamp is not None : self . timestamp = int ( self . args . timestamp ) m = { 'metric' : self . metricName , 'measure' : self . measurement } if self . source is not None : m [ 'source' ] = self . source if self . timestamp is not None : m [ 'timestamp' ] = int ( self . timestamp ) self . _process_properties ( ) if self . _properties is not None : m [ 'metadata' ] = self . _properties self . data = json . dumps ( m , sort_keys = True ) self . headers = { 'Content-Type' : 'application/json' , "Accept" : "application/json" }
def _handle_results ( self ) : if self . _api_result . status_code == requests . codes . ok : payload = json . loads ( self . _api_result . text ) out = json . dumps ( payload , sort_keys = True , indent = 4 , separators = ( ',' , ': ' ) ) print ( self . colorize_json ( out ) )
def grammar ( self ) : if self . _grammar is None : self . parser = Parser ( ) grammar = self . parser . parse ( self . input_source ) self . _grammar = grammar . trimmed ( ) . flattened ( ) . flattened ( self . _flatten ) return self . _grammar
def rules ( self ) : if self . _rules is None : self . _rules = [ ] for child in self . grammar . children : if child . is_type ( TokenType . rule ) : name , expression = child . children self . _rules . append ( Rule ( name . value , self . _expression_to_asn ( expression ) , name . position , child . consumed ) ) return self . _rules
def comments ( self ) : if self . _comments is None : self . _comments = [ c for c in self . grammar . children if c . is_type ( TokenType . comment ) ] return self . _comments
def directives ( self ) : if self . _directives is None : self . _directives = [ ] for comment in self . comments : self . _directives . extend ( self . directives_from_comment ( comment ) ) return self . _directives
def output_source ( self ) : if self . _output_source is None : self . _output_source = self . _compile ( ) return self . _output_source
def _compile ( self ) : fmt = fmt = self . _clean_fmt ( fmt ) return fmt . format ( date = datetime . utcnow ( ) . isoformat ( ) , imports = self . _get_imports ( ) , token_type_enum = self . _get_token_type_enum ( ) , class_definition = self . _get_class_definition ( ) )
def _get_imports ( self ) : import_directives = [ d for d in self . directives if d . name == "import" ] if import_directives : return "\n" + "\n" . join ( d . args [ "value" ] for d in import_directives ) else : return ""
def _get_token_type_enum ( self ) : fmt = "class TokenType(Enum):\n" "{indent}\"\"\"The token types for parse nodes generated by the Parser.\"\"\"\n" "{indent}" + "\n{indent}" . join ( "{1} = {0}" . format ( num + 1 , r . name ) for num , r in enumerate ( self . rules ) ) return fmt . format ( indent = self . indent )
def _get_class_definition ( self ) : fmt = fmt = self . _clean_fmt ( fmt ) return fmt . format ( parser_base = self . _get_parser_base ( ) , indent = self . indent , entry_point = self . _get_entry_point ( ) , rule_definitions = "\n" . join ( self . _get_rule_definitions ( ) ) )
def _get_entry_point ( self ) : ep = self . _find_directive ( "entry_point" ) if ep : return ep . args [ "value" ] else : return self . rules [ 0 ] . name
def _get_rule_definition ( self , rule ) : fmt = fmt = self . _clean_fmt ( fmt ) source = self . _indent ( self . _ast_to_code ( rule . expression ) , skip_first_line = True ) if self . use_terminal_shorthand and len ( source ) == 1 and source [ 0 ] . startswith ( ( "'" , '"' ) ) : source = [ "terminal({})" . format ( source [ 0 ] ) ] rule_source = fmt . format ( rule_fxn_name = self . _get_rule_fxn_name ( rule . name ) , indent = self . indent , rule_source = self . _get_rule_source ( rule ) , rule_definition = "\n" . join ( source ) , transform = self . _get_rule_transform ( rule ) ) return self . _indent ( rule_source , 1 )
def _get_rule_source ( self , rule ) : p = len ( self . input_source ) + rule . position source = self . input_source [ p : p + rule . consumed ] . rstrip ( ) return self . _indent ( source , depth = self . indent + "   " , skip_first_line = True )
def _expression_to_asn ( self , expression ) : new_children = [ self . _node_to_asn ( c ) for c in expression . children ] return self . _remove_grouping_groups ( infix_to_optree ( new_children ) )
def _node_to_asn ( self , node ) : if node . is_type ( TokenType . identifier ) : return Identifier ( node . svalue ) elif node . is_type ( TokenType . terminal ) : return Terminal ( node . svalue ) elif node . is_type ( TokenType . option_group ) : expr = node . children [ 0 ] return OptionGroup ( self . _expression_to_asn ( expr ) ) elif node . is_type ( TokenType . repetition_group ) : expr = node . children [ 0 ] return RepetitionGroup ( self . _expression_to_asn ( expr ) ) elif node . is_type ( TokenType . grouping_group ) : expr = node . children [ 0 ] return GroupingGroup ( self . _expression_to_asn ( expr ) ) elif node . is_type ( TokenType . special_handling ) : ident = node . children [ 0 ] return SpecialHandling ( ident ) elif node . is_type ( TokenType . number ) : return Number ( node . svalue ) elif node . is_type ( ( TokenType . operator , TokenType . op_mult , TokenType . op_add ) ) : return OperatorNode ( OPERATOR_INDEX [ node . svalue ] , node . position ) else : raise Exception ( "Unhandled parse tree node: {0}" . format ( node ) )
def _ast_to_code ( self , node , * * kwargs ) : if isinstance ( node , OptreeNode ) : return self . _ast_optree_node_to_code ( node , * * kwargs ) elif isinstance ( node , Identifier ) : return self . _ast_identifier_to_code ( node , * * kwargs ) elif isinstance ( node , Terminal ) : return self . _ast_terminal_to_code ( node , * * kwargs ) elif isinstance ( node , OptionGroup ) : return self . _ast_option_group_to_code ( node , * * kwargs ) elif isinstance ( node , RepetitionGroup ) : return self . _ast_repetition_group_to_code ( node , * * kwargs ) elif isinstance ( node , SpecialHandling ) : return self . _ast_special_handling_to_code ( node , * * kwargs ) elif isinstance ( node , Number ) : return self . _ast_number_to_code ( node , * * kwargs ) else : raise Exception ( "Unhandled ast node: {0}" . format ( node ) )
def _ast_optree_node_to_code ( self , node , * * kwargs ) : opnode = node . opnode if opnode is None : return self . _ast_to_code ( node . operands [ 0 ] ) else : operator = opnode . operator if operator is OP_ALTERNATE : return self . _ast_op_alternate_to_code ( node , * * kwargs ) elif operator is OP_WS_CONCAT : kwargs [ "ignore_whitespace" ] = False return self . _ast_op_concat_to_code ( node , * * kwargs ) elif operator is OP_CONCAT : kwargs [ "ignore_whitespace" ] = True return self . _ast_op_concat_to_code ( node , * * kwargs ) elif operator is OP_EXCLUDE : return self . _ast_op_exclude_to_code ( node , * * kwargs ) elif operator is OP_MULTIPLY : return self . _ast_op_multiply_to_code ( node , * * kwargs ) elif operator is OP_REPEAT : return self . _ast_op_repeat_to_code ( node , * * kwargs ) else : raise Exception ( "Unhandled optree node: {0}" . format ( node ) )
def _ast_terminal_to_code ( self , terminal , * * kwargs ) : value = _replace ( terminal . value ) if self . use_terminal_shorthand : return [ value ] else : return [ "terminal({})" . format ( value ) ]
def _ast_option_group_to_code ( self , option_group , * * kwargs ) : lines = [ "option(" ] lines . extend ( self . _indent ( self . _ast_to_code ( option_group . expression ) ) ) lines . append ( ")" ) return lines
def _ast_repetition_group_to_code ( self , repetition_group , ignore_whitespace = False , * * kwargs ) : lines = [ "zero_or_more(" ] lines . extend ( self . _indent ( self . _ast_to_code ( repetition_group . expression ) ) ) lines [ - 1 ] += "," lines . append ( self . _indent ( "ignore_whitespace={}" . format ( bool ( ignore_whitespace ) ) ) ) lines . append ( ")" ) return lines
def _ast_special_handling_to_code ( self , special_handling , * * kwargs ) : ident = special_handling . value . svalue if ident in PB_SPECIAL_HANDLING : return [ "PB.{0}" . format ( ident ) ] else : return [ "self.{0}" . format ( ident ) ]
def _ast_op_alternate_to_code ( self , opr , * * kwargs ) : hoist_target = OP_ALTERNATE operands = self . _hoist_operands ( opr . operands , lambda t : isinstance ( t , OptreeNode ) and t . opnode . operator is hoist_target ) lines = [ "alternation([" ] for op in operands : lines . extend ( self . _indent ( self . _ast_to_code ( op ) ) ) lines [ - 1 ] += "," lines . append ( "])" ) return lines
def _ast_op_concat_to_code ( self , opr , * , ignore_whitespace , * * kwargs ) : hoist_target = OP_CONCAT if ignore_whitespace else OP_WS_CONCAT operands = self . _hoist_operands ( opr . operands , lambda t : isinstance ( t , OptreeNode ) and t . opnode . operator is hoist_target ) lines = [ "concatenation([" ] for op in operands : lines . extend ( self . _indent ( self . _ast_to_code ( op , ignore_whitespace = ignore_whitespace ) ) ) lines [ - 1 ] += "," lines . append ( "], ignore_whitespace={})" . format ( bool ( ignore_whitespace ) ) ) return lines
def _ast_op_exclude_to_code ( self , opr , * * kwargs ) : opl , opr = opr . operands lines = [ "exclusion(" ] lines . extend ( self . _indent ( self . _ast_to_code ( opl ) ) ) lines [ - 1 ] += "," lines . extend ( self . _indent ( self . _ast_to_code ( opr ) ) ) lines . append ( ")" ) return lines
def _ast_op_multiply_to_code ( self , opr , ignore_whitespace = False , * * kwargs ) : opl , opr = opr . operands if isinstance ( opl , Number ) : times = opl . value subject = self . _ast_to_code ( opr ) else : times = opr . value subject = self . _ast_to_code ( opl ) lines = [ "repeated(" ] lines . extend ( self . _indent ( subject ) ) lines [ - 1 ] += "," lines . append ( "{0}times={1}," . format ( self . indent , times ) ) lines . append ( "{0}ignore_whitespace={1}" . format ( self . indent , bool ( ignore_whitespace ) ) ) lines . append ( ")" ) return lines
def _ast_op_repeat_to_code ( self , opr , ignore_whitespace = False , * * kwargs ) : lines = [ "one_or_more(" ] lines . extend ( self . _indent ( self . _ast_to_code ( opr . operands [ 0 ] ) ) ) lines [ - 1 ] += "," lines . append ( self . _indent ( "ignore_whitespace={}" . format ( bool ( ignore_whitespace ) ) ) ) lines . append ( ")" ) return lines
def _find_directives ( self , pred ) : if isinstance ( pred , str ) : return [ d for d in self . directives if d . name == pred ] else : return [ d for d in self . directives if pred ( d ) ]
def _flatten ( child , parent ) : return parent . is_type ( TokenType . expression ) and child . node_type == parent . node_type
def directives_from_comment ( cls , comment ) : comment_contents = comment . value [ 2 : - 2 ] . strip ( ) comment_lines = ( l . strip ( ) for l in comment_contents . split ( "\n" ) ) directives = ( l [ 1 : ] . strip ( ) for l in comment_lines if l . startswith ( "!" ) ) for directive_def in directives : yield cls . parse_directive_def ( directive_def )
def parse_directive_def ( cls , directive_def ) : name ,  * kwargs = esc_split ( directive_def , ignore_empty = True ) return Directive ( name , { key : value for key , value in ( esc_split ( arg , "=" ) for arg in kwargs ) } )
def get_arguments ( self ) : ApiCli . get_arguments ( self ) if self . args . hostGroupName is not None : self . url_parameters = { "name" : self . args . hostGroupName }
def get_arguments ( self ) : ApiCli . get_arguments ( self ) if self . args . plugin_name is not None : self . plugin_name = self . args . plugin_name self . path = "v1/plugins/{0}" . format ( self . plugin_name )
def get_arguments ( self ) : ApiCli . get_arguments ( self ) self . _alarm_id = self . args . alarm_id if self . args . alarm_id is not None else None
def _handle_results ( self ) : if self . _api_result . status_code != requests . codes . ok : print ( self . colorize_json ( self . _api_result . text ) )
def get_id ( id ) : if id == None : id = wx . NewId ( ) logger . debug ( 'Generated new ID %s.' , id ) else : logger . debug ( 'Using provided id %s.' , id ) return id
def add_arguments ( self ) : self . add_logging_argument ( ) self . parser . add_argument ( '-a' , '--api-host' , dest = 'api_host' , action = 'store' , metavar = "api_host" , help = '{0} API host endpoint' . format ( self . product_name ) ) self . parser . add_argument ( '-e' , '--email' , dest = 'email' , action = 'store' , metavar = "e_mail" , help = 'e-mail that has access to the {0} account' . format ( self . product_name ) ) self . parser . add_argument ( '-t' , '--api-token' , dest = 'api_token' , required = False , action = 'store' , metavar = "api_token" , help = 'API token for given e-mail that has access to the {0} account' . format ( self . product_name ) ) self . parser . add_argument ( '-z' , '--curl' , dest = 'curl' , required = False , action = 'store_true' , default = False , help = 'Output the corresponding curl command line and exit' )
def _configure_logging ( self ) : if self . args . logLevel is not None : logging . basicConfig ( level = self . levels [ self . args . logLevel ] ) logging . info ( "Set logging level to {0}" . format ( self . args . logLevel ) )
def execute ( self ) : self . _get_environment ( ) self . add_arguments ( ) self . _parse_args ( ) self . get_arguments ( ) self . get_api_parameters ( ) if self . _validate_arguments ( ) : if self . _curl : self . _curl_output ( ) else : self . _call_api ( ) self . _handle_results ( ) else : print ( self . _message )
def postfix_to_optree ( nodes ) : while len ( nodes ) > 1 : nodes = _reduce ( nodes ) if len ( nodes ) == 0 : raise OperatorError ( "Empty node list" ) node = nodes [ 0 ] if isinstance ( node , OperatorNode ) : raise OperatorError ( "Operator without operands" ) if isinstance ( node , OptreeNode ) : return node return OptreeNode ( None , ( node , ) )
def pprint ( root , depth = 0 , space_unit = "    " ) : spacing = space_unit * depth if isinstance ( root , OptreeNode ) : print ( "{0}Operator ({1})" . format ( spacing , root . opnode . operator . symbol if root . opnode else "None -> IDENTITY" ) ) for operand in root . operands : pprint ( operand , depth + 1 ) else : print ( "{0}• {1}".f o rmat(s p acing,  r ot))  
def get_arguments ( self ) : ApiCli . get_arguments ( self ) if self . args . pluginName is not None : self . pluginName = self . args . pluginName
def add_arguments ( self ) : MetricCommon . add_arguments ( self ) self . parser . add_argument ( '-n' , '--metric-name' , dest = 'metricName' , action = 'store' , required = True , metavar = 'metric_name' , help = 'Metric identifier' ) self . parser . add_argument ( '-d' , '--display-name' , dest = 'displayName' , action = 'store' , required = True , metavar = 'display_name' , help = 'Metric display name' ) self . parser . add_argument ( '-s' , '--display-name-short' , dest = 'displayNameShort' , action = 'store' , required = True , metavar = 'display_short_name' , help = 'Metric short display name' ) self . parser . add_argument ( '-i' , '--description' , dest = 'description' , action = 'store' , required = not self . update , metavar = 'description' , help = 'Metric description' ) self . parser . add_argument ( '-g' , '--aggregate' , dest = 'aggregate' , action = 'store' , required = True , choices = [ 'avg' , 'max' , 'min' , 'sum' ] , help = 'Metric default aggregate' ) self . parser . add_argument ( '-u' , '--unit' , dest = 'unit' , action = 'store' , required = False , choices = [ 'percent' , 'number' , 'bytecount' , 'duration' ] , help = 'Metric unit' ) self . parser . add_argument ( '-r' , '--resolution' , dest = 'resolution' , action = 'store' , metavar = 'resolution' , required = False , help = 'Metric default resolution' ) self . parser . add_argument ( '-y' , '--type' , dest = 'type' , action = 'store' , default = None , required = False , metavar = 'type' , help = 'Sets the type metadata field' ) self . parser . add_argument ( '-x' , '--is-disabled' , dest = 'isDisabled' , action = 'store' , default = None , required = False , choices = [ 'true' , 'false' ] , help = 'Enable or disable the metric definition' )
def get_arguments ( self ) : MetricCommon . get_arguments ( self ) if self . args . metricName is not None : self . metricName = self . args . metricName if self . args . displayName is not None : self . displayName = self . args . displayName if self . args . displayNameShort is not None : self . displayNameShort = self . args . displayNameShort if self . args . description is not None : self . description = self . args . description if self . args . aggregate is not None : self . aggregate = self . args . aggregate if self . args . unit is not None : self . unit = self . args . unit if self . args . resolution is not None : self . resolution = self . args . resolution if self . args . isDisabled is not None : self . isDisabled = self . args . isDisabled if self . args . type is not None : self . type = self . args . type data = { } if self . metricName is not None : data [ 'name' ] = self . metricName if self . displayName is not None : data [ 'displayName' ] = self . displayName if self . displayNameShort is not None : data [ 'displayNameShort' ] = self . displayNameShort if self . description is not None : data [ 'description' ] = self . description if self . aggregate is not None : data [ 'defaultAggregate' ] = self . aggregate if self . unit is not None : data [ 'unit' ] = self . unit if self . resolution is not None : data [ 'defaultResolutionMS' ] = self . resolution if self . isDisabled is not None : data [ 'isDisabled' ] = True if self . isDisabled == 'yes' else False if self . type is not None : data [ 'type' ] = self . type self . path = "v1/metrics/{0}" . format ( self . metricName ) self . data = json . dumps ( data , sort_keys = True ) self . headers = { 'Content-Type' : 'application/json' , "Accept" : "application/json" }
def get_arguments ( self ) : ApiCli . get_arguments ( self ) self . _alarm_name = self . args . alarm_name if self . args . alarm_name is not None else None
def read ( self ) : f = open ( self . path , "r" ) self . manifest_json = f . read ( )
def load ( self ) : manifest = PluginManifest ( self . file_path ) manifest . get ( ) self . manifest = manifest . get_manifest ( )
def getMetricDefinition ( self , name ) : metric = None for m in self . metric_definitions : if m [ 'name' ] == name : metric = m break return metric
def printMetricsHeader ( self , m , d ) : mstr = "Metric Name" dstr = "Description" print ( '|{0}{1}|{2}{3}|' . format ( mstr , ' ' * ( m - len ( mstr ) ) , dstr , ' ' * ( d - len ( dstr ) ) ) ) print ( '|:{0}|:{1}|' . format ( '-' * ( m - 1 ) , '-' * ( d - 1 ) ) )
def getFieldsColumnLengths ( self ) : nameLen = 0 descLen = 0 for f in self . fields : nameLen = max ( nameLen , len ( f [ 'title' ] ) ) descLen = max ( descLen , len ( f [ 'description' ] ) ) return ( nameLen , descLen )
def getMetricsColumnLengths ( self ) : displayLen = 0 descLen = 0 for m in self . metrics : displayLen = max ( displayLen , len ( m [ 'displayName' ] ) ) descLen = max ( descLen , len ( m [ 'description' ] ) ) return ( displayLen , descLen )
def escapeUnderscores ( self ) : new_metrics = [ ] for m in self . metrics : m [ 'name' ] = m [ 'name' ] . replace ( "_" , "\_" ) new_metrics . append ( m ) self . metrics = new_metrics
def printFieldsHeader ( self , f , d ) : fstr = "Field Name" dstr = "Description" f = max ( f , len ( fstr ) ) d = max ( d , len ( dstr ) ) print ( '|{0}{1}|{2}{3}|' . format ( fstr , ' ' * ( f - len ( fstr ) ) , dstr , ' ' * ( d - len ( dstr ) ) ) ) print ( '|:{0}|:{1}|' . format ( '-' * ( f - 1 ) , '-' * ( d - 1 ) ) ) return ( f , d )
def printMetrics ( self , m , d ) : for metric in self . metrics : mstr = metric [ 'displayName' ] dstr = metric [ 'description' ] mlen = m - len ( mstr ) dlen = d - len ( dstr ) print ( "|{0}{1}|{2}{3}|" . format ( mstr , ' ' * mlen , dstr , ' ' * dlen ) )
def printFields ( self , f , d ) : for field in self . fields : fstr = field [ "title" ] dstr = field [ "description" ] flen = f - len ( fstr ) dlen = d - len ( dstr ) print ( "|{0}{1}|{2}{3}|" . format ( fstr , ' ' * flen , dstr , ' ' * dlen ) )
def outputFieldMarkdown ( self ) : f , d = self . getFieldsColumnLengths ( ) fc , dc = self . printFieldsHeader ( f , d ) f = max ( fc , f ) d = max ( dc , d ) self . printFields ( f , d )
def outputMetricMarkdown ( self ) : self . escapeUnderscores ( ) m , d = self . getMetricsColumnLengths ( ) self . printMetricsHeader ( m , d ) self . printMetrics ( m , d )
def generateMarkdown ( self ) : self . generateMetricDefinitions ( ) self . generateFieldDefinitions ( ) self . generateDashboardDefinitions ( ) self . outputMarkdown ( )
def parse ( self , text ) : self . original_text = text try : return getattr ( self , self . entry_point ) ( text ) except ( DeadEnd ) as exc : raise ParserError ( self . most_consumed , "Failed to parse input" ) from exc return tree
def _attempting ( self , text ) : consumed = len ( self . original_text ) - len ( text ) self . most_consumed = max ( consumed , self . most_consumed )
def add_arguments ( self ) : ApiCli . add_arguments ( self ) self . parser . add_argument ( '-f' , '--format' , dest = 'format' , action = 'store' , required = False , choices = [ 'csv' , 'json' , 'raw' , 'xml' ] , help = 'Output format. Default is raw' ) self . parser . add_argument ( '-n' , '--name' , dest = 'metric_name' , action = 'store' , required = True , metavar = "metric_name" , help = 'Metric identifier' ) self . parser . add_argument ( '-g' , '--aggregate' , dest = 'aggregate' , action = 'store' , required = False , choices = [ 'sum' , 'avg' , 'max' , 'min' ] , help = 'Metric default aggregate' ) self . parser . add_argument ( '-r' , '--sample' , dest = 'sample' , action = 'store' , type = int , metavar = "sample" , help = 'Down sample rate sample in seconds' ) self . parser . add_argument ( '-s' , '--source' , dest = 'source' , action = 'store' , metavar = "source" , required = True , help = 'Source of measurement' ) self . parser . add_argument ( '-b' , '--start' , dest = 'start' , action = 'store' , required = True , metavar = "start" , help = 'Start of time range as ISO 8601 string or epoch seconds' ) self . parser . add_argument ( '-d' , '--end' , dest = 'end' , action = 'store' , metavar = "end" , required = False , help = 'End of time range as ISO 8601 string or epoch seconds' ) self . parser . add_argument ( '-o' , '--date-format' , dest = 'date_format' , action = 'store' , metavar = "format" , required = False , help = 'For CSV, JSON, and XML output formats dates (see Python date.strftime). ' + 'Default format is %%s' )
def get_arguments ( self ) : ApiCli . get_arguments ( self ) if self . args . metric_name is not None : self . _metric_name = self . args . metric_name if self . args . sample is not None : self . sample = self . args . sample if self . args . source is not None : self . source = self . args . source else : self . source = None if self . args . aggregate is not None : self . aggregate = self . args . aggregate else : self . aggregate = "avg" if self . args . format is not None : self . format = self . args . format else : self . format = "json" if self . args . date_format is not None : self . date_format = self . args . date_format start_time = int ( self . parse_time_date ( self . args . start ) . strftime ( "%s" ) ) if self . args . end is None : stop_time = int ( self . now . strftime ( "%s" ) ) else : stop_time = int ( self . parse_time_date ( self . args . end ) . strftime ( "%s" ) ) start_time *= 1000 stop_time *= 1000 self . path = "v1/measurements/{0}" . format ( self . _metric_name ) url_parameters = { "start" : str ( start_time ) , "end" : str ( stop_time ) , "sample" : str ( self . sample ) , "agg" : self . aggregate } if self . source is not None : url_parameters [ 'source' ] = self . source self . url_parameters = url_parameters
def output_csv ( self , text ) : payload = json . loads ( text ) print ( "{0},{1},{2},{3},{4}" . format ( 'timestamp' , 'metric' , 'aggregate' , 'source' , 'value' ) ) metric_name = self . _metric_name for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : timestamp = self . _format_timestamp ( r [ 0 ] [ 0 ] ) for s in r [ 1 ] : print ( '{0},"{1}","{2}","{3}",{4}' . format ( timestamp , metric_name , self . aggregate , s [ 0 ] , s [ 1 ] ) )
def output_json ( self , text ) : payload = json . loads ( text ) data = [ ] metric_name = self . _metric_name for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : timestamp = self . _format_timestamp ( r [ 0 ] [ 0 ] ) for s in r [ 1 ] : data . append ( { "timestamp" : timestamp , "metric" : metric_name , "aggregate" : self . aggregate , "source" : s [ 0 ] , "value" : s [ 1 ] , } ) payload = { "data" : data } out = json . dumps ( payload , indent = self . _indent , separators = ( ',' , ': ' ) ) print ( self . colorize_json ( out ) )
def output_raw ( self , text ) : payload = json . loads ( text ) out = json . dumps ( payload , sort_keys = True , indent = self . _indent , separators = ( ',' , ': ' ) ) print ( self . colorize_json ( out ) )
def output_xml ( self , text ) : document = Element ( 'results' ) comment = Comment ( 'Generated by TrueSight Pulse measurement-get CLI' ) document . append ( comment ) aggregates = SubElement ( document , 'aggregates' ) aggregate = SubElement ( aggregates , 'aggregate' ) measurements = SubElement ( aggregate , 'measurements' ) payload = json . loads ( text ) metric_name = self . _metric_name for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : timestamp = self . _format_timestamp ( r [ 0 ] [ 0 ] ) for s in r [ 1 ] : measure_node = SubElement ( measurements , 'measure' ) source = s [ 0 ] value = str ( s [ 1 ] ) ts_node = SubElement ( measure_node , 'timestamp' ) ts_node . text = str ( timestamp ) metric_node = SubElement ( measure_node , 'metric' ) metric_node . text = metric_name metric_node = SubElement ( measure_node , 'aggregate' ) metric_node . text = self . aggregate source_node = SubElement ( measure_node , 'source' ) source_node . text = source value_node = SubElement ( measure_node , 'value' ) value_node . text = value rough_string = ElementTree . tostring ( document , 'utf-8' ) reparse = minidom . parseString ( rough_string ) output = reparse . toprettyxml ( indent = " " ) print ( self . colorize_xml ( output ) )
def _handle_results ( self ) : if self . _api_result . status_code == requests . codes . ok : if self . format == "json" : self . output_json ( self . _api_result . text ) elif self . format == "csv" : self . output_csv ( self . _api_result . text ) elif self . format == "raw" : self . output_raw ( self . _api_result . text ) elif self . format == "xml" : self . output_xml ( self . _api_result . text ) else : pass
def trimmed_pred_default ( node , parent ) : return isinstance ( node , ParseNode ) and ( node . is_empty or node . is_type ( ParseNodeType . terminal ) )
def pprint ( root , depth = 0 , space_unit = "    " , * , source_len = 0 , file = None ) : spacing = space_unit * depth if isinstance ( root , str ) : print ( "{0}terminal@(?): {1}" . format ( spacing , root ) , file = file ) else : if root . position is None : position = - 1 elif root . position < 0 : position = source_len + root . position else : position = root . position if root . is_value : print ( "{0}{1}@({2}:{3}):\t{4}" . format ( spacing , root . node_type , position , root . consumed , root . svalue ) , file = file ) else : print ( "{0}{1}@({2}:{3}):" . format ( spacing , root . node_type , position , root . consumed ) , file = file ) for child in root . children : pprint ( child , depth + 1 , source_len = source_len , file = file )
def repetition ( extractor , bounds , * , ignore_whitespace = False ) : return partial ( _get_repetition , extractor , bounds = bounds , ignore_whitespace = ignore_whitespace )
def _count_leading_whitespace ( text ) : idx = 0 for idx , char in enumerate ( text ) : if not char . isspace ( ) : return idx return idx + 1
def retyped ( self , new_type ) : return ParseNode ( new_type , children = list ( self . children ) , consumed = self . consumed , position = self . position , ignored = self . ignored )
def get_arguments ( self ) : ApiCli . get_arguments ( self ) if self . args . host_group_name is not None : self . host_group_name = self . args . host_group_name if self . args . sources is not None : self . sources = self . args . sources payload = { } if self . host_group_name is not None : payload [ 'name' ] = self . host_group_name if self . sources is not None : source_list = str . split ( self . sources , ',' ) if 'hostnames' not in payload : payload [ 'hostnames' ] = [ ] for s in source_list : payload [ 'hostnames' ] . append ( s ) self . data = json . dumps ( payload , sort_keys = True ) self . headers = { 'Content-Type' : 'application/json' , "Accept" : "application/json" }
def get_scope_list ( self ) -> list : lstparent = [ self ] p = self . get_parent ( ) while p is not None : lstparent . append ( p ) p = p . get_parent ( ) return lstparent
def get_scope_names ( self ) -> list : lscope = [ ] for scope in reversed ( self . get_scope_list ( ) ) : if scope . name is not None : lscope . append ( scope . name ) return lscope
def position ( self ) -> Position : return Position ( self . _index , self . _lineno , self . _col_offset )
def max_readed_position ( self ) -> Position : return Position ( self . _maxindex , self . _maxline , self . _maxcol )
def step_next_char ( self ) : self . _index += 1 self . _col_offset += 1 if self . _index > self . _maxindex : self . _maxindex = self . _index self . _maxcol = self . _col_offset self . _maxline = self . _lineno
def step_next_line ( self ) : self . _eol . append ( self . position ) self . _lineno += 1 self . _col_offset = 0
def step_prev_line ( self ) : #TODO(bps): raise explicit error for unregistered eol #assert self._eol[-1].index == self._index if len ( self . _eol ) > 0 : self . position = self . _eol . pop ( )
def last_readed_line ( self ) -> str : mpos = self . _cursor . max_readed_position mindex = mpos . index prevline = mindex - 1 if mindex == self . eos_index else mindex while prevline >= 0 and self . _content [ prevline ] != '\n' : prevline -= 1 nextline = mindex while nextline < self . eos_index and self . _content [ nextline ] != '\n' : nextline += 1 last_line = self . _content [ prevline + 1 : nextline ] return last_line
def incpos ( self , length : int = 1 ) -> int : if length < 0 : raise ValueError ( "length must be positive" ) i = 0 while ( i < length ) : if self . _cursor . index < self . _len : if self . peek_char == '\n' : self . _cursor . step_next_line ( ) self . _cursor . step_next_char ( ) i += 1 return self . _cursor . index
def save_context ( self ) -> bool : self . _contexts . append ( self . _cursor . position ) return True
def restore_context ( self ) -> bool : self . _cursor . position = self . _contexts . pop ( ) return False
def to_fmt ( self ) -> fmt . indentable : qual = "scope" txt = fmt . sep ( " " , [ qual ] ) name = self . show_name ( ) if name != "" : txt . lsdata . append ( name ) if len ( self . _hsig ) > 0 or len ( self . mapTypeTranslate ) > 0 : lsb = [ ] if len ( self . mapTypeTranslate ) > 0 : lsb . append ( "translate:\n" ) lsb . append ( fmt . end ( "\n" , self . mapTypeTranslate . to_fmt ( ) ) ) for k in sorted ( self . _hsig . keys ( ) ) : s = self . _hsig [ k ] lsb . append ( fmt . end ( "\n" , [ s . to_fmt ( ) ] ) ) block = fmt . block ( ":\n" , "" , fmt . tab ( lsb ) ) txt . lsdata . append ( block ) return txt
def to_fmt ( self ) : qual = "evalctx" lseval = [ ] block = fmt . block ( ":\n" , "" , fmt . tab ( lseval ) ) txt = fmt . sep ( " " , [ qual , block ] ) lseval . append ( self . _sig . to_fmt ( ) ) if len ( self . resolution ) > 0 : lsb = [ ] for k in sorted ( self . resolution . keys ( ) ) : s = self . resolution [ k ] if s is not None : lsb . append ( fmt . end ( "\n" , [ "'%s': %s (%s)" % ( k , s , s ( ) . show_name ( ) ) ] ) ) else : lsb . append ( fmt . end ( "\n" , [ "'%s': Unresolved" % ( k ) ] ) ) if self . _translate_to is not None : lsb . append ( "use translator:" ) lsb . append ( self . _translate_to . to_fmt ( ) ) if self . _variadic_types is not None : lsb . append ( "variadic types:\n" ) arity = self . _sig . arity for t in self . _variadic_types : lsb . append ( "[%d] : %s\n" % ( arity , t ) ) arity += 1 lseval . append ( fmt . block ( "\nresolution :\n" , "" , fmt . tab ( lsb ) ) ) return txt
def to_fmt ( self , with_from = False ) -> fmt . indentable : txt = fmt . sep ( "\n" , [ fmt . sep ( " " , [ self . _type_source , "to" , self . _type_target , '=' , self . _fun . to_fmt ( ) ] ) , self . _notify . get_content ( with_from ) ] ) return txt
def to_fmt ( self ) : params = "" txt = fmt . sep ( " " , [ 'val' ] ) name = self . show_name ( ) if name != "" : txt . lsdata . append ( name ) txt . lsdata . append ( '(%s)' % self . value ) txt . lsdata . append ( ': ' + self . tret ) return txt
def to_fmt ( self ) : params = "" txt = fmt . sep ( " " , [ 'fun' ] ) name = self . show_name ( ) if name != "" : txt . lsdata . append ( name ) tparams = [ ] if self . tparams is not None : tparams = list ( self . tparams ) if self . variadic : tparams . append ( '...' ) params = '(' + ", " . join ( tparams ) + ')' txt . lsdata . append ( ': ' + params ) txt . lsdata . append ( '-> ' + self . tret ) return txt
def set_name ( self , name : str ) : self . name = name lsig = self . _hsig . values ( ) self . _hsig = { } for s in lsig : self . _hsig [ s . internal_name ( ) ] = s
def count_vars ( self ) -> int : n = 0 for s in self . _hsig . values ( ) : if hasattr ( s , 'is_var' ) and s . is_var : n += 1 return n
def count_funs ( self ) -> int : n = 0 for s in self . _hsig . values ( ) : if hasattr ( s , 'is_fun' ) and s . is_fun : n += 1 return n
def update ( self , sig : list or Scope ) -> Scope : values = sig if hasattr ( sig , 'values' ) : values = sig . values ( ) for s in values : if self . is_namespace : s . set_parent ( self ) if isinstance ( s , Scope ) : s . state = StateScope . EMBEDDED self . _hsig [ s . internal_name ( ) ] = s self . __update_count ( ) return self
def union ( self , sig : Scope ) -> Scope : new = Scope ( sig = self . _hsig . values ( ) , state = self . state ) new |= sig return new
def intersection_update ( self , oset : Scope ) -> Scope : keys = list ( self . _hsig . keys ( ) ) for k in keys : if k not in oset : del self . _hsig [ k ] else : self . _hsig [ k ] = oset . get ( k ) return self
def intersection ( self , sig : Scope ) -> Scope : new = Scope ( sig = self . _hsig . values ( ) , state = self . state ) new &= sig return new
def difference_update ( self , oset : Scope ) -> Scope : keys = list ( self . _hsig . keys ( ) ) for k in keys : if k in oset : del self . _hsig [ k ] return self
def difference ( self , sig : Scope ) -> Scope : new = Scope ( sig = self . _hsig . values ( ) , state = self . state ) new -= sig return new
def symmetric_difference ( self , sig : Scope ) -> Scope : new = Scope ( sig = self . _hsig . values ( ) , state = self . state ) new ^= sig return new
def add ( self , it : Signature ) -> bool : if isinstance ( it , Scope ) : it . state = StateScope . EMBEDDED txt = it . internal_name ( ) it . set_parent ( self ) if self . is_namespace : txt = it . internal_name ( ) if txt == "" : txt = '_' + str ( len ( self . _hsig ) ) if txt in self . _hsig : raise KeyError ( "Already exists %s" % txt ) self . _hsig [ txt ] = it self . __update_count ( ) return True
def remove ( self , it : Signature ) -> bool : txt = it . internal_name ( ) if txt not in self . _hsig : raise KeyError ( it . show_name ( ) + ' not in Set' ) sig = self . _hsig [ txt ] if isinstance ( sig , Scope ) : sig . state = StateScope . LINKED del self . _hsig [ txt ] return True
def discard ( self , it : Signature ) -> bool : txt = it . internal_name ( ) if txt in self . _hsig : sig = self . _hsig [ txt ] if isinstance ( sig , Scope ) : sig . state = StateScope . LINKED del self . _hsig [ txt ] return True return False
def first ( self ) -> Signature : k = sorted ( self . _hsig . keys ( ) ) return self . _hsig [ k [ 0 ] ]
def last ( self ) -> Signature : k = sorted ( self . _hsig . keys ( ) ) return self . _hsig [ k [ - 1 ] ]
def get ( self , key : str , default = None ) -> Signature : item = default if key in self . _hsig : item = self . _hsig [ key ] return item
def get_by_symbol_name ( self , name : str ) -> Scope : lst = [ ] for s in self . values ( ) : if s . name == name : lst . append ( EvalCtx . from_sig ( s ) ) if len ( lst ) == 0 : p = self . get_parent ( ) if p is not None : return p . get_by_symbol_name ( name ) rscope = Scope ( sig = lst , state = StateScope . LINKED , is_namespace = False ) rscope . set_parent ( self ) return rscope
def callInjector ( self , old : Node , trans : Translator ) -> Node : if self . astTranslatorInjector is None : if self . parent is not None : return self . parent ( ) . callInjector ( old , trans ) else : raise TypeError ( "Must define an Translator Injector" ) return self . astTranslatorInjector ( old , trans )
def set ( self , othernode ) : self . __class__ = othernode . __class__ self . clean ( ) if len ( othernode ) > 0 : for k , v in othernode . items ( ) : self [ k ] = v for k , v in vars ( othernode ) . items ( ) : setattr ( self , k , v )
def _hit_ok ( hit , min_hit_charge , max_hit_charge ) : if hit [ 'charge' ] < min_hit_charge : return False if max_hit_charge != 0 and hit [ 'charge' ] > max_hit_charge : return False return True
def resolve ( self ) : t2resolv = [ ] if hasattr ( self . _sig , 'tret' ) : t2resolv . append ( self . _sig . tret ) if hasattr ( self . _sig , 'tparams' ) and self . _sig . tparams is not None : for p in self . _sig . tparams : t2resolv . append ( p ) if self . _translate_to is not None : t2resolv . append ( self . _translate_to . target ) if self . _variadic_types is not None : for t in self . _variadic_types : t2resolv . append ( t ) for t in t2resolv : for c in t . components : if c not in self . resolution or self . resolution [ c ] is None : parent = self . get_parent ( ) if parent is not None : sc = parent . get_by_symbol_name ( c ) if len ( sc ) == 1 : sc = list ( sc . values ( ) ) [ 0 ] if isinstance ( sc , EvalCtx ) : sc = sc . _sig rtyp = weakref . ref ( sc ) self . resolution [ c ] = rtyp continue self . resolution [ c ] = None
def set_resolved_name ( self , ref : dict , type_name2solve : TypeName , type_name_ref : TypeName ) : if self . resolution [ type_name2solve . value ] is None : self . resolution [ type_name2solve . value ] = ref [ type_name_ref . value ]
def to_fmt ( self ) -> fmt . indentable : lsb = [ ] if len ( self . _lsig ) > 0 : for s in self . _lsig : lsb . append ( s . to_fmt ( ) ) block = fmt . block ( "(" , ")" , fmt . sep ( ', ' , lsb ) ) qual = "tuple" txt = fmt . sep ( "" , [ qual , block ] ) return txt
def internal_name ( self ) : unq = super ( ) . internal_name ( ) if self . tret is not None : unq += "_" + self . tret return unq
def _delete_local ( self , filename ) : if os . path . exists ( filename ) : os . remove ( filename )
def _delete_s3 ( self , filename , bucket_name ) : conn = S3Connection ( self . access_key_id , self . access_key_secret ) bucket = conn . get_bucket ( bucket_name ) if type ( filename ) . __name__ == 'Key' : filename = '/' + filename . name path = self . _get_s3_path ( filename ) k = Key ( bucket ) k . key = path try : bucket . delete_key ( k ) except S3ResponseError : pass
def delete ( self , filename , storage_type = None , bucket_name = None ) : if not ( storage_type and bucket_name ) : self . _delete_local ( filename ) else : if storage_type != 's3' : raise ValueError ( 'Storage type "%s" is invalid, the only supported storage type (apart from default local storage) is s3.' % storage_type ) self . _delete_s3 ( filename , bucket_name )
def _save_local ( self , temp_file , filename , obj ) : path = self . _get_path ( filename ) if not os . path . exists ( os . path . dirname ( path ) ) : os . makedirs ( os . path . dirname ( path ) , self . permission | 0o111 ) fd = open ( path , 'wb' ) temp_file . seek ( 0 ) t = temp_file . read ( 1048576 ) while t : fd . write ( t ) t = temp_file . read ( 1048576 ) fd . close ( ) if self . filesize_field : setattr ( obj , self . filesize_field , os . path . getsize ( path ) ) return filename
def _save_s3 ( self , temp_file , filename , obj ) : conn = S3Connection ( self . access_key_id , self . access_key_secret ) bucket = conn . get_bucket ( self . bucket_name ) path = self . _get_s3_path ( filename ) k = bucket . new_key ( path ) k . set_contents_from_string ( temp_file . getvalue ( ) ) k . set_acl ( self . acl ) if self . filesize_field : setattr ( obj , self . filesize_field , k . size ) return filename
def save ( self , temp_file , filename , obj ) : if not ( self . storage_type and self . bucket_name ) : ret = self . _save_local ( temp_file , filename , obj ) else : if self . storage_type != 's3' : raise ValueError ( 'Storage type "%s" is invalid, the only supported storage type (apart from default local storage) is s3.' % self . storage_type ) ret = self . _save_s3 ( temp_file , filename , obj ) if self . field_name : setattr ( obj , self . field_name , ret ) if self . storage_type == 's3' : if self . storage_type_field : setattr ( obj , self . storage_type_field , self . storage_type ) if self . bucket_name_field : setattr ( obj , self . bucket_name_field , self . bucket_name ) else : if self . storage_type_field : setattr ( obj , self . storage_type_field , '' ) if self . bucket_name_field : setattr ( obj , self . bucket_name_field , '' ) return ret
def _find_by_path_s3 ( self , path , bucket_name ) : conn = S3Connection ( self . access_key_id , self . access_key_secret ) bucket = conn . get_bucket ( bucket_name ) s3_path = self . _get_s3_path ( path ) return bucket . list ( prefix = s3_path )
def enum ( * sequential , * * named ) : #: build enums from parameter enums = dict ( zip ( sequential , range ( len ( sequential ) ) ) , * * named ) enums [ 'map' ] = copy . copy ( enums ) #: build reverse mapping enums [ 'rmap' ] = { } for key , value in enums . items ( ) : if type ( value ) is int : enums [ 'rmap' ] [ value ] = key return type ( 'Enum' , ( ) , enums )
def checktypes ( func ) : sig = inspect . signature ( func ) types = { } for param in sig . parameters . values ( ) : param_type = param . annotation if param_type is param . empty or not inspect . isclass ( param_type ) : continue types [ param . name ] = param_type if ( param . default is not param . empty and not isinstance ( param . default , param_type ) ) : raise ValueError ( "{func}: wrong type of a default value for {arg!r}" . format ( func = func . __qualname__ , arg = param . name ) ) def check_type ( sig , arg_name , arg_type , arg_value ) : if not isinstance ( arg_value , arg_type ) : raise ValueError ( "{func}: wrong type of {arg!r} argument, " "{exp!r} expected, got {got!r}" . format ( func = func . __qualname__ , arg = arg_name , exp = arg_type . __name__ , got = type ( arg_value ) . __name__ ) ) @ functools . wraps ( func ) def wrapper ( * args , * * kwargs ) : ba = sig . bind ( * args , * * kwargs ) for arg_name , arg in ba . arguments . items ( ) : try : type_ = types [ arg_name ] except KeyError : continue else : param = sig . parameters [ arg_name ] if param . kind == param . VAR_POSITIONAL : for value in arg : check_type ( sig , arg_name , type_ , value ) elif param . kind == param . VAR_KEYWORD : for subname , value in arg . items ( ) : check_type ( sig , arg_name + ':' + subname , type_ , value ) else : check_type ( sig , arg_name , type_ , arg ) result = func ( * ba . args , * * ba . kwargs ) return_type = sig . return_annotation if ( return_type is not sig . empty and isinstance ( return_type , type ) and not isinstance ( result , return_type ) ) : raise ValueError ( '{func}: wrong return type, {exp} expected, got {got}' . format ( func = func . __qualname__ , exp = return_type . __name__ , got = type ( result ) . __name__ ) ) return result return wrapper
def add_method ( cls ) : def wrapper ( f ) : #if hasattr(cls, f.__name__): setattr ( cls , f . __name__ , f ) return f return wrapper
def read_eol ( self ) -> bool : if self . read_eof ( ) : return False self . _stream . save_context ( ) self . read_char ( '\r' ) if self . read_char ( '\n' ) : return self . _stream . validate_context ( ) return self . _stream . restore_context ( )
def push_rule_nodes ( self ) -> bool : if self . rule_nodes is None : self . rule_nodes = collections . ChainMap ( ) self . tag_cache = collections . ChainMap ( ) self . id_cache = collections . ChainMap ( ) else : self . rule_nodes = self . rule_nodes . new_child ( ) self . tag_cache = self . tag_cache . new_child ( ) self . id_cache = self . id_cache . new_child ( ) return True
def pop_rule_nodes ( self ) -> bool : self . rule_nodes = self . rule_nodes . parents self . tag_cache = self . tag_cache . parents self . id_cache = self . id_cache . parents return True
def value ( self , n : Node ) -> str : id_n = id ( n ) idcache = self . id_cache if id_n not in idcache : return "" name = idcache [ id_n ] tag_cache = self . tag_cache if name not in tag_cache : raise Exception ( "Incoherent tag cache" ) tag = tag_cache [ name ] k = "%d:%d" % ( tag . _begin , tag . _end ) valcache = self . _streams [ - 1 ] . value_cache if k not in valcache : valcache [ k ] = str ( tag ) return valcache [ k ]
def begin_tag ( self , name : str ) -> Node : self . tag_cache [ name ] = Tag ( self . _stream , self . _stream . index ) return True
def end_tag ( self , name : str ) -> Node : self . tag_cache [ name ] . set_end ( self . _stream . index ) return True
def set_rules ( cls , rules : dict ) -> bool : cls . _rules = cls . _rules . new_child ( ) for rule_name , rule_pt in rules . items ( ) : if '.' not in rule_name : rule_name = cls . __module__ + '.' + cls . __name__ + '.' + rule_name meta . set_one ( cls . _rules , rule_name , rule_pt ) return True
def set_hooks ( cls , hooks : dict ) -> bool : cls . _hooks = cls . _hooks . new_child ( ) for hook_name , hook_pt in hooks . items ( ) : if '.' not in hook_name : hook_name = cls . __module__ + '.' + cls . __name__ + '.' + hook_name meta . set_one ( cls . _hooks , hook_name , hook_pt ) return True
def eval_rule ( self , name : str ) -> Node : n = Node ( ) id_n = id ( n ) self . rule_nodes [ '_' ] = n self . id_cache [ id_n ] = '_' if name not in self . __class__ . _rules : self . diagnostic . notify ( error . Severity . ERROR , "Unknown rule : %s" % name , error . LocationInfo . from_stream ( self . _stream , is_error = True ) ) raise self . diagnostic self . _lastRule = name rule_to_eval = self . __class__ . _rules [ name ] res = rule_to_eval ( self ) if res : res = self . rule_nodes [ '_' ] return res
def eval_hook ( self , name : str , ctx : list ) -> Node : if name not in self . __class__ . _hooks : self . diagnostic . notify ( error . Severity . ERROR , "Unknown hook : %s" % name , error . LocationInfo . from_stream ( self . _stream , is_error = True ) ) raise self . diagnostic self . _lastRule = '#' + name res = self . __class__ . _hooks [ name ] ( self , * ctx ) if type ( res ) is not bool : raise TypeError ( "Your hook %r didn't return a bool value" % name ) return res
def peek_text ( self , text : str ) -> bool : start = self . _stream . index stop = start + len ( text ) if stop > self . _stream . eos_index : return False return self . _stream [ self . _stream . index : stop ] == text
def one_char ( self ) -> bool : if self . read_eof ( ) : return False self . _stream . incpos ( ) return True
def read_until_eof ( self ) -> bool : if self . read_eof ( ) : return True self . _stream . save_context ( ) while not self . read_eof ( ) : self . _stream . incpos ( ) return self . _stream . validate_context ( )
def ignore_blanks ( self ) -> bool : self . _stream . save_context ( ) if not self . read_eof ( ) and self . _stream . peek_char in " \t\v\f\r\n" : while ( not self . read_eof ( ) and self . _stream . peek_char in " \t\v\f\r\n" ) : self . _stream . incpos ( ) return self . _stream . validate_context ( ) return self . _stream . validate_context ( )
def internal_name ( self ) : unq = 'f_' + super ( ) . internal_name ( ) if self . tparams is not None : unq += "_" + "_" . join ( self . tparams ) if self . tret is not None : unq += "_" + self . tret return unq
def _check_struct_compatibility ( self , hits ) : for key , _ in self . _cluster_hits_descr : if key in self . _hit_fields_mapping_inverse : mapped_key = self . _hit_fields_mapping_inverse [ key ] else : mapped_key = key if mapped_key in [ 'cluster_ID' , 'is_seed' , 'cluster_size' , 'n_cluster' ] : continue if key not in hits . dtype . names : raise TypeError ( 'Required hit field "%s" not found.' % key ) if self . _cluster_hits . dtype [ mapped_key ] != hits . dtype [ key ] : raise TypeError ( 'The dtype for hit data field "%s" does not match. Got/expected: %s/%s.' % ( key , hits . dtype [ key ] , self . _cluster_hits . dtype [ mapped_key ] ) ) additional_hit_fields = set ( hits . dtype . names ) - set ( [ key for key , val in self . _cluster_hits_descr ] ) if additional_hit_fields : logging . warning ( 'Found additional hit fields: %s' % ", " . join ( additional_hit_fields ) )
def add_mod ( self , seq , mod ) : modstr = self . value ( mod ) if modstr == '~' : seq . parser_tree = parsing . Complement ( seq . parser_tree ) elif modstr == '!!' : seq . parser_tree = parsing . LookAhead ( seq . parser_tree ) elif modstr == '!' : seq . parser_tree = parsing . Neg ( seq . parser_tree ) elif modstr == '->' : seq . parser_tree = parsing . Until ( seq . parser_tree ) return True
def add_ruleclause_name ( self , ns_name , rid ) -> bool : ns_name . parser_tree = parsing . Rule ( self . value ( rid ) ) return True
def add_rules ( self , bnf , r ) -> bool : bnf [ r . rulename ] = r . parser_tree return True
def add_rule ( self , rule , rn , alts ) -> bool : rule . rulename = self . value ( rn ) rule . parser_tree = alts . parser_tree return True
def add_sequences ( self , sequences , cla ) -> bool : if not hasattr ( sequences , 'parser_tree' ) : sequences . parser_tree = cla . parser_tree else : oldnode = sequences if isinstance ( oldnode . parser_tree , parsing . Seq ) : oldpt = list ( oldnode . parser_tree . ptlist ) else : oldpt = [ oldnode . parser_tree ] oldpt . append ( cla . parser_tree ) sequences . parser_tree = parsing . Seq ( * tuple ( oldpt ) ) return True
def add_alt ( self , alternatives , alt ) -> bool : if not hasattr ( alternatives , 'parser_tree' ) : if hasattr ( alt , 'parser_tree' ) : alternatives . parser_tree = alt . parser_tree else : alternatives . parser_tree = alt else : oldnode = alternatives if isinstance ( oldnode . parser_tree , parsing . Alt ) : oldpt = list ( oldnode . parser_tree . ptlist ) else : oldpt = [ oldnode . parser_tree ] oldpt . append ( alt . parser_tree ) alternatives . parser_tree = parsing . Alt ( * tuple ( oldpt ) ) return True
def add_range ( self , sequence , begin , end ) : sequence . parser_tree = parsing . Range ( self . value ( begin ) . strip ( "'" ) , self . value ( end ) . strip ( "'" ) ) return True
def add_rpt ( self , sequence , mod , pt ) : modstr = self . value ( mod ) if modstr == '!!' : self . _stream . restore_context ( ) self . diagnostic . notify ( error . Severity . ERROR , "Cannot repeat a lookahead rule" , error . LocationInfo . from_stream ( self . _stream , is_error = True ) ) raise self . diagnostic if modstr == '!' : self . _stream . restore_context ( ) self . diagnostic . notify ( error . Severity . ERROR , "Cannot repeat a negated rule" , error . LocationInfo . from_stream ( self . _stream , is_error = True ) ) raise self . diagnostic oldnode = sequence sequence . parser_tree = pt . functor ( oldnode . parser_tree ) return True
def add_capture ( self , sequence , cpt ) : cpt_value = self . value ( cpt ) sequence . parser_tree = parsing . Capture ( cpt_value , sequence . parser_tree ) return True
def add_bind ( self , sequence , cpt ) : cpt_value = self . value ( cpt ) sequence . parser_tree = parsing . Bind ( cpt_value , sequence . parser_tree ) return True
def add_hook ( self , sequence , h ) : sequence . parser_tree = parsing . Hook ( h . name , h . listparam ) return True
def param_num ( self , param , n ) : param . pair = ( int ( self . value ( n ) ) , int ) return True
def param_str ( self , param , s ) : param . pair = ( self . value ( s ) . strip ( '"' ) , str ) return True
def param_char ( self , param , c ) : param . pair = ( self . value ( c ) . strip ( "'" ) , str ) return True
def param_id ( self , param , i ) : param . pair = ( self . value ( i ) , parsing . Node ) return True
def hook_name ( self , hook , n ) : hook . name = self . value ( n ) hook . listparam = [ ] return True
def hook_param ( self , hook , p ) : hook . listparam . append ( p . pair ) return True
def add_directive2 ( self , sequence , d , s ) : sequence . parser_tree = parsing . Directive2 ( d . name , d . listparam , s . parser_tree ) return True
def add_directive ( self , sequence , d , s ) : if d . name in meta . _directives : the_class = meta . _directives [ d . name ] sequence . parser_tree = parsing . Directive ( the_class ( ) , d . listparam , s . parser_tree ) elif d . name in meta . _decorators : the_class = meta . _decorators [ d . name ] sequence . parser_tree = parsing . Decorator ( the_class , d . listparam , s . parser_tree ) else : raise TypeError ( "Unkown directive or decorator %s" % d . name ) return True
def ignore_cxx ( self ) -> bool : self . _stream . save_context ( ) while not self . read_eof ( ) : idxref = self . _stream . index if self . _stream . peek_char in " \t\v\f\r\n" : while ( not self . read_eof ( ) and self . _stream . peek_char in " \t\v\f\r\n" ) : self . _stream . incpos ( ) if self . peek_text ( "//" ) : while not self . read_eof ( ) and not self . peek_char ( "\n" ) : self . _stream . incpos ( ) if not self . read_char ( "\n" ) and self . read_eof ( ) : return self . _stream . validate_context ( ) if self . peek_text ( "/*" ) : while not self . read_eof ( ) and not self . peek_text ( "*/" ) : self . _stream . incpos ( ) if not self . read_text ( "*/" ) and self . read_eof ( ) : return self . _stream . restore_context ( ) if idxref == self . _stream . index : break return self . _stream . validate_context ( )
def add_state ( self , s : State ) : ids = id ( s ) uid = len ( self . states ) if ids not in self . states : self . states [ ids ] = ( uid , s )
def to_dot ( self ) -> str : txt = "" txt += "digraph S%d {\n" % id ( self ) if self . label is not None : txt += '\tlabel="%s";\n' % ( self . label + '\l' ) . replace ( '\n' , '\l' ) txt += "\trankdir=LR;\n" #txt += '\tlabelloc="t";\n' txt += '\tgraph [labeljust=l, labelloc=t, nojustify=true];\n' txt += "\tesep=1;\n" txt += '\tranksep="equally";\n' txt += "\tnode [shape = circle];\n" txt += "\tsplines = ortho;\n" for s in self . states . values ( ) : txt += s [ 1 ] . to_dot ( ) txt += "}\n" return txt
def to_dot_file ( self , fname : str ) : with open ( fname , 'w' ) as f : f . write ( self . to_dot ( ) )
def to_png_file ( self , fname : str ) : cmd = pipes . Template ( ) cmd . append ( 'dot -Tpng > %s' % fname , '-.' ) with cmd . open ( 'pipefile' , 'w' ) as f : f . write ( self . to_dot ( ) )
def to_fmt ( self ) -> str : infos = fmt . end ( ";\n" , [ ] ) s = fmt . sep ( ', ' , [ ] ) for ids in sorted ( self . states . keys ( ) ) : s . lsdata . append ( str ( ids ) ) infos . lsdata . append ( fmt . block ( '(' , ')' , [ s ] ) ) infos . lsdata . append ( "events:" + repr ( self . events ) ) infos . lsdata . append ( "named_events:" + repr ( list ( self . named_events . keys ( ) ) ) ) infos . lsdata . append ( "uid_events:" + repr ( list ( self . uid_events . keys ( ) ) ) ) return infos
def nextstate ( self , newstate , treenode = None , user_data = None ) : if newstate is None : return self if isinstance ( newstate , State ) and id ( newstate ) != id ( self ) : return newstate elif isinstance ( newstate , StateEvent ) : self . state_register . named_events [ newstate . name ] = True return newstate . st elif isinstance ( newstate , StatePrecond ) : return newstate . st elif isinstance ( newstate , StateHook ) : newstate . call ( treenode , user_data ) return newstate . st return self
def resetLivingState ( self ) : must_delete = [ ] l = len ( self . ls ) for idx , ls in zip ( range ( l ) , self . ls ) : ids = id ( ls [ 1 ] . thestate ( ) ) if ids == id ( ls [ 0 ] ) and ( ls [ 1 ] . have_finish or not ls [ 1 ] . alive ) : must_delete . append ( idx ) elif ls [ 1 ] . alive : ls [ 1 ] . alive = False for delete in reversed ( must_delete ) : self . ls . pop ( delete ) self . init_all ( )
def infer_block ( self , body , diagnostic = None ) : for e in body : e . infer_node = InferNode ( parent = self . infer_node ) e . infer_type ( diagnostic = diagnostic )
def infer_subexpr ( self , expr , diagnostic = None ) : expr . infer_node = InferNode ( parent = self . infer_node ) expr . infer_type ( diagnostic = diagnostic )
def list_dataset_uris ( cls , base_uri , config_path ) : uri_list = [ ] parse_result = generous_parse_uri ( base_uri ) bucket_name = parse_result . netloc bucket = boto3 . resource ( 's3' ) . Bucket ( bucket_name ) for obj in bucket . objects . filter ( Prefix = 'dtool' ) . all ( ) : uuid = obj . key . split ( '-' , 1 ) [ 1 ] uri = cls . generate_uri ( None , uuid , base_uri ) storage_broker = cls ( uri , config_path ) if storage_broker . has_admin_metadata ( ) : uri_list . append ( uri ) return uri_list
def list_overlay_names ( self ) : bucket = self . s3resource . Bucket ( self . bucket ) overlay_names = [ ] for obj in bucket . objects . filter ( Prefix = self . overlays_key_prefix ) . all ( ) : overlay_file = obj . key . rsplit ( '/' , 1 ) [ - 1 ] overlay_name , ext = overlay_file . split ( '.' ) overlay_names . append ( overlay_name ) return overlay_names
def iter_item_handles ( self ) : bucket = self . s3resource . Bucket ( self . bucket ) for obj in bucket . objects . filter ( Prefix = self . data_key_prefix ) . all ( ) : relpath = obj . get ( ) [ 'Metadata' ] [ 'handle' ] yield relpath
def list_set_indent ( lst : list , indent : int = 1 ) : for i in lst : if isinstance ( i , indentable ) : i . set_indent ( indent ) if isinstance ( i , list ) : list_set_indent ( i , indent )
def list_to_str ( lst : list , content : str , indent : int = 1 ) : for i in lst : if isinstance ( i , indentable ) : content = i . to_str ( content , indent ) elif isinstance ( i , list ) : content = list_to_str ( i , content , indent ) elif isinstance ( i , str ) : content = catend ( content , i , indent ) return content
def populate_from_sequence ( seq : list , r : ref ( Edge ) , sr : state . StateRegister ) : base_state = r idxlast = len ( seq ) - 1 idx = 0 for m in seq : if isinstance ( m , list ) : for item in m : populate_from_sequence ( item , r , sr ) elif isinstance ( m , MatchExpr ) : eX = r ( ) . get_next_edge ( m ) if eX is None : sX = None if idx != idxlast : sX = state . State ( sr ) sX . matchDefault ( base_state ( ) . s ) else : sX = base_state ( ) . s eX = Edge ( sX ) r ( ) . next_edge [ id ( sX ) ] = eX m . attach ( r ( ) . s , sX , sr ) r = ref ( eX ) idx += 1
def from_string ( bnf : str , entry = None , * optional_inherit ) -> Grammar : inherit = [ Grammar ] + list ( optional_inherit ) scope = { 'grammar' : bnf , 'entry' : entry } return build_grammar ( tuple ( inherit ) , scope )
def from_file ( fn : str , entry = None , * optional_inherit ) -> Grammar : import os . path if os . path . exists ( fn ) : f = open ( fn , 'r' ) bnf = f . read ( ) f . close ( ) inherit = [ Grammar ] + list ( optional_inherit ) scope = { 'grammar' : bnf , 'entry' : entry , 'source' : fn } return build_grammar ( tuple ( inherit ) , scope ) raise Exception ( "File not Found!" )
def parse ( self , source : str = None , entry : str = None ) -> parsing . Node : self . from_string = True if source is not None : self . parsed_stream ( source ) if entry is None : entry = self . entry if entry is None : raise ValueError ( "No entry rule name defined for {}" . format ( self . __class__ . __name__ ) ) return self . _do_parse ( entry )
def parse_file ( self , filename : str , entry : str = None ) -> parsing . Node : self . from_string = False import os . path with open ( filename , 'r' ) as f : self . parsed_stream ( f . read ( ) , os . path . abspath ( filename ) ) if entry is None : entry = self . entry if entry is None : raise ValueError ( "No entry rule name defined for {}" . format ( self . __class__ . __name__ ) ) return self . _do_parse ( entry )
def default_serializer ( o ) : defs = ( ( ( datetime . date , datetime . time ) , lambda x : x . isoformat ( ) , ) , ( ( datetime . datetime , ) , lambda x : dt2utc_timestamp ( x ) , ) , ) for types , fun in defs : if isinstance ( o , types ) : return fun ( o )
def dump ( deposition , from_date , with_json = True , latest_only = False , * * kwargs ) : dep_json = json . dumps ( deposition . __getstate__ ( ) , default = default_serializer ) dep_dict = json . loads ( dep_json ) dep_dict [ '_p' ] = { } dep_dict [ '_p' ] [ 'id' ] = deposition . id dep_dict [ '_p' ] [ 'created' ] = dt2utc_timestamp ( deposition . created ) dep_dict [ '_p' ] [ 'modified' ] = dt2utc_timestamp ( deposition . modified ) dep_dict [ '_p' ] [ 'user_id' ] = deposition . user_id dep_dict [ '_p' ] [ 'state' ] = deposition . state dep_dict [ '_p' ] [ 'has_sip' ] = deposition . has_sip ( ) dep_dict [ '_p' ] [ 'submitted' ] = deposition . submitted return dep_dict
def _get_recids_invenio12 ( from_date ) : from invenio . dbquery import run_sql return ( id [ 0 ] for id in run_sql ( 'select id_bibrec from ' 'bibrec_bibdoc as r join bibdoc as d on r.id_bibdoc=d.id ' 'where d.modification_date >=%s' , ( from_date , ) , run_on_slave = True ) )
def _get_recids_invenio2 ( from_date ) : from invenio . legacy . dbquery import run_sql return ( id [ 0 ] for id in run_sql ( 'select id_bibrec from ' 'bibrec_bibdoc as r join bibdoc as d on r.id_bibdoc=d.id ' 'where d.modification_date >=%s' , ( from_date , ) , run_on_slave = True ) )
def get_check ( ) : try : from invenio . dbquery import run_sql except ImportError : from invenio . legacy . dbquery import run_sql return ( run_sql ( 'select count(id) from bibdoc' , run_on_slave = True ) [ 0 ] [ 0 ] , [ id [ 0 ] for id in run_sql ( 'select id from bibdoc' , run_on_slave = True ) ] , )
def dump ( obj , from_date , with_json = True , latest_only = False , * * kwargs ) : return dict ( id = obj . id , client_id = obj . client_id , user_id = obj . user_id , token_type = obj . token_type , access_token = obj . access_token , refresh_token = obj . refresh_token , expires = dt2iso_or_empty ( obj . expires ) , _scopes = obj . _scopes , is_personal = obj . is_personal , is_internal = obj . is_internal )
def get ( * args , * * kwargs ) : try : from invenio . modules . accounts . models import UserEXT except ImportError : from invenio_accounts . models import UserEXT q = UserEXT . query return q . count ( ) , q . all ( )
def _get_modified_recids_invenio12 ( from_date ) : from invenio . search_engine import search_pattern from invenio . dbquery import run_sql return set ( ( id [ 0 ] for id in run_sql ( 'select id from bibrec where modification_date >= %s' , ( from_date , ) , run_on_slave = True ) ) ) , search_pattern
def _get_modified_recids_invenio2 ( from_date ) : from invenio . legacy . search_engine import search_pattern from invenio . modules . records . models import Record date = datetime . datetime . strptime ( from_date , '%Y-%m-%d %H:%M:%S' ) return set ( ( x [ 0 ] for x in Record . query . filter ( Record . modification_date >= date ) . values ( Record . id ) ) ) , search_pattern
def _get_collection_restrictions ( collection ) : try : from invenio . dbquery import run_sql from invenio . access_control_firerole import compile_role_definition except ImportError : from invenio . modules . access . firerole import compile_role_definition from invenio . legacy . dbquery import run_sql res = run_sql ( 'SELECT r.firerole_def_src, email ' 'FROM accROLE as r ' 'JOIN accROLE_accACTION_accARGUMENT ON r.id=id_accROLE ' 'JOIN accARGUMENT AS a ON a.id=id_accARGUMENT ' 'JOIN user_accROLE AS u ON r.id=u.id_accROLE ' 'JOIN user ON user.id=u.id_user ' 'WHERE a.keyword="collection" AND ' 'a.value=%s AND ' 'id_accACTION=(select id from accACTION where name="viewrestrcoll")' , ( collection , ) , run_on_slave = True ) fireroles = set ( ) users = set ( ) for f , u in res : fireroles . add ( compile_role_definition ( f ) ) users . add ( u ) return { 'fireroles' : list ( fireroles ) , 'users' : users }
def get_record_revisions ( recid , from_date ) : try : from invenio . dbquery import run_sql except ImportError : from invenio . legacy . dbquery import run_sql return run_sql ( 'SELECT job_date, marcxml ' 'FROM hstRECORD WHERE id_bibrec = %s AND job_date >= %s ' 'ORDER BY job_date ASC' , ( recid , from_date ) , run_on_slave = True )
def get_record_collections ( recid ) : try : from invenio . search_engine import ( get_all_collections_of_a_record , get_restricted_collections_for_recid ) except ImportError : from invenio . legacy . search_engine import ( get_all_collections_of_a_record , get_restricted_collections_for_recid ) collections = { 'all' : get_all_collections_of_a_record ( recid , recreate_cache_if_needed = False ) , } collections [ 'restricted' ] = dict ( ( coll , _get_collection_restrictions ( coll ) ) for coll in get_restricted_collections_for_recid ( recid , recreate_cache_if_needed = False ) ) return collections
def dump_record_json ( marcxml ) : try : from invenio . modules . records . api import Record d = Record . create ( marcxml , 'marc' ) return d . dumps ( clean = True ) except ImportError : from invenio . bibfield import create_record d = create_record ( marcxml , master_format = 'marc' ) return d . dumps ( )
def get ( query , from_date , * * kwargs ) : recids , search_pattern = get_modified_recids ( from_date ) recids = recids . union ( get_modified_bibdoc_recids ( from_date ) ) if query : recids = recids . intersection ( set ( search_pattern ( p = query . encode ( 'utf-8' ) ) ) ) return len ( recids ) , recids
def load_common ( model_cls , data ) : obj = model_cls ( * * data ) db . session . add ( obj ) db . session . commit ( )
def collect_things_entry_points ( ) : things = dict ( ) for entry_point in iter_entry_points ( group = 'invenio_migrator.things' ) : things [ entry_point . name ] = entry_point . load ( ) return things
def init_app_context ( ) : try : from invenio . base . factory import create_app app = create_app ( ) app . test_request_context ( '/' ) . push ( ) app . preprocess_request ( ) except ImportError : pass
def memoize ( func ) : cache = { } @ wraps ( func ) def wrap ( * args , * * kwargs ) : key = '{0}{1}' . format ( args , kwargs ) if key not in cache : cache [ key ] = func ( * args , * * kwargs ) return cache [ key ] return wrap
def get_connected_roles ( action_id ) : try : from invenio . access_control_admin import compile_role_definition except ImportError : from invenio . modules . access . firerole import compile_role_definition run_sql = _get_run_sql ( ) roles = { } res = run_sql ( 'select r.id, r.name, r.description, r.firerole_def_src, ' 'a.keyword, a.value, email from accROLE as r ' 'join accROLE_accACTION_accARGUMENT on r.id=id_accROLE ' 'join accARGUMENT as a on  a.id=id_accARGUMENT ' 'join user_accROLE as u on r.id=u.id_accROLE ' 'join user on user.id=u.id_user ' 'where id_accACTION=%s' , ( action_id , ) ) for r in res : role = roles . setdefault ( r [ 0 ] , { 'id' : r [ 0 ] , 'name' : r [ 1 ] , 'description' : r [ 2 ] , 'firerole_def' : r [ 3 ] , 'compiled_firerole_def' : compile_role_definition ( r [ 3 ] ) , 'users' : set ( ) , 'parameters' : { } } ) param = role [ 'parameters' ] . setdefault ( r [ 4 ] , set ( ) ) param . add ( r [ 5 ] ) role [ 'users' ] . add ( r [ 6 ] ) return six . itervalues ( roles )
def get ( query , * args , * * kwargs ) : run_sql = _get_run_sql ( ) actions = [ dict ( id = row [ 0 ] , name = row [ 1 ] , allowedkeywords = row [ 2 ] , optional = row [ 3 ] ) for action in query . split ( ',' ) for row in run_sql ( 'select id, name, description, allowedkeywords, optional ' 'from accACTION where name like %s' , ( action , ) , run_on_slave = True ) ] return len ( actions ) , actions
def load_token ( data ) : from invenio_oauth2server . models import Token data [ 'expires' ] = iso2dt_or_none ( data [ 'expires' ] ) load_common ( Token , data )
def config_imp_or_default ( app , config_var_imp , default ) : imp = app . config . get ( config_var_imp ) return import_string ( imp ) if imp else default
def init_app ( self , app ) : self . init_config ( app . config ) state = _InvenioMigratorState ( app ) app . extensions [ 'invenio-migrator' ] = state app . cli . add_command ( dumps ) return state
def dump ( obj , from_date , with_json = True , latest_only = False , * * kwargs ) : return dict ( name = obj . name , description = obj . description , website = obj . website , user_id = obj . user_id , client_id = obj . client_id , client_secret = obj . client_secret , is_confidential = obj . is_confidential , is_internal = obj . is_internal , _redirect_uris = obj . _redirect_uris , _default_scopes = obj . _default_scopes )
def _get_users_invenio12 ( * args , * * kwargs ) : from invenio . dbquery import run_sql , deserialize_via_marshal User = namedtuple ( 'User' , [ 'id' , 'email' , 'password' , 'password_salt' , 'note' , 'full_name' , 'settings' , 'nickname' , 'last_login' ] ) users = run_sql ( 'SELECT id, email, password, note, settings, nickname, last_login' ' FROM user' , run_on_slave = True ) return len ( users ) , [ User ( id = user [ 0 ] , email = user [ 1 ] , password = user [ 2 ] . decode ( 'latin1' ) , password_salt = user [ 1 ] , note = user [ 3 ] , full_name = user [ 5 ] , settings = deserialize_via_marshal ( user [ 4 ] ) if user [ 4 ] else { } , nickname = 'id_{0}' . format ( user [ 0 ] ) , last_login = user [ 6 ] ) for user in users ]
def _get_users_invenio2 ( * args , * * kwargs ) : from invenio . modules . accounts . models import User q = User . query return q . count ( ) , q . all ( )
def loadrecords ( sources , source_type , recid ) : if recid is not None : for source in sources : records = json . load ( source ) for item in records : if str ( item [ 'recid' ] ) == str ( recid ) : _loadrecord ( item , source_type , eager = True ) click . echo ( "Record '{recid}' loaded." . format ( recid = recid ) ) return click . echo ( "Record '{recid}' not found." . format ( recid = recid ) ) else : for idx , source in enumerate ( sources , 1 ) : click . echo ( 'Loading dump {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) data = json . load ( source ) with click . progressbar ( data ) as records : for item in records : _loadrecord ( item , source_type )
def inspectrecords ( sources , recid , entity = None ) : for idx , source in enumerate ( sources , 1 ) : click . echo ( 'Loading dump {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) data = json . load ( source ) if not recid : click . secho ( 'Record identifiers' , fg = 'green' ) total = 0 for r in ( d [ 'recid' ] for d in data ) : click . echo ( r ) total += 1 click . echo ( '{0} records found in dump.' . format ( total ) ) return data = list ( filter ( lambda d : d [ 'recid' ] == recid , data ) ) if not data : click . secho ( "Record not found." , fg = 'yellow' ) return for record in data : if entity is None : click . echo ( json . dumps ( record , indent = 2 ) ) if entity == 'files' : click . secho ( 'Files' , fg = 'green' ) click . echo ( json . dumps ( record [ 'files' ] , indent = 2 ) ) if entity == 'json' : click . secho ( 'Records (JSON)' , fg = 'green' ) for revision in record [ 'record' ] : click . secho ( 'Revision {0}' . format ( revision [ 'modification_datetime' ] ) , fg = 'yellow' ) click . echo ( json . dumps ( revision [ 'json' ] , indent = 2 ) ) if entity == 'marcxml' : click . secho ( 'Records (MARCXML)' , fg = 'green' ) for revision in record [ 'record' ] : click . secho ( 'Revision {0}' . format ( revision [ 'marcxml' ] ) , fg = 'yellow' ) click . echo ( revision )
def main ( port = 8888 ) : import tornado . ioloop routes = [ ] + TornadoProfiler ( ) . get_routes ( ) app = tornado . web . Application ( routes ) app . listen ( port ) tornado . ioloop . IOLoop . current ( ) . start ( )
def get ( self ) : sort = self . get_argument ( 'sort' , 'cum_time' ) count = self . get_argument ( 'count' , 20 ) strip_dirs = self . get_argument ( 'strip_dirs' , True ) error = '' sorts = ( 'num_calls' , 'cum_time' , 'total_time' , 'cum_time_per_call' , 'total_time_per_call' ) if sort not in sorts : error += "Invalid `sort` '%s', must be in %s." % ( sort , sorts ) try : count = int ( count ) except ( ValueError , TypeError ) : error += "Can't cast `count` '%s' to int." % count if count <= 0 : count = None strip_dirs = str ( strip_dirs ) . lower ( ) not in ( 'false' , 'no' , 'none' , 'null' , '0' , '' ) if error : self . write ( { 'error' : error } ) self . set_status ( 400 ) self . finish ( ) return try : statistics = get_profiler_statistics ( sort , count , strip_dirs ) self . write ( { 'statistics' : statistics } ) self . set_status ( 200 ) except TypeError : logger . exception ( 'Error while retrieving profiler statistics' ) self . write ( { 'error' : 'No stats available. Start and stop the profiler before trying to retrieve stats.' } ) self . set_status ( 404 ) self . finish ( )
def post ( self ) : if is_profiler_running ( ) : self . set_status ( 201 ) self . finish ( ) return start_profiling ( ) self . set_status ( 201 ) self . finish ( )
def post ( self ) : filename = self . get_argument ( 'filename' , 'dump.prof' ) CProfileWrapper . profiler . dump_stats ( filename ) self . finish ( )
def get ( self ) : CProfileWrapper . profiler . print_stats ( ) s = StringIO . StringIO ( ) sortby = 'cumulative' ps = pstats . Stats ( CProfileWrapper . profiler , stream = s ) . sort_stats ( sortby ) ps . print_stats ( ) self . set_status ( 200 ) self . write ( s . getvalue ( ) ) self . finish ( )
def delete ( self ) : CProfileWrapper . profiler . create_stats ( ) self . enable ( ) self . set_status ( 204 ) self . finish ( )
def post ( self ) : CProfileWrapper . profiler = cProfile . Profile ( ) CProfileWrapper . profiler . enable ( ) self . running = True self . set_status ( 201 ) self . finish ( )
def delete ( self ) : CProfileWrapper . profiler . disable ( ) self . running = False self . set_status ( 204 ) self . finish ( )
def get ( self ) : self . write ( { "running" : self . running } ) self . set_status ( 200 ) self . finish ( )
def disable_timestamp ( method ) : @ wraps ( method ) def wrapper ( * args , * * kwargs ) : result = None with correct_date ( ) : result = method ( * args , * * kwargs ) return result return wrapper
def _add_ones_dim ( arr ) : arr = arr [ ... , np . newaxis ] return np . concatenate ( ( arr , np . ones_like ( arr ) ) , axis = - 1 )
def create ( cls , dump ) : if not dump . data . get ( 'record' ) : try : PersistentIdentifier . get ( pid_type = 'recid' , pid_value = dump . recid ) except PIDDoesNotExistError : PersistentIdentifier . create ( 'recid' , dump . recid , status = PIDStatus . RESERVED ) db . session . commit ( ) return None dump . prepare_revisions ( ) dump . prepare_pids ( ) dump . prepare_files ( ) existing_files = [ ] if dump . record : existing_files = dump . record . get ( '_files' , [ ] ) record = cls . update_record ( revisions = dump . revisions , created = dump . created , record = dump . record ) pids = dump . missing_pids else : record = cls . create_record ( dump ) pids = dump . pids if pids : cls . create_pids ( record . id , pids ) if dump . files : cls . create_files ( record , dump . files , existing_files ) if dump . is_deleted ( record ) : cls . delete_record ( record ) return record
def create_record ( cls , dump ) : timestamp , data = dump . latest record = Record . create ( data ) record . model . created = dump . created . replace ( tzinfo = None ) record . model . updated = timestamp . replace ( tzinfo = None ) RecordIdentifier . insert ( dump . recid ) PersistentIdentifier . create ( pid_type = 'recid' , pid_value = str ( dump . recid ) , object_type = 'rec' , object_uuid = str ( record . id ) , status = PIDStatus . REGISTERED ) db . session . commit ( ) return cls . update_record ( revisions = dump . rest , record = record , created = dump . created )
def update_record ( cls , revisions , created , record ) : for timestamp , revision in revisions : record . model . json = revision record . model . created = created . replace ( tzinfo = None ) record . model . updated = timestamp . replace ( tzinfo = None ) db . session . commit ( ) return Record ( record . model . json , model = record . model )
def create_pids ( cls , record_uuid , pids ) : for p in pids : PersistentIdentifier . create ( pid_type = p . pid_type , pid_value = p . pid_value , pid_provider = p . provider . pid_provider if p . provider else None , object_type = 'rec' , object_uuid = record_uuid , status = PIDStatus . REGISTERED , ) db . session . commit ( )
def delete_record ( cls , record ) : record . delete ( ) PersistentIdentifier . query . filter_by ( object_type = 'rec' , object_uuid = record . id , ) . update ( { PersistentIdentifier . status : PIDStatus . DELETED } ) cls . delete_buckets ( record ) db . session . commit ( )
def create_file ( self , bucket , key , file_versions ) : objs = [ ] for file_ver in file_versions : f = FileInstance . create ( ) . set_uri ( file_ver [ 'full_path' ] , file_ver [ 'size' ] , 'md5:{0}' . format ( file_ver [ 'checksum' ] ) , ) obj = ObjectVersion . create ( bucket , key ) . set_file ( f ) obj . created = arrow . get ( file_ver [ 'creation_date' ] ) . datetime . replace ( tzinfo = None ) objs . append ( obj ) db . session . commit ( ) return objs [ - 1 ]
def delete_buckets ( cls , record ) : files = record . get ( '_files' , [ ] ) buckets = set ( ) for f in files : buckets . add ( f . get ( 'bucket' ) ) for b_id in buckets : b = Bucket . get ( b_id ) b . deleted = True
def missing_pids ( self ) : missing = [ ] for p in self . pids : try : PersistentIdentifier . get ( p . pid_type , p . pid_value ) except PIDDoesNotExistError : missing . append ( p ) return missing
def prepare_files ( self ) : files = { } for f in self . data [ 'files' ] : k = f [ 'full_name' ] if k not in files : files [ k ] = [ ] files [ k ] . append ( f ) for k in files . keys ( ) : files [ k ] . sort ( key = lambda x : x [ 'version' ] ) self . files = files
def prepare_pids ( self ) : self . pids = [ ] for fetcher in self . pid_fetchers : val = fetcher ( None , self . revisions [ - 1 ] [ 1 ] ) if val : self . pids . append ( val )
def is_deleted ( self , record = None ) : record = record or self . revisions [ - 1 ] [ 1 ] return any ( col == 'deleted' for col in record . get ( 'collections' , [ ] ) )
def dump ( thing , query , from_date , file_prefix , chunk_size , limit , thing_flags ) : init_app_context ( ) file_prefix = file_prefix if file_prefix else '{0}_dump' . format ( thing ) kwargs = dict ( ( f . strip ( '-' ) . replace ( '-' , '_' ) , True ) for f in thing_flags ) try : thing_func = collect_things_entry_points ( ) [ thing ] except KeyError : click . Abort ( '{0} is not in the list of available things to migrate: ' '{1}' . format ( thing , collect_things_entry_points ( ) ) ) click . echo ( "Querying {0}..." . format ( thing ) ) count , items = thing_func . get ( query , from_date , limit = limit , * * kwargs ) progress_i = 0 click . echo ( "Dumping {0}..." . format ( thing ) ) with click . progressbar ( length = count ) as bar : for i , chunk_ids in enumerate ( grouper ( items , chunk_size ) ) : with open ( '{0}_{1}.json' . format ( file_prefix , i ) , 'w' ) as fp : fp . write ( "[\n" ) for _id in chunk_ids : try : json . dump ( thing_func . dump ( _id , from_date , * * kwargs ) , fp , default = set_serializer ) fp . write ( "," ) except Exception as e : click . secho ( "Failed dump {0} {1} ({2})" . format ( thing , _id , e . message ) , fg = 'red' ) progress_i += 1 bar . update ( progress_i ) fp . seek ( fp . tell ( ) - 1 ) fp . write ( "\n]" )
def check ( thing ) : init_app_context ( ) try : thing_func = collect_things_entry_points ( ) [ thing ] except KeyError : click . Abort ( '{0} is not in the list of available things to migrate: ' '{1}' . format ( thing , collect_things_entry_points ( ) ) ) click . echo ( "Querying {0}..." . format ( thing ) ) count , items = thing_func . get_check ( ) i = 0 click . echo ( "Checking {0}..." . format ( thing ) ) with click . progressbar ( length = count ) as bar : for _id in items : thing_func . check ( _id ) i += 1 bar . update ( i )
def write_reports ( self , relative_path , suite_name , reports , package_name = None ) : dest_path = self . reserve_file ( relative_path ) with open ( dest_path , 'wb' ) as outf : outf . write ( toxml ( reports , suite_name , package_name = package_name ) ) return dest_path
def toxml ( test_reports , suite_name , hostname = gethostname ( ) , package_name = "tests" ) : testsuites = et . Element ( "testsuites" ) testsuite = et . SubElement ( testsuites , "testsuite" ) test_count = len ( test_reports ) if test_count < 1 : raise ValueError ( 'there must be at least one test report' ) assert test_count > 0 , 'expecting at least one test' error_count = len ( [ r for r in test_reports if r . errors ] ) failure_count = len ( [ r for r in test_reports if r . failures ] ) ts = test_reports [ 0 ] . start_ts start_timestamp = datetime . fromtimestamp ( ts ) . isoformat ( ) total_duration = test_reports [ - 1 ] . end_ts - test_reports [ 0 ] . start_ts def quote_attribute ( value ) : return value if value is not None else "(null)" testsuite . attrib = dict ( id = "0" , errors = str ( error_count ) , failures = str ( failure_count ) , tests = str ( test_count ) , hostname = quote_attribute ( hostname ) , timestamp = quote_attribute ( start_timestamp ) , time = "%f" % total_duration , name = quote_attribute ( suite_name ) , package = quote_attribute ( package_name ) , ) for r in test_reports : test_name = r . name test_duration = r . end_ts - r . start_ts class_name = r . src_location testcase = et . SubElement ( testsuite , "testcase" ) testcase . attrib = dict ( name = test_name , classname = quote_attribute ( class_name ) , time = "%f" % test_duration , ) if r . errors or r . failures : if r . failures : failure = et . SubElement ( testcase , "failure" ) failure . attrib = dict ( type = "exception" , message = quote_attribute ( '\n' . join ( [ '%s' % e for e in r . failures ] ) ) , ) else : error = et . SubElement ( testcase , "error" ) error . attrib = dict ( type = "exception" , message = quote_attribute ( '\n' . join ( [ '%s' % e for e in r . errors ] ) ) , ) return et . tostring ( testsuites , encoding = "utf-8" )
def addMenu ( self , menu ) : self . menus [ menu . name ] = menu self . peng . sendEvent ( "peng3d:window.menu.add" , { "peng" : self . peng , "window" : self , "menu" : menu } )
def redraw_label ( self ) : sx , sy = self . size x , y = self . pos self . _label . anchor_x = "left" self . _label . x = x + sx / 2. + sx self . _label . y = y + sy / 2. + sy * .15 self . _label . _update ( )
def render3d ( self , view = None ) : super ( StaticWorld , self ) . render3d ( view ) self . batch3d . draw ( )
def on_redraw ( self ) : x , y = self . pos sx , sy = self . size self . bg_vlist . vertices = [ x , y , x + sx , y , x + sx , y + sy , x , y + sy ] self . stencil_vlist . vertices = [ x , y , x + sx , y , x + sx , y + sy , x , y + sy ] if isinstance ( self . bg , Background ) : if not self . bg . initialized : self . bg . init_bg ( ) self . bg . initialized = True self . bg . redraw_bg ( )
def doAction ( self , action ) : if not hasattr ( self , "actions" ) : return for f , args , kwargs in self . actions . get ( action , [ ] ) : f ( * args , * * kwargs )
def getSize ( self ) : return self . widget . size [ 0 ] - self . border [ 0 ] * 2 , self . widget . size [ 1 ] - self . border [ 1 ] * 2
def _make_conn ( shape ) : shape = np . array ( shape ) Ne = shape . prod ( ) if len ( shape ) == 2 : nx , ny = np . array ( shape ) + 1 conn = np . zeros ( ( Ne , 4 ) , dtype = np . int32 ) counter = 0 pattern = np . array ( [ 0 , 1 , 1 + nx , nx ] ) for j in range ( shape [ 1 ] ) : for i in range ( shape [ 0 ] ) : conn [ counter ] = pattern + 1 + i + j * nx counter += 1 if len ( shape ) == 3 : nx , ny , nz = np . array ( shape ) + 1 conn = np . zeros ( ( Ne , 8 ) , dtype = np . int32 ) counter = 0 pattern = np . array ( [ 0 , 1 , 1 + nx , nx , nx * ny , 1 + nx * ny , 1 + ( nx + 1 ) * ny , ( nx + 1 ) * ny ] ) for k in range ( shape [ 2 ] ) : for j in range ( shape [ 1 ] ) : for i in range ( shape [ 0 ] ) : conn [ counter ] = pattern + 1 + i + j * nx + k * nx * ny counter += 1 return conn
def set_fields ( self , fields = None , * * kwargs ) : self . fields = [ ] if fields != None : for field in fields : self . fields . append ( field )
def add_fields ( self , fields = None , * * kwargs ) : if fields != None : for field in fields : self . fields . append ( field )
def check_elements ( self ) : existing_types = set ( self . elements . type . argiope . values . flatten ( ) ) allowed_types = set ( ELEMENTS . keys ( ) ) if ( existing_types <= allowed_types ) == False : raise ValueError ( "Element types {0} not in know elements {1}" . format ( existing_types - allowed_types , allowed_types ) ) print ( "<Elements: OK>" )
def space ( self ) : return self . elements . type . argiope . map ( lambda t : ELEMENTS [ t ] . space )
def centroids_and_volumes ( self , sort_index = True ) : elements = self . elements out = [ ] for etype , group in self . elements . groupby ( [ ( "type" , "argiope" , "" ) ] ) : etype_info = ELEMENTS [ etype ] simplices_info = etype_info . simplices index = group . index simplices_data = self . split ( into = "simplices" , loc = index , at = "coords" ) simplices = simplices_data . values . reshape ( index . size , simplices_info . shape [ 0 ] , simplices_info . shape [ 1 ] , 3 ) edges = simplices [ : , : , 1 : ] - simplices [ : , : , : 1 ] simplices_centroids = simplices . mean ( axis = 2 ) if etype_info . space == 2 : simplices_volumes = np . linalg . norm ( np . cross ( edges [ : , : , 0 ] , edges [ : , : , 1 ] , axis = 2 ) , axis = 2 ) / 2. elif etype_info . space == 3 : simplices_volumes = ( np . cross ( edges [ : , : , 0 ] , edges [ : , : , 1 ] , axis = 2 ) * edges [ : , : , 2 ] ) . sum ( axis = 2 ) / 6. elements_volumes = simplices_volumes . sum ( axis = 1 ) elements_centroids = ( ( simplices_volumes . reshape ( * simplices_volumes . shape , 1 ) * simplices_centroids ) . sum ( axis = 1 ) / elements_volumes . reshape ( * elements_volumes . shape , 1 ) ) volumes_df = pd . DataFrame ( index = index , data = elements_volumes , columns = pd . MultiIndex . from_product ( [ [ "volume" ] , [ "" ] ] ) ) centroids_df = pd . DataFrame ( index = index , data = elements_centroids , columns = pd . MultiIndex . from_product ( [ [ "centroid" ] , [ "x" , "y" , "z" ] ] ) ) out . append ( pd . concat ( [ volumes_df , centroids_df ] , axis = 1 ) ) out = pd . concat ( out ) if sort_index : out . sort_index ( inplace = True ) return out . sort_index ( axis = 1 )
def edges ( self , zfill = 3 ) : edges = self . split ( "edges" , at = "coords" ) . unstack ( ) edges [ "lx" ] = edges . x [ 1 ] - edges . x [ 0 ] edges [ "ly" ] = edges . y [ 1 ] - edges . y [ 0 ] edges [ "lz" ] = edges . z [ 1 ] - edges . z [ 0 ] edges [ "l" ] = np . linalg . norm ( edges [ [ "lx" , "ly" , "lz" ] ] , axis = 1 ) edges = ( edges . l ) . unstack ( ) edges . columns = pd . MultiIndex . from_product ( [ [ "length" ] , [ "e" + "{0}" . format ( s ) . zfill ( zfill ) for s in np . arange ( edges . shape [ 1 ] ) ] ] ) edges [ ( "stats" , "lmax" ) ] = edges . length . max ( axis = 1 ) edges [ ( "stats" , "lmin" ) ] = edges . length . min ( axis = 1 ) edges [ ( "stats" , "aspect_ratio" ) ] = edges . stats . lmax / edges . stats . lmin return edges . sort_index ( axis = 1 )
def stats ( self ) : cv = self . centroids_and_volumes ( ) angles = self . angles ( ) edges = self . edges ( ) return pd . concat ( [ cv , angles [ [ "stats" ] ] , edges [ [ "stats" ] ] ] , axis = 1 ) . sort_index ( axis = 1 )
def element_set_to_node_set ( self , tag ) : nodes , elements = self . nodes , self . elements loc = ( elements . conn [ elements [ ( "sets" , tag , "" ) ] ] . stack ( ) . stack ( ) . unique ( ) ) loc = loc [ loc != 0 ] nodes [ ( "sets" , tag ) ] = False nodes . loc [ loc , ( "sets" , tag ) ] = True
def node_set_to_surface ( self , tag ) : nodes = self . nodes . copy ( ) dummy = nodes . iloc [ 0 ] . copy ( ) dummy [ "coords" ] *= np . nan dummy [ "sets" ] = True nodes . loc [ 0 ] = dummy element_surfaces = self . split ( "surfaces" ) . unstack ( ) surf = pd . DataFrame ( nodes . sets [ tag ] . loc [ element_surfaces . values . flatten ( ) ] . values . reshape ( element_surfaces . shape ) . prod ( axis = 1 ) . astype ( np . bool ) , index = element_surfaces . index ) . unstack ( ) . fillna ( False ) for k in surf . keys ( ) : self . elements [ "surfaces" , tag , "f{0}" . format ( k [ 1 ] + 1 ) ] = surf . loc [ : , k ]
def surface_to_element_sets ( self , tag ) : surface = self . elements . surfaces [ tag ] for findex in surface . keys ( ) : if surface [ findex ] . sum ( ) != 0 : self . elements [ ( "sets" , "_SURF_{0}_FACE{1}" . format ( tag , findex [ 1 : ] ) , "" ) ] = surface [ findex ]
def fields_metadata ( self ) : return ( pd . concat ( [ f . metadata ( ) for f in self . fields ] , axis = 1 ) . transpose ( ) . sort_values ( [ "step_num" , "frame" , "label" , "position" ] ) )
def metadata ( self ) : return pd . Series ( { "part" : self . part , "step_num" : self . step_num , "step_label" : self . step_label , "frame" : self . frame , "frame_value" : self . frame_value , "label" : self . label , "position" : self . position , } )
def make_directories ( self ) : if os . path . isdir ( self . workdir ) == False : os . mkdir ( self . workdir )
def run_postproc ( self ) : t0 = time . time ( ) if self . verbose : print ( '#### POST-PROCESSING "{0}" USING POST-PROCESSOR "{1}"'. f ormat( s elf. l abel,   self . solver . upper ( ) ) ) if self . solver == "abaqus" : command = '{0} viewer noGUI={1}_abqpp.py' . format ( self . solver_path , self . label ) process = subprocess . Popen ( command , cwd = self . workdir , shell = True , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) for line in iter ( process . stdout . readline , b'' ) : line = line . rstrip ( ) . decode ( 'utf8' ) print ( "    " , line ) t1 = time . time ( ) if self . verbose : print ( '  => POST-PROCESSED {0}: DURATION = {1:.2f}s >' . format ( self . label , t1 - t0 ) )
def run_gmsh ( self ) : argiope . utils . run_gmsh ( gmsh_path = self . gmsh_path , gmsh_space = self . gmsh_space , gmsh_options = self . gmsh_options , name = self . file_name + ".geo" , workdir = self . workdir ) self . mesh = argiope . mesh . read_msh ( self . workdir + self . file_name + ".msh" )
def read_history_report ( path , steps , x_name = None ) : data = pd . read_csv ( path , delim_whitespace = True ) if x_name != None : data [ x_name ] = data . X del data [ "X" ] data [ "step" ] = 0 t = 0. for i in range ( len ( steps ) ) : dt = steps [ i ] . duration loc = data [ data . t == t ] . index if len ( loc ) == 2 : data . loc [ loc [ 1 ] : , "step" ] = i t += dt return data
def read_field_report ( path , data_flag = "*DATA" , meta_data_flag = "*METADATA" ) : text = open ( path ) . read ( ) mdpos = text . find ( meta_data_flag ) dpos = text . find ( data_flag ) mdata = io . StringIO ( "\n" . join ( text [ mdpos : dpos ] . split ( "\n" ) [ 1 : ] ) ) data = io . StringIO ( "\n" . join ( text [ dpos : ] . split ( "\n" ) [ 1 : ] ) ) data = pd . read_csv ( data , index_col = 0 ) data = data . groupby ( data . index ) . mean ( ) mdata = pd . read_csv ( mdata , sep = "=" , header = None , index_col = 0 ) [ 1 ] mdata = mdata . to_dict ( ) out = { } out [ "step_num" ] = int ( mdata [ "step_num" ] ) out [ "step_label" ] = mdata [ "step_label" ] out [ "frame" ] = int ( mdata [ "frame" ] ) out [ "frame_value" ] = float ( mdata [ "frame_value" ] ) out [ "part" ] = mdata [ "instance" ] position_map = { "NODAL" : "node" , "ELEMENT_CENTROID" : "element" , "WHOLE_ELEMENT" : "element" } out [ "position" ] = position_map [ mdata [ "position" ] ] out [ "label" ] = mdata [ "label" ] out [ "data" ] = data field_class = getattr ( argiope . mesh , mdata [ "argiope_class" ] ) return field_class ( * * out )
def list_to_string ( l = range ( 200 ) , width = 40 , indent = "  " ) : l = [ str ( v ) + "," for v in l ] counter = 0 out = "" + indent for w in l : s = len ( w ) if counter + s > width : out += "\n" + indent counter = 0 out += w counter += s return out . strip ( "," )
def _equation ( nodes = ( 1 , 2 ) , dofs = ( 1 , 1 ) , coefficients = ( 1. , 1. ) , comment = None ) : N = len ( nodes ) if comment == None : out = "" else : out = "**EQUATION: {0}\n" . format ( comment ) out += "*EQUATION\n  {0}\n  " . format ( N ) out += "\n  " . join ( [ "," . join ( [ str ( nodes [ i ] ) , str ( int ( dofs [ i ] ) ) , str ( coefficients [ i ] ) ] ) for i in range ( N ) ] ) return out
def _unsorted_set ( df , label , * * kwargs ) : out = "*NSET, NSET={0}, UNSORTED\n" . format ( label ) labels = df . index . values return out + argiope . utils . list_to_string ( labels , * * kwargs )
def write_inp ( self ) : template = self . get_template ( ) return template . substitute ( { "class" : self . __class__ . __name__ , "label" : self . label } ) . strip ( )
def write_inp ( self ) : template = self . get_template ( ) plastic_table = self . get_plastic_table ( ) return template . substitute ( { "class" : self . __class__ . __name__ , "label" : self . label , "young_modulus" : self . young_modulus , "poisson_ratio" : self . poisson_ratio , "plastic_table" : ( self . get_plastic_table ( ) [ [ "stress" , "plastic_strain" ] ] . to_csv ( header = False , index = False , sep = "," ) . strip ( ) ) } ) . strip ( )
def get_plastic_table ( self ) : E = self . young_modulus sy = self . yield_stress n = self . hardening_exponent eps_max = self . max_strain Np = self . strain_data_points ey = sy / E s = 10. ** np . linspace ( 0. , np . log10 ( eps_max / ey ) , Np ) strain = ey * s stress = sy * s ** n plastic_strain = strain - stress / E return pd . DataFrame ( { "strain" : strain , "stress" : stress , "plastic_strain" : plastic_strain } )
def get_plastic_table ( self ) : K = self . consistency sy = self . yield_stress n = self . hardening_exponent eps_max = self . max_strain Np = self . strain_data_points plastic_strain = np . linspace ( 0. , eps_max , Np ) stress = sy + K * plastic_strain ** n return pd . DataFrame ( { "stress" : stress , "plastic_strain" : plastic_strain } )
def write_xy_report ( odb , path , tags , columns , steps ) : xyData = [ session . XYDataFromHistory ( name = columns [ i ] , odb = odb , outputVariableName = tags [ i ] , steps = steps ) for i in xrange ( len ( tags ) ) ] session . xyReportOptions . setValues ( numDigits = 8 , numberFormat = SCIENTIFIC ) session . writeXYReport ( fileName = path , appendMode = OFF , xyData = xyData )
def write_field_report ( odb , path , label , argiope_class , variable , instance , output_position , step = - 1 , frame = - 1 , sortItem = 'Node Label' ) : stepKeys = get_steps ( odb ) step = xrange ( len ( stepKeys ) ) [ step ] frame = xrange ( get_frames ( odb , stepKeys [ step ] ) ) [ frame ] nf = NumberFormat ( numDigits = 9 , precision = 0 , format = SCIENTIFIC ) session . fieldReportOptions . setValues ( printTotal = OFF , printMinMax = OFF , numberFormat = nf ) leaf = dgo . LeafFromPartInstance ( partInstanceName = instance ) session . viewports [ 'Viewport: 1' ] . odbDisplay . displayGroup . replace ( leaf = leaf ) session . writeFieldReport ( fileName = path , append = OFF , sortItem = sortItem , odb = odb , step = step , frame = frame , outputPosition = output_position , variable = variable ) lines = [ line . strip ( ) for line in open ( path ) . readlines ( ) ] isdata = - 1 data = [ ] for line in lines : if isdata == 1 : if len ( line ) == 0 : isdata -= 1 else : data . append ( line ) elif isdata < 1 : if line . startswith ( "--" ) : isdata += 1 data = "\n" . join ( [ "," . join ( line . split ( ) ) for line in data if len ( line ) != 0 ] ) header = str ( output_position ) . lower ( ) + "," header += "," . join ( [ v [ 1 ] for v in variable [ 0 ] [ 2 ] ] ) + "\n" metadata = ( ( "label" , label ) , ( "argiope_class" , argiope_class ) , ( "odb" , odb . path ) , ( "instance" , instance ) , ( "position" , output_position ) , ( "step_num" , step ) , ( "step_label" , stepKeys [ step ] ) , ( "frame" , frame ) , ( "frame_value" , odb . steps [ stepKeys [ step ] ] . frames [ frame ] . frameValue ) ) out = "*METADATA\n{0}\n*DATA\n{1}" . format ( "\n" . join ( [ "{0}={1}" . format ( k , v ) for k , v in metadata ] ) , header + data ) open ( path , "w" ) . write ( out )
def list ( component_type ) : config_loader = initialise_component_loader ( ) component_types = sorted ( { "displays" : lambda : config_loader . load_by_type ( ComponentType . DISPLAY ) , "datafeeds" : lambda : config_loader . load_by_type ( ComponentType . DATA_FEED ) , "filters" : lambda : config_loader . load_by_type ( ComponentType . FILTER ) , "notifications" : lambda : config_loader . load_by_type ( ComponentType . NOTIFICATION ) } . items ( ) , key = lambda t : t [ 0 ] ) def print_ids ( creators ) : ids = { c . id_key_value [ 1 ] if hasattr ( c , "id_key_value" ) else c . get_id ( ) for c in creators } for i in sorted ( ids ) : click . echo ( " - %s" % i ) for k , v in component_types : if component_type == k or component_type == "all" : click . echo ( "Available %s:" % k ) print_ids ( v ( ) ) if component_type == "all" : click . echo ( "" )
def _set_data ( self ) : if getattr ( self , 'data' , False ) and not getattr ( self , '_x' , False ) and not getattr ( self , '_y' , False ) : _x = XVariable ( ) _y = YVariable ( ) _x . contribute_to_class ( self , 'X' , self . data ) _y . contribute_to_class ( self , 'Y' , self . data ) self [ 'data' ] = zip ( self . _x . points , self . _y . points ) else : for axis in ( '_x' , '_y' ) : axis_obj = getattr ( self , axis , False ) if not axis_obj : raise exception . MissingAxisException ( "%s missing" % axis ) if not getattr ( axis_obj , 'points' , False ) : raise exception . MissingDataException ( ) self [ 'data' ] = zip ( self . _x . points , self . _y . points )
def _get_axis_mode ( self , axis ) : if all ( [ isinstance ( getattr ( s , axis ) , TimeVariable ) for s in self . _series ] ) : return 'time' return None
def _set_options ( self ) : if 'xaxis' in self . _options . keys ( ) : self . _options [ 'xaxis' ] . update ( { 'mode' : self . _get_axis_mode ( XAxis . _var_name ) } ) if 'yaxis' in self . _options . keys ( ) : self . _options [ 'yaxis' ] . update ( { 'mode' : self . _get_axis_mode ( YAxis . _var_name ) } )
def create_setter ( func , attrs ) : def _set ( self , instance , value , name = None ) : args = [ getattr ( self , attr ) for attr in attrs ] if not func ( value , * args ) : raise ValueError ( self . err_msg ( instance , value ) ) return _set
def make_class ( clsname , func , attrs ) : clsdict = { "__set__" : create_setter ( func , attrs ) } if len ( attrs ) > 0 : clsdict [ "__init__" ] = create_init ( attrs ) clsobj = type ( str ( clsname ) , ( Descriptor , ) , clsdict ) clsobj . __doc__ = docstrings . get ( clsname ) return clsobj
def cycle ( self ) : messages = self . poll_datafeeds ( ) notifications = self . process_notifications ( messages ) self . draw_notifications ( notifications )
def plot ( parser , token ) : tokens = token . split_contents ( ) tokens . pop ( 0 ) graph = tokens . pop ( 0 ) attrs = dict ( [ token . split ( "=" ) for token in tokens ] ) if 'id' not in attrs . keys ( ) : attrs [ 'id' ] = '' . join ( [ chr ( choice ( range ( 65 , 90 ) ) ) for i in range ( 0 , 5 ) ] ) else : attrs [ 'id' ] = attrs [ 'id' ] [ 1 : len ( attrs [ 'id' ] ) - 1 ] attr_string = '' . join ( [ " %s=%s" % ( k , v ) for k , v in attrs . iteritems ( ) ] ) return GraphRenderer ( graph , attr_string , attrs [ 'id' ] )
def _read_varint ( self ) : buf = self . _read ( 8 ) ( n , l ) = _DecodeVarint ( buf , 0 ) self . _unread ( buf [ l : ] ) return n
def working_directory ( path ) : prev_dir = os . getcwd ( ) os . chdir ( str ( path ) ) try : yield finally : os . chdir ( prev_dir )
def exit ( self ) : if self . _server is not None : self . _server . shutdown ( ) self . _server . server_close ( ) self . _server = None
def _get_error_page_callback ( self ) : if self . response . status in self . _error_handlers : return self . _error_handlers [ self . response . status ] elif None in self . _error_handlers : return self . _error_handlers [ None ] else : self . response . media_type = 'text/plain' return lambda : self . response . status_line
def attempt_fetch ( work_unit , fpath ) : url = 'http://s3.amazonaws.com/aws-publicdatasets/' + work_unit . key . strip ( ) cmd = '(wget -O - %s | gpg --no-permission-warning --trust-model always --output - --decrypt - | xz --decompress) 2> %s-err' % ( url , fpath ) print cmd child = Popen ( cmd , stdout = PIPE , shell = True ) print 'child launched' sys . stdout . flush ( ) si_count = 0 serif_count = 0 exc = '' stream_ids = list ( ) clean_visible_bytes = 0 clean_visible_count = 0 try : for si in Chunk ( file_obj = child . stdout ) : print si . stream_id , si . abs_url if si . body . language : lang = si . body . language . code else : lang = '' stream_ids . append ( ( lang , si . stream_id ) ) if si . body . clean_visible : clean_visible_count += 1 clean_visible_bytes += len ( si . body . clean_visible ) si_count += 1 if 'serif' in si . body . sentences : serif_count += 1 except Exception , exc : exc = re . sub ( '\s+' , ' ' , str ( exc ) ) . strip ( ) child . terminate ( ) child . wait ( ) child . stdout . close ( ) return exc , si_count , serif_count , clean_visible_bytes , clean_visible_count , stream_ids
def get_file_lines ( file_name ) : file_path = path . join ( path . dirname ( path . abspath ( __file__ ) ) , file_name ) with open ( file_path ) as file_obj : return [ line for line in file_obj . read ( ) . splitlines ( ) if line ]
def _random_adjspecies_pair ( ) : describer , desc_position = random_describer ( ) if desc_position == 'prefix' : return ( describer , random_species ( ) ) elif desc_position == 'suffix' : return ( random_species ( ) , describer )
def morph ( ctx , app_id , sentence_file , json_flag , sentence , info_filter , pos_filter , request_id ) : app_id = clean_app_id ( app_id ) sentence = clean_sentence ( sentence , sentence_file ) if info_filter : info_filter = info_filter . replace ( ',' , '|' ) if pos_filter : pos_filter = pos_filter . replace ( ',' , '|' ) api = GoolabsAPI ( app_id ) ret = api . morph ( sentence = sentence , info_filter = info_filter , pos_filter = pos_filter , request_id = request_id , ) if json_flag : click . echo ( format_json ( api . response . json ( ) ) ) return for words in ret [ 'word_list' ] : for word in words : click . echo ( ',' . join ( word ) )
def similarity ( ctx , app_id , json_flag , query_pair , request_id ) : app_id = clean_app_id ( app_id ) api = GoolabsAPI ( app_id ) ret = api . similarity ( query_pair = query_pair , request_id = request_id ) if json_flag : click . echo ( format_json ( api . response . json ( ) ) ) return click . echo ( '{0:.16f}' . format ( ret [ 'score' ] ) )
def hiragana ( ctx , app_id , sentence_file , json_flag , sentence , output_type , request_id ) : app_id = clean_app_id ( app_id ) sentence = clean_sentence ( sentence , sentence_file ) api = GoolabsAPI ( app_id ) ret = api . hiragana ( sentence = sentence , output_type = output_type , request_id = request_id ) if json_flag : click . echo ( format_json ( api . response . json ( ) ) ) return click . echo ( ret [ 'converted' ] )
def entity ( ctx , app_id , sentence_file , json_flag , sentence , class_filter , request_id ) : app_id = clean_app_id ( app_id ) sentence = clean_sentence ( sentence , sentence_file ) if class_filter : class_filter = class_filter . replace ( ',' , '|' ) api = GoolabsAPI ( app_id ) ret = api . entity ( sentence = sentence , class_filter = class_filter , request_id = request_id ) if json_flag : click . echo ( format_json ( api . response . json ( ) ) ) return for ne in ret [ 'ne_list' ] : click . echo ( ',' . join ( ne ) )
def shortsum ( ctx , app_id , review_file , json_flag , review , length , request_id ) : app_id = clean_app_id ( app_id ) review_list = clean_review ( review , review_file ) length_int = clean_length ( length ) api = GoolabsAPI ( app_id ) ret = api . shortsum ( review_list = review_list , length = length_int , request_id = request_id , ) if json_flag : click . echo ( format_json ( api . response . json ( ) ) ) return click . echo ( ret [ 'summary' ] )
def keyword ( ctx , app_id , body_file , json_flag , title , body , max_num , forcus , request_id ) : app_id = clean_app_id ( app_id ) body = clean_body ( body , body_file ) api = GoolabsAPI ( app_id ) ret = api . keyword ( title = title , body = body , max_num = max_num , forcus = forcus , request_id = request_id , ) if json_flag : click . echo ( format_json ( api . response . json ( ) ) ) return for k in ret [ 'keywords' ] : k = dict ( ( key . encode ( 'utf-8' ) , k [ key ] ) for key in k . keys ( ) ) for keyword , score in six . iteritems ( k ) : click . echo ( u'{0},{1}' . format ( text ( keyword ) , score ) )
def chrono ( ctx , app_id , sentence_file , json_flag , sentence , doc_time , request_id ) : app_id = clean_app_id ( app_id ) sentence = clean_sentence ( sentence , sentence_file ) api = GoolabsAPI ( app_id ) ret = api . chrono ( sentence = sentence , doc_time = doc_time , request_id = request_id , ) if json_flag : click . echo ( format_json ( api . response . json ( ) ) ) return for pair in ret [ 'datetime_list' ] : click . echo ( u'{0}: {1}' . format ( text ( pair [ 0 ] ) , pair [ 1 ] ) )
def make_app ( ) : env = Environment ( ) args = parser . parse_args ( args = [ '/' , '--ignore-stdin' ] , env = env ) args . output_options = 'HB' server = 'HTTPony/{0}' . format ( __version__ ) def application ( environ , start_response ) : if environ . get ( 'CONTENT_LENGTH' ) == '' : del environ [ 'CONTENT_LENGTH' ] if environ . get ( 'CONTENT_TYPE' ) == '' : del environ [ 'CONTENT_TYPE' ] wrequest = WerkzeugRequest ( environ ) data = wrequest . get_data ( ) request = Request ( method = wrequest . method , url = wrequest . url , headers = wrequest . headers , data = data , ) prepared = request . prepare ( ) stream = streams . build_output_stream ( args , env , prepared , response = None , output_options = args . output_options ) streams . write_stream ( stream , env . stdout , env . stdout_isatty ) if data : print ( "\n" , file = env . stdout ) response = Response ( headers = { 'Server' : server } ) return response ( environ , start_response ) return application
def make_ner_file ( self , clean_visible_path , ner_xml_path ) : if self . template is None : raise exceptions . NotImplementedError ( ) tagger_config = dict ( tagger_root_path = self . config [ 'tagger_root_path' ] , clean_visible_path = clean_visible_path , ner_xml_path = ner_xml_path ) tagger_config [ 'java_heap_size' ] = self . config . get ( 'java_heap_size' , '' ) cmd = self . template % tagger_config start_time = time . time ( ) gc . collect ( ) try : self . _child = subprocess . Popen ( cmd , stderr = subprocess . PIPE , shell = True ) except OSError , exc : msg = traceback . format_exc ( exc ) msg += make_memory_info_msg ( clean_visible_path , ner_xml_path ) raise PipelineOutOfMemory ( msg ) s_out , errors = self . _child . communicate ( ) if not self . _child . returncode == 0 : if 'java.lang.OutOfMemoryError' in errors : msg = errors + make_memory_info_msg ( clean_visible_path , ner_xml_path ) raise PipelineOutOfMemory ( msg ) elif self . _child . returncode == 137 : msg = 'tagger returncode = 137\n' + errors msg += make_memory_info_msg ( clean_visible_path , ner_xml_path ) raise PipelineOutOfMemory ( msg ) elif 'Exception' in errors : raise PipelineBaseException ( errors ) else : raise PipelineBaseException ( 'tagger exited with %r' % self . _child . returncode ) elapsed = time . time ( ) - start_time logger . info ( 'finished tagging in %.1f seconds' % elapsed ) return elapsed
def align_chunk_with_ner ( self , ner_xml_path , i_chunk , o_chunk ) : input_iter = i_chunk . __iter__ ( ) all_ner = xml . dom . minidom . parse ( open ( ner_xml_path ) ) for ner_dom in all_ner . getElementsByTagName ( 'FILENAME' ) : #for stream_id, raw_ner in files(open(ner_xml_path).read().decode('utf8')): stream_item = input_iter . next ( ) stream_id = ner_dom . attributes . get ( 'stream_id' ) . value if stream_item . stream_id is None : assert not stream_id , 'out of sync: None != %r' % stream_id logger . critical ( 'si.stream_id is None... ignoring' ) continue assert stream_id and stream_id == stream_item . stream_id , '%s != %s' % ( stream_id , stream_item . stream_id ) if not stream_item . body : #assert not ner_dom....something continue tagging = Tagging ( ) tagging . tagger_id = self . tagger_id #tagging.raw_tagging = tagged_doc tagging . generation_time = streamcorpus . make_stream_time ( ) stream_item . body . taggings [ self . tagger_id ] = tagging sentences , relations , attributes = self . get_sentences ( ner_dom ) stream_item . body . sentences [ self . tagger_id ] = sentences stream_item . body . relations [ self . tagger_id ] = relations stream_item . body . attributes [ self . tagger_id ] = attributes logger . debug ( 'finished aligning tokens %s' % stream_item . stream_id ) if 'align_labels_by' in self . config and self . config [ 'align_labels_by' ] : assert 'aligner_data' in self . config , 'config missing "aligner_data"' aligner = AlignmentStrategies [ self . config [ 'align_labels_by' ] ] aligner ( stream_item , self . config [ 'aligner_data' ] ) gc . collect ( ) try : o_chunk . add ( stream_item ) except MemoryError , exc : msg = traceback . format_exc ( exc ) msg += make_memory_info_msg ( ) logger . critical ( msg ) raise PipelineOutOfMemory ( msg ) try : o_chunk . close ( ) logger . info ( 'finished chunk for %r' % ner_xml_path ) except MemoryError , exc : msg = traceback . format_exc ( exc ) msg += make_memory_info_msg ( ) logger . critical ( msg ) raise PipelineOutOfMemory ( msg )
def shutdown ( self ) : if self . _child : try : self . _child . terminate ( ) except OSError , exc : if exc . errno == 3 : pass
def mult ( p , n ) : np = P ( ) while n >= 1 : if n % 2 : np = np + p p = p + p n = n // 2 return np
def fix_emails ( text ) : emails = bracket_emails . findall ( text ) keys = [ ] for email in emails : _email = email . replace ( "<" , "&lt;" ) . replace ( ">" , "&gt;" ) text = text . replace ( email , _email ) return text
def _sentences ( self , clean_visible ) : previous_end = 0 clean_visible = clean_visible . decode ( 'utf8' ) for start , end in self . sentence_tokenizer . span_tokenize ( clean_visible ) : if start < previous_end : start = previous_end if start > end : continue try : label = self . label_index . find_le ( end ) except ValueError : label = None if label : off = label . offsets [ OffsetType . CHARS ] end = max ( off . first + off . length , end ) previous_end = end sent_str = clean_visible [ start : end ] yield start , end , sent_str
def make_label_index ( self , stream_item ) : labels = stream_item . body . labels . get ( self . annotator_id ) if not labels : labels = [ ] self . label_index = SortedCollection ( [ l for l in labels if OffsetType . CHARS in l . offsets ] , key = lambda label : label . offsets [ OffsetType . CHARS ] . first )
def make_sentences ( self , stream_item ) : self . make_label_index ( stream_item ) sentences = [ ] token_num = 0 new_mention_id = 0 for sent_start , sent_end , sent_str in self . _sentences ( stream_item . body . clean_visible ) : assert isinstance ( sent_str , unicode ) sent = Sentence ( ) sentence_pos = 0 for start , end in self . word_tokenizer . span_tokenize ( sent_str ) : token_str = sent_str [ start : end ] . encode ( 'utf8' ) tok = Token ( token_num = token_num , token = token_str , sentence_pos = sentence_pos , ) tok . offsets [ OffsetType . CHARS ] = Offset ( type = OffsetType . CHARS , first = sent_start + start , length = end - start , ) try : label = self . label_index . find_le ( sent_start + start ) except ValueError : label = None if label : off = label . offsets [ OffsetType . CHARS ] if off . first + off . length > sent_start + start : streamcorpus . add_annotation ( tok , label ) logger . debug ( 'adding label to tok: %r has %r' , tok . token , label . target . target_id ) if label in self . label_to_mention_id : mention_id = self . label_to_mention_id [ label ] else : mention_id = new_mention_id new_mention_id += 1 self . label_to_mention_id [ label ] = mention_id tok . mention_id = mention_id token_num += 1 sentence_pos += 1 sent . tokens . append ( tok ) sentences . append ( sent ) return sentences
def make_cleansed_file ( i_chunk , tmp_cleansed_path ) : tmp_cleansed = open ( tmp_cleansed_path , 'wb' ) for idx , si in enumerate ( i_chunk ) : tmp_cleansed . write ( '<FILENAME docid="%s">\n' % si . stream_id ) tmp_cleansed . write ( si . body . cleansed ) tmp_cleansed . write ( '</FILENAME>\n' ) tmp_cleansed . close ( ) print 'created %s' % tmp_cleansed_path
def make_ner_file ( tagger_id , tmp_cleansed_path , tmp_ner_path , pipeline_root ) : params = dict ( INPUT_FILE = tmp_cleansed_path , #RAW_OUTPUT_FILE=tmp_ner_raw_path, OUTPUT_FILE = tmp_ner_path , PIPELINE_ROOT = pipeline_root ) pipeline_cmd = pipeline_cmd_templates [ tagger_id ] % params print pipeline_cmd print 'creating %s' % tmp_ner_path start_time = time . time ( ) gpg_child = subprocess . Popen ( pipeline_cmd , stderr = subprocess . PIPE , shell = True ) s_out , errors = gpg_child . communicate ( ) assert gpg_child . returncode == 0 and 'Exception' not in errors , errors elapsed = time . time ( ) - start_time print 'created %s in %.1f sec' % ( tmp_ner_path , elapsed )
def make_clean_visible_file ( i_chunk , clean_visible_path ) : _clean = open ( clean_visible_path , 'wb' ) _clean . write ( '<?xml version="1.0" encoding="UTF-8"?>' ) _clean . write ( '<root>' ) for idx , si in enumerate ( i_chunk ) : if si . stream_id is None : stream_id = '' else : stream_id = si . stream_id doc = lxml . etree . Element ( "FILENAME" , stream_id = stream_id ) if si . body and si . body . clean_visible : try : doc . text = si . body . clean_visible . decode ( 'utf8' ) except ValueError : doc . text = drop_invalid_and_upper_utf8_chars ( si . body . clean_visible . decode ( 'utf8' ) ) except Exception , exc : logger . critical ( traceback . format_exc ( exc ) ) logger . critical ( 'failed on stream_id=%s to follow:' , si . stream_id ) logger . critical ( repr ( si . body . clean_visible ) ) logger . critical ( 'above was stream_id=%s' , si . stream_id ) raise else : doc . text = '' _clean . write ( lxml . etree . tostring ( doc , encoding = 'UTF-8' ) ) _clean . write ( '</root>' ) _clean . close ( ) logger . info ( clean_visible_path )
def main ( ) : import argparse import sys parser = argparse . ArgumentParser ( ) parser . add_argument ( 'path' ) args = parser . parse_args ( ) html = open ( args . path ) . read ( ) html = html . decode ( 'utf8' ) cursor = 0 for s in non_tag_chars_from_raw ( html ) : for c in s : if c != ' ' and c != html [ cursor ] : import pdb pdb . set_trace ( ) sys . stdout . write ( c . encode ( 'utf8' ) ) sys . stdout . flush ( ) cursor += 1
def paths ( input_dir ) : for root , dirs , fnames in os . walk ( input_dir ) : for i_fname in fnames : i_path = os . path . join ( root , i_fname ) yield i_path
def tasks ( self , key_prefix = '' ) : for row in self . _tasks . get_range ( ) : logger . debug ( row ) if not row [ 0 ] . startswith ( key_prefix ) : continue data = json . loads ( row [ 1 ] [ 'task_data' ] ) data [ 'task_key' ] = row [ 0 ] yield data
def get_random_available ( self , max_iter = 10000 ) : c = 1 keeper = None #random_key = hashlib.md5(str(random.random())).hexdigest() #random_key = '0' * 32 #logger.debug('available.get_range(%r)' % random_key) # for row in self . _available . get_range ( row_count = max_iter , read_consistency_level = pycassa . ConsistencyLevel . ALL ) : #for row in self._available.get_range(row_count=100): logger . debug ( 'considering %r' % ( row , ) ) if random . random ( ) < 1 / c : keeper = row [ 0 ] if c == max_iter : break c += 1 return keeper
def tokens ( self , sentence_dom ) : self . sent_pos = 0 mention_id = 0 while len ( sentence_dom . childNodes ) > 0 : node = sentence_dom . childNodes . pop ( 0 ) if node . nodeType == node . TEXT_NODE : for line in node . data . splitlines ( True ) : self . _input_string = line for start , end in self . word_tokenizer . span_tokenize ( line ) : tok = self . _make_token ( start , end ) if tok : yield tok if line . endswith ( '\n' ) : self . line_idx += 1 self . byte_idx += len ( line . encode ( 'utf-8' ) ) else : assert node . nodeName == 'ENAMEX' , node . nodeName chain_id = node . attributes . get ( 'ID' ) . value entity_type = node . attributes . get ( 'TYPE' ) . value for node in node . childNodes : assert node . nodeType == node . TEXT_NODE , node . nodeType for line in node . data . splitlines ( True ) : self . _input_string = line for start , end in self . word_tokenizer . span_tokenize ( line ) : tok = self . _make_token ( start , end ) if tok : if entity_type in _PRONOUNS : tok . mention_type = MentionType . PRO tok . entity_type = _ENTITY_TYPES [ entity_type ] attr = Attribute ( attribute_type = AttributeType . PER_GENDER , value = str ( _PRONOUNS [ entity_type ] ) ) self . attributes . append ( attr ) else : tok . mention_type = MentionType . NAME tok . entity_type = _ENTITY_TYPES [ entity_type ] tok . equiv_id = int ( chain_id ) tok . mention_id = mention_id yield tok if line . endswith ( '\n' ) : self . line_idx += 1 self . byte_idx += len ( line . encode ( 'utf-8' ) ) mention_id += 1
def get_sentences ( self , ner_dom ) : lp_parser = LingPipeParser ( self . config ) lp_parser . set ( ner_dom ) sentences = list ( lp_parser . sentences ( ) ) return sentences , lp_parser . relations , lp_parser . attributes
def verify_md5 ( md5_expected , data , other_errors = None ) : md5_recv = hashlib . md5 ( data ) . hexdigest ( ) if md5_expected != md5_recv : if other_errors is not None : logger . critical ( '\n' . join ( other_errors ) ) raise FailedVerification ( 'original md5 = %r != %r = received md5' % ( md5_expected , md5_recv ) ) return True
def main ( argv = sys . argv ) : args = parse ( argv ) hostname = args . listen port = args . port print ( "Making all your dreams for a pony come true on http://{0}:{1}.\n" "Press Ctrl+C to quit.\n" . format ( hostname , port ) ) logging . getLogger ( 'werkzeug' ) . setLevel ( logging . CRITICAL ) plugin_manager . load_installed_plugins ( ) app = make_app ( ) run_simple ( hostname , port , app )
def build_parser ( ) : description = ( 'HTTPony (pronounced aych-tee-tee-pony) is a simple HTTP ' 'server that pretty prints HTTP requests to a terminal. It ' 'is a useful aide for developing clients that send HTTP ' 'requests. HTTPony acts as a sink for a client so that a ' 'developer can understand what the client is sending.' ) parser = argparse . ArgumentParser ( description = description ) parser . add_argument ( '-l' , '--listen' , help = 'set the IP address or hostname' , default = 'localhost' ) parser . add_argument ( '-p' , '--port' , help = 'set the port' , default = 8000 , type = int ) return parser
def sentences_to_char_tokens ( si_sentences ) : for sentence in si_sentences : for token in sentence . tokens : if OffsetType . CHARS in token . offsets : yield token
def char_tokens_to_char_offsets ( si_tokens ) : for token in si_tokens : offset = token . offsets [ OffsetType . CHARS ] yield offset . first , offset . first + offset . length
def text_index ( self ) : i = self . tags . get ( TextElement , 0 ) if self . last_tag is not TextElement : i += 1 return i
def descendants ( elem ) : for child in elem . xml_children : if isinstance ( child , element ) : yield child yield from descendants ( child )
def following_siblings ( elem ) : it = itertools . dropwhile ( lambda x : x != elem , elem . xml_parent . xml_children ) next ( it ) #Skip the element itself return it
def svg2pdf ( svg_file_path , pdf_file_path , dpi = 150 , command_binpath = None , support_unicode = False ) : if support_unicode : return rsvg_export ( svg_file_path , pdf_file_path , dpi = dpi , rsvg_binpath = command_binpath ) return inkscape_export ( svg_file_path , pdf_file_path , export_flag = "-A" , dpi = dpi , inkscape_binpath = command_binpath )
def svg2png ( svg_file_path , png_file_path , dpi = 150 , inkscape_binpath = None ) : return inkscape_export ( svg_file_path , png_file_path , export_flag = "-e" , dpi = dpi , inkscape_binpath = inkscape_binpath )
def strval ( node , outermost = True ) : if not isinstance ( node , element ) : return node . xml_value if outermost else [ node . xml_value ] accumulator = [ ] for child in node . xml_children : if isinstance ( child , text ) : accumulator . append ( child . xml_value ) elif isinstance ( child , element ) : accumulator . extend ( strval ( child , outermost = False ) ) if outermost : accumulator = '' . join ( accumulator ) return accumulator
def parse_options ( ) : version = "%%prog {version}" . format ( version = __version__ ) parser = OptionParser ( version = version ) parser . add_option ( "-u" , "--username" , action = "store" , dest = "username" , type = "string" , default = "" , metavar = "RECIPIENT" , help = "user" ) parser . add_option ( "-C" , "--calendar" , metavar = "CALENDAR" , action = "store" , type = "string" , dest = "calendar" , default = "" , help = "google calendar ID" ) parser . add_option ( "-t" , "--timezone" , metavar = "TIMEZONE" , action = "store" , type = "string" , dest = "timezone" , default = "" , help = "user timezone" ) parser . add_option ( "-m" , "--message" , metavar = "MESSAGE" , action = "store" , type = "string" , dest = "message" , default = "" , help = "message text" ) parser . add_option ( "-c" , "--config" , metavar = "CONFIG" , action = "store" , type = "string" , dest = "config" , help = "path to config file" , default = "/etc/nagios/notification_google_calendar.ini" ) parser . add_option ( "-q" , "--quiet" , metavar = "QUIET" , action = "store_true" , default = False , dest = "quiet" , help = "be quiet" ) parser . add_option ( "-g" , "--get-google-credentials" , metavar = "GET-GOOGLE-CREDENTIALS" , action = "store_true" , default = False , dest = "get_google_credentials" , help = "get google API credentials for user" ) options = parser . parse_args ( sys . argv ) [ 0 ] mandatories = [ "username" , ] if not options . get_google_credentials : mandatories . append ( "calendar" ) mandatories . append ( "message" ) mandatories . append ( "timezone" ) if not all ( options . __dict__ [ mandatory ] for mandatory in mandatories ) : parser . error ( "Required command line option missing\n" ) return options
def parse_config ( options ) : if os . path . exists ( options . config ) : config = ConfigParser . ConfigParser ( ) try : config . read ( options . config ) except Exception , err : if not options . quiet : sys . stderr . write ( "ERROR: Config file read {config} error. {err}" . format ( config = options . config , err = err ) ) sys . exit ( - 1 ) try : configdata = { "secrets" : config . get ( "GOOGLE" , "secrets" ) , "credentials" : config . get ( "nagios-notification-google-calendar" , "credentials" ) , "start" : config . get ( "nagios-notification-google-calendar" , "start" ) , "end" : config . get ( "nagios-notification-google-calendar" , "end" ) , "message" : config . get ( "nagios-notification-google-calendar" , "message" ) , } except ConfigParser . NoOptionError , err : if not options . quiet : sys . stderr . write ( "ERROR: Config file missing option error. {err}\n" . format ( err = err ) ) sys . exit ( - 1 ) mandatories = [ "secrets" , "credentials" , "start" , "end" , "message" , ] if not all ( configdata [ mandatory ] for mandatory in mandatories ) : if not options . quiet : sys . stdout . write ( "Mandatory config option missing\n" ) sys . exit ( 0 ) return configdata else : if not options . quiet : sys . stderr . write ( "ERROR: Config file {config} does not exist\n" . format ( config = options . config ) ) sys . exit ( 0 )
def get_google_credentials ( options , config ) : try : if options . get_google_credentials : flow = flow_from_clientsecrets ( config [ "secrets" ] , scope = SCOPE , redirect_uri = "oob" ) sys . stdout . write ( "Follow this URL: {url} and grant access to calendar.\n" . format ( url = flow . step1_get_authorize_url ( ) ) ) token = raw_input ( "Enter token:" ) credentials = flow . step2_exchange ( token ) storage = Storage ( os . path . join ( config [ "credentials" ] , "{username}.json" . format ( username = options . username ) ) ) storage . put ( credentials ) credentials . set_store ( storage ) else : storage = Storage ( os . path . join ( config [ "credentials" ] , "{username}.json" . format ( username = options . username ) ) ) credentials = storage . get ( ) except Exception , err : if not options . quiet : sys . stderr . write ( "ERROR: Getting google API credentials error. {err}\n" . format ( err = err ) ) sys . exit ( - 1 ) return credentials
def create_event_datetimes ( options , config ) : now = datetime . datetime . now ( ) return { "start" : { "dateTime" : ( now + datetime . timedelta ( minutes = int ( config [ "start" ] ) ) ) . strftime ( DT_FORMAT ) , "timeZone" : options . timezone , } , "end" : { "dateTime" : ( now + datetime . timedelta ( minutes = int ( config [ "end" ] ) ) ) . strftime ( DT_FORMAT ) , "timeZone" : options . timezone , } , }
def create_event ( options , config , credentials ) : try : http = credentials . authorize ( httplib2 . Http ( ) ) service = build ( "calendar" , "v3" , http = http ) event = { "summary" : options . message , "location" : "" , "reminders" : { "useDefault" : False , "overrides" : [ { "method" : "sms" , "minutes" : config [ "message" ] , } , ] , } } event . update ( create_event_datetimes ( options , config ) ) service . events ( ) . insert ( calendarId = options . calendar , sendNotifications = True , body = event ) . execute ( ) except Exception , err : if not options . quiet : sys . stderr . write ( "ERROR: Creating google calendar event error. {err}\n" . format ( err = err ) ) sys . exit ( - 1 )
def main ( ) : options = parse_options ( ) config = parse_config ( options ) credentials = get_google_credentials ( options , config ) if not options . get_google_credentials : create_event ( options , config , credentials )
def parse ( self ) : for tag in self . soup . findAll ( 'span' ) : self . create_italic ( tag ) self . create_strong ( tag ) self . create_underline ( tag ) self . unwrap_span ( tag ) for tag in self . soup . findAll ( 'a' ) : self . remove_comments ( tag ) self . check_next ( tag ) if self . soup . body : for tag in self . soup . body . findAll ( ) : self . remove_empty ( tag ) self . remove_inline_comment ( tag ) self . parse_attrs ( tag ) for token , target in self . tokens : self . find_token ( tag , token , target ) self . remove_blacklisted_tags ( tag )
def check_next ( self , tag ) : if ( type ( tag . next_sibling ) == element . Tag and tag . next_sibling . name == 'a' ) : next_tag = tag . next_sibling if tag . get ( 'href' ) and next_tag . get ( 'href' ) : href = self . _parse_href ( tag . get ( 'href' ) ) next_href = self . _parse_href ( next_tag . get ( 'href' ) ) if href == next_href : next_text = next_tag . get_text ( ) tag . append ( next_text ) self . tags_blacklist . append ( next_tag )
def create_italic ( self , tag ) : style = tag . get ( 'style' ) if style and 'font-style:italic' in style : tag . wrap ( self . soup . new_tag ( 'em' ) )
def create_strong ( self , tag ) : style = tag . get ( 'style' ) if ( style and ( 'font-weight:bold' in style or 'font-weight:700' in style ) ) : tag . wrap ( self . soup . new_tag ( 'strong' ) )
def create_underline ( self , tag ) : style = tag . get ( 'style' ) if style and 'text-decoration:underline' in style : tag . wrap ( self . soup . new_tag ( 'u' ) )
def parse_attrs ( self , tag ) : if tag . name in ATTR_WHITELIST . keys ( ) : attrs = copy ( tag . attrs ) for attr , value in attrs . items ( ) : if attr in ATTR_WHITELIST [ tag . name ] : tag . attrs [ attr ] = self . _parse_attr ( tag . name , attr , value ) else : del tag . attrs [ attr ] else : tag . attrs = { }
def remove_empty ( self , tag ) : has_children = len ( tag . contents ) has_text = len ( list ( tag . stripped_strings ) ) if not has_children and not has_text and not tag . is_empty_element : tag . extract ( )
def boolean_arg ( ctx , obj ) : if hasattr ( obj , 'compute' ) : obj = next ( obj . compute ( ctx ) , False ) return to_boolean ( obj )
def number_arg ( ctx , obj ) : if hasattr ( obj , 'compute' ) : obj = next ( obj . compute ( ctx ) , False ) return to_number ( obj )
def string_arg ( ctx , obj ) : if hasattr ( obj , 'compute' ) : obj = next ( obj . compute ( ctx ) , False ) return to_string ( obj )
def concat ( ctx , * strings ) : strings = flatten ( [ ( s . compute ( ctx ) if callable ( s ) else s ) for s in strings ] ) strings = ( next ( string_arg ( ctx , s ) , '' ) for s in strings ) #assert(all(map(lambda x: isinstance(x, str), strings))) #FIXME: Check arg types yield '' . join ( strings )
def starts_with ( ctx , full , part ) : full = next ( string_arg ( ctx , full ) , '' ) part = next ( string_arg ( ctx , part ) , '' ) yield full . startswith ( part )
def contains ( ctx , full , part ) : full = next ( string_arg ( ctx , full ) , '' ) part = next ( string_arg ( ctx , part ) , '' ) yield part in full
def _check_inputs ( self ) : try : _ = self . _inputs [ 0 ] except TypeError : raise RuntimeError ( "inputs should be iterable but found type='{0}', value=" "'{1}'" . format ( type ( self . _inputs ) , str ( self . _inputs ) ) ) from melody . inputs import Input for check_input in self . _inputs : if not isinstance ( check_input , Input ) : raise RuntimeError ( "input should be a subclass of the Input class but " "found type='{0}', value='{1}'" . format ( type ( check_input ) , str ( check_input ) ) )
def _check_function ( self ) : if not callable ( self . _function ) : raise RuntimeError ( "provided function '{0}' is not callable" . format ( str ( self . _function ) ) ) from inspect import getargspec arg_info = getargspec ( self . _function ) if len ( arg_info . args ) != 1 : print str ( arg_info ) raise RuntimeError ( "provided function should have one argument but found " "{0}" . format ( len ( arg_info . args ) ) )
def _recurse ( self , inputs , output , depth , max_depth ) : if depth < max_depth : for index , option in enumerate ( inputs ) : my_output = list ( output ) my_output . append ( option ) self . _recurse ( inputs [ index + 1 : ] , my_output , depth + 1 , max_depth ) else : self . _options . append ( output )
def to_string ( obj ) : if isinstance ( obj , LiteralWrapper ) : val = obj . obj elif isinstance ( obj , Iterable ) and not isinstance ( obj , str ) : val = next ( obj , None ) else : val = obj if val is None : yield '' elif isinstance ( val , str ) : yield val elif isinstance ( val , node ) : yield strval ( val ) elif isinstance ( val , int ) or isinstance ( val , float ) : yield str ( val ) elif isinstance ( item , bool ) : yield 'true' if item else 'false' else : raise RuntimeError ( 'Unknown type for string conversion: {}' . format ( val ) )
def to_number ( obj ) : if isinstance ( obj , LiteralWrapper ) : val = obj . obj elif isinstance ( obj , Iterable ) and not isinstance ( obj , str ) : val = next ( obj , None ) else : val = obj if val is None : #FIXME: Should be NaN, not 0 yield 0 elif isinstance ( val , str ) : yield float ( val ) elif isinstance ( val , node ) : yield float ( strval ( val ) ) elif isinstance ( val , int ) or isinstance ( val , float ) : yield val else : raise RuntimeError ( 'Unknown type for number conversion: {}' . format ( val ) )
def to_boolean ( obj ) : #if hasattr(obj, '__iter__'): if isinstance ( obj , LiteralWrapper ) : val = obj . obj elif isinstance ( obj , Iterable ) and not isinstance ( obj , str ) : val = next ( obj , None ) else : val = obj if val is None : yield False elif isinstance ( val , bool ) : yield val elif isinstance ( val , str ) : yield bool ( str ) elif isinstance ( val , node ) : yield True elif isinstance ( val , float ) or isinstance ( val , int ) : yield bool ( val ) else : raise RuntimeError ( 'Unknown type for boolean conversion: {}' . format ( val ) )
def intersect ( self , other ) : inter = Envelope ( tuple ( self ) ) if inter . intersects ( other ) : mid = len ( other ) // 2 inter . ll = map ( max , inter . ll , other [ : mid ] ) inter . ur = map ( min , inter . ur , other [ mid : ] ) else : inter . ll = ( 0 , 0 ) inter . ur = ( 0 , 0 ) return inter
def polygon ( self ) : ring = ogr . Geometry ( ogr . wkbLinearRing ) for coord in self . ll , self . lr , self . ur , self . ul , self . ll : ring . AddPoint_2D ( * coord ) polyg = ogr . Geometry ( ogr . wkbPolygon ) polyg . AddGeometryDirectly ( ring ) return polyg
def from_name ( cls , name ) : filename = os . path . join ( package_dir , 'data' , name + '.txt' ) return cls . from_file ( filename , name )
def from_file ( cls , filename , name = '' ) : df = pd . read_csv ( filename , header = 0 , delim_whitespace = True , index_col = [ 0 , 1 ] ) [ 'M' ] df . name = name return cls ( df = df , name = name )
def odd_even ( self ) : return self . select ( lambda Z , N : ( Z % 2 ) and not ( N % 2 ) , name = self . name )
def even_odd ( self ) : return self . select ( lambda Z , N : not ( Z % 2 ) and ( N % 2 ) , name = self . name )
def even_even ( self ) : return self . select ( lambda Z , N : not ( Z % 2 ) and not ( N % 2 ) , name = self . name )
def binding_energy ( self ) : M_P = 938.2723 M_E = 0.5110 M_N = 939.5656 AMU = 931.494028 df = self . Z * ( M_P + M_E ) + ( self . A - self . Z ) * M_N - ( self . df + self . A * AMU ) return Table ( df = df , name = 'BE' + '(' + self . name + ')' )
def s2n ( self ) : M_N = 8.0713171 f = lambda parent , daugther : - parent + daugther + 2 * M_N return self . derived ( 's2n' , ( 0 , - 2 ) , f )
def s1n ( self ) : M_N = 8.0713171 f = lambda parent , daugther : - parent + daugther + M_N return self . derived ( 's1n' , ( 0 , - 1 ) , f )
def s2p ( self ) : M_P = 7.28897050 f = lambda parent , daugther : - parent + daugther + 2 * M_P return self . derived ( 's2p' , ( - 2 , 0 ) , f )
def s1p ( self ) : M_P = 7.28897050 f = lambda parent , daugther : - parent + daugther + M_P return self . derived ( 's1p' , ( - 1 , 0 ) , f )
def derived ( self , name , relative_coords , formula ) : relZ , relN = relative_coords daughter_idx = [ ( x [ 0 ] + relZ , x [ 1 ] + relN ) for x in self . df . index ] values = formula ( self . df . values , self . df . loc [ daughter_idx ] . values ) return Table ( df = pd . Series ( values , index = self . df . index , name = name + '(' + self . name + ')' ) )
def derive_key ( self , master_password ) : encoder = encoding . Encoder ( self . charset ) bytes = ( '%s:%s' % ( master_password , self . name ) ) . encode ( 'utf8' ) start_time = time . clock ( ) digest = scrypt . hash ( bytes , self . salt , N = 1 << 14 , r = 8 , p = 1 ) key = encoder . encode ( digest , self . key_length ) derivation_time_in_s = time . clock ( ) - start_time _logger . debug ( 'Key derivation took %.2fms' , derivation_time_in_s * 1000 ) return key
def search ( self , query ) : results = self . session . query ( Domain ) . filter ( Domain . name . ilike ( '%%%s%%' % query ) ) . all ( ) return results
def srid ( self ) : epsg_id = ( self . GetAuthorityCode ( 'PROJCS' ) or self . GetAuthorityCode ( 'GEOGCS' ) ) try : return int ( epsg_id ) except TypeError : return
def main ( ) : args = get_args ( ) ret_code = args . target ( args ) _logger . debug ( 'Exiting with code %d' , ret_code ) sys . exit ( ret_code )
def update_file ( url , filename ) : resp = urlopen ( url ) if resp . code != 200 : raise Exception ( 'GET {} failed.' . format ( url ) ) with open ( _get_package_path ( filename ) , 'w' ) as fp : for l in resp : if not l . startswith ( b'#' ) : fp . write ( l . decode ( 'utf8' ) ) print ( 'Updated {}' . format ( filename ) )
def driver ( self ) : if self . _driver is None : self . _driver = ImageDriver ( self . ds . GetDriver ( ) ) return self . _driver
def lookup_alphabet ( charset ) : if charset in PRESETS : return PRESETS [ charset ] if len ( charset ) < 16 : _logger . warning ( 'very small alphabet in use, possibly a failed lookup?' ) return charset
def _chunk_to_long ( self , chunk ) : return sum ( [ 256 ** ( self . chunklen [ 0 ] - 1 - i ) * ord_byte ( chunk [ i ] ) for i in range ( self . chunklen [ 0 ] ) ] )
def _get_chunk ( self , data , index ) : return data [ index * self . chunklen [ 0 ] : ( index + 1 ) * self . chunklen [ 0 ] ]
def memoize ( func ) : cache = { } @ wraps ( func ) def inner ( filename ) : if filename not in cache : cache [ filename ] = func ( filename ) return cache [ filename ] return inner
def _regexp ( filename ) : lines = _get_resource_content ( filename ) . decode ( 'utf-8' ) . splitlines ( ) return re . compile ( '|' . join ( lines ) )
def _detect_timezone ( ) : default_timezone = 'America/New_York' locale_code = locale . getdefaultlocale ( ) return default_timezone if not locale_code [ 0 ] else str ( pytz . country_timezones [ locale_code [ 0 ] [ - 2 : ] ] [ 0 ] )
def to_dict ( self ) : result = { } for attr , _ in iteritems ( self . swagger_types ) : value = getattr ( self , attr ) if isinstance ( value , list ) : result [ attr ] = list ( map ( lambda x : x . to_dict ( ) if hasattr ( x , "to_dict" ) else x , value ) ) elif hasattr ( value , "to_dict" ) : result [ attr ] = value . to_dict ( ) else : result [ attr ] = value return result
def activate_pdb_hook ( ) : def debug_exception ( type_exception , value , tb ) : import pdb pdb . post_mortem ( tb ) import sys sys . excepthook = debug_exception
def worker_main ( job_handler , host , port ) : loop = asyncio . new_event_loop ( ) asyncio . set_event_loop ( None ) loop . run_until_complete ( handle_jobs ( job_handler , host , port , loop = loop ) ) loop . close ( )
def _send_message ( self , msg ) : LWLink . the_queue . put_nowait ( msg ) if LWLink . thread is None or not LWLink . thread . isAlive ( ) : LWLink . thread = Thread ( target = self . _send_queue ) LWLink . thread . start ( )
def turn_on_light ( self , device_id , name ) : msg = "!%sFdP32|Turn On|%s" % ( device_id , name ) self . _send_message ( msg )
def turn_on_switch ( self , device_id , name ) : msg = "!%sF1|Turn On|%s" % ( device_id , name ) self . _send_message ( msg )
def turn_on_with_brightness ( self , device_id , name , brightness ) : brightness_value = round ( ( brightness * 31 ) / 255 ) + 1 msg = "!%sFdP%d|Lights %d|%s" % ( device_id , brightness_value , brightness_value , name ) self . _send_message ( msg )
def turn_off ( self , device_id , name ) : msg = "!%sF0|Turn Off|%s" % ( device_id , name ) self . _send_message ( msg )
def _send_queue ( self ) : while not LWLink . the_queue . empty ( ) : self . _send_reliable_message ( LWLink . the_queue . get_nowait ( ) )
def _send_reliable_message ( self , msg ) : result = False max_retries = 15 trans_id = next ( LWLink . transaction_id ) msg = "%d,%s" % ( trans_id , msg ) try : with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as write_sock , socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as read_sock : write_sock . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) read_sock . setsockopt ( socket . SOL_SOCKET , socket . SO_BROADCAST , 1 ) read_sock . settimeout ( self . SOCKET_TIMEOUT ) read_sock . bind ( ( '0.0.0.0' , self . RX_PORT ) ) while max_retries : max_retries -= 1 write_sock . sendto ( msg . encode ( 'UTF-8' ) , ( LWLink . link_ip , self . TX_PORT ) ) result = False while True : response , dummy = read_sock . recvfrom ( 1024 ) response = response . decode ( 'UTF-8' ) if "Not yet registered." in response : _LOGGER . error ( "Not yet registered" ) self . register ( ) result = True break if response . startswith ( "%d,OK" % trans_id ) : result = True break if response . startswith ( "%d,ERR" % trans_id ) : _LOGGER . error ( response ) break _LOGGER . info ( response ) if result : break time . sleep ( 0.25 ) except socket . timeout : _LOGGER . error ( "LW broker timeout!" ) return result except Exception as ex : _LOGGER . error ( ex ) raise if result : _LOGGER . info ( "LW broker OK!" ) else : _LOGGER . error ( "LW broker fail!" ) return result
def reset_ ( self ) : for opt , meta in self . defaults_ ( ) : self [ opt ] = meta . default
def _names ( section , option ) : meta = section . def_ [ option ] action = meta . cmd_kwargs . get ( 'action' ) if action is internal . Switch : names = [ '-{}' . format ( option ) , '+{}' . format ( option ) ] if meta . shortname is not None : names . append ( '-{}' . format ( meta . shortname ) ) names . append ( '+{}' . format ( meta . shortname ) ) else : names = [ '--{}' . format ( option ) ] if meta . shortname is not None : names . append ( '-{}' . format ( meta . shortname ) ) return names
def _cmd_opts_solver ( self , cmd_name ) : sections = self . sections_list ( cmd_name ) cmd_dict = self . _opt_cmds [ cmd_name ] if cmd_name else self . _opt_bare for sct in reversed ( sections ) : for opt , opt_meta in self . _conf [ sct ] . def_ . items ( ) : if not opt_meta . cmd_arg : continue if opt not in cmd_dict : cmd_dict [ opt ] = sct else : warnings . warn ( 'Command <{0}>: {1}.{2} shadowed by {3}.{2}' . format ( cmd_name , sct , opt , cmd_dict [ opt ] ) , error . LoamWarning , stacklevel = 4 )
def _add_options_to_parser ( self , opts_dict , parser ) : store_bool = ( 'store_true' , 'store_false' ) for opt , sct in opts_dict . items ( ) : meta = self . _conf [ sct ] . def_ [ opt ] kwargs = copy . deepcopy ( meta . cmd_kwargs ) action = kwargs . get ( 'action' ) if action is internal . Switch : kwargs . update ( nargs = 0 ) elif meta . default is not None and action not in store_bool : kwargs . setdefault ( 'type' , type ( meta . default ) ) kwargs . update ( help = meta . help ) kwargs . setdefault ( 'default' , self . _conf [ sct ] [ opt ] ) parser . add_argument ( * _names ( self . _conf [ sct ] , opt ) , * * kwargs )
async def start_master ( host = "" , port = 48484 , * , loop = None ) : loop = loop if loop is not None else asyncio . get_event_loop ( ) manager = jobs . JobManager ( loop = loop ) workers = set ( ) server = await loop . create_server ( lambda : WorkerProtocol ( manager , workers ) , host , port ) return Master ( server , manager , workers , loop = loop )
def run ( self , job_list ) : if self . _closed : raise RuntimeError ( "master is closed" ) return self . _manager . add_job_set ( job_list )
def add ( self , result ) : assert not self . _complete self . _results . append ( result ) self . _change ( )
def _done ( self ) : self . _results . complete ( ) waiters = self . _waiters for waiter in waiters : waiter . set_result ( None ) self . _manager . job_set_done ( self )
def get_job ( self , callback ) : assert not self . _closed if self . _active_js is None or not self . _active_js . job_available ( ) : self . _ready_callbacks . append ( callback ) else : job = self . _active_js . get_job ( ) self . _job_sources [ job ] = self . _active_js callback ( job )
def return_job ( self , job ) : if self . _closed : return js = self . _job_sources [ job ] if len ( self . _ready_callbacks ) > 0 : callback = self . _ready_callbacks . popleft ( ) callback ( job ) else : del self . _job_sources [ job ] js . return_job ( job )
def _uniquify ( _list ) : seen = set ( ) result = [ ] for x in _list : if x not in seen : result . append ( x ) seen . add ( x ) return result
def get_region ( ) : global _REGION if _REGION is None : region_name = os . getenv ( "AWS_DEFAULT_REGION" ) or "us-east-1" region_dict = { r . name : r for r in boto . regioninfo . get_regions ( "ec2" ) } if region_name not in region_dict : raise ValueError ( "No such EC2 region: {}. Check AWS_DEFAULT_REGION " "environment variable" . format ( region_name ) ) _REGION = region_dict [ region_name ] return _REGION
def sort_by ( cls , entries , attribute ) : def key ( entry ) : return entry . _get_attrib ( attribute , convert_to_str = True ) return sorted ( entries , key = key )
def add_timestamp ( logger_class , log_method , event_dict ) : event_dict [ 'timestamp' ] = calendar . timegm ( time . gmtime ( ) ) return event_dict
def logger ( name = __name__ , output = None , uuid = False , timestamp = False ) : processors = [ ] if output == 'json' : processors . append ( structlog . processors . JSONRenderer ( ) ) if uuid : processors . append ( add_unique_id ) if uuid : processors . append ( add_timestamp ) return structlog . wrap_logger ( logbook . Logger ( name ) , processors = processors )
def setup ( title , output = 'json' , timezone = None ) : timezone = timezone or dna . time_utils . _detect_timezone ( ) broker_url = 'redis://{}:{}/{}' . format ( os . environ . get ( 'BROKER_HOST' , 'localhost' ) , os . environ . get ( 'BROKER_PORT' , 6379 ) , 0 ) app = Celery ( title , broker = broker_url ) app . conf . update ( CELERY_TASK_SERIALIZER = output , CELERY_ACCEPT_CONTENT = [ output ] , CELERY_RESULT_SERIALIZER = output , CELERY_RESULT_BACKEND = broker_url , CELERY_TIMEZONE = timezone , CELERYD_FORCE_EXECV = True , CELERY_ENABLE_UTC = True , CELERY_IGNORE_RESULT = False ) return app
def delete ( self , worker_id ) : code = 200 if worker_id in self . jobs : self . jobs [ worker_id ] [ 'worker' ] . revoke ( terminate = True ) report = { 'id' : worker_id , 'revoked' : True } self . jobs . pop ( worker_id ) else : report = { 'error' : 'job {} unknown' . format ( worker_id ) } code = 404 return flask . jsonify ( report ) , code
def color ( number ) : if supports_256 ( ) : template = "\033[38;5;{number}m{text}\033[0m" else : template = "\033[{number}m{text}\033[0m" def _color ( text ) : if not all ( [ sys . stdout . isatty ( ) , sys . stderr . isatty ( ) ] ) : return text else : return template . format ( number = number , text = text ) return _color
def get_color_hash ( string , _min = MIN_COLOR_BRIGHT , _max = MAX_COLOR_BRIGHT ) : hash_num = int ( hashlib . sha1 ( string . encode ( 'utf-8' ) ) . hexdigest ( ) [ : 6 ] , 16 ) _range = _max - _min num_in_range = hash_num % _range return color ( _min + num_in_range )
def random_color ( _min = MIN_COLOR , _max = MAX_COLOR ) : return color ( random . randint ( _min , _max ) )
def requires_token_auth ( resource ) : @ functools . wraps ( resource ) def decorated ( * args , * * kwargs ) : ''' Check provided token ''' token = flask . request . headers . get ( 'Authorization' ) user = check_token ( token ) if not token or user is None : log . warn ( 'authentification failed' , token = token ) return auth_failed ( ) flask . g . user = user log . info ( 'authentification succeeded' , token = token , user = flask . g . user ) return resource ( * args , * * kwargs ) return decorated
def is_running ( process ) : try : pgrep = sh . Command ( '/usr/bin/pgrep' ) pgrep ( process ) flag = True except sh . ErrorReturnCode_1 : flag = False return flag
def dynamic_import ( mod_path , obj_name = None ) : try : module = __import__ ( mod_path , fromlist = [ 'whatever' ] ) except ImportError , error : raise errors . DynamicImportFailed ( module = '.' . join ( [ mod_path , obj_name ] ) , reason = error ) reload ( module ) if obj_name is None : obj = module elif hasattr ( module , obj_name ) : obj = getattr ( module , obj_name ) else : raise errors . DynamicImportFailed ( module = '.' . join ( [ mod_path , obj_name ] ) , reason = 'module {} has no attribute {}' . format ( module . __name__ , obj_name ) ) return None return obj
def self_ip ( public = False ) : try : if public : data = str ( urlopen ( 'http://checkip.dyndns.com/' ) . read ( ) ) ip_addr = re . compile ( r'Address: (\d+\.\d+\.\d+\.\d+)' ) . search ( data ) . group ( 1 ) else : sock = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) sock . connect ( ( 'google.com' , 0 ) ) ip_addr = sock . getsockname ( ) [ 0 ] except Exception , error : print ( 'Online test failed : {}' . format ( error ) ) raise return ip_addr
def request ( self , method , url , query_params = None , headers = None , post_params = None , body = None ) : if method == "GET" : return self . rest_client . GET ( url , query_params = query_params , headers = headers ) elif method == "HEAD" : return self . rest_client . HEAD ( url , query_params = query_params , headers = headers ) elif method == "OPTIONS" : return self . rest_client . OPTIONS ( url , query_params = query_params , headers = headers , post_params = post_params , body = body ) elif method == "POST" : return self . rest_client . POST ( url , query_params = query_params , headers = headers , post_params = post_params , body = body ) elif method == "PUT" : return self . rest_client . PUT ( url , query_params = query_params , headers = headers , post_params = post_params , body = body ) elif method == "PATCH" : return self . rest_client . PATCH ( url , query_params = query_params , headers = headers , post_params = post_params , body = body ) elif method == "DELETE" : return self . rest_client . DELETE ( url , query_params = query_params , headers = headers ) else : raise ValueError ( "http method must be `GET`, `HEAD`," " `POST`, `PATCH`, `PUT` or `DELETE`." )
def serve ( self , app_docopt = DEFAULT_DOC , description = '' ) : exit_status = 0 if isinstance ( app_docopt , str ) : args = docopt ( app_docopt , version = description ) elif isinstance ( app_docopt , dict ) : args = app_docopt else : raise ValueError ( 'unknown configuration object ({})' . format ( type ( app_docopt ) ) ) log_level = args . get ( '--log' , 'debug' ) is_debug = args . get ( '--debug' , False ) log_output = 'stdout' if is_debug else 'apy.log' safe_bind = args . get ( '--bind' , '127.0.0.1' ) safe_port = int ( args . get ( '--port' , 5000 ) ) log_setup = dna . logging . setup ( level = log_level , output = log_output ) with log_setup . applicationbound ( ) : try : log . info ( 'server ready' , version = description , log = log_level , debug = is_debug , bind = '{}:{}' . format ( safe_bind , safe_port ) ) self . app . run ( host = safe_bind , port = safe_port , debug = is_debug ) except Exception as error : if is_debug : raise log . error ( '{}: {}' . format ( type ( error ) . __name__ , str ( error ) ) ) exit_status = 1 finally : log . info ( 'session ended with status {}' . format ( exit_status ) ) return exit_status
def render ( self , name , value , attrs = None ) : context = attrs or { } context . update ( { 'name' : name , 'value' : value , } ) return render_to_string ( self . template_name , context )
def networkdays ( from_date , to_date , locale = 'en-US' ) : holidays = locales [ locale ] return workdays . networkdays ( from_date , to_date , holidays )
def _get_path ( cmd ) : if cmd in _PATHS : return _PATHS [ cmd ] out = subprocess . check_output ( 'which {}' . format ( cmd ) , shell = True ) _PATHS [ cmd ] = out . decode ( "utf-8" ) . strip ( ) return _PATHS [ cmd ]
def _build_ssh_command ( hostname , username , idfile , ssh_command , tunnel ) : command = [ _get_path ( 'ssh' ) , '-o' , 'StrictHostKeyChecking=no' , '-o' , 'ConnectTimeout=5' ] if idfile is not None : command . extend ( [ '-i' , idfile ] ) if tunnel is not None : command . extend ( [ '-A' , '-t' , tunnel , 'ssh' , '-A' , '-t' ] ) if username is not None : command . append ( '{}@{}' . format ( username , hostname ) ) else : command . append ( hostname ) if ssh_command is not None : command . append ( repr ( ssh_command ) ) return ( ' ' . join ( command ) )
def load ( cls , profile_name = None ) : lsi_location = os . path . expanduser ( '~/.lsi' ) if not os . path . exists ( lsi_location ) : return LsiProfile ( ) cfg_parser = ConfigParser ( ) cfg_parser . read ( lsi_location ) if profile_name is None : if cfg_parser . has_section ( 'default' ) : profile_name = 'default' else : return cls ( ) elif not cfg_parser . has_section ( profile_name ) : raise cls . LoadError ( 'No such profile {}' . format ( profile_name ) ) def _get ( option , alt = None ) : """Gets an option if it exists; else returns `alt`.""" if cfg_parser . has_option ( profile_name , option ) : return cfg_parser . get ( profile_name , option ) else : return alt if cfg_parser . has_option ( profile_name , 'inherit' ) : profile = cls . load ( cfg_parser . get ( profile_name , 'inherit' ) ) else : profile = cls ( ) profile . override ( 'username' , _get ( 'username' ) ) profile . override ( 'identity_file' , _get ( 'identity file' ) ) profile . override ( 'command' , _get ( 'command' ) ) filters = [ s for s in _get ( 'filters' , '' ) . split ( ',' ) if len ( s ) > 0 ] exclude = [ s for s in _get ( 'exclude' , '' ) . split ( ',' ) if len ( s ) > 0 ] profile . filters . extend ( filters ) profile . exclude . extend ( exclude ) return profile
def from_args ( args ) : if args . username is not None or args . identity_file is not None : profile = LsiProfile ( ) else : profile = LsiProfile . load ( args . profile ) profile . override ( 'username' , args . username ) profile . override ( 'identity_file' , args . identity_file ) profile . override ( 'command' , args . command ) profile . no_prompt = args . no_prompt profile . filters . extend ( args . filters ) profile . exclude . extend ( args . exclude ) if profile . identity_file is not None : profile . identity_file = os . path . expanduser ( profile . identity_file ) return profile
def relate ( self , part , id = None ) : assert part . name . startswith ( self . base ) name = part . name [ len ( self . base ) : ] . lstrip ( '/' ) rel = Relationship ( self , name , part . rel_type , id = id ) self . relationships . add ( rel ) return rel
def related ( self , reltype ) : parts = [ ] package = getattr ( self , 'package' , None ) or self for rel in self . relationships . types . get ( reltype , [ ] ) : parts . append ( package [ posixpath . join ( self . base , rel . target ) ] ) return parts
def _load_rels ( self , source ) : self . relationships . load ( source = self , data = source )
def _load_part ( self , rel_type , name , data ) : if self . content_types . find_for ( name ) is None : log . warning ( 'no content type found for part %(name)s' % vars ( ) ) return cls = Part . classes_by_rel_type [ rel_type ] part = cls ( self , name ) part . load ( data ) self [ name ] = part return part
def find_for ( self , name ) : map = self . items return map . get ( name , None ) or map . get ( get_ext ( name ) or None , None )
def from_element ( cls , element ) : ns , class_name = parse_tag ( element . tag ) class_ = getattr ( ContentType , class_name ) if not class_ : msg = 'Invalid Types child element: %(class_name)s' % vars ( ) raise ValueError ( msg ) key = element . get ( class_ . key_name ) name = element . get ( 'ContentType' ) return class_ ( name , key )
def as_stream ( self ) : stream = io . BytesIO ( ) self . _store ( stream ) stream . seek ( 0 ) return stream
def loud ( self , lang = 'englist' ) : lang_method = getattr ( self , lang , None ) if lang_method : return lang_method ( ) . upper ( ) else : return self . english ( ) . upper ( )
def upload ( ctx , product , git_ref , dirname , aws_id , aws_secret , ci_env , on_travis_push , on_travis_pr , on_travis_api , on_travis_cron , skip_upload ) : logger = logging . getLogger ( __name__ ) if skip_upload : click . echo ( 'Skipping ltd upload.' ) sys . exit ( 0 ) logger . debug ( 'CI environment: %s' , ci_env ) logger . debug ( 'Travis events settings. ' 'On Push: %r, PR: %r, API: %r, Cron: %r' , on_travis_push , on_travis_pr , on_travis_api , on_travis_cron ) if ci_env == 'travis' and _should_skip_travis_event ( on_travis_push , on_travis_pr , on_travis_api , on_travis_cron ) : sys . exit ( 0 ) ensure_login ( ctx ) git_refs = _get_git_refs ( ci_env , git_ref ) build_resource = register_build ( ctx . obj [ 'keeper_hostname' ] , ctx . obj [ 'token' ] , product , git_refs ) logger . debug ( 'Created build resource %r' , build_resource ) upload_dir ( build_resource [ 'bucket_name' ] , build_resource [ 'bucket_root_dir' ] , dirname , aws_access_key_id = aws_id , aws_secret_access_key = aws_secret , surrogate_key = build_resource [ 'surrogate_key' ] , cache_control = 'max-age=31536000' , surrogate_control = None , upload_dir_redirect_objects = True ) logger . debug ( 'Upload complete for %r' , build_resource [ 'self_url' ] ) confirm_build ( build_resource [ 'self_url' ] , ctx . obj [ 'token' ] ) logger . debug ( 'Build %r complete' , build_resource [ 'self_url' ] )
def part_edit_cmd ( ) : parser = argparse . ArgumentParser ( description = inspect . getdoc ( part_edit_cmd ) ) parser . add_argument ( 'path' , help = 'Path to part (including path to zip file, i.e. ./file.zipx/part)' , ) parser . add_argument ( '--reformat-xml' , action = 'store_true' , help = ( 'run the content through an XML pretty-printer ' 'first for improved editability' ) , ) args = parser . parse_args ( ) part_edit ( args . path , args . reformat_xml )
def pack_dir_cmd ( ) : parser = argparse . ArgumentParser ( description = inspect . getdoc ( part_edit_cmd ) ) parser . add_argument ( 'path' , help = ( 'Path to list (including path to zip file, ' 'i.e. ./file.zipx or ./file.zipx/subdir)' ) , ) args = parser . parse_args ( ) for item , is_file in sorted ( list_contents ( args . path ) ) : prefix = 'd ' if not is_file else '  ' msg = prefix + item print ( msg )
def process_module ( self , node ) : if self . config . file_header : if sys . version_info [ 0 ] < 3 : pattern = re . compile ( '\A' + self . config . file_header , re . LOCALE | re . MULTILINE ) else : pattern = re . compile ( '\A' + self . config . file_header , re . MULTILINE ) content = None with node . stream ( ) as stream : content = stream . read ( ) . decode ( 'utf-8' ) matches = pattern . findall ( content ) if len ( matches ) != 1 : self . add_message ( 'invalid-file-header' , 1 , args = self . config . file_header )
def html ( self , slug , name , chart_obj , filepath = None , html_before = "" , html_after = "" ) : try : html = "" if name : html = "<h3>" + name + "</h3>" json_data = chart_obj . to_json ( ) json_data = self . _patch_json ( json_data ) html = html_before + html + self . _json_to_html ( slug , json_data ) + html_after except Exception as e : tr . new ( e ) tr . check ( ) if filepath is not None : self . _write_file ( slug , filepath , html ) return None else : return html
def _patch_json ( self , json_data ) : json_data = json . loads ( json_data ) json_data [ "$schema" ] = "https://vega.github.io/schema/vega-lite/2.0.0-beta.15.json" json_data [ "width" ] = json_data [ "config" ] [ "cell" ] [ "width" ] json_data [ "height" ] = json_data [ "config" ] [ "cell" ] [ "height" ] del ( json_data [ "config" ] [ "cell" ] ) return json . dumps ( json_data )
def _json_to_html ( self , slug , json_data ) : html = '<div id="chart-' + slug + '"></div>' html += '<script>' html += 'var s' + slug + ' = ' + json_data + ';' html += 'vega.embed("#chart-' + slug + '", s' + slug + ');' #html += 'console.log(JSON.stringify(s{id}, null, 2));' html += '</script>' return html
def _dict_to_df ( self , dictobj , xfield , yfield ) : x = [ ] y = [ ] for datapoint in dictobj : x . append ( datapoint ) y . append ( dictobj [ datapoint ] ) df = pd . DataFrame ( { xfield [ 0 ] : x , yfield [ 0 ] : y } ) return df
def _write_file ( self , slug , folderpath , html ) : if not os . path . isdir ( folderpath ) : try : os . makedirs ( folderpath ) except Exception as e : tr . err ( e ) filepath = folderpath + "/" + slug + ".html" #~ write the file try : filex = open ( filepath , "w" ) filex . write ( html ) filex . close ( ) except Exception as e : tr . err ( e )
def _chart_class ( self , df , chart_type , * * kwargs ) : if chart_type == "bar" : return Chart ( df ) . mark_bar ( * * kwargs ) elif chart_type == "circle" : return Chart ( df ) . mark_circle ( * * kwargs ) elif chart_type == "line" : return Chart ( df ) . mark_line ( * * kwargs ) elif chart_type == "point" : return Chart ( df ) . mark_point ( * * kwargs ) elif chart_type == "area" : return Chart ( df ) . mark_area ( * * kwargs ) elif chart_type == "tick" : return Chart ( df ) . mark_tick ( * * kwargs ) elif chart_type == "text" : return Chart ( df ) . mark_text ( * * kwargs ) elif chart_type == "square" : return Chart ( df ) . mark_square ( * * kwargs ) elif chart_type == "rule" : return Chart ( df ) . mark_rule ( * * kwargs ) return None
def _encode_fields ( self , xfield , yfield , time_unit = None , scale = Scale ( zero = False ) ) : if scale is None : scale = Scale ( ) xfieldtype = xfield [ 1 ] yfieldtype = yfield [ 1 ] x_options = None if len ( xfield ) > 2 : x_options = xfield [ 2 ] y_options = None if len ( yfield ) > 2 : y_options = yfield [ 2 ] if time_unit is not None : if x_options is None : xencode = X ( xfieldtype , timeUnit = time_unit ) else : xencode = X ( xfieldtype , axis = Axis ( * * x_options ) , timeUnit = time_unit , scale = scale ) else : if x_options is None : xencode = X ( xfieldtype ) else : xencode = X ( xfieldtype , axis = Axis ( * * x_options ) , scale = scale ) if y_options is None : yencode = Y ( yfieldtype , scale = scale ) else : yencode = Y ( yfieldtype , axis = Axis ( * * y_options ) , scale = scale ) return xencode , yencode
def _infer_tarball_url ( ) : try : with click . open_file ( 'app.json' , 'r' ) as f : contents = f . read ( ) app_json = json . loads ( contents ) except IOError : return None repository = app_json . get ( 'repository' ) if not repository : return None else : return app_json . get ( 'repository' ) + '/tarball/master/'
def up ( tarball_url , auth_token , env , app_name ) : tarball_url = tarball_url or _infer_tarball_url ( ) if not tarball_url : click . echo ( 'No tarball URL found.' ) sys . exit ( 1 ) if env : env = { arg . split ( '=' ) [ 0 ] : arg . split ( '=' ) [ 1 ] for arg in env } happy = Happy ( auth_token = auth_token ) click . echo ( 'Creating app... ' , nl = False ) build_id , app_name = happy . create ( tarball_url = tarball_url , env = env , app_name = app_name , ) click . echo ( app_name ) click . echo ( 'Building... ' , nl = False ) happy . wait ( build_id ) _write_app_name ( app_name ) click . echo ( 'done' ) click . echo ( "It's up! :) https://%s.herokuapp.com" % app_name )
def down ( auth_token , force , app_name ) : if not app_name : click . echo ( 'WARNING: Inferring the app name when deleting is deprecated. ' 'Starting with happy 2.0, the app_name parameter will be required.' ) app_name = app_name or _read_app_name ( ) if not app_name : click . echo ( 'No app name given.' ) sys . exit ( 1 ) if not force : click . confirm ( 'Are you sure you want to delete %s?' % app_name , abort = True , ) happy = Happy ( auth_token = auth_token ) click . echo ( 'Destroying app %s... ' % app_name , nl = False ) happy . delete ( app_name = app_name ) _delete_app_name_file ( ) click . echo ( 'done' ) click . echo ( "It's down. :(" )
def date ( start , end ) : stime = date_to_timestamp ( start ) etime = date_to_timestamp ( end ) ptime = stime + random . random ( ) * ( etime - stime ) return datetime . date . fromtimestamp ( ptime )
def _get_session ( self ) : session = Session ( ) session . headers = { 'Content-type' : 'application/json' , 'Accept' : 'application/vnd.heroku+json; version=3' , } if self . _auth_token : session . trust_env = False session . headers [ 'Authorization' ] = 'Bearer %s' % self . _auth_token return session
def get_root_argparser ( self ) : return self . arg_parse_class ( description = self . get_help ( ) , formatter_class = self . get_formatter_class ( ) )
def get_description ( self ) : if self . description : return self . description elif self . __doc__ and self . __doc__ . strip ( ) : return self . __doc__ . strip ( ) . split ( '.' ) [ 0 ] + '.' else : return ''
def get_help ( self ) : if self . help : return self . help elif self . __doc__ and self . __doc__ . strip ( ) : return self . __doc__ . strip ( ) else : return ''
def get_version ( ) : with open ( os . path . join ( os . path . dirname ( __file__ ) , 'argparsetree' , '__init__.py' ) ) as init_py : return re . search ( '__version__ = [\'"]([^\'"]+)[\'"]' , init_py . read ( ) ) . group ( 1 )
def url_with_auth ( regex , view , kwargs = None , name = None , prefix = '' ) : from djapiauth . auth import api_auth if isinstance ( view , six . string_types ) : return url ( regex , api_auth ( import_by_path ( prefix + "." + view if prefix else view ) ) ) elif isinstance ( view , ( list , tuple ) ) : return url ( regex , view , name , prefix , * * kwargs ) else : return url ( regex , api_auth ( view ) )
def render ( self ) : for opt , values in self . data . items ( ) : if opt == 'ticks' : self [ 'chxtc' ] = '|' . join ( values ) else : self [ 'chx%s' % opt [ 0 ] ] = '|' . join ( values ) return self
def dataset ( self , data , series = '' ) : self . _dataset = data self . _series = series return self
def render ( self ) : self . update ( self . axes . render ( ) ) encoder = Encoder ( self . _encoding , None , self . _series ) if not 'chs' in self : self [ 'chs' ] = '300x150' else : size = self [ 'chs' ] . split ( 'x' ) assert len ( size ) == 2 , 'Invalid size, must be in the format WxH' self . check_size ( * map ( int , size ) ) assert 'cht' in self , 'No chart type defined, use type method' self [ 'cht' ] = self . check_type ( self [ 'cht' ] ) if ( 'any' in dir ( self . _dataset ) and self . _dataset . any ( ) ) or self . _dataset : self [ 'chd' ] = encoder . encode ( self . _dataset ) elif not 'choe' in self : assert 'chd' in self , 'You must have a dataset, or use chd' if self . _scale : assert self [ 'chd' ] . startswith ( 't' ) , 'You must use text encoding with chds' self [ 'chds' ] = ',' . join ( self . _scale ) if self . _geo and self . _ld : self [ 'chtm' ] = self . _geo self [ 'chld' ] = self . _ld if self . lines : self [ 'chls' ] = '|' . join ( self . lines ) if self . markers : self [ 'chm' ] = '|' . join ( self . markers ) if self . fills : self [ 'chf' ] = '|' . join ( self . fills )
def url ( self ) : self . render ( ) return self . _apiurl + '&' . join ( self . _parts ( ) ) . replace ( ' ' , '+' )
def urlopen ( self ) : req = Request ( str ( self ) ) try : return urlopen ( req ) except HTTPError : _print ( 'The server couldn\'t fulfill the request.' ) except URLError : _print ( 'We failed to reach a server.' )
def parse_args ( ) : usage = "Usage: create_concordance <infile> [<outfile>]" description = "Simple Concordance Generator" argparser = argparse . ArgumentParser ( usage = usage , description = description ) argparser . add_argument ( 'infile' , type = argparse . FileType ( 'r' ) , help = "File read in to create concordance" ) argparser . add_argument ( 'outfile' , nargs = '?' , type = argparse . FileType ( 'w' ) , default = sys . stdout , help = "File to write concordance to.  " "Default is stdout" ) argparser . add_argument ( '--word' , nargs = "?" , const = str , help = "Display a word in concordance" ) args = argparser . parse_args ( ) return args
def addCommandLineArgs ( arg_parser ) : arg_parser . register ( "action" , "log_levels" , LogLevelAction ) arg_parser . register ( "action" , "log_files" , LogFileAction ) arg_parser . register ( "action" , "log_help" , LogHelpAction ) group = arg_parser . add_argument_group ( "Logging options" ) group . add_argument ( "-l" , "--log-level" , dest = "log_levels" , action = "log_levels" , metavar = "LOGGER:LEVEL" , default = [ ] , help = "Set log levels for individual loggers. See --help-logging for " "complete details." ) group . add_argument ( "-L" , "--log-file" , dest = "log_files" , action = "log_files" , metavar = "LOGGER:FILE" , default = [ ] , help = "Set log the output file for individual loggers. " " See --help-logging for complete details." ) group . add_argument ( "--help-logging" , action = "log_help" , help = argparse . SUPPRESS )
def validate_page_number ( number ) : try : number = int ( number ) except ( TypeError , ValueError ) : raise PageNotAnInteger ( 'That page number is not an integer' ) if number < 1 : raise EmptyPage ( 'That page number is less than 1' ) return number
def chmod ( path , mode , recursive = True ) : if recursive : cmd = 'chmod -R %s %s' % ( mode , path ) else : cmd = 'chmod %s %s' % ( mode , path ) return sh ( cmd )
def _create_bundle ( self , data ) : kwargs = { } filters = None if isinstance ( data , dict ) : kwargs . update ( filters = data . get ( 'filters' , None ) , output = data . get ( 'output' , None ) , debug = data . get ( 'debug' , None ) , extra = data . get ( 'extra' , { } ) , config = data . get ( 'config' , { } ) , depends = data . get ( 'depends' , None ) ) bundle = Bundle ( * list ( self . _yield_bundle_contents ( data ) ) , * * kwargs ) return self . _auto_filter_bundle ( bundle )
def urls_for ( self , asset_type , * args , * * kwargs ) : return self . urls_for_depends ( asset_type , * args , * * kwargs ) + self . urls_for_self ( asset_type , * args , * * kwargs )
def html_tags_for ( self , asset_type , * args , * * kwargs ) : html = [ ] for ref in self . depends : html . append ( self . _ref ( ref ) . html_tags_for ( asset_type , * args , * * kwargs ) ) if asset_type in self . typed_bundles : html . append ( render_asset_html_tags ( asset_type , self . urls_for_self ( asset_type , * args , * * kwargs ) ) ) return "\n" . join ( html )
def html_tags ( self , * args , * * kwargs ) : html = [ ] for asset_type in list_asset_types ( ) : html . append ( self . html_tags_for ( asset_type . name , * args , * * kwargs ) ) return "\n" . join ( html )
def find_version ( filename ) : with io . open ( filename , encoding = "utf-8" ) as version_file : version_match = re . search ( r'^__version__ = [\'"]([^\'"]*)[\'"]' , version_file . read ( ) , re . M ) if version_match : return version_match . group ( 1 ) return "0.0-version-unknown"
def import_modules ( self ) : modules = self . get_modules ( ) log . info ( "import service modules: " + str ( modules ) ) try : for module in modules : __import__ ( module ) except ImportError as error : raise ImportModulesError ( error . msg )
def send ( self , peer , typename , data ) : def attempt_to_send ( _ ) : if peer not in self . _connections : d = self . _connect ( peer ) d . addCallback ( attempt_to_send ) return d else : conn = self . _connections [ peer ] [ 0 ] conn . send_packet ( typename , data ) return defer . succeed ( None ) d = attempt_to_send ( None ) self . _ongoing_sends . add ( d ) def send_completed ( result ) : if d in self . _ongoing_sends : self . _ongoing_sends . remove ( d ) return result d . addBoth ( send_completed ) return d
def receiveData ( self , connection , data ) : try : protocol = self . _protocols [ connection ] except KeyError : raise NoSuchConnection ( ) protocol . dataReceived ( data ) return { }
def disconnect ( self , connection ) : proto = self . _protocols . pop ( connection ) proto . transport = None return { }
def _sendData ( self , data ) : d = self . _callRemote ( Transmit , connection = self . connection , data = data ) d . addErrback ( log . err )
def getLocalProtocol ( self , connectionIdentifier ) : for factory in self . localFactories : try : return factory . protocols [ connectionIdentifier ] except KeyError : continue raise NoSuchConnection ( )
def disconnect ( self , connection ) : proto = self . getLocalProtocol ( connection ) proto . transport . loseConnection ( ) return { }
def centered ( mystring , linewidth = None , fill = " " ) : if linewidth is None : linewidth = get_terminal_size ( ) . columns - 1 sides = ( linewidth - length_no_ansi ( mystring ) ) // 2 extra = ( linewidth - length_no_ansi ( mystring ) ) % 2 fill = fill [ : 1 ] sidestring = fill * sides extrastring = fill * extra newstring = sidestring + mystring + sidestring + extrastring return newstring
def clock_on_right ( mystring ) : taken = length_no_ansi ( mystring ) padding = ( get_terminal_size ( ) . columns - 1 ) - taken - 5 clock = time . strftime ( "%I:%M" , time . localtime ( ) ) print ( mystring + " " * padding + clock )
def main ( arguments = None ) : if not arguments : arguments = sys . argv [ 1 : ] wordlist , sowpods , by_length , start , end = argument_parser ( arguments ) for word in wordlist : pretty_print ( word , anagrams_in_word ( word , sowpods , start , end ) , by_length , )
def _ping ( self , peerid , callid ) : if not ( peerid , callid ) in self . _remote_to_local : logger . warn ( "No remote call %s from %s. Might just be unfoutunate timing." % ( callid , peerid ) )
def _cmdRegex ( self , cmd_grp = None ) : cmd_grp = cmd_grp or "cmd" help_opts = ( "-h" , "--help" ) cmd = self . name ( ) names = "|" . join ( [ re . escape ( cmd ) ] + [ re . escape ( a ) for a in self . aliases ( ) ] ) opts = [ ] for action in self . parser . _actions : opts += [ a for a in action . option_strings if a not in help_opts ] opts_re = "|" . join ( [ re . escape ( o ) for o in opts ] ) if opts_re : opts_re = rf"(\s+(?P<{cmd_grp}_opts>{opts_re}))*" help_re = "|" . join ( [ re . escape ( o ) for o in help_opts ] ) help_re = rf"(\s+(?P<HELP_OPTS>{help_re}))*" completers = { } if opts_re : completers [ f"{cmd_grp}_opts" ] = WordCompleter ( opts ) return tuple ( [ rf"""(?P<{cmd_grp}>{names}){opts_re}{help_re}""" , completers ] )
def fromStringProto ( self , inString , proto ) : value , = amp . AmpList . fromStringProto ( self , inString , proto ) return value
def toStringProto ( self , inObject , proto ) : return amp . AmpList . toStringProto ( self , [ inObject ] , proto )
def _connection ( username = None , password = None , host = None , port = None , db = None ) : c_opts = { } if username : c_opts [ 'user' ] = username if password : c_opts [ 'password' ] = password if host : c_opts [ 'host' ] = host if port : c_opts [ 'port' ] = port if db : c_opts [ 'database' ] = db dbc = psycopg2 . connect ( * * c_opts ) dbc . autocommit = True return dbc
def db_list ( username = None , password = None , host = None , port = None , maintain_db = 'postgres' ) : conn = _connection ( username = username , password = password , host = host , port = port , db = maintain_db ) cur = conn . cursor ( ) cur . execute ( 'SELECT DATNAME from pg_database' ) rows = cur . fetchall ( ) conn . close ( ) result = [ ] for row in rows : result . append ( row [ 0 ] ) return result
def _get_local_files ( self , path ) : if not path : raise ValueError ( "No path specified" ) files = defaultdict ( lambda : None ) path_len = len ( path ) + 1 for root , dirs , filenames in os . walk ( path ) : for name in filenames : full_path = join ( root , name ) files [ full_path [ path_len : ] ] = compute_md5 ( full_path ) return files
def tokens_required ( service_list ) : def decorator ( func ) : @ wraps ( func ) def inner ( request , * args , * * kwargs ) : for service in service_list : if service not in request . session [ "user_tokens" ] : return redirect ( 'denied' ) return func ( request , * args , * * kwargs ) return inner return decorator
def login ( request , template_name = 'ci/login.html' , redirect_field_name = REDIRECT_FIELD_NAME , authentication_form = AuthenticationForm ) : redirect_to = request . POST . get ( redirect_field_name , request . GET . get ( redirect_field_name , '' ) ) if request . method == "POST" : form = authentication_form ( request , data = request . POST ) if form . is_valid ( ) : if not is_safe_url ( url = redirect_to , host = request . get_host ( ) ) : redirect_to = resolve_url ( settings . LOGIN_REDIRECT_URL ) user = form . get_user ( ) request . session [ 'user_token' ] = user [ "token" ] request . session [ 'user_email' ] = user [ "email" ] request . session [ 'user_permissions' ] = user [ "permissions" ] request . session [ 'user_id' ] = user [ "id" ] request . session [ 'user_list' ] = user [ "user_list" ] if not settings . HIDE_DASHBOARDS : dashboards = ciApi . get_user_dashboards ( user [ "id" ] ) dashboard_list = list ( dashboards [ 'results' ] ) if len ( dashboard_list ) > 0 : request . session [ 'user_dashboards' ] = dashboard_list [ 0 ] [ "dashboards" ] request . session [ 'user_default_dashboard' ] = dashboard_list [ 0 ] [ "default_dashboard" ] [ "id" ] else : request . session [ 'user_dashboards' ] = [ ] request . session [ 'user_default_dashboard' ] = None tokens = ciApi . get_user_service_tokens ( params = { "user_id" : user [ "id" ] } ) token_list = list ( tokens [ 'results' ] ) user_tokens = { } if len ( token_list ) > 0 : for token in token_list : user_tokens [ token [ "service" ] [ "name" ] ] = { "token" : token [ "token" ] , "url" : token [ "service" ] [ "url" ] + "/api/v1" } request . session [ 'user_tokens' ] = user_tokens return HttpResponseRedirect ( redirect_to ) else : form = authentication_form ( request ) current_site = get_current_site ( request ) context = { 'form' : form , redirect_field_name : redirect_to , 'site' : current_site , 'site_name' : current_site . name , } return TemplateResponse ( request , template_name , context )
def build ( cli , path , package ) : for _ , name , ispkg in iter_modules ( path ) : module = import_module ( f'.{name}' , package ) if ispkg : build ( cli . group ( name ) ( module . group ) , module . __path__ , module . __package__ ) else : cli . command ( name ) ( module . command )
def descovery ( testdir ) : from os . path import join , exists , isdir , splitext , basename , sep if not testdir or not exists ( testdir ) or not isdir ( testdir ) : return None from os import walk import fnmatch import imp for root , _ , filenames in walk ( testdir ) : for filename in fnmatch . filter ( filenames , '*.py' ) : path = join ( root , filename ) modulepath = splitext ( root ) [ 0 ] . replace ( sep , '.' ) imp . load_source ( modulepath , path )
def main ( clargs = None ) : from argparse import ArgumentParser from librarian . library import Library import sys parser = ArgumentParser ( description = "A test runner for each card in a librarian library." ) parser . add_argument ( "library" , help = "Library database" ) parser . add_argument ( "-t" , "--tests" , default = "test/" , help = "Test directory" ) args = parser . parse_args ( clargs ) descovery ( args . tests ) library = Library ( args . library ) cardcount , passes , failures = execute_tests ( library ) print ( RESULTS . format ( len ( SINGLES ) , len ( TESTS ) , cardcount , passes , failures ) ) sys . exit ( failures )
def _writeResponse ( self , response ) : encoded = dumps ( response , default = _default ) self . transport . write ( encoded )
def connectionLost ( self , reason ) : self . _remote . boxReceiver . stopReceivingBoxes ( reason ) return basic . NetstringReceiver . connectionLost ( self , reason )
def buildProtocol ( self , addr ) : proto = self . _factory . buildProtocol ( addr ) return JSONAMPDialectReceiver ( proto )
def pout ( msg , log = None ) : _print ( msg , sys . stdout , log_func = log . info if log else None )
def perr ( msg , log = None ) : _print ( msg , sys . stderr , log_func = log . error if log else None )
def register ( CommandSubClass ) : name = CommandSubClass . name ( ) if name in Command . _all_commands : raise ValueError ( "Command already exists: " + name ) Command . _all_commands [ name ] = CommandSubClass return CommandSubClass
def register ( Class , CommandSubClass ) : for name in [ CommandSubClass . name ( ) ] + CommandSubClass . aliases ( ) : if name in Class . _registered_commands [ Class ] : raise ValueError ( "Command already exists: " + name ) Class . _registered_commands [ Class ] [ name ] = CommandSubClass return CommandSubClass
def init_mq ( self ) : mq = self . init_connection ( ) self . init_consumer ( mq ) return mq . connection
def init_modules ( self ) : if not self . config : raise ValueError ( "please read your config file." ) log . debug ( "begin to import customer's service modules." ) modules = ServiceModules ( self . config ) modules . import_modules ( ) log . debug ( "end to import customer's service modules." )
def music_info ( songid ) : if isinstance ( songid , list ) : songid = ',' . join ( songid ) data = { "hq" : 1 , "songIds" : songid } res = requests . post ( MUSIC_INFO_URL , data = data ) info = res . json ( ) music_data = info [ "data" ] songs = [ ] for song in music_data [ "songList" ] : song_link , size = _song_link ( song , music_data [ "xcode" ] ) songs . append ( { "name" : song [ "songName" ] , "singer" : song [ "artistName" ] , "lrc_link" : song [ "lrcLink" ] , "song_link" : song_link , "size" : size } ) return songs
def download_music ( song , thread_num = 4 ) : filename = "{}.mp3" . format ( song [ "name" ] ) if os . path . exists ( filename ) : os . remove ( filename ) part = int ( song [ "size" ] / thread_num ) if part <= 1024 : thread_num = 1 _id = uuid . uuid4 ( ) . hex logger . info ( "downloading '{}'..." . format ( song [ "name" ] ) ) threads = [ ] for i in range ( thread_num ) : if i == thread_num - 1 : end = '' else : end = ( i + 1 ) * part - 1 thread = Worker ( ( i * part , end ) , song , _id ) thread . start ( ) threads . append ( thread ) for t in threads : t . join ( ) fileParts = glob . glob ( "part-{}-*" . format ( _id ) ) fileParts . sort ( key = lambda e : e . split ( '-' ) [ - 1 ] ) logger . info ( "'{}' combine parts..." . format ( song [ "name" ] ) ) with open ( filename , "ab" ) as f : for part in fileParts : with open ( part , "rb" ) as d : shutil . copyfileobj ( d , f ) os . remove ( part ) logger . info ( "'{}' finished" . format ( song [ "name" ] ) )
def load_name ( self , name ) : if name in self . globals_ : return self . globals_ [ name ] b = self . globals_ [ '__builtins__' ] if isinstance ( b , dict ) : return b [ name ] else : return getattr ( b , name )
def pop ( self , n ) : poped = self . __stack [ len ( self . __stack ) - n : ] del self . __stack [ len ( self . __stack ) - n : ] return poped
def _connection ( username = None , password = None , host = None , port = None ) : c_opts = { } if username : c_opts [ 'user' ] = username if password : c_opts [ 'passwd' ] = password if host : c_opts [ 'host' ] = host if port : c_opts [ 'port' ] = port dbc = MySQLdb . connect ( * * c_opts ) dbc . autocommit ( True ) return dbc
def render_ditaa ( self , code , options , prefix = 'ditaa' ) : hashkey = code . encode ( 'utf-8' ) + str ( options ) + str ( self . builder . config . ditaa ) + str ( self . builder . config . ditaa_args ) infname = '%s-%s.%s' % ( prefix , sha ( hashkey ) . hexdigest ( ) , "ditaa" ) outfname = '%s-%s.%s' % ( prefix , sha ( hashkey ) . hexdigest ( ) , "png" ) inrelfn = posixpath . join ( self . builder . imgpath , infname ) infullfn = path . join ( self . builder . outdir , '_images' , infname ) outrelfn = posixpath . join ( self . builder . imgpath , outfname ) outfullfn = path . join ( self . builder . outdir , '_images' , outfname ) if path . isfile ( outfullfn ) : return outrelfn , outfullfn ensuredir ( path . dirname ( outfullfn ) ) if isinstance ( code , unicode ) : code = code . encode ( 'utf-8' ) ditaa_args = [ self . builder . config . ditaa ] ditaa_args . extend ( self . builder . config . ditaa_args ) ditaa_args . extend ( options ) ditaa_args . extend ( [ infullfn ] ) ditaa_args . extend ( [ outfullfn ] ) f = open ( infullfn , 'w' ) f . write ( code ) f . close ( ) try : self . builder . warn ( ditaa_args ) p = Popen ( ditaa_args , stdout = PIPE , stdin = PIPE , stderr = PIPE ) except OSError , err : if err . errno != ENOENT : raise self . builder . warn ( 'ditaa command %r cannot be run (needed for ditaa ' 'output), check the ditaa setting' % self . builder . config . ditaa ) self . builder . _ditaa_warned_dot = True return None , None wentWrong = False try : stdout , stderr = p . communicate ( code ) except OSError , err : if err . errno != EPIPE : raise wentWrong = True except IOError , err : if err . errno != EINVAL : raise wentWrong = True if wentWrong : stdout , stderr = p . stdout . read ( ) , p . stderr . read ( ) p . wait ( ) if p . returncode != 0 : raise DitaaError ( 'ditaa exited with error:\n[stderr]\n%s\n' '[stdout]\n%s' % ( stderr , stdout ) ) return outrelfn , outfullfn
def _atexit ( self ) : self . log . debug ( "Application._atexit" ) if self . _atexit_func : self . _atexit_func ( self )
def run ( self , args_list = None ) : self . log . debug ( "Application.run: {args_list}" . format ( * * locals ( ) ) ) retval = None try : retval = self . _run ( args_list = args_list ) except KeyboardInterrupt : self . log . verbose ( "Interrupted" ) except SystemExit as exit : self . log . verbose ( "Exited" ) retval = exit . code except Exception : print ( "Uncaught exception" , file = sys . stderr ) traceback . print_exc ( ) if "debug_pdb" in self . args and self . args . debug_pdb : debugger ( ) retval = Application . UNCAUGHT_EXCEPTION_EXIT raise finally : try : self . _atexit ( ) finally : sys . stderr . flush ( ) sys . stdout . flush ( ) sys . exit ( retval )
def main ( ) : parser = optparse . OptionParser ( usage = "%prog [options] <model_path> [another_model_path...]" , formatter = optparse . TitledHelpFormatter ( ) ) parser . set_description ( __doc__ . strip ( ) ) parser . add_option ( "-f" , "--function" , dest = "function" , metavar = "NAME" , help = "append integrity checking actions to functions named NAME (required)" , action = "store" , default = None ) parser . add_option ( "-o" , "--output" , dest = 'output' , metavar = "PATH" , help = "save sql model instances to PATH (required)" , action = "store" , default = None ) parser . add_option ( "-v" , "--verbosity" , dest = 'verbosity' , action = "count" , help = "increase debug logging level" , default = 2 ) ( opts , args ) = parser . parse_args ( ) if len ( args ) == 0 or None in [ opts . output , opts . function ] : parser . print_help ( ) sys . exit ( 1 ) levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } logging . basicConfig ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) m = ooaofooa . load_metamodel ( args ) for c_c in m . select_many ( 'C_C' ) : filt = lambda sel : ooaofooa . is_contained_in ( sel , c_c ) and sel . Name == opts . function s_sync = m . select_any ( 'S_SYNC' , filt ) if not s_sync : s_sync = m . new ( 'S_SYNC' , Name = opts . function ) pe_pe = m . new ( 'PE_PE' ) s_dt = m . select_any ( 'S_DT' , where ( Name = 'boolean' ) ) relate ( pe_pe , s_sync , 8001 ) relate ( s_dt , s_sync , 25 ) generate_actions ( m , c_c , s_sync ) xtuml . persist_instances ( m , opts . output )
def scrape ( ctx , url ) : data = load_feed ( url ) feed = data [ 'feed' ] entries = data [ 'entries' ] _type = 'community' country = 'Czech Republic' for entry in entries : _id = sluggify ( entry [ 'id' ] ) city = entry [ 'tags' ] [ 0 ] [ 'term' ] landing = entry [ 'link' ] start_time = dt_normalize ( entry [ 'published_parsed' ] , local_tz = True ) title = entry [ 'title' ] summary = entry [ 'summary' ] link = entry [ 'link' ] ipdb . set_trace ( )
def fancy_tag_compiler ( params , defaults , takes_var_args , takes_var_kwargs , takes_context , name , node_class , parser , token ) : bits = token . split_contents ( ) [ 1 : ] if takes_context : if 'context' in params [ : 1 ] : params = params [ 1 : ] else : raise TemplateSyntaxError ( "Any tag function decorated with takes_context=True " "must have a first argument of 'context'" ) args = [ ] kwargs = { } kwarg_found = False unhandled_params = list ( params ) handled_params = [ ] if len ( bits ) > 1 and bits [ - 2 ] == 'as' : output_var = bits [ - 1 ] if len ( set ( output_var ) - set ( ALLOWED_VARIABLE_CHARS ) ) > 0 : raise TemplateSyntaxError ( "%s got output var name with forbidden chars: '%s'" % ( name , output_var ) ) bits = bits [ : - 2 ] else : output_var = None for bit in bits : kwarg_match = kwarg_re . match ( bit ) if kwarg_match : kw , var = kwarg_match . groups ( ) if kw not in params and not takes_var_kwargs : raise TemplateSyntaxError ( "%s got unknown keyword argument '%s'" % ( name , kw ) ) elif kw in handled_params : raise TemplateSyntaxError ( "%s got multiple values for keyword argument '%s'" % ( name , kw ) ) else : kwargs [ str ( kw ) ] = var kwarg_found = True handled_params . append ( kw ) else : if kwarg_found : raise TemplateSyntaxError ( "%s got non-keyword arg after keyword arg" % name ) else : args . append ( bit ) try : handled_params . append ( unhandled_params . pop ( 0 ) ) except IndexError : if not takes_var_args : raise TemplateSyntaxError ( "%s got too many arguments" % name ) if defaults is not None : unhandled_params = unhandled_params [ : - len ( defaults ) ] if len ( unhandled_params ) == 1 : raise TemplateSyntaxError ( "%s didn't get a value for argument '%s'" % ( name , unhandled_params [ 0 ] ) ) elif len ( unhandled_params ) > 1 : raise TemplateSyntaxError ( "%s didn't get values for arguments: %s" % ( name , ', ' . join ( [ "'%s'" % p for p in unhandled_params ] ) ) ) return node_class ( args , kwargs , output_var , takes_context )
def get_defining_component ( pe_pe ) : if pe_pe is None : return None if pe_pe . __class__ . __name__ != 'PE_PE' : pe_pe = xtuml . navigate_one ( pe_pe ) . PE_PE [ 8001 ] ( ) ep_pkg = xtuml . navigate_one ( pe_pe ) . EP_PKG [ 8000 ] ( ) if ep_pkg : return get_defining_component ( ep_pkg ) return xtuml . navigate_one ( pe_pe ) . C_C [ 8003 ] ( )
def main ( ) : parser = optparse . OptionParser ( usage = "%prog [options] <model_path> [another_model_path..]" , version = xtuml . version . complete_string , formatter = optparse . TitledHelpFormatter ( ) ) parser . add_option ( "-v" , "--verbosity" , dest = 'verbosity' , action = "count" , help = "increase debug logging level" , default = 1 ) parser . add_option ( "-o" , "--output" , dest = "output" , metavar = "PATH" , help = "set output to PATH" , action = "store" , default = None ) ( opts , args ) = parser . parse_args ( ) if len ( args ) == 0 or opts . output is None : parser . print_help ( ) sys . exit ( 1 ) levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } logging . basicConfig ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) m = ooaofooa . load_metamodel ( args ) prebuild_model ( m ) xtuml . persist_instances ( m , opts . output )
def find_symbol ( self , name = None , kind = None ) : for s in reversed ( self . stack ) : for symbol_name , handle in s . symbols . items ( ) : symbol_kind = handle . __class__ . __name__ if name == symbol_name and kind == symbol_kind : return handle elif name is None and kind == handle . __class__ . __name__ : return handle elif name == symbol_name and kind is None : return handle if name is None and kind == s . handle . __class__ . __name__ : return s . handle
def is_contained_in ( pe_pe , root ) : if not pe_pe : return False if type ( pe_pe ) . __name__ != 'PE_PE' : pe_pe = one ( pe_pe ) . PE_PE [ 8001 ] ( ) ep_pkg = one ( pe_pe ) . EP_PKG [ 8000 ] ( ) c_c = one ( pe_pe ) . C_C [ 8003 ] ( ) if root in [ ep_pkg , c_c ] : return True elif is_contained_in ( ep_pkg , root ) : return True elif is_contained_in ( c_c , root ) : return True else : return False
def is_global ( pe_pe ) : if type ( pe_pe ) . __name__ != 'PE_PE' : pe_pe = one ( pe_pe ) . PE_PE [ 8001 ] ( ) if one ( pe_pe ) . C_C [ 8003 ] ( ) : return False pe_pe = one ( pe_pe ) . EP_PKG [ 8000 ] . PE_PE [ 8001 ] ( ) if not pe_pe : return True return is_global ( pe_pe )
def _get_data_type_name ( s_dt ) : s_cdt = one ( s_dt ) . S_CDT [ 17 ] ( ) if s_cdt and s_cdt . Core_Typ in range ( 1 , 6 ) : return s_dt . Name . upper ( ) if one ( s_dt ) . S_EDT [ 17 ] ( ) : return 'INTEGER' s_dt = one ( s_dt ) . S_UDT [ 17 ] . S_DT [ 18 ] ( ) if s_dt : return _get_data_type_name ( s_dt )
def _get_related_attributes ( r_rgo , r_rto ) : l1 = list ( ) l2 = list ( ) ref_filter = lambda ref : ref . OIR_ID == r_rgo . OIR_ID for o_ref in many ( r_rto ) . O_RTIDA [ 110 ] . O_REF [ 111 ] ( ref_filter ) : o_attr = one ( o_ref ) . O_RATTR [ 108 ] . O_ATTR [ 106 ] ( ) l1 . append ( o_attr . Name ) o_attr = one ( o_ref ) . O_RTIDA [ 111 ] . O_OIDA [ 110 ] . O_ATTR [ 105 ] ( ) l2 . append ( o_attr . Name ) return l1 , l2
def mk_enum ( s_edt ) : s_dt = one ( s_edt ) . S_DT [ 17 ] ( ) enums = list ( ) kwlist = [ 'False' , 'None' , 'True' ] + keyword . kwlist for enum in many ( s_edt ) . S_ENUM [ 27 ] ( ) : if enum . Name in kwlist : enums . append ( enum . Name + '_' ) else : enums . append ( enum . Name ) Enum = collections . namedtuple ( s_dt . Name , enums ) return Enum ( * range ( len ( enums ) ) )
def mk_bridge ( metamodel , s_brg ) : action = s_brg . Action_Semantics_internal label = s_brg . Name return lambda * * kwargs : interpret . run_function ( metamodel , label , action , kwargs )
def mk_function ( metamodel , s_sync ) : action = s_sync . Action_Semantics_internal label = s_sync . Name return lambda * * kwargs : interpret . run_function ( metamodel , label , action , kwargs )
def mk_constant ( cnst_syc ) : s_dt = one ( cnst_syc ) . S_DT [ 1500 ] ( ) cnst_lsc = one ( cnst_syc ) . CNST_LFSC [ 1502 ] . CNST_LSC [ 1503 ] ( ) if s_dt . Name == 'boolean' : return cnst_lsc . Value . lower ( ) == 'true' if s_dt . Name == 'integer' : return int ( cnst_lsc . Value ) if s_dt . Name == 'real' : return float ( cnst_lsc . Value ) if s_dt . Name == 'string' : return str ( cnst_lsc . Value )
def mk_class ( m , o_obj , derived_attributes = False ) : first_filter = lambda selected : not one ( selected ) . O_ATTR [ 103 , 'succeeds' ] ( ) o_attr = one ( o_obj ) . O_ATTR [ 102 ] ( first_filter ) attributes = list ( ) while o_attr : s_dt = get_attribute_type ( o_attr ) ty = _get_data_type_name ( s_dt ) if not derived_attributes and one ( o_attr ) . O_BATTR [ 106 ] . O_DBATTR [ 107 ] ( ) : pass elif not ty : logger . warning ( 'Omitting unsupported attribute %s.%s ' % ( o_obj . Key_Lett , o_attr . Name ) ) else : attributes . append ( ( o_attr . Name , ty ) ) o_attr = one ( o_attr ) . O_ATTR [ 103 , 'precedes' ] ( ) metaclass = m . define_class ( o_obj . Key_Lett , list ( attributes ) , o_obj . Descrip ) for o_id in many ( o_obj ) . O_ID [ 104 ] ( ) : o_oida = many ( o_id ) . O_OIDA [ 105 ] ( ) o_attrs = many ( o_oida ) . O_ATTR [ 105 ] ( ) if not derived_attributes and one ( o_attrs ) . O_BATTR [ 106 ] . O_DBATTR [ 107 ] ( ) : logger . warning ( 'Omitting unique identifier %s.I%d' % ( o_obj . Key_Lett , o_id . Oid_ID + 1 ) ) continue names = [ o_attr . Name for o_attr in o_attrs ] m . define_unique_identifier ( o_obj . Key_Lett , o_id . Oid_ID + 1 , * names ) for o_tfr in many ( o_obj ) . O_TFR [ 115 ] ( ) : fn = mk_operation ( metaclass , o_tfr ) setattr ( metaclass . clazz , o_tfr . Name , fn ) for o_dbattr in many ( o_obj ) . O_ATTR [ 102 ] . O_BATTR [ 106 ] . O_DBATTR [ 107 ] ( ) : o_attr = one ( o_dbattr ) . O_BATTR [ 107 ] . O_ATTR [ 106 ] ( ) fn = mk_derived_attribute ( metaclass , o_dbattr ) setattr ( metaclass . clazz , o_attr . Name , fn ) return metaclass
def mk_simple_association ( m , r_simp ) : r_rel = one ( r_simp ) . R_REL [ 206 ] ( ) r_form = one ( r_simp ) . R_FORM [ 208 ] ( ) r_part = one ( r_simp ) . R_PART [ 207 ] ( ) r_rgo = one ( r_form ) . R_RGO [ 205 ] ( ) r_rto = one ( r_part ) . R_RTO [ 204 ] ( ) if not r_form : logger . info ( 'unformalized association R%s' % ( r_rel . Numb ) ) r_form = one ( r_simp ) . R_PART [ 207 ] ( lambda sel : sel != r_part ) r_rgo = one ( r_form ) . R_RTO [ 204 ] ( ) source_o_obj = one ( r_rgo ) . R_OIR [ 203 ] . O_OBJ [ 201 ] ( ) target_o_obj = one ( r_rto ) . R_OIR [ 203 ] . O_OBJ [ 201 ] ( ) source_ids , target_ids = _get_related_attributes ( r_rgo , r_rto ) if source_o_obj . Obj_ID != target_o_obj . Obj_ID : source_phrase = target_phrase = '' else : source_phrase = r_part . Txt_Phrs target_phrase = r_form . Txt_Phrs m . define_association ( rel_id = r_rel . Numb , source_kind = source_o_obj . Key_Lett , target_kind = target_o_obj . Key_Lett , source_keys = source_ids , target_keys = target_ids , source_conditional = r_form . Cond , target_conditional = r_part . Cond , source_phrase = source_phrase , target_phrase = target_phrase , source_many = r_form . Mult , target_many = r_part . Mult )
def mk_linked_association ( m , r_assoc ) : r_rel = one ( r_assoc ) . R_REL [ 206 ] ( ) r_rgo = one ( r_assoc ) . R_ASSR [ 211 ] . R_RGO [ 205 ] ( ) source_o_obj = one ( r_rgo ) . R_OIR [ 203 ] . O_OBJ [ 201 ] ( ) def _mk_assoc ( side1 , side2 ) : r_rto = one ( side1 ) . R_RTO [ 204 ] ( ) target_o_obj = one ( r_rto ) . R_OIR [ 203 ] . O_OBJ [ 201 ] ( ) source_ids , target_ids = _get_related_attributes ( r_rgo , r_rto ) if side1 . Obj_ID != side2 . Obj_ID : source_phrase = target_phrase = '' else : source_phrase = side1 . Txt_Phrs target_phrase = side2 . Txt_Phrs m . define_association ( rel_id = r_rel . Numb , source_kind = source_o_obj . Key_Lett , target_kind = target_o_obj . Key_Lett , source_keys = source_ids , target_keys = target_ids , source_conditional = side2 . Cond , target_conditional = False , source_phrase = source_phrase , target_phrase = target_phrase , source_many = side2 . Mult , target_many = False ) r_aone = one ( r_assoc ) . R_AONE [ 209 ] ( ) r_aoth = one ( r_assoc ) . R_AOTH [ 210 ] ( ) _mk_assoc ( r_aone , r_aoth ) _mk_assoc ( r_aoth , r_aone )
def mk_association ( m , r_rel ) : handler = { 'R_SIMP' : mk_simple_association , 'R_ASSOC' : mk_linked_association , 'R_SUBSUP' : mk_subsuper_association , 'R_COMP' : mk_derived_association , } inst = subtype ( r_rel , 206 ) fn = handler . get ( type ( inst ) . __name__ ) return fn ( m , inst )
def delete_globals ( m , disconnect = False ) : filt = lambda sel : ( 247728914420827907967735776184937480192 <= sel . DT_ID <= 247728914420827907967735776184937480208 ) for s_dt in m . select_many ( 'S_DT' , filt ) : xtuml . delete ( one ( s_dt ) . PE_PE [ 8001 ] ( ) , disconnect ) xtuml . delete ( subtype ( s_dt , 17 ) , disconnect ) xtuml . delete ( s_dt , disconnect )
def accept ( self , reply_socket , channel ) : info = self . info or b'' self . send_raw ( reply_socket , ACCEPT , info , * channel )
def reject ( self , reply_socket , call_id , topics = ( ) ) : info = self . info or b'' self . send_raw ( reply_socket , REJECT , info , call_id , b'' , topics )
def raise_ ( self , reply_socket , channel , exc_info = None ) : if not reply_socket : return if exc_info is None : exc_info = sys . exc_info ( ) exc_type , exc , tb = exc_info while tb . tb_next is not None : tb = tb . tb_next if issubclass ( exc_type , RemoteException ) : exc_type = exc_type . exc_type filename , lineno = tb . tb_frame . f_code . co_filename , tb . tb_lineno val = ( exc_type , str ( exc ) , filename , lineno ) try : state = exc . __getstate__ ( ) except AttributeError : pass else : val += ( state , ) self . send_reply ( reply_socket , RAISE , val , * channel )
def _call_wait ( self , hints , name , args , kwargs , topics = ( ) , raw = False , limit = None , retry = False , max_retries = None ) : col = self . collector if not col . is_running ( ) : col . start ( ) call_id = uuid4_bytes ( ) reply_to = ( DUPLEX if self . socket is col . socket else col . topic ) header = self . _make_header ( name , call_id , reply_to , hints ) payload = self . _pack ( args , kwargs , raw ) def send_call ( ) : try : safe ( send , self . socket , header , payload , topics , zmq . NOBLOCK ) except zmq . Again : raise Undelivered ( 'emission was not delivered' ) col . prepare ( call_id , self , name , args , kwargs ) send_call ( ) return col . establish ( call_id , self . timeout , limit , send_call if retry else None , max_retries = max_retries )
def dispatch_reply ( self , reply , value ) : method = reply . method call_id = reply . call_id task_id = reply . task_id if method & ACK : try : result_queue = self . result_queues [ call_id ] except KeyError : raise KeyError ( 'already established or unprepared call' ) if method == ACCEPT : worker_info = value result = RemoteResult ( self , call_id , task_id , worker_info ) self . results [ call_id ] [ task_id ] = result result_queue . put_nowait ( result ) elif method == REJECT : result_queue . put_nowait ( None ) else : result = self . results [ call_id ] [ task_id ] result . set_reply ( reply . method , value )
def guess_type_name ( value ) : value = str ( value ) if value . upper ( ) in [ 'TRUE' , 'FALSE' ] : return 'BOOLEAN' elif re . match ( r'(-)?(\d+)(\.\d+)' , value ) : return 'REAL' elif re . match ( r'(-)?(\d+)' , value ) : return 'INTEGER' elif re . match ( r'\'((\'\')|[^\'])*\'' , value ) : return 'STRING' elif re . match ( r'\"([^\\\n]|(\\.))*?\"' , value ) : return 'UNIQUE_ID'
def deserialize_value ( ty , value ) : uty = ty . upper ( ) if uty == 'BOOLEAN' : if value . isdigit ( ) : return bool ( int ( value ) ) elif value . upper ( ) == 'FALSE' : return False elif value . upper ( ) == 'TRUE' : return True else : return None elif uty == 'INTEGER' : if '"' in value : return uuid . UUID ( value [ 1 : - 1 ] ) . int else : return int ( value ) elif uty == 'REAL' : return float ( value ) elif uty == 'STRING' : return value [ 1 : - 1 ] . replace ( "''" , "'" ) elif uty == 'UNIQUE_ID' : if '"' in value : return uuid . UUID ( value [ 1 : - 1 ] ) . int else : return int ( value )
def populate_classes ( self , metamodel ) : for stmt in self . statements : if isinstance ( stmt , CreateClassStmt ) : metamodel . define_class ( stmt . kind , stmt . attributes )
def populate_connections ( self , metamodel ) : storage = dict ( ) for ass in metamodel . associations : source_class = ass . source_link . to_metaclass target_class = ass . target_link . to_metaclass if target_class not in storage : storage [ target_class ] = dict ( ) link_key = frozenset ( ass . source_link . key_map . values ( ) ) if link_key not in storage [ target_class ] : storage [ target_class ] [ link_key ] = dict ( ) for other_inst in target_class . storage : inst_key = ass . source_link . compute_index_key ( other_inst ) if inst_key is None : continue if inst_key not in storage [ target_class ] [ link_key ] : storage [ target_class ] [ link_key ] [ inst_key ] = xtuml . OrderedSet ( ) storage [ target_class ] [ link_key ] [ inst_key ] . add ( other_inst ) for inst in source_class . storage : inst_key = ass . source_link . compute_lookup_key ( inst ) if inst_key is None : continue if inst_key not in storage [ target_class ] [ link_key ] : continue for other_inst in storage [ target_class ] [ link_key ] [ inst_key ] : ass . source_link . connect ( other_inst , inst , check = False ) ass . target_link . connect ( inst , other_inst , check = False ) for inst in metamodel . instances : metaclass = xtuml . get_metaclass ( inst ) for attr in metaclass . referential_attributes : if attr in inst . __dict__ : delattr ( inst , attr )
def populate ( self , metamodel ) : self . populate_classes ( metamodel ) self . populate_unique_identifiers ( metamodel ) self . populate_associations ( metamodel ) self . populate_instances ( metamodel ) self . populate_connections ( metamodel )
def build_metamodel ( self , id_generator = None ) : m = xtuml . MetaModel ( id_generator ) self . populate ( m ) return m
def _source ( self , feature_names ) : if feature_names is None : return True elif isinstance ( feature_names , bool ) : return feature_names else : return map ( lambda n : 'fc.' + n , feature_names )
def _range_filters ( self , * key_ranges ) : filters = [ ] for s , e in key_ranges : if isinstance ( s , basestring ) : s = eid ( s ) if isinstance ( e , basestring ) : e += u'\U0010FFFF' e = eid ( e ) if s == ( ) and e == ( ) : filters . append ( { 'match_all' : { } } ) elif e == ( ) : filters . append ( { 'range' : { '_id' : { 'gte' : s } } } ) elif s == ( ) : filters . append ( { 'range' : { '_id' : { 'lte' : e } } } ) else : filters . append ( { 'range' : { '_id' : { 'gte' : s , 'lte' : e } } } ) if len ( filters ) == 0 : return [ { 'match_all' : { } } ] else : return filters
def _create_mappings ( self ) : self . conn . indices . put_mapping ( index = self . index , doc_type = self . type , timeout = 60 , request_timeout = 60 , body = { self . type : { 'dynamic_templates' : [ { 'default_no_analyze_fc' : { 'match' : 'fc.*' , 'mapping' : { 'index' : 'no' } , } , } ] , '_all' : { 'enabled' : False , } , '_id' : { 'index' : 'not_analyzed' , } , 'properties' : self . _get_index_mappings ( ) , } , } ) # self . conn . cluster . health ( index = self . index , wait_for_status = 'yellow' )
def _get_index_mappings ( self ) : maps = { } for fname in self . indexed_features : config = self . indexes . get ( fname , { } ) print ( fname , config ) maps [ fname_to_idx_name ( fname ) ] = { 'type' : config . get ( 'es_index_type' , 'integer' ) , 'store' : False , 'index' : 'not_analyzed' , } for fname in self . fulltext_indexed_features : maps [ fname_to_full_idx_name ( fname ) ] = { 'type' : 'string' , 'store' : False , 'index' : 'analyzed' , } return maps
def _get_field_types ( self ) : mapping = self . conn . indices . get_mapping ( index = self . index , doc_type = self . type ) return mapping [ self . index ] [ 'mappings' ] [ self . type ] [ 'properties' ]
def _fc_index_disjunction_from_query ( self , query_fc , fname ) : if len ( query_fc . get ( fname , [ ] ) ) == 0 : return [ ] terms = query_fc [ fname ] . keys ( ) disj = [ ] for fname in self . indexes [ fname ] [ 'feature_names' ] : disj . append ( { 'terms' : { fname_to_idx_name ( fname ) : terms } } ) return disj
def fc_bytes ( self , fc_dict ) : num_bytes = 0 for _ , feat in fc_dict . iteritems ( ) : num_bytes += len ( feat ) return num_bytes
def pretty_string ( fc ) : s = [ ] for fname , feature in sorted ( fc . items ( ) ) : if isinstance ( feature , StringCounter ) : feature = [ u'%s: %d' % ( k , v ) for ( k , v ) in feature . most_common ( ) ] feature = u'\n\t' + u'\n\t' . join ( feature ) s . append ( fname + u': ' + feature ) return u'\n' . join ( s )
def process_docopts ( ) : arguments = docopt ( __doc__ , version = "Find Known Secrets {0}" . format ( __version__ ) ) logger . debug ( arguments ) if arguments [ "here" ] : go ( ) else : files = arguments [ "--secrets" ] searcher = Searcher ( source = arguments [ "--source" ] , files = files ) searcher . go ( )
def default_formatter ( error ) : quoted = formencode . htmlfill . escape_formatter ( error ) return u'<span class="error-message">{0}</span>' . format ( quoted )
def pretty_to_link ( inst , link ) : values = '' prefix = '' metaclass = xtuml . get_metaclass ( inst ) for name , ty in metaclass . attributes : if name in link . key_map : value = getattr ( inst , name ) value = xtuml . serialize_value ( value , ty ) name = link . key_map [ name ] values += '%s%s=%s' % ( prefix , name , value ) prefix = ', ' return '%s(%s)' % ( link . kind , values )
def pretty_unique_identifier ( inst , identifier ) : values = '' prefix = '' metaclass = xtuml . get_metaclass ( inst ) for name , ty in metaclass . attributes : if name in metaclass . identifying_attributes : value = getattr ( inst , name ) value = xtuml . serialize_value ( value , ty ) values += '%s%s=%s' % ( prefix , name , value ) prefix = ', ' return '%s(%s)' % ( identifier , values )
def check_uniqueness_constraint ( m , kind = None ) : if kind is None : metaclasses = m . metaclasses . values ( ) else : metaclasses = [ m . find_metaclass ( kind ) ] res = 0 for metaclass in metaclasses : id_map = dict ( ) for identifier in metaclass . indices : id_map [ identifier ] = dict ( ) for inst in metaclass . select_many ( ) : for name , ty in metaclass . attributes : if name not in metaclass . identifying_attributes : continue value = getattr ( inst , name ) isnull = value is None isnull |= ( ty == 'UNIQUE_ID' and not value ) if isnull : res += 1 logger . warning ( '%s.%s is part of an identifier and is null' % ( metaclass . kind , name ) ) for identifier in metaclass . indices : kwargs = dict ( ) for name in metaclass . indices [ identifier ] : kwargs [ name ] = getattr ( inst , name ) index_key = frozenset ( kwargs . items ( ) ) if index_key in id_map [ identifier ] : res += 1 id_string = pretty_unique_identifier ( inst , identifier ) logger . warning ( 'uniqueness constraint violation in %s, %s' % ( metaclass . kind , id_string ) ) id_map [ identifier ] [ index_key ] = inst return res
def check_link_integrity ( m , link ) : res = 0 for inst in link . from_metaclass . select_many ( ) : q_set = list ( link . navigate ( inst ) ) if ( len ( q_set ) < 1 and not link . conditional ) or ( ( len ( q_set ) > 1 and not link . many ) ) : res += 1 logger . warning ( 'integrity violation in ' '%s --(%s)--> %s' % ( pretty_from_link ( inst , link ) , link . rel_id , pretty_to_link ( inst , link ) ) ) return res
def check_subtype_integrity ( m , super_kind , rel_id ) : if isinstance ( rel_id , int ) : rel_id = 'R%d' % rel_id res = 0 for inst in m . select_many ( super_kind ) : if not xtuml . navigate_subtype ( inst , rel_id ) : res += 1 logger . warning ( 'integrity violation across ' '%s[%s]' % ( super_kind , rel_id ) ) return res
def basic_transform ( val ) : if isinstance ( val , int ) : return struct . pack ( '>i' , val ) else : return safe_lower_utf8 ( val )
def get_type_name ( s_dt ) : s_cdt = nav_one ( s_dt ) . S_CDT [ 17 ] ( ) if s_cdt and s_cdt . Core_Typ in range ( 1 , 6 ) : return s_dt . Name s_edt = nav_one ( s_dt ) . S_EDT [ 17 ] ( ) if s_edt : return s_dt . Name s_udt = nav_one ( s_dt ) . S_UDT [ 17 ] ( ) if s_udt : return s_dt . Name
def get_refered_attribute ( o_attr ) : o_attr_ref = nav_one ( o_attr ) . O_RATTR [ 106 ] . O_BATTR [ 113 ] . O_ATTR [ 106 ] ( ) if o_attr_ref : return get_refered_attribute ( o_attr_ref ) else : return o_attr
def build_core_type ( s_cdt ) : s_dt = nav_one ( s_cdt ) . S_DT [ 17 ] ( ) if s_dt . name == 'void' : type_name = None elif s_dt . name == 'boolean' : type_name = 'xs:boolean' elif s_dt . name == 'integer' : type_name = 'xs:integer' elif s_dt . name == 'real' : type_name = 'xs:decimal' elif s_dt . name == 'string' : type_name = 'xs:string' elif s_dt . name == 'unique_id' : type_name = 'xs:integer' else : type_name = None if type_name : mapped_type = ET . Element ( 'xs:simpleType' , name = s_dt . name ) ET . SubElement ( mapped_type , 'xs:restriction' , base = type_name ) return mapped_type
def build_enum_type ( s_edt ) : s_dt = nav_one ( s_edt ) . S_DT [ 17 ] ( ) enum = ET . Element ( 'xs:simpleType' , name = s_dt . name ) enum_list = ET . SubElement ( enum , 'xs:restriction' , base = 'xs:string' ) first_filter = lambda selected : not nav_one ( selected ) . S_ENUM [ 56 , 'succeeds' ] ( ) s_enum = nav_any ( s_edt ) . S_ENUM [ 27 ] ( first_filter ) while s_enum : ET . SubElement ( enum_list , 'xs:enumeration' , value = s_enum . name ) s_enum = nav_one ( s_enum ) . S_ENUM [ 56 , 'precedes' ] ( ) return enum
def build_struct_type ( s_sdt ) : s_dt = nav_one ( s_sdt ) . S_DT [ 17 ] ( ) struct = ET . Element ( 'xs:complexType' , name = s_dt . name ) first_filter = lambda selected : not nav_one ( selected ) . S_MBR [ 46 , 'succeeds' ] ( ) s_mbr = nav_any ( s_sdt ) . S_MBR [ 44 ] ( first_filter ) while s_mbr : s_dt = nav_one ( s_mbr ) . S_DT [ 45 ] ( ) type_name = get_type_name ( s_dt ) ET . SubElement ( struct , 'xs:attribute' , name = s_mbr . name , type = type_name ) s_mbr = nav_one ( s_mbr ) . S_MBR [ 46 , 'precedes' ] ( ) return struct
def build_user_type ( s_udt ) : s_dt_user = nav_one ( s_udt ) . S_DT [ 17 ] ( ) s_dt_base = nav_one ( s_udt ) . S_DT [ 18 ] ( ) base_name = get_type_name ( s_dt_base ) if base_name : user = ET . Element ( 'xs:simpleType' , name = s_dt_user . name ) ET . SubElement ( user , 'xs:restriction' , base = base_name ) return user
def build_type ( s_dt ) : s_cdt = nav_one ( s_dt ) . S_CDT [ 17 ] ( ) if s_cdt : return build_core_type ( s_cdt ) s_edt = nav_one ( s_dt ) . S_EDT [ 17 ] ( ) if s_edt : return build_enum_type ( s_edt ) s_udt = nav_one ( s_dt ) . S_UDT [ 17 ] ( ) if s_udt : return build_user_type ( s_udt )
def build_class ( o_obj ) : cls = ET . Element ( 'xs:element' , name = o_obj . key_lett , minOccurs = '0' , maxOccurs = 'unbounded' ) attributes = ET . SubElement ( cls , 'xs:complexType' ) for o_attr in nav_many ( o_obj ) . O_ATTR [ 102 ] ( ) : o_attr_ref = get_refered_attribute ( o_attr ) s_dt = nav_one ( o_attr_ref ) . S_DT [ 114 ] ( ) while nav_one ( s_dt ) . S_UDT [ 17 ] ( ) : s_dt = nav_one ( s_dt ) . S_UDT [ 17 ] . S_DT [ 18 ] ( ) type_name = get_type_name ( s_dt ) if type_name and not nav_one ( o_attr ) . O_BATTR [ 106 ] . O_DBATTR [ 107 ] ( ) : ET . SubElement ( attributes , 'xs:attribute' , name = o_attr . name , type = type_name ) else : logger . warning ( 'Omitting %s.%s' % ( o_obj . key_lett , o_attr . Name ) ) return cls
def build_component ( m , c_c ) : component = ET . Element ( 'xs:element' , name = c_c . name ) classes = ET . SubElement ( component , 'xs:complexType' ) classes = ET . SubElement ( classes , 'xs:sequence' ) scope_filter = lambda selected : ooaofooa . is_contained_in ( selected , c_c ) for o_obj in m . select_many ( 'O_OBJ' , scope_filter ) : cls = build_class ( o_obj ) classes . append ( cls ) return component
def build_schema ( m , c_c ) : schema = ET . Element ( 'xs:schema' ) schema . set ( 'xmlns:xs' , 'http://www.w3.org/2001/XMLSchema' ) global_filter = lambda selected : ooaofooa . is_global ( selected ) for s_dt in m . select_many ( 'S_DT' , global_filter ) : datatype = build_type ( s_dt ) if datatype is not None : schema . append ( datatype ) scope_filter = lambda selected : ooaofooa . is_contained_in ( selected , c_c ) for s_dt in m . select_many ( 'S_DT' , scope_filter ) : datatype = build_type ( s_dt ) if datatype is not None : schema . append ( datatype ) component = build_component ( m , c_c ) schema . append ( component ) return schema
def prettify ( xml_string ) : reparsed = xml . dom . minidom . parseString ( xml_string ) return reparsed . toprettyxml ( indent = "    " )
def set_positional_info ( node , p ) : node . position = Position ( ) node . position . label = p . lexer . label node . position . start_stream = p . lexpos ( 1 ) node . position . start_line = p . lineno ( 1 ) node . position . start_column = find_column ( p . lexer . lexdata , node . position . start_stream ) _ , node . position . end_stream = p . lexspan ( len ( p ) - 1 ) _ , node . position . end_line = p . linespan ( len ( p ) - 1 ) node . position . end_column = find_column ( p . lexer . lexdata , node . position . end_stream ) - 1 node . character_stream = p . lexer . lexdata [ node . position . start_stream : node . position . end_stream ]
def track_production ( f ) : @ wraps ( f ) def wrapper ( self , p ) : r = f ( self , p ) node = p [ 0 ] if isinstance ( node , Node ) and len ( p ) > 1 : set_positional_info ( node , p ) return r return wrapper
def _create_msg ( self , to , subject , msgHtml , msgPlain , attachments = None ) : sender = self . sender if attachments and isinstance ( attachments , str ) : attachments = [ attachments ] else : attachments = list ( attachments or [ ] ) msg = MIMEMultipart ( 'alternative' ) msg [ 'Subject' ] = subject msg [ 'From' ] = sender msg [ 'To' ] = to msg . attach ( MIMEText ( msgPlain , 'plain' ) ) msg . attach ( MIMEText ( msgHtml , 'html' ) ) for path in attachments : _attachment = self . _prep_attachment ( path ) msg . attach ( _attachment ) raw = base64 . urlsafe_b64encode ( msg . as_bytes ( ) ) . decode ( ) #raw = raw.decode() body = { 'raw' : raw } return body
def read ( self ) : if self . connection . has_changed ( ) : image_path = self . connection . download_image ( ) image = Image . open ( image_path ) self . text_cache = pytesseract . image_to_string ( image ) image . close ( ) return self . text_cache
def main ( ) : parser = optparse . OptionParser ( usage = "%prog [options] <model_path> [another_model_path..]" , version = xtuml . version . complete_string , formatter = optparse . TitledHelpFormatter ( ) ) parser . add_option ( "-v" , "--verbosity" , dest = 'verbosity' , action = "count" , default = 1 , help = "increase debug logging level" ) parser . add_option ( "-f" , "--function" , dest = 'function' , action = "store" , help = "invoke function named NAME" , metavar = 'NAME' ) parser . add_option ( "-c" , "--component" , dest = 'component' , action = "store" , help = "look for the function in a component named NAME" , metavar = 'NAME' , default = None ) ( opts , args ) = parser . parse_args ( ) if len ( args ) == 0 or not opts . function : parser . print_help ( ) sys . exit ( 1 ) levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } logging . basicConfig ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) from bridgepoint import ooaofooa mm = ooaofooa . load_metamodel ( args ) c_c = mm . select_any ( 'C_C' , where ( Name = opts . component ) ) domain = ooaofooa . mk_component ( mm , c_c , derived_attributes = False ) func = domain . find_symbol ( opts . function ) return func ( )
def main ( ) : parser = optparse . OptionParser ( usage = "%prog [options] <model_path> [another_model_path...]" , version = xtuml . version . complete_string , formatter = optparse . TitledHelpFormatter ( ) ) parser . set_description ( __doc__ . strip ( ) ) parser . add_option ( "-c" , "--component" , dest = "component" , metavar = "NAME" , help = "export sql schema for the component named NAME" , action = "store" , default = None ) parser . add_option ( "-d" , "--derived-attributes" , dest = "derived" , help = "include derived attributes in the schema" , action = "store_true" , default = False ) parser . add_option ( "-o" , "--output" , dest = 'output' , metavar = "PATH" , help = "save sql schema to PATH (required)" , action = "store" , default = None ) parser . add_option ( "-v" , "--verbosity" , dest = 'verbosity' , action = "count" , help = "increase debug logging level" , default = 2 ) ( opts , args ) = parser . parse_args ( ) if len ( args ) == 0 or opts . output is None : parser . print_help ( ) sys . exit ( 1 ) levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } logging . basicConfig ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) loader = ooaofooa . Loader ( ) for filename in args : loader . filename_input ( filename ) c = loader . build_component ( opts . component , opts . derived ) xtuml . persist_database ( c , opts . output )
def serialize_value ( value , ty ) : ty = ty . upper ( ) null_value = { 'BOOLEAN' : False , 'INTEGER' : 0 , 'REAL' : 0.0 , 'STRING' : '' , 'UNIQUE_ID' : 0 } transfer_fn = { 'BOOLEAN' : lambda v : '%d' % int ( v ) , 'INTEGER' : lambda v : '%d' % v , 'REAL' : lambda v : '%f' % v , 'STRING' : lambda v : "'%s'" % v . replace ( "'" , "''" ) , 'UNIQUE_ID' : lambda v : '"%s"' % uuid . UUID ( int = v ) } if value is None : value = null_value [ ty ] return transfer_fn [ ty ] ( value )
def serialize_instance ( instance ) : attr_count = 0 metaclass = xtuml . get_metaclass ( instance ) s = 'INSERT INTO %s VALUES (' % metaclass . kind for name , ty in metaclass . attributes : value = getattr ( instance , name ) s += '\n    ' s += serialize_value ( value , ty ) attr_count += 1 if attr_count < len ( metaclass . attributes ) : s += ', -- %s : %s' % ( name , ty ) else : s += ' -- %s : %s' % ( name , ty ) s += '\n);\n' return s
def serialize_instances ( metamodel ) : s = '' for inst in metamodel . instances : s += serialize_instance ( inst ) return s
def serialize_association ( ass ) : s1 = '%s %s (%s)' % ( ass . source_link . cardinality , ass . source_link . to_metaclass . kind , ', ' . join ( ass . source_keys ) ) if ass . target_link . phrase : s1 += " PHRASE '%s'" % ass . target_link . phrase s2 = '%s %s (%s)' % ( ass . target_link . cardinality , ass . target_link . to_metaclass . kind , ', ' . join ( ass . target_keys ) ) if ass . source_link . phrase : s2 += " PHRASE '%s'" % ass . source_link . phrase return 'CREATE ROP REF_ID %s FROM %s TO %s;\n' % ( ass . rel_id , s1 , s2 )
def serialize_class ( Cls ) : metaclass = xtuml . get_metaclass ( Cls ) attributes = [ '%s %s' % ( name , ty . upper ( ) ) for name , ty in metaclass . attributes ] s = 'CREATE TABLE %s (\n    ' % metaclass . kind s += ',\n    ' . join ( attributes ) s += '\n);\n' return s
def serialize_schema ( metamodel ) : s = '' for kind in sorted ( metamodel . metaclasses . keys ( ) ) : s += serialize_class ( metamodel . metaclasses [ kind ] . clazz ) for ass in sorted ( metamodel . associations , key = lambda x : x . rel_id ) : s += serialize_association ( ass ) return s
def serialize ( resource ) : if isinstance ( resource , xtuml . MetaModel ) : return serialize_database ( resource ) elif isinstance ( resource , type ) and issubclass ( resource , xtuml . Class ) : return serialize_class ( resource ) elif isinstance ( resource , xtuml . Association ) : return serialize_association ( resource ) elif isinstance ( resource , xtuml . Class ) : return serialize_instance ( resource )
def main ( ) : parser = ArgumentParser ( description = "search files using n-grams" ) parser . add_argument ( '--path' , dest = 'path' , help = "where to search" , nargs = 1 , action = "store" , default = getcwd ( ) ) parser . add_argument ( '--update' , dest = 'update' , help = "update the index" , action = 'store_true' , default = True ) parser . add_argument ( '--filetype' , dest = 'filetype' , help = "any, images, documents, code, audio, video" , nargs = 1 , action = "store" , default = [ "any" ] ) parser . add_argument ( '--verbose' , dest = 'verbose' , help = "extended output" , action = 'store_true' , default = False ) parser . add_argument ( '--results' , dest = 'results' , help = "number of results to display" , action = "store" , default = 10 ) parser . add_argument ( 'query' , nargs = '+' , help = "what to search" , action = "store" ) args = parser . parse_args ( ) if args . verbose : verbose = 2 pprint ( args ) else : verbose = 0 query = args . query [ 0 ] for arg in args . query [ 1 : ] : query = query + " " + arg slb = min ( [ len ( w ) for w in query . split ( " " ) ] ) files = Files ( path = args . path , filetype = args . filetype [ 0 ] , exclude = [ ] , update = args . update , verbose = verbose ) index = Index ( files , slb = slb , verbose = verbose ) results = index . search ( query , verbose = verbose ) Handler ( results , results_number = int ( args . results ) )
def partition ( condition , collection ) -> Tuple [ List , List ] : succeed , fail = [ ] , [ ] for x in collection : if condition ( x ) : succeed . append ( x ) else : fail . append ( x ) return succeed , fail
def _find_link ( inst1 , inst2 , rel_id , phrase ) : metaclass1 = get_metaclass ( inst1 ) metaclass2 = get_metaclass ( inst2 ) if isinstance ( rel_id , int ) : rel_id = 'R%d' % rel_id for ass in metaclass1 . metamodel . associations : if ass . rel_id != rel_id : continue if ( ass . source_link . from_metaclass . kind == metaclass1 . kind and ass . source_link . to_metaclass . kind == metaclass2 . kind and ass . source_link . phrase == phrase ) : return inst1 , inst2 , ass if ( ass . target_link . from_metaclass . kind == metaclass1 . kind and ass . target_link . to_metaclass . kind == metaclass2 . kind and ass . target_link . phrase == phrase ) : return inst2 , inst1 , ass raise UnknownLinkException ( metaclass1 . kind , metaclass2 . kind , rel_id , phrase )
def get_metaclass ( class_or_instance ) : if isinstance ( class_or_instance , Class ) : return class_or_instance . __metaclass__ elif issubclass ( class_or_instance , Class ) : return class_or_instance . __metaclass__ raise MetaException ( "the provided argument is not an xtuml class or instance" )
def disconnect ( self , instance , another_instance ) : if instance not in self : return False if another_instance not in self [ instance ] : return False self [ instance ] . remove ( another_instance ) return True
def attribute_type ( self , attribute_name ) : attribute_name = attribute_name . upper ( ) for name , ty in self . attributes : if name . upper ( ) == attribute_name : return ty
def add_link ( self , metaclass , rel_id , phrase , conditional , many ) : link = Link ( self , rel_id , metaclass , phrase , conditional , many ) key = ( metaclass . kind . upper ( ) , rel_id , phrase ) self . links [ key ] = link return link
def delete_attribute ( self , name ) : for idx , attr in enumerate ( self . attributes ) : attr_name , _ = attr if attr_name == name : del self . attributes [ idx ] return
def default_value ( self , type_name ) : uname = type_name . upper ( ) if uname == 'BOOLEAN' : return False elif uname == 'INTEGER' : return 0 elif uname == 'REAL' : return 0.0 elif uname == 'STRING' : return '' elif uname == 'UNIQUE_ID' : if self . metamodel : return next ( self . metamodel . id_generator ) else : return None else : raise MetaException ( "Unknown type named '%s'" % type_name )
def new ( self , * args , * * kwargs ) : inst = self . clazz ( ) self . storage . append ( inst ) referential_attributes = dict ( ) for name , ty in self . attributes : if name not in self . referential_attributes : value = self . default_value ( ty ) setattr ( inst , name , value ) for attr , value in zip ( self . attributes , args ) : name , ty = attr if name not in self . referential_attributes : setattr ( inst , name , value ) else : referential_attributes [ name ] = value for name , value in kwargs . items ( ) : if name not in self . referential_attributes : setattr ( inst , name , value ) else : referential_attributes [ name ] = value if not referential_attributes : return inst for link in self . links . values ( ) : if set ( link . key_map . values ( ) ) - set ( referential_attributes . keys ( ) ) : continue kwargs = dict ( ) for key , value in link . key_map . items ( ) : kwargs [ key ] = referential_attributes [ value ] if not kwargs : continue for other_inst in link . to_metaclass . query ( kwargs ) : relate ( other_inst , inst , link . rel_id , link . phrase ) for name , value in referential_attributes . items ( ) : if getattr ( inst , name ) != value : logger . warning ( 'unable to assign %s to %s' , name , inst ) return inst
def instances ( self ) : for metaclass in self . metaclasses . values ( ) : for inst in metaclass . storage : yield inst
def define_class ( self , kind , attributes , doc = '' ) : ukind = kind . upper ( ) if ukind in self . metaclasses : raise MetaModelException ( 'A class with the name %s is already defined' % kind ) metaclass = MetaClass ( kind , self ) for name , ty in attributes : metaclass . append_attribute ( name , ty ) self . metaclasses [ ukind ] = metaclass return metaclass
def find_metaclass ( self , kind ) : ukind = kind . upper ( ) if ukind in self . metaclasses : return self . metaclasses [ ukind ] else : raise UnknownClassException ( kind )
def dead_code ( ) : with safe_cd ( SRC ) : if IS_TRAVIS : command = "{0} vulture {1}" . format ( PYTHON , PROJECT_NAME ) . strip ( ) . split ( ) else : command = "{0} vulture {1}" . format ( PIPENV , PROJECT_NAME ) . strip ( ) . split ( ) output_file_name = "dead_code.txt" with open ( output_file_name , "w" ) as outfile : env = config_pythonpath ( ) subprocess . call ( command , stdout = outfile , env = env ) cutoff = 20 num_lines = sum ( 1 for line in open ( output_file_name ) if line ) if num_lines > cutoff : print ( "Too many lines of dead code : {0}, max {1}" . format ( num_lines , cutoff ) ) exit ( - 1 )
def parse_emails ( values ) : emails = [ ] if isinstance ( values , str ) : values = [ values ] for value in values : matches = re_emails . findall ( value ) emails . extend ( [ match [ 2 ] for match in matches ] ) return emails
def rpc ( f = None , * * kwargs ) : if f is not None : if isinstance ( f , six . string_types ) : if 'name' in kwargs : raise ValueError ( 'name option duplicated' ) kwargs [ 'name' ] = f else : return rpc ( * * kwargs ) ( f ) return functools . partial ( _rpc , * * kwargs )
def rpc_spec_table ( app ) : table = { } for attr , value in inspect . getmembers ( app ) : rpc_spec = get_rpc_spec ( value , default = None ) if rpc_spec is None : continue table [ rpc_spec . name ] = ( value , rpc_spec ) return table
async def normalize_postcode_middleware ( request , handler ) : postcode : Optional [ str ] = request . match_info . get ( 'postcode' , None ) if postcode is None or postcode == "random" : return await handler ( request ) elif not is_uk_postcode ( postcode ) : raise web . HTTPNotFound ( text = "Invalid Postcode" ) postcode_processed = postcode . upper ( ) . replace ( " " , "" ) if postcode_processed == postcode : return await handler ( request ) else : url_name = request . match_info . route . name url = request . app . router [ url_name ] params = dict ( request . match_info ) params [ 'postcode' ] = postcode_processed raise web . HTTPMovedPermanently ( str ( url . url_for ( * * params ) ) )
def next ( self ) : val = self . _current self . _current = self . readfunc ( ) return val
def accept_S_SYS ( self , inst ) : for child in many ( inst ) . EP_PKG [ 1401 ] ( ) : self . accept ( child )
def accept_C_C ( self , inst ) : for child in many ( inst ) . PE_PE [ 8003 ] ( ) : self . accept ( child )
def accept_EP_PKG ( self , inst ) : for child in many ( inst ) . PE_PE [ 8000 ] ( ) : self . accept ( child )
def get_brightness ( self ) : if not self . connection . has_changed ( ) : return self . image_brightness image_path = self . connection . download_image ( ) converted_image = Image . open ( image_path ) . convert ( 'L' ) statistics = ImageStat . Stat ( converted_image ) self . image_brightness = statistics . mean [ 0 ] return self . image_brightness
def _selection_for_character ( self , position ) : selection = QtGui . QTextEdit . ExtraSelection ( ) cursor = self . _text_edit . textCursor ( ) cursor . setPosition ( position ) cursor . movePosition ( QtGui . QTextCursor . NextCharacter , QtGui . QTextCursor . KeepAnchor ) selection . cursor = cursor selection . format = self . format return selection
def _cursor_position_changed ( self ) : self . _text_edit . setExtraSelections ( [ ] ) cursor = self . _text_edit . textCursor ( ) if not cursor . hasSelection ( ) : position = cursor . position ( ) - 1 match_position = self . _find_match ( position ) if match_position != - 1 : extra_selections = [ self . _selection_for_character ( pos ) for pos in ( position , match_position ) ] self . _text_edit . setExtraSelections ( extra_selections )
def _exc_info ( self ) : e = self . exc_info ( ) if sys . platform == 'cli' : if isinstance ( e [ 0 ] , StringException ) : e = ( str ( e [ 0 ] ) , e [ 1 ] , e [ 2 ] ) return e
def run ( self , result ) : log . debug ( "suite %s (%s) run called, tests: %s" , id ( self ) , self , self . _tests ) #import pdb #pdb.set_trace() if self . resultProxy : result , orig = self . resultProxy ( result , self ) , result else : result , orig = result , result try : self . setUp ( ) except KeyboardInterrupt : raise except : self . error_context = 'setup' result . addError ( self , self . _exc_info ( ) ) return try : for test in self . _tests : if result . shouldStop : log . debug ( "stopping" ) break test ( orig ) finally : self . has_run = True try : self . tearDown ( ) except KeyboardInterrupt : raise except : self . error_context = 'teardown' result . addError ( self , self . _exc_info ( ) )
def options ( self , parser , env ) : parser . add_option ( '--collect-only' , action = 'store_true' , dest = self . enableOpt , default = env . get ( 'NOSE_COLLECT_ONLY' ) , help = "Enable collect-only: %s [COLLECT_ONLY]" % ( self . help ( ) ) )
def execute ( self , source = None , hidden = False , interactive = False ) : if not hidden : history = self . input_buffer if source is None else source executed = super ( HistoryConsoleWidget , self ) . execute ( source , hidden , interactive ) if executed and not hidden : history = history . rstrip ( ) if history and ( not self . _history or self . _history [ - 1 ] != history ) : self . _history . append ( history ) self . _history_edits = { } self . _history_index = len ( self . _history ) return executed
def _handle_execute_reply ( self , msg ) : msg_id = msg [ 'parent_header' ] [ 'msg_id' ] info = self . _request_info [ 'execute' ] . pop ( msg_id , None ) if info and info . kind == 'save_magic' and not self . _hidden : content = msg [ 'content' ] status = content [ 'status' ] if status == 'ok' : self . _max_session_history = ( int ( content [ 'user_expressions' ] [ 'hlen' ] ) )
def _history_locked ( self ) : return ( self . history_lock and ( self . _get_edited_history ( self . _history_index ) != self . input_buffer ) and ( self . _get_prompt_cursor ( ) . blockNumber ( ) != self . _get_end_cursor ( ) . blockNumber ( ) ) )
def _get_edited_history ( self , index ) : if index in self . _history_edits : return self . _history_edits [ index ] elif index == len ( self . _history ) : return unicode ( ) return self . _history [ index ]
def _set_history ( self , history ) : self . _history = list ( history ) self . _history_edits = { } self . _history_index = len ( self . _history )
def _store_edits ( self ) : current = self . input_buffer if self . _history_index == len ( self . _history ) or self . _history [ self . _history_index ] != current : self . _history_edits [ self . _history_index ] = current
def OnTimeToClose ( self , evt ) : print ( "See ya later!" ) sys . stdout . flush ( ) self . cleanup_consoles ( evt ) self . Close ( ) sys . exit ( )
def cleanup_files ( self ) : logger . debug ( 'Cleaning up...' ) with indent_log ( ) : for req in self . reqs_to_cleanup : req . remove_temporary_source ( ) if self . _pip_has_created_build_dir ( ) : logger . debug ( 'Removing temporary dir %s...' , self . build_dir ) rmtree ( self . build_dir )
def subscribe ( self ) : self . stream . setsockopt ( zmq . UNSUBSCRIBE , '' ) if '' in self . topics : self . log . debug ( "Subscribing to: everything" ) self . stream . setsockopt ( zmq . SUBSCRIBE , '' ) else : for topic in self . topics : self . log . debug ( "Subscribing to: %r" % ( topic ) ) self . stream . setsockopt ( zmq . SUBSCRIBE , topic )
def log_message ( self , raw ) : if len ( raw ) != 2 or '.' not in raw [ 0 ] : self . log . error ( "Invalid log message: %s" % raw ) return else : topic , msg = raw topic , level_name = topic . rsplit ( '.' , 1 ) level , topic = self . _extract_level ( topic ) if msg [ - 1 ] == '\n' : msg = msg [ : - 1 ] self . log . log ( level , "[%s] %s" % ( topic , msg ) )
def remote_iterator ( view , name ) : view . execute ( 'it%s=iter(%s)' % ( name , name ) , block = True ) while True : try : result = view . apply_sync ( lambda x : x . next ( ) , Reference ( 'it' + name ) ) except RemoteError as e : if e . ename == 'StopIteration' : raise StopIteration else : raise e else : yield result
def StringIO ( * args , * * kw ) : global StringIO try : from cStringIO import StringIO except ImportError : from StringIO import StringIO return StringIO ( * args , * * kw )
def insert_on ( self , path , loc = None ) : loc = loc or self . location if self . project_name == 'setuptools' : try : version = self . version except ValueError : version = '' if '0.7' in version : raise ValueError ( "A 0.7-series setuptools cannot be installed " "with distribute. Found one at %s" % str ( self . location ) ) if not loc : return if path is sys . path : self . check_version_conflict ( ) nloc = _normalize_cached ( loc ) bdir = os . path . dirname ( nloc ) npath = map ( _normalize_cached , path ) bp = None for p , item in enumerate ( npath ) : if item == nloc : break elif item == bdir and self . precedence == EGG_DIST : path . insert ( p , loc ) npath . insert ( p , nloc ) break else : path . append ( loc ) return while 1 : try : np = npath . index ( nloc , p + 1 ) except ValueError : break else : del npath [ np ] , path [ np ] p = np return
def _parsed_pkg_info ( self ) : try : return self . _pkg_info except AttributeError : from email . parser import Parser self . _pkg_info = Parser ( ) . parsestr ( self . get_metadata ( self . PKG_INFO ) ) return self . _pkg_info
def _compute_dependencies ( self ) : from _markerlib import compile as compile_marker dm = self . __dep_map = { None : [ ] } reqs = [ ] for req in self . _parsed_pkg_info . get_all ( 'Requires-Dist' ) or [ ] : distvers , mark = self . _preparse_requirement ( req ) parsed = parse_requirements ( distvers ) . next ( ) parsed . marker_fn = compile_marker ( mark ) reqs . append ( parsed ) def reqs_for_extra ( extra ) : for req in reqs : if req . marker_fn ( override = { 'extra' : extra } ) : yield req common = frozenset ( reqs_for_extra ( None ) ) dm [ None ] . extend ( common ) for extra in self . _parsed_pkg_info . get_all ( 'Provides-Extra' ) or [ ] : extra = safe_extra ( extra . strip ( ) ) dm [ extra ] = list ( frozenset ( reqs_for_extra ( extra ) ) - common ) return dm
def _collapse_leading_ws ( header , txt ) : if header . lower ( ) == 'description' : return '\n' . join ( [ x [ 8 : ] if x . startswith ( ' ' * 8 ) else x for x in txt . strip ( ) . splitlines ( ) ] ) else : return ' ' . join ( [ x . strip ( ) for x in txt . splitlines ( ) ] )
def hideEvent ( self , event ) : super ( CompletionWidget , self ) . hideEvent ( event ) self . _text_edit . cursorPositionChanged . disconnect ( self . _update_current ) self . _text_edit . removeEventFilter ( self )
def showEvent ( self , event ) : super ( CompletionWidget , self ) . showEvent ( event ) self . _text_edit . cursorPositionChanged . connect ( self . _update_current ) self . _text_edit . installEventFilter ( self )
def _complete_current ( self ) : self . _current_text_cursor ( ) . insertText ( self . currentItem ( ) . text ( ) ) self . hide ( )
def _update_current ( self ) : prefix = self . _current_text_cursor ( ) . selection ( ) . toPlainText ( ) if prefix : items = self . findItems ( prefix , ( QtCore . Qt . MatchStartsWith | QtCore . Qt . MatchCaseSensitive ) ) if items : self . setCurrentItem ( items [ 0 ] ) else : self . hide ( ) else : self . hide ( )
def registerAdminSite ( appName , excludeModels = [ ] ) : for model in apps . get_app_config ( appName ) . get_models ( ) : if model not in excludeModels : admin . site . register ( model )
def virtual_memory ( ) : mem = _psutil_mswindows . get_virtual_mem ( ) totphys , availphys , totpagef , availpagef , totvirt , freevirt = mem # total = totphys avail = availphys free = availphys used = total - avail percent = usage_percent ( ( total - avail ) , total , _round = 1 ) return nt_virtmem_info ( total , avail , percent , used , free )
def get_disk_usage ( path ) : try : total , free = _psutil_mswindows . get_disk_usage ( path ) except WindowsError : err = sys . exc_info ( ) [ 1 ] if not os . path . exists ( path ) : raise OSError ( errno . ENOENT , "No such file or directory: '%s'" % path ) raise used = total - free percent = usage_percent ( used , total , _round = 1 ) return nt_diskinfo ( total , used , free , percent )
def disk_partitions ( all ) : rawlist = _psutil_mswindows . get_disk_partitions ( all ) return [ nt_partition ( * x ) for x in rawlist ]
def get_system_cpu_times ( ) : user , system , idle = 0 , 0 , 0 for cpu_time in _psutil_mswindows . get_system_cpu_times ( ) : user += cpu_time [ 0 ] system += cpu_time [ 1 ] idle += cpu_time [ 2 ] return _cputimes_ntuple ( user , system , idle )
def get_system_per_cpu_times ( ) : ret = [ ] for cpu_t in _psutil_mswindows . get_system_cpu_times ( ) : user , system , idle = cpu_t item = _cputimes_ntuple ( user , system , idle ) ret . append ( item ) return ret
def get_system_users ( ) : retlist = [ ] rawlist = _psutil_mswindows . get_system_users ( ) for item in rawlist : user , hostname , tstamp = item nt = nt_user ( user , None , hostname , tstamp ) retlist . append ( nt ) return retlist
def _stdin_raw_nonblock ( self ) : handle = msvcrt . get_osfhandle ( sys . stdin . fileno ( ) ) result = WaitForSingleObject ( handle , 100 ) if result == WAIT_FAILED : raise ctypes . WinError ( ) elif result == WAIT_TIMEOUT : print ( "." , end = '' ) return None else : data = ctypes . create_string_buffer ( 256 ) bytesRead = DWORD ( 0 ) print ( '?' , end = '' ) if not ReadFile ( handle , data , 256 , ctypes . byref ( bytesRead ) , None ) : raise ctypes . WinError ( ) FlushConsoleInputBuffer ( handle ) data = data . value data = data . replace ( '\r\n' , '\n' ) data = data . replace ( '\r' , '\n' ) print ( repr ( data ) + " " , end = '' ) return data
def _stdin_raw_block ( self ) : try : data = sys . stdin . read ( 1 ) data = data . replace ( '\r' , '\n' ) return data except WindowsError as we : if we . winerror == ERROR_NO_DATA : return None else : raise we
def _stdout_raw ( self , s ) : print ( s , end = '' , file = sys . stdout ) sys . stdout . flush ( )
def _stderr_raw ( self , s ) : print ( s , end = '' , file = sys . stderr ) sys . stderr . flush ( )
def create_tab_with_current_kernel ( self ) : current_widget = self . tab_widget . currentWidget ( ) current_widget_index = self . tab_widget . indexOf ( current_widget ) current_widget_name = self . tab_widget . tabText ( current_widget_index ) widget = self . slave_frontend_factory ( current_widget ) if 'slave' in current_widget_name : name = current_widget_name else : name = '(%s) slave' % current_widget_name self . add_tab_with_frontend ( widget , name = name )
def add_tab_with_frontend ( self , frontend , name = None ) : if not name : name = 'kernel %i' % self . next_kernel_id self . tab_widget . addTab ( frontend , name ) self . update_tab_bar_visibility ( ) self . make_frontend_visible ( frontend ) frontend . exit_requested . connect ( self . close_tab )
def closeEvent ( self , event ) : if self . tab_widget . count ( ) == 0 : event . accept ( ) return title = self . window ( ) . windowTitle ( ) cancel = QtGui . QMessageBox . Cancel okay = QtGui . QMessageBox . Ok if self . confirm_exit : if self . tab_widget . count ( ) > 1 : msg = "Close all tabs, stop all kernels, and Quit?" else : msg = "Close console, stop kernel, and Quit?" info = "Kernels not started here (e.g. notebooks) will be left alone." closeall = QtGui . QPushButton ( "&Quit" , self ) closeall . setShortcut ( 'Q' ) box = QtGui . QMessageBox ( QtGui . QMessageBox . Question , title , msg ) box . setInformativeText ( info ) box . addButton ( cancel ) box . addButton ( closeall , QtGui . QMessageBox . YesRole ) box . setDefaultButton ( closeall ) box . setEscapeButton ( cancel ) pixmap = QtGui . QPixmap ( self . _app . icon . pixmap ( QtCore . QSize ( 64 , 64 ) ) ) box . setIconPixmap ( pixmap ) reply = box . exec_ ( ) else : reply = okay if reply == cancel : event . ignore ( ) return if reply == okay : while self . tab_widget . count ( ) >= 1 : widget = self . active_frontend widget . _confirm_exit = False self . close_tab ( widget ) event . accept ( )
def _toggle_boolean ( self , request ) : try : item_id = int ( request . POST . get ( 'item_id' , None ) ) attr = str ( request . POST . get ( 'attr' , None ) ) except : return HttpResponseBadRequest ( "Malformed request" ) if not request . user . is_staff : logging . warning ( "Denied AJAX request by non-staff %s to toggle boolean %s for object #%s" , request . user , attr , item_id ) return HttpResponseForbidden ( "You do not have permission to access this object" ) self . _collect_editable_booleans ( ) if not self . _ajax_editable_booleans . has_key ( attr ) : return HttpResponseBadRequest ( "not a valid attribute %s" % attr ) try : obj = self . model . _default_manager . get ( pk = item_id ) except self . model . DoesNotExist : return HttpResponseNotFound ( "Object does not exist" ) can_change = False if hasattr ( obj , "user_can" ) and obj . user_can ( request . user , change_page = True ) : can_change = True else : can_change = self . has_change_permission ( request , obj = obj ) if not can_change : logging . warning ( "Denied AJAX request by %s to toggle boolean %s for object %s" , request . user , attr , item_id ) return HttpResponseForbidden ( "You do not have permission to access this object" ) logging . info ( "Processing request by %s to toggle %s on %s" , request . user , attr , obj ) try : before_data = self . _ajax_editable_booleans [ attr ] ( self , obj ) setattr ( obj , attr , not getattr ( obj , attr ) ) obj . save ( ) self . _refresh_changelist_caches ( ) data = self . _ajax_editable_booleans [ attr ] ( self , obj ) except Exception : #, e: logging . exception ( "Unhandled exception while toggling %s on %s" , attr , obj ) return HttpResponseServerError ( "Unable to toggle %s on %s" % ( attr , obj ) ) d = [ ] for a , b in zip ( before_data , data ) : if a != b : d . append ( b ) return HttpResponse ( json . dumps ( d ) , mimetype = "application/json" )
def add_children ( G , parent , level , n = 2 ) : if level == 0 : return for i in range ( n ) : child = parent + str ( i ) G . add_node ( child ) G . add_edge ( parent , child ) add_children ( G , child , level - 1 , n )
def make_bintree ( levels ) : G = nx . DiGraph ( ) root = '0' G . add_node ( root ) add_children ( G , root , levels , 2 ) return G
def submit_jobs ( view , G , jobs ) : results = { } for node in nx . topological_sort ( G ) : with view . temp_flags ( after = [ results [ n ] for n in G . predecessors ( node ) ] ) : results [ node ] = view . apply ( jobs [ node ] ) return results
def validate_tree ( G , results ) : for node in G : started = results [ node ] . metadata . started for parent in G . predecessors ( node ) : finished = results [ parent ] . metadata . completed assert started > finished , "%s should have happened after %s" % ( node , parent )
def copy ( self , name = None ) : if name is None : name = self . name return ColorScheme ( name , self . colors . dict ( ) )
def add_scheme ( self , new_scheme ) : if not isinstance ( new_scheme , ColorScheme ) : raise ValueError , 'ColorSchemeTable only accepts ColorScheme instances' self [ new_scheme . name ] = new_scheme
def home_lib ( home ) : if hasattr ( sys , 'pypy_version_info' ) : lib = 'site-packages' else : lib = os . path . join ( 'lib' , 'python' ) return os . path . join ( home , lib )
def handle_stdin_request ( self , timeout = 0.1 ) : msg_rep = self . km . stdin_channel . get_msg ( timeout = timeout ) self . handle_iopub ( ) if self . session_id == msg_rep [ "parent_header" ] . get ( "session" ) : real_handler = signal . getsignal ( signal . SIGINT ) def double_int ( sig , frame ) : real_handler ( sig , frame ) raise KeyboardInterrupt signal . signal ( signal . SIGINT , double_int ) try : raw_data = raw_input ( msg_rep [ "content" ] [ "prompt" ] ) except EOFError : raw_data = '\x04' except KeyboardInterrupt : sys . stdout . write ( '\n' ) return finally : signal . signal ( signal . SIGINT , real_handler ) if not ( self . km . stdin_channel . msg_ready ( ) or self . km . shell_channel . msg_ready ( ) ) : self . km . stdin_channel . input ( raw_data )
def wait_for_kernel ( self , timeout = None ) : tic = time . time ( ) self . km . hb_channel . unpause ( ) while True : self . run_cell ( '1' , False ) if self . km . hb_channel . is_beating ( ) : break else : if timeout is not None and ( time . time ( ) - tic ) > timeout : return False return True
def interact ( self , display_banner = None ) : if self . exit_now : return if display_banner is None : display_banner = self . display_banner if isinstance ( display_banner , basestring ) : self . show_banner ( display_banner ) elif display_banner : self . show_banner ( ) more = False if not self . wait_for_kernel ( 3 ) : error ( "Kernel did not respond\n" ) return if self . has_readline : self . readline_startup_hook ( self . pre_readline ) hlen_b4_cell = self . readline . get_current_history_length ( ) else : hlen_b4_cell = 0 while not self . exit_now : if not self . km . is_alive : action = "restart" if self . km . has_kernel else "wait for restart" ans = self . ask_yes_no ( "kernel died, %s ([y]/n)?" % action , default = 'y' ) if ans : if self . km . has_kernel : self . km . restart_kernel ( True ) self . wait_for_kernel ( 3 ) else : self . exit_now = True continue try : self . hooks . pre_prompt_hook ( ) if more : try : prompt = self . prompt_manager . render ( 'in2' ) except Exception : self . showtraceback ( ) if self . autoindent : self . rl_do_indent = True else : try : prompt = self . separate_in + self . prompt_manager . render ( 'in' ) except Exception : self . showtraceback ( ) line = self . raw_input ( prompt ) if self . exit_now : break if self . autoindent : self . rl_do_indent = False except KeyboardInterrupt : #double-guard against keyboardinterrupts during kbdint handling try : self . write ( '\nKeyboardInterrupt\n' ) source_raw = self . input_splitter . source_raw_reset ( ) [ 1 ] hlen_b4_cell = self . _replace_rlhist_multiline ( source_raw , hlen_b4_cell ) more = False except KeyboardInterrupt : pass except EOFError : if self . autoindent : self . rl_do_indent = False if self . has_readline : self . readline_startup_hook ( None ) self . write ( '\n' ) self . exit ( ) except bdb . BdbQuit : warn ( 'The Python debugger has exited with a BdbQuit exception.\n' 'Because of how pdb handles the stack, it is impossible\n' 'for IPython to properly format this particular exception.\n' 'IPython will resume normal operation.' ) except : self . showtraceback ( ) else : self . input_splitter . push ( line ) more = self . input_splitter . push_accepts_more ( ) if ( self . SyntaxTB . last_syntax_error and self . autoedit_syntax ) : self . edit_syntax_error ( ) if not more : source_raw = self . input_splitter . source_raw_reset ( ) [ 1 ] hlen_b4_cell = self . _replace_rlhist_multiline ( source_raw , hlen_b4_cell ) self . run_cell ( source_raw ) self . exit_now = False
def set_style ( self , style ) : if isinstance ( style , basestring ) : style = get_style_by_name ( style ) self . _style = style self . _clear_caches ( )
def _get_format ( self , token ) : if token in self . _formats : return self . _formats [ token ] if self . _style is None : result = self . _get_format_from_document ( token , self . _document ) else : result = self . _get_format_from_style ( token , self . _style ) self . _formats [ token ] = result return result
def _get_format_from_document ( self , token , document ) : code , html = self . _formatter . _format_lines ( [ ( token , u'dummy' ) ] ) . next ( ) self . _document . setHtml ( html ) return QtGui . QTextCursor ( self . _document ) . charFormat ( )
def _get_format_from_style ( self , token , style ) : result = QtGui . QTextCharFormat ( ) for key , value in style . style_for_token ( token ) . items ( ) : if value : if key == 'color' : result . setForeground ( self . _get_brush ( value ) ) elif key == 'bgcolor' : result . setBackground ( self . _get_brush ( value ) ) elif key == 'bold' : result . setFontWeight ( QtGui . QFont . Bold ) elif key == 'italic' : result . setFontItalic ( True ) elif key == 'underline' : result . setUnderlineStyle ( QtGui . QTextCharFormat . SingleUnderline ) elif key == 'sans' : result . setFontStyleHint ( QtGui . QFont . SansSerif ) elif key == 'roman' : result . setFontStyleHint ( QtGui . QFont . Times ) elif key == 'mono' : result . setFontStyleHint ( QtGui . QFont . TypeWriter ) return result
def find_command ( cmd , paths = None , pathext = None ) : if paths is None : paths = os . environ . get ( 'PATH' , '' ) . split ( os . pathsep ) if isinstance ( paths , six . string_types ) : paths = [ paths ] if pathext is None : pathext = get_pathext ( ) pathext = [ ext for ext in pathext . lower ( ) . split ( os . pathsep ) if len ( ext ) ] if os . path . splitext ( cmd ) [ 1 ] . lower ( ) in pathext : pathext = [ '' ] for path in paths : cmd_path = os . path . join ( path , cmd ) for ext in pathext : cmd_path_ext = cmd_path + ext if os . path . isfile ( cmd_path_ext ) : return cmd_path_ext if os . path . isfile ( cmd_path ) : return cmd_path raise BadCommand ( 'Cannot find command %r' % cmd )
def normalize_path ( path ) : return os . path . normcase ( os . path . realpath ( os . path . expanduser ( path ) ) )
def check_nsp ( dist , attr , value ) : assert_string_list ( dist , attr , value ) for nsp in value : if not dist . has_contents_for ( nsp ) : raise DistutilsSetupError ( "Distribution contains no modules or packages for " + "namespace package %r" % nsp ) if '.' in nsp : parent = '.' . join ( nsp . split ( '.' ) [ : - 1 ] ) if parent not in value : distutils . log . warn ( "%r is declared as a package namespace, but %r is not:" " please correct this in setup.py" , nsp , parent )
def check_extras ( dist , attr , value ) : try : for k , v in value . items ( ) : list ( pkg_resources . parse_requirements ( v ) ) except ( TypeError , ValueError , AttributeError ) : raise DistutilsSetupError ( "'extras_require' must be a dictionary whose values are " "strings or lists of strings containing valid project/version " "requirement specifiers." )
def check_entry_points ( dist , attr , value ) : try : pkg_resources . EntryPoint . parse_map ( value ) except ValueError , e : raise DistutilsSetupError ( e )
def transform_assign_system ( line ) : m = _assign_system_re . match ( line ) if m is not None : cmd = m . group ( 'cmd' ) lhs = m . group ( 'lhs' ) new_line = '%s = get_ipython().getoutput(%r)' % ( lhs , cmd ) return new_line return line
def transform_assign_magic ( line ) : m = _assign_magic_re . match ( line ) if m is not None : cmd = m . group ( 'cmd' ) lhs = m . group ( 'lhs' ) new_line = '%s = get_ipython().magic(%r)' % ( lhs , cmd ) return new_line return line
def transform_ipy_prompt ( line ) : if not line or line . isspace ( ) : return line m = _ipy_prompt_re . match ( line ) if m : return line [ len ( m . group ( 0 ) ) : ] else : return line
def reset ( self ) : self . indent_spaces = 0 self . _buffer [ : ] = [ ] self . source = '' self . code = None self . _is_complete = False self . _full_dedent = False
def reset ( self ) : super ( IPythonInputSplitter , self ) . reset ( ) self . _buffer_raw [ : ] = [ ] self . source_raw = '' self . cell_magic_parts = [ ] self . processing_cell_magic = False
def source_raw_reset ( self ) : out = self . source out_r = self . source_raw self . reset ( ) return out , out_r
def _handle_cell_magic ( self , lines ) : self . processing_cell_magic = True first , _ , body = lines . partition ( '\n' ) magic_name , _ , line = first . partition ( ' ' ) magic_name = magic_name . lstrip ( ESC_MAGIC ) self . cell_magic_parts = [ body ] tpl = 'get_ipython()._run_cached_cell_magic(%r, %r)' tlines = tpl % ( magic_name , line ) self . _store ( tlines ) self . _store ( lines , self . _buffer_raw , 'source_raw' ) #self._is_complete = last_two_blanks(lines) self . _is_complete = last_blank ( lines ) return self . _is_complete
def _line_mode_cell_append ( self , lines ) : self . _store ( lines , self . _buffer_raw , 'source_raw' ) self . cell_magic_parts . append ( lines ) last_block = self . cell_magic_parts [ - 1 ] self . _is_complete = last_blank ( last_block ) and lines . isspace ( ) return self . _is_complete
def transform_cell ( self , cell ) : self . reset ( ) self . push ( cell ) return self . source_reset ( )
def _observers_for_notification ( self , ntype , sender ) : keys = ( ( ntype , sender ) , ( ntype , None ) , ( None , sender ) , ( None , None ) ) obs = set ( ) for k in keys : obs . update ( self . observers . get ( k , set ( ) ) ) return obs
def status ( self , verbose = 0 ) : self . _update_status ( ) self . _group_report ( self . running , 'Running' ) self . _group_report ( self . completed , 'Completed' ) self . _group_report ( self . dead , 'Dead' ) self . _comp_report [ : ] = [ ] self . _dead_report [ : ] = [ ]
def _init ( self ) : for attr in [ 'call' , 'strform' ] : assert hasattr ( self , attr ) , "Missing attribute <%s>" % attr self . num = None self . status = BackgroundJobBase . stat_created self . stat_code = BackgroundJobBase . stat_created_c self . finished = False self . result = '<BackgroundJob has not completed>' try : make_tb = get_ipython ( ) . InteractiveTB . text except : make_tb = AutoFormattedTB ( mode = 'Context' , color_scheme = 'NoColor' , tb_offset = 1 ) . text self . _make_tb = lambda : make_tb ( None , None , None ) self . _tb = None threading . Thread . __init__ ( self )
def energy ( self , state = None ) : state = self . state if state is None else state route = state e = 0 if self . distance_matrix : for i in range ( len ( route ) ) : e += self . distance_matrix [ "{},{}" . format ( route [ i - 1 ] , route [ i ] ) ] else : for i in range ( len ( route ) ) : e += distance ( self . cities [ route [ i - 1 ] ] , self . cities [ route [ i ] ] ) return e
def _defaults ( self , keys = None ) : d = { } keys = self . _keys if keys is None else keys for key in keys : d [ key ] = None return d
def _init_db ( self ) : sqlite3 . register_adapter ( dict , _adapt_dict ) sqlite3 . register_converter ( 'dict' , _convert_dict ) sqlite3 . register_adapter ( list , _adapt_bufs ) sqlite3 . register_converter ( 'bufs' , _convert_bufs ) dbfile = os . path . join ( self . location , self . filename ) self . _db = sqlite3 . connect ( dbfile , detect_types = sqlite3 . PARSE_DECLTYPES , cached_statements = 64 ) first_table = previous_table = self . table i = 0 while not self . _check_table ( ) : i += 1 self . table = first_table + '_%i' % i self . log . warn ( "Table %s exists and doesn't match db format, trying %s" % ( previous_table , self . table ) ) previous_table = self . table self . _db . execute ( % self . table ) self . _db . commit ( )
def _render_expression ( self , check ) : expressions = [ ] args = [ ] skeys = set ( check . keys ( ) ) skeys . difference_update ( set ( self . _keys ) ) skeys . difference_update ( set ( [ 'buffers' , 'result_buffers' ] ) ) if skeys : raise KeyError ( "Illegal testing key(s): %s" % skeys ) for name , sub_check in check . iteritems ( ) : if isinstance ( sub_check , dict ) : for test , value in sub_check . iteritems ( ) : try : op = operators [ test ] except KeyError : raise KeyError ( "Unsupported operator: %r" % test ) if isinstance ( op , tuple ) : op , join = op if value is None and op in null_operators : expr = "%s %s" % ( name , null_operators [ op ] ) else : expr = "%s %s ?" % ( name , op ) if isinstance ( value , ( tuple , list ) ) : if op in null_operators and any ( [ v is None for v in value ] ) : raise ValueError ( "Cannot use %r test with NULL values on SQLite backend" % test ) expr = '( %s )' % ( join . join ( [ expr ] * len ( value ) ) ) args . extend ( value ) else : args . append ( value ) expressions . append ( expr ) else : if sub_check is None : expressions . append ( "%s IS NULL" % name ) else : expressions . append ( "%s = ?" % name ) args . append ( sub_check ) expr = " AND " . join ( expressions ) return expr , args
def add_record ( self , msg_id , rec ) : d = self . _defaults ( ) d . update ( rec ) d [ 'msg_id' ] = msg_id line = self . _dict_to_list ( d ) tups = '(%s)' % ( ',' . join ( [ '?' ] * len ( line ) ) ) self . _db . execute ( "INSERT INTO %s VALUES %s" % ( self . table , tups ) , line )
def get_record ( self , msg_id ) : cursor = self . _db . execute ( """SELECT * FROM %s WHERE msg_id==?""" % self . table , ( msg_id , ) ) line = cursor . fetchone ( ) if line is None : raise KeyError ( "No such msg: %r" % msg_id ) return self . _list_to_dict ( line )
def update_record ( self , msg_id , rec ) : query = "UPDATE %s SET " % self . table sets = [ ] keys = sorted ( rec . keys ( ) ) values = [ ] for key in keys : sets . append ( '%s = ?' % key ) values . append ( rec [ key ] ) query += ', ' . join ( sets ) query += ' WHERE msg_id == ?' values . append ( msg_id ) self . _db . execute ( query , values )
def drop_matching_records ( self , check ) : expr , args = self . _render_expression ( check ) query = "DELETE FROM %s WHERE %s" % ( self . table , expr ) self . _db . execute ( query , args )
def get_history ( self ) : query = """SELECT msg_id FROM %s ORDER by submitted ASC""" % self . table cursor = self . _db . execute ( query ) return [ tup [ 0 ] for tup in cursor . fetchall ( ) ]
def table ( rows ) : output = '<table>' for row in rows : output += '<tr>' for column in row : output += '<td>{s}</td>' . format ( s = column ) output += '</tr>' output += '</table>' return output
def link ( url , text = '' , classes = '' , target = '' , get = "" , * * kwargs ) : if not ( url . startswith ( 'http' ) or url . startswith ( '/' ) ) : urlargs = { } for arg , val in kwargs . items ( ) : if arg [ : 4 ] == "url_" : urlargs [ arg [ 4 : ] ] = val url = reverse ( url , kwargs = urlargs ) if get : url += '?' + get return html . tag ( 'a' , text or url , { 'class' : classes , 'target' : target , 'href' : url } )
def jsfile ( url ) : if not url . startswith ( 'http://' ) and not url [ : 1 ] == '/' : #add media_url for relative paths url = settings . STATIC_URL + url return '<script type="text/javascript" src="{src}"></script>' . format ( src = url )
def cssfile ( url ) : if not url . startswith ( 'http://' ) and not url [ : 1 ] == '/' : #add media_url for relative paths url = settings . STATIC_URL + url return '<link href="{src}" rel="stylesheet">' . format ( src = url )
def img ( url , alt = '' , classes = '' , style = '' ) : if not url . startswith ( 'http://' ) and not url [ : 1 ] == '/' : #add media_url for relative paths url = settings . STATIC_URL + url attr = { 'class' : classes , 'alt' : alt , 'style' : style , 'src' : url } return html . tag ( 'img' , '' , attr )
def sub ( value , arg ) : try : return valid_numeric ( value ) - valid_numeric ( arg ) except ( ValueError , TypeError ) : try : return value - arg except Exception : return ''
def mul ( value , arg ) : try : return valid_numeric ( value ) * valid_numeric ( arg ) except ( ValueError , TypeError ) : try : return value * arg except Exception : return ''
def div ( value , arg ) : try : return valid_numeric ( value ) / valid_numeric ( arg ) except ( ValueError , TypeError ) : try : return value / arg except Exception : return ''
def mod ( value , arg ) : try : return valid_numeric ( value ) % valid_numeric ( arg ) except ( ValueError , TypeError ) : try : return value % arg except Exception : return ''
def options ( self , parser , env ) : parser . add_option ( "--processes" , action = "store" , default = env . get ( 'NOSE_PROCESSES' , 0 ) , dest = "multiprocess_workers" , metavar = "NUM" , help = "Spread test run among this many processes. " "Set a number equal to the number of processors " "or cores in your machine for best results. " "[NOSE_PROCESSES]" ) parser . add_option ( "--process-timeout" , action = "store" , default = env . get ( 'NOSE_PROCESS_TIMEOUT' , 10 ) , dest = "multiprocess_timeout" , metavar = "SECONDS" , help = "Set timeout for return of results from each " "test runner process. [NOSE_PROCESS_TIMEOUT]" ) parser . add_option ( "--process-restartworker" , action = "store_true" , default = env . get ( 'NOSE_PROCESS_RESTARTWORKER' , False ) , dest = "multiprocess_restartworker" , help = "If set, will restart each worker process once" " their tests are done, this helps control memory " "leaks from killing the system. " "[NOSE_PROCESS_RESTARTWORKER]" )
def run ( self , result ) : log . debug ( "suite %s (%s) run called, tests: %s" , id ( self ) , self , self . _tests ) if self . resultProxy : result , orig = self . resultProxy ( result , self ) , result else : result , orig = result , result try : #log.debug('setUp for %s', id(self)); self . setUp ( ) except KeyboardInterrupt : raise except : self . error_context = 'setup' result . addError ( self , self . _exc_info ( ) ) return try : for test in self . _tests : if ( isinstance ( test , nose . case . Test ) and self . arg is not None ) : test . test . arg = self . arg else : test . arg = self . arg test . testQueue = self . testQueue test . tasks = self . tasks if result . shouldStop : log . debug ( "stopping" ) break #log.debug('running test %s in suite %s', test, self); try : test ( orig ) except KeyboardInterrupt , e : timeout = isinstance ( e , TimedOutException ) if timeout : msg = 'Timeout when running test %s in suite %s' else : msg = 'KeyboardInterrupt when running test %s in suite %s' log . debug ( msg , test , self ) err = ( TimedOutException , TimedOutException ( str ( test ) ) , sys . exc_info ( ) [ 2 ] ) test . config . plugins . addError ( test , err ) orig . addError ( test , err ) if not timeout : raise finally : self . has_run = True try : #log.debug('tearDown for %s', id(self)); self . tearDown ( ) except KeyboardInterrupt : raise except : self . error_context = 'teardown' result . addError ( self , self . _exc_info ( ) )
def add_builtin ( self , key , value ) : bdict = __builtin__ . __dict__ orig = bdict . get ( key , BuiltinUndefined ) if value is HideBuiltin : if orig is not BuiltinUndefined : #same as 'key in bdict' self . _orig_builtins [ key ] = orig del bdict [ key ] else : self . _orig_builtins [ key ] = orig bdict [ key ] = value
def remove_builtin ( self , key , orig ) : if orig is BuiltinUndefined : del __builtin__ . __dict__ [ key ] else : __builtin__ . __dict__ [ key ] = orig
def activate ( self ) : add_builtin = self . add_builtin for name , func in self . auto_builtins . iteritems ( ) : add_builtin ( name , func )
def explicit_rel_links ( self , rels = ( 'homepage' , 'download' ) ) : rels = set ( rels ) for anchor in self . parsed . findall ( ".//a" ) : if anchor . get ( "rel" ) and anchor . get ( "href" ) : found_rels = set ( anchor . get ( "rel" ) . split ( ) ) if found_rels & rels : href = anchor . get ( "href" ) url = self . clean_link ( urllib_parse . urljoin ( self . base_url , href ) ) yield Link ( url , self , trusted = False )
def unshell_list ( s ) : if not s : return None if sys . platform == 'win32' : s = s . strip ( "'" ) return s . split ( ',' )
def add_action ( self , dash , dashdash , action_code ) : option = self . add_option ( dash , dashdash , action = 'callback' , callback = self . _append_action ) option . action_code = action_code
def _append_action ( self , option , opt_unused , value_unused , parser ) : parser . values . actions . append ( option . action_code )
def help ( self , error = None , topic = None , parser = None ) : assert error or topic or parser if error : print ( error ) print ( "Use 'coverage help' for help." ) elif parser : print ( parser . format_help ( ) . strip ( ) ) else : help_msg = HELP_TOPICS . get ( topic , '' ) . strip ( ) if help_msg : print ( help_msg % self . covpkg . __dict__ ) else : print ( "Don't know topic %r" % topic )
def do_execute ( self , options , args ) : old_path0 = sys . path [ 0 ] self . coverage . start ( ) code_ran = True try : try : if options . module : sys . path [ 0 ] = '' self . run_python_module ( args [ 0 ] , args ) else : filename = args [ 0 ] sys . path [ 0 ] = os . path . abspath ( os . path . dirname ( filename ) ) self . run_python_file ( filename , args ) except NoSource : code_ran = False raise finally : self . coverage . stop ( ) if code_ran : self . coverage . save ( ) sys . path [ 0 ] = old_path0
def do_debug ( self , args ) : if not args : self . help_fn ( "What information would you like: data, sys?" ) return ERR for info in args : if info == 'sys' : print ( "-- sys ----------------------------------------" ) for line in info_formatter ( self . coverage . sysinfo ( ) ) : print ( " %s" % line ) elif info == 'data' : print ( "-- data ---------------------------------------" ) self . coverage . load ( ) print ( "path: %s" % self . coverage . data . filename ) print ( "has_arcs: %r" % self . coverage . data . has_arcs ( ) ) summary = self . coverage . data . summary ( fullpath = True ) if summary : filenames = sorted ( summary . keys ( ) ) print ( "\n%d files:" % len ( filenames ) ) for f in filenames : print ( "%s: %d lines" % ( f , summary [ f ] ) ) else : print ( "No data collected" ) else : self . help_fn ( "Don't know what you mean by %r" % info ) return ERR return OK
def unserialize_object ( bufs ) : bufs = list ( bufs ) sobj = pickle . loads ( bufs . pop ( 0 ) ) if isinstance ( sobj , ( list , tuple ) ) : for s in sobj : if s . data is None : s . data = bufs . pop ( 0 ) return uncanSequence ( map ( unserialize , sobj ) ) , bufs elif isinstance ( sobj , dict ) : newobj = { } for k in sorted ( sobj . iterkeys ( ) ) : s = sobj [ k ] if s . data is None : s . data = bufs . pop ( 0 ) newobj [ k ] = uncan ( unserialize ( s ) ) return newobj , bufs else : if sobj . data is None : sobj . data = bufs . pop ( 0 ) return uncan ( unserialize ( sobj ) ) , bufs
def set ( self ) : if sys . displayhook is not self . hook : self . old_hook = sys . displayhook sys . displayhook = self . hook
def is_url ( url ) : if '://' not in url : return False proto , addr = url . split ( '://' , 1 ) if proto . lower ( ) not in [ 'tcp' , 'pgm' , 'epgm' , 'ipc' , 'inproc' ] : return False return True
def validate_url ( url ) : if not isinstance ( url , basestring ) : raise TypeError ( "url must be a string, not %r" % type ( url ) ) url = url . lower ( ) proto_addr = url . split ( '://' ) assert len ( proto_addr ) == 2 , 'Invalid url: %r' % url proto , addr = proto_addr assert proto in [ 'tcp' , 'pgm' , 'epgm' , 'ipc' , 'inproc' ] , "Invalid protocol: %r" % proto pat = re . compile ( r'^([\w\d]([\w\d\-]{0,61}[\w\d])?\.)*[\w\d]([\w\d\-]{0,61}[\w\d])?$' ) if proto == 'tcp' : lis = addr . split ( ':' ) assert len ( lis ) == 2 , 'Invalid url: %r' % url addr , s_port = lis try : port = int ( s_port ) except ValueError : raise AssertionError ( "Invalid port %r in url: %r" % ( port , url ) ) assert addr == '*' or pat . match ( addr ) is not None , 'Invalid url: %r' % url else : pass return True
def validate_url_container ( container ) : if isinstance ( container , basestring ) : url = container return validate_url ( url ) elif isinstance ( container , dict ) : container = container . itervalues ( ) for element in container : validate_url_container ( element )
def _pull ( keys ) : user_ns = globals ( ) if isinstance ( keys , ( list , tuple , set ) ) : for key in keys : if not user_ns . has_key ( key ) : raise NameError ( "name '%s' is not defined" % key ) return map ( user_ns . get , keys ) else : if not user_ns . has_key ( keys ) : raise NameError ( "name '%s' is not defined" % keys ) return user_ns . get ( keys )
def select_random_ports ( n ) : ports = [ ] for i in xrange ( n ) : sock = socket . socket ( ) sock . bind ( ( '' , 0 ) ) while sock . getsockname ( ) [ 1 ] in _random_ports : sock . close ( ) sock = socket . socket ( ) sock . bind ( ( '' , 0 ) ) ports . append ( sock ) for i , sock in enumerate ( ports ) : port = sock . getsockname ( ) [ 1 ] sock . close ( ) ports [ i ] = port _random_ports . add ( port ) return ports
def get_readline_tail ( self , n = 10 ) : end = self . shell . readline . get_current_history_length ( ) + 1 start = max ( end - n , 1 ) ghi = self . shell . readline . get_history_item return [ ghi ( x ) for x in range ( start , end ) ]
def init_logstart ( self ) : if self . logappend : self . magic ( 'logstart %s append' % self . logappend ) elif self . logfile : self . magic ( 'logstart %s' % self . logfile ) elif self . logstart : self . magic ( 'logstart' )
def restore_sys_module_state ( self ) : try : for k , v in self . _orig_sys_module_state . iteritems ( ) : setattr ( sys , k , v ) except AttributeError : pass if self . _orig_sys_modules_main_mod is not None : sys . modules [ self . _orig_sys_modules_main_name ] = self . _orig_sys_modules_main_mod
def register_post_execute ( self , func ) : if not callable ( func ) : raise ValueError ( 'argument %s must be callable' % func ) self . _post_execute [ func ] = True
def new_main_mod ( self , ns = None ) : main_mod = self . _user_main_module init_fakemod_dict ( main_mod , ns ) return main_mod
def _ofind_property ( self , oname , info ) : if info . found : path = oname . split ( '.' ) root = '.' . join ( path [ : - 1 ] ) if info . parent is not None : try : target = getattr ( info . parent , '__class__' ) try : target = getattr ( target , path [ - 1 ] ) if isinstance ( target , property ) : oname = root + '.__class__.' + path [ - 1 ] info = Struct ( self . _ofind ( oname ) ) except AttributeError : pass except AttributeError : pass return info
def _object_find ( self , oname , namespaces = None ) : inf = Struct ( self . _ofind ( oname , namespaces ) ) return Struct ( self . _ofind_property ( oname , inf ) )
def init_history ( self ) : self . history_manager = HistoryManager ( shell = self , config = self . config ) self . configurables . append ( self . history_manager )
def set_completer_frame ( self , frame = None ) : if frame : self . Completer . namespace = frame . f_locals self . Completer . global_namespace = frame . f_globals else : self . Completer . namespace = self . user_ns self . Completer . global_namespace = self . user_global_ns
def ex ( self , cmd ) : with self . builtin_trap : exec cmd in self . user_global_ns , self . user_ns
def _run_cached_cell_magic ( self , magic_name , line ) : cell = self . _current_cell_magic_body self . _current_cell_magic_body = None return self . run_cell_magic ( magic_name , line , cell )
def broadcast ( client , sender , msg_name , dest_name = None , block = None ) : dest_name = msg_name if dest_name is None else dest_name client [ sender ] . execute ( 'com.publish(%s)' % msg_name , block = None ) targets = client . ids targets . remove ( sender ) return client [ targets ] . execute ( '%s=com.consume()' % dest_name , block = None )
def send ( client , sender , targets , msg_name , dest_name = None , block = None ) : dest_name = msg_name if dest_name is None else dest_name def _send ( targets , m_name ) : msg = globals ( ) [ m_name ] return com . send ( targets , msg ) client [ sender ] . apply_async ( _send , targets , msg_name ) return client [ targets ] . execute ( '%s=com.recv()' % dest_name , block = None )
def list_profiles_in ( path ) : files = os . listdir ( path ) profiles = [ ] for f in files : full_path = os . path . join ( path , f ) if os . path . isdir ( full_path ) and f . startswith ( 'profile_' ) : profiles . append ( f . split ( '_' , 1 ) [ - 1 ] ) return profiles
def list_bundled_profiles ( ) : path = os . path . join ( get_ipython_package_dir ( ) , u'config' , u'profile' ) files = os . listdir ( path ) profiles = [ ] for profile in files : full_path = os . path . join ( path , profile ) if os . path . isdir ( full_path ) and profile != "__pycache__" : profiles . append ( profile ) return profiles
def next ( self ) : result = self . readline ( ) if result == self . _empty_buffer : raise StopIteration return result
def _prepare_regex_pattern ( self , p ) : if isinstance ( p . pattern , unicode ) : p = re . compile ( p . pattern . encode ( 'utf-8' ) , p . flags & ~ re . UNICODE ) return p
def _prepare_regex_pattern ( self , p ) : if isinstance ( p . pattern , bytes ) : p = re . compile ( p . pattern . decode ( self . encoding ) , p . flags ) return p
def finish_displayhook ( self ) : sys . stdout . flush ( ) sys . stderr . flush ( ) self . session . send ( self . pub_socket , self . msg , ident = self . topic ) self . msg = None
def log_listener ( log : logging . Logger = None , level = logging . INFO ) : if log is None : log = logging . getLogger ( "ProgressMonitor" ) def listen ( monitor ) : name = "{}: " . format ( monitor . name ) if monitor . name is not None else "" perc = int ( monitor . progress * 100 ) msg = "[{name}{perc:3d}%] {monitor.message}" . format ( * * locals ( ) ) log . log ( level , msg ) return listen
def last_error ( self ) : if not len ( self . log ) : raise RuntimeError ( 'Nothing executed' ) try : errs = [ l for l in self . log if l [ 1 ] != 0 ] return errs [ - 1 ] [ 2 ] except IndexError : #TODO return 'no last error'
def check_output ( self , cmd ) : ret , output = self . _exec ( cmd ) if not ret == 0 : raise CommandError ( self ) return output
def arcs_executed ( self ) : executed = self . coverage . data . executed_arcs ( self . filename ) m2fl = self . parser . first_line executed = [ ( m2fl ( l1 ) , m2fl ( l2 ) ) for ( l1 , l2 ) in executed ] return sorted ( executed )
def arcs_missing ( self ) : possible = self . arc_possibilities ( ) executed = self . arcs_executed ( ) missing = [ p for p in possible if p not in executed and p [ 0 ] not in self . no_branch ] return sorted ( missing )
def arcs_unpredicted ( self ) : possible = self . arc_possibilities ( ) executed = self . arcs_executed ( ) unpredicted = [ e for e in executed if e not in possible and e [ 0 ] != e [ 1 ] ] return sorted ( unpredicted )
def branch_lines ( self ) : exit_counts = self . parser . exit_counts ( ) return [ l1 for l1 , count in iitems ( exit_counts ) if count > 1 ]
def total_branches ( self ) : exit_counts = self . parser . exit_counts ( ) return sum ( [ count for count in exit_counts . values ( ) if count > 1 ] )
def set_precision ( cls , precision ) : assert 0 <= precision < 10 cls . _precision = precision cls . _near0 = 1.0 / 10 ** precision cls . _near100 = 100.0 - cls . _near0
def _get_pc_covered ( self ) : if self . n_statements > 0 : pc_cov = ( 100.0 * ( self . n_executed + self . n_executed_branches ) / ( self . n_statements + self . n_branches ) ) else : pc_cov = 100.0 return pc_cov
def highlight_text ( needles , haystack , cls_name = 'highlighted' , words = False , case = False ) : if not needles : return haystack if not haystack : return '' if words : pattern = r"(%s)" % "|" . join ( [ '\\b{}\\b' . format ( re . escape ( n ) ) for n in needles ] ) else : pattern = r"(%s)" % "|" . join ( [ re . escape ( n ) for n in needles ] ) if case : regex = re . compile ( pattern ) else : regex = re . compile ( pattern , re . I ) i , out = 0 , "" for m in regex . finditer ( haystack ) : out += "" . join ( [ haystack [ i : m . start ( ) ] , '<span class="%s">' % cls_name , haystack [ m . start ( ) : m . end ( ) ] , "</span>" ] ) i = m . end ( ) return mark_safe ( out + haystack [ i : ] )
def highlight ( string , keywords , cls_name = 'highlighted' ) : if not keywords : return string if not string : return '' include , exclude = get_text_tokenizer ( keywords ) highlighted = highlight_text ( include , string , cls_name ) return highlighted
def highlight_words ( string , keywords , cls_name = 'highlighted' ) : if not keywords : return string if not string : return '' include , exclude = get_text_tokenizer ( keywords ) highlighted = highlight_text ( include , string , cls_name , words = True ) return highlighted
def run_setup ( setup_script , args ) : old_dir = os . getcwd ( ) save_argv = sys . argv [ : ] save_path = sys . path [ : ] setup_dir = os . path . abspath ( os . path . dirname ( setup_script ) ) temp_dir = os . path . join ( setup_dir , 'temp' ) if not os . path . isdir ( temp_dir ) : os . makedirs ( temp_dir ) save_tmp = tempfile . tempdir save_modules = sys . modules . copy ( ) pr_state = pkg_resources . __getstate__ ( ) try : tempfile . tempdir = temp_dir os . chdir ( setup_dir ) try : sys . argv [ : ] = [ setup_script ] + list ( args ) sys . path . insert ( 0 , setup_dir ) DirectorySandbox ( setup_dir ) . run ( lambda : execfile ( "setup.py" , { '__file__' : setup_script , '__name__' : '__main__' } ) ) except SystemExit , v : if v . args and v . args [ 0 ] : raise finally : pkg_resources . __setstate__ ( pr_state ) sys . modules . update ( save_modules ) del_modules = [ mod_name for mod_name in sys . modules if mod_name not in save_modules and not mod_name . startswith ( 'encodings.' ) ] map ( sys . modules . __delitem__ , del_modules ) os . chdir ( old_dir ) sys . path [ : ] = save_path sys . argv [ : ] = save_argv tempfile . tempdir = save_tmp
def run ( self , func ) : try : self . _copy ( self ) if _file : __builtin__ . file = self . _file __builtin__ . open = self . _open self . _active = True return func ( ) finally : self . _active = False if _file : __builtin__ . file = _file __builtin__ . open = _open self . _copy ( _os )
def unquote_ends ( istr ) : if not istr : return istr if ( istr [ 0 ] == "'" and istr [ - 1 ] == "'" ) or ( istr [ 0 ] == '"' and istr [ - 1 ] == '"' ) : return istr [ 1 : - 1 ] else : return istr
def _find_optimal ( rlist , separator_size = 2 , displaywidth = 80 ) : for nrow in range ( 1 , len ( rlist ) + 1 ) : chk = map ( max , _chunks ( rlist , nrow ) ) sumlength = sum ( chk ) ncols = len ( chk ) if sumlength + separator_size * ( ncols - 1 ) <= displaywidth : break return { 'columns_numbers' : ncols , 'optimal_separator_width' : ( displaywidth - sumlength ) / ( ncols - 1 ) if ( ncols - 1 ) else 0 , 'rows_numbers' : nrow , 'columns_width' : chk }
def _get_or_default ( mylist , i , default = None ) : if i >= len ( mylist ) : return default else : return mylist [ i ]
def build_kernel_argv ( self , argv = None ) : if argv is None : argv = sys . argv [ 1 : ] self . kernel_argv = swallow_argv ( argv , self . frontend_aliases , self . frontend_flags ) self . kernel_argv . append ( "--KernelApp.parent_appname='%s'" % self . name )
def init_ssh ( self ) : if not self . sshserver and not self . sshkey : return if self . sshkey and not self . sshserver : self . sshserver = self . ip self . ip = LOCALHOST info = dict ( ip = self . ip , shell_port = self . shell_port , iopub_port = self . iopub_port , stdin_port = self . stdin_port , hb_port = self . hb_port ) self . log . info ( "Forwarding connections to %s via %s" % ( self . ip , self . sshserver ) ) self . ip = LOCALHOST try : newports = tunnel_to_kernel ( info , self . sshserver , self . sshkey ) except : self . log . error ( "Could not setup tunnels" , exc_info = True ) self . exit ( 1 ) self . shell_port , self . iopub_port , self . stdin_port , self . hb_port = newports cf = self . connection_file base , ext = os . path . splitext ( cf ) base = os . path . basename ( base ) self . connection_file = os . path . basename ( base ) + '-ssh' + ext self . log . critical ( "To connect another client via this tunnel, use:" ) self . log . critical ( "--existing %s" % self . connection_file )
def pretty ( obj , verbose = False , max_width = 79 , newline = '\n' ) : stream = StringIO ( ) printer = RepresentationPrinter ( stream , verbose , max_width , newline ) printer . pretty ( obj ) printer . flush ( ) return stream . getvalue ( )
def pprint ( obj , verbose = False , max_width = 79 , newline = '\n' ) : printer = RepresentationPrinter ( sys . stdout , verbose , max_width , newline ) printer . pretty ( obj ) printer . flush ( ) sys . stdout . write ( newline ) sys . stdout . flush ( )
def _super_pprint ( obj , p , cycle ) : p . begin_group ( 8 , '<super: ' ) p . pretty ( obj . __self_class__ ) p . text ( ',' ) p . breakable ( ) p . pretty ( obj . __self__ ) p . end_group ( 8 , '>' )
def _re_pattern_pprint ( obj , p , cycle ) : p . text ( 're.compile(' ) pattern = repr ( obj . pattern ) if pattern [ : 1 ] in 'uU' : pattern = pattern [ 1 : ] prefix = 'ur' else : prefix = 'r' pattern = prefix + pattern . replace ( '\\\\' , '\\' ) p . text ( pattern ) if obj . flags : p . text ( ',' ) p . breakable ( ) done_one = False for flag in ( 'TEMPLATE' , 'IGNORECASE' , 'LOCALE' , 'MULTILINE' , 'DOTALL' , 'UNICODE' , 'VERBOSE' , 'DEBUG' ) : if obj . flags & getattr ( re , flag ) : if done_one : p . text ( '|' ) p . text ( 're.' + flag ) done_one = True p . text ( ')' )
def _type_pprint ( obj , p , cycle ) : if obj . __module__ in ( '__builtin__' , 'exceptions' ) : name = obj . __name__ else : name = obj . __module__ + '.' + obj . __name__ p . text ( name )
def _function_pprint ( obj , p , cycle ) : if obj . __module__ in ( '__builtin__' , 'exceptions' ) or not obj . __module__ : name = obj . __name__ else : name = obj . __module__ + '.' + obj . __name__ p . text ( '<function %s>' % name )
def _exception_pprint ( obj , p , cycle ) : if obj . __class__ . __module__ in ( 'exceptions' , 'builtins' ) : name = obj . __class__ . __name__ else : name = '%s.%s' % ( obj . __class__ . __module__ , obj . __class__ . __name__ ) step = len ( name ) + 1 p . begin_group ( step , name + '(' ) for idx , arg in enumerate ( getattr ( obj , 'args' , ( ) ) ) : if idx : p . text ( ',' ) p . breakable ( ) p . pretty ( arg ) p . end_group ( step , ')' )
def for_type ( typ , func ) : oldfunc = _type_pprinters . get ( typ , None ) if func is not None : _type_pprinters [ typ ] = func return oldfunc
def text ( self , obj ) : width = len ( obj ) if self . buffer : text = self . buffer [ - 1 ] if not isinstance ( text , Text ) : text = Text ( ) self . buffer . append ( text ) text . add ( obj , width ) self . buffer_width += width self . _break_outer_groups ( ) else : self . output . write ( obj ) self . output_width += width
def end_group ( self , dedent = 0 , close = '' ) : self . indentation -= dedent group = self . group_stack . pop ( ) if not group . breakables : self . group_queue . remove ( group ) if close : self . text ( close )
def flush ( self ) : for data in self . buffer : self . output_width += data . output ( self . output , self . output_width ) self . buffer . clear ( ) self . buffer_width = 0
def pretty ( self , obj ) : obj_id = id ( obj ) cycle = obj_id in self . stack self . stack . append ( obj_id ) self . begin_group ( ) try : obj_class = getattr ( obj , '__class__' , None ) or type ( obj ) try : printer = self . singleton_pprinters [ obj_id ] except ( TypeError , KeyError ) : pass else : return printer ( obj , self , cycle ) for cls in _get_mro ( obj_class ) : if cls in self . type_pprinters : return self . type_pprinters [ cls ] ( obj , self , cycle ) else : printer = self . _in_deferred_types ( cls ) if printer is not None : return printer ( obj , self , cycle ) else : if '_repr_pretty_' in obj_class . __dict__ : meth = obj_class . _repr_pretty_ if callable ( meth ) : return meth ( obj , self , cycle ) return _default_pprint ( obj , self , cycle ) finally : self . end_group ( ) self . stack . pop ( )
def _write_row_into_ods ( ods , sheet_no , row_no , row ) : ods . content . getSheet ( sheet_no ) for j , col in enumerate ( row ) : cell = ods . content . getCell ( j , row_no + 1 ) cell . stringValue ( _escape_apostrophe ( col ) ) if j % 2 == 1 : cell . setCellColor ( settings . EVEN_COLUMN_BG_COLOR ) else : cell . setCellColor ( settings . ODD_COLUMN_BG_COLOR )
def osx_clipboard_get ( ) : p = subprocess . Popen ( [ 'pbpaste' , '-Prefer' , 'ascii' ] , stdout = subprocess . PIPE ) text , stderr = p . communicate ( ) text = text . replace ( '\r' , '\n' ) return text
def _get_build_prefix ( ) : path = os . path . join ( tempfile . gettempdir ( ) , 'pip_build_%s' % __get_username ( ) . replace ( ' ' , '_' ) ) if WINDOWS : """ on windows(tested on 7) temp dirs are isolated """ return path try : os . mkdir ( path ) write_delete_marker_file ( path ) except OSError : file_uid = None try : file_uid = get_path_uid ( path ) except OSError : file_uid = None if file_uid != os . geteuid ( ) : msg = ( "The temporary folder for building (%s) is either not owned by" " you, or is a symlink." % path ) print ( msg ) print ( "pip will not work until the temporary folder is either " "deleted or is a real directory owned by your user account." ) raise exceptions . InstallationError ( msg ) return path
def prepare_communication ( self ) : RectPartitioner . prepare_communication ( self ) if self . lower_neighbors [ 0 ] >= 0 : self . in_lower_buffers = [ zeros ( 1 , float ) ] self . out_lower_buffers = [ zeros ( 1 , float ) ] if self . upper_neighbors [ 0 ] >= 0 : self . in_upper_buffers = [ zeros ( 1 , float ) ] self . out_upper_buffers = [ zeros ( 1 , float ) ]
def prepare_communication ( self ) : RectPartitioner . prepare_communication ( self ) self . in_lower_buffers = [ [ ] , [ ] ] self . out_lower_buffers = [ [ ] , [ ] ] self . in_upper_buffers = [ [ ] , [ ] ] self . out_upper_buffers = [ [ ] , [ ] ] size1 = self . subd_hi_ix [ 1 ] - self . subd_lo_ix [ 1 ] + 1 if self . lower_neighbors [ 0 ] >= 0 : self . in_lower_buffers [ 0 ] = zeros ( size1 , float ) self . out_lower_buffers [ 0 ] = zeros ( size1 , float ) if self . upper_neighbors [ 0 ] >= 0 : self . in_upper_buffers [ 0 ] = zeros ( size1 , float ) self . out_upper_buffers [ 0 ] = zeros ( size1 , float ) size0 = self . subd_hi_ix [ 0 ] - self . subd_lo_ix [ 0 ] + 1 if self . lower_neighbors [ 1 ] >= 0 : self . in_lower_buffers [ 1 ] = zeros ( size0 , float ) self . out_lower_buffers [ 1 ] = zeros ( size0 , float ) if self . upper_neighbors [ 1 ] >= 0 : self . in_upper_buffers [ 1 ] = zeros ( size0 , float ) self . out_upper_buffers [ 1 ] = zeros ( size0 , float )
def extract_dates ( obj ) : if isinstance ( obj , dict ) : obj = dict ( obj ) for k , v in obj . iteritems ( ) : obj [ k ] = extract_dates ( v ) elif isinstance ( obj , ( list , tuple ) ) : obj = [ extract_dates ( o ) for o in obj ] elif isinstance ( obj , basestring ) : if ISO8601_PAT . match ( obj ) : obj = datetime . strptime ( obj , ISO8601 ) return obj
def squash_dates ( obj ) : if isinstance ( obj , dict ) : obj = dict ( obj ) for k , v in obj . iteritems ( ) : obj [ k ] = squash_dates ( v ) elif isinstance ( obj , ( list , tuple ) ) : obj = [ squash_dates ( o ) for o in obj ] elif isinstance ( obj , datetime ) : obj = obj . strftime ( ISO8601 ) return obj
def date_default ( obj ) : if isinstance ( obj , datetime ) : return obj . strftime ( ISO8601 ) else : raise TypeError ( "%r is not JSON serializable" % obj )
def check_site_dir ( self ) : instdir = normalize_path ( self . install_dir ) pth_file = os . path . join ( instdir , 'easy-install.pth' ) is_site_dir = instdir in self . all_site_dirs if not is_site_dir and not self . multi_version : is_site_dir = self . check_pth_processing ( ) else : testfile = self . pseudo_tempname ( ) + '.write-test' test_exists = os . path . exists ( testfile ) try : if test_exists : os . unlink ( testfile ) open ( testfile , 'w' ) . close ( ) os . unlink ( testfile ) except ( OSError , IOError ) : self . cant_write_to_target ( ) if not is_site_dir and not self . multi_version : raise DistutilsError ( self . no_default_version_msg ( ) ) if is_site_dir : if self . pth_file is None : self . pth_file = PthDistributions ( pth_file , self . all_site_dirs ) else : self . pth_file = None PYTHONPATH = os . environ . get ( 'PYTHONPATH' , '' ) . split ( os . pathsep ) if instdir not in map ( normalize_path , [ _f for _f in PYTHONPATH if _f ] ) : self . sitepy_installed = True elif self . multi_version and not os . path . exists ( pth_file ) : self . sitepy_installed = True self . pth_file = None self . install_dir = instdir
def write_script ( self , script_name , contents , mode = "t" , * ignored ) : from setuptools . command . easy_install import chmod , current_umask log . info ( "Installing %s script to %s" , script_name , self . install_dir ) target = os . path . join ( self . install_dir , script_name ) self . outfiles . append ( target ) mask = current_umask ( ) if not self . dry_run : ensure_directory ( target ) f = open ( target , "w" + mode ) f . write ( contents ) f . close ( ) chmod ( target , 0777 - mask )
def sleep_here ( count , t ) : import time , sys print ( "hi from engine %i" % id ) sys . stdout . flush ( ) time . sleep ( t ) return count , t
def _convert_pyx_sources_to_c ( self ) : def pyx_to_c ( source ) : if source . endswith ( '.pyx' ) : source = source [ : - 4 ] + '.c' return source self . sources = map ( pyx_to_c , self . sources )
def main ( connection_file ) : ctx = zmq . Context . instance ( ) with open ( connection_file ) as f : cfg = json . loads ( f . read ( ) ) location = cfg [ 'location' ] reg_url = cfg [ 'url' ] session = Session ( key = str_to_bytes ( cfg [ 'exec_key' ] ) ) query = ctx . socket ( zmq . DEALER ) query . connect ( disambiguate_url ( cfg [ 'url' ] , location ) ) session . send ( query , "connection_request" ) idents , msg = session . recv ( query , mode = 0 ) c = msg [ 'content' ] iopub_url = disambiguate_url ( c [ 'iopub' ] , location ) sub = ctx . socket ( zmq . SUB ) sub . setsockopt ( zmq . SUBSCRIBE , b'' ) sub . connect ( iopub_url ) while True : try : idents , msg = session . recv ( sub , mode = 0 ) except KeyboardInterrupt : return topic = idents [ 0 ] if msg [ 'msg_type' ] == 'stream' : print ( "%s: %s" % ( topic , msg [ 'content' ] [ 'data' ] ) ) elif msg [ 'msg_type' ] == 'pyerr' : c = msg [ 'content' ] print ( topic + ':' ) for line in c [ 'traceback' ] : print ( '    ' + line )
def _log_level_changed ( self , name , old , new ) : if isinstance ( new , basestring ) : new = getattr ( logging , new ) self . log_level = new self . log . setLevel ( new )
def _flags_changed ( self , name , old , new ) : for key , value in new . iteritems ( ) : assert len ( value ) == 2 , "Bad flag: %r:%s" % ( key , value ) assert isinstance ( value [ 0 ] , ( dict , Config ) ) , "Bad flag: %r:%s" % ( key , value ) assert isinstance ( value [ 1 ] , basestring ) , "Bad flag: %r:%s" % ( key , value )
def print_alias_help ( self ) : if not self . aliases : return lines = [ ] classdict = { } for cls in self . classes : for c in cls . mro ( ) [ : - 3 ] : classdict [ c . __name__ ] = c for alias , longname in self . aliases . iteritems ( ) : classname , traitname = longname . split ( '.' , 1 ) cls = classdict [ classname ] trait = cls . class_traits ( config = True ) [ traitname ] help = cls . class_get_trait_help ( trait ) . splitlines ( ) help [ 0 ] = help [ 0 ] . replace ( longname , alias ) + ' (%s)' % longname if len ( alias ) == 1 : help [ 0 ] = help [ 0 ] . replace ( '--%s=' % alias , '-%s ' % alias ) lines . extend ( help ) print os . linesep . join ( lines )
def print_flag_help ( self ) : if not self . flags : return lines = [ ] for m , ( cfg , help ) in self . flags . iteritems ( ) : prefix = '--' if len ( m ) > 1 else '-' lines . append ( prefix + m ) lines . append ( indent ( dedent ( help . strip ( ) ) ) ) print os . linesep . join ( lines )
def print_subcommands ( self ) : if not self . subcommands : return lines = [ "Subcommands" ] lines . append ( '-' * len ( lines [ 0 ] ) ) lines . append ( '' ) for p in wrap_paragraphs ( self . subcommand_description ) : lines . append ( p ) lines . append ( '' ) for subc , ( cls , help ) in self . subcommands . iteritems ( ) : lines . append ( subc ) if help : lines . append ( indent ( dedent ( help . strip ( ) ) ) ) lines . append ( '' ) print os . linesep . join ( lines )
def update_config ( self , config ) : newconfig = deepcopy ( self . config ) newconfig . _merge ( config ) self . config = newconfig
def initialize_subcommand ( self , subc , argv = None ) : subapp , help = self . subcommands . get ( subc ) if isinstance ( subapp , basestring ) : subapp = import_item ( subapp ) self . __class__ . clear_instance ( ) self . subapp = subapp . instance ( ) self . subapp . initialize ( argv )
def parse_command_line ( self , argv = None ) : argv = sys . argv [ 1 : ] if argv is None else argv if argv and argv [ 0 ] == 'help' : argv = argv [ 1 : ] + [ '-h' ] if self . subcommands and len ( argv ) > 0 : subc , subargv = argv [ 0 ] , argv [ 1 : ] if re . match ( r'^\w(\-?\w)*$' , subc ) and subc in self . subcommands : return self . initialize_subcommand ( subc , subargv ) if '-h' in argv or '--help' in argv or '--help-all' in argv : self . print_description ( ) self . print_help ( '--help-all' in argv ) self . print_examples ( ) self . exit ( 0 ) if '--version' in argv or '-V' in argv : self . print_version ( ) self . exit ( 0 ) flags , aliases = self . flatten_flags ( ) loader = KVArgParseConfigLoader ( argv = argv , aliases = aliases , flags = flags ) config = loader . load_config ( ) self . update_config ( config ) self . extra_args = loader . extra_args
def load_config_file ( self , filename , path = None ) : loader = PyFileConfigLoader ( filename , path = path ) try : config = loader . load_config ( ) except ConfigFileNotFound : raise except Exception : filename = loader . full_filename or filename self . log . error ( "Exception while loading config file %s" , filename , exc_info = True ) else : self . log . debug ( "Loaded config file: %s" , loader . full_filename ) self . update_config ( config )
def generate_config_file ( self ) : lines = [ % self . name ] lines . append ( '' ) lines . append ( 'c = get_config()' ) lines . append ( '' ) for cls in self . classes : lines . append ( cls . class_config_section ( ) ) return '\n' . join ( lines )
def downsample ( array , k ) : length = array . shape [ 0 ] indices = random . sample ( xrange ( length ) , k ) return array [ indices ]
def write ( self , msg ) : if self . should ( 'pid' ) : msg = "pid %5d: %s" % ( os . getpid ( ) , msg ) self . output . write ( msg + "\n" ) self . output . flush ( )
def class_config_section ( cls ) : def c ( s ) : """return a commented, wrapped block.""" s = '\n\n' . join ( wrap_paragraphs ( s , 78 ) ) return + s . replace ( '\n' , ) breaker = '#' + '-' * 78 s = % cls . __name__ lines = [ breaker , s , breaker , '' ] desc = cls . class_traits ( ) . get ( 'description' ) if desc : desc = desc . default_value else : desc = getattr ( cls , '__doc__' , '' ) if desc : lines . append ( c ( desc ) ) lines . append ( '' ) parents = [ ] for parent in cls . mro ( ) : if parent is not cls and issubclass ( parent , Configurable ) and parent . class_traits ( config = True ) : parents . append ( parent ) if parents : pstr = ', ' . join ( [ p . __name__ for p in parents ] ) lines . append ( c ( '%s will inherit config from: %s' % ( cls . __name__ , pstr ) ) ) lines . append ( '' ) for name , trait in cls . class_traits ( config = True ) . iteritems ( ) : help = trait . get_metadata ( 'help' ) or '' lines . append ( c ( help ) ) lines . append ( % ( cls . __name__ , name , trait . get_default_value ( ) ) ) lines . append ( '' ) return '\n' . join ( lines )
def clear_instance ( cls ) : if not cls . initialized ( ) : return for subclass in cls . _walk_mro ( ) : if isinstance ( subclass . _instance , cls ) : subclass . _instance = None
def formatFailure ( self , test , err ) : ec , ev , tb = err tbinfo = inspect_traceback ( tb ) test . tbinfo = tbinfo return ( ec , '\n' . join ( [ str ( ev ) , tbinfo ] ) , tb )
def crash_handler_lite ( etype , evalue , tb ) : traceback . print_exception ( etype , evalue , tb ) from IPython . core . interactiveshell import InteractiveShell if InteractiveShell . initialized ( ) : config = "%config " else : config = "c." print >> sys . stderr , _lite_message_template . format ( email = author_email , config = config )
def make_report ( self , traceback ) : sec_sep = self . section_sep report = [ '*' * 75 + '\n\n' + 'IPython post-mortem report\n\n' ] rpt_add = report . append rpt_add ( sys_info ( ) ) try : config = pformat ( self . app . config ) rpt_add ( sec_sep ) rpt_add ( 'Application name: %s\n\n' % self . app_name ) rpt_add ( 'Current user configuration structure:\n\n' ) rpt_add ( config ) except : pass rpt_add ( sec_sep + 'Crash traceback:\n\n' + traceback ) return '' . join ( report )
def call_handlers ( self , msg ) : self . message_received . emit ( msg ) msg_type = msg [ 'header' ] [ 'msg_type' ] signal = getattr ( self , msg_type , None ) if signal : signal . emit ( msg ) if not self . _handlers_called : self . first_reply . emit ( ) self . _handlers_called = True
def call_handlers ( self , msg ) : self . message_received . emit ( msg ) msg_type = msg [ 'header' ] [ 'msg_type' ] signal = getattr ( self , msg_type + '_received' , None ) if signal : signal . emit ( msg ) elif msg_type in ( 'stdout' , 'stderr' ) : self . stream_received . emit ( msg )
def flush ( self ) : super ( QtSubSocketChannel , self ) . flush ( ) QtCore . QCoreApplication . instance ( ) . processEvents ( )
def call_handlers ( self , msg ) : self . message_received . emit ( msg ) msg_type = msg [ 'header' ] [ 'msg_type' ] if msg_type == 'input_request' : self . input_requested . emit ( msg )
def start_kernel ( self , * args , * * kw ) : if self . _shell_channel is not None : self . _shell_channel . reset_first_reply ( ) super ( QtKernelManager , self ) . start_kernel ( * args , * * kw ) self . started_kernel . emit ( )
def start_channels ( self , * args , * * kw ) : super ( QtKernelManager , self ) . start_channels ( * args , * * kw ) self . started_channels . emit ( )
def shell_channel ( self ) : if self . _shell_channel is None : self . _shell_channel = super ( QtKernelManager , self ) . shell_channel self . _shell_channel . first_reply . connect ( self . _first_reply ) return self . _shell_channel
def read ( self , fp , * * kwargs ) : nbs = fp . read ( ) if not py3compat . PY3 and not isinstance ( nbs , unicode ) : nbs = py3compat . str_to_unicode ( nbs ) return self . reads ( nbs , * * kwargs )
def write ( self , nb , fp , * * kwargs ) : nbs = self . writes ( nb , * * kwargs ) if not py3compat . PY3 and not isinstance ( nbs , unicode ) : nbs = py3compat . str_to_unicode ( nbs ) return fp . write ( nbs )
def _method_magic_marker ( magic_kind ) : validate_type ( magic_kind ) def magic_deco ( arg ) : call = lambda f , * a , * * k : f ( * a , * * k ) if callable ( arg ) : func = arg name = func . func_name retval = decorator ( call , func ) record_magic ( magics , magic_kind , name , name ) elif isinstance ( arg , basestring ) : name = arg def mark ( func , * a , * * kw ) : record_magic ( magics , magic_kind , name , func . func_name ) return decorator ( call , func ) retval = mark else : raise TypeError ( "Decorator can only be called with " "string or function" ) return retval magic_deco . __doc__ = _docstring_template . format ( 'method' , magic_kind ) return magic_deco
def _function_magic_marker ( magic_kind ) : validate_type ( magic_kind ) def magic_deco ( arg ) : call = lambda f , * a , * * k : f ( * a , * * k ) caller = sys . _getframe ( 1 ) for ns in [ 'f_locals' , 'f_globals' , 'f_builtins' ] : get_ipython = getattr ( caller , ns ) . get ( 'get_ipython' ) if get_ipython is not None : break else : raise NameError ( 'Decorator can only run in context where ' '`get_ipython` exists' ) ip = get_ipython ( ) if callable ( arg ) : func = arg name = func . func_name ip . register_magic_function ( func , magic_kind , name ) retval = decorator ( call , func ) elif isinstance ( arg , basestring ) : name = arg def mark ( func , * a , * * kw ) : ip . register_magic_function ( func , magic_kind , name ) return decorator ( call , func ) retval = mark else : raise TypeError ( "Decorator can only be called with " "string or function" ) return retval ds = _docstring_template . format ( 'function' , magic_kind ) ds += dedent ( ) magic_deco . __doc__ = ds return magic_deco
def format_latex ( self , strng ) : escape_re = re . compile ( r'(%|_|\$|#|&)' , re . MULTILINE ) cmd_name_re = re . compile ( r'^(%s.*?):' % ESC_MAGIC , re . MULTILINE ) cmd_re = re . compile ( r'(?P<cmd>%s.+?\b)(?!\}\}:)' % ESC_MAGIC , re . MULTILINE ) par_re = re . compile ( r'\\$' , re . MULTILINE ) newline_re = re . compile ( r'\\n' ) #strng = cmd_name_re.sub(r'\n\\texttt{\\textsl{\\large \1}}:',strng) strng = cmd_name_re . sub ( r'\n\\bigskip\n\\texttt{\\textbf{ \1}}:' , strng ) strng = cmd_re . sub ( r'\\texttt{\g<cmd>}' , strng ) strng = par_re . sub ( r'\\\\' , strng ) strng = escape_re . sub ( r'\\\1' , strng ) strng = newline_re . sub ( r'\\textbackslash{}n' , strng ) return strng
def default_option ( self , fn , optstr ) : if fn not in self . lsmagic ( ) : error ( "%s is not a magic function" % fn ) self . options_table [ fn ] = optstr
def page_guiref ( arg_s = None ) : from IPython . core import page page . page ( gui_reference , auto_html = True )
def task_with_callable ( the_callable , label = None , schedule = DEFAULT_SCHEDULE , userdata = None , pk_override = None ) : task = Task ( ) if isinstance ( the_callable , str ) : if pk_override is not None : components = the_callable . split ( '.' ) info = dict ( func_type = 'instancemethod' , module_name = '.' . join ( components [ : - 2 ] ) , class_name = components [ - 2 ] , class_path = '.' . join ( components [ : - 1 ] ) , model_pk = pk_override , func_name = components [ - 1 ] , func_path = the_callable , ) task . funcinfo = info else : task . funcinfo = get_func_info ( func_from_string ( the_callable ) ) else : task . funcinfo = get_func_info ( the_callable ) if label is None : task . label = task . funcinfo [ 'func_path' ] else : task . label = label task . schedule = schedule if not croniter . is_valid ( task . schedule ) : raise ValueError ( f"Cron schedule {task.schedule} is not valid" ) if userdata is None : task . userdata = dict ( ) else : if isinstance ( userdata , dict ) : task . userdata = userdata else : raise ValueError ( "Userdata must be a dictionary of JSON-serializable data" ) return task
def func_from_info ( self ) : info = self . funcinfo functype = info [ 'func_type' ] if functype in [ 'instancemethod' , 'classmethod' , 'staticmethod' ] : the_modelclass = get_module_member_by_dottedpath ( info [ 'class_path' ] ) if functype == 'instancemethod' : the_modelobject = the_modelclass . objects . get ( pk = info [ 'model_pk' ] ) the_callable = get_member ( the_modelobject , info [ 'func_name' ] ) else : the_callable = get_member ( the_modelclass , info [ 'func_name' ] ) return the_callable elif functype == 'function' : mod = import_module ( info [ 'module_name' ] ) the_callable = get_member ( mod , info [ 'func_name' ] ) return the_callable else : raise ValueError ( f"Unknown functype '{functype} in task {self.pk} ({self.label})" )
def calc_next_run ( self ) : base_time = self . last_run if self . last_run == HAS_NOT_RUN : if self . wait_for_schedule is False : self . next_run = timezone . now ( ) self . wait_for_schedule = False self . save ( ) return else : base_time = timezone . now ( ) self . next_run = croniter ( self . schedule , base_time ) . get_next ( datetime ) self . save ( )
def run ( self , message ) : the_callable = self . func_from_info ( ) try : task_message = dict ( task = self , channel_message = message , ) the_callable ( task_message ) finally : if self . end_running < self . next_run : self . enabled = False Channel ( KILL_TASK_CHANNEL ) . send ( { 'id' : self . pk } ) return if self . iterations == 0 : return else : self . iterations -= 1 if self . iterations == 0 : self . enabled = False Channel ( KILL_TASK_CHANNEL ) . send ( { 'id' : self . pk } ) self . save ( )
def run_asap ( self ) : now = timezone . now ( ) self . last_run = now self . calc_next_run ( ) self . save ( ) self . submit ( now )
def run_iterations ( cls , the_callable , iterations = 1 , label = None , schedule = '* * * * * *' , userdata = None , run_immediately = False , delay_until = None ) : task = task_with_callable ( the_callable , label = label , schedule = schedule , userdata = userdata ) task . iterations = iterations if delay_until is not None : if isinstance ( delay_until , datetime ) : if delay_until > timezone . now ( ) : task . start_running = delay_until else : raise ValueError ( "Task cannot start running in the past" ) else : raise ValueError ( "delay_until must be a datetime.datetime instance" ) if run_immediately : task . next_run = timezone . now ( ) else : task . calc_next_run ( ) task . save ( )
def run_once ( cls , the_callable , userdata = None , delay_until = None ) : cls . run_iterations ( the_callable , userdata = userdata , run_immediately = True , delay_until = delay_until )
def bind_kernel ( self , * * kwargs ) : if self . kernel_app is not None : return self . log . info ( "Opening ports for direct connections as an IPython kernel" ) kernel = self . kernel kwargs . setdefault ( 'config' , self . config ) kwargs . setdefault ( 'log' , self . log ) kwargs . setdefault ( 'profile_dir' , self . profile_dir ) kwargs . setdefault ( 'session' , self . engine . session ) app = self . kernel_app = IPKernelApp ( * * kwargs ) IPKernelApp . _instance = app app . init_connection_file ( ) app . shell_port = app . _bind_socket ( kernel . shell_streams [ 0 ] , app . shell_port ) app . log . debug ( "shell ROUTER Channel on port: %i" , app . shell_port ) app . iopub_port = app . _bind_socket ( kernel . iopub_socket , app . iopub_port ) app . log . debug ( "iopub PUB Channel on port: %i" , app . iopub_port ) kernel . stdin_socket = self . engine . context . socket ( zmq . ROUTER ) app . stdin_port = app . _bind_socket ( kernel . stdin_socket , app . stdin_port ) app . log . debug ( "stdin ROUTER Channel on port: %i" , app . stdin_port ) app . init_heartbeat ( ) app . log_connection_info ( ) app . write_connection_file ( )
def pid_exists ( pid ) : if not isinstance ( pid , int ) : raise TypeError ( 'an integer is required' ) if pid < 0 : return False try : os . kill ( pid , 0 ) except OSError : e = sys . exc_info ( ) [ 1 ] return e . errno == errno . EPERM else : return True
def get_disk_usage ( path ) : st = os . statvfs ( path ) free = ( st . f_bavail * st . f_frsize ) total = ( st . f_blocks * st . f_frsize ) used = ( st . f_blocks - st . f_bfree ) * st . f_frsize percent = usage_percent ( used , total , _round = 1 ) return nt_diskinfo ( total , used , free , percent )
def run ( self ) : try : from _winapi import WAIT_OBJECT_0 , INFINITE except ImportError : from _subprocess import WAIT_OBJECT_0 , INFINITE handles = [ ] if self . interrupt_handle : handles . append ( self . interrupt_handle ) if self . parent_handle : handles . append ( self . parent_handle ) arch = platform . architecture ( ) [ 0 ] c_int = ctypes . c_int64 if arch . startswith ( '64' ) else ctypes . c_int while True : result = ctypes . windll . kernel32 . WaitForMultipleObjects ( len ( handles ) , ( c_int * len ( handles ) ) ( * handles ) , False , INFINITE ) if WAIT_OBJECT_0 <= result < len ( handles ) : handle = handles [ result - WAIT_OBJECT_0 ] if handle == self . interrupt_handle : interrupt_main ( ) elif handle == self . parent_handle : os . _exit ( 1 ) elif result < 0 : warn ( ) return
def filter_ns ( ns , name_pattern = "*" , type_pattern = "all" , ignore_case = True , show_all = True ) : pattern = name_pattern . replace ( "*" , ".*" ) . replace ( "?" , "." ) if ignore_case : reg = re . compile ( pattern + "$" , re . I ) else : reg = re . compile ( pattern + "$" ) return dict ( ( key , obj ) for key , obj in ns . iteritems ( ) if reg . match ( key ) and show_hidden ( key , show_all ) and is_type ( obj , type_pattern ) )
def draw_if_interactive ( ) : fig = Gcf . get_active ( ) . canvas . figure if not hasattr ( fig , 'show' ) : fig . show = lambda * a : send_figure ( fig ) if not matplotlib . is_interactive ( ) : return try : show . _to_draw . remove ( fig ) except ValueError : pass show . _to_draw . append ( fig ) show . _draw_called = True
def send_figure ( fig ) : fmt = InlineBackend . instance ( ) . figure_format data = print_figure ( fig , fmt ) if data is None : return mimetypes = { 'png' : 'image/png' , 'svg' : 'image/svg+xml' } mime = mimetypes [ fmt ] sys . stdout . flush ( ) sys . stderr . flush ( ) publish_display_data ( 'IPython.zmq.pylab.backend_inline.send_figure' , { mime : data } )
def _handle_sigint ( self , sig , frame ) : signal . signal ( signal . SIGINT , self . _signal_stop ) thread = threading . Thread ( target = self . _confirm_exit ) thread . daemon = True thread . start ( )
def _render ( self , name , color = True , * * kwargs ) : if name == 'rewrite' : return self . _render_rewrite ( color = color ) if color : scheme = self . color_scheme_table . active_colors if name == 'out' : colors = color_lists [ 'normal' ] colors . number , colors . prompt , colors . normal = scheme . out_number , scheme . out_prompt , scheme . normal else : colors = color_lists [ 'inp' ] colors . number , colors . prompt , colors . normal = scheme . in_number , scheme . in_prompt , scheme . in_normal if name == 'in2' : colors . prompt = scheme . in_prompt2 else : colors = color_lists [ 'nocolor' ] colors . number , colors . prompt , colors . normal = '' , '' , '' count = self . shell . execution_count fmtargs = dict ( color = colors , count = count , dots = "." * len ( str ( count ) ) , width = self . width , txtwidth = self . txtwidth ) fmtargs . update ( self . lazy_evaluate_fields ) fmtargs . update ( kwargs ) prompt = colors . prompt + self . templates [ name ] + colors . normal return self . _formatter . format ( prompt , * * fmtargs )
def mappable ( obj ) : if isinstance ( obj , ( tuple , list ) ) : return True for m in arrayModules : if isinstance ( obj , m [ 'type' ] ) : return True return False
def getPartition ( self , seq , p , q ) : if p < 0 or p >= q : print "No partition exists." return remainder = len ( seq ) % q basesize = len ( seq ) // q hi = [ ] lo = [ ] for n in range ( q ) : if n < remainder : lo . append ( n * ( basesize + 1 ) ) hi . append ( lo [ - 1 ] + basesize + 1 ) else : lo . append ( n * basesize + remainder ) hi . append ( lo [ - 1 ] + basesize ) try : result = seq [ lo [ p ] : hi [ p ] ] except TypeError : result = list ( islice ( seq , lo [ p ] , hi [ p ] ) ) return result
def main ( ) : parser = optparse . OptionParser ( usage = MAIN_USAGE ) newopt = parser . add_option newopt ( '--ipython' , action = 'store_const' , dest = 'mode' , const = 'ipython' , help = 'IPython interactive runner (default).' ) newopt ( '--python' , action = 'store_const' , dest = 'mode' , const = 'python' , help = 'Python interactive runner.' ) newopt ( '--sage' , action = 'store_const' , dest = 'mode' , const = 'sage' , help = 'SAGE interactive runner.' ) opts , args = parser . parse_args ( ) runners = dict ( ipython = IPythonRunner , python = PythonRunner , sage = SAGERunner ) try : ext = os . path . splitext ( args [ 0 ] ) [ - 1 ] except IndexError : ext = '' modes = { '.ipy' : 'ipython' , '.py' : 'python' , '.sage' : 'sage' } mode = modes . get ( ext , "ipython" ) if opts . mode : mode = opts . mode runners [ mode ] ( ) . main ( args )
def main ( self , argv = None ) : parser = optparse . OptionParser ( usage = USAGE % self . __class__ . __name__ ) newopt = parser . add_option newopt ( '-i' , '--interact' , action = 'store_true' , default = False , help = 'Interact with the program after the script is run.' ) opts , args = parser . parse_args ( argv ) if len ( args ) != 1 : print >> sys . stderr , "You must supply exactly one file to run." sys . exit ( 1 ) self . run_file ( args [ 0 ] , opts . interact )
def xml_file ( self , cu , analysis ) : package_name = rpartition ( cu . name , "." ) [ 0 ] className = cu . name package = self . packages . setdefault ( package_name , [ { } , 0 , 0 , 0 , 0 ] ) xclass = self . xml_out . createElement ( "class" ) xclass . appendChild ( self . xml_out . createElement ( "methods" ) ) xlines = self . xml_out . createElement ( "lines" ) xclass . appendChild ( xlines ) xclass . setAttribute ( "name" , className ) filename = cu . file_locator . relative_filename ( cu . filename ) xclass . setAttribute ( "filename" , filename . replace ( "\\" , "/" ) ) xclass . setAttribute ( "complexity" , "0" ) branch_stats = analysis . branch_stats ( ) for line in sorted ( analysis . statements ) : xline = self . xml_out . createElement ( "line" ) xline . setAttribute ( "number" , str ( line ) ) xline . setAttribute ( "hits" , str ( int ( line not in analysis . missing ) ) ) if self . arcs : if line in branch_stats : total , taken = branch_stats [ line ] xline . setAttribute ( "branch" , "true" ) xline . setAttribute ( "condition-coverage" , "%d%% (%d/%d)" % ( 100 * taken / total , taken , total ) ) xlines . appendChild ( xline ) class_lines = len ( analysis . statements ) class_hits = class_lines - len ( analysis . missing ) if self . arcs : class_branches = sum ( [ t for t , k in branch_stats . values ( ) ] ) missing_branches = sum ( [ t - k for t , k in branch_stats . values ( ) ] ) class_br_hits = class_branches - missing_branches else : class_branches = 0.0 class_br_hits = 0.0 xclass . setAttribute ( "line-rate" , rate ( class_hits , class_lines ) ) xclass . setAttribute ( "branch-rate" , rate ( class_br_hits , class_branches ) ) package [ 0 ] [ className ] = xclass package [ 1 ] += class_hits package [ 2 ] += class_lines package [ 3 ] += class_br_hits package [ 4 ] += class_branches
def reduce_freqs ( freqlist ) : allfreqs = np . zeros_like ( freqlist [ 0 ] ) for f in freqlist : allfreqs += f return allfreqs
def compute_n_digit_freqs ( filename , n ) : d = txt_file_to_digits ( filename ) freqs = n_digit_freqs ( d , n ) return freqs
def txt_file_to_digits ( filename , the_type = str ) : with open ( filename , 'r' ) as f : for line in f . readlines ( ) : for c in line : if c != '\n' and c != ' ' : yield the_type ( c )
def one_digit_freqs ( digits , normalize = False ) : freqs = np . zeros ( 10 , dtype = 'i4' ) for d in digits : freqs [ int ( d ) ] += 1 if normalize : freqs = freqs / freqs . sum ( ) return freqs
def two_digit_freqs ( digits , normalize = False ) : freqs = np . zeros ( 100 , dtype = 'i4' ) last = digits . next ( ) this = digits . next ( ) for d in digits : index = int ( last + this ) freqs [ index ] += 1 last = this this = d if normalize : freqs = freqs / freqs . sum ( ) return freqs
def plot_two_digit_freqs ( f2 ) : f2_copy = f2 . copy ( ) f2_copy . shape = ( 10 , 10 ) ax = plt . matshow ( f2_copy ) plt . colorbar ( ) for i in range ( 10 ) : for j in range ( 10 ) : plt . text ( i - 0.2 , j + 0.2 , str ( j ) + str ( i ) ) plt . ylabel ( 'First digit' ) plt . xlabel ( 'Second digit' ) return ax
def plot_one_digit_freqs ( f1 ) : ax = plt . plot ( f1 , 'bo-' ) plt . title ( 'Single digit counts in pi' ) plt . xlabel ( 'Digit' ) plt . ylabel ( 'Count' ) return ax
def debug_src ( src , pm = False , globs = None ) : testsrc = script_from_examples ( src ) debug_script ( testsrc , pm , globs )
def debug_script ( src , pm = False , globs = None ) : import pdb srcfilename = tempfile . mktemp ( ".py" , "doctestdebug" ) f = open ( srcfilename , 'w' ) f . write ( src ) f . close ( ) try : if globs : globs = globs . copy ( ) else : globs = { } if pm : try : execfile ( srcfilename , globs , globs ) except : print sys . exc_info ( ) [ 1 ] pdb . post_mortem ( sys . exc_info ( ) [ 2 ] ) else : pdb . run ( "execfile(%r)" % srcfilename , globs , globs ) finally : os . remove ( srcfilename )
def hdict ( self , hashroot ) : hfiles = self . keys ( hashroot + "/*" ) hfiles . sort ( ) last = len ( hfiles ) and hfiles [ - 1 ] or '' if last . endswith ( 'xx' ) : hfiles = [ last ] + hfiles [ : - 1 ] all = { } for f in hfiles : try : all . update ( self [ f ] ) except KeyError : print "Corrupt" , f , "deleted - hset is not threadsafe!" del self [ f ] self . uncache ( f ) return all
def keys ( self , globpat = None ) : if globpat is None : files = self . root . walkfiles ( ) else : files = [ Path ( p ) for p in glob . glob ( self . root / globpat ) ] return [ self . _normalized ( p ) for p in files if p . isfile ( ) ]
def allow ( self , record ) : if not self : return True return self . _allow ( record ) and not self . _deny ( record )
def options ( self , parser , env ) : parser . add_option ( "--nologcapture" , action = "store_false" , default = not env . get ( self . env_opt ) , dest = "logcapture" , help = "Disable logging capture plugin. " "Logging configurtion will be left intact." " [NOSE_NOLOGCAPTURE]" ) parser . add_option ( "--logging-format" , action = "store" , dest = "logcapture_format" , default = env . get ( 'NOSE_LOGFORMAT' ) or self . logformat , metavar = "FORMAT" , help = "Specify custom format to print statements. " "Uses the same format as used by standard logging handlers." " [NOSE_LOGFORMAT]" ) parser . add_option ( "--logging-datefmt" , action = "store" , dest = "logcapture_datefmt" , default = env . get ( 'NOSE_LOGDATEFMT' ) or self . logdatefmt , metavar = "FORMAT" , help = "Specify custom date/time format to print statements. " "Uses the same format as used by standard logging handlers." " [NOSE_LOGDATEFMT]" ) parser . add_option ( "--logging-filter" , action = "store" , dest = "logcapture_filters" , default = env . get ( 'NOSE_LOGFILTER' ) , metavar = "FILTER" , help = "Specify which statements to filter in/out. " "By default, everything is captured. If the output is too" " verbose,\nuse this option to filter out needless output.\n" "Example: filter=foo will capture statements issued ONLY to\n" " foo or foo.what.ever.sub but not foobar or other logger.\n" "Specify multiple loggers with comma: filter=foo,bar,baz.\n" "If any logger name is prefixed with a minus, eg filter=-foo,\n" "it will be excluded rather than included. Default: " "exclude logging messages from nose itself (-nose)." " [NOSE_LOGFILTER]\n" ) parser . add_option ( "--logging-clear-handlers" , action = "store_true" , default = False , dest = "logcapture_clear" , help = "Clear all other logging handlers" ) parser . add_option ( "--logging-level" , action = "store" , default = 'NOTSET' , dest = "logcapture_level" , help = "Set the log level to capture" )
def formatError ( self , test , err ) : test . capturedLogging = records = self . formatLogRecords ( ) if not records : return err ec , ev , tb = err return ( ec , self . addCaptureToErr ( ev , records ) , tb )
def _get_new_csv_writers ( trans_title , meta_title , trans_csv_path , meta_csv_path ) : trans_writer = UnicodeWriter ( trans_csv_path ) trans_writer . writerow ( trans_title ) meta_writer = UnicodeWriter ( meta_csv_path ) meta_writer . writerow ( meta_title ) return trans_writer , meta_writer
def subscribe_user ( self , user ) : url = self . root_url + "subscribe_user" values = { } values [ "username" ] = user return self . _query ( url , values )
def init_parser ( ) : usage = parser = OptionParser ( usage , version = "%prog " + notifo . __version__ ) parser . add_option ( "-u" , "--user" , action = "store" , dest = "user" , help = "your notifo username" ) parser . add_option ( "-s" , "--secret" , action = "store" , dest = "secret" , help = "your notifo API secret" ) parser . add_option ( "-n" , "--name" , action = "store" , dest = "name" , help = "recipient for the notification" ) parser . add_option ( "-l" , "--label" , action = "store" , dest = "label" , help = "label for the notification" ) parser . add_option ( "-t" , "--title" , action = "store" , dest = "title" , help = "title of the notification" ) parser . add_option ( "-c" , "--callback" , action = "store" , dest = "callback" , help = "callback URL to call" ) parser . add_option ( "-m" , "--message" , action = "store_true" , dest = "message" , default = False , help = "send message instead of notification" ) ( options , args ) = parser . parse_args ( ) return ( parser , options , args )
def make_code_from_py ( filename ) : try : source_file = open_source ( filename ) except IOError : raise NoSource ( "No file to run: %r" % filename ) try : source = source_file . read ( ) finally : source_file . close ( ) if not source or source [ - 1 ] != '\n' : source += '\n' code = compile ( source , filename , "exec" ) return code
def make_code_from_pyc ( filename ) : try : fpyc = open ( filename , "rb" ) except IOError : raise NoCode ( "No file to run: %r" % filename ) try : magic = fpyc . read ( 4 ) if magic != imp . get_magic ( ) : raise NoCode ( "Bad magic number in .pyc file" ) fpyc . read ( 4 ) if sys . version_info >= ( 3 , 3 ) : fpyc . read ( 4 ) code = marshal . load ( fpyc ) finally : fpyc . close ( ) return code
def html_tableify ( item_matrix , select = None , header = None , footer = None ) : if not item_matrix : return '' html_cols = [ ] tds = lambda text : u'<td>' + text + u'  </td>' trs = lambda text : u'<tr>' + text + u'</tr>' tds_items = [ map ( tds , row ) for row in item_matrix ] if select : row , col = select tds_items [ row ] [ col ] = u'<td class="inverted">' + item_matrix [ row ] [ col ] + u'  </td>' #select the right item html_cols = map ( trs , ( u'' . join ( row ) for row in tds_items ) ) head = '' foot = '' if header : head = ( u'<tr>' + '' . join ( ( u'<td>' + header + u'</td>' ) * len ( item_matrix [ 0 ] ) ) + '</tr>' ) if footer : foot = ( u'<tr>' + '' . join ( ( u'<td>' + footer + u'</td>' ) * len ( item_matrix [ 0 ] ) ) + '</tr>' ) html = ( u'<table class="completion" style="white-space:pre">' + head + ( u'' . join ( html_cols ) ) + foot + u'</table>' ) return html
def current ( self , value ) : current = min ( max ( self . _min , value ) , self . _max ) self . _current = current if current > self . _stop : self . _stop = current self . _start = current - self . _width elif current < self . _start : self . _start = current self . _stop = current + self . _width if abs ( self . _start - self . _min ) <= self . _sticky_lenght : self . _start = self . _min if abs ( self . _stop - self . _max ) <= self . _sticky_lenght : self . _stop = self . _max
def _update_list ( self , hilight = True ) : self . _sliding_interval . current = self . _index [ 0 ] head = None foot = None if self . _sliding_interval . start > 0 : head = '...' if self . _sliding_interval . stop < self . _sliding_interval . _max : foot = '...' items_m = self . _justified_items [ self . _sliding_interval . start : self . _sliding_interval . stop + 1 ] self . _console_widget . _clear_temporary_buffer ( ) if ( hilight ) : sel = ( self . _sliding_interval . nth , self . _index [ 1 ] ) else : sel = None strng = html_tableify ( items_m , select = sel , header = head , footer = foot ) self . _console_widget . _fill_temporary_buffer ( self . _old_cursor , strng , html = True )
def _complete_current ( self ) : i = self . _index item = self . _items [ i [ 0 ] ] [ i [ 1 ] ] item = item . strip ( ) if item : self . _current_text_cursor ( ) . insertText ( item ) self . cancel_completion ( )
def wordfreq ( text , is_filename = False ) : if is_filename : with open ( text ) as f : text = f . read ( ) freqs = { } for word in text . split ( ) : lword = word . lower ( ) freqs [ lword ] = freqs . get ( lword , 0 ) + 1 return freqs
def print_wordfreq ( freqs , n = 10 ) : words , counts = freqs . keys ( ) , freqs . values ( ) items = zip ( counts , words ) items . sort ( reverse = True ) for ( count , word ) in items [ : n ] : print ( word , count )
def tostring ( self ) : root = self . as_element ( ) indent ( root ) txt = ET . tostring ( root , encoding = "utf-8" ) txt = re . sub ( r'_[A-Z]_' , '' , txt ) txt = '<?xml version="1.0" encoding="utf-8"?>\n' + txt return txt
def write ( self , filename ) : txt = self . tostring ( ) with open ( filename , 'w' ) as f : f . write ( txt )
def begin ( self , total : int , name = None , message = None ) : self . total = total message = message or name or "Working..." self . name = name or "ProgressMonitor" self . update ( 0 , message )
def task ( self , total : int , name = None , message = None ) : self . begin ( total , name , message ) try : yield self finally : self . done ( )
def subtask ( self , units : int ) : sm = self . submonitor ( units ) try : yield sm finally : if sm . total is None : self . update ( units ) else : sm . done ( )
def update ( self , units : int = 1 , message : str = None ) : if self . total is None : raise Exception ( "Cannot call progressmonitor.update before calling begin" ) self . worked = min ( self . total , self . worked + units ) if message : self . message = message for listener in self . listeners : listener ( self )
def load_config ( self ) : self . clear ( ) try : self . _find_file ( ) except IOError as e : raise ConfigFileNotFound ( str ( e ) ) self . _read_file_as_dict ( ) self . _convert_to_config ( ) return self . config
def _read_file_as_dict ( self ) : def load_subconfig ( fname , profile = None ) : from IPython . core . profiledir import ProfileDir , ProfileDirError if profile is not None : try : profile_dir = ProfileDir . find_profile_dir_by_name ( get_ipython_dir ( ) , profile , ) except ProfileDirError : return path = profile_dir . location else : path = self . path loader = PyFileConfigLoader ( fname , path ) try : sub_config = loader . load_config ( ) except ConfigFileNotFound : pass else : self . config . _merge ( sub_config ) def get_config ( ) : return self . config namespace = dict ( load_subconfig = load_subconfig , get_config = get_config ) fs_encoding = sys . getfilesystemencoding ( ) or 'ascii' conf_filename = self . full_filename . encode ( fs_encoding ) py3compat . execfile ( conf_filename , namespace )
def _load_flag ( self , cfg ) : if isinstance ( cfg , ( dict , Config ) ) : for sec , c in cfg . iteritems ( ) : self . config [ sec ] . update ( c ) else : raise TypeError ( "Invalid flag: %r" % cfg )
def _decode_argv ( self , argv , enc = None ) : uargv = [ ] if enc is None : enc = DEFAULT_ENCODING for arg in argv : if not isinstance ( arg , unicode ) : arg = arg . decode ( enc ) uargv . append ( arg ) return uargv
def interrupt_then_kill ( self , delay = 2.0 ) : try : self . signal ( SIGINT ) except Exception : self . log . debug ( "interrupt failed" ) pass self . killer = ioloop . DelayedCallback ( lambda : self . signal ( SIGKILL ) , delay * 1000 , self . loop ) self . killer . start ( )
def start ( self , n ) : dlist = [ ] for i in range ( n ) : if i > 0 : time . sleep ( self . delay ) el = self . launcher_class ( work_dir = self . work_dir , config = self . config , log = self . log , profile_dir = self . profile_dir , cluster_id = self . cluster_id , ) el . engine_cmd = copy . deepcopy ( self . engine_cmd ) el . engine_args = copy . deepcopy ( self . engine_args ) el . on_stop ( self . _notice_engine_stopped ) d = el . start ( ) self . launchers [ i ] = el dlist . append ( d ) self . notify_start ( dlist ) return dlist
def find_args ( self ) : return self . mpi_cmd + [ '-n' , str ( self . n ) ] + self . mpi_args + self . program + self . program_args
def start ( self , n ) : self . n = n return super ( MPILauncher , self ) . start ( )
def start ( self , n ) : self . n = n return super ( MPIEngineSetLauncher , self ) . start ( n )
def _send_file ( self , local , remote ) : remote = "%s:%s" % ( self . location , remote ) for i in range ( 10 ) : if not os . path . exists ( local ) : self . log . debug ( "waiting for %s" % local ) time . sleep ( 1 ) else : break self . log . info ( "sending %s to %s" , local , remote ) check_output ( self . scp_cmd + [ local , remote ] )
def _fetch_file ( self , remote , local ) : full_remote = "%s:%s" % ( self . location , remote ) self . log . info ( "fetching %s from %s" , local , full_remote ) for i in range ( 10 ) : check = check_output ( self . ssh_cmd + self . ssh_args + [ self . location , 'test -e' , remote , "&& echo 'yes' || echo 'no'" ] ) check = check . strip ( ) if check == 'no' : time . sleep ( 1 ) elif check == 'yes' : break check_output ( self . scp_cmd + [ full_remote , local ] )
def engine_count ( self ) : count = 0 for n in self . engines . itervalues ( ) : if isinstance ( n , ( tuple , list ) ) : n , args = n count += n return count
def start ( self , n ) : self . write_job_file ( n ) args = [ 'submit' , '/jobfile:%s' % self . job_file , '/scheduler:%s' % self . scheduler ] self . log . debug ( "Starting Win HPC Job: %s" % ( self . job_cmd + ' ' + ' ' . join ( args ) , ) ) output = check_output ( [ self . job_cmd ] + args , env = os . environ , cwd = self . work_dir , stderr = STDOUT ) job_id = self . parse_job_id ( output ) self . notify_start ( job_id ) return job_id
def parse_job_id ( self , output ) : m = self . job_id_regexp . search ( output ) if m is not None : job_id = m . group ( ) else : raise LauncherError ( "Job id couldn't be determined: %s" % output ) self . job_id = job_id self . log . info ( 'Job submitted with job id: %r' , job_id ) return job_id
def write_batch_script ( self , n ) : self . n = n if self . batch_template_file and not self . batch_template : with open ( self . batch_template_file ) as f : self . batch_template = f . read ( ) if not self . batch_template : self . batch_template = self . default_template if not self . job_array_regexp . search ( self . batch_template ) : self . log . debug ( "adding job array settings to batch script" ) firstline , rest = self . batch_template . split ( '\n' , 1 ) self . batch_template = u'\n' . join ( [ firstline , self . job_array_template , rest ] ) if self . queue and not self . queue_regexp . search ( self . batch_template ) : self . log . debug ( "adding PBS queue settings to batch script" ) firstline , rest = self . batch_template . split ( '\n' , 1 ) self . batch_template = u'\n' . join ( [ firstline , self . queue_template , rest ] ) script_as_string = self . formatter . format ( self . batch_template , * * self . context ) self . log . debug ( 'Writing batch script: %s' , self . batch_file ) with open ( self . batch_file , 'w' ) as f : f . write ( script_as_string ) os . chmod ( self . batch_file , stat . S_IRUSR | stat . S_IWUSR | stat . S_IXUSR )
def start ( self , n ) : self . log . debug ( "Starting %s: %r" , self . __class__ . __name__ , self . args ) self . write_batch_script ( n ) output = check_output ( self . args , env = os . environ ) job_id = self . parse_job_id ( output ) self . notify_start ( job_id ) return job_id
def _context_menu_make ( self , pos ) : format = self . _control . cursorForPosition ( pos ) . charFormat ( ) name = format . stringProperty ( QtGui . QTextFormat . ImageName ) if name : menu = QtGui . QMenu ( ) menu . addAction ( 'Copy Image' , lambda : self . _copy_image ( name ) ) menu . addAction ( 'Save Image As...' , lambda : self . _save_image ( name ) ) menu . addSeparator ( ) svg = self . _name_to_svg_map . get ( name , None ) if svg is not None : menu . addSeparator ( ) menu . addAction ( 'Copy SVG' , lambda : svg_to_clipboard ( svg ) ) menu . addAction ( 'Save SVG As...' , lambda : save_svg ( svg , self . _control ) ) else : menu = super ( RichIPythonWidget , self ) . _context_menu_make ( pos ) return menu
def _handle_pyout ( self , msg ) : if not self . _hidden and self . _is_from_this_session ( msg ) : content = msg [ 'content' ] prompt_number = content . get ( 'execution_count' , 0 ) data = content [ 'data' ] if data . has_key ( 'image/svg+xml' ) : self . _pre_image_append ( msg , prompt_number ) self . _append_svg ( data [ 'image/svg+xml' ] , True ) self . _append_html ( self . output_sep2 , True ) elif data . has_key ( 'image/png' ) : self . _pre_image_append ( msg , prompt_number ) self . _append_png ( decodestring ( data [ 'image/png' ] . encode ( 'ascii' ) ) , True ) self . _append_html ( self . output_sep2 , True ) elif data . has_key ( 'image/jpeg' ) and self . _jpg_supported : self . _pre_image_append ( msg , prompt_number ) self . _append_jpg ( decodestring ( data [ 'image/jpeg' ] . encode ( 'ascii' ) ) , True ) self . _append_html ( self . output_sep2 , True ) else : return super ( RichIPythonWidget , self ) . _handle_pyout ( msg )
def _handle_display_data ( self , msg ) : if not self . _hidden and self . _is_from_this_session ( msg ) : source = msg [ 'content' ] [ 'source' ] data = msg [ 'content' ] [ 'data' ] metadata = msg [ 'content' ] [ 'metadata' ] if data . has_key ( 'image/svg+xml' ) : self . log . debug ( "display: %s" , msg . get ( 'content' , '' ) ) svg = data [ 'image/svg+xml' ] self . _append_svg ( svg , True ) elif data . has_key ( 'image/png' ) : self . log . debug ( "display: %s" , msg . get ( 'content' , '' ) ) png = decodestring ( data [ 'image/png' ] . encode ( 'ascii' ) ) self . _append_png ( png , True ) elif data . has_key ( 'image/jpeg' ) and self . _jpg_supported : self . log . debug ( "display: %s" , msg . get ( 'content' , '' ) ) jpg = decodestring ( data [ 'image/jpeg' ] . encode ( 'ascii' ) ) self . _append_jpg ( jpg , True ) else : return super ( RichIPythonWidget , self ) . _handle_display_data ( msg )
def _append_jpg ( self , jpg , before_prompt = False ) : self . _append_custom ( self . _insert_jpg , jpg , before_prompt )
def _append_png ( self , png , before_prompt = False ) : self . _append_custom ( self . _insert_png , png , before_prompt )
def _append_svg ( self , svg , before_prompt = False ) : self . _append_custom ( self . _insert_svg , svg , before_prompt )
def _copy_image ( self , name ) : image = self . _get_image ( name ) QtGui . QApplication . clipboard ( ) . setImage ( image )
def _get_image ( self , name ) : document = self . _control . document ( ) image = document . resource ( QtGui . QTextDocument . ImageResource , QtCore . QUrl ( name ) ) return image
def _insert_img ( self , cursor , img , fmt ) : try : image = QtGui . QImage ( ) image . loadFromData ( img , fmt . upper ( ) ) except ValueError : self . _insert_plain_text ( cursor , 'Received invalid %s data.' % fmt ) else : format = self . _add_image ( image ) cursor . insertBlock ( ) cursor . insertImage ( format ) cursor . insertBlock ( )
def _insert_svg ( self , cursor , svg ) : try : image = svg_to_image ( svg ) except ValueError : self . _insert_plain_text ( cursor , 'Received invalid SVG data.' ) else : format = self . _add_image ( image ) self . _name_to_svg_map [ format . name ( ) ] = svg cursor . insertBlock ( ) cursor . insertImage ( format ) cursor . insertBlock ( )
def _save_image ( self , name , format = 'PNG' ) : dialog = QtGui . QFileDialog ( self . _control , 'Save Image' ) dialog . setAcceptMode ( QtGui . QFileDialog . AcceptSave ) dialog . setDefaultSuffix ( format . lower ( ) ) dialog . setNameFilter ( '%s file (*.%s)' % ( format , format . lower ( ) ) ) if dialog . exec_ ( ) : filename = dialog . selectedFiles ( ) [ 0 ] image = self . _get_image ( name ) image . save ( filename , format )
def _exit_now_changed ( self , name , old , new ) : if new : loop = ioloop . IOLoop . instance ( ) loop . add_timeout ( time . time ( ) + 0.1 , loop . stop )
def init_environment ( self ) : env = os . environ env [ 'TERM' ] = 'xterm-color' env [ 'CLICOLOR' ] = '1' env [ 'PAGER' ] = 'cat' env [ 'GIT_PAGER' ] = 'cat' install_payload_page ( )
def ask_exit ( self ) : self . exit_now = True payload = dict ( source = 'IPython.zmq.zmqshell.ZMQInteractiveShell.ask_exit' , exit = True , keepkernel = self . keepkernel_on_exit , ) self . payload_manager . write_payload ( payload )
def read ( self , filename ) : kwargs = { } if sys . version_info >= ( 3 , 2 ) : kwargs [ 'encoding' ] = "utf-8" return configparser . RawConfigParser . read ( self , filename , * * kwargs )
def from_environment ( self , env_var ) : env = os . environ . get ( env_var , '' ) if env : self . timid = ( '--timid' in env )
def from_args ( self , * * kwargs ) : for k , v in iitems ( kwargs ) : if v is not None : if k in self . MUST_BE_LIST and isinstance ( v , string_class ) : v = [ v ] setattr ( self , k , v )
def set_attr_from_config_option ( self , cp , attr , where , type_ = '' ) : section , option = where . split ( ":" ) if cp . has_option ( section , option ) : method = getattr ( cp , 'get' + type_ ) setattr ( self , attr , method ( section , option ) )
def delims ( self , delims ) : expr = '[' + '' . join ( '\\' + c for c in delims ) + ']' self . _delim_re = re . compile ( expr ) self . _delims = delims self . _delim_expr = expr
def split_line ( self , line , cursor_pos = None ) : l = line if cursor_pos is None else line [ : cursor_pos ] return self . _delim_re . split ( l ) [ - 1 ]
def _greedy_changed ( self , name , old , new ) : if new : self . splitter . delims = GREEDY_DELIMS else : self . splitter . delims = DELIMS if self . readline : self . readline . set_completer_delims ( self . splitter . delims )
def alias_matches ( self , text ) : main_text = self . text_until_cursor . lstrip ( ) if ' ' in main_text and not main_text . startswith ( 'sudo' ) : return [ ] text = os . path . expanduser ( text ) aliases = self . alias_table . keys ( ) if text == '' : return aliases else : return [ a for a in aliases if a . startswith ( text ) ]
def python_matches ( self , text ) : if "." in text : try : matches = self . attr_matches ( text ) if text . endswith ( '.' ) and self . omit__names : if self . omit__names == 1 : no__name = ( lambda txt : re . match ( r'.*\.__.*?__' , txt ) is None ) else : no__name = ( lambda txt : re . match ( r'.*\._.*?' , txt ) is None ) matches = filter ( no__name , matches ) except NameError : matches = [ ] else : matches = self . global_matches ( text ) return matches
def _match_one ( self , rec , tests ) : for key , test in tests . iteritems ( ) : if not test ( rec . get ( key , None ) ) : return False return True
def _match ( self , check ) : matches = [ ] tests = { } for k , v in check . iteritems ( ) : if isinstance ( v , dict ) : tests [ k ] = CompositeFilter ( v ) else : tests [ k ] = lambda o : o == v for rec in self . _records . itervalues ( ) : if self . _match_one ( rec , tests ) : matches . append ( copy ( rec ) ) return matches
def _extract_subdict ( self , rec , keys ) : d = { } d [ 'msg_id' ] = rec [ 'msg_id' ] for key in keys : d [ key ] = rec [ key ] return copy ( d )
def add_record ( self , msg_id , rec ) : if self . _records . has_key ( msg_id ) : raise KeyError ( "Already have msg_id %r" % ( msg_id ) ) self . _records [ msg_id ] = rec
def get_record ( self , msg_id ) : if not msg_id in self . _records : raise KeyError ( "No such msg_id %r" % ( msg_id ) ) return copy ( self . _records [ msg_id ] )
def drop_matching_records ( self , check ) : matches = self . _match ( check ) for m in matches : del self . _records [ m [ 'msg_id' ] ]
def get_history ( self ) : msg_ids = self . _records . keys ( ) return sorted ( msg_ids , key = lambda m : self . _records [ m ] [ 'submitted' ] )
def quiet ( self ) : try : cell = self . shell . history_manager . input_hist_parsed [ self . prompt_count ] if cell . rstrip ( ) . endswith ( ';' ) : return True except IndexError : pass return False
def update_user_ns ( self , result ) : if result is not self . shell . user_ns [ '_oh' ] : if len ( self . shell . user_ns [ '_oh' ] ) >= self . cache_size and self . do_full_cache : warn ( 'Output cache limit (currently ' + `self.cache_size` + ' entries) hit.\n' 'Flushing cache and resetting history counter...\n' 'The only history variables available will be _,__,___ and _1\n' 'with the current result.' ) self . flush ( ) if '_' not in __builtin__ . __dict__ : self . ___ = self . __ self . __ = self . _ self . _ = result self . shell . push ( { '_' : self . _ , '__' : self . __ , '___' : self . ___ } , interactive = False ) to_main = { } if self . do_full_cache : new_result = '_' + `self.prompt_count` to_main [ new_result ] = result self . shell . push ( to_main , interactive = False ) self . shell . user_ns [ '_oh' ] [ self . prompt_count ] = result
def log_output ( self , format_dict ) : if self . shell . logger . log_output : self . shell . logger . log_write ( format_dict [ 'text/plain' ] , 'output' ) self . shell . history_manager . output_hist_reprs [ self . prompt_count ] = format_dict [ 'text/plain' ]
def finish_displayhook ( self ) : io . stdout . write ( self . shell . separate_out2 ) io . stdout . flush ( )
def load_ipython_extension ( ip ) : global _loaded if not _loaded : plugin = StoreMagic ( shell = ip , config = ip . config ) ip . plugin_manager . register_plugin ( 'storemagic' , plugin ) _loaded = True
def raise_if_freezed ( self ) : if self . is_freezed : name = type ( self ) . __name__ raise InvalidOperationException ( 'obj {name} is freezed.' . format ( name = name ) )
def mysql_timestamp_converter ( s ) : if s [ 4 ] == '-' : return DateTime_or_None ( s ) s = s + "0" * ( 14 - len ( s ) ) parts = map ( int , filter ( None , ( s [ : 4 ] , s [ 4 : 6 ] , s [ 6 : 8 ] , s [ 8 : 10 ] , s [ 10 : 12 ] , s [ 12 : 14 ] ) ) ) try : return Timestamp ( * parts ) except ( SystemExit , KeyboardInterrupt ) : raise except : return None
def _eventloop_changed ( self , name , old , new ) : loop = ioloop . IOLoop . instance ( ) loop . add_timeout ( time . time ( ) + 0.1 , self . enter_eventloop )
def start ( self ) : self . shell . exit_now = False if self . control_stream : self . control_stream . on_recv ( self . dispatch_control , copy = False ) def make_dispatcher ( stream ) : def dispatcher ( msg ) : return self . dispatch_shell ( stream , msg ) return dispatcher for s in self . shell_streams : s . on_recv ( make_dispatcher ( s ) , copy = False )
def do_one_iteration ( self ) : if self . control_stream : self . control_stream . flush ( ) for stream in self . shell_streams : stream . flush ( zmq . POLLIN , 1 ) stream . flush ( zmq . POLLOUT )
def _publish_pyin ( self , code , parent , execution_count ) : self . session . send ( self . iopub_socket , u'pyin' , { u'code' : code , u'execution_count' : execution_count } , parent = parent , ident = self . _topic ( 'pyin' ) )
def abort_request ( self , stream , ident , parent ) : msg_ids = parent [ 'content' ] . get ( 'msg_ids' , None ) if isinstance ( msg_ids , basestring ) : msg_ids = [ msg_ids ] if not msg_ids : self . abort_queues ( ) for mid in msg_ids : self . aborted . add ( str ( mid ) ) content = dict ( status = 'ok' ) reply_msg = self . session . send ( stream , 'abort_reply' , content = content , parent = parent , ident = ident ) self . log . debug ( "%s" , reply_msg )
def clear_request ( self , stream , idents , parent ) : self . shell . reset ( False ) msg = self . session . send ( stream , 'clear_reply' , ident = idents , parent = parent , content = dict ( status = 'ok' ) )
def _topic ( self , topic ) : if self . int_id >= 0 : base = "engine.%i" % self . int_id else : base = "kernel.%s" % self . ident return py3compat . cast_bytes ( "%s.%s" % ( base , topic ) )
def _at_shutdown ( self ) : if self . _shutdown_message is not None : self . session . send ( self . iopub_socket , self . _shutdown_message , ident = self . _topic ( 'shutdown' ) ) self . log . debug ( "%s" , self . _shutdown_message ) [ s . flush ( zmq . POLLOUT ) for s in self . shell_streams ]
def init_gui_pylab ( self ) : shell = self . shell _showtraceback = shell . _showtraceback try : def print_tb ( etype , evalue , stb ) : print ( "GUI event loop or pylab initialization failed" , file = io . stderr ) print ( shell . InteractiveTB . stb2text ( stb ) , file = io . stderr ) shell . _showtraceback = print_tb InteractiveShellApp . init_gui_pylab ( self ) finally : shell . _showtraceback = _showtraceback
def beforeContext ( self ) : mods = sys . modules . copy ( ) self . _mod_stack . append ( mods )
def virtual_memory ( ) : total , active , inactive , wired , free = _psutil_osx . get_virtual_mem ( ) avail = inactive + free used = active + inactive + wired percent = usage_percent ( ( total - avail ) , total , _round = 1 ) return nt_virtmem_info ( total , avail , percent , used , free , active , inactive , wired )
def get_system_cpu_times ( ) : user , nice , system , idle = _psutil_osx . get_system_cpu_times ( ) return _cputimes_ntuple ( user , nice , system , idle )
def get_system_per_cpu_times ( ) : ret = [ ] for cpu_t in _psutil_osx . get_system_per_cpu_times ( ) : user , nice , system , idle = cpu_t item = _cputimes_ntuple ( user , nice , system , idle ) ret . append ( item ) return ret
def get_process_cmdline ( self ) : if not pid_exists ( self . pid ) : raise NoSuchProcess ( self . pid , self . _process_name ) return _psutil_osx . get_process_cmdline ( self . pid )
def get_memory_info ( self ) : rss , vms = _psutil_osx . get_process_memory_info ( self . pid ) [ : 2 ] return nt_meminfo ( rss , vms )
def get_ext_memory_info ( self ) : rss , vms , pfaults , pageins = _psutil_osx . get_process_memory_info ( self . pid ) return self . _nt_ext_mem ( rss , vms , pfaults * _PAGESIZE , pageins * _PAGESIZE )
def get_open_files ( self ) : if self . pid == 0 : return [ ] files = [ ] rawlist = _psutil_osx . get_process_open_files ( self . pid ) for path , fd in rawlist : if isfile_strict ( path ) : ntuple = nt_openfile ( path , fd ) files . append ( ntuple ) return files
def usage_percent ( used , total , _round = None ) : try : ret = ( used / total ) * 100 except ZeroDivisionError : ret = 0 if _round is not None : return round ( ret , _round ) else : return ret
def memoize ( f ) : cache = { } def memf ( * x ) : if x not in cache : cache [ x ] = f ( * x ) return cache [ x ] return memf
def deprecated ( replacement = None ) : def outer ( fun ) : msg = "psutil.%s is deprecated" % fun . __name__ if replacement is not None : msg += "; use %s instead" % replacement if fun . __doc__ is None : fun . __doc__ = msg @ wraps ( fun ) def inner ( * args , * * kwargs ) : warnings . warn ( msg , category = DeprecationWarning , stacklevel = 2 ) return fun ( * args , * * kwargs ) return inner return outer
def _login ( self ) : try : self . gd_client = gdata . docs . client . DocsClient ( ) self . gd_client . ClientLogin ( self . email , self . password , self . source ) except RequestError as e : raise PODocsError ( e )
def _get_gdocs_key ( self ) : try : args = urlparse . parse_qs ( urlparse . urlparse ( self . url ) . query ) self . key = args [ 'key' ] [ 0 ] except KeyError as e : raise PODocsError ( e )
def _ensure_temp_path_exists ( self ) : try : if not os . path . exists ( self . temp_path ) : os . mkdir ( self . temp_path ) except OSError as e : raise PODocsError ( e )
def download ( self ) : trans_csv_path = os . path . realpath ( os . path . join ( self . temp_path , GDOCS_TRANS_CSV ) ) meta_csv_path = os . path . realpath ( os . path . join ( self . temp_path , GDOCS_META_CSV ) ) self . _download_csv_from_gdocs ( trans_csv_path , meta_csv_path ) try : csv_to_po ( trans_csv_path , meta_csv_path , self . locale_root , self . po_files_path , header = self . header ) except IOError as e : raise PODocsError ( e ) self . _clear_temp ( )
def clear ( self ) : empty_file_path = os . path . join ( self . temp_path , 'empty.csv' ) try : empty_file = open ( empty_file_path , 'w' ) empty_file . write ( ',' ) empty_file . close ( ) except IOError as e : raise PODocsError ( e ) self . _upload_file_to_gdoc ( empty_file_path , content_type = 'text/csv' ) os . remove ( empty_file_path )
def new_qt_console ( self , evt = None ) : return connect_qtconsole ( self . ipkernel . connection_file , profile = self . ipkernel . profile )
def url_has_contents ( url , contents , case_sensitive = False , timeout = 10 ) : try : req = urllib2 . urlopen ( url , timeout = timeout ) except Exception , _ : False else : rep = req . read ( ) if ( not case_sensitive and rep . lower ( ) . find ( contents . lower ( ) ) >= 0 ) or ( case_sensitive and rep . find ( contents ) >= 0 ) : return True else : return False
def get_response_code ( url , timeout = 10 ) : try : req = urllib2 . urlopen ( url , timeout = timeout ) except HTTPError , e : return e . getcode ( ) except Exception , _ : fail ( "Couldn't reach the URL '%s'" % url ) else : return req . getcode ( )
def clear_output ( self , stdout = True , stderr = True , other = True ) : if stdout : print ( '\033[2K\r' , file = io . stdout , end = '' ) io . stdout . flush ( ) if stderr : print ( '\033[2K\r' , file = io . stderr , end = '' ) io . stderr . flush ( )
def source_file ( self ) : if os . path . exists ( self . filename ) : return open_source ( self . filename ) source = self . file_locator . get_zip_data ( self . filename ) if source is not None : return StringIO ( source ) raise CoverageException ( "No source for code '%s'." % self . filename )
def _total_seconds ( td ) : try : return td . total_seconds ( ) except AttributeError : return 1e-6 * ( td . microseconds + ( td . seconds + td . days * 24 * 3600 ) * 10 ** 6 )
def abort ( self ) : assert not self . ready ( ) , "Can't abort, I am already done!" return self . _client . abort ( self . msg_ids , targets = self . _targets , block = True )
def elapsed ( self ) : if self . ready ( ) : return self . wall_time now = submitted = datetime . now ( ) for msg_id in self . msg_ids : if msg_id in self . _client . metadata : stamp = self . _client . metadata [ msg_id ] [ 'submitted' ] if stamp and stamp < submitted : submitted = stamp return _total_seconds ( now - submitted )
def wait_interactive ( self , interval = 1. , timeout = None ) : N = len ( self ) tic = time . time ( ) while not self . ready ( ) and ( timeout is None or time . time ( ) - tic <= timeout ) : self . wait ( interval ) clear_output ( ) print ( "%4i/%i tasks finished after %4i s" % ( self . progress , N , self . elapsed ) , end = "" ) sys . stdout . flush ( ) print ( ) print ( "done" )
def _republish_displaypub ( self , content , eid ) : try : ip = get_ipython ( ) except NameError : return md = content [ 'metadata' ] or { } md [ 'engine' ] = eid ip . display_pub . publish ( content [ 'source' ] , content [ 'data' ] , md )
def _wait_for_outputs ( self , timeout = - 1 ) : if not self . _success : return tic = time . time ( ) while not all ( md [ 'outputs_ready' ] for md in self . _metadata ) : time . sleep ( 0.01 ) self . _client . _flush_iopub ( self . _client . _iopub_socket ) if timeout >= 0 and time . time ( ) > tic + timeout : break
def _unordered_iter ( self ) : try : rlist = self . get ( 0 ) except error . TimeoutError : pending = set ( self . msg_ids ) while pending : try : self . _client . wait ( pending , 1e-3 ) except error . TimeoutError : pass ready = pending . difference ( self . _client . outstanding ) pending = pending . difference ( ready ) while ready : msg_id = ready . pop ( ) ar = AsyncResult ( self . _client , msg_id , self . _fname ) rlist = ar . get ( ) try : for r in rlist : yield r except TypeError : yield rlist else : for r in rlist : yield r
def wait ( self , timeout = - 1 ) : start = time . time ( ) if self . _ready : return local_ids = filter ( lambda msg_id : msg_id in self . _client . outstanding , self . msg_ids ) local_ready = self . _client . wait ( local_ids , timeout ) if local_ready : remote_ids = filter ( lambda msg_id : msg_id not in self . _client . results , self . msg_ids ) if not remote_ids : self . _ready = True else : rdict = self . _client . result_status ( remote_ids , status_only = False ) pending = rdict [ 'pending' ] while pending and ( timeout < 0 or time . time ( ) < start + timeout ) : rdict = self . _client . result_status ( remote_ids , status_only = False ) pending = rdict [ 'pending' ] if pending : time . sleep ( 0.1 ) if not pending : self . _ready = True if self . _ready : try : results = map ( self . _client . results . get , self . msg_ids ) self . _result = results if self . _single_result : r = results [ 0 ] if isinstance ( r , Exception ) : raise r else : results = error . collect_exceptions ( results , self . _fname ) self . _result = self . _reconstruct_result ( results ) except Exception , e : self . _exception = e self . _success = False else : self . _success = True finally : self . _metadata = map ( self . _client . metadata . get , self . msg_ids )
def abs_file ( filename ) : path = os . path . expandvars ( os . path . expanduser ( filename ) ) path = os . path . abspath ( os . path . realpath ( path ) ) path = actual_path ( path ) return path
def sep ( s ) : sep_match = re . search ( r"[\\/]" , s ) if sep_match : the_sep = sep_match . group ( 0 ) else : the_sep = os . sep return the_sep
def match ( self , fpath ) : for d in self . dirs : if fpath . startswith ( d ) : if fpath == d : return True if fpath [ len ( d ) ] == os . sep : return True return False
def match ( self , fpath ) : for pat in self . pats : if fnmatch . fnmatch ( fpath , pat ) : return True return False
def loop_qt4 ( kernel ) : from IPython . external . qt_for_kernel import QtCore from IPython . lib . guisupport import get_app_qt4 , start_event_loop_qt4 kernel . app = get_app_qt4 ( [ " " ] ) kernel . app . setQuitOnLastWindowClosed ( False ) kernel . timer = QtCore . QTimer ( ) kernel . timer . timeout . connect ( kernel . do_one_iteration ) kernel . timer . start ( 1000 * kernel . _poll_interval ) start_event_loop_qt4 ( kernel . app )
def loop_wx ( kernel ) : import wx from IPython . lib . guisupport import start_event_loop_wx doi = kernel . do_one_iteration poll_interval = int ( 1000 * kernel . _poll_interval ) class TimerFrame ( wx . Frame ) : def __init__ ( self , func ) : wx . Frame . __init__ ( self , None , - 1 ) self . timer = wx . Timer ( self ) self . timer . Start ( poll_interval ) self . Bind ( wx . EVT_TIMER , self . on_timer ) self . func = func def on_timer ( self , event ) : self . func ( ) class IPWxApp ( wx . App ) : def OnInit ( self ) : self . frame = TimerFrame ( doi ) self . frame . Show ( False ) return True kernel . app = IPWxApp ( redirect = False ) import signal if not callable ( signal . getsignal ( signal . SIGINT ) ) : signal . signal ( signal . SIGINT , signal . default_int_handler ) start_event_loop_wx ( kernel . app )
def loop_tk ( kernel ) : import Tkinter doi = kernel . do_one_iteration poll_interval = int ( 1000 * kernel . _poll_interval ) class Timer ( object ) : def __init__ ( self , func ) : self . app = Tkinter . Tk ( ) self . app . withdraw ( ) self . func = func def on_timer ( self ) : self . func ( ) self . app . after ( poll_interval , self . on_timer ) def start ( self ) : self . on_timer ( ) self . app . mainloop ( ) kernel . timer = Timer ( doi ) kernel . timer . start ( )
def loop_gtk ( kernel ) : from . gui . gtkembed import GTKEmbed gtk_kernel = GTKEmbed ( kernel ) gtk_kernel . start ( )
def enable_gui ( gui , kernel = None ) : if gui not in loop_map : raise ValueError ( "GUI %r not supported" % gui ) if kernel is None : if Application . initialized ( ) : kernel = getattr ( Application . instance ( ) , 'kernel' , None ) if kernel is None : raise RuntimeError ( "You didn't specify a kernel," " and no IPython Application with a kernel appears to be running." ) loop = loop_map [ gui ] if kernel . eventloop is not None and kernel . eventloop is not loop : raise RuntimeError ( "Cannot activate multiple GUI eventloops" ) kernel . eventloop = loop
def GOE ( N ) : m = ra . standard_normal ( ( N , N ) ) m += m . T return m / 2
def center_eigenvalue_diff ( mat ) : N = len ( mat ) evals = np . sort ( la . eigvals ( mat ) ) diff = np . abs ( evals [ N / 2 ] - evals [ N / 2 - 1 ] ) return diff
def ensemble_diffs ( num , N ) : diffs = np . empty ( num ) for i in xrange ( num ) : mat = GOE ( N ) diffs [ i ] = center_eigenvalue_diff ( mat ) return diffs
def init_crash_handler ( self ) : self . crash_handler = self . crash_handler_class ( self ) sys . excepthook = self . excepthook def unset_crashhandler ( ) : sys . excepthook = sys . __excepthook__ atexit . register ( unset_crashhandler )
def init_profile_dir ( self ) : try : location = self . config . ProfileDir . location except AttributeError : try : p = ProfileDir . find_profile_dir_by_name ( self . ipython_dir , self . profile , self . config ) except ProfileDirError : if self . auto_create or self . profile == 'default' : try : p = ProfileDir . create_profile_dir_by_name ( self . ipython_dir , self . profile , self . config ) except ProfileDirError : self . log . fatal ( "Could not create profile: %r" % self . profile ) self . exit ( 1 ) else : self . log . info ( "Created profile dir: %r" % p . location ) else : self . log . fatal ( "Profile %r not found." % self . profile ) self . exit ( 1 ) else : self . log . info ( "Using existing profile dir: %r" % p . location ) else : try : p = ProfileDir . find_profile_dir ( location , self . config ) except ProfileDirError : if self . auto_create : try : p = ProfileDir . create_profile_dir ( location , self . config ) except ProfileDirError : self . log . fatal ( "Could not create profile directory: %r" % location ) self . exit ( 1 ) else : self . log . info ( "Creating new profile dir: %r" % location ) else : self . log . fatal ( "Profile directory %r not found." % location ) self . exit ( 1 ) else : self . log . info ( "Using existing profile dir: %r" % location ) self . profile_dir = p self . config_file_paths . append ( p . location )
def stage_default_config_file ( self ) : s = self . generate_config_file ( ) fname = os . path . join ( self . profile_dir . location , self . config_file_name ) if self . overwrite or not os . path . exists ( fname ) : self . log . warn ( "Generating default config file: %r" % ( fname ) ) with open ( fname , 'w' ) as f : f . write ( s )
def erase ( self ) : if self . use_file : if self . filename : file_be_gone ( self . filename ) self . lines = { } self . arcs = { }
def line_data ( self ) : return dict ( [ ( f , sorted ( lmap . keys ( ) ) ) for f , lmap in iitems ( self . lines ) ] )
def arc_data ( self ) : return dict ( [ ( f , sorted ( amap . keys ( ) ) ) for f , amap in iitems ( self . arcs ) ] )
def write_file ( self , filename ) : data = { } data [ 'lines' ] = self . line_data ( ) arcs = self . arc_data ( ) if arcs : data [ 'arcs' ] = arcs if self . collector : data [ 'collector' ] = self . collector if self . debug and self . debug . should ( 'dataio' ) : self . debug . write ( "Writing data to %r" % ( filename , ) ) fdata = open ( filename , 'wb' ) try : pickle . dump ( data , fdata , 2 ) finally : fdata . close ( )
def read_file ( self , filename ) : self . lines , self . arcs = self . _read_file ( filename )
def raw_data ( self , filename ) : if self . debug and self . debug . should ( 'dataio' ) : self . debug . write ( "Reading data from %r" % ( filename , ) ) fdata = open ( filename , 'rb' ) try : data = pickle . load ( fdata ) finally : fdata . close ( ) return data
def add_to_hash ( self , filename , hasher ) : hasher . update ( self . executed_lines ( filename ) ) hasher . update ( self . executed_arcs ( filename ) )
def get_pasted_lines ( sentinel , l_input = py3compat . input ) : print "Pasting code; enter '%s' alone on the line to stop or use Ctrl-D." % sentinel while True : try : l = l_input ( ':' ) if l == sentinel : return else : yield l except EOFError : print '<EOF>' return
def _replace_rlhist_multiline ( self , source_raw , hlen_before_cell ) : if not self . has_readline or not self . multiline_history : return hlen_before_cell if not hasattr ( self . readline , "remove_history_item" ) : return hlen_before_cell if not source_raw . rstrip ( ) : return hlen_before_cell hlen = self . readline . get_current_history_length ( ) if hlen == hlen_before_cell : return hlen_before_cell for i in range ( hlen - hlen_before_cell ) : self . readline . remove_history_item ( hlen - i - 1 ) stdin_encoding = get_stream_enc ( sys . stdin , 'utf-8' ) self . readline . add_history ( py3compat . unicode_to_str ( source_raw . rstrip ( ) , stdin_encoding ) ) return self . readline . get_current_history_length ( )
def interact ( self , display_banner = None ) : if self . exit_now : return if display_banner is None : display_banner = self . display_banner if isinstance ( display_banner , basestring ) : self . show_banner ( display_banner ) elif display_banner : self . show_banner ( ) more = False if self . has_readline : self . readline_startup_hook ( self . pre_readline ) hlen_b4_cell = self . readline . get_current_history_length ( ) else : hlen_b4_cell = 0 while not self . exit_now : self . hooks . pre_prompt_hook ( ) if more : try : prompt = self . prompt_manager . render ( 'in2' ) except : self . showtraceback ( ) if self . autoindent : self . rl_do_indent = True else : try : prompt = self . separate_in + self . prompt_manager . render ( 'in' ) except : self . showtraceback ( ) try : line = self . raw_input ( prompt ) if self . exit_now : break if self . autoindent : self . rl_do_indent = False except KeyboardInterrupt : #double-guard against keyboardinterrupts during kbdint handling try : self . write ( '\nKeyboardInterrupt\n' ) source_raw = self . input_splitter . source_raw_reset ( ) [ 1 ] hlen_b4_cell = self . _replace_rlhist_multiline ( source_raw , hlen_b4_cell ) more = False except KeyboardInterrupt : pass except EOFError : if self . autoindent : self . rl_do_indent = False if self . has_readline : self . readline_startup_hook ( None ) self . write ( '\n' ) self . exit ( ) except bdb . BdbQuit : warn ( 'The Python debugger has exited with a BdbQuit exception.\n' 'Because of how pdb handles the stack, it is impossible\n' 'for IPython to properly format this particular exception.\n' 'IPython will resume normal operation.' ) except : self . showtraceback ( ) else : self . input_splitter . push ( line ) more = self . input_splitter . push_accepts_more ( ) if ( self . SyntaxTB . last_syntax_error and self . autoedit_syntax ) : self . edit_syntax_error ( ) if not more : source_raw = self . input_splitter . source_raw_reset ( ) [ 1 ] self . run_cell ( source_raw , store_history = True ) hlen_b4_cell = self . _replace_rlhist_multiline ( source_raw , hlen_b4_cell ) self . exit_now = False
def _should_recompile ( self , e ) : if e . filename in ( '<ipython console>' , '<input>' , '<string>' , '<console>' , '<BackgroundJob compilation>' , None ) : return False try : if ( self . autoedit_syntax and not self . ask_yes_no ( 'Return to editor to correct syntax error? ' '[Y/n] ' , 'y' ) ) : return False except EOFError : return False def int0 ( x ) : try : return int ( x ) except TypeError : return 0 try : self . hooks . fix_error_editor ( e . filename , int0 ( e . lineno ) , int0 ( e . offset ) , e . msg ) except TryNext : warn ( 'Could not open editor' ) return False return True
def new_frontend_master ( self ) : ip = self . ip if self . ip in LOCAL_IPS else LOCALHOST kernel_manager = self . kernel_manager_class ( ip = ip , connection_file = self . _new_connection_file ( ) , config = self . config , ) kwargs = dict ( ) kwargs [ 'extra_arguments' ] = self . kernel_argv kernel_manager . start_kernel ( * * kwargs ) kernel_manager . start_channels ( ) widget = self . widget_factory ( config = self . config , local_kernel = True ) self . init_colors ( widget ) widget . kernel_manager = kernel_manager widget . _existing = False widget . _may_close = True widget . _confirm_exit = self . confirm_exit return widget
def init_colors ( self , widget ) : try : colors = self . config . ZMQInteractiveShell . colors except AttributeError : colors = None try : style = self . config . IPythonWidget . syntax_style except AttributeError : style = None try : sheet = self . config . IPythonWidget . style_sheet except AttributeError : sheet = None if colors : colors = colors . lower ( ) if colors in ( 'lightbg' , 'light' ) : colors = 'lightbg' elif colors in ( 'dark' , 'linux' ) : colors = 'linux' else : colors = 'nocolor' elif style : if style == 'bw' : colors = 'nocolor' elif styles . dark_style ( style ) : colors = 'linux' else : colors = 'lightbg' else : colors = None if style : widget . style_sheet = styles . sheet_from_template ( style , colors ) widget . syntax_style = style widget . _syntax_style_changed ( ) widget . _style_sheet_changed ( ) elif colors : widget . set_default_style ( colors = colors ) if self . stylesheet : if os . path . isfile ( self . stylesheet ) : with open ( self . stylesheet ) as f : sheet = f . read ( ) else : raise IOError ( "Stylesheet %r not found." % self . stylesheet ) if sheet : widget . style_sheet = sheet widget . _style_sheet_changed ( )
def info ( self ) : return ( self . identity , self . url , self . pub_url , self . location )
def set_colors ( self , * args , * * kw ) : self . color_scheme_table . set_active_scheme ( * args , * * kw ) self . Colors = self . color_scheme_table . active_colors if hasattr ( self , 'pdb' ) and self . pdb is not None : self . pdb . set_colors ( * args , * * kw )
def color_toggle ( self ) : if self . color_scheme_table . active_scheme_name == 'NoColor' : self . color_scheme_table . set_active_scheme ( self . old_scheme ) self . Colors = self . color_scheme_table . active_colors else : self . old_scheme = self . color_scheme_table . active_scheme_name self . color_scheme_table . set_active_scheme ( 'NoColor' ) self . Colors = self . color_scheme_table . active_colors
def group_required ( group , login_url = None , redirect_field_name = REDIRECT_FIELD_NAME , skip_superuser = True ) : def decorator ( view_func ) : @ login_required ( redirect_field_name = redirect_field_name , login_url = login_url ) def _wrapped_view ( request , * args , * * kwargs ) : if not ( request . user . is_superuser and skip_superuser ) : if request . user . groups . filter ( name = group ) . count ( ) == 0 : raise PermissionDenied return view_func ( request , * args , * * kwargs ) return _wrapped_view return decorator
def add_submodule ( mod , submod , fullname , subname ) : if mod is None : return #Nothing to do here. if submod is None : submod = sys . modules [ fullname ] setattr ( mod , subname , submod ) return
def ensure_fromlist ( mod , fromlist , buf , recursive ) : if not hasattr ( mod , '__path__' ) : return for item in fromlist : if not hasattr ( item , 'rindex' ) : raise TypeError ( "Item in ``from list'' not a string" ) if item == '*' : if recursive : continue try : all = mod . __all__ except AttributeError : pass else : ret = ensure_fromlist ( mod , all , buf , 1 ) if not ret : return 0 elif not hasattr ( mod , item ) : import_submodule ( mod , item , buf + '.' + item )
def add_section ( self ) : sect = CodeBuilder ( self . indent_amount ) self . code . append ( sect ) return sect
def get_function ( self , fn_name ) : assert self . indent_amount == 0 g = { } code_text = str ( self ) exec ( code_text , g ) return g [ fn_name ]
def expr_code ( self , expr ) : if "|" in expr : pipes = expr . split ( "|" ) code = self . expr_code ( pipes [ 0 ] ) for func in pipes [ 1 : ] : self . all_vars . add ( func ) code = "c_%s(%s)" % ( func , code ) elif "." in expr : dots = expr . split ( "." ) code = self . expr_code ( dots [ 0 ] ) args = [ repr ( d ) for d in dots [ 1 : ] ] code = "dot(%s, %s)" % ( code , ", " . join ( args ) ) else : self . all_vars . add ( expr ) code = "c_%s" % expr return code
def do_dots ( self , value , * dots ) : for dot in dots : try : value = getattr ( value , dot ) except AttributeError : value = value [ dot ] if hasattr ( value , '__call__' ) : value = value ( ) return value
def _formatters_default ( self ) : formatter_classes = [ PlainTextFormatter , HTMLFormatter , SVGFormatter , PNGFormatter , JPEGFormatter , LatexFormatter , JSONFormatter , JavascriptFormatter ] d = { } for cls in formatter_classes : f = cls ( config = self . config ) d [ f . format_type ] = f return d
def user_config_files ( ) : return filter ( os . path . exists , map ( os . path . expanduser , config_files ) )
def configureWhere ( self , where ) : from nose . importer import add_path self . workingDir = None where = tolist ( where ) warned = False for path in where : if not self . workingDir : abs_path = absdir ( path ) if abs_path is None : raise ValueError ( "Working directory %s not found, or " "not a directory" % path ) log . info ( "Set working dir to %s" , abs_path ) self . workingDir = abs_path if self . addPaths and os . path . exists ( os . path . join ( abs_path , '__init__.py' ) ) : log . info ( "Working directory %s is a package; " "adding to sys.path" % abs_path ) add_path ( abs_path ) continue if not warned : warn ( "Use of multiple -w arguments is deprecated and " "support may be removed in a future release. You can " "get the same behavior by passing directories without " "the -w argument on the command line, or by using the " "--tests argument in a configuration file." , DeprecationWarning ) self . testNames . append ( path )
def page_file ( fname , start = 0 , pager_cmd = None ) : pager_cmd = get_pager_cmd ( pager_cmd ) pager_cmd += ' ' + get_pager_start ( pager_cmd , start ) try : if os . environ [ 'TERM' ] in [ 'emacs' , 'dumb' ] : raise EnvironmentError system ( pager_cmd + ' ' + fname ) except : try : if start > 0 : start -= 1 page ( open ( fname ) . read ( ) , start ) except : print 'Unable to show file' , `fname`
def print_basic_unicode ( o , p , cycle ) : if cycle : return p . text ( 'Basic(...)' ) out = pretty ( o , use_unicode = True ) if '\n' in out : p . text ( u'\n' ) p . text ( out )
def print_png ( o ) : s = latex ( o , mode = 'inline' ) s = s . replace ( '\\operatorname' , '' ) s = s . replace ( '\\overline' , '\\bar' ) png = latex_to_png ( s ) return png
def print_display_png ( o ) : s = latex ( o , mode = 'plain' ) s = s . strip ( '$' ) png = latex_to_png ( '$$%s$$' % s , backend = 'dvipng' ) return png
def load_ipython_extension ( ip ) : import sympy try : import sympy . interactive . ipythonprinting except ImportError : pass else : warnings . warn ( "The sympyprinting extension in IPython is deprecated, " "use sympy.interactive.ipythonprinting" ) ip . extension_manager . load_extension ( 'sympy.interactive.ipythonprinting' ) return global _loaded if not _loaded : plaintext_formatter = ip . display_formatter . formatters [ 'text/plain' ] for cls in ( object , str ) : plaintext_formatter . for_type ( cls , print_basic_unicode ) printable_containers = [ list , tuple ] if sympy . __version__ > '0.7.1' : printable_containers += [ set , frozenset ] else : plaintext_formatter . for_type ( cls , print_basic_unicode ) plaintext_formatter . for_type_by_name ( 'sympy.core.basic' , 'Basic' , print_basic_unicode ) plaintext_formatter . for_type_by_name ( 'sympy.matrices.matrices' , 'Matrix' , print_basic_unicode ) png_formatter = ip . display_formatter . formatters [ 'image/png' ] png_formatter . for_type_by_name ( 'sympy.core.basic' , 'Basic' , print_png ) png_formatter . for_type_by_name ( 'sympy.matrices.matrices' , 'Matrix' , print_display_png ) for cls in [ dict , int , long , float ] + printable_containers : png_formatter . for_type ( cls , print_png ) latex_formatter = ip . display_formatter . formatters [ 'text/latex' ] latex_formatter . for_type_by_name ( 'sympy.core.basic' , 'Basic' , print_latex ) latex_formatter . for_type_by_name ( 'sympy.matrices.matrices' , 'Matrix' , print_latex ) for cls in printable_containers : latex_formatter . for_type ( cls , print_latex ) _loaded = True
def _run_loop ( self ) : while True : try : self . ioloop . start ( ) except ZMQError as e : if e . errno == errno . EINTR : continue else : raise except Exception : if self . _exiting : break else : raise else : break
def input ( self , string ) : content = dict ( value = string ) msg = self . session . msg ( 'input_reply' , content ) self . _queue_send ( msg )
def stop_channels ( self ) : if self . shell_channel . is_alive ( ) : self . shell_channel . stop ( ) if self . sub_channel . is_alive ( ) : self . sub_channel . stop ( ) if self . stdin_channel . is_alive ( ) : self . stdin_channel . stop ( ) if self . hb_channel . is_alive ( ) : self . hb_channel . stop ( )
def channels_running ( self ) : return ( self . shell_channel . is_alive ( ) or self . sub_channel . is_alive ( ) or self . stdin_channel . is_alive ( ) or self . hb_channel . is_alive ( ) )
def load_connection_file ( self ) : with open ( self . connection_file ) as f : cfg = json . loads ( f . read ( ) ) self . ip = cfg [ 'ip' ] self . shell_port = cfg [ 'shell_port' ] self . stdin_port = cfg [ 'stdin_port' ] self . iopub_port = cfg [ 'iopub_port' ] self . hb_port = cfg [ 'hb_port' ] self . session . key = str_to_bytes ( cfg [ 'key' ] )
def write_connection_file ( self ) : if self . _connection_file_written : return self . connection_file , cfg = write_connection_file ( self . connection_file , ip = self . ip , key = self . session . key , stdin_port = self . stdin_port , iopub_port = self . iopub_port , shell_port = self . shell_port , hb_port = self . hb_port ) self . shell_port = cfg [ 'shell_port' ] self . stdin_port = cfg [ 'stdin_port' ] self . iopub_port = cfg [ 'iopub_port' ] self . hb_port = cfg [ 'hb_port' ] self . _connection_file_written = True
def kill_kernel ( self ) : if self . has_kernel : if self . _hb_channel is not None : self . _hb_channel . pause ( ) try : self . kernel . kill ( ) except OSError , e : if sys . platform == 'win32' : if e . winerror != 5 : raise else : from errno import ESRCH if e . errno != ESRCH : raise self . kernel = None else : raise RuntimeError ( "Cannot kill kernel. No kernel is running!" )
def is_alive ( self ) : if self . has_kernel : if self . kernel . poll ( ) is None : return True else : return False elif self . _hb_channel is not None : return self . _hb_channel . is_beating ( ) else : return True
def shell_channel ( self ) : if self . _shell_channel is None : self . _shell_channel = self . shell_channel_class ( self . context , self . session , ( self . ip , self . shell_port ) ) return self . _shell_channel
def sub_channel ( self ) : if self . _sub_channel is None : self . _sub_channel = self . sub_channel_class ( self . context , self . session , ( self . ip , self . iopub_port ) ) return self . _sub_channel
def walk_egg ( egg_dir ) : walker = os . walk ( egg_dir ) base , dirs , files = walker . next ( ) if 'EGG-INFO' in dirs : dirs . remove ( 'EGG-INFO' ) yield base , dirs , files for bdf in walker : yield bdf
def scan_module ( egg_dir , base , name , stubs ) : filename = os . path . join ( base , name ) if filename [ : - 1 ] in stubs : return True pkg = base [ len ( egg_dir ) + 1 : ] . replace ( os . sep , '.' ) module = pkg + ( pkg and '.' or '' ) + os . path . splitext ( name ) [ 0 ] if sys . version_info < ( 3 , 3 ) : skip = 8 else : skip = 12 f = open ( filename , 'rb' ) f . read ( skip ) code = marshal . load ( f ) f . close ( ) safe = True symbols = dict . fromkeys ( iter_symbols ( code ) ) for bad in [ '__file__' , '__path__' ] : if bad in symbols : log . warn ( "%s: module references %s" , module , bad ) safe = False if 'inspect' in symbols : for bad in [ 'getsource' , 'getabsfile' , 'getsourcefile' , 'getfile' 'getsourcelines' , 'findsource' , 'getcomments' , 'getframeinfo' , 'getinnerframes' , 'getouterframes' , 'stack' , 'trace' ] : if bad in symbols : log . warn ( "%s: module MAY be using inspect.%s" , module , bad ) safe = False if '__name__' in symbols and '__main__' in symbols and '.' not in module : if sys . version [ : 3 ] == "2.4" : log . warn ( "%s: top-level module may be 'python -m' script" , module ) safe = False return safe
def make_init_files ( self ) : init_files = [ ] for base , dirs , files in walk_egg ( self . bdist_dir ) : if base == self . bdist_dir : continue for name in files : if name . endswith ( '.py' ) : if '__init__.py' not in files : pkg = base [ len ( self . bdist_dir ) + 1 : ] . replace ( os . sep , '.' ) if self . distribution . has_contents_for ( pkg ) : log . warn ( "Creating missing __init__.py for %s" , pkg ) filename = os . path . join ( base , '__init__.py' ) if not self . dry_run : f = open ( filename , 'w' ) f . write ( NS_PKG_STUB ) f . close ( ) init_files . append ( filename ) break else : dirs [ : ] = [ ] return init_files
def launch_new_instance ( ) : if sys . platform == 'win32' : import multiprocessing p = multiprocessing . current_process ( ) if p . name != 'MainProcess' : return app = IPControllerApp . instance ( ) app . initialize ( ) app . start ( )
def save_connection_dict ( self , fname , cdict ) : c = self . config url = cdict [ 'url' ] location = cdict [ 'location' ] if not location : try : proto , ip , port = split_url ( url ) except AssertionError : pass else : try : location = socket . gethostbyname_ex ( socket . gethostname ( ) ) [ 2 ] [ - 1 ] except ( socket . gaierror , IndexError ) : self . log . warn ( "Could not identify this machine's IP, assuming 127.0.0.1." " You may need to specify '--location=<external_ip_address>' to help" " IPython decide when to connect via loopback." ) location = '127.0.0.1' cdict [ 'location' ] = location fname = os . path . join ( self . profile_dir . security_dir , fname ) self . log . info ( "writing connection info to %s" , fname ) with open ( fname , 'w' ) as f : f . write ( json . dumps ( cdict , indent = 2 ) ) os . chmod ( fname , stat . S_IRUSR | stat . S_IWUSR )
def load_config_from_json ( self ) : c = self . config self . log . debug ( "loading config from JSON" ) fname = os . path . join ( self . profile_dir . security_dir , self . engine_json_file ) self . log . info ( "loading connection info from %s" , fname ) with open ( fname ) as f : cfg = json . loads ( f . read ( ) ) key = cfg [ 'exec_key' ] c . Session . key = key . encode ( 'ascii' ) xport , addr = cfg [ 'url' ] . split ( '://' ) c . HubFactory . engine_transport = xport ip , ports = addr . split ( ':' ) c . HubFactory . engine_ip = ip c . HubFactory . regport = int ( ports ) self . location = cfg [ 'location' ] if not self . engine_ssh_server : self . engine_ssh_server = cfg [ 'ssh' ] fname = os . path . join ( self . profile_dir . security_dir , self . client_json_file ) self . log . info ( "loading connection info from %s" , fname ) with open ( fname ) as f : cfg = json . loads ( f . read ( ) ) assert key == cfg [ 'exec_key' ] , "exec_key mismatch between engine and client keys" xport , addr = cfg [ 'url' ] . split ( '://' ) c . HubFactory . client_transport = xport ip , ports = addr . split ( ':' ) c . HubFactory . client_ip = ip if not self . ssh_server : self . ssh_server = cfg [ 'ssh' ] assert int ( ports ) == c . HubFactory . regport , "regport mismatch"
def load_secondary_config ( self ) : if self . reuse_files : try : self . load_config_from_json ( ) except ( AssertionError , IOError ) as e : self . log . error ( "Could not load config from JSON: %s" % e ) else : self . write_connection_files = False default_secure ( self . config ) self . log . debug ( "Config changed" ) self . log . debug ( repr ( self . config ) )
def script_args ( f ) : args = [ magic_arguments . argument ( '--out' , type = str , help = ) , magic_arguments . argument ( '--err' , type = str , help = ) , magic_arguments . argument ( '--bg' , action = "store_true" , help = ) , magic_arguments . argument ( '--proc' , type = str , help = ) , ] for arg in args : f = arg ( f ) return f
def parallel_execute ( self , cell , block = None , groupby = 'type' , save_name = None ) : block = self . view . block if block is None else block base = "Parallel" if block else "Async parallel" targets = self . view . targets if isinstance ( targets , list ) and len ( targets ) > 10 : str_targets = str ( targets [ : 4 ] ) [ : - 1 ] + ', ..., ' + str ( targets [ - 4 : ] ) [ 1 : ] else : str_targets = str ( targets ) if self . verbose : print base + " execution on engine(s): %s" % str_targets result = self . view . execute ( cell , silent = False , block = False ) self . last_result = result if save_name : self . shell . user_ns [ save_name ] = result if block : result . get ( ) result . display_outputs ( groupby ) else : return result
def _disable_autopx ( self ) : if self . _autopx : self . shell . run_cell = self . _original_run_cell self . _autopx = False print "%autopx disabled"
def run_heartbeat ( message ) : then = arrow . get ( message [ 'time' ] ) now = arrow . get ( ) if ( now - then ) > timezone . timedelta ( seconds = ( TICK_FREQ + 1 ) ) : pass else : Task . run_tasks ( )
def run_task ( message ) : task = Task . objects . get ( pk = message [ 'id' ] ) if task . allow_overlap : task . run ( message ) else : if not task . running : task . running = True task . save ( ) try : task . run ( message ) finally : task . running = False task . save ( )
def remove_task ( message ) : task = Task . objects . get ( pk = message [ 'id' ] ) task . delete ( )
def patch_if_missing ( obj , name , method ) : setattr ( obj , name , getattr ( obj , name , method ) )
def accept_connection ( self ) : assert self . pending , "Connection is not pending." self . server_protocol = self . server . server_factory . buildProtocol ( None ) self . _accept_d . callback ( FakeServerProtocolWrapper ( self , self . server_protocol ) ) return self . await_connected ( )
def reject_connection ( self , reason = None ) : assert self . pending , "Connection is not pending." if reason is None : reason = ConnectionRefusedError ( ) self . _accept_d . errback ( reason )
def get_agent ( self , reactor = None , contextFactory = None ) : return ProxyAgentWithContext ( self . endpoint , reactor = reactor , contextFactory = contextFactory )
def form_valid ( self , form ) : self . object = form . save ( commit = False ) response = self . pre_save ( self . object ) if response : return response self . object . save ( ) form . save_m2m ( ) self . post_save ( self . object ) return HttpResponseRedirect ( self . get_success_url ( ) )
def delete ( self , request , * args , * * kwargs ) : self . object = self . get_object ( ) success_url = self . get_success_url ( ) self . pre_delete ( self . object ) self . object . delete ( ) self . post_delete ( self . object ) return HttpResponseRedirect ( success_url )
def pre_save ( self , instance ) : super ( UserViewMixin , self ) . pre_save ( instance ) if self . request . user . is_authenticated ( ) : for field in self . user_field : setattr ( instance , field , self . request . user )
def check ( self , check_all = False ) : if not self . enabled and not check_all : return if check_all or self . check_all : modules = sys . modules . keys ( ) else : modules = self . modules . keys ( ) for modname in modules : m = sys . modules . get ( modname , None ) if modname in self . skip_modules : continue if not hasattr ( m , '__file__' ) : continue if m . __name__ == '__main__' : continue filename = m . __file__ path , ext = os . path . splitext ( filename ) if ext . lower ( ) == '.py' : ext = PY_COMPILED_EXT pyc_filename = pyfile . cache_from_source ( filename ) py_filename = filename else : pyc_filename = filename try : py_filename = pyfile . source_from_cache ( filename ) except ValueError : continue try : pymtime = os . stat ( py_filename ) . st_mtime if pymtime <= os . stat ( pyc_filename ) . st_mtime : continue if self . failed . get ( py_filename , None ) == pymtime : continue except OSError : continue try : superreload ( m , reload , self . old_objects ) if py_filename in self . failed : del self . failed [ py_filename ] except : print >> sys . stderr , "[autoreload of %s failed: %s]" % ( modname , traceback . format_exc ( 1 ) ) self . failed [ py_filename ] = pymtime
def clipboard_get ( self ) : from IPython . lib . clipboard import ( osx_clipboard_get , tkinter_clipboard_get , win32_clipboard_get ) if sys . platform == 'win32' : chain = [ win32_clipboard_get , tkinter_clipboard_get ] elif sys . platform == 'darwin' : chain = [ osx_clipboard_get , tkinter_clipboard_get ] else : chain = [ tkinter_clipboard_get ] dispatcher = CommandChainDispatcher ( ) for func in chain : dispatcher . add ( func ) text = dispatcher ( ) return text
def add ( self , func , priority = 0 ) : self . chain . append ( ( priority , func ) ) self . chain . sort ( key = lambda x : x [ 0 ] )
def configure ( self , options , conf ) : self . conf = conf self . enabled = options . debugErrors or options . debugFailures self . enabled_for_errors = options . debugErrors self . enabled_for_failures = options . debugFailures
def import_item ( name ) : package = '.' . join ( name . split ( '.' ) [ 0 : - 1 ] ) obj = name . split ( '.' ) [ - 1 ] if package : module = __import__ ( package , fromlist = [ obj ] ) try : pak = module . __dict__ [ obj ] except KeyError : raise ImportError ( 'No module named %s' % obj ) return pak else : return __import__ ( obj )
def _try_passwordless_openssh ( server , keyfile ) : if pexpect is None : raise ImportError ( "pexpect unavailable, use paramiko" ) cmd = 'ssh -f ' + server if keyfile : cmd += ' -i ' + keyfile cmd += ' exit' p = pexpect . spawn ( cmd ) while True : try : p . expect ( '[Pp]assword:' , timeout = .1 ) except pexpect . TIMEOUT : continue except pexpect . EOF : return True else : return False
def _try_passwordless_paramiko ( server , keyfile ) : if paramiko is None : msg = "Paramiko unavaliable, " if sys . platform == 'win32' : msg += "Paramiko is required for ssh tunneled connections on Windows." else : msg += "use OpenSSH." raise ImportError ( msg ) username , server , port = _split_server ( server ) client = paramiko . SSHClient ( ) client . load_system_host_keys ( ) client . set_missing_host_key_policy ( paramiko . WarningPolicy ( ) ) try : client . connect ( server , port , username = username , key_filename = keyfile , look_for_keys = True ) except paramiko . AuthenticationException : return False else : client . close ( ) return True
def _unwrap_exception ( self , content ) : e = error . unwrap_exception ( content ) if e . engine_info : e_uuid = e . engine_info [ 'engine_uuid' ] eid = self . _engines [ e_uuid ] e . engine_info [ 'engine_id' ] = eid return e
def _register_engine ( self , msg ) : content = msg [ 'content' ] eid = content [ 'id' ] d = { eid : content [ 'queue' ] } self . _update_engines ( d )
def _unregister_engine ( self , msg ) : content = msg [ 'content' ] eid = int ( content [ 'id' ] ) if eid in self . _ids : self . _ids . remove ( eid ) uuid = self . _engines . pop ( eid ) self . _handle_stranded_msgs ( eid , uuid ) if self . _task_socket and self . _task_scheme == 'pure' : self . _stop_scheduling_tasks ( )
def _flush_results ( self , sock ) : idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) while msg is not None : if self . debug : pprint ( msg ) msg_type = msg [ 'header' ] [ 'msg_type' ] handler = self . _queue_handlers . get ( msg_type , None ) if handler is None : raise Exception ( "Unhandled message type: %s" % msg . msg_type ) else : handler ( msg ) idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK )
def _flush_ignored_control ( self ) : while self . _ignored_control_replies > 0 : self . session . recv ( self . _control_socket ) self . _ignored_control_replies -= 1
def _spin_every ( self , interval = 1 ) : while True : if self . _stop_spinning . is_set ( ) : return time . sleep ( interval ) self . spin ( )
def stop_spin_thread ( self ) : if self . _spin_thread is not None : self . _stop_spinning . set ( ) self . _spin_thread . join ( ) self . _spin_thread = None
def send_execute_request ( self , socket , code , silent = True , subheader = None , ident = None ) : if self . _closed : raise RuntimeError ( "Client cannot be used after its sockets have been closed" ) subheader = subheader if subheader is not None else { } if not isinstance ( code , basestring ) : raise TypeError ( "code must be text, not %s" % type ( code ) ) if not isinstance ( subheader , dict ) : raise TypeError ( "subheader must be dict, not %s" % type ( subheader ) ) content = dict ( code = code , silent = bool ( silent ) , user_variables = [ ] , user_expressions = { } ) msg = self . session . send ( socket , "execute_request" , content = content , ident = ident , subheader = subheader ) msg_id = msg [ 'header' ] [ 'msg_id' ] self . outstanding . add ( msg_id ) if ident : if isinstance ( ident , list ) : ident = ident [ - 1 ] if ident in self . _engines . values ( ) : self . _outstanding_dict [ ident ] . add ( msg_id ) self . history . append ( msg_id ) self . metadata [ msg_id ] [ 'submitted' ] = datetime . now ( ) return msg
def _opcode_set ( * names ) : s = set ( ) for name in names : try : s . add ( _opcode ( name ) ) except KeyError : pass return s
def _get_byte_parser ( self ) : if not self . _byte_parser : self . _byte_parser = ByteParser ( text = self . text , filename = self . filename ) return self . _byte_parser
def first_line ( self , line ) : rng = self . multiline . get ( line ) if rng : first_line = rng [ 0 ] else : first_line = line return first_line
def _block_stack_repr ( self , block_stack ) : blocks = ", " . join ( [ "(%s, %r)" % ( dis . opname [ b [ 0 ] ] , b [ 1 ] ) for b in block_stack ] ) return "[" + blocks + "]"
def validate_chunks ( self , chunks ) : starts = set ( [ ch . byte for ch in chunks ] ) for ch in chunks : assert all ( [ ( ex in starts or ex < 0 ) for ex in ch . exits ] )
def options ( self , parser , env ) : super ( Coverage , self ) . options ( parser , env ) parser . add_option ( "--cover-package" , action = "append" , default = env . get ( 'NOSE_COVER_PACKAGE' ) , metavar = "PACKAGE" , dest = "cover_packages" , help = "Restrict coverage output to selected packages " "[NOSE_COVER_PACKAGE]" ) parser . add_option ( "--cover-erase" , action = "store_true" , default = env . get ( 'NOSE_COVER_ERASE' ) , dest = "cover_erase" , help = "Erase previously collected coverage " "statistics before run" ) parser . add_option ( "--cover-tests" , action = "store_true" , dest = "cover_tests" , default = env . get ( 'NOSE_COVER_TESTS' ) , help = "Include test modules in coverage report " "[NOSE_COVER_TESTS]" ) parser . add_option ( "--cover-min-percentage" , action = "store" , dest = "cover_min_percentage" , default = env . get ( 'NOSE_COVER_MIN_PERCENTAGE' ) , help = "Minimum percentage of coverage for tests" "to pass [NOSE_COVER_MIN_PERCENTAGE]" ) parser . add_option ( "--cover-inclusive" , action = "store_true" , dest = "cover_inclusive" , default = env . get ( 'NOSE_COVER_INCLUSIVE' ) , help = "Include all python files under working " "directory in coverage report.  Useful for " "discovering holes in test coverage if not all " "files are imported by the test suite. " "[NOSE_COVER_INCLUSIVE]" ) parser . add_option ( "--cover-html" , action = "store_true" , default = env . get ( 'NOSE_COVER_HTML' ) , dest = 'cover_html' , help = "Produce HTML coverage information" ) parser . add_option ( '--cover-html-dir' , action = 'store' , default = env . get ( 'NOSE_COVER_HTML_DIR' , 'cover' ) , dest = 'cover_html_dir' , metavar = 'DIR' , help = 'Produce HTML coverage information in dir' ) parser . add_option ( "--cover-branches" , action = "store_true" , default = env . get ( 'NOSE_COVER_BRANCHES' ) , dest = "cover_branches" , help = "Include branch coverage in coverage report " "[NOSE_COVER_BRANCHES]" ) parser . add_option ( "--cover-xml" , action = "store_true" , default = env . get ( 'NOSE_COVER_XML' ) , dest = "cover_xml" , help = "Produce XML coverage information" ) parser . add_option ( "--cover-xml-file" , action = "store" , default = env . get ( 'NOSE_COVER_XML_FILE' , 'coverage.xml' ) , dest = "cover_xml_file" , metavar = "FILE" , help = "Produce XML coverage information in file" )
def begin ( self ) : log . debug ( "Coverage begin" ) self . skipModules = sys . modules . keys ( ) [ : ] if self . coverErase : log . debug ( "Clearing previously collected coverage statistics" ) self . coverInstance . combine ( ) self . coverInstance . erase ( ) self . coverInstance . exclude ( '#pragma[: ]+[nN][oO] [cC][oO][vV][eE][rR]' ) self . coverInstance . load ( ) self . coverInstance . start ( )
def report ( self , stream ) : log . debug ( "Coverage report" ) self . coverInstance . stop ( ) self . coverInstance . combine ( ) self . coverInstance . save ( ) modules = [ module for name , module in sys . modules . items ( ) if self . wantModuleCoverage ( name , module ) ] log . debug ( "Coverage report will cover modules: %s" , modules ) self . coverInstance . report ( modules , file = stream ) if self . coverHtmlDir : log . debug ( "Generating HTML coverage report" ) self . coverInstance . html_report ( modules , self . coverHtmlDir ) if self . coverXmlFile : log . debug ( "Generating XML coverage report" ) self . coverInstance . xml_report ( modules , self . coverXmlFile ) if self . coverMinPercentage : f = StringIO . StringIO ( ) self . coverInstance . report ( modules , file = f ) m = re . search ( r'-------\s\w+\s+\d+\s+\d+\s+(\d+)%\s+\d*\s{0,1}$' , f . getvalue ( ) ) if m : percentage = int ( m . groups ( ) [ 0 ] ) if percentage < self . coverMinPercentage : log . error ( 'TOTAL Coverage did not reach minimum ' 'required: %d%%' % self . coverMinPercentage ) sys . exit ( 1 ) else : log . error ( "No total percentage was found in coverage output, " "something went wrong." )
def open_with_auth ( url ) : scheme , netloc , path , params , query , frag = urlparse . urlparse ( url ) if netloc . endswith ( ':' ) : raise httplib . InvalidURL ( "nonnumeric port: ''" ) if scheme in ( 'http' , 'https' ) : auth , host = urllib2 . splituser ( netloc ) else : auth = None if auth : auth = "Basic " + _encode_auth ( auth ) new_url = urlparse . urlunparse ( ( scheme , host , path , params , query , frag ) ) request = urllib2 . Request ( new_url ) request . add_header ( "Authorization" , auth ) else : request = urllib2 . Request ( url ) request . add_header ( 'User-Agent' , user_agent ) fp = urllib2 . urlopen ( request ) if auth : s2 , h2 , path2 , param2 , query2 , frag2 = urlparse . urlparse ( fp . url ) if s2 == scheme and h2 == host : fp . url = urlparse . urlunparse ( ( s2 , netloc , path2 , param2 , query2 , frag2 ) ) return fp
def get_parent ( obj ) : names = obj . __qualname__ . split ( '.' ) [ : - 1 ] if '<locals>' in names : raise ValueError ( 'cannot get parent from locals object.' ) module = sys . modules [ obj . __module__ ] parent = module while names : parent = getattr ( parent , names . pop ( 0 ) ) return parent
def render_template ( content , context ) : rendered = Template ( content ) . render ( Context ( context ) ) return rendered
def configure ( self , options , conf ) : self . conf = conf if not options . capture : self . enabled = False
def formatError ( self , test , err ) : test . capturedOutput = output = self . buffer self . _buf = None if not output : return err ec , ev , tb = err return ( ec , self . addCaptureToErr ( ev , output ) , tb )
def splitBy ( data , num ) : return [ data [ i : i + num ] for i in range ( 0 , len ( data ) , num ) ]
def hex_to_rgb ( color ) : if color . startswith ( '#' ) : color = color [ 1 : ] if len ( color ) == 3 : color = '' . join ( [ c * 2 for c in color ] ) if len ( color ) != 6 : return False try : r = int ( color [ : 2 ] , 16 ) g = int ( color [ 2 : 4 ] , 16 ) b = int ( color [ 4 : ] , 16 ) except ValueError : return False else : return r , g , b
def _handle_complete_reply ( self , rep ) : self . log . debug ( "complete: %s" , rep . get ( 'content' , '' ) ) cursor = self . _get_cursor ( ) info = self . _request_info . get ( 'complete' ) if info and info . id == rep [ 'parent_header' ] [ 'msg_id' ] and info . pos == cursor . position ( ) : matches = rep [ 'content' ] [ 'matches' ] text = rep [ 'content' ] [ 'matched_text' ] offset = len ( text ) if len ( matches ) > 1 and matches [ 0 ] [ : offset ] == text : parts = re . split ( r'[./\\]' , text ) sep_count = len ( parts ) - 1 if sep_count : chop_length = sum ( map ( len , parts [ : sep_count ] ) ) + sep_count matches = [ match [ chop_length : ] for match in matches ] offset -= chop_length cursor . movePosition ( QtGui . QTextCursor . Left , n = offset ) self . _complete_with_items ( cursor , matches )
def _handle_execute_reply ( self , msg ) : msg_id = msg [ 'parent_header' ] . get ( 'msg_id' ) info = self . _request_info [ 'execute' ] . get ( msg_id ) if info and info . kind == 'prompt' : number = msg [ 'content' ] [ 'execution_count' ] + 1 self . _show_interpreter_prompt ( number ) self . _request_info [ 'execute' ] . pop ( msg_id ) else : super ( IPythonWidget , self ) . _handle_execute_reply ( msg )
def _handle_pyout ( self , msg ) : self . log . debug ( "pyout: %s" , msg . get ( 'content' , '' ) ) if not self . _hidden and self . _is_from_this_session ( msg ) : content = msg [ 'content' ] prompt_number = content . get ( 'execution_count' , 0 ) data = content [ 'data' ] if data . has_key ( 'text/html' ) : self . _append_plain_text ( self . output_sep , True ) self . _append_html ( self . _make_out_prompt ( prompt_number ) , True ) html = data [ 'text/html' ] self . _append_plain_text ( '\n' , True ) self . _append_html ( html + self . output_sep2 , True ) elif data . has_key ( 'text/plain' ) : self . _append_plain_text ( self . output_sep , True ) self . _append_html ( self . _make_out_prompt ( prompt_number ) , True ) text = data [ 'text/plain' ] if "\n" in text and not self . output_sep . endswith ( "\n" ) : self . _append_plain_text ( '\n' , True ) self . _append_plain_text ( text + self . output_sep2 , True )
def _handle_display_data ( self , msg ) : self . log . debug ( "display: %s" , msg . get ( 'content' , '' ) ) if not self . _hidden and self . _is_from_this_session ( msg ) : source = msg [ 'content' ] [ 'source' ] data = msg [ 'content' ] [ 'data' ] metadata = msg [ 'content' ] [ 'metadata' ] if data . has_key ( 'text/html' ) : html = data [ 'text/html' ] self . _append_html ( html , True ) elif data . has_key ( 'text/plain' ) : text = data [ 'text/plain' ] self . _append_plain_text ( text , True ) self . _append_plain_text ( u'\n' , True )
def _started_channels ( self ) : super ( IPythonWidget , self ) . _started_channels ( ) self . _load_guiref_magic ( ) self . kernel_manager . shell_channel . history ( hist_access_type = 'tail' , n = 1000 )
def execute_file ( self , path , hidden = False ) : if sys . platform == 'win32' : path = os . path . normpath ( path ) . replace ( '\\' , '/' ) if ' ' in path or "'" in path or '"' in path : path = '"%s"' % path . replace ( '"' , '\\"' ) self . execute ( '%%run %s' % path , hidden = hidden )
def _complete ( self ) : text = '' msg_id = self . kernel_manager . shell_channel . complete ( text , self . _get_input_buffer_cursor_line ( ) , self . _get_input_buffer_cursor_column ( ) , self . input_buffer ) pos = self . _get_cursor ( ) . position ( ) info = self . _CompletionRequest ( msg_id , pos ) self . _request_info [ 'complete' ] = info
def _process_execute_error ( self , msg ) : content = msg [ 'content' ] traceback = '\n' . join ( content [ 'traceback' ] ) + '\n' if False : traceback = traceback . replace ( ' ' , '&nbsp;' ) traceback = traceback . replace ( '\n' , '<br/>' ) ename = content [ 'ename' ] ename_styled = '<span class="error">%s</span>' % ename traceback = traceback . replace ( ename , ename_styled ) self . _append_html ( traceback ) else : self . _append_plain_text ( traceback )
def _process_execute_payload ( self , item ) : handler = self . _payload_handlers . get ( item [ 'source' ] ) if handler is None : return False else : handler ( item ) return True
def _show_interpreter_prompt ( self , number = None ) : if number is None : msg_id = self . kernel_manager . shell_channel . execute ( '' , silent = True ) info = self . _ExecutionRequest ( msg_id , 'prompt' ) self . _request_info [ 'execute' ] [ msg_id ] = info return self . _prompt_sep = self . input_sep self . _show_prompt ( self . _make_in_prompt ( number ) , html = True ) block = self . _control . document ( ) . lastBlock ( ) length = len ( self . _prompt ) self . _previous_prompt_obj = self . _PromptBlock ( block , length , number ) self . _set_continuation_prompt ( self . _make_continuation_prompt ( self . _prompt ) , html = True )
def _show_interpreter_prompt_for_reply ( self , msg ) : content = msg [ 'content' ] if content [ 'status' ] == 'aborted' : if self . _previous_prompt_obj : previous_prompt_number = self . _previous_prompt_obj . number else : previous_prompt_number = 0 else : previous_prompt_number = content [ 'execution_count' ] if self . _previous_prompt_obj and self . _previous_prompt_obj . number != previous_prompt_number : block = self . _previous_prompt_obj . block if block . isValid ( ) and block . text ( ) : cursor = QtGui . QTextCursor ( block ) cursor . movePosition ( QtGui . QTextCursor . Right , QtGui . QTextCursor . KeepAnchor , self . _previous_prompt_obj . length ) prompt = self . _make_in_prompt ( previous_prompt_number ) self . _prompt = self . _insert_html_fetching_plain_text ( cursor , prompt ) self . _highlighter . rehighlightBlock ( cursor . block ( ) ) self . _previous_prompt_obj = None self . _show_interpreter_prompt ( previous_prompt_number + 1 )
def _make_in_prompt ( self , number ) : try : body = self . in_prompt % number except TypeError : body = self . in_prompt return '<span class="in-prompt">%s</span>' % body
def _style_sheet_changed ( self ) : self . setStyleSheet ( self . style_sheet ) if self . _control is not None : self . _control . document ( ) . setDefaultStyleSheet ( self . style_sheet ) bg_color = self . _control . palette ( ) . window ( ) . color ( ) self . _ansi_processor . set_background_color ( bg_color ) if self . _page_control is not None : self . _page_control . document ( ) . setDefaultStyleSheet ( self . style_sheet )
def _syntax_style_changed ( self ) : if self . _highlighter is None : return if self . syntax_style : self . _highlighter . set_style ( self . syntax_style ) else : self . _highlighter . set_style_sheet ( self . style_sheet )
def virtual_memory ( ) : mem = _psutil_bsd . get_virtual_mem ( ) total , free , active , inactive , wired , cached , buffers , shared = mem avail = inactive + cached + free used = active + wired + cached percent = usage_percent ( ( total - avail ) , total , _round = 1 ) return nt_virtmem_info ( total , avail , percent , used , free , active , inactive , buffers , cached , shared , wired )
def get_system_cpu_times ( ) : user , nice , system , idle , irq = _psutil_bsd . get_system_cpu_times ( ) return _cputimes_ntuple ( user , nice , system , idle , irq )
def get_system_per_cpu_times ( ) : ret = [ ] for cpu_t in _psutil_bsd . get_system_per_cpu_times ( ) : user , nice , system , idle , irq = cpu_t item = _cputimes_ntuple ( user , nice , system , idle , irq ) ret . append ( item ) return ret
def get_process_uids ( self ) : real , effective , saved = _psutil_bsd . get_process_uids ( self . pid ) return nt_uids ( real , effective , saved )
def get_process_gids ( self ) : real , effective , saved = _psutil_bsd . get_process_gids ( self . pid ) return nt_gids ( real , effective , saved )
def get_memory_info ( self ) : rss , vms = _psutil_bsd . get_process_memory_info ( self . pid ) [ : 2 ] return nt_meminfo ( rss , vms )
def get_process_threads ( self ) : rawlist = _psutil_bsd . get_process_threads ( self . pid ) retlist = [ ] for thread_id , utime , stime in rawlist : ntuple = nt_thread ( thread_id , utime , stime ) retlist . append ( ntuple ) return retlist
def get_open_files ( self ) : if hasattr ( _psutil_bsd , "get_process_open_files" ) : rawlist = _psutil_bsd . get_process_open_files ( self . pid ) return [ nt_openfile ( path , fd ) for path , fd in rawlist ] else : lsof = _psposix . LsofParser ( self . pid , self . _process_name ) return lsof . get_process_open_files ( )
def _num_cpus_darwin ( ) : p = subprocess . Popen ( [ 'sysctl' , '-n' , 'hw.ncpu' ] , stdout = subprocess . PIPE ) return p . stdout . read ( )
def fetchone ( self ) : self . _check_executed ( ) r = self . _fetch_row ( 1 ) if not r : self . _warning_check ( ) return None self . rownumber = self . rownumber + 1 return r [ 0 ]
def fetchall ( self ) : self . _check_executed ( ) r = self . _fetch_row ( 0 ) self . rownumber = self . rownumber + len ( r ) self . _warning_check ( ) return r
def connect ( com , peers , tree , pub_url , root_id ) : com . connect ( peers , tree , pub_url , root_id )
def reads_json ( s , * * kwargs ) : nbf , minor , d = parse_json ( s , * * kwargs ) if nbf == 1 : nb = v1 . to_notebook_json ( d , * * kwargs ) nb = v3 . convert_to_this_nbformat ( nb , orig_version = 1 ) elif nbf == 2 : nb = v2 . to_notebook_json ( d , * * kwargs ) nb = v3 . convert_to_this_nbformat ( nb , orig_version = 2 ) elif nbf == 3 : nb = v3 . to_notebook_json ( d , * * kwargs ) nb = v3 . convert_to_this_nbformat ( nb , orig_version = 3 , orig_minor = minor ) else : raise NBFormatError ( 'Unsupported JSON nbformat version: %i' % nbf ) return nb
def reads_py ( s , * * kwargs ) : nbf , nbm , s = parse_py ( s , * * kwargs ) if nbf == 2 : nb = v2 . to_notebook_py ( s , * * kwargs ) elif nbf == 3 : nb = v3 . to_notebook_py ( s , * * kwargs ) else : raise NBFormatError ( 'Unsupported PY nbformat version: %i' % nbf ) return nb
def _convert_to_metadata ( ) : import glob for fname in glob . glob ( '*.ipynb' ) : print ( 'Converting file:' , fname ) with open ( fname , 'r' ) as f : nb = read ( f , u'json' ) md = new_metadata ( ) if u'name' in nb : md . name = nb . name del nb [ u'name' ] nb . metadata = md with open ( fname , 'w' ) as f : write ( nb , f , u'json' )
def wantFunction ( self , function ) : try : if hasattr ( function , 'compat_func_name' ) : funcname = function . compat_func_name else : funcname = function . __name__ except AttributeError : return False declared = getattr ( function , '__test__' , None ) if declared is not None : wanted = declared else : wanted = not funcname . startswith ( '_' ) and self . matches ( funcname ) plug_wants = self . plugins . wantFunction ( function ) if plug_wants is not None : wanted = plug_wants log . debug ( "wantFunction %s? %s" , function , wanted ) return wanted
def wantMethod ( self , method ) : try : method_name = method . __name__ except AttributeError : return False if method_name . startswith ( '_' ) : return False declared = getattr ( method , '__test__' , None ) if declared is not None : wanted = declared else : wanted = self . matches ( method_name ) plug_wants = self . plugins . wantMethod ( method ) if plug_wants is not None : wanted = plug_wants log . debug ( "wantMethod %s? %s" , method , wanted ) return wanted
def list_command_pydb ( self , arg ) : filename , first , last = OldPdb . parse_list_cmd ( self , arg ) if filename is not None : self . print_list_lines ( filename , first , last )
def do_pdef ( self , arg ) : namespaces = [ ( 'Locals' , self . curframe . f_locals ) , ( 'Globals' , self . curframe . f_globals ) ] self . shell . find_line_magic ( 'pdef' ) ( arg , namespaces = namespaces )
def conversion_factor ( from_symbol , to_symbol , date ) : from_currency = Currency . objects . get ( symbol = from_symbol ) try : from_currency_price = CurrencyPrice . objects . get ( currency = from_currency , date = date ) . mid_price except CurrencyPrice . DoesNotExist : print "Cannot fetch prices for %s on %s" % ( str ( from_currency ) , str ( date ) ) return None to_currency = Currency . objects . get ( symbol = to_symbol ) try : to_currency_price = CurrencyPrice . objects . get ( currency = to_currency , date = date ) . mid_price except CurrencyPrice . DoesNotExist : print "Cannot fetch prices for %s on %s" % ( str ( to_currency ) , str ( date ) ) return None return to_currency_price / from_currency_price
def convert_currency ( from_symbol , to_symbol , value , date ) : if from_symbol == to_symbol : return value factor = conversion_factor ( from_symbol , to_symbol , date ) if type ( value ) == float : output = value * float ( factor ) elif type ( value ) == Decimal : output = Decimal ( format ( value * factor , '.%sf' % str ( PRICE_PRECISION ) ) ) elif type ( value ) in [ np . float16 , np . float32 , np . float64 , np . float128 , np . float ] : output = float ( value ) * float ( factor ) else : output = None return output
def compute_return ( self , start_date , end_date , rate = "MID" ) : if rate not in [ "MID" , "ASK" , "BID" ] : raise ValueError ( "Unknown rate type (%s)- must be 'MID', 'ASK' or 'BID'" % str ( rate ) ) if end_date <= start_date : raise ValueError ( "End date must be on or after start date" ) df = self . generate_dataframe ( start_date = start_date , end_date = end_date ) start_price = df . ix [ start_date ] [ rate ] end_price = df . ix [ end_date ] [ rate ] currency_return = ( end_price / start_price ) - 1.0 return currency_return
def write_connection_file ( self ) : if os . path . basename ( self . connection_file ) == self . connection_file : cf = os . path . join ( self . profile_dir . security_dir , self . connection_file ) else : cf = self . connection_file write_connection_file ( cf , ip = self . ip , key = self . session . key , shell_port = self . shell_port , stdin_port = self . stdin_port , hb_port = self . hb_port , iopub_port = self . iopub_port ) self . _full_connection_file = cf
def init_heartbeat ( self ) : hb_ctx = zmq . Context ( ) self . heartbeat = Heartbeat ( hb_ctx , ( self . ip , self . hb_port ) ) self . hb_port = self . heartbeat . port self . log . debug ( "Heartbeat REP Channel on port: %i" % self . hb_port ) self . heartbeat . start ( ) self . log . critical ( "To connect another client to this kernel, use:" )
def log_connection_info ( self ) : basename = os . path . basename ( self . connection_file ) if basename == self . connection_file or os . path . dirname ( self . connection_file ) == self . profile_dir . security_dir : tail = basename if self . profile != 'default' : tail += " --profile %s" % self . profile else : tail = self . connection_file self . log . critical ( "--existing %s" , tail ) self . ports = dict ( shell = self . shell_port , iopub = self . iopub_port , stdin = self . stdin_port , hb = self . hb_port )
def init_session ( self ) : default_secure ( self . config ) self . session = Session ( config = self . config , username = u'kernel' )
def init_io ( self ) : if self . outstream_class : outstream_factory = import_item ( str ( self . outstream_class ) ) sys . stdout = outstream_factory ( self . session , self . iopub_socket , u'stdout' ) sys . stderr = outstream_factory ( self . session , self . iopub_socket , u'stderr' ) if self . displayhook_class : displayhook_factory = import_item ( str ( self . displayhook_class ) ) sys . displayhook = displayhook_factory ( self . session , self . iopub_socket )
def init_kernel ( self ) : kernel_factory = import_item ( str ( self . kernel_class ) ) self . kernel = kernel_factory ( config = self . config , session = self . session , shell_socket = self . shell_socket , iopub_socket = self . iopub_socket , stdin_socket = self . stdin_socket , log = self . log ) self . kernel . record_ports ( self . ports )
def init_connector ( self ) : self . using_ssh = bool ( self . sshkey or self . sshserver ) if self . sshkey and not self . sshserver : self . sshserver = self . url . split ( '://' ) [ 1 ] . split ( ':' ) [ 0 ] if self . using_ssh : if tunnel . try_passwordless_ssh ( self . sshserver , self . sshkey , self . paramiko ) : password = False else : password = getpass ( "SSH Password for %s: " % self . sshserver ) else : password = False def connect ( s , url ) : url = disambiguate_url ( url , self . location ) if self . using_ssh : self . log . debug ( "Tunneling connection to %s via %s" % ( url , self . sshserver ) ) return tunnel . tunnel_connection ( s , url , self . sshserver , keyfile = self . sshkey , paramiko = self . paramiko , password = password , ) else : return s . connect ( url ) def maybe_tunnel ( url ) : """like connect, but don't complete the connection (for use by heartbeat)""" url = disambiguate_url ( url , self . location ) if self . using_ssh : self . log . debug ( "Tunneling connection to %s via %s" % ( url , self . sshserver ) ) url , tunnelobj = tunnel . open_tunnel ( url , self . sshserver , keyfile = self . sshkey , paramiko = self . paramiko , password = password , ) return url return connect , maybe_tunnel
def html_to_text ( content ) : text = None h2t = html2text . HTML2Text ( ) h2t . ignore_links = False text = h2t . handle ( content ) return text
def md_to_text ( content ) : text = None html = markdown . markdown ( content ) if html : text = html_to_text ( content ) return text
def domain_to_fqdn ( domain , proto = None ) : from . generic import get_site_proto proto = proto or get_site_proto ( ) fdqn = '{proto}://{domain}' . format ( proto = proto , domain = domain ) return fdqn
def options ( self , parser , env = os . environ ) : super ( NoseExclude , self ) . options ( parser , env ) env_dirs = [ ] if 'NOSE_EXCLUDE_DIRS' in env : exclude_dirs = env . get ( 'NOSE_EXCLUDE_DIRS' , '' ) env_dirs . extend ( exclude_dirs . split ( ';' ) ) parser . add_option ( "--exclude-dir" , action = "append" , dest = "exclude_dirs" , default = env_dirs , help = ) parser . add_option ( "--exclude-dir-file" , type = "string" , dest = "exclude_dir_file" , default = env . get ( 'NOSE_EXCLUDE_DIRS_FILE' , False ) , help = )
def configure ( self , options , conf ) : super ( NoseExclude , self ) . configure ( options , conf ) self . exclude_dirs = { } if options . exclude_dir_file : if not options . exclude_dirs : options . exclude_dirs = [ ] new_dirs = self . _load_from_file ( options . exclude_dir_file ) options . exclude_dirs . extend ( new_dirs ) if not options . exclude_dirs : self . enabled = False return self . enabled = True root = os . getcwd ( ) log . debug ( 'cwd: %s' % root ) for exclude_param in options . exclude_dirs : for d in exclude_param . split ( '\n' ) : d = d . strip ( ) abs_d = self . _force_to_abspath ( d ) if abs_d : self . exclude_dirs [ abs_d ] = True exclude_str = "excluding dirs: %s" % "," . join ( self . exclude_dirs . keys ( ) ) log . debug ( exclude_str )
def wantDirectory ( self , dirname ) : if dirname in self . exclude_dirs : log . debug ( "excluded: %s" % dirname ) return False else : return None
def links_to_dynamic ( self , ext ) : libnames = dict . fromkeys ( [ lib . _full_name for lib in self . shlibs ] ) pkg = '.' . join ( ext . _full_name . split ( '.' ) [ : - 1 ] + [ '' ] ) for libname in ext . libraries : if pkg + libname in libnames : return True return False
def append_func ( self , func , * args , * * kwargs ) : wraped_func = partial ( func , * args , * * kwargs ) self . append ( wraped_func )
def insert_func ( self , index , func , * args , * * kwargs ) : wraped_func = partial ( func , * args , * * kwargs ) self . insert ( index , wraped_func )
def construct_parser ( magic_func ) : kwds = getattr ( magic_func , 'argcmd_kwds' , { } ) if 'description' not in kwds : kwds [ 'description' ] = getattr ( magic_func , '__doc__' , None ) arg_name = real_name ( magic_func ) parser = MagicArgumentParser ( arg_name , * * kwds ) group = None for deco in magic_func . decorators [ : : - 1 ] : result = deco . add_to_parser ( parser , group ) if result is not None : group = result help_text = parser . format_help ( ) if help_text . startswith ( 'usage: ' ) : help_text = help_text . replace ( 'usage: ' , '%' , 1 ) else : help_text = '%' + help_text magic_func . __doc__ = help_text return parser
def real_name ( magic_func ) : magic_name = magic_func . __name__ if magic_name . startswith ( 'magic_' ) : magic_name = magic_name [ len ( 'magic_' ) : ] return getattr ( magic_func , 'argcmd_name' , magic_name )
def add_to_parser ( self , parser , group ) : if group is not None : parser = group parser . add_argument ( * self . args , * * self . kwds ) return None
def add_to_parser ( self , parser , group ) : return parser . add_argument_group ( * self . args , * * self . kwds )
def highlightBlock ( self , string ) : if not self . highlighting_on : return current_block = self . currentBlock ( ) string = self . _frontend . _get_block_plain_text ( current_block ) if current_block . contains ( self . _frontend . _prompt_pos ) : prompt = self . _frontend . _prompt else : prompt = self . _frontend . _continuation_prompt if string . startswith ( prompt ) : self . _current_offset = len ( prompt ) string = string [ len ( prompt ) : ] super ( FrontendHighlighter , self ) . highlightBlock ( string )
def rehighlightBlock ( self , block ) : old = self . highlighting_on self . highlighting_on = True super ( FrontendHighlighter , self ) . rehighlightBlock ( block ) self . highlighting_on = old
def setFormat ( self , start , count , format ) : start += self . _current_offset super ( FrontendHighlighter , self ) . setFormat ( start , count , format )
def copy ( self ) : if self . _page_control is not None and self . _page_control . hasFocus ( ) : self . _page_control . copy ( ) elif self . _control . hasFocus ( ) : text = self . _control . textCursor ( ) . selection ( ) . toPlainText ( ) if text : lines = map ( self . _transform_prompt , text . splitlines ( ) ) text = '\n' . join ( lines ) QtGui . QApplication . clipboard ( ) . setText ( text ) else : self . log . debug ( "frontend widget : unknown copy target" )
def _context_menu_make ( self , pos ) : menu = super ( FrontendWidget , self ) . _context_menu_make ( pos ) for before_action in menu . actions ( ) : if before_action . shortcut ( ) . matches ( QtGui . QKeySequence . Paste ) == QtGui . QKeySequence . ExactMatch : menu . insertAction ( before_action , self . _copy_raw_action ) break return menu
def _event_filter_console_keypress ( self , event ) : key = event . key ( ) if self . _control_key_down ( event . modifiers ( ) , include_command = False ) : if key == QtCore . Qt . Key_C and self . _executing : self . request_interrupt_kernel ( ) return True elif key == QtCore . Qt . Key_Period : self . request_restart_kernel ( ) return True elif not event . modifiers ( ) & QtCore . Qt . AltModifier : if key == QtCore . Qt . Key_Backspace : col = self . _get_input_buffer_cursor_column ( ) cursor = self . _control . textCursor ( ) if col > 3 and not cursor . hasSelection ( ) : text = self . _get_input_buffer_cursor_line ( ) [ : col ] if text . endswith ( '    ' ) and not text . strip ( ) : cursor . movePosition ( QtGui . QTextCursor . Left , QtGui . QTextCursor . KeepAnchor , 4 ) cursor . removeSelectedText ( ) return True return super ( FrontendWidget , self ) . _event_filter_console_keypress ( event )
def _insert_continuation_prompt ( self , cursor ) : super ( FrontendWidget , self ) . _insert_continuation_prompt ( cursor ) cursor . insertText ( ' ' * self . _input_splitter . indent_spaces )
def _handle_complete_reply ( self , rep ) : self . log . debug ( "complete: %s" , rep . get ( 'content' , '' ) ) cursor = self . _get_cursor ( ) info = self . _request_info . get ( 'complete' ) if info and info . id == rep [ 'parent_header' ] [ 'msg_id' ] and info . pos == cursor . position ( ) : text = '.' . join ( self . _get_context ( ) ) cursor . movePosition ( QtGui . QTextCursor . Left , n = len ( text ) ) self . _complete_with_items ( cursor , rep [ 'content' ] [ 'matches' ] )
def _handle_execute_reply ( self , msg ) : self . log . debug ( "execute: %s" , msg . get ( 'content' , '' ) ) msg_id = msg [ 'parent_header' ] [ 'msg_id' ] info = self . _request_info [ 'execute' ] . get ( msg_id ) self . _reading = False if info and info . kind == 'user' and not self . _hidden : self . kernel_manager . sub_channel . flush ( ) if self . ansi_codes : self . _ansi_processor . reset_sgr ( ) content = msg [ 'content' ] status = content [ 'status' ] if status == 'ok' : self . _process_execute_ok ( msg ) elif status == 'error' : self . _process_execute_error ( msg ) elif status == 'aborted' : self . _process_execute_abort ( msg ) self . _show_interpreter_prompt_for_reply ( msg ) self . executed . emit ( msg ) self . _request_info [ 'execute' ] . pop ( msg_id ) elif info and info . kind == 'silent_exec_callback' and not self . _hidden : self . _handle_exec_callback ( msg ) self . _request_info [ 'execute' ] . pop ( msg_id ) else : super ( FrontendWidget , self ) . _handle_execute_reply ( msg )
def _handle_input_request ( self , msg ) : self . log . debug ( "input: %s" , msg . get ( 'content' , '' ) ) if self . _hidden : raise RuntimeError ( 'Request for raw input during hidden execution.' ) self . kernel_manager . sub_channel . flush ( ) def callback ( line ) : self . kernel_manager . stdin_channel . input ( line ) if self . _reading : self . log . debug ( "Got second input request, assuming first was interrupted." ) self . _reading = False self . _readline ( msg [ 'content' ] [ 'prompt' ] , callback = callback )
def _handle_kernel_died ( self , since_last_heartbeat ) : self . log . debug ( "kernel died: %s" , since_last_heartbeat ) if self . custom_restart : self . custom_restart_kernel_died . emit ( since_last_heartbeat ) else : message = 'The kernel heartbeat has been inactive for %.2f ' 'seconds. Do you want to restart the kernel? You may ' 'first want to check the network connection.' % since_last_heartbeat self . restart_kernel ( message , now = True )
def _handle_object_info_reply ( self , rep ) : self . log . debug ( "oinfo: %s" , rep . get ( 'content' , '' ) ) cursor = self . _get_cursor ( ) info = self . _request_info . get ( 'call_tip' ) if info and info . id == rep [ 'parent_header' ] [ 'msg_id' ] and info . pos == cursor . position ( ) : content = rep [ 'content' ] if content . get ( 'ismagic' , False ) : call_info , doc = None , None else : call_info , doc = call_tip ( content , format_call = True ) if call_info or doc : self . _call_tip_widget . show_call_info ( call_info , doc )
def _handle_pyout ( self , msg ) : self . log . debug ( "pyout: %s" , msg . get ( 'content' , '' ) ) if not self . _hidden and self . _is_from_this_session ( msg ) : text = msg [ 'content' ] [ 'data' ] self . _append_plain_text ( text + '\n' , before_prompt = True )
def _handle_stream ( self , msg ) : self . log . debug ( "stream: %s" , msg . get ( 'content' , '' ) ) if not self . _hidden and self . _is_from_this_session ( msg ) : text = msg [ 'content' ] [ 'data' ] . expandtabs ( 8 ) self . _append_plain_text ( text , before_prompt = True ) self . _control . moveCursor ( QtGui . QTextCursor . End )
def _handle_shutdown_reply ( self , msg ) : self . log . debug ( "shutdown: %s" , msg . get ( 'content' , '' ) ) if not self . _hidden and not self . _is_from_this_session ( msg ) : if self . _local_kernel : if not msg [ 'content' ] [ 'restart' ] : self . exit_requested . emit ( self ) else : time . sleep ( 0.25 ) self . reset ( ) else : title = self . window ( ) . windowTitle ( ) if not msg [ 'content' ] [ 'restart' ] : reply = QtGui . QMessageBox . question ( self , title , "Kernel has been shutdown permanently. " "Close the Console?" , QtGui . QMessageBox . Yes , QtGui . QMessageBox . No ) if reply == QtGui . QMessageBox . Yes : self . exit_requested . emit ( self ) else : reply = QtGui . QMessageBox . question ( self , title , "Kernel has been reset. Clear the Console?" , QtGui . QMessageBox . Yes , QtGui . QMessageBox . No ) if reply == QtGui . QMessageBox . Yes : time . sleep ( 0.25 ) self . reset ( )
def restart_kernel ( self , message , now = False ) : if self . custom_restart : self . custom_restart_requested . emit ( ) elif self . kernel_manager . has_kernel : self . kernel_manager . hb_channel . pause ( ) if self . confirm_restart : buttons = QtGui . QMessageBox . Yes | QtGui . QMessageBox . No result = QtGui . QMessageBox . question ( self , 'Restart kernel?' , message , buttons ) do_restart = result == QtGui . QMessageBox . Yes else : do_restart = True if do_restart : try : self . kernel_manager . restart_kernel ( now = now ) except RuntimeError : self . _append_plain_text ( 'Kernel started externally. ' 'Cannot restart.\n' , before_prompt = True ) else : self . reset ( ) else : self . kernel_manager . hb_channel . unpause ( ) else : self . _append_plain_text ( 'Kernel process is either remote or ' 'unspecified. Cannot restart.\n' , before_prompt = True )
def _call_tip ( self ) : if not self . enable_calltips : return False cursor = self . _get_cursor ( ) cursor . movePosition ( QtGui . QTextCursor . Left ) if cursor . document ( ) . characterAt ( cursor . position ( ) ) != '(' : return False context = self . _get_context ( cursor ) if not context : return False name = '.' . join ( context ) msg_id = self . kernel_manager . shell_channel . object_info ( name ) pos = self . _get_cursor ( ) . position ( ) self . _request_info [ 'call_tip' ] = self . _CallTipRequest ( msg_id , pos ) return True
def _complete ( self ) : context = self . _get_context ( ) if context : msg_id = self . kernel_manager . shell_channel . complete ( '.' . join ( context ) , self . _get_input_buffer_cursor_line ( ) , self . _get_input_buffer_cursor_column ( ) , self . input_buffer ) pos = self . _get_cursor ( ) . position ( ) info = self . _CompletionRequest ( msg_id , pos ) self . _request_info [ 'complete' ] = info
def _process_execute_error ( self , msg ) : content = msg [ 'content' ] if content [ 'ename' ] == 'SystemExit' : keepkernel = content [ 'evalue' ] == '-k' or content [ 'evalue' ] == 'True' self . _keep_kernel_on_exit = keepkernel self . exit_requested . emit ( self ) else : traceback = '' . join ( content [ 'traceback' ] ) self . _append_plain_text ( traceback )
def _process_execute_ok ( self , msg ) : payload = msg [ 'content' ] [ 'payload' ] for item in payload : if not self . _process_execute_payload ( item ) : warning = 'Warning: received unknown payload of type %s' print ( warning % repr ( item [ 'source' ] ) )
def generate ( self , * arg , * * kw ) : for p , meth in self . plugins : result = None try : result = meth ( * arg , * * kw ) if result is not None : for r in result : yield r except ( KeyboardInterrupt , SystemExit ) : raise except : exc = sys . exc_info ( ) yield Failure ( * exc ) continue
def simple ( self , * arg , * * kw ) : for p , meth in self . plugins : result = meth ( * arg , * * kw ) if result is not None : return result
def loadPlugins ( self ) : from pkg_resources import iter_entry_points loaded = { } for entry_point , adapt in self . entry_points : for ep in iter_entry_points ( entry_point ) : if ep . name in loaded : continue loaded [ ep . name ] = True log . debug ( '%s load plugin %s' , self . __class__ . __name__ , ep ) try : plugcls = ep . load ( ) except KeyboardInterrupt : raise except Exception , e : warn ( "Unable to load plugin %s: %s" % ( ep , e ) , RuntimeWarning ) continue if adapt : plug = adapt ( plugcls ( ) ) else : plug = plugcls ( ) self . addPlugin ( plug ) super ( EntryPointPluginManager , self ) . loadPlugins ( )
def loadPlugins ( self ) : from nose . plugins import builtin for plug in builtin . plugins : self . addPlugin ( plug ( ) ) super ( BuiltinPluginManager , self ) . loadPlugins ( )
def cleanup_files ( self , bundle = False ) : logger . notify ( 'Cleaning up...' ) logger . indent += 2 for req in self . reqs_to_cleanup : req . remove_temporary_source ( ) remove_dir = [ ] if self . _pip_has_created_build_dir ( ) : remove_dir . append ( self . build_dir ) if bundle : remove_dir . append ( self . src_dir ) for dir in remove_dir : if os . path . exists ( dir ) : logger . info ( 'Removing temporary dir %s...' % dir ) rmtree ( dir ) logger . indent -= 2
def name ( self ) : name = self . _platform_impl . get_process_name ( ) if os . name == 'posix' : try : cmdline = self . cmdline except AccessDenied : pass else : if cmdline : extended_name = os . path . basename ( cmdline [ 0 ] ) if extended_name . startswith ( name ) : name = extended_name self . _platform_impl . _process_name = name return name
def exe ( self ) : def guess_it ( fallback ) : cmdline = self . cmdline if cmdline and hasattr ( os , 'access' ) and hasattr ( os , 'X_OK' ) : exe = cmdline [ 0 ] rexe = os . path . realpath ( exe ) if os . path . isabs ( rexe ) and os . path . isfile ( rexe ) and os . access ( rexe , os . X_OK ) : return exe if isinstance ( fallback , AccessDenied ) : raise fallback return fallback try : exe = self . _platform_impl . get_process_exe ( ) except AccessDenied : err = sys . exc_info ( ) [ 1 ] return guess_it ( fallback = err ) else : if not exe : try : exe = guess_it ( fallback = exe ) except AccessDenied : pass return exe
def is_running ( self ) : if self . _gone : return False try : return self . create_time == self . _platform_impl . get_process_create_time ( ) except NoSuchProcess : self . _gone = True return False
def suspend ( self ) : if not self . is_running ( ) : name = self . _platform_impl . _process_name raise NoSuchProcess ( self . pid , name ) if hasattr ( self . _platform_impl , "suspend_process" ) : self . _platform_impl . suspend_process ( ) else : self . send_signal ( signal . SIGSTOP )
def resume ( self ) : if not self . is_running ( ) : name = self . _platform_impl . _process_name raise NoSuchProcess ( self . pid , name ) if hasattr ( self . _platform_impl , "resume_process" ) : self . _platform_impl . resume_process ( ) else : self . send_signal ( signal . SIGCONT )
def kill ( self ) : if not self . is_running ( ) : name = self . _platform_impl . _process_name raise NoSuchProcess ( self . pid , name ) if os . name == 'posix' : self . send_signal ( signal . SIGKILL ) else : self . _platform_impl . kill_process ( )
def init_transformers ( self ) : self . _transformers = [ ] for transformer_cls in _default_transformers : transformer_cls ( shell = self . shell , prefilter_manager = self , config = self . config )
def register_transformer ( self , transformer ) : if transformer not in self . _transformers : self . _transformers . append ( transformer ) self . sort_transformers ( )
def unregister_transformer ( self , transformer ) : if transformer in self . _transformers : self . _transformers . remove ( transformer )
def init_checkers ( self ) : self . _checkers = [ ] for checker in _default_checkers : checker ( shell = self . shell , prefilter_manager = self , config = self . config )
def register_checker ( self , checker ) : if checker not in self . _checkers : self . _checkers . append ( checker ) self . sort_checkers ( )
def unregister_checker ( self , checker ) : if checker in self . _checkers : self . _checkers . remove ( checker )
def init_handlers ( self ) : self . _handlers = { } self . _esc_handlers = { } for handler in _default_handlers : handler ( shell = self . shell , prefilter_manager = self , config = self . config )
def register_handler ( self , name , handler , esc_strings ) : self . _handlers [ name ] = handler for esc_str in esc_strings : self . _esc_handlers [ esc_str ] = handler
def unregister_handler ( self , name , handler , esc_strings ) : try : del self . _handlers [ name ] except KeyError : pass for esc_str in esc_strings : h = self . _esc_handlers . get ( esc_str ) if h is handler : del self . _esc_handlers [ esc_str ]
def find_handler ( self , line_info ) : for checker in self . checkers : if checker . enabled : handler = checker . check ( line_info ) if handler : return handler return self . get_handler_by_name ( 'normal' )
def transform_line ( self , line , continue_prompt ) : for transformer in self . transformers : if transformer . enabled : line = transformer . transform ( line , continue_prompt ) return line
def check ( self , line_info ) : obj = self . shell . user_ns . get ( line_info . ifun , None ) if isinstance ( obj , IPyAutocall ) : obj . set_ip ( self . shell ) return self . prefilter_manager . get_handler_by_name ( 'auto' ) else : return None
def check ( self , line_info ) : if line_info . continue_prompt and self . prefilter_manager . multi_line_specials : if line_info . esc == ESC_MAGIC : return self . prefilter_manager . get_handler_by_name ( 'magic' ) else : return None
def check ( self , line_info ) : head = line_info . ifun . split ( '.' , 1 ) [ 0 ] if line_info . ifun not in self . shell . alias_manager or head not in self . shell . alias_manager or is_shadowed ( head , self . shell ) : return None return self . prefilter_manager . get_handler_by_name ( 'alias' )
def handle ( self , line_info ) : line = line_info . line continue_prompt = line_info . continue_prompt if ( continue_prompt and self . shell . autoindent and line . isspace ( ) and 0 < abs ( len ( line ) - self . shell . indent_current_nsp ) <= 2 ) : line = '' return line
def handle ( self , line_info ) : transformed = self . shell . alias_manager . expand_aliases ( line_info . ifun , line_info . the_rest ) line_out = '%sget_ipython().system(%r)' % ( line_info . pre_whitespace , transformed ) return line_out
def handle ( self , line_info ) : magic_handler = self . prefilter_manager . get_handler_by_name ( 'magic' ) line = line_info . line if line . lstrip ( ) . startswith ( ESC_SH_CAP ) : new_rest = line . lstrip ( ) [ 2 : ] line_info . line = '%ssx %s' % ( ESC_MAGIC , new_rest ) line_info . ifun = 'sx' line_info . the_rest = new_rest return magic_handler . handle ( line_info ) else : cmd = line . lstrip ( ) . lstrip ( ESC_SHELL ) line_out = '%sget_ipython().system(%r)' % ( line_info . pre_whitespace , cmd ) return line_out
def handle ( self , line_info ) : ifun = line_info . ifun the_rest = line_info . the_rest cmd = '%sget_ipython().magic(%r)' % ( line_info . pre_whitespace , ( ifun + " " + the_rest ) ) return cmd
def handle ( self , line_info ) : line = line_info . line ifun = line_info . ifun the_rest = line_info . the_rest pre = line_info . pre esc = line_info . esc continue_prompt = line_info . continue_prompt obj = line_info . ofind ( self . shell ) [ 'obj' ] if continue_prompt : return line force_auto = isinstance ( obj , IPyAutocall ) try : auto_rewrite = obj . rewrite except Exception : auto_rewrite = True if esc == ESC_QUOTE : newcmd = '%s("%s")' % ( ifun , '", "' . join ( the_rest . split ( ) ) ) elif esc == ESC_QUOTE2 : newcmd = '%s("%s")' % ( ifun , the_rest ) elif esc == ESC_PAREN : newcmd = '%s(%s)' % ( ifun , "," . join ( the_rest . split ( ) ) ) else : if force_auto : do_rewrite = not the_rest . startswith ( '(' ) else : if not the_rest : do_rewrite = ( self . shell . autocall >= 2 ) elif the_rest . startswith ( '[' ) and hasattr ( obj , '__getitem__' ) : do_rewrite = False else : do_rewrite = True if do_rewrite : if the_rest . endswith ( ';' ) : newcmd = '%s(%s);' % ( ifun . rstrip ( ) , the_rest [ : - 1 ] ) else : newcmd = '%s(%s)' % ( ifun . rstrip ( ) , the_rest ) else : normal_handler = self . prefilter_manager . get_handler_by_name ( 'normal' ) return normal_handler . handle ( line_info ) if auto_rewrite : self . shell . auto_rewrite_input ( newcmd ) return newcmd
def enterEvent ( self , event ) : super ( CallTipWidget , self ) . enterEvent ( event ) self . _hide_timer . stop ( )
def paintEvent ( self , event ) : painter = QtGui . QStylePainter ( self ) option = QtGui . QStyleOptionFrame ( ) option . initFrom ( self ) painter . drawPrimitive ( QtGui . QStyle . PE_PanelTipLabel , option ) painter . end ( ) super ( CallTipWidget , self ) . paintEvent ( event )
def show_tip ( self , tip ) : text_edit = self . _text_edit document = text_edit . document ( ) cursor = text_edit . textCursor ( ) search_pos = cursor . position ( ) - 1 self . _start_position , _ = self . _find_parenthesis ( search_pos , forward = False ) if self . _start_position == - 1 : return False self . setText ( tip ) self . resize ( self . sizeHint ( ) ) padding = 3 cursor_rect = text_edit . cursorRect ( cursor ) screen_rect = QtGui . qApp . desktop ( ) . screenGeometry ( text_edit ) point = text_edit . mapToGlobal ( cursor_rect . bottomRight ( ) ) point . setY ( point . y ( ) + padding ) tip_height = self . size ( ) . height ( ) tip_width = self . size ( ) . width ( ) vertical = 'bottom' horizontal = 'Right' if point . y ( ) + tip_height > screen_rect . height ( ) : point_ = text_edit . mapToGlobal ( cursor_rect . topRight ( ) ) if point_ . y ( ) - tip_height < padding : if 2 * point . y ( ) < screen_rect . height ( ) : vertical = 'bottom' else : vertical = 'top' else : vertical = 'top' if point . x ( ) + tip_width > screen_rect . width ( ) : point_ = text_edit . mapToGlobal ( cursor_rect . topRight ( ) ) if point_ . x ( ) - tip_width < padding : if 2 * point . x ( ) < screen_rect . width ( ) : horizontal = 'Right' else : horizontal = 'Left' else : horizontal = 'Left' pos = getattr ( cursor_rect , '%s%s' % ( vertical , horizontal ) ) point = text_edit . mapToGlobal ( pos ( ) ) if vertical == 'top' : point . setY ( point . y ( ) - tip_height - padding ) if horizontal == 'Left' : point . setX ( point . x ( ) - tip_width - padding ) self . move ( point ) self . show ( ) return True
def _cursor_position_changed ( self ) : cursor = self . _text_edit . textCursor ( ) if cursor . position ( ) <= self . _start_position : self . hide ( ) else : position , commas = self . _find_parenthesis ( self . _start_position + 1 ) if position != - 1 : self . hide ( )
def read ( * paths ) : with open ( os . path . join ( * paths ) , 'r' ) as file_handler : return file_handler . read ( )
def virtualenv_no_global ( ) : #this mirrors the logic in virtualenv.py for locating the no-global-site-packages.txt file site_mod_dir = os . path . dirname ( os . path . abspath ( site . __file__ ) ) no_global_file = os . path . join ( site_mod_dir , 'no-global-site-packages.txt' ) if running_under_virtualenv ( ) and os . path . isfile ( no_global_file ) : return True
def default_aliases ( ) : if os . name == 'posix' : default_aliases = [ ( 'mkdir' , 'mkdir' ) , ( 'rmdir' , 'rmdir' ) , ( 'mv' , 'mv -i' ) , ( 'rm' , 'rm -i' ) , ( 'cp' , 'cp -i' ) , ( 'cat' , 'cat' ) , ] if sys . platform . startswith ( 'linux' ) : ls_aliases = [ ( 'ls' , 'ls -F --color' ) , ( 'll' , 'ls -F -o --color' ) , ( 'lf' , 'ls -F -o --color %l | grep ^-' ) , ( 'lk' , 'ls -F -o --color %l | grep ^l' ) , ( 'ldir' , 'ls -F -o --color %l | grep /$' ) , ( 'lx' , 'ls -F -o --color %l | grep ^-..x' ) , ] else : ls_aliases = [ ( 'ls' , 'ls -F' ) , ( 'll' , 'ls -F -l' ) , ( 'lf' , 'ls -F -l %l | grep ^-' ) , ( 'lk' , 'ls -F -l %l | grep ^l' ) , ( 'ldir' , 'ls -F -l %l | grep /$' ) , ( 'lx' , 'ls -F -l %l | grep ^-..x' ) , ] default_aliases = default_aliases + ls_aliases elif os . name in [ 'nt' , 'dos' ] : default_aliases = [ ( 'ls' , 'dir /on' ) , ( 'ddir' , 'dir /ad /on' ) , ( 'ldir' , 'dir /ad /on' ) , ( 'mkdir' , 'mkdir' ) , ( 'rmdir' , 'rmdir' ) , ( 'echo' , 'echo' ) , ( 'ren' , 'ren' ) , ( 'copy' , 'copy' ) , ] else : default_aliases = [ ] return default_aliases
def soft_define_alias ( self , name , cmd ) : try : self . define_alias ( name , cmd ) except AliasError , e : error ( "Invalid alias: %s" % e )
def validate_alias ( self , name , cmd ) : if name in self . no_alias : raise InvalidAliasError ( "The name %s can't be aliased " "because it is a keyword or builtin." % name ) if not ( isinstance ( cmd , basestring ) ) : raise InvalidAliasError ( "An alias command must be a string, " "got: %r" % cmd ) nargs = cmd . count ( '%s' ) if nargs > 0 and cmd . find ( '%l' ) >= 0 : raise InvalidAliasError ( 'The %s and %l specifiers are mutually ' 'exclusive in alias definitions.' ) return nargs
def call_alias ( self , alias , rest = '' ) : cmd = self . transform_alias ( alias , rest ) try : self . shell . system ( cmd ) except : self . shell . showtraceback ( )
def transform_alias ( self , alias , rest = '' ) : nargs , cmd = self . alias_table [ alias ] if ' ' in cmd and os . path . isfile ( cmd ) : cmd = '"%s"' % cmd if cmd . find ( '%l' ) >= 0 : cmd = cmd . replace ( '%l' , rest ) rest = '' if nargs == 0 : cmd = '%s %s' % ( cmd , rest ) else : args = rest . split ( None , nargs ) if len ( args ) < nargs : raise AliasError ( 'Alias <%s> requires %s arguments, %s given.' % ( alias , nargs , len ( args ) ) ) cmd = '%s %s' % ( cmd % tuple ( args [ : nargs ] ) , ' ' . join ( args [ nargs : ] ) ) return cmd
def autohelp_directive ( dirname , arguments , options , content , lineno , content_offset , block_text , state , state_machine ) : config = Config ( parserClass = OptBucket , plugins = BuiltinPluginManager ( ) ) parser = config . getParser ( TestProgram . usage ( ) ) rst = ViewList ( ) for line in parser . format_help ( ) . split ( '\n' ) : rst . append ( line , '<autodoc>' ) rst . append ( 'Options' , '<autodoc>' ) rst . append ( '-------' , '<autodoc>' ) rst . append ( '' , '<autodoc>' ) for opt in parser : rst . append ( opt . options ( ) , '<autodoc>' ) rst . append ( '   \n' , '<autodoc>' ) rst . append ( '   ' + opt . help + '\n' , '<autodoc>' ) rst . append ( '\n' , '<autodoc>' ) node = nodes . section ( ) node . document = state . document surrounding_title_styles = state . memo . title_styles surrounding_section_level = state . memo . section_level state . memo . title_styles = [ ] state . memo . section_level = 0 state . nested_parse ( rst , 0 , node , match_titles = 1 ) state . memo . title_styles = surrounding_title_styles state . memo . section_level = surrounding_section_level return node . children
def reset_sgr ( self ) : self . intensity = 0 self . italic = False self . bold = False self . underline = False self . foreground_color = None self . background_color = None
def split_string ( self , string ) : self . actions = [ ] start = 0 last_char = '\n' if len ( string ) > 0 and string [ - 1 ] == '\n' else None string = string [ : - 1 ] if last_char is not None else string for match in ANSI_OR_SPECIAL_PATTERN . finditer ( string ) : raw = string [ start : match . start ( ) ] substring = SPECIAL_PATTERN . sub ( self . _replace_special , raw ) if substring or self . actions : yield substring self . actions = [ ] start = match . end ( ) groups = filter ( lambda x : x is not None , match . groups ( ) ) g0 = groups [ 0 ] if g0 == '\a' : self . actions . append ( BeepAction ( 'beep' ) ) yield None self . actions = [ ] elif g0 == '\r' : self . actions . append ( CarriageReturnAction ( 'carriage-return' ) ) yield None self . actions = [ ] elif g0 == '\b' : self . actions . append ( BackSpaceAction ( 'backspace' ) ) yield None self . actions = [ ] elif g0 == '\n' or g0 == '\r\n' : self . actions . append ( NewLineAction ( 'newline' ) ) yield g0 self . actions = [ ] else : params = [ param for param in groups [ 1 ] . split ( ';' ) if param ] if g0 . startswith ( '[' ) : try : params = map ( int , params ) except ValueError : pass else : self . set_csi_code ( groups [ 2 ] , params ) elif g0 . startswith ( ']' ) : self . set_osc_code ( params ) raw = string [ start : ] substring = SPECIAL_PATTERN . sub ( self . _replace_special , raw ) if substring or self . actions : yield substring if last_char is not None : self . actions . append ( NewLineAction ( 'newline' ) ) yield last_char
def get_format ( self ) : format = QtGui . QTextCharFormat ( ) qcolor = self . get_color ( self . foreground_color , self . intensity ) if qcolor is not None : format . setForeground ( qcolor ) qcolor = self . get_color ( self . background_color , self . intensity ) if qcolor is not None : format . setBackground ( qcolor ) if self . bold : format . setFontWeight ( QtGui . QFont . Bold ) else : format . setFontWeight ( QtGui . QFont . Normal ) format . setFontItalic ( self . italic ) format . setFontUnderline ( self . underline ) return format
def generate ( secret , age , * * payload ) : jti = str ( uuid . uuid1 ( ) ) if not payload : payload = { } payload [ 'exp' ] = int ( time . time ( ) + age ) payload [ 'jti' ] = jti return jwt . encode ( payload , decode_secret ( secret ) )
def mutex ( func ) : def wrapper ( * args , * * kwargs ) : """Decorator Wrapper""" lock = args [ 0 ] . lock lock . acquire ( True ) try : return func ( * args , * * kwargs ) except : raise finally : lock . release ( ) return wrapper
def _clean ( self ) : now = time . time ( ) for jwt in self . jwts . keys ( ) : if ( now - self . jwts [ jwt ] ) > ( self . age * 2 ) : del self . jwts [ jwt ]
def already_used ( self , tok ) : if tok in self . jwts : return True self . jwts [ tok ] = time . time ( ) return False
def valid ( self , token ) : now = time . time ( ) if 'Bearer ' in token : token = token [ 7 : ] data = None for secret in self . secrets : try : data = jwt . decode ( token , secret ) break except jwt . DecodeError : continue except jwt . ExpiredSignatureError : raise JwtFailed ( "Jwt expired" ) if not data : raise JwtFailed ( "Jwt cannot be decoded" ) exp = data . get ( 'exp' ) if not exp : raise JwtFailed ( "Jwt missing expiration (exp)" ) if now - exp > self . age : raise JwtFailed ( "Jwt bad expiration - greater than I want to accept" ) jti = data . get ( 'jti' ) if not jti : raise JwtFailed ( "Jwt missing one-time id (jti)" ) if self . already_used ( jti ) : raise JwtFailed ( "Jwt re-use disallowed (jti={})" . format ( jti ) ) return data
def write ( self , nb , fp , * * kwargs ) : return fp . write ( self . writes ( nb , * * kwargs ) )
def can_cut ( self ) : cursor = self . _control . textCursor ( ) return ( cursor . hasSelection ( ) and self . _in_buffer ( cursor . anchor ( ) ) and self . _in_buffer ( cursor . position ( ) ) )
def can_paste ( self ) : if self . _control . textInteractionFlags ( ) & QtCore . Qt . TextEditable : return bool ( QtGui . QApplication . clipboard ( ) . text ( ) ) return False
def _set_font ( self , font ) : font_metrics = QtGui . QFontMetrics ( font ) self . _control . setTabStopWidth ( self . tab_width * font_metrics . width ( ' ' ) ) self . _completion_widget . setFont ( font ) self . _control . document ( ) . setDefaultFont ( font ) if self . _page_control : self . _page_control . document ( ) . setDefaultFont ( font ) self . font_changed . emit ( font )
def print_ ( self , printer = None ) : if ( not printer ) : printer = QtGui . QPrinter ( ) if ( QtGui . QPrintDialog ( printer ) . exec_ ( ) != QtGui . QDialog . Accepted ) : return self . _control . print_ ( printer )
def prompt_to_top ( self ) : if not self . _executing : prompt_cursor = self . _get_prompt_cursor ( ) if self . _get_cursor ( ) . blockNumber ( ) < prompt_cursor . blockNumber ( ) : self . _set_cursor ( prompt_cursor ) self . _set_top_cursor ( prompt_cursor )
def reset_font ( self ) : if sys . platform == 'win32' : fallback = 'Courier' elif sys . platform == 'darwin' : fallback = 'Monaco' else : fallback = 'Monospace' font = get_font ( self . font_family , fallback ) if self . font_size : font . setPointSize ( self . font_size ) else : font . setPointSize ( QtGui . qApp . font ( ) . pointSize ( ) ) font . setStyleHint ( QtGui . QFont . TypeWriter ) self . _set_font ( font )
def _append_html ( self , html , before_prompt = False ) : self . _append_custom ( self . _insert_html , html , before_prompt )
def _append_html_fetching_plain_text ( self , html , before_prompt = False ) : return self . _append_custom ( self . _insert_html_fetching_plain_text , html , before_prompt )
def _append_plain_text ( self , text , before_prompt = False ) : self . _append_custom ( self . _insert_plain_text , text , before_prompt )
def _complete_with_items ( self , cursor , items ) : self . _cancel_completion ( ) if len ( items ) == 1 : cursor . setPosition ( self . _control . textCursor ( ) . position ( ) , QtGui . QTextCursor . KeepAnchor ) cursor . insertText ( items [ 0 ] ) elif len ( items ) > 1 : current_pos = self . _control . textCursor ( ) . position ( ) prefix = commonprefix ( items ) if prefix : cursor . setPosition ( current_pos , QtGui . QTextCursor . KeepAnchor ) cursor . insertText ( prefix ) current_pos = cursor . position ( ) cursor . movePosition ( QtGui . QTextCursor . Left , n = len ( prefix ) ) self . _completion_widget . show_items ( cursor , items )
def _fill_temporary_buffer ( self , cursor , text , html = False ) : current_pos = self . _control . textCursor ( ) . position ( ) cursor . beginEditBlock ( ) self . _append_plain_text ( '\n' ) self . _page ( text , html = html ) cursor . endEditBlock ( ) cursor . setPosition ( current_pos ) self . _control . moveCursor ( QtGui . QTextCursor . End ) self . _control . setTextCursor ( cursor ) self . _temp_buffer_filled = True
def _create_control ( self ) : if self . custom_control : control = self . custom_control ( ) elif self . kind == 'plain' : control = QtGui . QPlainTextEdit ( ) elif self . kind == 'rich' : control = QtGui . QTextEdit ( ) control . setAcceptRichText ( False ) control . installEventFilter ( self ) control . viewport ( ) . installEventFilter ( self ) control . customContextMenuRequested . connect ( self . _custom_context_menu_requested ) control . copyAvailable . connect ( self . copy_available ) control . redoAvailable . connect ( self . redo_available ) control . undoAvailable . connect ( self . undo_available ) layout = control . document ( ) . documentLayout ( ) layout . documentSizeChanged . disconnect ( ) layout . documentSizeChanged . connect ( self . _adjust_scrollbars ) control . setAttribute ( QtCore . Qt . WA_InputMethodEnabled , True ) control . setContextMenuPolicy ( QtCore . Qt . CustomContextMenu ) control . setReadOnly ( True ) control . setUndoRedoEnabled ( False ) control . setVerticalScrollBarPolicy ( QtCore . Qt . ScrollBarAlwaysOn ) return control
def _create_page_control ( self ) : if self . custom_page_control : control = self . custom_page_control ( ) elif self . kind == 'plain' : control = QtGui . QPlainTextEdit ( ) elif self . kind == 'rich' : control = QtGui . QTextEdit ( ) control . installEventFilter ( self ) viewport = control . viewport ( ) viewport . installEventFilter ( self ) control . setReadOnly ( True ) control . setUndoRedoEnabled ( False ) control . setVerticalScrollBarPolicy ( QtCore . Qt . ScrollBarAlwaysOn ) return control
def _get_block_plain_text ( self , block ) : cursor = QtGui . QTextCursor ( block ) cursor . movePosition ( QtGui . QTextCursor . StartOfBlock ) cursor . movePosition ( QtGui . QTextCursor . EndOfBlock , QtGui . QTextCursor . KeepAnchor ) return cursor . selection ( ) . toPlainText ( )
def _get_end_cursor ( self ) : cursor = self . _control . textCursor ( ) cursor . movePosition ( QtGui . QTextCursor . End ) return cursor
def _get_prompt_cursor ( self ) : cursor = self . _control . textCursor ( ) cursor . setPosition ( self . _prompt_pos ) return cursor
def _insert_continuation_prompt ( self , cursor ) : if self . _continuation_prompt_html is None : self . _insert_plain_text ( cursor , self . _continuation_prompt ) else : self . _continuation_prompt = self . _insert_html_fetching_plain_text ( cursor , self . _continuation_prompt_html )
def _keyboard_quit ( self ) : if self . _temp_buffer_filled : self . _cancel_completion ( ) self . _clear_temporary_buffer ( ) else : self . input_buffer = ''
def _prompt_started ( self ) : self . _control . document ( ) . setMaximumBlockCount ( 0 ) self . _control . setUndoRedoEnabled ( True ) self . _control . setReadOnly ( False ) self . _control . setAttribute ( QtCore . Qt . WA_InputMethodEnabled , True ) if not self . _reading : self . _executing = False self . _prompt_started_hook ( ) if self . _input_buffer_pending : self . input_buffer = self . _input_buffer_pending self . _input_buffer_pending = '' self . _control . moveCursor ( QtGui . QTextCursor . End )
def _set_top_cursor ( self , cursor ) : scrollbar = self . _control . verticalScrollBar ( ) scrollbar . setValue ( scrollbar . maximum ( ) ) original_cursor = self . _control . textCursor ( ) self . _control . setTextCursor ( cursor ) self . _control . ensureCursorVisible ( ) self . _control . setTextCursor ( original_cursor )
def _adjust_scrollbars ( self ) : document = self . _control . document ( ) scrollbar = self . _control . verticalScrollBar ( ) viewport_height = self . _control . viewport ( ) . height ( ) if isinstance ( self . _control , QtGui . QPlainTextEdit ) : maximum = max ( 0 , document . lineCount ( ) - 1 ) step = viewport_height / self . _control . fontMetrics ( ) . lineSpacing ( ) else : maximum = document . size ( ) . height ( ) step = viewport_height diff = maximum - scrollbar . maximum ( ) scrollbar . setRange ( 0 , maximum ) scrollbar . setPageStep ( step ) if diff < 0 and document . blockCount ( ) == document . maximumBlockCount ( ) : scrollbar . setValue ( scrollbar . value ( ) + diff )
def dist_in_usersite ( dist ) : if user_site : return normalize_path ( dist_location ( dist ) ) . startswith ( normalize_path ( user_site ) ) else : return False
def print_results ( distributions , list_all_files ) : results_printed = False for dist in distributions : results_printed = True logger . info ( "---" ) logger . info ( "Name: %s" % dist [ 'name' ] ) logger . info ( "Version: %s" % dist [ 'version' ] ) logger . info ( "Location: %s" % dist [ 'location' ] ) logger . info ( "Requires: %s" % ', ' . join ( dist [ 'requires' ] ) ) if list_all_files : logger . info ( "Files:" ) if dist [ 'files' ] is not None : for line in dist [ 'files' ] : logger . info ( "  %s" % line . strip ( ) ) else : logger . info ( "Cannot locate installed-files.txt" ) return results_printed
def main ( args = None ) : options , paths = _parse_options ( args ) format = getattr ( options , 'output' , 'simple' ) formatter = _FORMATTERS [ format ] ( options ) for path in paths : meta = get_metadata ( path , options . metadata_version ) if meta is None : continue if options . download_url_prefix : if meta . download_url is None : filename = os . path . basename ( path ) meta . download_url = '%s/%s' % ( options . download_url_prefix , filename ) formatter ( meta ) formatter . finish ( )
def cmp_to_key ( mycmp ) : class Key ( object ) : def __init__ ( self , obj ) : self . obj = obj def __lt__ ( self , other ) : return mycmp ( self . obj , other . obj ) < 0 def __gt__ ( self , other ) : return mycmp ( self . obj , other . obj ) > 0 def __eq__ ( self , other ) : return mycmp ( self . obj , other . obj ) == 0 return Key
def file_read ( filename ) : fobj = open ( filename , 'r' ) source = fobj . read ( ) fobj . close ( ) return source
def close ( self ) : self . flush ( ) setattr ( sys , self . channel , self . ostream ) self . file . close ( ) self . _closed = True
def write ( self , data ) : self . file . write ( data ) self . ostream . write ( data ) self . ostream . flush ( )
def add_new_heart_handler ( self , handler ) : self . log . debug ( "heartbeat::new_heart_handler: %s" , handler ) self . _new_handlers . add ( handler )
def add_heart_failure_handler ( self , handler ) : self . log . debug ( "heartbeat::new heart failure handler: %s" , handler ) self . _failure_handlers . add ( handler )
def handle_pong ( self , msg ) : current = str_to_bytes ( str ( self . lifetime ) ) last = str_to_bytes ( str ( self . last_ping ) ) if msg [ 1 ] == current : delta = time . time ( ) - self . tic self . responses . add ( msg [ 0 ] ) elif msg [ 1 ] == last : delta = time . time ( ) - self . tic + ( self . lifetime - self . last_ping ) self . log . warn ( "heartbeat::heart %r missed a beat, and took %.2f ms to respond" , msg [ 0 ] , 1000 * delta ) self . responses . add ( msg [ 0 ] ) else : self . log . warn ( "heartbeat::got bad heartbeat (possibly old?): %s (current=%.3f)" , msg [ 1 ] , self . lifetime )
def displayAll ( elapsed , display_amt , est_end , nLoops , count , numPrints ) : if numPrints > nLoops : display_amt = 1 else : display_amt = round ( nLoops / numPrints ) if count % display_amt == 0 : avg = elapsed / count est_end = round ( avg * nLoops ) ( disp_elapsed , disp_avg , disp_est ) = timeUnit ( int ( round ( elapsed ) ) , int ( round ( avg ) ) , int ( round ( est_end ) ) ) print "%s%%" % str ( round ( count / float ( nLoops ) * 100 ) ) , "@" + str ( count ) , totalTime = disp_est [ 0 ] unit = disp_est [ 1 ] if str ( unit ) == "secs" : remain = totalTime - round ( elapsed ) remainUnit = "secs" elif str ( unit ) == "mins" : remain = totalTime - round ( elapsed ) / 60 remainUnit = "mins" elif str ( unit ) == "hr" : remain = totalTime - round ( elapsed ) / 3600 remainUnit = "hr" print "ETA: %s %s" % ( str ( remain ) , remainUnit ) print return
def timeUnit ( elapsed , avg , est_end ) : minute = 60 hr = 3600 day = 86400 if elapsed <= 3 * minute : unit_elapsed = ( elapsed , "secs" ) if elapsed > 3 * minute : unit_elapsed = ( ( elapsed / 60 ) , "mins" ) if elapsed > 3 * hr : unit_elapsed = ( ( elapsed / 3600 ) , "hr" ) if avg <= 3 * minute : unit_avg = ( avg , "secs" ) if avg > 3 * minute : unit_avg = ( ( avg / 60 ) , "mins" ) if avg > 3 * hr : unit_avg = ( ( avg / 3600 ) , "hr" ) if est_end <= 3 * minute : unit_estEnd = ( est_end , "secs" ) if est_end > 3 * minute : unit_estEnd = ( ( est_end / 60 ) , "mins" ) if est_end > 3 * hr : unit_estEnd = ( ( est_end / 3600 ) , "hr" ) return [ unit_elapsed , unit_avg , unit_estEnd ]
def uncache_zipdir ( path ) : from zipimport import _zip_directory_cache as zdc _uncache ( path , zdc ) _uncache ( path , sys . path_importer_cache )
def nt_quote_arg ( arg ) : result = [ ] needquote = False nb = 0 needquote = ( " " in arg ) or ( "\t" in arg ) if needquote : result . append ( '"' ) for c in arg : if c == '\\' : nb += 1 elif c == '"' : result . append ( '\\' * ( nb * 2 ) + '\\"' ) nb = 0 else : if nb : result . append ( '\\' * nb ) nb = 0 result . append ( c ) if nb : result . append ( '\\' * nb ) if needquote : result . append ( '\\' * nb ) result . append ( '"' ) return '' . join ( result )
def install_script ( self , dist , script_name , script_text , dev_path = None ) : spec = str ( dist . as_requirement ( ) ) is_script = is_python_script ( script_text , script_name ) def get_template ( filename ) : raw_bytes = resource_string ( 'setuptools' , template_name ) template_str = raw_bytes . decode ( 'utf-8' ) clean_template = template_str . replace ( '"""' , '' ) return clean_template if is_script : template_name = 'script template.py' if dev_path : template_name = template_name . replace ( '.py' , ' (dev).py' ) script_text = ( get_script_header ( script_text ) + get_template ( template_name ) % locals ( ) ) self . write_script ( script_name , _to_ascii ( script_text ) , 'b' )
def check_conflicts ( self , dist ) : return dist from imp import find_module , get_suffixes from glob import glob blockers = [ ] names = dict . fromkeys ( dist . _get_metadata ( 'top_level.txt' ) ) exts = { '.pyc' : 1 , '.pyo' : 1 } for ext , mode , typ in get_suffixes ( ) : exts [ ext ] = 1 for path , files in expand_paths ( [ self . install_dir ] + self . all_site_dirs ) : for filename in files : base , ext = os . path . splitext ( filename ) if base in names : if not ext : try : f , filename , descr = find_module ( base , [ path ] ) except ImportError : continue else : if f : f . close ( ) if filename not in blockers : blockers . append ( filename ) elif ext in exts and base != 'site' : blockers . append ( os . path . join ( path , filename ) ) if blockers : self . found_conflicts ( dist , blockers ) return dist
def create_home_path ( self ) : if not self . user : return home = convert_path ( os . path . expanduser ( "~" ) ) for name , path in self . config_vars . iteritems ( ) : if path . startswith ( home ) and not os . path . isdir ( path ) : self . debug_print ( "os.makedirs('%s', 0700)" % path ) os . makedirs ( path , 0700 )
def is_archive_file ( name ) : archives = ( '.zip' , '.tar.gz' , '.tar.bz2' , '.tgz' , '.tar' , '.whl' ) ext = splitext ( name ) [ 1 ] . lower ( ) if ext in archives : return True return False
def new_output ( output_type = None , output_text = None , output_png = None , output_html = None , output_svg = None , output_latex = None , output_json = None , output_javascript = None , output_jpeg = None , prompt_number = None , etype = None , evalue = None , traceback = None ) : output = NotebookNode ( ) if output_type is not None : output . output_type = unicode ( output_type ) if output_type != 'pyerr' : if output_text is not None : output . text = unicode ( output_text ) if output_png is not None : output . png = bytes ( output_png ) if output_jpeg is not None : output . jpeg = bytes ( output_jpeg ) if output_html is not None : output . html = unicode ( output_html ) if output_svg is not None : output . svg = unicode ( output_svg ) if output_latex is not None : output . latex = unicode ( output_latex ) if output_json is not None : output . json = unicode ( output_json ) if output_javascript is not None : output . javascript = unicode ( output_javascript ) if output_type == u'pyout' : if prompt_number is not None : output . prompt_number = int ( prompt_number ) if output_type == u'pyerr' : if etype is not None : output . etype = unicode ( etype ) if evalue is not None : output . evalue = unicode ( evalue ) if traceback is not None : output . traceback = [ unicode ( frame ) for frame in list ( traceback ) ] return output
def new_code_cell ( input = None , prompt_number = None , outputs = None , language = u'python' , collapsed = False , metadata = None ) : cell = NotebookNode ( ) cell . cell_type = u'code' if language is not None : cell . language = unicode ( language ) if input is not None : cell . input = unicode ( input ) if prompt_number is not None : cell . prompt_number = int ( prompt_number ) if outputs is None : cell . outputs = [ ] else : cell . outputs = outputs if collapsed is not None : cell . collapsed = bool ( collapsed ) cell . metadata = NotebookNode ( metadata or { } ) return cell
def new_text_cell ( cell_type , source = None , rendered = None , metadata = None ) : cell = NotebookNode ( ) if cell_type == 'plaintext' : cell_type = 'raw' if source is not None : cell . source = unicode ( source ) if rendered is not None : cell . rendered = unicode ( rendered ) cell . metadata = NotebookNode ( metadata or { } ) cell . cell_type = cell_type return cell
def new_heading_cell ( source = None , rendered = None , level = 1 , metadata = None ) : cell = NotebookNode ( ) cell . cell_type = u'heading' if source is not None : cell . source = unicode ( source ) if rendered is not None : cell . rendered = unicode ( rendered ) cell . level = int ( level ) cell . metadata = NotebookNode ( metadata or { } ) return cell
def new_notebook ( name = None , metadata = None , worksheets = None ) : nb = NotebookNode ( ) nb . nbformat = nbformat nb . nbformat_minor = nbformat_minor if worksheets is None : nb . worksheets = [ ] else : nb . worksheets = list ( worksheets ) if metadata is None : nb . metadata = new_metadata ( ) else : nb . metadata = NotebookNode ( metadata ) if name is not None : nb . metadata . name = unicode ( name ) return nb
def new_metadata ( name = None , authors = None , license = None , created = None , modified = None , gistid = None ) : metadata = NotebookNode ( ) if name is not None : metadata . name = unicode ( name ) if authors is not None : metadata . authors = list ( authors ) if created is not None : metadata . created = unicode ( created ) if modified is not None : metadata . modified = unicode ( modified ) if license is not None : metadata . license = unicode ( license ) if gistid is not None : metadata . gistid = unicode ( gistid ) return metadata
def new_author ( name = None , email = None , affiliation = None , url = None ) : author = NotebookNode ( ) if name is not None : author . name = unicode ( name ) if email is not None : author . email = unicode ( email ) if affiliation is not None : author . affiliation = unicode ( affiliation ) if url is not None : author . url = unicode ( url ) return author
def _writable_dir ( path ) : return os . path . isdir ( path ) and os . access ( path , os . W_OK )
def unquote_filename ( name , win32 = ( sys . platform == 'win32' ) ) : if win32 : if name . startswith ( ( "'" , '"' ) ) and name . endswith ( ( "'" , '"' ) ) : name = name [ 1 : - 1 ] return name
def get_ipython_package_dir ( ) : ipdir = os . path . dirname ( IPython . __file__ ) return py3compat . cast_unicode ( ipdir , fs_encoding )
def update_suggestions_dictionary ( request , object ) : if request . user . is_authenticated ( ) : user = request . user content_type = ContentType . objects . get_for_model ( type ( object ) ) try : ObjectView . objects . get ( user = user , object_id = object . id , content_type = content_type ) except : ObjectView . objects . create ( user = user , content_object = object ) viewed = ObjectView . objects . filter ( user = user ) else : update_dict_for_guests ( request , object , content_type ) return if viewed : for obj in viewed : if content_type == obj . content_type : if not exists_in_dictionary ( request , object , content_type , obj , True ) : if object . id != obj . object_id : ObjectViewDictionary . objects . create ( current_object = object , visited_before_object = obj . content_object ) if not exists_in_dictionary ( request , obj , obj . content_type , object , False ) : ObjectViewDictionary . objects . create ( current_object = obj . content_object , visited_before_object = object ) return
def get_suggestions_with_size ( object , size ) : content_type = ContentType . objects . get_for_model ( type ( object ) ) try : return ObjectViewDictionary . objects . filter ( current_object_id = object . id , current_content_type = content_type ) . extra ( order_by = [ '-visits' ] ) [ : size ] except : return ObjectViewDictionary . objects . filter ( current_object_id = object . id , current_content_type = content_type ) . extra ( order_by = [ '-visits' ] )
def get_suggestions ( object ) : content_type = ContentType . objects . get_for_model ( type ( object ) ) return ObjectViewDictionary . objects . filter ( current_object_id = object . id , current_content_type = content_type ) . extra ( order_by = [ '-visits' ] )
def options ( self , parser , env ) : if not self . available ( ) : return Plugin . options ( self , parser , env ) parser . add_option ( '--profile-sort' , action = 'store' , dest = 'profile_sort' , default = env . get ( 'NOSE_PROFILE_SORT' , 'cumulative' ) , metavar = "SORT" , help = "Set sort order for profiler output" ) parser . add_option ( '--profile-stats-file' , action = 'store' , dest = 'profile_stats_file' , metavar = "FILE" , default = env . get ( 'NOSE_PROFILE_STATS_FILE' ) , help = 'Profiler stats file; default is a new ' 'temp file on each run' ) parser . add_option ( '--profile-restrict' , action = 'append' , dest = 'profile_restrict' , metavar = "RESTRICT" , default = env . get ( 'NOSE_PROFILE_RESTRICT' ) , help = "Restrict profiler output. See help for " "pstats.Stats for details" )
def begin ( self ) : if not self . available ( ) : return self . _create_pfile ( ) self . prof = hotshot . Profile ( self . pfile )
def report ( self , stream ) : log . debug ( 'printing profiler report' ) self . prof . close ( ) prof_stats = stats . load ( self . pfile ) prof_stats . sort_stats ( self . sort ) compat_25 = hasattr ( prof_stats , 'stream' ) if compat_25 : tmp = prof_stats . stream prof_stats . stream = stream else : tmp = sys . stdout sys . stdout = stream try : if self . restrict : log . debug ( 'setting profiler restriction to %s' , self . restrict ) prof_stats . print_stats ( * self . restrict ) else : prof_stats . print_stats ( ) finally : if compat_25 : prof_stats . stream = tmp else : sys . stdout = tmp
def finalize ( self , result ) : if not self . available ( ) : return try : self . prof . close ( ) except AttributeError : pass if self . clean_stats_file : if self . fileno : try : os . close ( self . fileno ) except OSError : pass try : os . unlink ( self . pfile ) except OSError : pass return None
def setup_partitioner ( index , num_procs , gnum_cells , parts ) : global partitioner p = MPIRectPartitioner2D ( my_id = index , num_procs = num_procs ) p . redim ( global_num_cells = gnum_cells , num_parts = parts ) p . prepare_communication ( ) partitioner = p
def wave_saver ( u , x , y , t ) : global u_hist global t_hist t_hist . append ( t ) u_hist . append ( 1.0 * u )
def init_db ( self ) : self . db = sqlite3 . connect ( self . hist_file , detect_types = sqlite3 . PARSE_DECLTYPES | sqlite3 . PARSE_COLNAMES ) self . db . execute ( ) self . db . execute ( ) self . db . execute ( ) self . db . commit ( )
def new_session ( self , conn = None ) : if conn is None : conn = self . db with conn : cur = conn . execute ( , ( datetime . datetime . now ( ) , ) ) self . session_number = cur . lastrowid
def end_session ( self ) : self . writeout_cache ( ) with self . db : self . db . execute ( , ( datetime . datetime . now ( ) , len ( self . input_hist_parsed ) - 1 , self . session_number ) ) self . session_number = 0
def name_session ( self , name ) : with self . db : self . db . execute ( "UPDATE sessions SET remark=? WHERE session==?" , ( name , self . session_number ) )
def writeout_cache ( self , conn = None ) : if conn is None : conn = self . db with self . db_input_cache_lock : try : self . _writeout_input_cache ( conn ) except sqlite3 . IntegrityError : self . new_session ( conn ) print ( "ERROR! Session/line number was not unique in" , "database. History logging moved to new session" , self . session_number ) try : self . _writeout_input_cache ( conn ) except sqlite3 . IntegrityError : pass finally : self . db_input_cache = [ ] with self . db_output_cache_lock : try : self . _writeout_output_cache ( conn ) except sqlite3 . IntegrityError : print ( "!! Session/line number for output was not unique" , "in database. Output will not be stored." ) finally : self . db_output_cache = [ ]
def _get_num_cpus ( ) : try : return os . sysconf ( "SC_NPROCESSORS_ONLN" ) except ValueError : num = 0 f = open ( '/proc/cpuinfo' , 'r' ) try : lines = f . readlines ( ) finally : f . close ( ) for line in lines : if line . lower ( ) . startswith ( 'processor' ) : num += 1 if num == 0 : f = open ( '/proc/stat' , 'r' ) try : lines = f . readlines ( ) finally : f . close ( ) search = re . compile ( 'cpu\d' ) for line in lines : line = line . split ( ' ' ) [ 0 ] if search . match ( line ) : num += 1 if num == 0 : raise RuntimeError ( "can't determine number of CPUs" ) return num
def disk_partitions ( all = False ) : phydevs = [ ] f = open ( "/proc/filesystems" , "r" ) try : for line in f : if not line . startswith ( "nodev" ) : phydevs . append ( line . strip ( ) ) finally : f . close ( ) retlist = [ ] partitions = _psutil_linux . get_disk_partitions ( ) for partition in partitions : device , mountpoint , fstype , opts = partition if device == 'none' : device = '' if not all : if device == '' or fstype not in phydevs : continue ntuple = nt_partition ( device , mountpoint , fstype , opts ) retlist . append ( ntuple ) return retlist
def get_system_users ( ) : retlist = [ ] rawlist = _psutil_linux . get_system_users ( ) for item in rawlist : user , tty , hostname , tstamp , user_process = item if not user_process : continue if hostname == ':0.0' : hostname = 'localhost' nt = nt_user ( user , tty or None , hostname , tstamp ) retlist . append ( nt ) return retlist
def get_pid_list ( ) : pids = [ int ( x ) for x in os . listdir ( '/proc' ) if x . isdigit ( ) ] return pids
def short_stack ( ) : stack = inspect . stack ( ) [ : 0 : - 1 ] return "\n" . join ( [ "%30s : %s @%d" % ( t [ 3 ] , t [ 1 ] , t [ 2 ] ) for t in stack ] )
def join_regex ( regexes ) : if len ( regexes ) > 1 : return "|" . join ( [ "(%s)" % r for r in regexes ] ) elif regexes : return regexes [ 0 ] else : return ""
def file_be_gone ( path ) : try : os . remove ( path ) except OSError : _ , e , _ = sys . exc_info ( ) if e . errno != errno . ENOENT : raise
def update ( self , v ) : self . md5 . update ( to_bytes ( str ( type ( v ) ) ) ) if isinstance ( v , string_class ) : self . md5 . update ( to_bytes ( v ) ) elif v is None : pass elif isinstance ( v , ( int , float ) ) : self . md5 . update ( to_bytes ( str ( v ) ) ) elif isinstance ( v , ( tuple , list ) ) : for e in v : self . update ( e ) elif isinstance ( v , dict ) : keys = v . keys ( ) for k in sorted ( keys ) : self . update ( k ) self . update ( v [ k ] ) else : for k in dir ( v ) : if k . startswith ( '__' ) : continue a = getattr ( v , k ) if inspect . isroutine ( a ) : continue self . update ( k ) self . update ( a )
def update_profiles ( self ) : for path in [ get_ipython_dir ( ) , os . getcwdu ( ) ] : for profile in list_profiles_in ( path ) : pd = self . get_profile_dir ( profile , path ) if profile not in self . profiles : self . log . debug ( "Adding cluster profile '%s'" % profile ) self . profiles [ profile ] = { 'profile' : profile , 'profile_dir' : pd , 'status' : 'stopped' }
def start_cluster ( self , profile , n = None ) : self . check_profile ( profile ) data = self . profiles [ profile ] if data [ 'status' ] == 'running' : raise web . HTTPError ( 409 , u'cluster already running' ) cl , esl , default_n = self . build_launchers ( data [ 'profile_dir' ] ) n = n if n is not None else default_n def clean_data ( ) : data . pop ( 'controller_launcher' , None ) data . pop ( 'engine_set_launcher' , None ) data . pop ( 'n' , None ) data [ 'status' ] = 'stopped' def engines_stopped ( r ) : self . log . debug ( 'Engines stopped' ) if cl . running : cl . stop ( ) clean_data ( ) esl . on_stop ( engines_stopped ) def controller_stopped ( r ) : self . log . debug ( 'Controller stopped' ) if esl . running : esl . stop ( ) clean_data ( ) cl . on_stop ( controller_stopped ) dc = ioloop . DelayedCallback ( lambda : cl . start ( ) , 0 , self . loop ) dc . start ( ) dc = ioloop . DelayedCallback ( lambda : esl . start ( n ) , 1000 * self . delay , self . loop ) dc . start ( ) self . log . debug ( 'Cluster started' ) data [ 'controller_launcher' ] = cl data [ 'engine_set_launcher' ] = esl data [ 'n' ] = n data [ 'status' ] = 'running' return self . profile_info ( profile )
def stop_cluster ( self , profile ) : self . check_profile ( profile ) data = self . profiles [ profile ] if data [ 'status' ] == 'stopped' : raise web . HTTPError ( 409 , u'cluster not running' ) data = self . profiles [ profile ] cl = data [ 'controller_launcher' ] esl = data [ 'engine_set_launcher' ] if cl . running : cl . stop ( ) if esl . running : esl . stop ( ) result = { 'profile' : data [ 'profile' ] , 'profile_dir' : data [ 'profile_dir' ] , 'status' : 'stopped' } return result
def _find_cmd ( cmd ) : try : from win32api import SearchPath except ImportError : raise ImportError ( 'you need to have pywin32 installed for this to work' ) else : PATH = os . environ [ 'PATH' ] extensions = [ '.exe' , '.com' , '.bat' , '.py' ] path = None for ext in extensions : try : path = SearchPath ( PATH , cmd + ext ) [ 0 ] except : pass if path is None : raise OSError ( "command %r not found" % cmd ) else : return path
def _system_body ( p ) : enc = DEFAULT_ENCODING for line in read_no_interrupt ( p . stdout ) . splitlines ( ) : line = line . decode ( enc , 'replace' ) print ( line , file = sys . stdout ) for line in read_no_interrupt ( p . stderr ) . splitlines ( ) : line = line . decode ( enc , 'replace' ) print ( line , file = sys . stderr ) return p . wait ( )
def setup_partitioner ( comm , addrs , index , num_procs , gnum_cells , parts ) : global partitioner p = ZMQRectPartitioner2D ( comm , addrs , my_id = index , num_procs = num_procs ) p . redim ( global_num_cells = gnum_cells , num_parts = parts ) p . prepare_communication ( ) partitioner = p
def init_gui_pylab ( self ) : if self . gui or self . pylab : shell = self . shell try : if self . pylab : gui , backend = pylabtools . find_gui_and_backend ( self . pylab ) self . log . info ( "Enabling GUI event loop integration, " "toolkit=%s, pylab=%s" % ( gui , self . pylab ) ) shell . enable_pylab ( gui , import_all = self . pylab_import_all ) else : self . log . info ( "Enabling GUI event loop integration, " "toolkit=%s" % self . gui ) shell . enable_gui ( self . gui ) except Exception : self . log . warn ( "GUI event loop or pylab initialization failed" ) self . shell . showtraceback ( )
def init_code ( self ) : self . _run_startup_files ( ) self . _run_exec_lines ( ) self . _run_exec_files ( ) self . _run_cmd_line_code ( ) self . _run_module ( ) sys . stdout . flush ( ) sys . stderr . flush ( ) self . shell . user_ns_hidden . update ( self . shell . user_ns )
def _run_exec_lines ( self ) : if not self . exec_lines : return try : self . log . debug ( "Running code from IPythonApp.exec_lines..." ) for line in self . exec_lines : try : self . log . info ( "Running code in user namespace: %s" % line ) self . shell . run_cell ( line , store_history = False ) except : self . log . warn ( "Error in executing line in user " "namespace: %s" % line ) self . shell . showtraceback ( ) except : self . log . warn ( "Unknown error in handling IPythonApp.exec_lines:" ) self . shell . showtraceback ( )
def _run_startup_files ( self ) : startup_dir = self . profile_dir . startup_dir startup_files = glob . glob ( os . path . join ( startup_dir , '*.py' ) ) startup_files += glob . glob ( os . path . join ( startup_dir , '*.ipy' ) ) if not startup_files : return self . log . debug ( "Running startup files from %s..." , startup_dir ) try : for fname in sorted ( startup_files ) : self . _exec_file ( fname ) except : self . log . warn ( "Unknown error in handling startup files:" ) self . shell . showtraceback ( )
def _run_exec_files ( self ) : if not self . exec_files : return self . log . debug ( "Running files in IPythonApp.exec_files..." ) try : for fname in self . exec_files : self . _exec_file ( fname ) except : self . log . warn ( "Unknown error in handling IPythonApp.exec_files:" ) self . shell . showtraceback ( )
def _run_cmd_line_code ( self ) : if self . code_to_run : line = self . code_to_run try : self . log . info ( "Running code given at command line (c=): %s" % line ) self . shell . run_cell ( line , store_history = False ) except : self . log . warn ( "Error in executing line in user namespace: %s" % line ) self . shell . showtraceback ( ) elif self . file_to_run : fname = self . file_to_run try : self . _exec_file ( fname ) except : self . log . warn ( "Error in executing file in user namespace: %s" % fname ) self . shell . showtraceback ( )
def _run_module ( self ) : if self . module_to_run : save_argv = sys . argv sys . argv = [ sys . executable ] + self . extra_args try : self . shell . safe_run_module ( self . module_to_run , self . shell . user_ns ) finally : sys . argv = save_argv
def generic ( func ) : _sentinel = object ( ) def _by_class ( * args , * * kw ) : cls = args [ 0 ] . __class__ for t in type ( cls . __name__ , ( cls , object ) , { } ) . __mro__ : f = _gbt ( t , _sentinel ) if f is not _sentinel : return f ( * args , * * kw ) else : return func ( * args , * * kw ) _by_type = { object : func } try : _by_type [ InstanceType ] = _by_class except NameError : pass _gbt = _by_type . get def when_type ( * types ) : """Decorator to add a method that will be called for the given types""" for t in types : if not isinstance ( t , classtypes ) : raise TypeError ( "%r is not a type or class" % ( t , ) ) def decorate ( f ) : for t in types : if _by_type . setdefault ( t , f ) is not f : raise TypeError ( "%r already has method for type %r" % ( func , t ) ) return f return decorate _by_object = { } _gbo = _by_object . get def when_object ( * obs ) : """Decorator to add a method to be called for the given object(s)""" def decorate ( f ) : for o in obs : if _by_object . setdefault ( id ( o ) , ( o , f ) ) [ 1 ] is not f : raise TypeError ( "%r already has method for object %r" % ( func , o ) ) return f return decorate def dispatch ( * args , * * kw ) : f = _gbo ( id ( args [ 0 ] ) , _sentinel ) if f is _sentinel : for t in type ( args [ 0 ] ) . __mro__ : f = _gbt ( t , _sentinel ) if f is not _sentinel : return f ( * args , * * kw ) else : return func ( * args , * * kw ) else : return f [ 1 ] ( * args , * * kw ) dispatch . __name__ = func . __name__ dispatch . __dict__ = func . __dict__ . copy ( ) dispatch . __doc__ = func . __doc__ dispatch . __module__ = func . __module__ dispatch . when_type = when_type dispatch . when_object = when_object dispatch . default = func dispatch . has_object = lambda o : id ( o ) in _by_object dispatch . has_type = lambda t : t in _by_type return dispatch
def data ( fname ) : data_file = open ( data_filename ( fname ) ) try : return data_file . read ( ) finally : data_file . close ( )
def escape ( t ) : return ( t . replace ( "&" , "&amp;" ) . replace ( "<" , "&lt;" ) . replace ( ">" , "&gt;" ) . replace ( "'" , "&#39;" ) . replace ( '"' , "&quot;" ) . replace ( "  " , "&nbsp; " ) . replace ( "  " , "&nbsp; " ) )
def make_local_static_report_files ( self ) : for static , pkgdir in self . STATIC_FILES : shutil . copyfile ( data_filename ( static , pkgdir ) , os . path . join ( self . directory , static ) ) if self . extra_css : shutil . copyfile ( self . config . extra_css , os . path . join ( self . directory , self . extra_css ) )
def write_html ( self , fname , html ) : fout = open ( fname , "wb" ) try : fout . write ( html . encode ( 'ascii' , 'xmlcharrefreplace' ) ) finally : fout . close ( )
def file_hash ( self , source , cu ) : m = Hasher ( ) m . update ( source ) self . coverage . data . add_to_hash ( cu . filename , m ) return m . digest ( )
def index_file ( self ) : index_tmpl = Templite ( data ( "index.html" ) , self . template_globals ) self . totals = sum ( [ f [ 'nums' ] for f in self . files ] ) html = index_tmpl . render ( { 'arcs' : self . arcs , 'extra_css' : self . extra_css , 'files' : self . files , 'totals' : self . totals , } ) if sys . version_info < ( 3 , 0 ) : html = html . decode ( "utf-8" ) self . write_html ( os . path . join ( self . directory , "index.html" ) , html ) self . status . write ( self . directory )
def read ( self , directory ) : usable = False try : status_file = os . path . join ( directory , self . STATUS_FILE ) fstatus = open ( status_file , "rb" ) try : status = pickle . load ( fstatus ) finally : fstatus . close ( ) except ( IOError , ValueError ) : usable = False else : usable = True if status [ 'format' ] != self . STATUS_FORMAT : usable = False elif status [ 'version' ] != coverage . __version__ : usable = False if usable : self . files = status [ 'files' ] self . settings = status [ 'settings' ] else : self . reset ( )
def write ( self , directory ) : status_file = os . path . join ( directory , self . STATUS_FILE ) status = { 'format' : self . STATUS_FORMAT , 'version' : coverage . __version__ , 'settings' : self . settings , 'files' : self . files , } fout = open ( status_file , "wb" ) try : pickle . dump ( status , fout ) finally : fout . close ( )
def get_slice ( seq , start = 0 , stop = None , step = 1 ) : if stop == None : stop = len ( seq ) item = lambda i : seq [ i ] return map ( item , xrange ( start , stop , step ) )
def chop ( seq , size ) : chunk = lambda i : seq [ i : i + size ] return map ( chunk , xrange ( 0 , len ( seq ) , size ) )
def read_config ( ) : config = ConfigParser . ConfigParser ( ) config . read ( [ 'setup.cfg' ] ) if not config . has_section ( 'check-manifest' ) : return if ( config . has_option ( 'check-manifest' , 'ignore-default-rules' ) and config . getboolean ( 'check-manifest' , 'ignore-default-rules' ) ) : del IGNORE [ : ] if config . has_option ( 'check-manifest' , 'ignore' ) : patterns = [ p . strip ( ) for p in config . get ( 'check-manifest' , 'ignore' ) . splitlines ( ) ] IGNORE . extend ( p for p in patterns if p )
def file_matches ( filename , patterns ) : return any ( fnmatch . fnmatch ( filename , pat ) for pat in patterns )
def get_versioned_files ( ) : encoding = 'UTF-8' if sys . platform == 'win32' else None output = run ( [ 'git' , 'ls-files' , '-z' ] , encoding = encoding ) return add_directories ( output . split ( '\0' ) [ : - 1 ] )
def start_kernel ( self , * * kwargs ) : kernel_id = unicode ( uuid . uuid4 ( ) ) km = self . kernel_manager_factory ( connection_file = os . path . join ( self . connection_dir , "kernel-%s.json" % kernel_id ) , config = self . config , ) km . start_kernel ( * * kwargs ) km . start_channels ( shell = True , sub = False , stdin = False , hb = False ) self . _kernels [ kernel_id ] = km return kernel_id
def notebook_for_kernel ( self , kernel_id ) : notebook_ids = [ k for k , v in self . _notebook_mapping . iteritems ( ) if v == kernel_id ] if len ( notebook_ids ) == 1 : return notebook_ids [ 0 ] else : return None
def shutdown_kernel ( self , kernel_id ) : self . _check_kernel_id ( kernel_id ) super ( MappingKernelManager , self ) . shutdown_kernel ( kernel_id ) self . delete_mapping_for_kernel ( kernel_id ) self . log . info ( "Kernel shutdown: %s" % kernel_id )
def interrupt_kernel ( self , kernel_id ) : self . _check_kernel_id ( kernel_id ) super ( MappingKernelManager , self ) . interrupt_kernel ( kernel_id ) self . log . info ( "Kernel interrupted: %s" % kernel_id )
def restart_kernel ( self , kernel_id ) : self . _check_kernel_id ( kernel_id ) km = self . get_kernel ( kernel_id ) km . restart_kernel ( ) self . log . info ( "Kernel restarted: %s" % kernel_id ) return kernel_id notebook_id = self . notebook_for_kernel ( kernel_id ) new_kernel_id = self . start_kernel ( ) self . kill_kernel ( kernel_id ) self . set_kernel_for_notebook ( notebook_id , new_kernel_id ) self . log . info ( "Kernel restarted: %s" % new_kernel_id ) return new_kernel_id
def create_iopub_stream ( self , kernel_id ) : self . _check_kernel_id ( kernel_id ) return super ( MappingKernelManager , self ) . create_iopub_stream ( kernel_id )
def create_shell_stream ( self , kernel_id ) : self . _check_kernel_id ( kernel_id ) return super ( MappingKernelManager , self ) . create_shell_stream ( kernel_id )
def create_hb_stream ( self , kernel_id ) : self . _check_kernel_id ( kernel_id ) return super ( MappingKernelManager , self ) . create_hb_stream ( kernel_id )
def reset ( self ) : instdict = self . __dict__ classdict = self . __class__ . __dict__ for mname , mval in classdict . items ( ) : if mname in instdict and isinstance ( mval , OneTimeProperty ) : delattr ( self , mname )
def ensure_utf8 ( image_tag ) : if py3compat . PY3 : return image_tag def utf8_image_tag ( * args , * * kwargs ) : s = image_tag ( * args , * * kwargs ) if isinstance ( s , unicode ) : s = s . encode ( 'utf8' ) return s return utf8_image_tag
def get_unique_or_none ( klass , * args , * * kwargs ) : try : return klass . objects . get ( * args , * * kwargs ) except klass . DoesNotExist : return None except klass . MultipleObjectsReturned : return None return None
def get_query_includes ( tokenized_terms , search_fields ) : query = None for term in tokenized_terms : or_query = None for field_name in search_fields : q = Q ( * * { "%s__icontains" % field_name : term } ) if or_query is None : or_query = q else : or_query = or_query | q if query is None : query = or_query else : query = query & or_query return query
def get_text_query ( query_string , search_fields ) : include_terms , exclude_terms = get_text_tokenizer ( query_string ) include_q = get_query_includes ( include_terms , search_fields ) exclude_q = get_query_excludes ( exclude_terms , search_fields ) query = None if include_q and exclude_q : query = include_q & ~ exclude_q elif not exclude_q : query = include_q else : query = ~ exclude_q return query
def get_date_greater_query ( days , date_field ) : query = None days = get_integer ( days ) if days : past = get_days_ago ( days ) query = Q ( * * { "%s__gte" % date_field : past . isoformat ( ) } ) return query
def get_date_less_query ( days , date_field ) : query = None days = get_integer ( days ) if days : future = get_days_from_now ( days ) query = Q ( * * { "%s__lte" % date_field : future . isoformat ( ) } ) return query
def get_null_or_blank_query ( field = None ) : if not field : return field null_q = get_null_query ( field ) blank_q = get_blank_query ( field ) return ( null_q | blank_q )
def case_insensitive ( self , fields_dict ) : if hasattr ( self . model , 'CASE_INSENSITIVE_FIELDS' ) : for field in self . model . CASE_INSENSITIVE_FIELDS : if field in fields_dict : fields_dict [ field + '__iexact' ] = fields_dict [ field ] del fields_dict [ field ]
def options ( self , parser , env ) : parser . add_option ( "-a" , "--attr" , dest = "attr" , action = "append" , default = env . get ( 'NOSE_ATTR' ) , metavar = "ATTR" , help = "Run only tests that have attributes " "specified by ATTR [NOSE_ATTR]" ) if compat_24 : parser . add_option ( "-A" , "--eval-attr" , dest = "eval_attr" , metavar = "EXPR" , action = "append" , default = env . get ( 'NOSE_EVAL_ATTR' ) , help = "Run only tests for whose attributes " "the Python expression EXPR evaluates " "to True [NOSE_EVAL_ATTR]" )
def wantMethod ( self , method ) : try : cls = method . im_class except AttributeError : return False return self . validateAttrib ( method , cls )
def rotate ( self ) : if self . _prev_yank : text = self . _ring . rotate ( ) if text : self . _skip_cursor = True cursor = self . _text_edit . textCursor ( ) cursor . movePosition ( QtGui . QTextCursor . Left , QtGui . QTextCursor . KeepAnchor , n = len ( self . _prev_yank ) ) cursor . insertText ( text ) self . _prev_yank = text
def start_hb ( self , callback ) : if not self . _beating : self . _kernel_alive = True def ping_or_dead ( ) : self . hb_stream . flush ( ) if self . _kernel_alive : self . _kernel_alive = False self . hb_stream . send ( b'ping' ) self . hb_stream . flush ( ) else : try : callback ( ) except : pass finally : self . stop_hb ( ) def beat_received ( msg ) : self . _kernel_alive = True self . hb_stream . on_recv ( beat_received ) loop = ioloop . IOLoop . instance ( ) self . _hb_periodic_callback = ioloop . PeriodicCallback ( ping_or_dead , self . time_to_dead * 1000 , loop ) loop . add_timeout ( time . time ( ) + self . first_beat , self . _really_start_hb ) self . _beating = True
def stop_hb ( self ) : if self . _beating : self . _beating = False self . _hb_periodic_callback . stop ( ) if not self . hb_stream . closed ( ) : self . hb_stream . on_recv ( None )
def fload ( self ) : if hasattr ( self , 'fobj' ) and self . fobj is not None : self . fobj . close ( ) if hasattr ( self . src , "read" ) : self . fobj = self . src else : self . fobj = open ( self . fname )
def reload ( self ) : self . fload ( ) self . src = self . fobj . read ( ) src_b = [ b . strip ( ) for b in self . re_stop . split ( self . src ) if b ] self . _silent = [ bool ( self . re_silent . findall ( b ) ) for b in src_b ] self . _auto = [ bool ( self . re_auto . findall ( b ) ) for b in src_b ] if self . auto_all is None : self . auto_all = bool ( self . re_auto_all . findall ( src_b [ 0 ] ) ) else : self . auto_all = bool ( self . auto_all ) src_blocks = [ ] auto_strip = lambda s : self . re_auto . sub ( '' , s ) for i , b in enumerate ( src_b ) : if self . _auto [ i ] : src_blocks . append ( auto_strip ( b ) ) else : src_blocks . append ( b ) src_blocks [ 0 ] = self . re_auto_all . sub ( '' , src_blocks [ 0 ] ) self . nblocks = len ( src_blocks ) self . src_blocks = src_blocks self . src_blocks_colored = map ( self . ip_colorize , self . src_blocks ) self . reset ( )
def show ( self , index = None ) : index = self . _get_index ( index ) if index is None : return print >> io . stdout , self . marquee ( % ( self . title , index , self . nblocks - index - 1 ) ) print >> io . stdout , ( self . src_blocks_colored [ index ] ) sys . stdout . flush ( )
def show_all ( self ) : fname = self . title title = self . title nblocks = self . nblocks silent = self . _silent marquee = self . marquee for index , block in enumerate ( self . src_blocks_colored ) : if silent [ index ] : print >> io . stdout , marquee ( % ( title , index , nblocks - index - 1 ) ) else : print >> io . stdout , marquee ( % ( title , index , nblocks - index - 1 ) ) print >> io . stdout , block , sys . stdout . flush ( )
def reload ( self ) : self . fload ( ) lines = self . fobj . readlines ( ) src_b = [ l for l in lines if l . strip ( ) ] nblocks = len ( src_b ) self . src = '' . join ( lines ) self . _silent = [ False ] * nblocks self . _auto = [ True ] * nblocks self . auto_all = True self . nblocks = nblocks self . src_blocks = src_b self . src_blocks_colored = map ( self . ip_colorize , self . src_blocks ) self . reset ( )
def thread ( function , sequence , cores = None , runSeries = False , quiet = False ) : if cores is None : pool = ThreadPool ( ) else : pool = ThreadPool ( cores ) tic = time . time ( ) if runSeries is False : try : results = pool . map ( function , sequence ) pool . close ( ) pool . join ( ) except : print 'thread Failed... running in series :-(' results = series ( sequence , function ) else : results = series ( sequence , function ) toc = time . time ( ) elapsed = toc - tic if quiet is False : if cores is None : print "Elapsed time: %s  :-)\n" % str ( elapsed ) else : print "Elapsed time: %s  on %s threads :-)\n" % ( str ( elapsed ) , str ( cores ) ) return results
def with_objattrs ( * names ) : def _wrap ( func ) : @ functools . wraps ( func ) def wrapper ( self , * args , * * kwargs ) : with contextlib . ExitStack ( ) as stack : for name in names : stack . enter_context ( getattr ( self , name ) ) return func ( self , * args , * * kwargs ) return wrapper return _wrap
def countdown ( name , date , description = '' , id = '' , granularity = 'sec' , start = None , progressbar = False , progressbar_inversed = False , showpct = False ) : end_date = dateparse . parse_datetime ( date ) end = dateformat . format ( end_date , 'U' ) content = '<div class="name">' + name + '</div>' content += '<div class="description">' + description + '</div>' if progressbar : if not end : raise Exception ( 'For progressbar, start date is requried.' ) parsed_date = datetime . datetime . combine ( dateparse . parse_date ( start ) , datetime . time ( ) ) start_date = dateparse . parse_datetime ( start ) or parsed_date now = datetime . datetime . now ( ) pct = ( now - start_date ) . total_seconds ( ) / ( end_date - start_date ) . total_seconds ( ) pct = int ( pct * 100 ) if progressbar_inversed : pct = 100 - pct bar = '<div class="progress progress-striped active">' bar += '<div class="progress-bar"  role="progressbar" aria-valuenow="{pct}" aria-valuemin="0" aria-valuemax="100" style="width: {pct}%">' bar += '<span class="sr-only">{pct}% Complete</span>' bar += '</div>' bar += '</div>' if showpct : bar += '<div class="percentage">{pct}%</div>' bar = bar . format ( pct = pct ) content += bar content += '<div class="counter"></div>' attr = { 'class' : 'countdownbox' , 'data-datetime' : end , 'data-granularity' : granularity } if id : attr [ 'id' ] = id return html . tag ( 'div' , content , attr )
def cleanup ( controller , engines ) : import signal , time print ( 'Starting cleanup' ) print ( 'Stopping engines...' ) for e in engines : e . send_signal ( signal . SIGINT ) print ( 'Stopping controller...' ) controller . send_signal ( signal . SIGINT ) time . sleep ( 0.1 ) print ( 'Killing controller...' ) controller . kill ( ) print ( 'Cleanup done' )
def save_ids ( f , self , * args , * * kwargs ) : n_previous = len ( self . client . history ) try : ret = f ( self , * args , * * kwargs ) finally : nmsgs = len ( self . client . history ) - n_previous msg_ids = self . client . history [ - nmsgs : ] self . history . extend ( msg_ids ) map ( self . outstanding . add , msg_ids ) return ret
def sync_results ( f , self , * args , * * kwargs ) : ret = f ( self , * args , * * kwargs ) delta = self . outstanding . difference ( self . client . outstanding ) completed = self . outstanding . intersection ( delta ) self . outstanding = self . outstanding . difference ( completed ) return ret
def spin_after ( f , self , * args , * * kwargs ) : ret = f ( self , * args , * * kwargs ) self . spin ( ) return ret
def add_record ( self , msg_id , rec ) : rec = self . _binary_buffers ( rec ) self . _records . insert ( rec )
def get_record ( self , msg_id ) : r = self . _records . find_one ( { 'msg_id' : msg_id } ) if not r : raise KeyError ( msg_id ) return r
def update_record ( self , msg_id , rec ) : rec = self . _binary_buffers ( rec ) self . _records . update ( { 'msg_id' : msg_id } , { '$set' : rec } )
def get_history ( self ) : cursor = self . _records . find ( { } , { 'msg_id' : 1 } ) . sort ( 'submitted' ) return [ rec [ 'msg_id' ] for rec in cursor ]
def get_msgs ( self ) : msgs = [ ] while True : try : msgs . append ( self . get_msg ( block = False ) ) except Empty : break return msgs
def get_msg ( self , block = True , timeout = None ) : return self . _in_queue . get ( block , timeout )
def parse ( url ) : config = { } if not isinstance ( url , six . string_types ) : url = '' url = urlparse . urlparse ( url ) path = url . path [ 1 : ] path = path . split ( '?' , 2 ) [ 0 ] config . update ( { 'NAME' : path , 'USER' : url . username , 'PASSWORD' : url . password , 'HOST' : url . hostname , 'PORT' : url . port , } ) if url . scheme in SCHEMES : config [ 'ENGINE' ] = SCHEMES [ url . scheme ] return config
def magic_run_completer ( self , event ) : comps = arg_split ( event . line , strict = False ) relpath = ( len ( comps ) > 1 and comps [ - 1 ] or '' ) . strip ( "'\"" ) lglob = glob . glob isdir = os . path . isdir relpath , tilde_expand , tilde_val = expand_user ( relpath ) dirs = [ f . replace ( '\\' , '/' ) + "/" for f in lglob ( relpath + '*' ) if isdir ( f ) ] if filter ( magic_run_re . match , comps ) : pys = [ f . replace ( '\\' , '/' ) for f in lglob ( '*' ) ] else : pys = [ f . replace ( '\\' , '/' ) for f in lglob ( relpath + '*.py' ) + lglob ( relpath + '*.ipy' ) + lglob ( relpath + '*.pyw' ) ] return [ compress_user ( p , tilde_expand , tilde_val ) for p in dirs + pys ]
def cd_completer ( self , event ) : ip = get_ipython ( ) relpath = event . symbol if event . line . endswith ( '-b' ) or ' -b ' in event . line : bkms = self . db . get ( 'bookmarks' , None ) if bkms : return bkms . keys ( ) else : return [ ] if event . symbol == '-' : width_dh = str ( len ( str ( len ( ip . user_ns [ '_dh' ] ) + 1 ) ) ) fmt = '-%0' + width_dh + 'd [%s]' ents = [ fmt % ( i , s ) for i , s in enumerate ( ip . user_ns [ '_dh' ] ) ] if len ( ents ) > 1 : return ents return [ ] if event . symbol . startswith ( '--' ) : return [ "--" + os . path . basename ( d ) for d in ip . user_ns [ '_dh' ] ] relpath , tilde_expand , tilde_val = expand_user ( relpath ) relpath = relpath . replace ( '\\' , '/' ) found = [ ] for d in [ f . replace ( '\\' , '/' ) + '/' for f in glob . glob ( relpath + '*' ) if os . path . isdir ( f ) ] : if ' ' in d : raise TryNext found . append ( d ) if not found : if os . path . isdir ( relpath ) : return [ compress_user ( relpath , tilde_expand , tilde_val ) ] bks = self . db . get ( 'bookmarks' , { } ) . iterkeys ( ) bkmatches = [ s for s in bks if s . startswith ( event . symbol ) ] if bkmatches : return bkmatches raise TryNext return [ compress_user ( p , tilde_expand , tilde_val ) for p in found ]
def _quoteattr ( self , attr ) : attr = xml_safe ( attr ) if isinstance ( attr , unicode ) and not UNICODE_STRINGS : attr = attr . encode ( self . encoding ) return saxutils . quoteattr ( attr )
def configure ( self , options , config ) : Plugin . configure ( self , options , config ) self . config = config if self . enabled : self . stats = { 'errors' : 0 , 'failures' : 0 , 'passes' : 0 , 'skipped' : 0 } self . errorlist = [ ] self . error_report_file = codecs . open ( options . xunit_file , 'w' , self . encoding , 'replace' )
def addError ( self , test , err , capt = None ) : taken = self . _timeTaken ( ) if issubclass ( err [ 0 ] , SkipTest ) : type = 'skipped' self . stats [ 'skipped' ] += 1 else : type = 'error' self . stats [ 'errors' ] += 1 tb = '' . join ( traceback . format_exception ( * err ) ) id = test . id ( ) self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s time="%(taken).3f">' '<%(type)s type=%(errtype)s message=%(message)s><![CDATA[%(tb)s]]>' '</%(type)s></testcase>' % { 'cls' : self . _quoteattr ( id_split ( id ) [ 0 ] ) , 'name' : self . _quoteattr ( id_split ( id ) [ - 1 ] ) , 'taken' : taken , 'type' : type , 'errtype' : self . _quoteattr ( nice_classname ( err [ 0 ] ) ) , 'message' : self . _quoteattr ( exc_message ( err ) ) , 'tb' : escape_cdata ( tb ) , } )
def addFailure ( self , test , err , capt = None , tb_info = None ) : taken = self . _timeTaken ( ) tb = '' . join ( traceback . format_exception ( * err ) ) self . stats [ 'failures' ] += 1 id = test . id ( ) self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s time="%(taken).3f">' '<failure type=%(errtype)s message=%(message)s><![CDATA[%(tb)s]]>' '</failure></testcase>' % { 'cls' : self . _quoteattr ( id_split ( id ) [ 0 ] ) , 'name' : self . _quoteattr ( id_split ( id ) [ - 1 ] ) , 'taken' : taken , 'errtype' : self . _quoteattr ( nice_classname ( err [ 0 ] ) ) , 'message' : self . _quoteattr ( exc_message ( err ) ) , 'tb' : escape_cdata ( tb ) , } )
def addSuccess ( self , test , capt = None ) : taken = self . _timeTaken ( ) self . stats [ 'passes' ] += 1 id = test . id ( ) self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s ' 'time="%(taken).3f" />' % { 'cls' : self . _quoteattr ( id_split ( id ) [ 0 ] ) , 'name' : self . _quoteattr ( id_split ( id ) [ - 1 ] ) , 'taken' : taken , } )
def _register_engine ( self , uid ) : self . targets . insert ( 0 , uid ) self . loads . insert ( 0 , 0 ) self . completed [ uid ] = set ( ) self . failed [ uid ] = set ( ) self . pending [ uid ] = { } self . update_graph ( None )
def _unregister_engine ( self , uid ) : if len ( self . targets ) == 1 : pass self . engine_stream . flush ( ) idx = self . targets . index ( uid ) self . targets . pop ( idx ) self . loads . pop ( idx ) if self . pending [ uid ] : dc = ioloop . DelayedCallback ( lambda : self . handle_stranded_tasks ( uid ) , 5000 , self . loop ) dc . start ( ) else : self . completed . pop ( uid ) self . failed . pop ( uid )
def handle_stranded_tasks ( self , engine ) : lost = self . pending [ engine ] for msg_id in lost . keys ( ) : if msg_id not in self . pending [ engine ] : continue raw_msg = lost [ msg_id ] . raw_msg idents , msg = self . session . feed_identities ( raw_msg , copy = False ) parent = self . session . unpack ( msg [ 1 ] . bytes ) idents = [ engine , idents [ 0 ] ] try : raise error . EngineError ( "Engine %r died while running task %r" % ( engine , msg_id ) ) except : content = error . wrap_exception ( ) header = dict ( status = 'error' , engine = engine , date = datetime . now ( ) , ) msg = self . session . msg ( 'apply_reply' , content , parent = parent , subheader = header ) raw_reply = map ( zmq . Message , self . session . serialize ( msg , ident = idents ) ) self . dispatch_result ( raw_reply ) self . completed . pop ( engine ) self . failed . pop ( engine )
def audit_timeouts ( self ) : now = datetime . now ( ) for msg_id in self . depending . keys ( ) : if msg_id in self . depending : job = self . depending [ msg_id ] if job . timeout and job . timeout < now : self . fail_unreachable ( msg_id , error . TaskTimeout )
def maybe_run ( self , job ) : msg_id = job . msg_id self . log . debug ( "Attempting to assign task %s" , msg_id ) if not self . targets : return False if job . follow or job . targets or job . blacklist or self . hwm : def can_run ( idx ) : if self . hwm and self . loads [ idx ] == self . hwm : return False target = self . targets [ idx ] if target in job . blacklist : return False if job . targets and target not in job . targets : return False return job . follow . check ( self . completed [ target ] , self . failed [ target ] ) indices = filter ( can_run , range ( len ( self . targets ) ) ) if not indices : if job . follow . all : dests = set ( ) relevant = set ( ) if job . follow . success : relevant = self . all_completed if job . follow . failure : relevant = relevant . union ( self . all_failed ) for m in job . follow . intersection ( relevant ) : dests . add ( self . destinations [ m ] ) if len ( dests ) > 1 : self . depending [ msg_id ] = job self . fail_unreachable ( msg_id ) return False if job . targets : job . targets . difference_update ( job . blacklist ) if not job . targets or not job . targets . intersection ( self . targets ) : self . depending [ msg_id ] = job self . fail_unreachable ( msg_id ) return False return False else : indices = None self . submit_task ( job , indices ) return True
def save_unmet ( self , job ) : msg_id = job . msg_id self . depending [ msg_id ] = job for dep_id in job . after . union ( job . follow ) . difference ( self . all_done ) : if dep_id not in self . graph : self . graph [ dep_id ] = set ( ) self . graph [ dep_id ] . add ( msg_id )
def submit_task ( self , job , indices = None ) : if indices : loads = [ self . loads [ i ] for i in indices ] else : loads = self . loads idx = self . scheme ( loads ) if indices : idx = indices [ idx ] target = self . targets [ idx ] self . engine_stream . send ( target , flags = zmq . SNDMORE , copy = False ) self . engine_stream . send_multipart ( job . raw_msg , copy = False ) self . add_job ( idx ) self . pending [ target ] [ job . msg_id ] = job content = dict ( msg_id = job . msg_id , engine_id = target . decode ( 'ascii' ) ) self . session . send ( self . mon_stream , 'task_destination' , content = content , ident = [ b'tracktask' , self . ident ] )
def dispatch_result ( self , raw_msg ) : try : idents , msg = self . session . feed_identities ( raw_msg , copy = False ) msg = self . session . unserialize ( msg , content = False , copy = False ) engine = idents [ 0 ] try : idx = self . targets . index ( engine ) except ValueError : pass else : self . finish_job ( idx ) except Exception : self . log . error ( "task::Invaid result: %r" , raw_msg , exc_info = True ) return header = msg [ 'header' ] parent = msg [ 'parent_header' ] if header . get ( 'dependencies_met' , True ) : success = ( header [ 'status' ] == 'ok' ) msg_id = parent [ 'msg_id' ] retries = self . retries [ msg_id ] if not success and retries > 0 : self . retries [ msg_id ] = retries - 1 self . handle_unmet_dependency ( idents , parent ) else : del self . retries [ msg_id ] self . handle_result ( idents , parent , raw_msg , success ) self . mon_stream . send_multipart ( [ b'outtask' ] + raw_msg , copy = False ) else : self . handle_unmet_dependency ( idents , parent )
def handle_result ( self , idents , parent , raw_msg , success = True ) : engine = idents [ 0 ] client = idents [ 1 ] raw_msg [ : 2 ] = [ client , engine ] self . client_stream . send_multipart ( raw_msg , copy = False ) msg_id = parent [ 'msg_id' ] self . pending [ engine ] . pop ( msg_id ) if success : self . completed [ engine ] . add ( msg_id ) self . all_completed . add ( msg_id ) else : self . failed [ engine ] . add ( msg_id ) self . all_failed . add ( msg_id ) self . all_done . add ( msg_id ) self . destinations [ msg_id ] = engine self . update_graph ( msg_id , success )
def handle_unmet_dependency ( self , idents , parent ) : engine = idents [ 0 ] msg_id = parent [ 'msg_id' ] job = self . pending [ engine ] . pop ( msg_id ) job . blacklist . add ( engine ) if job . blacklist == job . targets : self . depending [ msg_id ] = job self . fail_unreachable ( msg_id ) elif not self . maybe_run ( job ) : if msg_id not in self . all_failed : self . save_unmet ( job ) if self . hwm : try : idx = self . targets . index ( engine ) except ValueError : pass else : if self . loads [ idx ] == self . hwm - 1 : self . update_graph ( None )
def logstate ( self ) : if self . logfile is None : print 'Logging has not been activated.' else : state = self . log_active and 'active' or 'temporarily suspended' print 'Filename       :' , self . logfname print 'Mode           :' , self . logmode print 'Output logging :' , self . log_output print 'Raw input log  :' , self . log_raw_input print 'Timestamping   :' , self . timestamp print 'State          :' , state
def log_write ( self , data , kind = 'input' ) : if self . log_active and data : write = self . logfile . write if kind == 'input' : if self . timestamp : write ( str_to_unicode ( time . strftime ( , time . localtime ( ) ) ) ) write ( data ) elif kind == 'output' and self . log_output : odata = u'\n' . join ( [ % s for s in data . splitlines ( ) ] ) write ( u'%s\n' % odata ) self . logfile . flush ( )
def new_worksheet ( name = None , cells = None ) : ws = NotebookNode ( ) if name is not None : ws . name = unicode ( name ) if cells is None : ws . cells = [ ] else : ws . cells = list ( cells ) return ws
def new_notebook ( metadata = None , worksheets = None ) : nb = NotebookNode ( ) nb . nbformat = 2 if worksheets is None : nb . worksheets = [ ] else : nb . worksheets = list ( worksheets ) if metadata is None : nb . metadata = new_metadata ( ) else : nb . metadata = NotebookNode ( metadata ) return nb
def add_s ( self , s , obj , priority = 0 ) : chain = self . strs . get ( s , CommandChainDispatcher ( ) ) chain . add ( obj , priority ) self . strs [ s ] = chain
def add_re ( self , regex , obj , priority = 0 ) : chain = self . regexs . get ( regex , CommandChainDispatcher ( ) ) chain . add ( obj , priority ) self . regexs [ regex ] = chain
def dispatch ( self , key ) : if key in self . strs : yield self . strs [ key ] for r , obj in self . regexs . items ( ) : if re . match ( r , key ) : yield obj else : pass
def flat_matches ( self , key ) : for val in self . dispatch ( key ) : for el in val : yield el [ 1 ] return
def _notebook_dir_changed ( self , name , old , new ) : if os . path . exists ( new ) and not os . path . isdir ( new ) : raise TraitError ( "notebook dir %r is not a directory" % new ) if not os . path . exists ( new ) : self . log . info ( "Creating notebook dir %s" , new ) try : os . mkdir ( new ) except : raise TraitError ( "Couldn't create notebook dir %r" % new )
def new_notebook_id ( self , name ) : #notebook_id = unicode(uuid.uuid5(uuid.NAMESPACE_URL, notebook_id = unicode ( uuid . uuid4 ( ) ) self . mapping [ notebook_id ] = name self . rev_mapping [ name ] = notebook_id return notebook_id
def delete_notebook_id ( self , notebook_id ) : name = self . mapping [ notebook_id ] del self . mapping [ notebook_id ] del self . rev_mapping [ name ]
def notebook_exists ( self , notebook_id ) : if notebook_id not in self . mapping : return False path = self . get_path_by_name ( self . mapping [ notebook_id ] ) return os . path . isfile ( path )
def find_path ( self , notebook_id ) : try : name = self . mapping [ notebook_id ] except KeyError : raise web . HTTPError ( 404 , u'Notebook does not exist: %s' % notebook_id ) return self . get_path_by_name ( name )
def get_path_by_name ( self , name ) : filename = name + self . filename_ext path = os . path . join ( self . notebook_dir , filename ) return path
def get_notebook ( self , notebook_id , format = u'json' ) : format = unicode ( format ) if format not in self . allowed_formats : raise web . HTTPError ( 415 , u'Invalid notebook format: %s' % format ) last_modified , nb = self . get_notebook_object ( notebook_id ) kwargs = { } if format == 'json' : kwargs [ 'split_lines' ] = False data = current . writes ( nb , format , * * kwargs ) name = nb . metadata . get ( 'name' , 'notebook' ) return last_modified , name , data
def get_notebook_object ( self , notebook_id ) : path = self . find_path ( notebook_id ) if not os . path . isfile ( path ) : raise web . HTTPError ( 404 , u'Notebook does not exist: %s' % notebook_id ) info = os . stat ( path ) last_modified = datetime . datetime . utcfromtimestamp ( info . st_mtime ) with open ( path , 'r' ) as f : s = f . read ( ) try : nb = current . reads ( s , u'json' ) except : raise web . HTTPError ( 500 , u'Unreadable JSON notebook.' ) nb . metadata . name = os . path . splitext ( os . path . basename ( path ) ) [ 0 ] return last_modified , nb
def save_notebook ( self , notebook_id , data , name = None , format = u'json' ) : if format not in self . allowed_formats : raise web . HTTPError ( 415 , u'Invalid notebook format: %s' % format ) try : nb = current . reads ( data . decode ( 'utf-8' ) , format ) except : raise web . HTTPError ( 400 , u'Invalid JSON data' ) if name is not None : nb . metadata . name = name self . save_notebook_object ( notebook_id , nb )
def save_notebook_object ( self , notebook_id , nb ) : if notebook_id not in self . mapping : raise web . HTTPError ( 404 , u'Notebook does not exist: %s' % notebook_id ) old_name = self . mapping [ notebook_id ] try : new_name = nb . metadata . name except AttributeError : raise web . HTTPError ( 400 , u'Missing notebook name' ) path = self . get_path_by_name ( new_name ) try : with open ( path , 'w' ) as f : current . write ( nb , f , u'json' ) except Exception as e : raise web . HTTPError ( 400 , u'Unexpected error while saving notebook: %s' % e ) if self . save_script : pypath = os . path . splitext ( path ) [ 0 ] + '.py' try : with io . open ( pypath , 'w' , encoding = 'utf-8' ) as f : current . write ( nb , f , u'py' ) except Exception as e : raise web . HTTPError ( 400 , u'Unexpected error while saving notebook as script: %s' % e ) if old_name != new_name : old_path = self . get_path_by_name ( old_name ) if os . path . isfile ( old_path ) : os . unlink ( old_path ) if self . save_script : old_pypath = os . path . splitext ( old_path ) [ 0 ] + '.py' if os . path . isfile ( old_pypath ) : os . unlink ( old_pypath ) self . mapping [ notebook_id ] = new_name self . rev_mapping [ new_name ] = notebook_id del self . rev_mapping [ old_name ]
def delete_notebook ( self , notebook_id ) : path = self . find_path ( notebook_id ) if not os . path . isfile ( path ) : raise web . HTTPError ( 404 , u'Notebook does not exist: %s' % notebook_id ) os . unlink ( path ) self . delete_notebook_id ( notebook_id )
def new_notebook ( self ) : path , name = self . increment_filename ( 'Untitled' ) notebook_id = self . new_notebook_id ( name ) metadata = current . new_metadata ( name = name ) nb = current . new_notebook ( metadata = metadata ) with open ( path , 'w' ) as f : current . write ( nb , f , u'json' ) return notebook_id
def copy_notebook ( self , notebook_id ) : last_mod , nb = self . get_notebook_object ( notebook_id ) name = nb . metadata . name + '-Copy' path , name = self . increment_filename ( name ) nb . metadata . name = name notebook_id = self . new_notebook_id ( name ) self . save_notebook_object ( notebook_id , nb ) return notebook_id
def make_report ( self , traceback ) : sec_sep = self . section_sep report = [ super ( IPAppCrashHandler , self ) . make_report ( traceback ) ] rpt_add = report . append try : rpt_add ( sec_sep + "History of session input:" ) for line in self . app . shell . user_ns [ '_ih' ] : rpt_add ( line ) rpt_add ( '\n*** Last line of input (may not be in above history):\n' ) rpt_add ( self . app . shell . _last_input_line + '\n' ) except : pass return '' . join ( report )
def _classes_default ( self ) : return [ InteractiveShellApp , self . __class__ , TerminalInteractiveShell , PromptManager , HistoryManager , ProfileDir , PlainTextFormatter , IPCompleter , ScriptMagics , ]
def parse_command_line ( self , argv = None ) : argv = sys . argv [ 1 : ] if argv is None else argv if '-pylab' in argv : argv = argv [ : ] idx = argv . index ( '-pylab' ) warn . warn ( "`-pylab` flag has been deprecated.\n" "    Use `--pylab` instead, or `--pylab=foo` to specify a backend." ) sub = '--pylab' if len ( argv ) > idx + 1 : gui = argv [ idx + 1 ] if gui in ( 'wx' , 'qt' , 'qt4' , 'gtk' , 'auto' ) : sub = '--pylab=' + gui argv . pop ( idx + 1 ) argv [ idx ] = sub return super ( TerminalIPythonApp , self ) . parse_command_line ( argv )
def initialize ( self , argv = None ) : super ( TerminalIPythonApp , self ) . initialize ( argv ) if self . subapp is not None : return if not self . ignore_old_config : check_for_old_config ( self . ipython_dir ) if self . extra_args and not self . something_to_run : self . file_to_run = self . extra_args [ 0 ] self . init_path ( ) self . init_shell ( ) self . init_banner ( ) self . init_gui_pylab ( ) self . init_extensions ( ) self . init_code ( )
def init_shell ( self ) : self . shell = TerminalInteractiveShell . instance ( config = self . config , display_banner = False , profile_dir = self . profile_dir , ipython_dir = self . ipython_dir ) self . shell . configurables . append ( self )
def init_banner ( self ) : if self . display_banner and self . interact : self . shell . show_banner ( ) if self . log_level <= logging . INFO : print
def _pylab_changed ( self , name , old , new ) : if new == 'inline' : warn . warn ( "'inline' not available as pylab backend, " "using 'auto' instead.\n" ) self . pylab = 'auto'
def trait_metadata ( self , traitname , key ) : try : trait = getattr ( self . __class__ , traitname ) except AttributeError : raise TraitError ( "Class %s does not have a trait named %s" % ( self . __class__ . __name__ , traitname ) ) else : return trait . get_metadata ( key )
def validate ( self , obj , value ) : try : if issubclass ( value , self . klass ) : return value except : if ( value is None ) and ( self . _allow_none ) : return value self . error ( obj , value )
def info ( self ) : if isinstance ( self . klass , basestring ) : klass = self . klass else : klass = self . klass . __name__ result = 'a subclass of ' + klass if self . _allow_none : return result + ' or None' return result
def info ( self ) : result = 'any of ' + repr ( self . values ) if self . _allow_none : return result + ' or None' return result
def check ( self , completed , failed = None ) : if len ( self ) == 0 : return True against = set ( ) if self . success : against = completed if failed is not None and self . failure : against = against . union ( failed ) if self . all : return self . issubset ( against ) else : return not self . isdisjoint ( against )
def unreachable ( self , completed , failed = None ) : if len ( self ) == 0 : return False against = set ( ) if not self . success : against = completed if failed is not None and not self . failure : against = against . union ( failed ) if self . all : return not self . isdisjoint ( against ) else : return self . issubset ( against )
def as_dict ( self ) : return dict ( dependencies = list ( self ) , all = self . all , success = self . success , failure = self . failure )
def Ainv ( self ) : if not hasattr ( self , '_Ainv' ) : self . _Ainv = self . Solver ( self . A ) return self . _Ainv
def Ainv ( self ) : if getattr ( self , '_Ainv' , None ) is None : self . _Ainv = self . Solver ( self . A , 13 ) self . _Ainv . run_pardiso ( 12 ) return self . _Ainv
def depth ( n , tree ) : d = 0 parent = tree [ n ] while parent is not None : d += 1 parent = tree [ parent ] return d
def print_bintree ( tree , indent = '  ' ) : for n in sorted ( tree . keys ( ) ) : print "%s%s" % ( indent * depth ( n , tree ) , n )
def disambiguate_dns_url ( url , location ) : if not ip_pat . match ( location ) : location = socket . gethostbyname ( location ) return disambiguate_url ( url , location )
def allreduce ( self , f , value , flat = True ) : return self . reduce ( f , value , flat = flat , all = True )
def _validate_targets ( self , targets ) : if targets is None : return self . ids if isinstance ( targets , ( int , str , unicode ) ) : targets = [ targets ] _targets = [ ] for t in targets : if isinstance ( t , ( str , unicode ) ) : t = self . by_ident . get ( cast_bytes ( t ) , t ) _targets . append ( t ) targets = _targets bad_targets = [ t for t in targets if t not in self . ids ] if bad_targets : raise IndexError ( "No Such Engine: %r" % bad_targets ) if not targets : raise IndexError ( "No Engines Registered" ) return targets
def dispatch_query ( self , msg ) : try : idents , msg = self . session . feed_identities ( msg ) except ValueError : idents = [ ] if not idents : self . log . error ( "Bad Query Message: %r" , msg ) return client_id = idents [ 0 ] try : msg = self . session . unserialize ( msg , content = True ) except Exception : content = error . wrap_exception ( ) self . log . error ( "Bad Query Message: %r" , msg , exc_info = True ) self . session . send ( self . query , "hub_error" , ident = client_id , content = content ) return #switch on message type: msg_type = msg [ 'header' ] [ 'msg_type' ] self . log . info ( "client::client %r requested %r" , client_id , msg_type ) handler = self . query_handlers . get ( msg_type , None ) try : assert handler is not None , "Bad Message Type: %r" % msg_type except : content = error . wrap_exception ( ) self . log . error ( "Bad Message Type: %r" , msg_type , exc_info = True ) self . session . send ( self . query , "hub_error" , ident = client_id , content = content ) return else : handler ( idents , msg )
def save_task_request ( self , idents , msg ) : client_id = idents [ 0 ] try : msg = self . session . unserialize ( msg ) except Exception : self . log . error ( "task::client %r sent invalid task message: %r" , client_id , msg , exc_info = True ) return record = init_record ( msg ) record [ 'client_uuid' ] = client_id . decode ( 'ascii' ) record [ 'queue' ] = 'task' header = msg [ 'header' ] msg_id = header [ 'msg_id' ] self . pending . add ( msg_id ) self . unassigned . add ( msg_id ) try : existing = self . db . get_record ( msg_id ) if existing [ 'resubmitted' ] : for key in ( 'submitted' , 'client_uuid' , 'buffers' ) : record . pop ( key ) for key , evalue in existing . iteritems ( ) : if key . endswith ( 'buffers' ) : continue rvalue = record . get ( key , None ) if evalue and rvalue and evalue != rvalue : self . log . warn ( "conflicting initial state for record: %r:%r <%r> %r" , msg_id , rvalue , key , evalue ) elif evalue and not rvalue : record [ key ] = evalue try : self . db . update_record ( msg_id , record ) except Exception : self . log . error ( "DB Error updating record %r" , msg_id , exc_info = True ) except KeyError : try : self . db . add_record ( msg_id , record ) except Exception : self . log . error ( "DB Error adding record %r" , msg_id , exc_info = True ) except Exception : self . log . error ( "DB Error saving task request %r" , msg_id , exc_info = True )
def save_task_result ( self , idents , msg ) : client_id = idents [ 0 ] try : msg = self . session . unserialize ( msg ) except Exception : self . log . error ( "task::invalid task result message send to %r: %r" , client_id , msg , exc_info = True ) return parent = msg [ 'parent_header' ] if not parent : self . log . warn ( "Task %r had no parent!" , msg ) return msg_id = parent [ 'msg_id' ] if msg_id in self . unassigned : self . unassigned . remove ( msg_id ) header = msg [ 'header' ] engine_uuid = header . get ( 'engine' , u'' ) eid = self . by_ident . get ( cast_bytes ( engine_uuid ) , None ) status = header . get ( 'status' , None ) if msg_id in self . pending : self . log . info ( "task::task %r finished on %s" , msg_id , eid ) self . pending . remove ( msg_id ) self . all_completed . add ( msg_id ) if eid is not None : if status != 'aborted' : self . completed [ eid ] . append ( msg_id ) if msg_id in self . tasks [ eid ] : self . tasks [ eid ] . remove ( msg_id ) completed = header [ 'date' ] started = header . get ( 'started' , None ) result = { 'result_header' : header , 'result_content' : msg [ 'content' ] , 'started' : started , 'completed' : completed , 'received' : datetime . now ( ) , 'engine_uuid' : engine_uuid , } result [ 'result_buffers' ] = msg [ 'buffers' ] try : self . db . update_record ( msg_id , result ) except Exception : self . log . error ( "DB Error saving task request %r" , msg_id , exc_info = True ) else : self . log . debug ( "task::unknown task %r finished" , msg_id )
def save_iopub_message ( self , topics , msg ) : try : msg = self . session . unserialize ( msg , content = True ) except Exception : self . log . error ( "iopub::invalid IOPub message" , exc_info = True ) return parent = msg [ 'parent_header' ] if not parent : self . log . warn ( "iopub::IOPub message lacks parent: %r" , msg ) return msg_id = parent [ 'msg_id' ] msg_type = msg [ 'header' ] [ 'msg_type' ] content = msg [ 'content' ] try : rec = self . db . get_record ( msg_id ) except KeyError : rec = empty_record ( ) rec [ 'msg_id' ] = msg_id self . db . add_record ( msg_id , rec ) d = { } if msg_type == 'stream' : name = content [ 'name' ] s = rec [ name ] or '' d [ name ] = s + content [ 'data' ] elif msg_type == 'pyerr' : d [ 'pyerr' ] = content elif msg_type == 'pyin' : d [ 'pyin' ] = content [ 'code' ] elif msg_type in ( 'display_data' , 'pyout' ) : d [ msg_type ] = content elif msg_type == 'status' : pass else : self . log . warn ( "unhandled iopub msg_type: %r" , msg_type ) if not d : return try : self . db . update_record ( msg_id , d ) except Exception : self . log . error ( "DB Error saving iopub message %r" , msg_id , exc_info = True )
def connection_request ( self , client_id , msg ) : self . log . info ( "client::client %r connected" , client_id ) content = dict ( status = 'ok' ) content . update ( self . client_info ) jsonable = { } for k , v in self . keytable . iteritems ( ) : if v not in self . dead_engines : jsonable [ str ( k ) ] = v . decode ( 'ascii' ) content [ 'engines' ] = jsonable self . session . send ( self . query , 'connection_reply' , content , parent = msg , ident = client_id )
def register_engine ( self , reg , msg ) : content = msg [ 'content' ] try : queue = cast_bytes ( content [ 'queue' ] ) except KeyError : self . log . error ( "registration::queue not specified" , exc_info = True ) return heart = content . get ( 'heartbeat' , None ) if heart : heart = cast_bytes ( heart ) """register a new engine, and create the socket(s) necessary""" eid = self . _next_id self . log . debug ( "registration::register_engine(%i, %r, %r, %r)" , eid , queue , reg , heart ) content = dict ( id = eid , status = 'ok' ) content . update ( self . engine_info ) if queue in self . by_ident : try : raise KeyError ( "queue_id %r in use" % queue ) except : content = error . wrap_exception ( ) self . log . error ( "queue_id %r in use" , queue , exc_info = True ) elif heart in self . hearts : try : raise KeyError ( "heart_id %r in use" % heart ) except : self . log . error ( "heart_id %r in use" , heart , exc_info = True ) content = error . wrap_exception ( ) else : for h , pack in self . incoming_registrations . iteritems ( ) : if heart == h : try : raise KeyError ( "heart_id %r in use" % heart ) except : self . log . error ( "heart_id %r in use" , heart , exc_info = True ) content = error . wrap_exception ( ) break elif queue == pack [ 1 ] : try : raise KeyError ( "queue_id %r in use" % queue ) except : self . log . error ( "queue_id %r in use" , queue , exc_info = True ) content = error . wrap_exception ( ) break msg = self . session . send ( self . query , "registration_reply" , content = content , ident = reg ) if content [ 'status' ] == 'ok' : if heart in self . heartmonitor . hearts : self . incoming_registrations [ heart ] = ( eid , queue , reg [ 0 ] , None ) self . finish_registration ( heart ) else : purge = lambda : self . _purge_stalled_registration ( heart ) dc = ioloop . DelayedCallback ( purge , self . registration_timeout , self . loop ) dc . start ( ) self . incoming_registrations [ heart ] = ( eid , queue , reg [ 0 ] , dc ) else : self . log . error ( "registration::registration %i failed: %r" , eid , content [ 'evalue' ] ) return eid
def unregister_engine ( self , ident , msg ) : try : eid = msg [ 'content' ] [ 'id' ] except : self . log . error ( "registration::bad engine id for unregistration: %r" , ident , exc_info = True ) return self . log . info ( "registration::unregister_engine(%r)" , eid ) uuid = self . keytable [ eid ] content = dict ( id = eid , queue = uuid . decode ( 'ascii' ) ) self . dead_engines . add ( uuid ) # handleit = lambda : self . _handle_stranded_msgs ( eid , uuid ) dc = ioloop . DelayedCallback ( handleit , self . registration_timeout , self . loop ) dc . start ( ) if self . notifier : self . session . send ( self . notifier , "unregistration_notification" , content = content )
def shutdown_request ( self , client_id , msg ) : self . session . send ( self . query , 'shutdown_reply' , content = { 'status' : 'ok' } , ident = client_id ) self . session . send ( self . notifier , 'shutdown_notice' , content = { 'status' : 'ok' } ) dc = ioloop . DelayedCallback ( lambda : self . _shutdown ( ) , 1000 , self . loop ) dc . start ( )
def resubmit_task ( self , client_id , msg ) : def finish ( reply ) : self . session . send ( self . query , 'resubmit_reply' , content = reply , ident = client_id ) content = msg [ 'content' ] msg_ids = content [ 'msg_ids' ] reply = dict ( status = 'ok' ) try : records = self . db . find_records ( { 'msg_id' : { '$in' : msg_ids } } , keys = [ 'header' , 'content' , 'buffers' ] ) except Exception : self . log . error ( 'db::db error finding tasks to resubmit' , exc_info = True ) return finish ( error . wrap_exception ( ) ) found_ids = [ rec [ 'msg_id' ] for rec in records ] pending_ids = [ msg_id for msg_id in found_ids if msg_id in self . pending ] if len ( records ) > len ( msg_ids ) : try : raise RuntimeError ( "DB appears to be in an inconsistent state." "More matching records were found than should exist" ) except Exception : return finish ( error . wrap_exception ( ) ) elif len ( records ) < len ( msg_ids ) : missing = [ m for m in msg_ids if m not in found_ids ] try : raise KeyError ( "No such msg(s): %r" % missing ) except KeyError : return finish ( error . wrap_exception ( ) ) elif pending_ids : pass resubmitted = { } for rec in records : header = rec [ 'header' ] msg = self . session . msg ( header [ 'msg_type' ] , parent = header ) msg_id = msg [ 'msg_id' ] msg [ 'content' ] = rec [ 'content' ] fresh = msg [ 'header' ] header [ 'msg_id' ] = fresh [ 'msg_id' ] header [ 'date' ] = fresh [ 'date' ] msg [ 'header' ] = header self . session . send ( self . resubmit , msg , buffers = rec [ 'buffers' ] ) resubmitted [ rec [ 'msg_id' ] ] = msg_id self . pending . add ( msg_id ) msg [ 'buffers' ] = rec [ 'buffers' ] try : self . db . add_record ( msg_id , init_record ( msg ) ) except Exception : self . log . error ( "db::DB Error updating record: %s" , msg_id , exc_info = True ) finish ( dict ( status = 'ok' , resubmitted = resubmitted ) ) for msg_id , resubmit_id in resubmitted . iteritems ( ) : try : self . db . update_record ( msg_id , { 'resubmitted' : resubmit_id } ) except Exception : self . log . error ( "db::DB Error updating record: %s" , msg_id , exc_info = True )
def _extract_record ( self , rec ) : io_dict = { } for key in ( 'pyin' , 'pyout' , 'pyerr' , 'stdout' , 'stderr' ) : io_dict [ key ] = rec [ key ] content = { 'result_content' : rec [ 'result_content' ] , 'header' : rec [ 'header' ] , 'result_header' : rec [ 'result_header' ] , 'received' : rec [ 'received' ] , 'io' : io_dict , } if rec [ 'result_buffers' ] : buffers = map ( bytes , rec [ 'result_buffers' ] ) else : buffers = [ ] return content , buffers
def get_results ( self , client_id , msg ) : content = msg [ 'content' ] msg_ids = sorted ( set ( content [ 'msg_ids' ] ) ) statusonly = content . get ( 'status_only' , False ) pending = [ ] completed = [ ] content = dict ( status = 'ok' ) content [ 'pending' ] = pending content [ 'completed' ] = completed buffers = [ ] if not statusonly : try : matches = self . db . find_records ( dict ( msg_id = { '$in' : msg_ids } ) ) records = { } for rec in matches : records [ rec [ 'msg_id' ] ] = rec except Exception : content = error . wrap_exception ( ) self . session . send ( self . query , "result_reply" , content = content , parent = msg , ident = client_id ) return else : records = { } for msg_id in msg_ids : if msg_id in self . pending : pending . append ( msg_id ) elif msg_id in self . all_completed : completed . append ( msg_id ) if not statusonly : c , bufs = self . _extract_record ( records [ msg_id ] ) content [ msg_id ] = c buffers . extend ( bufs ) elif msg_id in records : if rec [ 'completed' ] : completed . append ( msg_id ) c , bufs = self . _extract_record ( records [ msg_id ] ) content [ msg_id ] = c buffers . extend ( bufs ) else : pending . append ( msg_id ) else : try : raise KeyError ( 'No such message: ' + msg_id ) except : content = error . wrap_exception ( ) break self . session . send ( self . query , "result_reply" , content = content , parent = msg , ident = client_id , buffers = buffers )
def get_history ( self , client_id , msg ) : try : msg_ids = self . db . get_history ( ) except Exception as e : content = error . wrap_exception ( ) else : content = dict ( status = 'ok' , history = msg_ids ) self . session . send ( self . query , "history_reply" , content = content , parent = msg , ident = client_id )
def db_query ( self , client_id , msg ) : content = msg [ 'content' ] query = content . get ( 'query' , { } ) keys = content . get ( 'keys' , None ) buffers = [ ] empty = list ( ) try : records = self . db . find_records ( query , keys ) except Exception as e : content = error . wrap_exception ( ) else : if keys is not None : buffer_lens = [ ] if 'buffers' in keys else None result_buffer_lens = [ ] if 'result_buffers' in keys else None else : buffer_lens = None result_buffer_lens = None for rec in records : b = rec . pop ( 'buffers' , empty ) or empty if buffer_lens is not None : buffer_lens . append ( len ( b ) ) buffers . extend ( b ) rb = rec . pop ( 'result_buffers' , empty ) or empty if result_buffer_lens is not None : result_buffer_lens . append ( len ( rb ) ) buffers . extend ( rb ) content = dict ( status = 'ok' , records = records , buffer_lens = buffer_lens , result_buffer_lens = result_buffer_lens ) self . session . send ( self . query , "db_reply" , content = content , parent = msg , ident = client_id , buffers = buffers )
def cd ( self , newdir ) : prevdir = os . getcwd ( ) os . chdir ( newdir ) try : yield finally : os . chdir ( prevdir )
def decode_cmd_out ( self , completed_cmd ) : try : stdout = completed_cmd . stdout . encode ( 'utf-8' ) . decode ( ) except AttributeError : try : stdout = str ( bytes ( completed_cmd . stdout ) , 'big5' ) . strip ( ) except AttributeError : stdout = str ( bytes ( completed_cmd . stdout ) . decode ( 'utf-8' ) ) . strip ( ) try : stderr = completed_cmd . stderr . encode ( 'utf-8' ) . decode ( ) except AttributeError : try : stderr = str ( bytes ( completed_cmd . stderr ) , 'big5' ) . strip ( ) except AttributeError : stderr = str ( bytes ( completed_cmd . stderr ) . decode ( 'utf-8' ) ) . strip ( ) return ParsedCompletedCommand ( completed_cmd . returncode , completed_cmd . args , stdout , stderr )
def run_command_under_r_root ( self , cmd , catched = True ) : RPATH = self . path with self . cd ( newdir = RPATH ) : if catched : process = sp . run ( cmd , stdout = sp . PIPE , stderr = sp . PIPE ) else : process = sp . run ( cmd ) return process
def get_installed_version ( name ) : pattern = re . compile ( r'''Installed:\s+(?P<version>.*)''' ) cmd = 'apt-cache policy %s' % name args = shlex . split ( cmd ) try : output = subprocess . check_output ( args ) if not output : return None except CalledProcessError : return None match = pattern . search ( output ) if match : version = match . groupdict ( ) [ 'version' ] if version == '(none)' : return None else : return version
def squash_unicode ( obj ) : if isinstance ( obj , dict ) : for key in obj . keys ( ) : obj [ key ] = squash_unicode ( obj [ key ] ) if isinstance ( key , unicode ) : obj [ squash_unicode ( key ) ] = obj . pop ( key ) elif isinstance ( obj , list ) : for i , v in enumerate ( obj ) : obj [ i ] = squash_unicode ( v ) elif isinstance ( obj , unicode ) : obj = obj . encode ( 'utf8' ) return obj
def extract_header ( msg_or_header ) : if not msg_or_header : return { } try : h = msg_or_header [ 'header' ] except KeyError : try : h = msg_or_header [ 'msg_id' ] except KeyError : raise else : h = msg_or_header if not isinstance ( h , dict ) : h = dict ( h ) return h
def _check_packers ( self ) : pack = self . pack unpack = self . unpack msg = dict ( a = [ 1 , 'hi' ] ) try : packed = pack ( msg ) except Exception : raise ValueError ( "packer could not serialize a simple message" ) if not isinstance ( packed , bytes ) : raise ValueError ( "message packed to %r, but bytes are required" % type ( packed ) ) try : unpacked = unpack ( packed ) except Exception : raise ValueError ( "unpacker could not handle the packer's output" ) msg = dict ( t = datetime . now ( ) ) try : unpacked = unpack ( pack ( msg ) ) except Exception : self . pack = lambda o : pack ( squash_dates ( o ) ) self . unpack = lambda s : extract_dates ( unpack ( s ) )
def object_info ( * * kw ) : infodict = dict ( izip_longest ( info_fields , [ None ] ) ) infodict . update ( kw ) return infodict
def __head ( self , h ) : return '%s%s%s' % ( self . color_table . active_colors . header , h , self . color_table . active_colors . normal )
def noinfo ( self , msg , oname ) : print 'No %s found' % msg , if oname : print 'for %s' % oname else : print
def psource ( self , obj , oname = '' ) : linecache . checkcache ( ) try : src = getsource ( obj ) except : self . noinfo ( 'source' , oname ) else : page . page ( self . format ( py3compat . unicode_to_str ( src ) ) )
def pfile ( self , obj , oname = '' ) : lineno = find_source_lines ( obj ) if lineno is None : self . noinfo ( 'file' , oname ) return ofile = find_file ( obj ) if ofile . endswith ( ( '.so' , '.dll' , '.pyd' ) ) : print 'File %r is binary, not printing.' % ofile elif not os . path . isfile ( ofile ) : print 'File %r does not exist, not printing.' % ofile else : page . page ( self . format ( open ( ofile ) . read ( ) ) , lineno - 1 )
def print_figure ( fig , fmt = 'png' ) : if not fig . axes and not fig . lines : return fc = fig . get_facecolor ( ) ec = fig . get_edgecolor ( ) fig . set_facecolor ( 'white' ) fig . set_edgecolor ( 'white' ) try : bytes_io = BytesIO ( ) fig . canvas . print_figure ( bytes_io , format = fmt , bbox_inches = 'tight' ) data = bytes_io . getvalue ( ) finally : fig . set_facecolor ( fc ) fig . set_edgecolor ( ec ) return data
def activate_matplotlib ( backend ) : import matplotlib if backend . startswith ( 'module://' ) : matplotlib . rcParams [ 'backend' ] = backend else : matplotlib . use ( backend ) matplotlib . interactive ( True ) import matplotlib . pylab as pylab #import matplotlib.pyplot #matplotlib.pyplot.switch_backend(backend) pylab . show . _needmain = False pylab . draw_if_interactive = flag_calls ( pylab . draw_if_interactive )
def import_pylab ( user_ns , import_all = True ) : s = ( "import numpy\n" "import matplotlib\n" "from matplotlib import pylab, mlab, pyplot\n" "np = numpy\n" "plt = pyplot\n" ) exec s in user_ns if import_all : s = ( "from matplotlib.pylab import *\n" "from numpy import *\n" ) exec s in user_ns
def _trace ( self , frame , event , arg_unused ) : if self . stopped : return if 0 : sys . stderr . write ( "trace event: %s %r @%d\n" % ( event , frame . f_code . co_filename , frame . f_lineno ) ) if self . last_exc_back : if frame == self . last_exc_back : if self . arcs and self . cur_file_data : pair = ( self . last_line , - self . last_exc_firstlineno ) self . cur_file_data [ pair ] = None self . cur_file_data , self . last_line = self . data_stack . pop ( ) self . last_exc_back = None if event == 'call' : self . data_stack . append ( ( self . cur_file_data , self . last_line ) ) filename = frame . f_code . co_filename if filename not in self . should_trace_cache : tracename = self . should_trace ( filename , frame ) self . should_trace_cache [ filename ] = tracename else : tracename = self . should_trace_cache [ filename ] #print("called, stack is %d deep, tracename is %r" % ( if tracename : if tracename not in self . data : self . data [ tracename ] = { } self . cur_file_data = self . data [ tracename ] else : self . cur_file_data = None self . last_line = - 1 elif event == 'line' : if self . cur_file_data is not None : if self . arcs : #print("lin", self.last_line, frame.f_lineno) self . cur_file_data [ ( self . last_line , frame . f_lineno ) ] = None else : #print("lin", frame.f_lineno) self . cur_file_data [ frame . f_lineno ] = None self . last_line = frame . f_lineno elif event == 'return' : if self . arcs and self . cur_file_data : first = frame . f_code . co_firstlineno self . cur_file_data [ ( self . last_line , - first ) ] = None self . cur_file_data , self . last_line = self . data_stack . pop ( ) #print("returned, stack is %d deep" % (len(self.data_stack))) elif event == 'exception' : #print("exc", self.last_line, frame.f_lineno) self . last_exc_back = frame . f_back self . last_exc_firstlineno = frame . f_code . co_firstlineno return self . _trace
def stop ( self ) : self . stopped = True if self . thread != threading . currentThread ( ) : return if hasattr ( sys , "gettrace" ) and self . warn : if sys . gettrace ( ) != self . _trace : msg = "Trace function changed, measurement is likely wrong: %r" self . warn ( msg % ( sys . gettrace ( ) , ) ) #print("Stopping tracer on %s" % threading.current_thread().ident) sys . settrace ( None )
def _start_tracer ( self ) : tracer = self . _trace_class ( ) tracer . data = self . data tracer . arcs = self . branch tracer . should_trace = self . should_trace tracer . should_trace_cache = self . should_trace_cache tracer . warn = self . warn fn = tracer . start ( ) self . tracers . append ( tracer ) return fn
def _installation_trace ( self , frame_unused , event_unused , arg_unused ) : sys . settrace ( None ) fn = self . _start_tracer ( ) if fn : fn = fn ( frame_unused , event_unused , arg_unused ) return fn
def start ( self ) : if self . _collectors : self . _collectors [ - 1 ] . pause ( ) self . _collectors . append ( self ) #print("Started: %r" % self._collectors, file=sys.stderr) traces0 = [ ] if hasattr ( sys , "gettrace" ) : fn0 = sys . gettrace ( ) if fn0 : tracer0 = getattr ( fn0 , '__self__' , None ) if tracer0 : traces0 = getattr ( tracer0 , 'traces' , [ ] ) fn = self . _start_tracer ( ) for args in traces0 : ( frame , event , arg ) , lineno = args try : fn ( frame , event , arg , lineno = lineno ) except TypeError : raise Exception ( "fullcoverage must be run with the C trace function." ) threading . settrace ( self . _installation_trace )
def stop ( self ) : #print >>sys.stderr, "Stopping: %r" % self._collectors assert self . _collectors assert self . _collectors [ - 1 ] is self self . pause ( ) self . tracers = [ ] self . _collectors . pop ( ) if self . _collectors : self . _collectors [ - 1 ] . resume ( )
def pause ( self ) : for tracer in self . tracers : tracer . stop ( ) stats = tracer . get_stats ( ) if stats : print ( "\nCoverage.py tracer stats:" ) for k in sorted ( stats . keys ( ) ) : print ( "%16s: %s" % ( k , stats [ k ] ) ) threading . settrace ( None )
def resume ( self ) : for tracer in self . tracers : tracer . start ( ) threading . settrace ( self . _installation_trace )
def new_code_cell ( code = None , prompt_number = None ) : cell = NotebookNode ( ) cell . cell_type = u'code' if code is not None : cell . code = unicode ( code ) if prompt_number is not None : cell . prompt_number = int ( prompt_number ) return cell
def new_text_cell ( text = None ) : cell = NotebookNode ( ) if text is not None : cell . text = unicode ( text ) cell . cell_type = u'text' return cell
def new_notebook ( cells = None ) : nb = NotebookNode ( ) if cells is not None : nb . cells = cells else : nb . cells = [ ] return nb
def render_traceback ( self , excid = None ) : lines = [ ] if excid is None : for ( en , ev , etb , ei ) in self . elist : lines . append ( self . _get_engine_str ( ei ) ) lines . extend ( ( etb or 'No traceback available' ) . splitlines ( ) ) lines . append ( '' ) else : try : en , ev , etb , ei = self . elist [ excid ] except : raise IndexError ( "an exception with index %i does not exist" % excid ) else : lines . append ( self . _get_engine_str ( ei ) ) lines . extend ( ( etb or 'No traceback available' ) . splitlines ( ) ) return lines
def _canonical_dir ( self , morf ) : return os . path . split ( CodeUnit ( morf , self . file_locator ) . filename ) [ 0 ]
def _source_for_file ( self , filename ) : if not filename . endswith ( ".py" ) : if filename [ - 4 : - 1 ] == ".py" : filename = filename [ : - 1 ] elif filename . endswith ( "$py.class" ) : filename = filename [ : - 9 ] + ".py" return filename
def _warn ( self , msg ) : self . _warnings . append ( msg ) sys . stderr . write ( "Coverage.py warning: %s\n" % msg )
def _check_for_packages ( self ) : if self . source_pkgs : found = [ ] for pkg in self . source_pkgs : try : mod = sys . modules [ pkg ] except KeyError : continue found . append ( pkg ) try : pkg_file = mod . __file__ except AttributeError : pkg_file = None else : d , f = os . path . split ( pkg_file ) if f . startswith ( '__init__' ) : pkg_file = d else : pkg_file = self . _source_for_file ( pkg_file ) pkg_file = self . file_locator . canonical_filename ( pkg_file ) if not os . path . exists ( pkg_file ) : pkg_file = None if pkg_file : self . source . append ( pkg_file ) self . source_match . add ( pkg_file ) else : self . _warn ( "Module %s has no Python source." % pkg ) for pkg in found : self . source_pkgs . remove ( pkg )
def _atexit ( self ) : if self . _started : self . stop ( ) if self . auto_data : self . save ( )
def _exclude_regex ( self , which ) : if which not in self . _exclude_re : excl_list = getattr ( self . config , which + "_list" ) self . _exclude_re [ which ] = join_regex ( excl_list ) return self . _exclude_re [ which ]
def save ( self ) : data_suffix = self . data_suffix if data_suffix is True : extra = "" if _TEST_NAME_FILE : f = open ( _TEST_NAME_FILE ) test_name = f . read ( ) f . close ( ) extra = "." + test_name data_suffix = "%s%s.%s.%06d" % ( socket . gethostname ( ) , extra , os . getpid ( ) , random . randint ( 0 , 999999 ) ) self . _harvest_data ( ) self . data . write ( suffix = data_suffix )
def analysis ( self , morf ) : f , s , _ , m , mf = self . analysis2 ( morf ) return f , s , m , mf
def reload ( self ) : if self . filename is not None : with open ( self . filename , self . _read_flags ) as f : self . data = f . read ( ) elif self . url is not None : try : import urllib2 response = urllib2 . urlopen ( self . url ) self . data = response . read ( ) encoding = None for sub in response . headers [ 'content-type' ] . split ( ';' ) : sub = sub . strip ( ) if sub . startswith ( 'charset' ) : encoding = sub . split ( '=' ) [ - 1 ] . strip ( ) break if encoding : self . data = self . data . decode ( encoding , 'replace' ) except : self . data = None
def _find_cmd ( cmd ) : path = sp . Popen ( [ '/usr/bin/env' , 'which' , cmd ] , stdout = sp . PIPE , stderr = sp . PIPE ) . communicate ( ) [ 0 ] return py3compat . bytes_to_str ( path )
def run ( self ) : line = self . fd . readline ( ) if isinstance ( line , unicode ) : send = self . sock . send_unicode else : send = self . sock . send while line : send ( line ) line = self . fd . readline ( ) self . fd . close ( ) self . sock . close ( )
def start ( self ) : try : pid = self . get_pid_from_file ( ) except PIDFileError : self . log . critical ( 'Could not read pid file, cluster is probably not running.' ) self . remove_pid_file ( ) self . exit ( ALREADY_STOPPED ) if not self . check_pid ( pid ) : self . log . critical ( 'Cluster [pid=%r] is not running.' % pid ) self . remove_pid_file ( ) self . exit ( ALREADY_STOPPED ) elif os . name == 'posix' : sig = self . signal self . log . info ( "Stopping cluster [pid=%r] with [signal=%r]" % ( pid , sig ) ) try : os . kill ( pid , sig ) except OSError : self . log . error ( "Stopping cluster failed, assuming already dead." , exc_info = True ) self . remove_pid_file ( ) elif os . name == 'nt' : try : p = check_call ( [ 'taskkill' , '-pid' , str ( pid ) , '-t' , '-f' ] , stdout = PIPE , stderr = PIPE ) except ( CalledProcessError , OSError ) : self . log . error ( "Stopping cluster failed, assuming already dead." , exc_info = True ) self . remove_pid_file ( )
def build_launcher ( self , clsname , kind = None ) : try : klass = find_launcher_class ( clsname , kind ) except ( ImportError , KeyError ) : self . log . fatal ( "Could not import launcher class: %r" % clsname ) self . exit ( 1 ) launcher = klass ( work_dir = u'.' , config = self . config , log = self . log , profile_dir = self . profile_dir . location , cluster_id = self . cluster_id , ) return launcher
def start ( self ) : self . log . info ( "IPython cluster: started" ) self . log . info ( 'Starting engines with [daemon=%r]' % self . daemonize ) if self . daemonize : if os . name == 'posix' : daemonize ( ) dc = ioloop . DelayedCallback ( self . start_engines , 0 , self . loop ) dc . start ( ) try : self . loop . start ( ) except KeyboardInterrupt : pass except zmq . ZMQError as e : if e . errno == errno . EINTR : pass else : raise
def start ( self ) : try : pid = self . get_pid_from_file ( ) except PIDFileError : pass else : if self . check_pid ( pid ) : self . log . critical ( 'Cluster is already running with [pid=%s]. ' 'use "ipcluster stop" to stop the cluster.' % pid ) self . exit ( ALREADY_STARTED ) else : self . remove_pid_file ( ) self . log . info ( 'Starting ipcluster with [daemon=%r]' % self . daemonize ) if self . daemonize : if os . name == 'posix' : daemonize ( ) dc = ioloop . DelayedCallback ( self . start_controller , 0 , self . loop ) dc . start ( ) dc = ioloop . DelayedCallback ( self . start_engines , 1000 * self . delay , self . loop ) dc . start ( ) self . write_pid_file ( ) try : self . loop . start ( ) except KeyboardInterrupt : pass except zmq . ZMQError as e : if e . errno == errno . EINTR : pass else : raise finally : self . remove_pid_file ( )
def get_app_wx ( * args , * * kwargs ) : import wx app = wx . GetApp ( ) if app is None : if not kwargs . has_key ( 'redirect' ) : kwargs [ 'redirect' ] = False app = wx . PySimpleApp ( * args , * * kwargs ) return app
def is_event_loop_running_wx ( app = None ) : if app is None : app = get_app_wx ( ) if hasattr ( app , '_in_event_loop' ) : return app . _in_event_loop else : return app . IsMainLoopRunning ( )
def start_event_loop_wx ( app = None ) : if app is None : app = get_app_wx ( ) if not is_event_loop_running_wx ( app ) : app . _in_event_loop = True app . MainLoop ( ) app . _in_event_loop = False else : app . _in_event_loop = True
def get_app_qt4 ( * args , * * kwargs ) : from IPython . external . qt_for_kernel import QtGui app = QtGui . QApplication . instance ( ) if app is None : if not args : args = ( [ '' ] , ) app = QtGui . QApplication ( * args , * * kwargs ) return app
def is_event_loop_running_qt4 ( app = None ) : if app is None : app = get_app_qt4 ( [ '' ] ) if hasattr ( app , '_in_event_loop' ) : return app . _in_event_loop else : return False
def start_event_loop_qt4 ( app = None ) : if app is None : app = get_app_qt4 ( [ '' ] ) if not is_event_loop_running_qt4 ( app ) : app . _in_event_loop = True app . exec_ ( ) app . _in_event_loop = False else : app . _in_event_loop = True
def check_package ( self , package , package_dir ) : try : return self . packages_checked [ package ] except KeyError : pass init_py = _build_py . check_package ( self , package , package_dir ) self . packages_checked [ package ] = init_py if not init_py or not self . distribution . namespace_packages : return init_py for pkg in self . distribution . namespace_packages : if pkg == package or pkg . startswith ( package + '.' ) : break else : return init_py f = open ( init_py , 'rbU' ) if 'declare_namespace' . encode ( ) not in f . read ( ) : from distutils import log log . warn ( "WARNING: %s is a namespace package, but its __init__.py does\n" "not declare_namespace(); setuptools 0.7 will REQUIRE this!\n" '(See the setuptools manual under "Namespace Packages" for ' "details.)\n" , package ) f . close ( ) return init_py
def get_unique_key_from_get ( get_dict ) : site = Site . objects . get_current ( ) key = get_dict_to_encoded_url ( get_dict ) cache_key = '{}_{}' . format ( site . domain , key ) return hashlib . md5 ( cache_key ) . hexdigest ( )
def get_domain ( url ) : if 'http' not in url . lower ( ) : url = 'http://{}' . format ( url ) return urllib . parse . urlparse ( url ) . hostname
def get_url_args ( url ) : url_data = urllib . parse . urlparse ( url ) arg_dict = urllib . parse . parse_qs ( url_data . query ) return arg_dict
