{"id": 0, "code": "def get vid from url ( url ) : return match1 ( url , r'youtu\\.be/([^?/]+)' ) or match1 ( url , r'youtube\\.com/embed/([^/?]+)' ) or match1 ( url , r'youtube\\.com/v/([^/?]+)' ) or match1 ( url , r'youtube\\.com/watch/([^/?]+)' ) or parse query param ( url , 'v' ) or parse query param ( parse query param ( url , 'u' ) , 'v' )", "predictions": ["get vid vid from url url ."], "references": ["extracts video id from url ."], "bleu": 0.29071536848410967, "rouge_l": 0.4680306905370844}
{"id": 1, "code": "def dailymotion download ( url , output dir = '.' , merge = True , info only = False , * * kwargs ) : html = get content ( rebuilt url ( url ) ) info = json . loads ( match1 ( html , r'qualities\":({.+?}),\"' ) ) title = match1 ( html , r'\"video title\"\\s*:\\s*\"([^\"]+)\"' ) or match1 ( html , r'\"title\"\\s*:\\s*\"([^\"]+)\"' ) title = unicodize ( title ) for quality in [ '1080' , '720' , '480' , '380' , '240' , '144' , 'auto' ] : try : real url = info [ quality ] [ 1 ] [ \"url\" ] if real url : break except Key Error : pass mime , ext , size = url info ( real url ) print info ( site info , title , mime , size ) if not info only : download urls ( [ real url ] , title , ext , size , output dir = output dir , merge = merge )", "predictions": ["download the dailymotion from a specific url"], "references": ["downloads dailymotion videos by url ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 2, "code": "def sina download ( url , output dir = '.' , merge = True , info only = False , * * kwargs ) : if 'news.sina.com.cn/zxt' in url : sina zxt ( url , output dir = output dir , merge = merge , info only = info only , * * kwargs ) return vid = match1 ( url , r'vid=(\\d+)' ) if vid is None : video page = get content ( url ) vid = hd vid = match1 ( video page , r'hd vid\\s*:\\s*\\'([^\\']+)\\'' ) if hd vid == '0' : vids = match1 ( video page , r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'' ) . split ( '|' ) vid = vids [ - 1 ] if vid is None : vid = match1 ( video page , r'vid:\"?(\\d+)\"?' ) if vid : #title = match1(video page, r'title\\s*:\\s*\\'([^\\']+)\\'') sina download by vid ( vid , output dir = output dir , merge = merge , info only = info only ) else : vkey = match1 ( video page , r'vkey\\s*:\\s*\"([^\"]+)\"' ) if vkey is None : vid = match1 ( url , r'#(\\d+)' ) sina download by vid ( vid , output dir = output dir , merge = merge , info only = info only ) return title = match1 ( video page , r'title\\s*:\\s*\"([^\"]+)\"' ) sina download by vkey ( vkey , title = title , output dir = output dir , merge = merge , info only = info only )", "predictions": ["download sina from sina ."], "references": ["downloads sina videos by url ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 3, "code": "def get vid from url ( self , url ) : hit = re . search ( r'live.qq.com/(\\d+)' , url ) if hit is not None : return hit . group ( 1 ) hit = re . search ( r'live.qq.com/directory/match/(\\d+)' , url ) if hit is not None : return self . get room id from url ( hit . group ( 1 ) ) html = get content ( url ) room id = match1 ( html , r'room id\\\":(\\d+)' ) if room id is None : log . wtf ( 'Unknown page {}' . format ( url ) ) return room id", "predictions": ["return the room id for the given url"], "references": ["extracts video id from live . qq . com ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 4, "code": "def sprint ( text , * colors ) : return \"\\33[{}m{content}\\33[{}m\" . format ( \";\" . join ( [ str ( color ) for color in colors ] ) , RESET , content = text ) if IS ANSI TERMINAL and colors else text", "predictions": ["return a string with the sprint sprint ."], "references": ["format text with color or other effects into ansi escaped string ."], "bleu": 0.11567041937737582, "rouge_l": 0.1930379746835443}
{"id": 5, "code": "def print log ( text , * colors ) : sys . stderr . write ( sprint ( \"{}: {}\" . format ( script name , text ) , * colors ) + \"\\n\" )", "predictions": ["print a log message to stdout ."], "references": ["print a log message to standard error ."], "bleu": 0.6129752413741055, "rouge_l": 0.7904967602591793}
{"id": 6, "code": "def e ( message , exit code = None ) : print log ( message , YELLOW , BOLD ) if exit code is not None : sys . exit ( exit code )", "predictions": ["print a log message"], "references": ["print an error log message ."], "bleu": 0.3081980909598119, "rouge_l": 0.5791139240506329}
{"id": 7, "code": "def wtf ( message , exit code = 1 ) : print log ( message , RED , BOLD ) if exit code is not None : sys . exit ( exit code )", "predictions": ["print a log message"], "references": ["what a terrible failure!"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 8, "code": "def detect os ( ) : syst = system ( ) . lower ( ) os = 'unknown' if 'cygwin' in syst : os = 'cygwin' elif 'darwin' in syst : os = 'mac' elif 'linux' in syst : os = 'linux' try : with open ( '/proc/version' , 'r' ) as f : if 'microsoft' in f . read ( ) . lower ( ) : os = 'wsl' except : pass elif 'windows' in syst : os = 'windows' elif 'bsd' in syst : os = 'bsd' return os", "predictions": ["detect the os os os ."], "references": ["detect operating system ."], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 9, "code": "def get vid from url ( url ) : vid = match1 ( url , 'https?://www.mgtv.com/(?:b|l)/\\d+/(\\d+).html' ) if not vid : vid = match1 ( url , 'https?://www.mgtv.com/hz/bdpz/\\d+/(\\d+).html' ) return vid", "predictions": ["return the vid from the given url"], "references": ["extracts video id from url ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 10, "code": "def legitimize ( text , os = detect os ( ) ) : text = text . translate ( { 0 : None , ord ( '/' ) : '-' , ord ( '|' ) : '-' , } ) if os == 'windows' or os == 'cygwin' or os == 'wsl' : text = text . translate ( { ord ( ':' ) : '-' , ord ( '*' ) : '-' , ord ( '?' ) : '-' , ord ( '\\\\' ) : '-' , ord ( '\\\"' ) : '\\'' , ord ( '+' ) : '-' , ord ( '<' ) : '-' , ord ( '>' ) : '-' , ord ( '[' ) : '(' , ord ( ']' ) : ')' , ord ( '\\t' ) : ' ' , } ) else : if os == 'mac' : text = text . translate ( { ord ( ':' ) : '-' , } ) if text . startswith ( \".\" ) : text = text [ 1 : ] text = text [ : 80 ] return text", "predictions": ["convert text to html ."], "references": ["converts a string to a valid filename ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 11, "code": "def cbs download ( url , output dir = '.' , merge = True , info only = False , * * kwargs ) : html = get content ( url ) pid = match1 ( html , r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'' ) title = match1 ( html , r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"' ) theplatform download by pid ( pid , title , output dir = output dir , merge = merge , info only = info only )", "predictions": ["download cbs from github website ."], "references": ["downloads cbs videos by url ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 12, "code": "def parse host ( host ) : if re . match ( r'^(\\d+)$' , host ) is not None : return ( \"0.0.0.0\" , int ( host ) ) if re . match ( r'^(\\w+)://' , host ) is None : host = \"//\" + host o = parse . urlparse ( host ) hostname = o . hostname or \"0.0.0.0\" port = o . port or 0 return ( hostname , port )", "predictions": ["parse a host host from a host ."], "references": ["parses host name and port number from a string ."], "bleu": 0.1867587389639562, "rouge_l": 0.43571428571428567}
{"id": 13, "code": "def get conn ( self ) : conn config = self . get conn params ( ) conn = snowflake . connector . connect ( * * conn config ) return conn", "predictions": ["return a conn object for the passed vm data"], "references": ["returns a snowflake . connection object"], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 14, "code": "def bulk load ( self , table , tmp file ) : self . copy expert ( \"COPY {table} FROM STDIN\" . format ( table = table ) , tmp file )", "predictions": ["bulk load a table from a table ."], "references": ["loads a tab - delimited file into a database table"], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 15, "code": "def bulk dump ( self , table , tmp file ) : self . copy expert ( \"COPY {table} TO STDOUT\" . format ( table = table ) , tmp file )", "predictions": ["dump the contents of a table to a table ."], "references": ["dumps a database table into a tab - delimited file"], "bleu": 0.14991106946711685, "rouge_l": 0.3}
{"id": 16, "code": "def execute ( self , context ) : hook = Google Cloud Storage Hook ( google cloud storage conn id = self . google cloud storage conn id , delegate to = self . delegate to ) hook . upload ( bucket name = self . bucket , object name = self . dst , mime type = self . mime type , filename = self . src , gzip = self . gzip , )", "predictions": ["execute the hook ."], "references": ["uploads the file to google cloud storage"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 17, "code": "def get conn ( self ) : conn = self . get connection ( self . mysql conn id ) conn config = { \"user\" : conn . login , \"passwd\" : conn . password or '' , \"host\" : conn . host or 'localhost' , \"db\" : self . schema or conn . schema or '' } if not conn . port : conn config [ \"port\" ] = 3306 else : conn config [ \"port\" ] = int ( conn . port ) if conn . extra dejson . get ( 'charset' , False ) : conn config [ \"charset\" ] = conn . extra dejson [ \"charset\" ] if ( conn config [ \"charset\" ] ) . lower ( ) == 'utf8' or ( conn config [ \"charset\" ] ) . lower ( ) == 'utf-8' : conn config [ \"use unicode\" ] = True if conn . extra dejson . get ( 'cursor' , False ) : if ( conn . extra dejson [ \"cursor\" ] ) . lower ( ) == 'sscursor' : conn config [ \"cursorclass\" ] = My SQ Ldb . cursors . SS Cursor elif ( conn . extra dejson [ \"cursor\" ] ) . lower ( ) == 'dictcursor' : conn config [ \"cursorclass\" ] = My SQ Ldb . cursors . Dict Cursor elif ( conn . extra dejson [ \"cursor\" ] ) . lower ( ) == 'ssdictcursor' : conn config [ \"cursorclass\" ] = My SQ Ldb . cursors . SS Dict Cursor local infile = conn . extra dejson . get ( 'local infile' , False ) if conn . extra dejson . get ( 'ssl' , False ) : dejson ssl = conn . extra dejson [ 'ssl' ] if isinstance ( dejson ssl , six . string types ) : dejson ssl = json . loads ( dejson ssl ) conn config [ 'ssl' ] = dejson ssl if conn . extra dejson . get ( 'unix socket' ) : conn config [ 'unix socket' ] = conn . extra dejson [ 'unix socket' ] if local infile : conn config [ \"local infile\" ] = 1 conn = My SQ Ldb . connect ( * * conn config ) return conn", "predictions": ["return a conn object for the passed vm data"], "references": ["returns a mysql connection object"], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 18, "code": "def bulk load ( self , table , tmp file ) : conn = self . get conn ( ) cur = conn . cursor ( ) cur . execute ( . format ( tmp file = tmp file , table = table ) ) conn . commit ( )", "predictions": ["bulk bulk load data from a table ."], "references": ["loads a tab - delimited file into a database table"], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 19, "code": "def get proxy version ( self ) : self . download sql proxy if needed ( ) command to run = [ self . sql proxy path ] command to run . extend ( [ '--version' ] ) command to run . extend ( self . get credential parameters ( ) ) result = subprocess . check output ( command to run ) . decode ( 'utf-8' ) pattern = re . compile ( \"^.*[V|v]ersion ([^;]*);.*$\" ) m = pattern . match ( result ) if m : return m . group ( 1 ) else : return None", "predictions": ["returns the proxy version of the proxy ."], "references": ["returns version of the cloud sql proxy ."], "bleu": 0.3688939732334406, "rouge_l": 0.75}
{"id": 20, "code": "def cleanup database hook ( self ) : if self . database type == 'postgres' : if hasattr ( self . db hook , 'conn' ) and self . db hook . conn and self . db hook . conn . notices : for output in self . db hook . conn . notices : self . log . info ( output )", "predictions": ["cleanup all database hook ."], "references": ["clean up database hook after it was used ."], "bleu": 0.17348474258688365, "rouge_l": 0.40757238307349664}
{"id": 21, "code": "def reserve free tcp port ( self ) : self . reserved tcp socket = socket . socket ( socket . AF INET , socket . SOCK STREAM ) self . reserved tcp socket . bind ( ( '127.0.0.1' , 0 ) ) self . sql proxy tcp port = self . reserved tcp socket . getsockname ( ) [ 1 ]", "predictions": ["reserve free socket port ."], "references": ["reserve free tcp port to be used by cloud sql proxy"], "bleu": 0.11629030063732083, "rouge_l": 0.35124760076775424}
{"id": 22, "code": "def get error code ( self , e ) : try : matches = self . error code pattern . match ( str ( e ) ) code = int ( matches . group ( 0 ) ) return code except Value Error : return e", "predictions": ["return the error code code ."], "references": ["extract error code from ftp exception"], "bleu": 0.2907153684841096, "rouge_l": 0.3333333333333333}
{"id": 23, "code": "def integrate plugins ( ) : import sys from airflow . plugins manager import sensors modules for sensors module in sensors modules : sys . modules [ sensors module . name ] = sensors module globals ( ) [ sensors module . name ] = sensors module", "predictions": ["integrate all sensors in the module ."], "references": ["integrate plugins to the context"], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 24, "code": "def clear dag runs ( ) : session = settings . Session ( ) drs = session . query ( Dag Run ) . filter ( Dag Run . dag id . in ( DAG IDS ) , ) . all ( ) for dr in drs : logging . info ( 'Deleting Dag Run :: {}' . format ( dr ) ) session . delete ( dr )", "predictions": ["clears all dag runs ."], "references": ["remove any existing dag runs for the perf test dags ."], "bleu": 0.11629030063732083, "rouge_l": 0.35124760076775424}
{"id": 25, "code": "def clear dag task instances ( ) : session = settings . Session ( ) TI = Task Instance tis = ( session . query ( TI ) . filter ( TI . dag id . in ( DAG IDS ) ) . all ( ) ) for ti in tis : logging . info ( 'Deleting Task Instance :: {}' . format ( ti ) ) session . delete ( ti ) session . commit ( )", "predictions": ["clear all task instances ."], "references": ["remove any existing task instances for the perf test dags ."], "bleu": 0.11629030063732083, "rouge_l": 0.35124760076775424}
{"id": 26, "code": "def set dags paused state ( is paused ) : session = settings . Session ( ) dms = session . query ( Dag Model ) . filter ( Dag Model . dag id . in ( DAG IDS ) ) for dm in dms : logging . info ( 'Setting DAG :: {} is paused={}' . format ( dm , is paused ) ) dm . is paused = is paused session . commit ( )", "predictions": ["set the dags state of the dag ."], "references": ["toggle the pause state of the dags in the test ."], "bleu": 0.2535368728139476, "rouge_l": 0.511744966442953}
{"id": 27, "code": "def print stats ( self ) : session = settings . Session ( ) TI = Task Instance tis = ( session . query ( TI ) . filter ( TI . dag id . in ( DAG IDS ) ) . all ( ) ) successful tis = [ x for x in tis if x . state == State . SUCCESS ] ti perf = [ ( ti . dag id , ti . task id , ti . execution date , ( ti . queued dttm - self . start date ) . total seconds ( ) , ( ti . start date - self . start date ) . total seconds ( ) , ( ti . end date - self . start date ) . total seconds ( ) , ti . duration ) for ti in successful tis ] ti perf df = pd . Data Frame ( ti perf , columns = [ 'dag id' , 'task id' , 'execution date' , 'queue delay' , 'start delay' , 'land time' , 'duration' ] ) print ( 'Performance Results' ) print ( '###################' ) for dag id in DAG IDS : print ( 'DAG {}' . format ( dag id ) ) print ( ti perf df [ ti perf df [ 'dag id' ] == dag id ] ) print ( '###################' ) if len ( tis ) > len ( successful tis ) : print ( \"WARNING!! The following task instances haven't completed\" ) print ( pd . Data Frame ( [ ( ti . dag id , ti . task id , ti . execution date , ti . state ) for ti in filter ( lambda x : x . state != State . SUCCESS , tis ) ] , columns = [ 'dag id' , 'task id' , 'execution date' , 'state' ] ) ) session . commit ( )", "predictions": ["print statistics for the dag"], "references": ["print operational metrics for the scheduler test ."], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 28, "code": "def heartbeat ( self ) : super ( Scheduler Metrics Job , self ) . heartbeat ( ) session = settings . Session ( ) TI = Task Instance successful tis = ( session . query ( TI ) . filter ( TI . dag id . in ( DAG IDS ) ) . filter ( TI . state . in ( [ State . SUCCESS ] ) ) . all ( ) ) session . commit ( ) dagbag = Dag Bag ( SUBDIR ) dags = [ dagbag . dags [ dag id ] for dag id in DAG IDS ] num task instances = sum ( [ ( timezone . utcnow ( ) - task . start date ) . days for dag in dags for task in dag . tasks ] ) if ( len ( successful tis ) == num task instances or ( timezone . utcnow ( ) - self . start date ) . total seconds ( ) > MAX RUNTIME SECS ) : if len ( successful tis ) == num task instances : self . log . info ( \"All tasks processed! Printing stats.\" ) else : self . log . info ( \"Test timeout reached. Printing available stats.\" ) self . print stats ( ) set dags paused state ( True ) sys . exit ( )", "predictions": ["perform a dag of the dag of the dag ."], "references": ["override the scheduler heartbeat to determine when the test is complete"], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 29, "code": "def get dag run state ( dag id , execution date ) : dagbag = Dag Bag ( ) if dag id not in dagbag . dags : error message = \"Dag id {} not found\" . format ( dag id ) raise Dag Not Found ( error message ) dag = dagbag . get dag ( dag id ) dagrun = dag . get dagrun ( execution date = execution date ) if not dagrun : error message = ( 'Dag Run for date {} not found in dag {}' . format ( execution date , dag id ) ) raise Dag Run Not Found ( error message ) return { 'state' : dagrun . get state ( ) }", "predictions": ["get the state of the dag run ."], "references": ["return the task object identified by the given dag_id and task_id ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 30, "code": "def get conn ( self ) : conn = self . get connection ( self . druid broker conn id ) druid broker conn = connect ( host = conn . host , port = conn . port , path = conn . extra dejson . get ( 'endpoint' , '/druid/v2/sql' ) , scheme = conn . extra dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to druid broker on %s' , conn . host ) return druid broker conn", "predictions": ["return a conn object for the passed vm data"], "references": ["establish a connection to druid broker ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 31, "code": "def create session ( ) : session = settings . Session ( ) try : yield session session . commit ( ) except Exception : session . rollback ( ) raise finally : session . close ( )", "predictions": ["create a series of operations ."], "references": ["contextmanager that will create and teardown a session ."], "bleu": 0.1593301391270729, "rouge_l": 0.3860759493670886}
{"id": 32, "code": "def resetdb ( ) : from airflow import models from alembic . migration import Migration Context log . info ( \"Dropping tables that exist\" ) models . base . Base . metadata . drop all ( settings . engine ) mc = Migration Context . configure ( settings . engine ) if mc . version . exists ( settings . engine ) : mc . version . drop ( settings . engine ) from flask appbuilder . models . sqla import Base Base . metadata . drop all ( settings . engine ) initdb ( )", "predictions": ["drop all all all"], "references": ["clear out the database"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 33, "code": "def execute ( self , context ) : hook = Wasb Hook ( wasb conn id = self . wasb conn id ) self . log . info ( 'Uploading %s to wasb://%s ' 'as %s' . format ( self . file path , self . container name , self . blob name ) ) hook . load file ( self . file path , self . container name , self . blob name , * * self . load options )", "predictions": ["execute the = blob"], "references": ["upload a file to azure blob storage ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 34, "code": "def get conn ( self ) : db = self . get connection ( self . presto conn id ) reqkwargs = None if db . password is not None : reqkwargs = { 'auth' : HTTP Basic Auth ( db . login , db . password ) } return presto . connect ( host = db . host , port = db . port , username = db . login , source = db . extra dejson . get ( 'source' , 'airflow' ) , protocol = db . extra dejson . get ( 'protocol' , 'http' ) , catalog = db . extra dejson . get ( 'catalog' , 'hive' ) , requests kwargs = reqkwargs , schema = db . schema )", "predictions": ["is the mysql connection to use for the mysql database"], "references": ["returns a connection object"], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 35, "code": "def get pretty exception message ( e ) : if ( hasattr ( e , 'message' ) and 'error Name' in e . message and 'message' in e . message ) : return ( '{name}: {message}' . format ( name = e . message [ 'error Name' ] , message = e . message [ 'message' ] ) ) else : return str ( e )", "predictions": ["returns a vid url url for the given from the vid"], "references": ["parses some databaseerror to provide a better error message"], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 36, "code": "def get records ( self , hql , parameters = None ) : try : return super ( ) . get records ( self . strip sql ( hql ) , parameters ) except Database Error as e : raise Presto Exception ( self . get pretty exception message ( e ) )", "predictions": ["sprint method to sprint records and returns a list of records ."], "references": ["get a set of records from presto"], "bleu": 0.14694106251955755, "rouge_l": 0.3315217391304348}
{"id": 37, "code": "def get pandas df ( self , hql , parameters = None ) : import pandas cursor = self . get cursor ( ) try : cursor . execute ( self . strip sql ( hql ) , parameters ) data = cursor . fetchall ( ) except Database Error as e : raise Presto Exception ( self . get pretty exception message ( e ) ) column descriptions = cursor . description if data : df = pandas . Data Frame ( data ) df . columns = [ c [ 0 ] for c in column descriptions ] else : df = pandas . Data Frame ( ) return df", "predictions": ["print a log df"], "references": ["get a pandas dataframe from a sql query ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 38, "code": "def run ( self , hql , parameters = None ) : return super ( ) . run ( self . strip sql ( hql ) , parameters )", "predictions": ["runs the specified if it s not already provided . ."], "references": ["execute the statement against presto . can be used to create views ."], "bleu": 0.11294012253658708, "rouge_l": 0.24629878869448185}
{"id": 39, "code": "def get conn ( self ) : if self . cosmos client is not None : return self . cosmos client self . cosmos client = cosmos client . Cosmos Client ( self . endpoint uri , { 'master Key' : self . master key } ) return self . cosmos client", "predictions": ["return a conn connection object for the passed vm 1 1 1 1 1 1 1 1 1 1 ."], "references": ["return a cosmos db client ."], "bleu": 0.08638804535733371, "rouge_l": 0.25558659217877094}
{"id": 40, "code": "def does collection exist ( self , collection name , database name = None ) : if collection name is None : raise Airflow Bad Request ( \"Collection name cannot be None.\" ) existing container = list ( self . get conn ( ) . Query Containers ( get database link ( self . get database name ( database name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection name } ] } ) ) if len ( existing container ) == 0 : return False return True", "predictions": ["check if a system system exists"], "references": ["checks if a collection exists in cosmosdb ."], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 41, "code": "def create collection ( self , collection name , database name = None ) : if collection name is None : raise Airflow Bad Request ( \"Collection name cannot be None.\" ) existing container = list ( self . get conn ( ) . Query Containers ( get database link ( self . get database name ( database name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection name } ] } ) ) if len ( existing container ) == 0 : self . get conn ( ) . Create Container ( get database link ( self . get database name ( database name ) ) , { \"id\" : collection name } )", "predictions": ["creates a vid vid vid"], "references": ["creates a new collection in the cosmosdb database ."], "bleu": 0.1614457444314309, "rouge_l": 0.2717149220489978}
{"id": 42, "code": "def does database exist ( self , database name ) : if database name is None : raise Airflow Bad Request ( \"Database name cannot be None.\" ) existing database = list ( self . get conn ( ) . Query Databases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database name } ] } ) ) if len ( existing database ) == 0 : return False return True", "predictions": ["check if a database text text text text text text text is already registered"], "references": ["checks if a database exists in cosmosdb ."], "bleu": 0.16451929399933107, "rouge_l": 0.28683385579937304}
{"id": 43, "code": "def create database ( self , database name ) : if database name is None : raise Airflow Bad Request ( \"Database name cannot be None.\" ) existing database = list ( self . get conn ( ) . Query Databases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database name } ] } ) ) if len ( existing database ) == 0 : self . get conn ( ) . Create Database ( { \"id\" : database name } )", "predictions": ["cbs the download download"], "references": ["creates a new database in cosmosdb ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 44, "code": "def delete database ( self , database name ) : if database name is None : raise Airflow Bad Request ( \"Database name cannot be None.\" ) self . get conn ( ) . Delete Database ( get database link ( database name ) )", "predictions": ["parse the host from the db ."], "references": ["deletes an existing database in cosmosdb ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 45, "code": "def delete collection ( self , collection name , database name = None ) : if collection name is None : raise Airflow Bad Request ( \"Collection name cannot be None.\" ) self . get conn ( ) . Delete Container ( get collection link ( self . get database name ( database name ) , collection name ) )", "predictions": ["get a conn from the conn ."], "references": ["deletes an existing collection in the cosmosdb database ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 46, "code": "def insert documents ( self , documents , database name = None , collection name = None ) : if documents is None : raise Airflow Bad Request ( \"You cannot insert empty documents\" ) created documents = [ ] for single document in documents : created documents . append ( self . get conn ( ) . Create Item ( get collection link ( self . get database name ( database name ) , self . get collection name ( collection name ) ) , single document ) ) return created documents", "predictions": ["inserts load load load load load load load load load load load load"], "references": ["insert a list of new documents into an existing collection in the cosmosdb database ."], "bleu": 0.06886905670533619, "rouge_l": 0.0}
{"id": 47, "code": "def delete document ( self , document id , database name = None , collection name = None ) : if document id is None : raise Airflow Bad Request ( \"Cannot delete a document without an id\" ) self . get conn ( ) . Delete Item ( get document link ( self . get database name ( database name ) , self . get collection name ( collection name ) , document id ) )", "predictions": ["deletes a dump from the collection ."], "references": ["delete an existing document out of a collection in the cosmosdb database ."], "bleu": 0.09912033646614596, "rouge_l": 0.2846034214618974}
{"id": 48, "code": "def get document ( self , document id , database name = None , collection name = None ) : if document id is None : raise Airflow Bad Request ( \"Cannot get a document without an id\" ) try : return self . get conn ( ) . Read Item ( get document link ( self . get database name ( database name ) , self . get collection name ( collection name ) , document id ) ) except HTTP Failure : return None", "predictions": ["gets a document by id"], "references": ["get a document from an existing collection in the cosmosdb database ."], "bleu": 0.08860330314183162, "rouge_l": 0.2190305206463196}
{"id": 49, "code": "def get documents ( self , sql string , database name = None , collection name = None , partition key = None ) : if sql string is None : raise Airflow Bad Request ( \"SQL query string cannot be None\" ) query = { 'query' : sql string } try : result iterable = self . get conn ( ) . Query Items ( get collection link ( self . get database name ( database name ) , self . get collection name ( collection name ) ) , query , partition key ) return list ( result iterable ) except HTTP Failure : return None", "predictions": ["returns the conn conn for the given sql"], "references": ["get a list of documents from an existing collection in the cosmosdb database via sql query ."], "bleu": 0.05761738913376924, "rouge_l": 0.1502463054187192}
{"id": 50, "code": "def get code ( dag id ) : session = settings . Session ( ) DM = models . Dag Model dag = session . query ( DM ) . filter ( DM . dag id == dag id ) . first ( ) session . close ( ) if dag is None : error message = \"Dag id {} not found\" . format ( dag id ) raise Dag Not Found ( error message ) try : with wwwutils . open maybe zipped ( dag . fileloc , 'r' ) as f : code = f . read ( ) return code except IO Error as e : error message = \"Error {} while reading Dag id {} Code\" . format ( str ( e ) , dag id ) raise Airflow Exception ( error message )", "predictions": ["bulk bulk bulk bulk bulk bulk bulk bulk bulk bulk conn"], "references": ["return python code of a given dag_id ."], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 51, "code": "def get conn ( self ) : conn = self . get connection ( self . vertica conn id ) conn config = { \"user\" : conn . login , \"password\" : conn . password or '' , \"database\" : conn . schema , \"host\" : conn . host or 'localhost' } if not conn . port : conn config [ \"port\" ] = 5433 else : conn config [ \"port\" ] = int ( conn . port ) conn = connect ( * * conn config ) return conn", "predictions": ["group connection to sqlite database ."], "references": ["returns verticaql connection object"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 52, "code": "def flush ( self ) : if len ( self . buffer ) > 0 : self . logger . log ( self . level , self . buffer ) self . buffer = str ( )", "predictions": ["cleanup the . ."], "references": ["ensure all logging output has been flushed"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 53, "code": "def start ( self ) : self . process = self . launch process ( self . dag directory , self . file paths , self . max runs , self . processor factory , self . child signal conn , self . stat queue , self . result queue , self . async mode ) self . log . info ( \"Launched Dag File Processor Manager with pid: %s\" , self . process . pid )", "predictions": ["reserve the reserved process self self self self self self self self self self self self self self self self self self self self self self self self self self self"], "references": ["launch dagfileprocessormanager processor and start dag parsing loop in manager ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 54, "code": "def exit gracefully ( self , signum , frame ) : self . log . info ( \"Exiting gracefully upon receiving signal %s\" , signum ) self . terminate ( ) self . end ( ) self . log . debug ( \"Finished terminating DAG processors.\" ) sys . exit ( os . EX OK )", "predictions": ["group was received try to group try to group try to group try to match ."], "references": ["helper method to clean up dag file processors to avoid leaving orphan processes ."], "bleu": 0.09147827112247602, "rouge_l": 0.20243362831858405}
{"id": 55, "code": "def start in async ( self ) : while True : loop start time = time . time ( ) if self . signal conn . poll ( ) : agent signal = self . signal conn . recv ( ) if agent signal == Dag Parsing Signal . TERMINATE MANAGER : self . terminate ( ) break elif agent signal == Dag Parsing Signal . END MANAGER : self . end ( ) sys . exit ( os . EX OK ) self . refresh dag dir ( ) simple dags = self . heartbeat ( ) for simple dag in simple dags : self . result queue . put ( simple dag ) self . print stat ( ) all files processed = all ( self . get last finish time ( x ) is not None for x in self . file paths ) max runs reached = self . max runs reached ( ) dag parsing stat = Dag Parsing Stat ( self . file paths , self . get all pids ( ) , max runs reached , all files processed , len ( simple dags ) ) self . stat queue . put ( dag parsing stat ) if max runs reached : self . log . info ( \"Exiting dag parsing loop as all files \" \"have been processed %s times\" , self . max runs ) break loop duration = time . time ( ) - loop start time if loop duration < 1 : sleep length = 1 - loop duration self . log . debug ( \"Sleeping for %.2f seconds to prevent excessive logging\" , sleep length ) time . sleep ( sleep length )", "predictions": ["integrate plugins plugins plugins"], "references": ["parse dag files repeatedly in a standalone loop ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 56, "code": "def refresh dag dir ( self ) : elapsed time since refresh = ( timezone . utcnow ( ) - self . last dag dir refresh time ) . total seconds ( ) if elapsed time since refresh > self . dag dir list interval : self . log . info ( \"Searching for files in %s\" , self . dag directory ) self . file paths = list py file paths ( self . dag directory ) self . last dag dir refresh time = timezone . utcnow ( ) self . log . info ( \"There are %s files in %s\" , len ( self . file paths ) , self . dag directory ) self . set file paths ( self . file paths ) try : self . log . debug ( \"Removing old import errors\" ) self . clear nonexistent import errors ( ) except Exception : self . log . exception ( \"Error removing old import errors\" )", "predictions": ["clear the dag runs"], "references": ["refresh file paths from dag dir if we haven t done it for too long ."], "bleu": 0.017888698387160718, "rouge_l": 0.09023668639053255}
{"id": 57, "code": "def print stat ( self ) : if ( ( timezone . utcnow ( ) - self . last stat print time ) . total seconds ( ) > self . print stats interval ) : if len ( self . file paths ) > 0 : self . log file processing stats ( self . file paths ) self . last stat print time = timezone . utcnow ( )", "predictions": ["clear the dag query time = 0 = 1 = 1 = 0 = 1 = 0 = 1 = 1 = 1 = 1 = 1 = 1 = 1"], "references": ["occasionally print out stats about how fast the files are getting processed"], "bleu": 0.03901663112717908, "rouge_l": 0.05053852526926264}
{"id": 58, "code": "def wait until finished ( self ) : for file path , processor in self . processors . items ( ) : while not processor . done : time . sleep ( 0.1 )", "predictions": ["set up all query query query query query query dms dms dms dms dms dms dms dms dms dms dms dms dms dms dms dms dms dms dms dms dms dms"], "references": ["sleeps until all the processors are done ."], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 59, "code": "def open slots ( self , session ) : from airflow . models . taskinstance import Task Instance as TI used slots = session . query ( func . count ( ) ) . filter ( TI . pool == self . pool ) . filter ( TI . state . in ( [ State . RUNNING , State . QUEUED ] ) ) . scalar ( ) return self . slots - used slots", "predictions": ["print the stats for the current dag . ."], "references": ["returns the number of slots open at the moment"], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 60, "code": "def run command ( command ) : process = subprocess . Popen ( shlex . split ( command ) , stdout = subprocess . PIPE , stderr = subprocess . PIPE , close fds = True ) output , stderr = [ stream . decode ( sys . getdefaultencoding ( ) , 'ignore' ) for stream in process . communicate ( ) ] if process . returncode != 0 : raise Airflow Config Exception ( \"Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}\" . format ( command , process . returncode , output , stderr ) ) return output", "predictions": ["heartbeat heartbeat command and return its output ."], "references": ["runs command and returns stdout"], "bleu": 0.21105340631872638, "rouge_l": 0.32105263157894737}
{"id": 61, "code": "def get task ( dag id , task id ) : dagbag = Dag Bag ( ) if dag id not in dagbag . dags : error message = \"Dag id {} not found\" . format ( dag id ) raise Dag Not Found ( error message ) dag = dagbag . get dag ( dag id ) if not dag . has task ( task id ) : error message = 'Task {} not found in dag {}' . format ( task id , dag id ) raise Task Not Found ( error message ) return dag . get task ( task id )", "predictions": ["return a state by state"], "references": ["return the task object identified by the given dag_id and task_id ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 62, "code": "def dispose orm ( ) : log . debug ( \"Disposing DB connection pool (PID %s)\" , os . getpid ( ) ) global engine global Session if Session : Session . remove ( ) Session = None if engine : engine . dispose ( ) engine = None", "predictions": ["cleanup all connections in the conn id"], "references": ["properly close pooled database connections"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 63, "code": "def prepare classpath ( ) : if DAGS FOLDER not in sys . path : sys . path . append ( DAGS FOLDER ) config path = os . path . join ( AIRFLOW HOME , 'config' ) if config path not in sys . path : sys . path . append ( config path ) if PLUGINS FOLDER not in sys . path : sys . path . append ( PLUGINS FOLDER )", "predictions": ["create session s session except it s not present ."], "references": ["ensures that certain subfolders of airflow_home are on the classpath"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 64, "code": "def alchemy to dict ( obj ) : if not obj : return None d = { } for c in obj . table . columns : value = getattr ( obj , c . name ) if type ( value ) == datetime : value = value . isoformat ( ) d [ c . name ] = value return d", "predictions": ["convert alchemy object to a dictionary ."], "references": ["transforms a sqlalchemy model instance into a dictionary"], "bleu": 0.21191828141393895, "rouge_l": 0.2634989200863931}
{"id": 65, "code": "def chunks ( items , chunk size ) : if chunk size <= 0 : raise Value Error ( 'Chunk size must be a positive integer' ) for i in range ( 0 , len ( items ) , chunk size ) : yield items [ i : i + chunk size ]", "predictions": ["yield successive n - sized chunks from items ."], "references": ["yield successive chunks of a given size from a list of items"], "bleu": 0.158278836853973, "rouge_l": 0.4642313546423136}
{"id": 66, "code": "def get task instance ( dag id , task id , execution date ) : dagbag = Dag Bag ( ) if dag id not in dagbag . dags : error message = \"Dag id {} not found\" . format ( dag id ) raise Dag Not Found ( error message ) dag = dagbag . get dag ( dag id ) if not dag . has task ( task id ) : error message = 'Task {} not found in dag {}' . format ( task id , dag id ) raise Task Not Found ( error message ) dagrun = dag . get dagrun ( execution date = execution date ) if not dagrun : error message = ( 'Dag Run for date {} not found in dag {}' . format ( execution date , dag id ) ) raise Dag Run Not Found ( error message ) task instance = dagrun . get task instance ( task id ) if not task instance : error message = ( 'Task {} instance for date {} not found' . format ( task id , execution date ) ) raise Task Instance Not Found ( error message ) return task instance", "predictions": ["return a task instance based on the dag id"], "references": ["return the task object identified by the given dag_id and task_id ."], "bleu": 0.12026590852507517, "rouge_l": 0.2785388127853881}
{"id": 67, "code": "def integrate plugins ( ) : import sys from airflow . plugins manager import operators modules for operators module in operators modules : sys . modules [ operators module . name ] = operators module globals ( ) [ operators module . name ] = operators module", "predictions": ["integrate all plugins in the operators"], "references": ["integrate plugins to the context"], "bleu": 0.2626909894424158, "rouge_l": 0.5545454545454546}
{"id": 68, "code": "def get conn ( self ) : http authorized = self . authorize ( ) return build ( 'dataproc' , self . api version , http = http authorized , cache discovery = False )", "predictions": ["returns a connection object for the passed vm data ."], "references": ["returns a google cloud dataproc service object ."], "bleu": 0.18850319022747347, "rouge_l": 0.4535315985130111}
{"id": 69, "code": "def wait ( self , operation ) : submitted = Data Proc Operation ( self . get conn ( ) , operation , self . num retries ) submitted . wait for done ( )", "predictions": ["wait for the operation to be sent ."], "references": ["awaits for google cloud dataproc operation to complete ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 70, "code": "def get conn ( self ) : authed http = self . authorize ( ) return build ( 'ml' , 'v1' , http = authed http , cache discovery = False )", "predictions": ["returns a connection object for the specified transformation ."], "references": ["returns a google mlengine service object ."], "bleu": 0.21105340631872635, "rouge_l": 0.5115303983228512}
{"id": 71, "code": "def set default version ( self , project id , model name , version name ) : full version name = 'projects/{}/models/{}/versions/{}' . format ( project id , model name , version name ) request = self . mlengine . projects ( ) . models ( ) . versions ( ) . set Default ( name = full version name , body = { } ) try : response = request . execute ( ) self . log . info ( 'Successfully set version: %s to default' , response ) return response except Http Error as e : self . log . error ( 'Something went wrong: %s' , e ) raise", "predictions": ["set the default version of the given model ."], "references": ["sets a version to be the default . blocks until finished ."], "bleu": 0.15122637383061946, "rouge_l": 0.2785388127853881}
{"id": 72, "code": "def list versions ( self , project id , model name ) : result = [ ] full parent name = 'projects/{}/models/{}' . format ( project id , model name ) request = self . mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full parent name , page Size = 100 ) response = request . execute ( ) next page token = response . get ( 'next Page Token' , None ) result . extend ( response . get ( 'versions' , [ ] ) ) while next page token is not None : next request = self . mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full parent name , page Token = next page token , page Size = 100 ) response = next request . execute ( ) next page token = response . get ( 'next Page Token' , None ) result . extend ( response . get ( 'versions' , [ ] ) ) time . sleep ( 5 ) return result", "predictions": ["list all versions of a project ."], "references": ["lists all available versions of a model . blocks until finished ."], "bleu": 0.1873000789958672, "rouge_l": 0.5024711696869852}
{"id": 73, "code": "def delete version ( self , project id , model name , version name ) : full name = 'projects/{}/models/{}/versions/{}' . format ( project id , model name , version name ) delete request = self . mlengine . projects ( ) . models ( ) . versions ( ) . delete ( name = full name ) response = delete request . execute ( ) get request = self . mlengine . projects ( ) . operations ( ) . get ( name = response [ 'name' ] ) return poll with exponential delay ( request = get request , max n = 9 , is done func = lambda resp : resp . get ( 'done' , False ) , is error func = lambda resp : resp . get ( 'error' , None ) is not None )", "predictions": ["delete a poll version of a project ."], "references": ["deletes the given version of a model . blocks until finished ."], "bleu": 0.19142013845510458, "rouge_l": 0.3860759493670886}
{"id": 74, "code": "def create model ( self , project id , model ) : if not model [ 'name' ] : raise Value Error ( \"Model name must be provided and \" \"could not be an empty string\" ) project = 'projects/{}' . format ( project id ) request = self . mlengine . projects ( ) . models ( ) . create ( parent = project , body = model ) return request . execute ( )", "predictions": ["creates a new model ."], "references": ["create a model . blocks until finished ."], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 75, "code": "def get model ( self , project id , model name ) : if not model name : raise Value Error ( \"Model name must be provided and \" \"it could not be an empty string\" ) full model name = 'projects/{}/models/{}' . format ( project id , model name ) request = self . mlengine . projects ( ) . models ( ) . get ( name = full model name ) try : return request . execute ( ) except Http Error as e : if e . resp . status == 404 : self . log . error ( 'Model was not found: %s' , e ) return None raise", "predictions": ["get a model by name ."], "references": ["gets a model . blocks until finished ."], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 76, "code": "def write batch data ( self , items ) : dynamodb conn = self . get conn ( ) try : table = dynamodb conn . Table ( self . table name ) with table . batch writer ( overwrite by pkeys = self . table keys ) as batch : for item in items : batch . put item ( Item = item ) return True except Exception as general error : raise Airflow Exception ( 'Failed to insert items in dynamodb, error: {error}' . format ( error = str ( general error ) ) )", "predictions": ["writes batch data to the database ."], "references": ["write batch items to dynamodb table with provisioned throughout capacity ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 77, "code": "def integrate plugins ( ) : from airflow . plugins manager import executors modules for executors module in executors modules : sys . modules [ executors module . name ] = executors module globals ( ) [ executors module . name ] = executors module", "predictions": ["integrate all plugins in the airflow module ."], "references": ["integrate plugins to the context ."], "bleu": 0.20164945583740668, "rouge_l": 0.5865384615384615}
{"id": 78, "code": "def get default executor ( ) : global DEFAULT EXECUTOR if DEFAULT EXECUTOR is not None : return DEFAULT EXECUTOR executor name = configuration . conf . get ( 'core' , 'EXECUTOR' ) DEFAULT EXECUTOR = get executor ( executor name ) log = Logging Mixin ( ) . log log . info ( \"Using executor %s\" , executor name ) return DEFAULT EXECUTOR", "predictions": ["returns the default executor executor for the current executor ."], "references": ["creates a new instance of the configured executor if none exists and returns it"], "bleu": 0.1004883949864497, "rouge_l": 0.16180371352785147}
{"id": 79, "code": "def on error ( self , error , items ) : self . log . error ( 'Encountered Segment error: {segment error} with ' 'items: {with items}' . format ( segment error = error , with items = items ) ) raise Airflow Exception ( 'Segment error: {}' . format ( error ) )", "predictions": ["called by the error when an error occurs ."], "references": ["handles error callbacks when using segment with segment_debug_mode set to true"], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 80, "code": "def get conn ( self ) : conn = self . get connection ( self . mssql conn id ) conn = pymssql . connect ( server = conn . host , user = conn . login , password = conn . password , database = self . schema or conn . schema , port = conn . port ) return conn", "predictions": ["return a conn object for the passed vm data ."], "references": ["returns a mssql connection object"], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 81, "code": "def execute ( self , context ) : self . hook = Spark Submit Hook ( conf = self . conf , conn id = self . conn id , files = self . files , py files = self . py files , archives = self . archives , driver class path = self . driver class path , jars = self . jars , java class = self . java class , packages = self . packages , exclude packages = self . exclude packages , repositories = self . repositories , total executor cores = self . total executor cores , executor cores = self . executor cores , executor memory = self . executor memory , driver memory = self . driver memory , keytab = self . keytab , principal = self . principal , name = self . name , num executors = self . num executors , application args = self . application args , env vars = self . env vars , verbose = self . verbose , spark binary = self . spark binary ) self . hook . submit ( self . application )", "predictions": ["execute the command ."], "references": ["call the sparksubmithook to run the provided spark job"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 82, "code": "def delete dag ( dag id ) : try : count = delete . delete dag ( dag id ) except Airflow Exception as err : log . error ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status code = err . status code return response return jsonify ( message = \"Removed {} record(s)\" . format ( count ) , count = count )", "predictions": ["delete a dag by id ."], "references": ["delete all db records related to the specified dag ."], "bleu": 0.13487005099534619, "rouge_l": 0.3588235294117647}
{"id": 83, "code": "def get dag code ( dag id ) : try : return get code ( dag id ) except Airflow Exception as err : log . info ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status code = err . status code return response", "predictions": ["get the dag code by id"], "references": ["return python code of a given dag_id ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 84, "code": "def task info ( dag id , task id ) : try : info = get task ( dag id , task id ) except Airflow Exception as err : log . info ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status code = err . status code return response fields = { k : str ( v ) for k , v in vars ( info ) . items ( ) if not k . startswith ( ' ' ) } return jsonify ( fields )", "predictions": ["return information about a task"], "references": ["returns a json with a task s public instance variables ."], "bleu": 0.10822031883953476, "rouge_l": 0.2341650671785029}
{"id": 85, "code": "def get pools ( ) : try : pools = pool api . get pools ( ) except Airflow Exception as err : log . error ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status code = err . status code return response else : return jsonify ( [ p . to json ( ) for p in pools ] )", "predictions": ["get the pools from the pool"], "references": ["get all pools ."], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 86, "code": "def create pool ( ) : params = request . get json ( force = True ) try : pool = pool api . create pool ( * * params ) except Airflow Exception as err : log . error ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status code = err . status code return response else : return jsonify ( pool . to json ( ) )", "predictions": ["create a pool ."], "references": ["create a pool ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 87, "code": "def get task instances ( self , state = None , session = None ) : from airflow . models . taskinstance import Task Instance tis = session . query ( Task Instance ) . filter ( Task Instance . dag id == self . dag id , Task Instance . execution date == self . execution date , ) if state : if isinstance ( state , six . string types ) : tis = tis . filter ( Task Instance . state == state ) else : if None in state : tis = tis . filter ( or ( Task Instance . state . in ( state ) , Task Instance . state . is ( None ) ) ) else : tis = tis . filter ( Task Instance . state . in ( state ) ) if self . dag and self . dag . partial : tis = tis . filter ( Task Instance . task id . in ( self . dag . task ids ) ) return tis . all ( )", "predictions": ["return all instances of this task ."], "references": ["returns the task instances for this dag run"], "bleu": 0.19148978368719022, "rouge_l": 0.2634989200863931}
{"id": 88, "code": "def get previous dagrun ( self , session = None ) : return session . query ( Dag Run ) . filter ( Dag Run . dag id == self . dag id , Dag Run . execution date < self . execution date ) . order by ( Dag Run . execution date . desc ( ) ) . first ( )", "predictions": ["return the previous dagrun dagrun ."], "references": ["the previous dagrun if there is one"], "bleu": 0.34801709319446883, "rouge_l": 0.45522388059701485}
{"id": 89, "code": "def get previous scheduled dagrun ( self , session = None ) : dag = self . get dag ( ) return session . query ( Dag Run ) . filter ( Dag Run . dag id == self . dag id , Dag Run . execution date == dag . previous schedule ( self . execution date ) ) . first ( )", "predictions": ["get all previous scheduled schedule for the dag"], "references": ["the previous scheduled dagrun if there is one"], "bleu": 0.22679164443904004, "rouge_l": 0.25}
{"id": 90, "code": "def conditionally trigger ( context , dag run obj ) : c p = context [ 'params' ] [ 'condition param' ] print ( \"Controller DAG : conditionally trigger = {}\" . format ( c p ) ) if context [ 'params' ] [ 'condition param' ] : dag run obj . payload = { 'message' : context [ 'params' ] [ 'message' ] } pp . pprint ( dag run obj . payload ) return dag run obj", "predictions": ["trigger the dag for the dag run ."], "references": ["this function decides whether or not to trigger the remote dag"], "bleu": 0.15587146574232644, "rouge_l": 0.3070469798657718}
{"id": 91, "code": "def get dag ( self , dag id ) : from airflow . models . dag import Dag Model root dag id = dag id if dag id in self . dags : dag = self . dags [ dag id ] if dag . is subdag : root dag id = dag . parent dag . dag id orm dag = Dag Model . get current ( root dag id ) if orm dag and ( root dag id not in self . dags or ( orm dag . last expired and dag . last loaded < orm dag . last expired ) ) : found dags = self . process file ( filepath = orm dag . fileloc , only if updated = False ) if found dags and dag id in [ found dag . dag id for found dag in found dags ] : return self . dags [ dag id ] elif dag id in self . dags : del self . dags [ dag id ] return self . dags . get ( dag id )", "predictions": ["get dag for dag id"], "references": ["gets the dag out of the dictionary and refreshes it if expired"], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 92, "code": "def dagbag report ( self ) : report = textwrap . dedent ( ) stats = self . dagbag stats return report . format ( dag folder = self . dag folder , duration = sum ( [ o . duration for o in stats ] ) , dag num = sum ( [ o . dag num for o in stats ] ) , task num = sum ( [ o . task num for o in stats ] ) , table = pprinttable ( stats ) , )", "predictions": ["report the dagbag report"], "references": ["prints a report around dagbag loading stats"], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 93, "code": "def execute ( self , context ) : self . hook = Spark JDBC Hook ( spark app name = self . spark app name , spark conn id = self . spark conn id , spark conf = self . spark conf , spark py files = self . spark py files , spark files = self . spark files , spark jars = self . spark jars , num executors = self . num executors , executor cores = self . executor cores , executor memory = self . executor memory , driver memory = self . driver memory , verbose = self . verbose , keytab = self . keytab , principal = self . principal , cmd type = self . cmd type , jdbc table = self . jdbc table , jdbc conn id = self . jdbc conn id , jdbc driver = self . jdbc driver , metastore table = self . metastore table , jdbc truncate = self . jdbc truncate , save mode = self . save mode , save format = self . save format , batch size = self . batch size , fetch size = self . fetch size , num partitions = self . num partitions , partition column = self . partition column , lower bound = self . lower bound , upper bound = self . upper bound , create table column types = self . create table column types ) self . hook . submit jdbc job ( )", "predictions": ["execute the spark job ."], "references": ["call the sparksubmithook to run the provided spark job"], "bleu": 0.17348474258688365, "rouge_l": 0.40757238307349664}
{"id": 94, "code": "def integrate plugins ( ) : import sys from airflow . plugins manager import macros modules for macros module in macros modules : sys . modules [ macros module . name ] = macros module globals ( ) [ macros module . name ] = macros module", "predictions": ["integrate all macros modules ."], "references": ["integrate plugins to the context"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 95, "code": "def error ( self , session = None ) : self . log . error ( \"Recording the task instance as FAILED\" ) self . state = State . FAILED session . merge ( self ) session . commit ( )", "predictions": ["merge the task instance ."], "references": ["forces the task instance s state to failed in the database ."], "bleu": 0.13249284136531225, "rouge_l": 0.4380610412926392}
{"id": 96, "code": "def clear xcom data ( self , session = None ) : session . query ( X Com ) . filter ( X Com . dag id == self . dag id , X Com . task id == self . task id , X Com . execution date == self . execution date ) . delete ( ) session . commit ( )", "predictions": ["clears all to the dict of to be added to the c"], "references": ["clears all xcom data from the database for the task instance"], "bleu": 0.15537125692760353, "rouge_l": 0.3505747126436781}
{"id": 97, "code": "def key ( self ) : return self . dag id , self . task id , self . execution date , self . try number", "predictions": ["size of the dag . . ."], "references": ["returns a tuple that identifies the task instance uniquely"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 98, "code": "def init run context ( self , raw = False ) : self . raw = raw self . set context ( self )", "predictions": ["set up the instance of the instance of the instance ."], "references": ["sets the log context ."], "bleu": 0.12605968092174913, "rouge_l": 0.2681318681318681}
{"id": 99, "code": "def close ( self ) : if self . closed : return super ( ) . close ( ) if not self . upload on close : return local loc = os . path . join ( self . local base , self . log relative path ) remote loc = os . path . join ( self . remote base , self . log relative path ) if os . path . exists ( local loc ) : with open ( local loc , 'r' ) as logfile : log = logfile . read ( ) self . wasb write ( log , remote loc , append = True ) if self . delete local copy : shutil . rmtree ( os . path . dirname ( local loc ) ) self . closed = True", "predictions": ["integrate the file and integrate the for this file sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys sys"], "references": ["close and upload local log file to remote storage wasb ."], "bleu": 0.04317900023606586, "rouge_l": 0.10418445772843724}
{"id": 100, "code": "def query cassandra ( self ) : self . hook = Cassandra Hook ( cassandra conn id = self . cassandra conn id ) session = self . hook . get conn ( ) cursor = session . execute ( self . cql ) return cursor", "predictions": ["get conn api api api api api"], "references": ["queries cassandra and returns a cursor to the results ."], "bleu": 0.10175282441454783, "rouge_l": 0.0}
{"id": 101, "code": "def execute ( self , context ) : self . hook = Spark Sql Hook ( sql = self . sql , conf = self . conf , conn id = self . conn id , total executor cores = self . total executor cores , executor cores = self . executor cores , executor memory = self . executor memory , keytab = self . keytab , principal = self . principal , name = self . name , num executors = self . num executors , master = self . master , yarn queue = self . yarn queue ) self . hook . run query ( )", "predictions": ["wait for the given operation to be executed ."], "references": ["call the sparksqlhook to run the provided sql query"], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 102, "code": "def get conn ( self ) : conn = self . get connection ( self . conn id ) service options = conn . extra dejson self . account name = service options . get ( 'account name' ) adl Creds = lib . auth ( tenant id = service options . get ( 'tenant' ) , client secret = conn . password , client id = conn . login ) adls File System Client = core . Azure DL File System ( adl Creds , store name = self . account name ) adls File System Client . connect ( ) return adls File System Client", "predictions": ["get a connection to the mysql database http connection http http connection http account"], "references": ["return a azuredlfilesystem object ."], "bleu": 0.08839374326825923, "rouge_l": 0.11509433962264153}
{"id": 103, "code": "def execute ( self , context ) : self . hook = self . get hook ( ) self . hook . get conn ( ) self . query execution context [ 'Database' ] = self . database self . result configuration [ 'Output Location' ] = self . output location self . query execution id = self . hook . run query ( self . query , self . query execution context , self . result configuration , self . client request token ) query status = self . hook . poll query status ( self . query execution id , self . max tries ) if query status in AWS Athena Hook . FAILURE STATES : raise Exception ( 'Final state of Athena job is {}, query execution id is {}.' . format ( query status , self . query execution id ) ) elif not query status or query status in AWS Athena Hook . INTERMEDIATE STATES : raise Exception ( 'Final state of Athena job is {}. ' 'Max tries of poll status exceeded, query execution id is {}.' . format ( query status , self . query execution id ) )", "predictions": ["set up the full full full full full full full full full full full full full full full log id id"], "references": ["run presto query on athena"], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 104, "code": "def on kill ( self ) : if self . query execution id : self . log . info ( '\u26b0\ufe0f\u26b0\ufe0f\u26b0\ufe0f Received a kill Signal. Time to Die') self . log . info ( 'Stopping Query with execution Id - %s' , self . query execution id ) response = self . hook . stop query ( self . query execution id ) http status code = None try : http status code = response [ 'Response Metadata' ] [ 'HTTP Status Code' ] except Exception as ex : self . log . error ( 'Exception while cancelling query' , ex ) finally : if http status code is None or http status code != 200 : self . log . error ( 'Unable to request query cancel on athena. Exiting' ) else : self . log . info ( 'Polling Athena for query with id %s to reach final state' , self . query execution id ) self . hook . poll query status ( self . query execution id )", "predictions": ["handle versions of versions of the versions of the name model model model model model model model model model model model model model model model model model model model model model"], "references": ["cancel the submitted athena query"], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 105, "code": "def uncompress file ( input file name , file extension , dest dir ) : if file extension . lower ( ) not in ( '.gz' , '.bz2' ) : raise Not Implemented Error ( \"Received {} format. Only gz and bz2 \" \"files can currently be uncompressed.\" . format ( file extension ) ) if file extension . lower ( ) == '.gz' : fmodule = gzip . Gzip File elif file extension . lower ( ) == '.bz2' : fmodule = bz2 . BZ2File with fmodule ( input file name , mode = 'rb' ) as f compressed , Named Temporary File ( dir = dest dir , mode = 'wb' , delete = False ) as f uncompressed : shutil . copyfileobj ( f compressed , f uncompressed ) return f uncompressed . name", "predictions": ["check if version exists and delete the delete file"], "references": ["uncompress gz and bz2 files"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 106, "code": "def get conn ( self ) : if not self . conn : connection = self . get connection ( self . conn id ) extras = connection . extra dejson self . conn = Salesforce ( username = connection . login , password = connection . password , security token = extras [ 'security token' ] , instance url = connection . host , sandbox = extras . get ( 'sandbox' , False ) ) return self . conn", "predictions": ["create a connection to use with passed information"], "references": ["sign into salesforce only if we are not already signed in ."], "bleu": 0.08179133792443427, "rouge_l": 0.0}
{"id": 107, "code": "def put records ( self , records ) : firehose conn = self . get conn ( ) response = firehose conn . put record batch ( Delivery Stream Name = self . delivery stream , Records = records ) return response", "predictions": ["upload model and store results to database"], "references": ["write batch records to kinesis firehose"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 108, "code": "def send email ( to , subject , html content , files = None , dryrun = False , cc = None , bcc = None , mime subtype = 'mixed' , mime charset = 'utf-8' , * * kwargs ) : path , attr = configuration . conf . get ( 'email' , 'EMAIL BACKEND' ) . rsplit ( '.' , 1 ) module = importlib . import module ( path ) backend = getattr ( module , attr ) to = get email address list ( to ) to = \", \" . join ( to ) return backend ( to , subject , html content , files = files , dryrun = dryrun , cc = cc , bcc = bcc , mime subtype = mime subtype , mime charset = mime charset , * * kwargs )", "predictions": ["write an batch of the batch to an batch"], "references": ["send email using backend specified in email_backend ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 109, "code": "def get conn ( self ) : if self . conn is None : params = self . get connection ( self . ftp conn id ) pasv = params . extra dejson . get ( \"passive\" , True ) self . conn = ftplib . FTP ( params . host , params . login , params . password ) self . conn . set pasv ( pasv ) return self . conn", "predictions": ["establish a connection to module"], "references": ["returns a ftp connection object"], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 110, "code": "def execute ( self , context ) : self . hook = Discord Webhook Hook ( self . http conn id , self . webhook endpoint , self . message , self . username , self . avatar url , self . tts , self . proxy ) self . hook . execute ( )", "predictions": ["executes the given context"], "references": ["call the discordwebhookhook to post message"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 111, "code": "def get conn ( self ) : conn = self . get connection ( self . conn id ) service options = conn . extra dejson return File Service ( account name = conn . login , account key = conn . password , * * service options )", "predictions": ["return a connection to a broker"], "references": ["return the fileservice object ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 112, "code": "def get conn ( self ) : if not self . conn : self . conn = storage . Client ( credentials = self . get credentials ( ) ) return self . conn", "predictions": ["server should be a connection or a connection to the remote host connection connection"], "references": ["returns a google cloud storage service object ."], "bleu": 0.08839374326825923, "rouge_l": 0.09561128526645768}
{"id": 113, "code": "def describe training job with log ( self , job name , positions , stream names , instance count , state , last description , last describe job call ) : log group = '/aws/sagemaker/Training Jobs' if len ( stream names ) < instance count : logs conn = self . get log conn ( ) try : streams = logs conn . describe log streams ( log Group Name = log group , log Stream Name Prefix = job name + '/' , order By = 'Log Stream Name' , limit = instance count ) stream names = [ s [ 'log Stream Name' ] for s in streams [ 'log Streams' ] ] positions . update ( [ ( s , Position ( timestamp = 0 , skip = 0 ) ) for s in stream names if s not in positions ] ) except logs conn . exceptions . Resource Not Found Exception : pass if len ( stream names ) > 0 : for idx , event in self . multi stream iter ( log group , stream names , positions ) : self . log . info ( event [ 'message' ] ) ts , count = positions [ stream names [ idx ] ] if event [ 'timestamp' ] == ts : positions [ stream names [ idx ] ] = Position ( timestamp = ts , skip = count + 1 ) else : positions [ stream names [ idx ] ] = Position ( timestamp = event [ 'timestamp' ] , skip = 1 ) if state == Log State . COMPLETE : return state , last description , last describe job call if state == Log State . JOB COMPLETE : state = Log State . COMPLETE elif time . time ( ) - last describe job call >= 30 : description = self . describe training job ( job name ) last describe job call = time . time ( ) if secondary training status changed ( description , last description ) : self . log . info ( secondary training status message ( description , last description ) ) last description = description status = description [ 'Training Job Status' ] if status not in self . non terminal states : state = Log State . JOB COMPLETE return state , last description , last describe job call", "predictions": ["execute a training with"], "references": ["return the training job info associated with job_name and print cloudwatch logs"], "bleu": 0.0538140946637381, "rouge_l": 0.22932330827067668}
{"id": 114, "code": "def execute ( self , context ) : bucket helper = Google Cloud Bucket Helper ( self . gcp conn id , self . delegate to ) self . py file = bucket helper . google cloud to local ( self . py file ) hook = Data Flow Hook ( gcp conn id = self . gcp conn id , delegate to = self . delegate to , poll sleep = self . poll sleep ) dataflow options = self . dataflow default options . copy ( ) dataflow options . update ( self . options ) camel to snake = lambda name : re . sub ( r'[A-Z]' , lambda x : ' ' + x . group ( 0 ) . lower ( ) , name ) formatted options = { camel to snake ( key ) : dataflow options [ key ] for key in dataflow options } hook . start python dataflow ( self . job name , formatted options , self . py file , self . py options )", "predictions": ["delete all the return return in the return return information ."], "references": ["execute the python dataflow job ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 115, "code": "def prepare cli cmd ( self ) : conn = self . conn hive bin = 'hive' cmd extra = [ ] if self . use beeline : hive bin = 'beeline' jdbc url = \"jdbc:hive2://{host}:{port}/{schema}\" . format ( host = conn . host , port = conn . port , schema = conn . schema ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : template = conn . extra dejson . get ( 'principal' , \"hive/ HOST@EXAMPLE.COM\" ) if \" HOST\" in template : template = utils . replace hostname pattern ( utils . get components ( template ) ) proxy user = \"\" if conn . extra dejson . get ( 'proxy user' ) == \"login\" and conn . login : proxy user = \"hive.server2.proxy.user={0}\" . format ( conn . login ) elif conn . extra dejson . get ( 'proxy user' ) == \"owner\" and self . run as : proxy user = \"hive.server2.proxy.user={0}\" . format ( self . run as ) jdbc url += \";principal={template};{proxy user}\" . format ( template = template , proxy user = proxy user ) elif self . auth : jdbc url += \";auth=\" + self . auth jdbc url = '\"{}\"' . format ( jdbc url ) cmd extra += [ '-u' , jdbc url ] if conn . login : cmd extra += [ '-n' , conn . login ] if conn . password : cmd extra += [ '-p' , conn . password ] hive params list = self . hive cli params . split ( ) return [ hive bin ] + cmd extra + hive params list", "predictions": ["get the code to get the data from the dag"], "references": ["this function creates the command list from available information"], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 116, "code": "def get metastore client ( self ) : import hmsclient from thrift . transport import T Socket , T Transport from thrift . protocol import T Binary Protocol ms = self . metastore conn auth mechanism = ms . extra dejson . get ( 'auth Mechanism' , 'NOSASL' ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : auth mechanism = ms . extra dejson . get ( 'auth Mechanism' , 'GSSAPI' ) kerberos service name = ms . extra dejson . get ( 'kerberos service name' , 'hive' ) socket = T Socket . T Socket ( ms . host , ms . port ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' and auth mechanism == 'GSSAPI' : try : import saslwrapper as sasl except Import Error : import sasl def sasl factory ( ) : sasl client = sasl . Client ( ) sasl client . set Attr ( \"host\" , ms . host ) sasl client . set Attr ( \"service\" , kerberos service name ) sasl client . init ( ) return sasl client from thrift sasl import T Sasl Client Transport transport = T Sasl Client Transport ( sasl factory , \"GSSAPI\" , socket ) else : transport = T Transport . T Buffered Transport ( socket ) protocol = T Binary Protocol . T Binary Protocol ( transport ) return hmsclient . HMS Client ( iprot = protocol )", "predictions": ["create a info from the info about the info except it exists ."], "references": ["returns a hive thrift client ."], "bleu": 0.10571070857151538, "rouge_l": 0.22550831792975967}
{"id": 117, "code": "def get tables ( self , db , pattern = '*' ) : with self . metastore as client : tables = client . get tables ( db name = db , pattern = pattern ) return client . get table objects by name ( db , tables )", "predictions": ["get all pools objects for a given = = 0 . . . . ."], "references": ["get a metastore table object"], "bleu": 0.09103526405546068, "rouge_l": 0.21981981981981977}
{"id": 118, "code": "def get conn ( self , schema = None ) : db = self . get connection ( self . hiveserver2 conn id ) auth mechanism = db . extra dejson . get ( 'auth Mechanism' , 'NONE' ) if auth mechanism == 'NONE' and db . login is None : username = 'airflow' kerberos service name = None if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : auth mechanism = db . extra dejson . get ( 'auth Mechanism' , 'KERBEROS' ) kerberos service name = db . extra dejson . get ( 'kerberos service name' , 'hive' ) if auth mechanism == 'GSSAPI' : self . log . warning ( \"Detected deprecated 'GSSAPI' for auth Mechanism \" \"for %s. Please use 'KERBEROS' instead\" , self . hiveserver2 conn id ) auth mechanism = 'KERBEROS' from pyhive . hive import connect return connect ( host = db . host , port = db . port , auth = auth mechanism , kerberos service name = kerberos service name , username = db . login or username , password = db . password , database = schema or db . schema or 'default' )", "predictions": ["create a connection to the mysql database"], "references": ["returns a hive connection object ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 119, "code": "def get endpoint ( self ) : conn = self . get connection ( self . http conn id ) token = conn . password if not token : raise Airflow Exception ( 'Dingding token is requests but get nothing, ' 'check you conn id configuration.' ) return 'robot/send?access token={}' . format ( token )", "predictions": ["get an task from the client"], "references": ["get dingding endpoint for sending message ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 120, "code": "def bind parameters ( operation , parameters ) : string parameters = { } for ( name , value ) in iteritems ( parameters ) : if value is None : string parameters [ name ] = 'NULL' elif isinstance ( value , basestring ) : string parameters [ name ] = \"'\" + escape ( value ) + \"'\" else : string parameters [ name ] = str ( value ) return operation % string parameters", "predictions": ["get previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous"], "references": ["helper method that binds parameters to a sql query ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 121, "code": "def escape ( s ) : e = s e = e . replace ( '\\\\' , '\\\\\\\\' ) e = e . replace ( '\\n' , '\\\\n' ) e = e . replace ( '\\r' , '\\\\r' ) e = e . replace ( \"'\" , \"\\\\'\" ) e = e . replace ( '\"' , '\\\\\"' ) return e", "predictions": ["get a string from a string session session session session session session session session session session session session session session session"], "references": ["helper method that escapes parameters to a sql query ."], "bleu": 0.05809665204409193, "rouge_l": 0.06892655367231638}
{"id": 122, "code": "def get conn ( self ) : service = self . get service ( ) project = self . get field ( 'project' ) return Big Query Connection ( service = service , project id = project , use legacy sql = self . use legacy sql , location = self . location , num retries = self . num retries )", "predictions": ["return a trigger object for the passed vm data . ."], "references": ["returns a bigquery pep 249 connection object ."], "bleu": 0.1354599427337814, "rouge_l": 0.32504440497335696}
{"id": 123, "code": "def get service ( self ) : http authorized = self . authorize ( ) return build ( 'bigquery' , 'v2' , http = http authorized , cache discovery = False )", "predictions": ["returns the dag object for the current dag airflow airflow airflow ."], "references": ["returns a bigquery service object ."], "bleu": 0.1235622127262679, "rouge_l": 0.3546511627906977}
{"id": 124, "code": "def cancel query ( self ) : jobs = self . service . jobs ( ) if ( self . running job id and not self . poll job complete ( self . running job id ) ) : self . log . info ( 'Attempting to cancel job : %s, %s' , self . project id , self . running job id ) if self . location : jobs . cancel ( project Id = self . project id , job Id = self . running job id , location = self . location ) . execute ( num retries = self . num retries ) else : jobs . cancel ( project Id = self . project id , job Id = self . running job id ) . execute ( num retries = self . num retries ) else : self . log . info ( 'No running Big Query jobs to cancel.' ) return max polling attempts = 12 polling attempts = 0 job complete = False while polling attempts < max polling attempts and not job complete : polling attempts = polling attempts + 1 job complete = self . poll job complete ( self . running job id ) if job complete : self . log . info ( 'Job successfully canceled: %s, %s' , self . project id , self . running job id ) elif polling attempts == max polling attempts : self . log . info ( \"Stopping polling due to timeout. Job with id %s \" \"has not completed cancel and may or may not finish.\" , self . running job id ) else : self . log . info ( 'Waiting for canceled job with id %s to finish.' , self . running job id ) time . sleep ( 5 )", "predictions": ["cancel the report of the report ."], "references": ["cancel all started queries that have not yet completed"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 125, "code": "def query postgres ( self ) : postgres = Postgres Hook ( postgres conn id = self . postgres conn id ) conn = postgres . get conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql , self . parameters ) return cursor", "predictions": ["conf the postgres spark spark table"], "references": ["queries postgres and returns a cursor to the results ."], "bleu": 0.1255107248036171, "rouge_l": 0.11960784313725491}
{"id": 126, "code": "def integrate plugins ( ) : from airflow . plugins manager import hooks modules for hooks module in hooks modules : sys . modules [ hooks module . name ] = hooks module globals ( ) [ hooks module . name ] = hooks module", "predictions": ["integrate all manager macros from the sys from the sys from the sys from the sys from the for the sys from the for all macros from the macros from the"], "references": ["integrate plugins to the context"], "bleu": 0.04317900023606586, "rouge_l": 0.12774869109947642}
{"id": 127, "code": "def on finish ( self ) : if self . cfg path and os . path . isfile ( self . cfg path ) : if self . run as user : subprocess . call ( [ 'sudo' , 'rm' , self . cfg path ] , close fds = True ) else : os . remove ( self . cfg path )", "predictions": ["remove up the config file . ."], "references": ["a callback that should be called when this is done running ."], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 128, "code": "def main ( ) : usage = \"usage: nvd3.py [options]\" parser = Option Parser ( usage = usage , version = ( \"python-nvd3 - Charts generator with \" \"nvd3.js and d3.js\" ) ) parser . add option ( \"-q\" , \"--quiet\" , action = \"store false\" , dest = \"verbose\" , default = True , help = \"don't print messages to stdout\" ) ( options , args ) = parser . parse args ( )", "predictions": ["main function for command line usage"], "references": ["parse options and process commands"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 129, "code": "def buildhtmlheader ( self ) : self . htmlheader = '' global js initialized if ' js initialized' not in globals ( ) or not js initialized : for css in self . header css : self . htmlheader += css for js in self . header js : self . htmlheader += js", "predictions": ["write the header of the header to the global header ."], "references": ["generate html header content"], "bleu": 0.11390778025531027, "rouge_l": 0.14558472553699284}
{"id": 130, "code": "def buildjschart ( self ) : self . jschart = '' if self . tooltip condition string == '' : self . tooltip condition string = 'var y = String(graph.point.y);\\n' self . series js = json . dumps ( self . series )", "predictions": ["set the series of this series ."], "references": ["generate javascript code for the chart"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 131, "code": "def create x axis ( self , name , label = None , format = None , date = False , custom format = False ) : axis = { } if custom format and format : axis [ 'tick Format' ] = format elif format : if format == 'AM PM' : axis [ 'tick Format' ] = \"function(d) { return get am pm(parse Int(d)); }\" else : axis [ 'tick Format' ] = \"d3.format(',%s')\" % format if label : axis [ 'axis Label' ] = \"'\" + label + \"'\" if date : self . dateformat = format axis [ 'tick Format' ] = ( \"function(d) { return d3.time.format('%s')\" \"(new Date(parse Int(d))) }\\n\" \"\" % self . dateformat ) if name [ 0 ] == 'x' : self . x axis date = True self . axislist [ name ] = axis if name == \"x Axis\" and self . focus enable : self . axislist [ 'x2Axis' ] = axis", "predictions": ["create a x - axis axis"], "references": ["create x - axis"], "bleu": 0.4347208719449915, "rouge_l": 0.8299319727891156}
{"id": 132, "code": "def create y axis ( self , name , label = None , format = None , custom format = False ) : axis = { } if custom format and format : axis [ 'tick Format' ] = format elif format : axis [ 'tick Format' ] = \"d3.format(',%s')\" % format if label : axis [ 'axis Label' ] = \"'\" + label + \"'\" self . axislist [ name ] = axis", "predictions": ["create a y axis axis"], "references": ["create y - axis"], "bleu": 0.32466791547509893, "rouge_l": 0.6802973977695167}
{"id": 133, "code": "def get conn ( self ) : conn = self . get connection ( self . sqlite conn id ) conn = sqlite3 . connect ( conn . host ) return conn", "predictions": ["return a conn connection to the mysql database ."], "references": ["returns a sqlite connection object"], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 134, "code": "def action logging ( f ) : @ functools . wraps ( f ) def wrapper ( * args , * * kwargs ) : with create session ( ) as session : if g . user . is anonymous : user = 'anonymous' else : user = g . user . username log = Log ( event = f . name , task instance = None , owner = user , extra = str ( list ( request . args . items ( ) ) ) , task id = request . args . get ( 'task id' ) , dag id = request . args . get ( 'dag id' ) ) if 'execution date' in request . args : log . execution date = pendulum . parse ( request . args . get ( 'execution date' ) ) session . add ( log ) return f ( * args , * * kwargs ) return wrapper", "predictions": ["add action logging ."], "references": ["decorator to log user actions"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 135, "code": "def gzipped ( f ) : @ functools . wraps ( f ) def view func ( * args , * * kwargs ) : @ after this request def zipper ( response ) : accept encoding = request . headers . get ( 'Accept-Encoding' , '' ) if 'gzip' not in accept encoding . lower ( ) : return response response . direct passthrough = False if ( response . status code < 200 or response . status code >= 300 or 'Content-Encoding' in response . headers ) : return response gzip buffer = IO ( ) gzip file = gzip . Gzip File ( mode = 'wb' , fileobj = gzip buffer ) gzip file . write ( response . data ) gzip file . close ( ) response . data = gzip buffer . getvalue ( ) response . headers [ 'Content-Encoding' ] = 'gzip' response . headers [ 'Vary' ] = 'Accept-Encoding' response . headers [ 'Content-Length' ] = len ( response . data ) return response return f ( * args , * * kwargs ) return view func", "predictions": ["decorator to gzipped a gzipped gzipped gzipped gzipped ."], "references": ["decorator to make a view compressed"], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 136, "code": "def json response ( obj ) : return Response ( response = json . dumps ( obj , indent = 4 , cls = Airflow Json Encoder ) , status = 200 , mimetype = \"application/json\" )", "predictions": ["return json - serializable response ."], "references": ["returns a json response from a json serializable python object"], "bleu": 0.13487005099534619, "rouge_l": 0.23921568627450981}
{"id": 137, "code": "def make cache key ( * args , * * kwargs ) : path = request . path args = str ( hash ( frozenset ( request . args . items ( ) ) ) ) return ( path + args ) . encode ( 'ascii' , 'ignore' )", "predictions": ["make a cache key from the given args ."], "references": ["used by cache to get a unique key per url"], "bleu": 0.15019394384099988, "rouge_l": 0.20854700854700853}
{"id": 138, "code": "def get api key ( self ) : conn = self . get connection ( self . http conn id ) api key = conn . password if not api key : raise Airflow Exception ( 'Opsgenie API Key is required for this hook, ' 'please check your conn id configuration.' ) return api key", "predictions": ["return the api key for the current api ."], "references": ["get opsgenie api_key for creating alert"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 139, "code": "def execute ( self , context ) : self . hook = Opsgenie Alert Hook ( self . opsgenie conn id ) self . hook . execute ( self . build opsgenie payload ( ) )", "predictions": ["execute the hook in the hook ."], "references": ["call the opsgeniealerthook to post message"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 140, "code": "def get conn ( self ) : if self . conn is None : cnopts = pysftp . Cn Opts ( ) if self . no host key check : cnopts . hostkeys = None cnopts . compression = self . compress conn params = { 'host' : self . remote host , 'port' : self . port , 'username' : self . username , 'cnopts' : cnopts } if self . password and self . password . strip ( ) : conn params [ 'password' ] = self . password if self . key file : conn params [ 'private key' ] = self . key file if self . private key pass : conn params [ 'private key pass' ] = self . private key pass self . conn = pysftp . Connection ( * * conn params ) return self . conn", "predictions": ["get a connection to remote host"], "references": ["returns an sftp connection object"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 141, "code": "def execute ( self , context ) : s3 conn = S3Hook ( self . s3 conn id ) if self . is pipeline : results = Mongo Hook ( self . mongo conn id ) . aggregate ( mongo collection = self . mongo collection , aggregate query = self . mongo query , mongo db = self . mongo db ) else : results = Mongo Hook ( self . mongo conn id ) . find ( mongo collection = self . mongo collection , query = self . mongo query , mongo db = self . mongo db ) docs str = self . stringify ( self . transform ( results ) ) s3 conn . load string ( string data = docs str , key = self . s3 key , bucket name = self . s3 bucket , replace = self . replace ) return True", "predictions": ["execute the mongo query"], "references": ["executed by task_instance at runtime"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 142, "code": "def get pool ( name , session = None ) : if not ( name and name . strip ( ) ) : raise Airflow Bad Request ( \"Pool name shouldn't be empty\" ) pool = session . query ( Pool ) . filter by ( pool = name ) . first ( ) if pool is None : raise Pool Not Found ( \"Pool '%s' doesn't exist\" % name ) return pool", "predictions": ["get a pool by name ."], "references": ["get pool by a given name ."], "bleu": 0.3365910691208744, "rouge_l": 0.7587064676616916}
{"id": 143, "code": "def create pool ( name , slots , description , session = None ) : if not ( name and name . strip ( ) ) : raise Airflow Bad Request ( \"Pool name shouldn't be empty\" ) try : slots = int ( slots ) except Value Error : raise Airflow Bad Request ( \"Bad value for `slots`: %s\" % slots ) session . expire on commit = False pool = session . query ( Pool ) . filter by ( pool = name ) . first ( ) if pool is None : pool = Pool ( pool = name , slots = slots , description = description ) session . add ( pool ) else : pool . slots = slots pool . description = description session . commit ( ) return pool", "predictions": ["create a pool pool ."], "references": ["create a pool with a given parameters ."], "bleu": 0.2948682411907622, "rouge_l": 0.5907990314769976}
{"id": 144, "code": "def delete pool ( name , session = None ) : if not ( name and name . strip ( ) ) : raise Airflow Bad Request ( \"Pool name shouldn't be empty\" ) pool = session . query ( Pool ) . filter by ( pool = name ) . first ( ) if pool is None : raise Pool Not Found ( \"Pool '%s' doesn't exist\" % name ) session . delete ( pool ) session . commit ( ) return pool", "predictions": ["delete a pool ."], "references": ["delete pool by a given name ."], "bleu": 0.21341568174752815, "rouge_l": 0.5198863636363635}
{"id": 145, "code": "def execute ( self ) : proxies = { } if self . proxy : proxies = { 'https' : self . proxy } discord payload = self . build discord payload ( ) self . run ( endpoint = self . webhook endpoint , data = discord payload , headers = { 'Content-type' : 'application/json' } , extra options = { 'proxies' : proxies } )", "predictions": ["execute the webhook and run the webhook"], "references": ["execute the discord webhook call"], "bleu": 0.2626909894424158, "rouge_l": 0.5154929577464789}
{"id": 146, "code": "def close ( self ) : if self . closed : return super ( ) . close ( ) if not self . upload on close : return local loc = os . path . join ( self . local base , self . log relative path ) remote loc = os . path . join ( self . remote base , self . log relative path ) if os . path . exists ( local loc ) : with open ( local loc , 'r' ) as logfile : log = logfile . read ( ) self . s3 write ( log , remote loc ) self . closed = True", "predictions": ["close the file and close the file ."], "references": ["close and upload local log file to remote storage s3 ."], "bleu": 0.13859150907108325, "rouge_l": 0.4093959731543625}
{"id": 147, "code": "def get init containers ( self ) : if self . kube config . dags volume claim or self . kube config . dags volume host or self . kube config . dags in image : return [ ] init environment = [ { 'name' : 'GIT SYNC REPO' , 'value' : self . kube config . git repo } , { 'name' : 'GIT SYNC BRANCH' , 'value' : self . kube config . git branch } , { 'name' : 'GIT SYNC ROOT' , 'value' : self . kube config . git sync root } , { 'name' : 'GIT SYNC DEST' , 'value' : self . kube config . git sync dest } , { 'name' : 'GIT SYNC DEPTH' , 'value' : '1' } , { 'name' : 'GIT SYNC ONE TIME' , 'value' : 'true' } ] if self . kube config . git user : init environment . append ( { 'name' : 'GIT SYNC USERNAME' , 'value' : self . kube config . git user } ) if self . kube config . git password : init environment . append ( { 'name' : 'GIT SYNC PASSWORD' , 'value' : self . kube config . git password } ) volume mounts = [ { 'mount Path' : self . kube config . git sync root , 'name' : self . dags volume name , 'read Only' : False } ] if self . kube config . git ssh key secret name : volume mounts . append ( { 'name' : self . git sync ssh secret volume name , 'mount Path' : '/etc/git-secret/ssh' , 'sub Path' : 'ssh' } ) init environment . extend ( [ { 'name' : 'GIT SSH KEY FILE' , 'value' : '/etc/git-secret/ssh' } , { 'name' : 'GIT SYNC SSH' , 'value' : 'true' } ] ) if self . kube config . git ssh known hosts configmap name : volume mounts . append ( { 'name' : self . git sync ssh known hosts volume name , 'mount Path' : '/etc/git-secret/known hosts' , 'sub Path' : 'known hosts' } ) init environment . extend ( [ { 'name' : 'GIT KNOWN HOSTS' , 'value' : 'true' } , { 'name' : 'GIT SSH KNOWN HOSTS FILE' , 'value' : '/etc/git-secret/known hosts' } ] ) else : init environment . append ( { 'name' : 'GIT KNOWN HOSTS' , 'value' : 'false' } ) return [ { 'name' : self . kube config . git sync init container name , 'image' : self . kube config . git sync container , 'security Context' : { 'run As User' : 65533 } , 'env' : init environment , 'volume Mounts' : volume mounts } ]", "predictions": ["returns a list of hosts for the current hosts ."], "references": ["when using git to retrieve the dags use the gitsync init container"], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 148, "code": "def get environment ( self ) : env = { } for env var name , env var val in six . iteritems ( self . kube config . kube env vars ) : env [ env var name ] = env var val env [ \"AIRFLOW CORE EXECUTOR\" ] = \"Local Executor\" if self . kube config . airflow configmap : env [ 'AIRFLOW HOME' ] = self . worker airflow home env [ 'AIRFLOW CORE DAGS FOLDER' ] = self . worker airflow dags if ( not self . kube config . airflow configmap and 'AIRFLOW CORE SQL ALCHEMY CONN' not in self . kube config . kube secrets ) : env [ 'AIRFLOW CORE SQL ALCHEMY CONN' ] = conf . get ( \"core\" , \"SQL ALCHEMY CONN\" ) if self . kube config . git dags folder mount point : dag volume mount path = os . path . join ( self . kube config . git dags folder mount point , self . kube config . git sync dest , self . kube config . git subpath ) env [ 'AIRFLOW CORE DAGS FOLDER' ] = dag volume mount path return env", "predictions": ["get dag environment variables ."], "references": ["defines any necessary environment variables for the pod executor"], "bleu": 0.1614457444314309, "rouge_l": 0.2717149220489978}
{"id": 149, "code": "def get secrets ( self ) : worker secrets = [ ] for env var name , obj key pair in six . iteritems ( self . kube config . kube secrets ) : k8s secret obj , k8s secret key = obj key pair . split ( '=' ) worker secrets . append ( Secret ( 'env' , env var name , k8s secret obj , k8s secret key ) ) if self . kube config . env from secret ref : for secret ref in self . kube config . env from secret ref . split ( ',' ) : worker secrets . append ( Secret ( 'env' , None , secret ref ) ) return worker secrets", "predictions": ["get secrets from environment variables ."], "references": ["defines any necessary secrets for the pod executor"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 150, "code": "def get security context ( self ) : security context = { } if self . kube config . worker run as user : security context [ 'run As User' ] = self . kube config . worker run as user if self . kube config . worker fs group : security context [ 'fs Group' ] = self . kube config . worker fs group if self . kube config . git ssh key secret name and security context . get ( 'fs Group' ) is None : security context [ 'fs Group' ] = 65533 return security context", "predictions": ["returns the security context"], "references": ["defines the security context"], "bleu": 0.668740304976422, "rouge_l": 0.75}
{"id": 151, "code": "def start ( self ) : self . process = Dag File Processor . launch process ( self . result queue , self . file path , self . pickle dags , self . dag id white list , \"Dag File Processor{}\" . format ( self . instance id ) , self . zombies ) self . start time = timezone . utcnow ( )", "predictions": ["start the dag ."], "references": ["launch the process and start processing the dag ."], "bleu": 0.20258948470231466, "rouge_l": 0.5754716981132075}
{"id": 152, "code": "def exit gracefully ( self , signum , frame ) : self . log . info ( \"Exiting gracefully upon receiving signal %s\" , signum ) if self . processor agent : self . processor agent . end ( ) sys . exit ( os . EX OK )", "predictions": ["exit on the processor agent ."], "references": ["helper method to clean up processor_agent to avoid leaving orphan processes ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 153, "code": "def process executor events ( self , simple dag bag , session = None ) : TI = models . Task Instance for key , state in list ( self . executor . get event buffer ( simple dag bag . dag ids ) . items ( ) ) : dag id , task id , execution date , try number = key self . log . info ( \"Executor reports execution of %s.%s execution date=%s \" \"exited with status %s for try number %s\" , dag id , task id , execution date , state , try number ) if state == State . FAILED or state == State . SUCCESS : qry = session . query ( TI ) . filter ( TI . dag id == dag id , TI . task id == task id , TI . execution date == execution date ) ti = qry . first ( ) if not ti : self . log . warning ( \"Task Instance %s went missing from the database\" , ti ) continue if ti . try number == try number and ti . state == State . QUEUED : msg = ( \"Executor reports task instance {} finished ({}) \" \"although the task says its {}. Was the task \" \"killed externally?\" . format ( ti , state , ti . state ) ) self . log . error ( msg ) try : simple dag = simple dag bag . get dag ( dag id ) dagbag = models . Dag Bag ( simple dag . full filepath ) dag = dagbag . get dag ( dag id ) ti . task = dag . get task ( task id ) ti . handle failure ( msg ) except Exception : self . log . error ( \"Cannot load the dag bag to handle failure for %s\" \". Setting task to FAILED without callbacks or \" \"retries. Do you have enough resources?\" , ti ) ti . state = State . FAILED session . merge ( ti ) session . commit ( )", "predictions": ["process the simple execution events from the dag"], "references": ["respond to executor events ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 154, "code": "def heartbeat callback ( self , session = None ) : if self . terminating : self . task runner . terminate ( ) return self . task instance . refresh from db ( ) ti = self . task instance fqdn = get hostname ( ) same hostname = fqdn == ti . hostname same process = ti . pid == os . getpid ( ) if ti . state == State . RUNNING : if not same hostname : self . log . warning ( \"The recorded hostname %s \" \"does not match this instance's hostname \" \"%s\" , ti . hostname , fqdn ) raise Airflow Exception ( \"Hostname of job runner does not match\" ) elif not same process : current pid = os . getpid ( ) self . log . warning ( \"Recorded pid %s does not match \" \"the current pid %s\" , ti . pid , current pid ) raise Airflow Exception ( \"PID of job runner does not match\" ) elif ( self . task runner . return code ( ) is None and hasattr ( self . task runner , 'process' ) ) : self . log . warning ( \"State of this instance has been externally set to %s. \" \"Taking the poison pill.\" , ti . state ) self . task runner . terminate ( ) self . terminating = True", "predictions": ["process the job callback ."], "references": ["self destruct task if state has been moved away from running externally"], "bleu": 0.0566124695559154, "rouge_l": 0.0}
{"id": 155, "code": "def get conn ( self ) : if self . session and not self . session . is shutdown : return self . session self . session = self . cluster . connect ( self . keyspace ) return self . session", "predictions": ["return a conn object for the given vm ."], "references": ["returns a cassandra session object"], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 156, "code": "def query mysql ( self ) : mysql = My Sql Hook ( mysql conn id = self . mysql conn id ) conn = mysql . get conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql ) return cursor", "predictions": ["returns the cursor cursor for the mysql db"], "references": ["queries mysql and returns a cursor to the results ."], "bleu": 0.157044754112095, "rouge_l": 0.3267857142857143}
{"id": 157, "code": "def get col type dict ( self ) : schema = [ ] if isinstance ( self . schema , string types ) : schema = json . loads ( self . schema ) elif isinstance ( self . schema , list ) : schema = self . schema elif self . schema is not None : self . log . warn ( 'Using default schema due to unexpected type.' 'Should be a string or list.' ) col type dict = { } try : col type dict = { col [ 'name' ] : col [ 'type' ] for col in schema } except Key Error : self . log . warn ( 'Using default schema due to missing name or type. Please ' 'refer to: https://cloud.google.com/bigquery/docs/schemas' '#specifying a json schema file' ) return col type dict", "predictions": ["returns the type of the type of the schema"], "references": ["return a dict of column name and column type based on self . schema if not none ."], "bleu": 0.061746576272781344, "rouge_l": 0.20962199312714777}
{"id": 158, "code": "def extra dejson ( self ) : obj = { } if self . extra : try : obj = json . loads ( self . extra ) except Exception as e : self . log . exception ( e ) self . log . error ( \"Failed parsing the json for conn id %s\" , self . conn id ) return obj", "predictions": ["returns the extra data from the json encoded string ."], "references": ["returns the extra property by deserializing json ."], "bleu": 0.25965358893403384, "rouge_l": 0.5669144981412639}
{"id": 159, "code": "def scale time units ( time seconds arr , unit ) : if unit == 'minutes' : return list ( map ( lambda x : x * 1.0 / 60 , time seconds arr ) ) elif unit == 'hours' : return list ( map ( lambda x : x * 1.0 / ( 60 * 60 ) , time seconds arr ) ) elif unit == 'days' : return list ( map ( lambda x : x * 1.0 / ( 24 * 60 * 60 ) , time seconds arr ) ) return time seconds arr", "predictions": ["scale time units to given unit"], "references": ["convert an array of time durations in seconds to the specified time unit ."], "bleu": 0.06924459302580939, "rouge_l": 0.2798165137614679}
{"id": 160, "code": "def get all permissions views ( self ) : perms views = set ( ) for role in self . get user roles ( ) : perms views . update ( { ( perm view . permission . name , perm view . view menu . name ) for perm view in role . permissions } ) return perms views", "predictions": ["returns a set of all permissions views views"], "references": ["returns a set of tuples with the perm name and view menu name"], "bleu": 0.23889981509649538, "rouge_l": 0.3652694610778443}
{"id": 161, "code": "def has role ( self , role name or list ) : if not isinstance ( role name or list , list ) : role name or list = [ role name or list ] return any ( [ r . name in role name or list for r in self . get user roles ( ) ] )", "predictions": ["check if role is globals"], "references": ["whether the user has this role name"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 162, "code": "def has perm ( self , permission name , view menu name ) : if hasattr ( self , 'perms' ) : if ( permission name , view menu name ) in self . perms : return True self . get and cache perms ( ) return ( permission name , view menu name ) in self . perms", "predictions": ["returns true if the . . . perm is a perm menu y - 1 y - 1 y - 1 y menu"], "references": ["whether the user has this perm"], "bleu": 0.05856458233275369, "rouge_l": 0.15423514538558786}
{"id": 163, "code": "def clean perms ( self ) : self . log . debug ( 'Cleaning faulty perms' ) sesh = self . get session pvms = ( sesh . query ( sqla models . Permission View ) . filter ( or ( sqla models . Permission View . permission == None , sqla models . Permission View . view menu == None , ) ) ) deleted count = pvms . delete ( ) sesh . commit ( ) if deleted count : self . log . info ( 'Deleted %s faulty permissions' , deleted count )", "predictions": ["remove all format format format name and permissions'"], "references": ["fab leaves faulty permissions that need to be cleaned up"], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 164, "code": "def create perm vm for all dag ( self ) : for dag vm in self . DAG VMS : for perm in self . DAG PERMS : self . merge perm ( permission name = perm , view menu name = dag vm )", "predictions": ["creates the dag for for for for"], "references": ["create perm - vm if not exist and insert into fab security model for all - dags ."], "bleu": 0.038589346254072475, "rouge_l": 0.07411907654921021}
{"id": 165, "code": "def poke ( self , context ) : if '.' in self . table name : self . database name , self . table name = self . table name . split ( '.' ) self . log . info ( 'Poking for table %s. %s, expression %s' , self . database name , self . table name , self . expression ) return self . get hook ( ) . check for partition ( self . database name , self . table name , self . expression )", "predictions": ["return the sqlite3 sqlite3 host"], "references": ["checks for existence of the partition in the aws glue catalog table"], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 166, "code": "def get conn ( self ) : effective user = self . proxy user autoconfig = self . autoconfig use sasl = configuration . conf . get ( 'core' , 'security' ) == 'kerberos' try : connections = self . get connections ( self . hdfs conn id ) if not effective user : effective user = connections [ 0 ] . login if not autoconfig : autoconfig = connections [ 0 ] . extra dejson . get ( 'autoconfig' , False ) hdfs namenode principal = connections [ 0 ] . extra dejson . get ( 'hdfs namenode principal' ) except Airflow Exception : if not autoconfig : raise if autoconfig : client = Auto Config Client ( effective user = effective user , use sasl = use sasl ) elif len ( connections ) == 1 : client = Client ( connections [ 0 ] . host , connections [ 0 ] . port , effective user = effective user , use sasl = use sasl , hdfs namenode principal = hdfs namenode principal ) elif len ( connections ) > 1 : nn = [ Namenode ( conn . host , conn . port ) for conn in connections ] client = HA Client ( nn , effective user = effective user , use sasl = use sasl , hdfs namenode principal = hdfs namenode principal ) else : raise HDFS Hook Exception ( \"conn id doesn't exist in the repository \" \"and autoconfig is not specified\" ) return client", "predictions": ["return a logging client connection"], "references": ["returns a snakebite hdfsclient object ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 167, "code": "def get conn ( self ) : conn = self . get connection ( self . pinot broker conn id ) pinot broker conn = connect ( host = conn . host , port = conn . port , path = conn . extra dejson . get ( 'endpoint' , '/pql' ) , scheme = conn . extra dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to pinot ' 'broker on {host}' . format ( host = conn . host ) ) return pinot broker conn", "predictions": ["gzipped for a postgres connection"], "references": ["establish a connection to pinot broker through pinot dbqpi ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 168, "code": "def convert date to dict ( field date ) : return { DAY : field date . day , MONTH : field date . month , YEAR : field date . year }", "predictions": ["convert response field to json string"], "references": ["convert native python datetime . date object to a format supported by the api"], "bleu": 0.06443935473636557, "rouge_l": 0.18654434250764526}
{"id": 169, "code": "def convert time to dict ( time ) : return { HOURS : time . hour , MINUTES : time . minute , SECONDS : time . second }", "predictions": ["convert cache cache cache cache to dict"], "references": ["convert native python datetime . time object to a format supported by the api"], "bleu": 0.07562380261607851, "rouge_l": 0.17967599410898377}
{"id": 170, "code": "def get conn ( self ) : conn = self . get connection ( self . redis conn id ) self . host = conn . host self . port = conn . port self . password = None if str ( conn . password ) . lower ( ) in [ 'none' , 'false' , '' ] else conn . password self . db = conn . extra dejson . get ( 'db' , None ) if not self . redis : self . log . debug ( 'Initializing redis object for conn id \"%s\" on %s:%s:%s' , self . redis conn id , self . host , self . port , self . db ) self . redis = Redis ( host = self . host , port = self . port , password = self . password , db = self . db ) return self . redis", "predictions": ["return will be a . . . . . . . . . ."], "references": ["returns a redis connection ."], "bleu": 0.09782375748961449, "rouge_l": 0.23018867924528305}
{"id": 171, "code": "def get conn ( self ) : db = self . get connection ( getattr ( self , self . conn name attr ) ) return self . connector . connect ( host = db . host , port = db . port , username = db . login , schema = db . schema )", "predictions": ["execute a connection to the mysql database hook hook hook hook hook hook hook hook hook"], "references": ["returns a connection object"], "bleu": 0.10123734869668824, "rouge_l": 0.22426470588235295}
{"id": 172, "code": "def set autocommit ( self , conn , autocommit ) : if not self . supports autocommit and autocommit : self . log . warn ( ( \"%s connection doesn't support \" \"autocommit but autocommit activated.\" ) , getattr ( self , self . conn name attr ) ) conn . autocommit = autocommit", "predictions": ["get or set the conn for this host . . . . ."], "references": ["sets the autocommit flag on the connection"], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 173, "code": "def get query ( self ) : return ( super ( ) . get query ( ) . filter ( or ( models . Dag Model . is active , models . Dag Model . is paused ) ) . filter ( ~ models . Dag Model . is subdag ) )", "predictions": ["returns the query instance conn conn conn conn conn conn conn conn conn conn conn conn conn conn conn conn conn conn conn as a query"], "references": ["default filters for model"], "bleu": 0.03925345689749394, "rouge_l": 0.0}
{"id": 174, "code": "def get count query ( self ) : return ( super ( ) . get count query ( ) . filter ( models . Dag Model . is active ) . filter ( ~ models . Dag Model . is subdag ) )", "predictions": ["returns the pool pool pool pool pool ."], "references": ["default filters for model"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 175, "code": "def execute ( self , context ) : self . hook = Slack Webhook Hook ( self . http conn id , self . webhook token , self . message , self . attachments , self . channel , self . username , self . icon emoji , self . link names , self . proxy ) self . hook . execute ( )", "predictions": ["executes the given context"], "references": ["call the slackwebhookhook to post the provided slack message"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 176, "code": "def get credentials ( self ) : key path = self . get field ( 'key path' , False ) keyfile dict = self . get field ( 'keyfile dict' , False ) scope = self . get field ( 'scope' , None ) if scope : scopes = [ s . strip ( ) for s in scope . split ( ',' ) ] else : scopes = DEFAULT SCOPES if not key path and not keyfile dict : self . log . info ( 'Getting connection using `google.auth.default()` ' 'since no key file is defined for hook.' ) credentials , = google . auth . default ( scopes = scopes ) elif key path : if key path . endswith ( '.json' ) : self . log . debug ( 'Getting connection using JSON key file %s' % key path ) credentials = ( google . oauth2 . service account . Credentials . from service account file ( key path , scopes = scopes ) ) elif key path . endswith ( '.p12' ) : raise Airflow Exception ( 'Legacy P12 key file are not supported, ' 'use a JSON key file.' ) else : raise Airflow Exception ( 'Unrecognised extension for key file.' ) else : try : keyfile dict = json . loads ( keyfile dict ) keyfile dict [ 'private key' ] = keyfile dict [ 'private key' ] . replace ( '\\\\n' , '\\n' ) credentials = ( google . oauth2 . service account . Credentials . from service account info ( keyfile dict , scopes = scopes ) ) except json . decoder . JSON Decode Error : raise Airflow Exception ( 'Invalid key JSON.' ) return credentials . with subject ( self . delegate to ) if self . delegate to else credentials", "predictions": ["delete the connection pool from the service . ."], "references": ["returns the credentials object for google api"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 177, "code": "def read image file ( data dir , image ext , n ) : def PIL2array ( img ) : return np . array ( img . getdata ( ) , dtype = np . uint8 ) . reshape ( 64 , 64 ) def find files ( data dir , image ext ) : files = [ ] for file dir in os . listdir ( data dir ) : if file dir . endswith ( image ext ) : files . append ( os . path . join ( data dir , file dir ) ) return sorted ( files ) patches = [ ] list files = find files ( data dir , image ext ) for fpath in list files : img = Image . open ( fpath ) for y in range ( 0 , 1024 , 64 ) : for x in range ( 0 , 1024 , 64 ) : patch = img . crop ( ( x , y , x + 64 , y + 64 ) ) patches . append ( PIL2array ( patch ) ) return torch . Byte Tensor ( np . array ( patches [ : n ] ) )", "predictions": ["execute } self build and return a list of }"], "references": ["return a tensor containing the patches"], "bleu": 0.16590387014219712, "rouge_l": 0.26180257510729615}
{"id": 178, "code": "def accuracy ( output , target , topk = ( 1 , ) ) : with torch . no grad ( ) : maxk = max ( topk ) batch size = target . size ( 0 ) , pred = output . topk ( maxk , 1 , True , True ) pred = pred . t ( ) correct = pred . eq ( target [ None ] ) res = [ ] for k in topk : correct k = correct [ : k ] . flatten ( ) . sum ( dtype = torch . float32 ) res . append ( correct k * ( 100.0 / batch size ) ) return res", "predictions": ["close the close close of the given target"], "references": ["computes the accuracy over the k top predictions for the specified values of k"], "bleu": 0.09008421318929809, "rouge_l": 0.1732954545454545}
{"id": 179, "code": "def setup for distributed ( is master ) : import builtins as builtin builtin print = builtin . print def print ( * args , * * kwargs ) : force = kwargs . pop ( 'force' , False ) if is master or force : builtin print ( * args , * * kwargs ) builtin . print = print", "predictions": ["get containers for containers or dags or dags or volume or volume or dags"], "references": ["this function disables printing when not in master process"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 180, "code": "def download ( self ) : import tarfile if self . check integrity ( ) : print ( 'Files already downloaded and verified' ) return download url ( self . url , self . root , self . filename , self . md5 checksum ) with tarfile . open ( os . path . join ( self . root , self . filename ) , 'r:gz' ) as tar : tar . extractall ( path = self . root ) with open ( os . path . join ( self . root , 'dataset' , 'SBU captioned photo dataset urls.txt' ) ) as fh : for line in fh : url = line . rstrip ( ) try : download url ( url , os . path . join ( self . root , 'dataset' ) ) except OS Error : pass", "predictions": ["get the iteritems dags dags dags { tablename } { % } { % } { { { { { { { % } { tablename } { % } {"], "references": ["download and extract the tarball and download each individual photo ."], "bleu": 0.03901663112717908, "rouge_l": 0.05209222886421862}
{"id": 181, "code": "def download ( self ) : if self . check exists ( ) : return makedir exist ok ( self . raw folder ) makedir exist ok ( self . processed folder ) for url in self . urls : filename = url . rpartition ( '/' ) [ 2 ] file path = os . path . join ( self . raw folder , filename ) download url ( url , root = self . raw folder , filename = filename , md5 = None ) self . extract gzip ( gzip path = file path , remove finished = True ) print ( 'Processing...' ) training set = ( read image file ( os . path . join ( self . raw folder , 'train-images-idx3-ubyte' ) ) , read label file ( os . path . join ( self . raw folder , 'train-labels-idx1-ubyte' ) ) ) test set = ( read image file ( os . path . join ( self . raw folder , 't10k-images-idx3-ubyte' ) ) , read label file ( os . path . join ( self . raw folder , 't10k-labels-idx1-ubyte' ) ) ) with open ( os . path . join ( self . processed folder , self . training file ) , 'wb' ) as f : torch . save ( training set , f ) with open ( os . path . join ( self . processed folder , self . test file ) , 'wb' ) as f : torch . save ( test set , f ) print ( 'Done!' )", "predictions": ["get the test . worker"], "references": ["download the mnist data if it doesn t exist in processed_folder already ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 182, "code": "def download ( self ) : import shutil import zipfile if self . check exists ( ) : return makedir exist ok ( self . raw folder ) makedir exist ok ( self . processed folder ) filename = self . url . rpartition ( '/' ) [ 2 ] file path = os . path . join ( self . raw folder , filename ) download url ( self . url , root = self . raw folder , filename = filename , md5 = None ) print ( 'Extracting zip archive' ) with zipfile . Zip File ( file path ) as zip f : zip f . extractall ( self . raw folder ) os . unlink ( file path ) gzip folder = os . path . join ( self . raw folder , 'gzip' ) for gzip file in os . listdir ( gzip folder ) : if gzip file . endswith ( '.gz' ) : self . extract gzip ( gzip path = os . path . join ( gzip folder , gzip file ) ) for split in self . splits : print ( 'Processing ' + split ) training set = ( read image file ( os . path . join ( gzip folder , 'emnist-{}-train-images-idx3-ubyte' . format ( split ) ) ) , read label file ( os . path . join ( gzip folder , 'emnist-{}-train-labels-idx1-ubyte' . format ( split ) ) ) ) test set = ( read image file ( os . path . join ( gzip folder , 'emnist-{}-test-images-idx3-ubyte' . format ( split ) ) ) , read label file ( os . path . join ( gzip folder , 'emnist-{}-test-labels-idx1-ubyte' . format ( split ) ) ) ) with open ( os . path . join ( self . processed folder , self . training file ( split ) ) , 'wb' ) as f : torch . save ( training set , f ) with open ( os . path . join ( self . processed folder , self . test file ( split ) ) , 'wb' ) as f : torch . save ( test set , f ) shutil . rmtree ( gzip folder ) print ( 'Done!' )", "predictions": ["get the test . } }"], "references": ["download the emnist data if it doesn t exist in processed_folder already ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 183, "code": "def preferences ( ) : if request . method == 'POST' : resp = make response ( redirect ( urljoin ( settings [ 'server' ] [ 'base url' ] , url for ( 'index' ) ) ) ) try : request . preferences . parse form ( request . form ) except Validation Exception : request . errors . append ( gettext ( 'Invalid settings, please edit your preferences' ) ) return resp return request . preferences . save ( resp ) image proxy = request . preferences . get value ( 'image proxy' ) lang = request . preferences . get value ( 'language' ) disabled engines = request . preferences . engines . get disabled ( ) allowed plugins = request . preferences . plugins . get enabled ( ) stats = { } for c in categories : for e in categories [ c ] : stats [ e . name ] = { 'time' : None , 'warn timeout' : False , 'warn time' : False } if e . timeout > settings [ 'outgoing' ] [ 'request timeout' ] : stats [ e . name ] [ 'warn timeout' ] = True stats [ e . name ] [ 'supports selected language' ] = is selected language supported ( e , request . preferences ) for engine stat in get engines stats ( ) [ 0 ] [ 1 ] : stats [ engine stat . get ( 'name' ) ] [ 'time' ] = round ( engine stat . get ( 'avg' ) , 3 ) if engine stat . get ( 'avg' ) > settings [ 'outgoing' ] [ 'request timeout' ] : stats [ engine stat . get ( 'name' ) ] [ 'warn time' ] = True return render ( 'preferences.html' , locales = settings [ 'locales' ] , current locale = get locale ( ) , image proxy = image proxy , engines by category = categories , stats = stats , answerers = [ { 'info' : a . self info ( ) , 'keywords' : a . keywords } for a in answerers ] , disabled engines = disabled engines , autocomplete backends = autocomplete backends , shortcuts = { y : x for x , y in engine shortcuts . items ( ) } , themes = themes , plugins = plugins , doi resolvers = settings [ 'doi resolvers' ] , current doi resolver = get doi resolver ( request . args , request . preferences . get value ( 'doi resolver' ) ) , allowed plugins = allowed plugins , theme = get current theme name ( ) , preferences url params = request . preferences . get as url params ( ) , base url = get base url ( ) , preferences = True )", "predictions": ["edit view of the start of the page process process process process process process process process process process process process process process process process process process process process process process process"], "references": ["render preferences page && save user preferences"], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 184, "code": "def get themes ( templates path ) : themes = os . listdir ( templates path ) if ' common ' in themes : themes . remove ( ' common ' ) return themes", "predictions": ["return gracefully from gracefully"], "references": ["returns available themes list ."], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 185, "code": "def searx bang ( full query ) : if len ( full query . get Search Query ( ) ) == 0 : return [ ] results = [ ] first char = full query . get Search Query ( ) [ 0 ] if first char == '!' or first char == '?' : if len ( full query . get Search Query ( ) ) == 1 : results . append ( first char + \"images\" ) results . append ( first char + \"wikipedia\" ) results . append ( first char + \"osm\" ) else : engine query = full query . get Search Query ( ) [ 1 : ] for categorie in categories : if categorie . startswith ( engine query ) : results . append ( first char + '{categorie}' . format ( categorie = categorie ) ) for engine in engines : if engine . startswith ( engine query . replace ( ' ' , ' ' ) ) : results . append ( first char + '{engine}' . format ( engine = engine . replace ( ' ' , ' ' ) ) ) for engine shortcut in engine shortcuts : if engine shortcut . startswith ( engine query ) : results . append ( first char + '{engine shortcut}' . format ( engine shortcut = engine shortcut ) ) elif first char == ':' : if len ( full query . get Search Query ( ) ) == 1 : results . append ( \":en\" ) results . append ( \":en us\" ) results . append ( \":english\" ) results . append ( \":united kingdom\" ) else : engine query = full query . get Search Query ( ) [ 1 : ] for lc in language codes : lang id , lang name , country , english name = map ( unicode . lower , lc ) if lang id . startswith ( engine query ) : if len ( engine query ) <= 2 : results . append ( u':{lang id}' . format ( lang id = lang id . split ( '-' ) [ 0 ] ) ) else : results . append ( u':{lang id}' . format ( lang id = lang id ) ) if lang name . startswith ( engine query ) or english name . startswith ( engine query ) : results . append ( u':{lang name}' . format ( lang name = lang name ) ) if country . startswith ( engine query . replace ( ' ' , ' ' ) ) : results . append ( u':{country}' . format ( country = country . replace ( ' ' , ' ' ) ) ) result set = set ( results ) for query part in full query . query parts : if query part in result set : result set . remove ( query part ) return list ( result set )", "predictions": ["models from process process"], "references": ["check if the searchquery contain a bang and create fitting autocompleter results"], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 186, "code": "def response ( resp ) : json resp = resp . text [ resp . text . find ( '\\n' ) + 1 : resp . text . rfind ( '\\n' ) - 2 ] results = [ ] try : conversion rate = float ( json . loads ( json resp ) [ 'conversion' ] [ 'converted-amount' ] ) except : return results answer = '{0} {1} = {2} {3}, 1 {1} ({5}) = {4} {3} ({6})' . format ( resp . search params [ 'amount' ] , resp . search params [ 'from' ] , resp . search params [ 'amount' ] * conversion rate , resp . search params [ 'to' ] , conversion rate , resp . search params [ 'from name' ] , resp . search params [ 'to name' ] , ) url = 'https://duckduckgo.com/js/spice/currency/1/{0}/{1}' . format ( resp . search params [ 'from' ] . upper ( ) , resp . search params [ 'to' ] ) results . append ( { 'answer' : answer , 'url' : url } ) return results", "predictions": ["query the heartbeat api"], "references": ["remove first and last lines to get only json"], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 187, "code": "def mvn ( * args , * * kwargs ) : return tfd . Independent ( tfd . Normal ( * args , * * kwargs ) , reinterpreted batch ndims = 1 )", "predictions": ["wrapper for the and is a and is called by the and is not executed ."], "references": ["convenience function to efficiently construct a multivariatenormaldiag ."], "bleu": 0.08513012360883544, "rouge_l": 0.17732558139534885}
{"id": 188, "code": "def eight schools joint log prob ( treatment effects , treatment stddevs , avg effect , avg stddev , school effects standard ) : rv avg effect = tfd . Normal ( loc = 0. , scale = 10. ) rv avg stddev = tfd . Normal ( loc = 5. , scale = 1. ) rv school effects standard = mvn ( loc = tf . zeros like ( school effects standard ) , scale = tf . ones like ( school effects standard ) ) rv treatment effects = mvn ( loc = ( avg effect + tf . exp ( avg stddev ) * school effects standard ) , scale = treatment stddevs ) return ( rv avg effect . log prob ( avg effect ) + rv avg stddev . log prob ( avg stddev ) + rv school effects standard . log prob ( school effects standard ) + rv treatment effects . log prob ( treatment effects ) )", "predictions": ["normally normally self . id ."], "references": ["eight - schools joint log - prob ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 189, "code": "def benchmark eight schools hmc ( num results = int ( 5e3 ) , num burnin steps = int ( 3e3 ) , num leapfrog steps = 3 , step size = 0.4 ) : num schools = 8 treatment effects = tf . constant ( [ 28 , 8 , - 3 , 7 , - 1 , 1 , 18 , 12 ] , dtype = np . float32 , name = 'treatment effects' ) treatment stddevs = tf . constant ( [ 15 , 10 , 16 , 11 , 9 , 11 , 10 , 18 ] , dtype = np . float32 , name = 'treatment stddevs' ) def unnormalized posterior log prob ( avg effect , avg stddev , school effects standard ) : \"\"\"Eight-schools unnormalized log posterior.\"\"\" return eight schools joint log prob ( treatment effects , treatment stddevs , avg effect , avg stddev , school effects standard ) if tf . executing eagerly ( ) : sample chain = tf . function ( tfp . mcmc . sample chain ) else : sample chain = tfp . mcmc . sample chain def computation ( ) : \"\"\"The benchmark computation.\"\"\" , kernel results = sample chain ( num results = num results , num burnin steps = num burnin steps , current state = ( tf . zeros ( [ ] , name = 'init avg effect' ) , tf . zeros ( [ ] , name = 'init avg stddev' ) , tf . ones ( [ num schools ] , name = 'init school effects standard' ) , ) , kernel = tfp . mcmc . Hamiltonian Monte Carlo ( target log prob fn = unnormalized posterior log prob , step size = step size , num leapfrog steps = num leapfrog steps ) ) return kernel results . is accepted is accepted tensor = computation ( ) if not tf . executing eagerly ( ) : session = tf . compat . v1 . Session ( ) session . run ( is accepted tensor ) start time = time . time ( ) if tf . executing eagerly ( ) : is accepted = computation ( ) else : is accepted = session . run ( is accepted tensor ) wall time = time . time ( ) - start time num accepted = np . sum ( is accepted ) acceptance rate = np . float32 ( num accepted ) / np . float32 ( num results ) return dict ( iters = ( num results + num burnin steps ) * num leapfrog steps , extras = { 'acceptance rate' : acceptance rate } , wall time = wall time )", "predictions": ["get name of name and name of name log log ."], "references": ["runs hmc on the eight - schools unnormalized posterior ."], "bleu": 0.11390778025531027, "rouge_l": 0.09606299212598425}
{"id": 190, "code": "def build custom rv ( distribution , sample shape , value , name ) : del name return Random Variable ( distribution = distribution , sample shape = sample shape , value = value )", "predictions": ["extra custom for a given distribution"], "references": ["randomvariable constructor with a dummy name argument ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 191, "code": "def make random variable ( distribution cls ) : @ interceptable @ functools . wraps ( distribution cls , assigned = ( ' module ' , ' name ' ) ) @ docstring util . expand docstring ( cls = distribution cls . name , doc = inspect . cleandoc ( distribution cls . init . doc or '' ) ) def func ( * args , * * kwargs ) : sample shape = kwargs . pop ( 'sample shape' , ( ) ) value = kwargs . pop ( 'value' , None ) return Random Variable ( distribution = distribution cls ( * args , * * kwargs ) , sample shape = sample shape , value = value ) return func", "predictions": ["creates a time units of a time units == the seconds == the seconds == seconds == the seconds == seconds"], "references": ["factory function to make random variable given distribution class ."], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 192, "code": "def max mask non finite ( x , axis = - 1 , keepdims = False , mask = 0 ) : m = np . max ( x , axis = astuple ( axis ) , keepdims = keepdims ) needs masking = ~ np . isfinite ( m ) if needs masking . ndim > 0 : m [ needs masking ] = mask elif needs masking : m = mask return m", "predictions": ["compute the max mask of a mask x ."], "references": ["returns max or mask if max is not finite ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 193, "code": "def eval all one hot ( fn , dist , name = None ) : with tf . compat . v1 . name scope ( name , 'eval all one hot' ) : event size = dist . event shape tensor ( ) [ - 1 ] batch ndims = tf . size ( input = dist . batch shape tensor ( ) ) x = tf . reshape ( tf . eye ( event size , dtype = dist . dtype ) , shape = tf . pad ( tensor = tf . ones ( batch ndims , tf . int32 ) , paddings = [ [ 1 , 1 ] ] , constant values = event size ) ) perm = tf . pad ( tensor = tf . range ( 1 , batch ndims + 1 ) , paddings = [ [ 0 , 1 ] ] ) return tf . transpose ( a = fn ( dist , x ) , perm = perm )", "predictions": ["one dimensional hot with one of the one - hot given dist ."], "references": ["onehotcategorical helper computing probs cdf etc over its support ."], "bleu": 0.09552040806823771, "rouge_l": 0.08905109489051095}
{"id": 194, "code": "def get convert to tensor fn ( identifier ) : if identifier is None : return None if isinstance ( identifier , six . string types ) : identifier = str ( identifier ) return deserialize ( identifier ) if isinstance ( identifier , dict ) : return deserialize ( identifier ) if isinstance ( identifier , property ) : identifier = identifier . fget if callable ( identifier ) : return identifier raise Value Error ( 'Could not interpret ' 'convert-to-tensor function identifier:' , identifier )", "predictions": ["convert identifier to tensor"], "references": ["return a convert - to - tensor func given a name config callable etc ."], "bleu": 0.02731554444032802, "rouge_l": 0.2859375}
{"id": 195, "code": "def new ( params , event size , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Multivariate Normal Tri L' , [ params , event size ] ) : params = tf . convert to tensor ( value = params , name = 'params' ) scale tril = tfb . Scale Tri L ( diag shift = np . array ( 1e-5 , params . dtype . as numpy dtype ( ) ) , validate args = validate args ) return tfd . Multivariate Normal Tri L ( loc = params [ ... , : event size ] , scale tril = scale tril ( params [ ... , event size : ] ) , validate args = validate args )", "predictions": ["create a new tensor ."], "references": ["create the distribution instance from a params vector ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 196, "code": "def params size ( event size , name = None ) : with tf . compat . v1 . name scope ( name , 'Multivariate Normal Tri L params size' , [ event size ] ) : return event size + event size * ( event size + 1 ) // 2", "predictions": ["returns the size of the event params ."], "references": ["the number of params needed to create a single distribution ."], "bleu": 0.13859150907108325, "rouge_l": 0.4093959731543625}
{"id": 197, "code": "def new ( params , event size , dtype = None , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'One Hot Categorical' , [ params , event size ] ) : return tfd . One Hot Categorical ( logits = params , dtype = dtype or params . dtype . base dtype , validate args = validate args )", "predictions": ["create a new event ."], "references": ["create the distribution instance from a params vector ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 198, "code": "def new ( params , event size , num components , dtype = None , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Categorical Mixture Of One Hot Categorical' , [ params , event size , num components ] ) : dist = Mixture Same Family . new ( params , num components , One Hot Categorical ( event size , validate args = False , name = name ) , validate args = validate args , name = name ) dist . mean = functools . partial ( eval all one hot , tfd . Distribution . prob , dist ) dist . log mean = functools . partial ( eval all one hot , tfd . Distribution . log prob , dist ) return dist", "predictions": ["create a new event ."], "references": ["create the distribution instance from a params vector ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 199, "code": "def params size ( event size , num components , name = None ) : with tf . compat . v1 . name scope ( name , 'Categorical Mixture Of One Hot Categorical params size' , [ event size , num components ] ) : return Mixture Same Family . params size ( num components , One Hot Categorical . params size ( event size , name = name ) , name = name )", "predictions": ["returns the size of the event params ."], "references": ["the number of params needed to create a single distribution ."], "bleu": 0.13859150907108325, "rouge_l": 0.4093959731543625}
{"id": 200, "code": "def new ( params , event shape = ( ) , dtype = None , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Independent Bernoulli' , [ params , event shape ] ) : params = tf . convert to tensor ( value = params , name = 'params' ) event shape = dist util . expand to vector ( tf . convert to tensor ( value = event shape , name = 'event shape' , dtype hint = tf . int32 ) , tensor name = 'event shape' ) new shape = tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , event shape , ] , axis = 0 ) dist = tfd . Independent ( tfd . Bernoulli ( logits = tf . reshape ( params , new shape ) , dtype = dtype or params . dtype . base dtype , validate args = validate args ) , reinterpreted batch ndims = tf . size ( input = event shape ) , validate args = validate args ) dist . logits = dist . distribution . logits dist . probs = dist . distribution . probs dist . logits = tfd . Bernoulli . logits dist . probs = tfd . Bernoulli . probs return dist", "predictions": ["create a new event object from a tf . v1 ."], "references": ["create the distribution instance from a params vector ."], "bleu": 0.17033186037639278, "rouge_l": 0.4073455759599332}
{"id": 201, "code": "def new ( params , event shape = ( ) , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Independent Logistic' , [ params , event shape ] ) : params = tf . convert to tensor ( value = params , name = 'params' ) event shape = dist util . expand to vector ( tf . convert to tensor ( value = event shape , name = 'event shape' , dtype hint = tf . int32 ) , tensor name = 'event shape' ) output shape = tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , event shape , ] , axis = 0 ) loc params , scale params = tf . split ( params , 2 , axis = - 1 ) return tfd . Independent ( tfd . Logistic ( loc = tf . reshape ( loc params , output shape ) , scale = tf . math . softplus ( tf . reshape ( scale params , output shape ) ) , validate args = validate args ) , reinterpreted batch ndims = tf . size ( input = event shape ) , validate args = validate args )", "predictions": ["create a new tensor from a tensor ."], "references": ["create the distribution instance from a params vector ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 202, "code": "def params size ( event shape = ( ) , name = None ) : with tf . compat . v1 . name scope ( name , 'Independent Normal params size' , [ event shape ] ) : event shape = tf . convert to tensor ( value = event shape , name = 'event shape' , dtype hint = tf . int32 ) return 2 * event size ( event shape , name = name or 'Independent Normal params size' )", "predictions": ["wrapper for tf . params ."], "references": ["the number of params needed to create a single distribution ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 203, "code": "def new ( params , event shape = ( ) , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Independent Poisson' , [ params , event shape ] ) : params = tf . convert to tensor ( value = params , name = 'params' ) event shape = dist util . expand to vector ( tf . convert to tensor ( value = event shape , name = 'event shape' , dtype hint = tf . int32 ) , tensor name = 'event shape' ) output shape = tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , event shape , ] , axis = 0 ) return tfd . Independent ( tfd . Poisson ( log rate = tf . reshape ( params , output shape ) , validate args = validate args ) , reinterpreted batch ndims = tf . size ( input = event shape ) , validate args = validate args )", "predictions": ["create a new tensor from a tensor ."], "references": ["create the distribution instance from a params vector ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 204, "code": "def new ( params , num components , component layer , validate args = False , name = None ) : with tf . compat . v1 . name scope ( name , 'Mixture Same Family' , [ params , num components , component layer ] ) : params = tf . convert to tensor ( value = params , name = 'params' ) num components = tf . convert to tensor ( value = num components , name = 'num components' , dtype hint = tf . int32 ) components dist = component layer ( tf . reshape ( params [ ... , num components : ] , tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , [ num components , - 1 ] ] , axis = 0 ) ) ) mixture dist = tfd . Categorical ( logits = params [ ... , : num components ] ) return tfd . Mixture Same Family ( mixture dist , components dist , validate args = False )", "predictions": ["creates a new component tensor ."], "references": ["create the distribution instance from a params vector ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 205, "code": "def params size ( num components , event shape = ( ) , name = None ) : return Mixture Same Family . params size ( num components , Independent Normal . params size ( event shape , name = name ) , name = name )", "predictions": ["return the size of the event params ."], "references": ["the number of params needed to create a single distribution ."], "bleu": 0.13859150907108325, "rouge_l": 0.4093959731543625}
{"id": 206, "code": "def new ( params , num components , event shape = ( ) , validate args = False , name = None ) : return Mixture Same Family . new ( params , num components , Independent Logistic ( event shape , validate args = validate args , name = name ) , validate args = validate args , name = name )", "predictions": ["create a new event ."], "references": ["create the distribution instance from a params vector ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 207, "code": "def params size ( num components , event shape = ( ) , name = None ) : return Mixture Same Family . params size ( num components , Independent Logistic . params size ( event shape , name = name ) , name = name )", "predictions": ["return the size of the event components ."], "references": ["the number of params needed to create a single distribution ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 208, "code": "def maybe check valid map values ( map values , validate args ) : assertions = [ ] message = 'Rank of map values must be 1.' if tensorshape util . rank ( map values . shape ) is not None : if tensorshape util . rank ( map values . shape ) != 1 : raise Value Error ( message ) elif validate args : assertions . append ( assert util . assert rank ( map values , 1 , message = message ) ) message = 'Size of map values must be greater than 0.' if tensorshape util . num elements ( map values . shape ) is not None : if tensorshape util . num elements ( map values . shape ) == 0 : raise Value Error ( message ) elif validate args : assertions . append ( assert util . assert greater ( tf . size ( input = map values ) , 0 , message = message ) ) if validate args : assertions . append ( assert util . assert equal ( tf . math . is strictly increasing ( map values ) , True , message = 'map values is not strictly increasing.' ) ) return assertions", "predictions": ["check that the values of the map values are valid ."], "references": ["validate map_values if validate_args == true ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 209, "code": "def as tensor ( x , name , dtype ) : return None if x is None else tf . convert to tensor ( value = x , name = name , dtype = dtype )", "predictions": ["convert x to tensor ."], "references": ["convenience to convert to tensor or leave as none ."], "bleu": 0.1501861529550426, "rouge_l": 0.5030927835051546}
{"id": 210, "code": "def get default reinterpreted batch ndims ( self , distribution ) : ndims = prefer static . rank from shape ( distribution . batch shape tensor , distribution . batch shape ) return prefer static . maximum ( 0 , ndims - 1 )", "predictions": ["returns the default reinterpreted for the given distribution ."], "references": ["computes the default value for reinterpreted_batch_ndim __init__ arg ."], "bleu": 0.21105340631872635, "rouge_l": 0.4444444444444444}
{"id": 211, "code": "def cat probs ( self , log probs ) : which softmax = tf . nn . log softmax if log probs else tf . nn . softmax cat probs = which softmax ( self . cat . logits ) cat probs = tf . unstack ( cat probs , num = self . num components , axis = - 1 ) return cat probs", "predictions": ["probability probability of the log - softmax ."], "references": ["get a list of num_components batchwise probabilities ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 212, "code": "def maybe validate args ( outcomes , logits , probs , validate args ) : assertions = [ ] def validate equal last dim ( tensor a , tensor b , message ) : if tensor a . shape . is fully defined ( ) and tensor b . shape . is fully defined ( ) : if tensor a . shape [ - 1 ] != tensor b . shape [ - 1 ] : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert equal ( tf . shape ( input = tensor a ) [ - 1 ] , tf . shape ( input = tensor b ) [ - 1 ] , message = message ) ) if logits is not None : validate equal last dim ( outcomes , logits , message = 'Last dimension of outcomes and logits must be equal size.' ) if probs is not None : validate equal last dim ( outcomes , probs , message = 'Last dimension of outcomes and probs must be equal size.' ) message = 'Rank of outcomes must be 1.' if outcomes . shape . ndims is not None : if outcomes . shape . ndims != 1 : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert rank ( outcomes , 1 , message = message ) ) message = 'Size of outcomes must be greater than 0.' if outcomes . shape . num elements ( ) is not None : if outcomes . shape . num elements ( ) == 0 : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert greater ( tf . size ( input = outcomes ) , 0 , message = message ) ) if validate args : assertions . append ( tf . compat . v1 . assert equal ( tf . math . is strictly increasing ( outcomes ) , True , message = 'outcomes is not strictly increasing.' ) ) return assertions", "predictions": ["validate that the outcomes is correctly representable ."], "references": ["validate outcomes logits and probs s shapes ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 213, "code": "def logistic regression ( features ) : coeffs = ed . Multivariate Normal Diag ( loc = tf . zeros ( features . shape [ 1 ] ) , name = \"coeffs\" ) labels = ed . Bernoulli ( logits = tf . tensordot ( features , coeffs , [ [ 1 ] , [ 0 ] ] ) , name = \"labels\" ) return labels", "predictions": ["logistic regression regression ."], "references": ["bayesian logistic regression which returns labels given features ."], "bleu": 0.14558246978804804, "rouge_l": 0.43160377358490565}
{"id": 214, "code": "def covertype ( ) : import sklearn . datasets data = sklearn . datasets . covtype . fetch covtype ( ) features = data . data labels = data . target features -= features . mean ( 0 ) features /= features . std ( 0 ) features = np . hstack ( [ features , np . ones ( [ features . shape [ 0 ] , 1 ] ) ] ) features = tf . cast ( features , dtype = tf . float32 ) , counts = np . unique ( labels , return counts = True ) specific category = np . argmax ( counts ) labels = ( labels == specific category ) labels = tf . cast ( labels , dtype = tf . int32 ) return features , labels", "predictions": ["fetch the labels of the covertype ."], "references": ["builds the covertype data set ."], "bleu": 0.2626909894424158, "rouge_l": 0.4680306905370844}
{"id": 215, "code": "def maybe assert valid concentration ( self , concentration , validate args ) : if not validate args : return concentration return distribution util . with dependencies ( [ assert util . assert positive ( concentration , message = \"Concentration parameter must be positive.\" ) , assert util . assert rank at least ( concentration , 1 , message = \"Concentration parameter must have >=1 dimensions.\" ) , assert util . assert less ( 1 , tf . shape ( input = concentration ) [ - 1 ] , message = \"Concentration parameter must have event size >= 2.\" ) , ] , concentration )", "predictions": ["assert that the concentration is valid ."], "references": ["checks the validity of the concentration parameter ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 216, "code": "def maybe assert valid sample ( self , x ) : if not self . validate args : return x return distribution util . with dependencies ( [ assert util . assert positive ( x , message = \"samples must be positive\" ) , assert util . assert near ( tf . ones ( [ ] , dtype = self . dtype ) , tf . reduce sum ( input tensor = x , axis = - 1 ) , message = \"sample last-dimension must sum to `1`\" ) , ] , x )", "predictions": ["assert that x is valid ."], "references": ["checks the validity of a sample ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 217, "code": "def make positive axis ( axis , ndims ) : axis = make list or 1d tensor ( axis ) ndims = tf . convert to tensor ( value = ndims , name = 'ndims' , dtype = tf . int32 ) ndims = tf . get static value ( ndims ) if is list like ( axis ) and ndims is not None : positive axis = [ ] for a in axis : if a < 0 : a = ndims + a positive axis . append ( a ) else : axis = tf . convert to tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) positive axis = tf . where ( axis >= 0 , axis , axis + ndims ) return positive axis", "predictions": ["make a positive axis from an axis ."], "references": ["rectify possibly negatively axis . prefer return python list ."], "bleu": 0.1643685581109115, "rouge_l": 0.21785714285714283}
{"id": 218, "code": "def squeeze ( x , axis ) : x = tf . convert to tensor ( value = x , name = 'x' ) if axis is None : return tf . squeeze ( x , axis = None ) axis = tf . convert to tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) axis += tf . zeros ( [ 1 ] , dtype = axis . dtype ) keep axis , = tf . compat . v1 . setdiff1d ( tf . range ( 0 , tf . rank ( x ) ) , axis ) return tf . reshape ( x , tf . gather ( tf . shape ( input = x ) , keep axis ) )", "predictions": ["squeeze x along axis ."], "references": ["a version of squeeze that works with dynamic axis ."], "bleu": 0.14203729394569906, "rouge_l": 0.37731958762886597}
{"id": 219, "code": "def z ( self , x ) : with tf . name scope ( \"standardize\" ) : return ( x - self . loc ) / self . scale", "predictions": ["compute the gradient of the vector ."], "references": ["standardize input x to a unit normal ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 220, "code": "def inv z ( self , z ) : with tf . name scope ( \"reconstruct\" ) : return z * self . scale + self . loc", "predictions": ["scale the vector scope ."], "references": ["reconstruct input x from a its normalized version ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 221, "code": "def semilocal linear trend transition matrix ( autoregressive coef ) : fixed entries = tf . constant ( [ [ 1. , 1. ] , [ 0. , 0. ] ] , dtype = autoregressive coef . dtype ) autoregressive coef mask = tf . constant ( [ [ 0. , 0. ] , [ 0. , 1. ] ] , dtype = autoregressive coef . dtype ) bottom right entry = ( autoregressive coef [ ... , tf . newaxis , tf . newaxis ] * autoregressive coef mask ) return tf . linalg . Linear Operator Full Matrix ( fixed entries + bottom right entry )", "predictions": ["semilocal trend matrix for linear autoregressive ."], "references": ["build the transition matrix for a semi - local linear trend model ."], "bleu": 0.12337170820562471, "rouge_l": 0.3794712286158632}
{"id": 222, "code": "def semilocal linear trend transition noise ( level scale , slope mean , slope scale , autoregressive coef ) : broadcast batch shape = dist util . get broadcast shape ( level scale , slope mean , slope scale , autoregressive coef ) broadcast ones = tf . ones ( broadcast batch shape , dtype = level scale . dtype ) scale diag = tf . stack ( [ level scale * broadcast ones , slope scale * broadcast ones ] , axis = - 1 ) bias = tf . stack ( [ tf . zeros like ( broadcast ones ) , slope mean * ( 1 - autoregressive coef ) * broadcast ones ] , axis = - 1 ) return tfd . Multivariate Normal Diag ( loc = bias , scale diag = scale diag )", "predictions": ["semilocal transition noise ."], "references": ["build the transition noise model for a semi - local linear trend model ."], "bleu": 0.041710075933029465, "rouge_l": 0.3029801324503311}
{"id": 223, "code": "def machine eps ( dtype ) : if isinstance ( dtype , tf . D Type ) : dtype = dtype . as numpy dtype ( ) return np . finfo ( dtype ) . eps", "predictions": ["return the machine - eps machine for a dtype ."], "references": ["returns the machine epsilon for the supplied dtype ."], "bleu": 0.21834177214239062, "rouge_l": 0.5313588850174217}
{"id": 224, "code": "def fix step size ( value and gradients function , val c input , active , step size shrink param ) : iter max = np . ceil ( - np . log2 ( machine eps ( val c input . x . dtype ) ) ) def cond ( i , val c , to fix ) : del val c return ( i < iter max ) & tf . reduce any ( input tensor = to fix ) def body ( i , val c , to fix ) : next c = tf . where ( to fix , val c . x * step size shrink param , val c . x ) next val c = value and gradients function ( next c ) still to fix = to fix & ~ hzl . is finite ( next val c ) return ( i + 1 , next val c , still to fix ) to fix = active & ~ hzl . is finite ( val c input ) return tf . while loop ( cond = cond , body = body , loop vars = ( 0 , val c input , to fix ) )", "predictions": ["max non - gradients non - gradients characters masking masking masking masking masking masking masking masking masking masking masking masking masking masking masking masking masking masking masking masking masking masking masking"], "references": ["shrinks the input step size until the value and grad become finite ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 225, "code": "def line search inner bisection ( value and gradients function , search interval , active , f lim ) : midpoint = ( search interval . left . x + search interval . right . x ) / 2 val mid = value and gradients function ( midpoint ) is valid mid = hzl . is finite ( val mid ) still active = active & is valid mid new failed = active & ~ is valid mid next inteval = search interval . replace ( failed = search interval . failed | new failed , func evals = search interval . func evals + 1 ) def apply update ( ) : update result = hzl . update ( value and gradients function , next inteval . left , next inteval . right , val mid , f lim , active = still active ) return Hager Zhang Line Search Result ( converged = next inteval . converged , failed = next inteval . failed | update result . failed , iterations = next inteval . iterations + update result . iteration , func evals = next inteval . func evals + update result . num evals , left = update result . left , right = update result . right ) return prefer static . cond ( tf . reduce any ( input tensor = still active ) , apply update , lambda : next inteval )", "predictions": ["all one eval eval in the dist"], "references": ["performs bisection and updates the interval ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 226, "code": "def print ( pass through tensor , values ) : flat values = [ ] for value in values : if hasattr ( value , ' fields' ) : for field in value . fields : flat values . extend ( [ field , to str ( getattr ( value , field ) ) ] ) continue if isinstance ( value , ( list , tuple ) ) : for v in value : flat values . append ( to str ( v ) ) continue flat values . append ( to str ( value ) ) return tf . compat . v1 . Print ( pass through tensor , flat values )", "predictions": ["get to function which takes a fn and if it is not a list of identifier deserialize it ."], "references": ["wrapper for tf . print which supports lists and namedtuples for printing ."], "bleu": 0.07658412276041004, "rouge_l": 0.19406150583244963}
{"id": 227, "code": "def maybe check quadrature param ( param , name , validate args ) : with tf . name scope ( \"check \" + name ) : assertions = [ ] if tensorshape util . rank ( param . shape ) is not None : if tensorshape util . rank ( param . shape ) == 0 : raise Value Error ( \"Mixing params must be a (batch of) vector; \" \"{}.rank={} is not at least one.\" . format ( name , tensorshape util . rank ( param . shape ) ) ) elif validate args : assertions . append ( assert util . assert rank at least ( param , 1 , message = ( \"Mixing params must be a (batch of) vector; \" \"{}.rank is not at least one.\" . format ( name ) ) ) ) if tensorshape util . with rank at least ( param . shape , 1 ) [ - 1 ] is not None : if tf . compat . dimension value ( param . shape [ - 1 ] ) != 1 : raise Not Implemented Error ( \"Currently only bimixtures are supported; \" \"{}.shape[-1]={} is not 1.\" . format ( name , tf . compat . dimension value ( param . shape [ - 1 ] ) ) ) elif validate args : assertions . append ( assert util . assert equal ( tf . shape ( input = param ) [ - 1 ] , 1 , message = ( \"Currently only bimixtures are supported; \" \"{}.shape[-1] is not 1.\" . format ( name ) ) ) ) if assertions : return distribution util . with dependencies ( assertions , param ) return param", "predictions": ["check checks if the params is valid . . . . . . . . . ."], "references": ["helper which checks validity of loc and scale init args ."], "bleu": 0.07994607499472013, "rouge_l": 0.14859926918392205}
{"id": 228, "code": "def determine batch event shapes ( grid , endpoint affine ) : with tf . name scope ( \"determine batch event shapes\" ) : batch shape = grid . shape [ : - 2 ] batch shape tensor = tf . shape ( input = grid ) [ : - 2 ] event shape = None event shape tensor = None def set event shape ( shape , shape tensor ) : if event shape is None : return shape , shape tensor return ( tf . broadcast static shape ( event shape , shape ) , tf . broadcast dynamic shape ( event shape tensor , shape tensor ) ) for aff in endpoint affine : if aff . shift is not None : batch shape = tf . broadcast static shape ( batch shape , aff . shift . shape [ : - 1 ] ) batch shape tensor = tf . broadcast dynamic shape ( batch shape tensor , tf . shape ( input = aff . shift ) [ : - 1 ] ) event shape , event shape tensor = set event shape ( aff . shift . shape [ - 1 : ] , tf . shape ( input = aff . shift ) [ - 1 : ] ) if aff . scale is not None : batch shape = tf . broadcast static shape ( batch shape , aff . scale . batch shape ) batch shape tensor = tf . broadcast dynamic shape ( batch shape tensor , aff . scale . batch shape tensor ( ) ) event shape , event shape tensor = set event shape ( tf . Tensor Shape ( [ aff . scale . range dimension ] ) , aff . scale . range dimension tensor ( ) [ tf . newaxis ] ) return batch shape , batch shape tensor , event shape , event shape tensor", "predictions": ["returns the event event event event event event for a size of the name of the name of the name and aff compat compat compat compat compat compat compat compat ."], "references": ["helper to infer batch_shape and event_shape ."], "bleu": 0.04317900023606586, "rouge_l": 0.11879259980525803}
{"id": 229, "code": "def interpolate loc ( grid , loc ) : if len ( loc ) != 2 : raise Not Implemented Error ( \"Currently only bimixtures are supported; \" \"len(scale)={} is not 2.\" . format ( len ( loc ) ) ) deg = tf . compat . dimension value ( tensorshape util . with rank at least ( grid . shape , 1 ) [ - 1 ] ) if deg is None : raise Value Error ( \"Num quadrature grid points must be known prior \" \"to graph execution.\" ) with tf . name scope ( \"interpolate loc\" ) : if loc is None or loc [ 0 ] is None and loc [ 1 ] is None : return [ None ] * deg w = grid [ ... , tf . newaxis , : , : ] loc = [ x [ ... , tf . newaxis ] if x is not None else None for x in loc ] if loc [ 0 ] is None : x = w [ ... , 1 , : ] * loc [ 1 ] elif loc [ 1 ] is None : x = w [ ... , 0 , : ] * loc [ 0 ] else : delta = loc [ 0 ] - loc [ 1 ] x = w [ ... , 0 , : ] * delta + loc [ 1 ] return [ x [ ... , k ] for k in range ( deg ) ]", "predictions": ["new quadrature points points points ."], "references": ["helper which interpolates between two locs ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 230, "code": "def interpolate scale ( grid , scale ) : if len ( scale ) != 2 : raise Not Implemented Error ( \"Currently only bimixtures are supported; \" \"len(scale)={} is not 2.\" . format ( len ( scale ) ) ) deg = tf . compat . dimension value ( tensorshape util . with rank at least ( grid . shape , 1 ) [ - 1 ] ) if deg is None : raise Value Error ( \"Num quadrature grid points must be known prior \" \"to graph execution.\" ) with tf . name scope ( \"interpolate scale\" ) : return [ linop add lib . add operators ( [ linop scale ( grid [ ... , k , q ] , s ) for k , s in enumerate ( scale ) ] ) [ 0 ] for q in range ( deg ) ]", "predictions": ["new new new new grid points points points points"], "references": ["helper which interpolates between two scales ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 231, "code": "def linop scale ( w , op ) : with tf . name scope ( \"linop scale\" ) : def scaled identity ( w ) : return tf . linalg . Linear Operator Scaled Identity ( num rows = op . range dimension tensor ( ) , multiplier = w , is non singular = op . is non singular , is self adjoint = op . is self adjoint , is positive definite = op . is positive definite ) if isinstance ( op , tf . linalg . Linear Operator Identity ) : return scaled identity ( w ) if isinstance ( op , tf . linalg . Linear Operator Scaled Identity ) : return scaled identity ( w * op . multiplier ) if isinstance ( op , tf . linalg . Linear Operator Diag ) : return tf . linalg . Linear Operator Diag ( diag = w [ ... , tf . newaxis ] * op . diag part ( ) , is non singular = op . is non singular , is self adjoint = op . is self adjoint , is positive definite = op . is positive definite ) if isinstance ( op , tf . linalg . Linear Operator Lower Triangular ) : return tf . linalg . Linear Operator Lower Triangular ( tril = w [ ... , tf . newaxis , tf . newaxis ] * op . to dense ( ) , is non singular = op . is non singular , is self adjoint = op . is self adjoint , is positive definite = op . is positive definite ) raise Not Implemented Error ( \"Unsupported Linop type ({})\" . format ( type ( op ) . name ) )", "predictions": ["define the v1 v1 v1 v1 v1 . . ."], "references": ["creates weighted linop from existing linop ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 232, "code": "def concat vectors ( * args ) : args = [ tf . get static value ( x ) for x in args ] if any ( vec is None for vec in args ) : return tf . concat ( args , axis = 0 ) return [ val for vec in args for val in vec ]", "predictions": ["concatenate v1 vectors from v1 to scope ."], "references": ["concatenates input vectors statically if possible ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 233, "code": "def log vector matrix ( vs , ms ) : return tf . reduce logsumexp ( input tensor = vs [ ... , tf . newaxis ] + ms , axis = - 2 )", "predictions": ["new vector params for the = 0 name ."], "references": ["multiply tensor of vectors by matrices assuming values stored are logs ."], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 234, "code": "def log matrix vector ( ms , vs ) : return tf . reduce logsumexp ( input tensor = ms + vs [ ... , tf . newaxis , : ] , axis = - 1 )", "predictions": ["params vector for the params with the same vector ."], "references": ["multiply tensor of matrices by vectors assuming values stored are logs ."], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 235, "code": "def vector matrix ( vs , ms ) : return tf . reduce sum ( input tensor = vs [ ... , tf . newaxis ] * ms , axis = - 2 )", "predictions": ["compute the new new new new new new new new new new new new new with shape ."], "references": ["multiply tensor of vectors by matrices ."], "bleu": 0.06809398432036522, "rouge_l": 0.08689458689458689}
{"id": 236, "code": "def extract log probs ( num states , dist ) : states = tf . reshape ( tf . range ( num states ) , tf . concat ( [ [ num states ] , tf . ones like ( dist . batch shape tensor ( ) ) ] , axis = 0 ) ) return distribution util . move dimension ( dist . log prob ( states ) , 0 , - 1 )", "predictions": ["new log for the log components of a tensor name name name name name name name name name name name name ."], "references": ["tabulate log probabilities from a batch of distributions ."], "bleu": 0.06964541799727335, "rouge_l": 0.20938215102974828}
{"id": 237, "code": "def marginal hidden probs ( self ) : initial log probs = tf . broadcast to ( self . log init , tf . concat ( [ self . batch shape tensor ( ) , [ self . num states ] ] , axis = 0 ) ) if self . num steps > 1 : transition log probs = self . log trans def forward step ( log probs , ) : return log vector matrix ( log probs , transition log probs ) dummy index = tf . zeros ( self . num steps - 1 , dtype = tf . float32 ) forward log probs = tf . scan ( forward step , dummy index , initializer = initial log probs , name = \"forward log probs\" ) forward log probs = tf . concat ( [ [ initial log probs ] , forward log probs ] , axis = 0 ) else : forward log probs = initial log probs [ tf . newaxis , ... ] return tf . exp ( forward log probs )", "predictions": ["the params for the params . . . . . . . . . . . . . ."], "references": ["compute marginal pdf for each individual observable ."], "bleu": 0.0712695567709093, "rouge_l": 0.1598951507208388}
{"id": 238, "code": "def choose random direction ( current state parts , batch rank , seed = None ) : seed gen = distributions . Seed Stream ( seed , salt = ' choose random direction' ) rnd direction parts = [ tf . random . normal ( tf . shape ( input = current state part ) , dtype = tf . float32 , seed = seed gen ( ) ) for current state part in current state parts ] sum squares = sum ( tf . reduce sum ( input tensor = rnd direction ** 2. , axis = tf . range ( batch rank , tf . rank ( rnd direction ) ) , keepdims = True ) for rnd direction in rnd direction parts ) rnd direction parts = [ rnd direction / tf . sqrt ( sum squares ) for rnd direction in rnd direction parts ] return rnd direction parts", "predictions": ["new random params for the num params . ."], "references": ["chooses a random direction in the event space ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 239, "code": "def maybe call fn ( fn , fn arg list , fn result = None , description = 'target log prob' ) : fn arg list = ( list ( fn arg list ) if mcmc util . is list like ( fn arg list ) else [ fn arg list ] ) if fn result is None : fn result = fn ( * fn arg list ) if not fn result . dtype . is floating : raise Type Error ( '`{}` must be a `Tensor` with `float` `dtype`.' . format ( description ) ) return fn result", "predictions": ["wrapper for the function to size of a function . . . . . . ."], "references": ["helper which computes fn_result if needed ."], "bleu": 0.07692375026049747, "rouge_l": 0.09355828220858894}
{"id": 240, "code": "def prepare args ( target log prob fn , state , step size , target log prob = None , maybe expand = False , description = 'target log prob' ) : state parts = list ( state ) if mcmc util . is list like ( state ) else [ state ] state parts = [ tf . convert to tensor ( value = s , name = 'current state' ) for s in state parts ] target log prob = maybe call fn ( target log prob fn , state parts , target log prob , description ) step sizes = ( list ( step size ) if mcmc util . is list like ( step size ) else [ step size ] ) step sizes = [ tf . convert to tensor ( value = s , name = 'step size' , dtype = target log prob . dtype ) for s in step sizes ] if len ( step sizes ) == 1 : step sizes *= len ( state parts ) if len ( state parts ) != len ( step sizes ) : raise Value Error ( 'There should be exactly one `step size` or it should ' 'have same length as `current state`.' ) def maybe flatten ( x ) : return x if maybe expand or mcmc util . is list like ( state ) else x [ 0 ] return [ maybe flatten ( state parts ) , maybe flatten ( step sizes ) , target log prob ]", "predictions": ["maybe maybe processing processing check arguments ."], "references": ["processes input args to meet list - like assumptions ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 241, "code": "def build trainable posterior ( param , initial loc fn ) : loc = tf . compat . v1 . get variable ( param . name + ' loc' , initializer = lambda : initial loc fn ( param ) , dtype = param . prior . dtype , use resource = True ) scale = tf . nn . softplus ( tf . compat . v1 . get variable ( param . name + ' scale' , initializer = lambda : - 4 * tf . ones like ( initial loc fn ( param ) ) , dtype = param . prior . dtype , use resource = True ) ) q = tfd . Normal ( loc = loc , scale = scale ) if ( param . prior . event shape . ndims is None or param . prior . event shape . ndims > 0 ) : q = tfd . Independent ( q , reinterpreted batch ndims = param . prior . event shape . ndims ) return tfd . Transformed Distribution ( q , param . bijector )", "predictions": ["as posterior for the tensor else none else none else none else ."], "references": ["built a transformed - normal variational dist over a parameter s support ."], "bleu": 0.09552040806823771, "rouge_l": 0.07692307692307691}
{"id": 242, "code": "def minimize in graph ( build loss fn , num steps = 200 , optimizer = None ) : optimizer = tf . compat . v1 . train . Adam Optimizer ( 0.1 ) if optimizer is None else optimizer def train loop body ( step ) : train op = optimizer . minimize ( build loss fn if tf . executing eagerly ( ) else build loss fn ( ) ) return tf . tuple ( tensors = [ tf . add ( step , 1 ) ] , control inputs = [ train op ] ) minimize op = tf . compat . v1 . while loop ( cond = lambda step : step < num steps , body = train loop body , loop vars = [ tf . constant ( 0 ) ] , return same structure = True ) [ 0 ] return minimize op", "predictions": ["get loss default graph default loss shape shape shape shape shape shape shape shape shape shape ."], "references": ["run an optimizer within the graph to minimize a loss function ."], "bleu": 0.0859076483566362, "rouge_l": 0.21353558926487748}
{"id": 243, "code": "def broadcast batch shape ( distributions ) : batch shape = distributions [ 0 ] . batch shape for distribution in distributions : batch shape = tf . broadcast static shape ( batch shape , distribution . batch shape ) if batch shape . is fully defined ( ) : return batch shape . as list ( ) batch shape = distributions [ 0 ] . batch shape tensor ( ) for distribution in distributions : batch shape = tf . broadcast dynamic shape ( batch shape , distribution . batch shape tensor ( ) ) return tf . convert to tensor ( value = batch shape )", "predictions": ["cat shape shape and convert shape ."], "references": ["get broadcast batch shape from distributions statically if possible ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 244, "code": "def range ( self , name = \"range\" ) : with self . name scope ( name ) : return self . high - self . low", "predictions": ["return the maybe maybe maybe the outcomes"], "references": ["high - low ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 245, "code": "def make summary statistic ( attr ) : def fn ( self ) : if any ( self . dist fn args ) : raise Value Error ( 'Can only compute ' + attr + ' when all distributions are ' 'independent; {}' . format ( self . model ) ) return self . unflatten ( getattr ( d ( ) , attr ) ( ) for d in self . dist fn wrapped ) return fn", "predictions": ["create a statistic statistic statistic statistic"], "references": ["factory for making summary statistics eg mean mode stddev ."], "bleu": 0.09536752763778475, "rouge_l": 0.0}
{"id": 246, "code": "def resolve distribution names ( dist fn args , dist names , leaf name ) : if dist names is None : dist names = [ ] else : dist names = dist names . copy ( ) n = len ( dist fn args ) dist names . extend ( [ None ] * ( n - len ( dist names ) ) ) for i , args in enumerate ( reversed ( dist fn args ) ) : if not args : continue i = n - i - 1 for j , arg name in enumerate ( args ) : dist names [ i - j - 1 ] = arg name j = 0 for i in range ( len ( dist names ) ) : i = n - i - 1 if dist names [ i ] is None : dist names [ i ] = leaf name if j == 0 else leaf name + str ( j ) j += 1 return tuple ( dist names )", "predictions": ["resolve distribution names names function"], "references": ["uses arg names to resolve distribution names ."], "bleu": 0.2948682411907622, "rouge_l": 0.44309927360774815}
{"id": 247, "code": "def get required args ( fn ) : argspec = tf inspect . getfullargspec ( fn ) args = argspec . args if tf inspect . isclass ( fn ) : args = args [ 1 : ] if argspec . defaults : args = args [ : - len ( argspec . defaults ) ] return tuple ( args )", "predictions": ["maybe assert with validate arguments if any ."], "references": ["returns the distribution s required args ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 248, "code": "def build ( self , model ) : if not isinstance ( model , collections . Sequence ) : raise Type Error ( '`model` must be `list`-like (saw: {}).' . format ( type ( model ) . name ) ) self . dist fn = model self . dist fn wrapped , self . dist fn args = zip ( * [ unify call signature ( i , dist fn ) for i , dist fn in enumerate ( model ) ] )", "predictions": ["maybe maybe model not ."], "references": ["creates dist_fn dist_fn_wrapped dist_fn_args ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 249, "code": "def entropy ( self ) : if any ( self . dist fn args ) : raise Value Error ( 'Can only compute entropy when all distributions are independent.' ) return sum ( joint distribution lib . maybe check wont broadcast ( ( d ( ) . entropy ( ) for d in self . dist fn wrapped ) , self . validate args ) )", "predictions": ["get the make make a make a tuple of distributions value . . ."], "references": ["shannon entropy in nats ."], "bleu": 0.08839374326825923, "rouge_l": 0.11509433962264153}
{"id": 250, "code": "def prepare args ( log likelihood fn , state , log likelihood = None , description = 'log likelihood' ) : state parts = list ( state ) if mcmc util . is list like ( state ) else [ state ] state parts = [ tf . convert to tensor ( s , name = 'current state' ) for s in state parts ] log likelihood = maybe call fn ( log likelihood fn , state parts , log likelihood , description ) return [ state parts , log likelihood ]", "predictions": ["squeeze the parameters to be sent to the requested log ."], "references": ["processes input args to meet list - like assumptions ."], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 251, "code": "def vector size to square matrix size ( d , validate args , name = None ) : if isinstance ( d , ( float , int , np . generic , np . ndarray ) ) : n = ( - 1 + np . sqrt ( 1 + 8 * d ) ) / 2. if float ( int ( n ) ) != n : raise Value Error ( \"Vector length is not a triangular number.\" ) return int ( n ) else : with tf . name scope ( name or \"vector size to square matrix size\" ) as name : n = ( - 1. + tf . sqrt ( 1 + 8. * tf . cast ( d , dtype = tf . float32 ) ) ) / 2. if validate args : with tf . control dependencies ( [ assert util . assert equal ( tf . cast ( tf . cast ( n , dtype = tf . int32 ) , dtype = tf . float32 ) , n , message = \"Vector length is not a triangular number\" ) ] ) : n = tf . identity ( n ) return tf . cast ( n , d . dtype )", "predictions": ["size size size size . . . . . . . . . . . . . . . . ."], "references": ["convert a vector size to a matrix size ."], "bleu": 0.0821610732492254, "rouge_l": 0.21554770318021202}
{"id": 252, "code": "def argsort ( values , axis = - 1 , direction = 'ASCENDING' , stable = False , name = None ) : if direction == 'ASCENDING' : pass elif direction == 'DESCENDING' : values = np . negative ( values ) else : raise Value Error ( 'Unrecognized direction: {}.' . format ( direction ) ) return np . argsort ( values , axis , kind = 'stable' if stable else 'quicksort' )", "predictions": ["return - . values"], "references": ["numpy implementation of tf . argsort ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 253, "code": "def sort ( values , axis = - 1 , direction = 'ASCENDING' , stable = False , name = None ) : if direction == 'ASCENDING' : pass elif direction == 'DESCENDING' : values = np . negative ( values ) else : raise Value Error ( 'Unrecognized direction: {}.' . format ( direction ) ) result = np . sort ( values , axis , kind = 'stable' if stable else 'quicksort' ) if direction == 'DESCENDING' : return np . negative ( result ) return result", "predictions": ["sort the trend of the trend"], "references": ["numpy implementation of tf . sort ."], "bleu": 0.20693220168471366, "rouge_l": 0.1517412935323383}
{"id": 254, "code": "def ndtr ( x ) : half sqrt 2 = tf . constant ( 0.5 * np . sqrt ( 2. ) , dtype = x . dtype , name = \"half sqrt 2\" ) w = x * half sqrt 2 z = tf . abs ( w ) y = tf . where ( tf . less ( z , half sqrt 2 ) , 1. + tf . math . erf ( w ) , tf . where ( tf . greater ( w , 0. ) , 2. - tf . math . erfc ( z ) , tf . math . erfc ( z ) ) ) return 0.5 * y", "predictions": ["ndtr - vector version of trend mean trend mean mean mean mean mean mean mean mean mean mean mean mean mean mean trend mean and tf mean ."], "references": ["implements ndtr core logic ."], "bleu": 0.0478968583748614, "rouge_l": 0.13863636363636364}
{"id": 255, "code": "def log ndtr asymptotic series ( x , series order ) : npdt = dtype util . as numpy dtype ( x . dtype ) if series order <= 0 : return npdt ( 1 ) x 2 = tf . square ( x ) even sum = tf . zeros like ( x ) odd sum = tf . zeros like ( x ) x 2n = x 2 for n in range ( 1 , series order + 1 ) : y = npdt ( double factorial ( 2 * n - 1 ) ) / x 2n if n % 2 : odd sum += y else : even sum += y x 2n *= x 2 return 1. + even sum - odd sum", "predictions": ["machine dtype of x x"], "references": ["calculates the asymptotic series used in log_ndtr ."], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 256, "code": "def text messages joint log prob ( count data , lambda 1 , lambda 2 , tau ) : alpha = ( 1. / tf . reduce mean ( input tensor = count data ) ) rv lambda = tfd . Exponential ( rate = alpha ) rv tau = tfd . Uniform ( ) lambda = tf . gather ( [ lambda 1 , lambda 2 ] , indices = tf . cast ( tau * tf . cast ( tf . size ( input = count data ) , dtype = tf . float32 ) <= tf . cast ( tf . range ( tf . size ( input = count data ) ) , dtype = tf . float32 ) , dtype = tf . int32 ) ) rv observation = tfd . Poisson ( rate = lambda ) return ( rv lambda . log prob ( lambda 1 ) + rv lambda . log prob ( lambda 2 ) + rv tau . log prob ( tau ) + tf . reduce sum ( input tensor = rv observation . log prob ( count data ) ) )", "predictions": ["joint text messages for the text log ."], "references": ["joint log probability function ."], "bleu": 0.19070828081828378, "rouge_l": 0.48157894736842105}
{"id": 257, "code": "def outer squared difference ( x , y ) : z = x - y return z [ ... , tf . newaxis , : ] * z [ ... , tf . newaxis ]", "predictions": ["outer difference between x and y and y ."], "references": ["convenience function analogous to tf . squared_difference ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 258, "code": "def split covariance into marginals ( covariance , block sizes ) : start dim = 0 marginals = [ ] for size in block sizes : end dim = start dim + size marginals . append ( covariance [ ... , start dim : end dim , start dim : end dim ] ) start dim = end dim return marginals", "predictions": ["split covariance into marginals ."], "references": ["split a covariance matrix into block - diagonal marginals of given sizes ."], "bleu": 0.07254224910650854, "rouge_l": 0.5143338954468802}
{"id": 259, "code": "def numpy text ( tensor , is repr = False ) : if tensor . dtype . is numpy compatible : text = repr ( tensor . numpy ( ) ) if is repr else str ( tensor . numpy ( ) ) else : text = \"<unprintable>\" if \"\\n\" in text : text = \"\\n\" + text return text", "predictions": ["return a numpy text representation of a tensor ."], "references": ["human - readable representation of a tensor s numpy value ."], "bleu": 0.34214468780181806, "rouge_l": 0.4911433172302737}
{"id": 260, "code": "def sample shape ( self ) : if isinstance ( self . sample shape , tf . Tensor ) : return tf . Tensor Shape ( tf . get static value ( self . sample shape ) ) return tf . Tensor Shape ( self . sample shape )", "predictions": ["sample shape of tf . shape ."], "references": ["sample shape of random variable as a tensorshape ."], "bleu": 0.27470644934024185, "rouge_l": 0.48897795591182364}
{"id": 261, "code": "def value ( self ) : if self . value is None : try : self . value = self . distribution . sample ( self . sample shape tensor ( ) ) except Not Implemented Error : raise Not Implemented Error ( \"sample is not implemented for {0}. You must either pass in the \" \"value argument or implement sample for {0}.\" . format ( self . distribution . class . name ) ) return self . value", "predictions": ["returns the value of the distribution ."], "references": ["get tensor that the random variable corresponds to ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 262, "code": "def numpy ( self ) : if not isinstance ( self . value , ops . Eager Tensor ) : raise Not Implemented Error ( \"value argument must be a Eager Tensor.\" ) return self . value . numpy ( )", "predictions": ["returns a numpy array containing the value of the value ."], "references": ["value as numpy array only available for tf eager ."], "bleu": 0.17033186037639278, "rouge_l": 0.28818897637795277}
{"id": 263, "code": "def uniform unit norm ( dimension , shape , dtype , seed ) : raw = normal . Normal ( loc = dtype util . as numpy dtype ( dtype ) ( 0 ) , scale = dtype util . as numpy dtype ( dtype ) ( 1 ) ) . sample ( tf . concat ( [ shape , [ dimension ] ] , axis = 0 ) , seed = seed ( ) ) unit norm = raw / tf . norm ( tensor = raw , ord = 2 , axis = - 1 ) [ ... , tf . newaxis ] return unit norm", "predictions": ["uniform unit norm norm ."], "references": ["returns a batch of points chosen uniformly from the unit hypersphere ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 264, "code": "def common dtype ( args list , preferred dtype = None ) : dtype = None preferred dtype = ( None if preferred dtype is None else tf . as dtype ( preferred dtype ) ) for a in tf . nest . flatten ( args list ) : if hasattr ( a , 'dtype' ) : dt = tf . as dtype ( a . dtype ) else : continue if dtype is None : dtype = dt elif dtype != dt : raise Type Error ( 'Found incompatible dtypes, {} and {}.' . format ( dtype , dt ) ) if dtype is None and preferred dtype is None : return None return ( preferred dtype if dtype is None else dtype ) . as numpy dtype", "predictions": ["returns the common dtype of a list of args ."], "references": ["returns explict dtype from args_list if exists else preferred_dtype ."], "bleu": 0.14991106946711685, "rouge_l": 0.3}
{"id": 265, "code": "def make summary statistic ( attr ) : def fn ( self , * * kwargs ) : \"\"\"Implements summary statistic, eg, mean, stddev, mode.\"\"\" x = getattr ( self . distribution , attr ) ( * * kwargs ) shape = prefer static . concat ( [ self . distribution . batch shape tensor ( ) , prefer static . ones ( prefer static . rank from shape ( self . sample shape ) , dtype = self . sample shape . dtype ) , self . distribution . event shape tensor ( ) , ] , axis = 0 ) x = tf . reshape ( x , shape = shape ) shape = prefer static . concat ( [ self . distribution . batch shape tensor ( ) , self . sample shape , self . distribution . event shape tensor ( ) , ] , axis = 0 ) return tf . broadcast to ( x , shape ) return fn", "predictions": ["create a summary of the summary summary ."], "references": ["factory for implementing summary statistics eg mean stddev mode ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 266, "code": "def broadcast to ( tensor to broadcast , target tensors ) : output = tensor to broadcast for tensor in target tensors : output += tf . zeros like ( tensor ) return output", "predictions": ["given a tensor and a tensor transform it to the given target ."], "references": ["helper to broadcast a tensor using a list of target tensors ."], "bleu": 0.1553712569276035, "rouge_l": 0.4029062087186261}
{"id": 267, "code": "def pdf at peak ( self ) : return ( self . peak - self . low ) / ( self . high - self . low )", "predictions": ["returns the pdf at the peak of the peak ."], "references": ["pdf evaluated at the peak ."], "bleu": 0.3087818953963449, "rouge_l": 0.6545064377682404}
{"id": 268, "code": "def effective sample size single state ( states , filter beyond lag , filter threshold ) : with tf . compat . v1 . name scope ( 'effective sample size single state' , values = [ states , filter beyond lag , filter threshold ] ) : states = tf . convert to tensor ( value = states , name = 'states' ) dt = states . dtype auto corr = stats . auto correlation ( states , axis = 0 , max lags = filter beyond lag ) if filter threshold is not None : filter threshold = tf . convert to tensor ( value = filter threshold , dtype = dt , name = 'filter threshold' ) mask = auto corr < filter threshold mask = tf . cast ( mask , dtype = dt ) mask = tf . cumsum ( mask , axis = 0 ) mask = tf . maximum ( 1. - mask , 0. ) auto corr *= mask n = axis size ( states , axis = 0 ) k = tf . range ( 0. , axis size ( auto corr , axis = 0 ) ) nk factor = ( n - k ) / n if auto corr . shape . ndims is not None : new shape = [ - 1 ] + [ 1 ] * ( auto corr . shape . ndims - 1 ) else : new shape = tf . concat ( ( [ - 1 ] , tf . ones ( [ tf . rank ( auto corr ) - 1 ] , dtype = tf . int32 ) ) , axis = 0 ) nk factor = tf . reshape ( nk factor , new shape ) return n / ( - 1 + 2 * tf . reduce sum ( input tensor = nk factor * auto corr , axis = 0 ) )", "predictions": ["effective effective sample size ."], "references": ["ess computation for one single tensor argument ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 269, "code": "def potential scale reduction single state ( state , independent chain ndims ) : with tf . compat . v1 . name scope ( 'potential scale reduction single state' , values = [ state , independent chain ndims ] ) : state = tf . convert to tensor ( value = state , name = 'state' ) sample ndims = 1 sample axis = tf . range ( 0 , sample ndims ) chain axis = tf . range ( sample ndims , sample ndims + independent chain ndims ) sample and chain axis = tf . range ( 0 , sample ndims + independent chain ndims ) n = axis size ( state , sample axis ) m = axis size ( state , chain axis ) b div n = reduce variance ( tf . reduce mean ( input tensor = state , axis = sample axis , keepdims = True ) , sample and chain axis , biased = False ) w = tf . reduce mean ( input tensor = reduce variance ( state , sample axis , keepdims = True , biased = True ) , axis = sample and chain axis ) sigma 2 plus = w + b div n return ( ( m + 1. ) / m ) * sigma 2 plus / w - ( n - 1. ) / ( m * n )", "predictions": ["potential reduction reduction reduction reduction ."], "references": ["potential_scale_reduction for one single state tensor ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 270, "code": "def axis size ( x , axis = None ) : if axis is None : return tf . cast ( tf . size ( input = x ) , x . dtype ) return tf . cast ( tf . reduce prod ( input tensor = tf . gather ( tf . shape ( input = x ) , axis ) ) , x . dtype )", "predictions": ["size x along axis ."], "references": ["get number of elements of x in axis as type x . dtype ."], "bleu": 0.053667245469253895, "rouge_l": 0.2909379968203497}
{"id": 271, "code": "def broadcast maybelist arg ( states , secondary arg , name ) : if is list like ( secondary arg ) : if len ( secondary arg ) != len ( states ) : raise Value Error ( 'Argument `%s` was a list of different length ({}) than ' '`states` ({})' . format ( name , len ( states ) ) ) else : secondary arg = [ secondary arg ] * len ( states ) return secondary arg", "predictions": ["broadcast a list of secondary arguments to the given states ."], "references": ["broadcast a listable secondary_arg to that of states ."], "bleu": 0.20504572236241866, "rouge_l": 0.5091819699499166}
{"id": 272, "code": "def remove ( self , field ) : return Mapping ( x = None if field == \"x\" else self . x , y = None if field == \"y\" else self . y , ildj = self . ildj , kwargs = self . kwargs )", "predictions": ["remove a field from the model ."], "references": ["to support weak referencing removes cache key from the cache value ."], "bleu": 0.1285981829222983, "rouge_l": 0.30148270181219106}
{"id": 273, "code": "def merge ( self , old , new , use equals = False ) : if old is None : return new if new is None : return old if ( old == new ) if use equals else ( old is new ) : return old raise Value Error ( \"Incompatible values: %s != %s\" % ( old , new ) )", "predictions": ["merges old value into new one ."], "references": ["helper to merge which handles merging one value ."], "bleu": 0.16599826150636804, "rouge_l": 0.24448897795591182}
{"id": 274, "code": "def deep tuple ( self , x ) : if isinstance ( x , dict ) : return self . deep tuple ( tuple ( sorted ( x . items ( ) ) ) ) elif isinstance ( x , ( list , tuple ) ) : return tuple ( map ( self . deep tuple , x ) ) return x", "predictions": ["recursively convert a tuple into a tuple of tuples ."], "references": ["converts nested tuple list or dict to nested tuple ."], "bleu": 0.14991106946711685, "rouge_l": 0.3}
{"id": 275, "code": "def vggconv block ( x , filters , kernel , stride , kernel posterior fn ) : out = tfp . layers . Convolution2D Flipout ( filters , kernel , padding = 'same' , kernel posterior fn = kernel posterior fn ) ( x ) out = tf . keras . layers . Batch Normalization ( ) ( out ) out = tf . keras . layers . Activation ( 'relu' ) ( out ) out = tfp . layers . Convolution2D Flipout ( filters , kernel , padding = 'same' , kernel posterior fn = kernel posterior fn ) ( out ) out = tf . keras . layers . Batch Normalization ( ) ( out ) out = tf . keras . layers . Activation ( 'relu' ) ( out ) out = tf . keras . layers . Max Pooling2D ( pool size = ( 2 , 2 ) , strides = stride ) ( out ) return out", "predictions": ["vggconv block of vggconv ."], "references": ["network block for vgg ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 276, "code": "def embed no none gradient check ( value and gradients fn ) : @ functools . wraps ( value and gradients fn ) def func wrapped ( * args , * * kwargs ) : \"\"\"Wrapped function which checks for None gradients.\"\"\" value , grads = value and gradients fn ( * args , * * kwargs ) if any ( grad is None for grad in grads ) : raise Value Error ( \"Gradient is None for a state.\" ) return value , grads return func wrapped", "predictions": ["decorator to check if a function is none none ."], "references": ["wraps value and gradients function to assist with none gradients ."], "bleu": 0.1434272783816789, "rouge_l": 0.28328173374613}
{"id": 277, "code": "def has no u turn ( state one , state two , momentum ) : dot product = sum ( [ tf . reduce sum ( input tensor = ( s1 - s2 ) * m ) for s1 , s2 , m in zip ( state one , state two , momentum ) ] ) return dot product > 0", "predictions": ["returns true if state has no u u ."], "references": ["if two given states and momentum do not exhibit a u - turn pattern ."], "bleu": 0.08617428905281956, "rouge_l": 0.23921568627450981}
{"id": 278, "code": "def leapfrog ( value and gradients fn , current state , current grads target log prob , current momentum , step size ) : mid momentum = [ m + 0.5 * step * g for m , step , g in zip ( current momentum , step size , current grads target log prob ) ] next state = [ s + step * m for s , step , m in zip ( current state , step size , mid momentum ) ] next target log prob , next grads target log prob = value and gradients fn ( * next state ) next momentum = [ m + 0.5 * step * g for m , step , g in zip ( mid momentum , step size , next grads target log prob ) ] return [ next state , next target log prob , next grads target log prob , next momentum , ]", "predictions": ["leapfrog value and gradients between target and gradients ."], "references": ["runs one step of leapfrog integration ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 279, "code": "def log joint ( current target log prob , current momentum ) : momentum log prob = - sum ( [ tf . reduce sum ( input tensor = 0.5 * ( m ** 2. ) ) for m in current momentum ] ) return current target log prob + momentum log prob", "predictions": ["log joint log - joint log - log joint ."], "references": ["log - joint probability given a state s log - probability and momentum ."], "bleu": 0.19437836223653274, "rouge_l": 0.48541114058355433}
{"id": 280, "code": "def random bernoulli ( shape , probs , dtype = tf . int32 , seed = None , name = None ) : with tf . compat . v1 . name scope ( name , \"random bernoulli\" , [ shape , probs ] ) : probs = tf . convert to tensor ( value = probs ) random uniform = tf . random . uniform ( shape , dtype = probs . dtype , seed = seed ) return tf . cast ( tf . less ( random uniform , probs ) , dtype )", "predictions": ["random bernoulli bernoulli ."], "references": ["returns samples from a bernoulli distribution ."], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 281, "code": "def expand as args ( args ) : return ( isinstance ( args , collections . Sequence ) and not is namedtuple ( args ) and not force leaf ( args ) )", "predictions": ["expand a leaf - separated list ."], "references": ["returns true if args should be expanded as * args ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 282, "code": "def nested convert to tensor ( struct , dtype = None , name = None ) : if dtype is not None or not tf . nest . is nested ( struct ) : return tf . convert to tensor ( struct , dtype = dtype ) if maybe convertible to tensor ( struct ) : try : return tf . convert to tensor ( value = struct , name = name ) except ( Value Error , Type Error ) : pass shallow struct = get shallow structure ( struct ) return nest . map structure up to ( shallow struct , lambda s : nested convert to tensor ( s , name = name ) , struct )", "predictions": ["convert a struct to a tensor ."], "references": ["eagerly converts struct to tensor recursing upon failure ."], "bleu": 0.20873176328735715, "rouge_l": 0.48897795591182364}
{"id": 283, "code": "def get tensor like attributes ( ) : attrs = dict ( ) attrs . update ( ( attr , wrap method ( tf . Tensor , attr ) ) for attr in tf . Tensor . OVERLOADABLE OPERATORS . union ( { ' iter ' } ) ) attrs . update ( ( attr , getattr ( tf . Tensor , attr ) ) for attr in { ' nonzero ' , ' bool ' , ' array priority ' } ) return attrs", "predictions": ["get the attributes of the tensor like the tf ."], "references": ["returns tensor attributes related to shape and python builtins ."], "bleu": 0.14991106946711685, "rouge_l": 0.2}
{"id": 284, "code": "def pack images ( images , rows , cols ) : shape = tf . shape ( input = images ) width = shape [ - 3 ] height = shape [ - 2 ] depth = shape [ - 1 ] images = tf . reshape ( images , ( - 1 , width , height , depth ) ) batch = tf . shape ( input = images ) [ 0 ] rows = tf . minimum ( rows , batch ) cols = tf . minimum ( batch // rows , cols ) images = images [ : rows * cols ] images = tf . reshape ( images , ( rows , cols , width , height , depth ) ) images = tf . transpose ( a = images , perm = [ 0 , 2 , 1 , 3 , 4 ] ) images = tf . reshape ( images , [ 1 , rows * width , cols * height , depth ] ) return images", "predictions": ["pack images into tf ."], "references": ["helper utility to make a field of images ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 285, "code": "def download ( directory , filename ) : filepath = os . path . join ( directory , filename ) if tf . io . gfile . exists ( filepath ) : return filepath if not tf . io . gfile . exists ( directory ) : tf . io . gfile . makedirs ( directory ) url = os . path . join ( ROOT PATH , filename ) print ( \"Downloading %s to %s\" % ( url , filepath ) ) urllib . request . urlretrieve ( url , filepath ) return filepath", "predictions": ["download and download a file from disk ."], "references": ["downloads a file ."], "bleu": 0.22679164443904004, "rouge_l": 0.5319767441860466}
{"id": 286, "code": "def build fake input fns ( batch size ) : random sample = np . random . rand ( batch size , * IMAGE SHAPE ) . astype ( \"float32\" ) def train input fn ( ) : dataset = tf . data . Dataset . from tensor slices ( random sample ) . map ( lambda row : ( row , 0 ) ) . batch ( batch size ) . repeat ( ) return tf . compat . v1 . data . make one shot iterator ( dataset ) . get next ( ) def eval input fn ( ) : dataset = tf . data . Dataset . from tensor slices ( random sample ) . map ( lambda row : ( row , 0 ) ) . batch ( batch size ) return tf . compat . v1 . data . make one shot iterator ( dataset ) . get next ( ) return train input fn , eval input fn", "predictions": ["build a train of images ."], "references": ["builds fake mnist - style data for unit testing ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 287, "code": "def build input fns ( data dir , batch size ) : def train input fn ( ) : dataset = static mnist dataset ( data dir , \"train\" ) dataset = dataset . shuffle ( 50000 ) . repeat ( ) . batch ( batch size ) return tf . compat . v1 . data . make one shot iterator ( dataset ) . get next ( ) def eval input fn ( ) : eval dataset = static mnist dataset ( data dir , \"valid\" ) eval dataset = eval dataset . batch ( batch size ) return tf . compat . v1 . data . make one shot iterator ( eval dataset ) . get next ( ) return train input fn , eval input fn", "predictions": ["build mnist dataset ."], "references": ["builds an iterator switching between train and heldout data ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 288, "code": "def validate block sizes ( block sizes , bijectors , validate args ) : block sizes shape = block sizes . shape if tensorshape util . is fully defined ( block sizes shape ) : if ( tensorshape util . rank ( block sizes shape ) != 1 or ( tensorshape util . num elements ( block sizes shape ) != len ( bijectors ) ) ) : raise Value Error ( '`block sizes` must be `None`, or a vector of the same length as ' '`bijectors`. Got a `Tensor` with shape {} and `bijectors` of ' 'length {}' . format ( block sizes shape , len ( bijectors ) ) ) return block sizes elif validate args : message = ( '`block sizes` must be `None`, or a vector of the same length ' 'as `bijectors`.' ) with tf . control dependencies ( [ assert util . assert equal ( tf . size ( input = block sizes ) , len ( bijectors ) , message = message ) , assert util . assert equal ( tf . rank ( block sizes ) , 1 ) ] ) : return tf . identity ( block sizes ) else : return block sizes", "predictions": ["text for messages 2 2 - d messages 2 2 2 2 2 - d - dimensional messages ."], "references": ["helper to validate block sizes ."], "bleu": 0.06439931429457924, "rouge_l": 0.0882778581765557}
{"id": 289, "code": "def maybe check wont broadcast ( flat xs , validate args ) : flat xs = tuple ( flat xs ) if not validate args : return flat xs msg = 'Broadcasting probably indicates an error in model specification.' s = tuple ( x . shape for x in flat xs ) if all ( tensorshape util . is fully defined ( s ) for s in s ) : if not all ( a == b for a , b in zip ( s [ 1 : ] , s [ : - 1 ] ) ) : raise Value Error ( msg ) return flat xs assertions = [ assert util . assert equal ( a , b , message = msg ) for a , b in zip ( s [ 1 : ] , s [ : - 1 ] ) ] with tf . control dependencies ( assertions ) : return tuple ( tf . identity ( x ) for x in flat xs )", "predictions": ["squared squared broadcast function for difference between difference and b ."], "references": ["verifies that parts don t broadcast ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 290, "code": "def maybe call volatility fn and grads ( volatility fn , state , volatility fn results = None , grads volatility fn = None , sample shape = None , parallel iterations = 10 ) : state parts = list ( state ) if mcmc util . is list like ( state ) else [ state ] needs volatility fn gradients = grads volatility fn is None if volatility fn results is None : volatility fn results = volatility fn ( * state parts ) volatility fn results = ( list ( volatility fn results ) if mcmc util . is list like ( volatility fn results ) else [ volatility fn results ] ) if len ( volatility fn results ) == 1 : volatility fn results *= len ( state parts ) if len ( state parts ) != len ( volatility fn results ) : raise Value Error ( '`volatility fn` should return a tensor or a list ' 'of the same length as `current state`.' ) volatility fn results = maybe broadcast volatility ( volatility fn results , state parts ) if grads volatility fn is None : [ , grads volatility fn , ] = diag jacobian ( xs = state parts , ys = volatility fn results , sample shape = sample shape , parallel iterations = parallel iterations , fn = volatility fn ) if needs volatility fn gradients : grads volatility fn = [ 2. * g * volatility if g is not None else tf . zeros like ( fn arg , dtype = fn arg . dtype . base dtype ) for g , volatility , fn arg in zip ( grads volatility fn , volatility fn results , state parts ) ] return volatility fn results , grads volatility fn", "predictions": ["covariance function for into into account s grads ."], "references": ["helper which computes volatility_fn results and grads if needed ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 291, "code": "def maybe broadcast volatility ( volatility parts , state parts ) : return [ v + tf . zeros like ( sp , dtype = sp . dtype . base dtype ) for v , sp in zip ( volatility parts , state parts ) ]", "predictions": ["text for all volatility parts ."], "references": ["helper to broadcast volatility_parts to the shape of state_parts ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 292, "code": "def prepare args ( target log prob fn , volatility fn , state , step size , target log prob = None , grads target log prob = None , volatility = None , grads volatility fn = None , diffusion drift = None , parallel iterations = 10 ) : state parts = list ( state ) if mcmc util . is list like ( state ) else [ state ] [ target log prob , grads target log prob , ] = mcmc util . maybe call fn and grads ( target log prob fn , state parts , target log prob , grads target log prob ) [ volatility parts , grads volatility , ] = maybe call volatility fn and grads ( volatility fn , state parts , volatility , grads volatility fn , distribution util . prefer static shape ( target log prob ) , parallel iterations ) step sizes = ( list ( step size ) if mcmc util . is list like ( step size ) else [ step size ] ) step sizes = [ tf . convert to tensor ( value = s , name = 'step size' , dtype = target log prob . dtype ) for s in step sizes ] if len ( step sizes ) == 1 : step sizes *= len ( state parts ) if len ( state parts ) != len ( step sizes ) : raise Value Error ( 'There should be exactly one `step size` or it should ' 'have same length as `current state`.' ) if diffusion drift is None : diffusion drift parts = get drift ( step sizes , volatility parts , grads volatility , grads target log prob ) else : diffusion drift parts = ( list ( diffusion drift ) if mcmc util . is list like ( diffusion drift ) else [ diffusion drift ] ) if len ( state parts ) != len ( diffusion drift ) : raise Value Error ( 'There should be exactly one `diffusion drift` or it ' 'should have same length as list-like `current state`.' ) return [ state parts , step sizes , target log prob , grads target log prob , volatility parts , grads volatility , diffusion drift parts , ]", "predictions": ["sample and convert the shape of the self . . ."], "references": ["helper which processes input args to meet list - like assumptions ."], "bleu": 0.10400927574124633, "rouge_l": 0.08628005657708629}
{"id": 293, "code": "def validate init args statically ( distribution , batch shape ) : if tensorshape util . rank ( batch shape . shape ) is not None : if tensorshape util . rank ( batch shape . shape ) != 1 : raise Value Error ( \"`batch shape` must be a vector \" \"(saw rank: {}).\" . format ( tensorshape util . rank ( batch shape . shape ) ) ) batch shape static = tensorshape util . constant value as shape ( batch shape ) batch size static = tensorshape util . num elements ( batch shape static ) dist batch size static = tensorshape util . num elements ( distribution . batch shape ) if batch size static is not None and dist batch size static is not None : if batch size static != dist batch size static : raise Value Error ( \"`batch shape` size ({}) must match \" \"`distribution.batch shape` size ({}).\" . format ( batch size static , dist batch size static ) ) if tensorshape util . dims ( batch shape static ) is not None : if any ( tf . compat . dimension value ( dim ) is not None and tf . compat . dimension value ( dim ) < 1 for dim in batch shape static ) : raise Value Error ( \"`batch shape` elements must be >=-1.\" )", "predictions": ["validates that the self - statically parameters are statically ."], "references": ["helper to __init__ which makes or raises assertions ."], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 294, "code": "def sample shape ( self , x ) : x ndims = ( tf . rank ( x ) if tensorshape util . rank ( x . shape ) is None else tensorshape util . rank ( x . shape ) ) event ndims = ( tf . size ( input = self . event shape tensor ( ) ) if tensorshape util . rank ( self . event shape ) is None else tensorshape util . rank ( self . event shape ) ) batch ndims = ( tf . size ( input = self . batch shape unexpanded ) if tensorshape util . rank ( self . batch shape ) is None else tensorshape util . rank ( self . batch shape ) ) sample ndims = x ndims - batch ndims - event ndims if isinstance ( sample ndims , int ) : static sample shape = x . shape [ : sample ndims ] else : static sample shape = tf . Tensor Shape ( None ) if tensorshape util . is fully defined ( static sample shape ) : sample shape = np . int32 ( static sample shape ) else : sample shape = tf . shape ( input = x ) [ : sample ndims ] return sample shape , static sample shape", "predictions": ["numpy implementation of numpy s numpy arrays ."], "references": ["computes graph and static sample_shape ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 295, "code": "def call reshape input output ( self , fn , x , extra kwargs = None ) : with tf . control dependencies ( self . runtime assertions + self . validate sample arg ( x ) ) : sample shape , static sample shape = self . sample shape ( x ) old shape = tf . concat ( [ sample shape , self . distribution . batch shape tensor ( ) , self . event shape tensor ( ) , ] , axis = 0 ) x reshape = tf . reshape ( x , old shape ) result = fn ( x reshape , * * extra kwargs ) if extra kwargs else fn ( x reshape ) new shape = tf . concat ( [ sample shape , self . batch shape unexpanded , ] , axis = 0 ) result = tf . reshape ( result , new shape ) if ( tensorshape util . rank ( static sample shape ) is not None and tensorshape util . rank ( self . batch shape ) is not None ) : new shape = tensorshape util . concatenate ( static sample shape , self . batch shape ) tensorshape util . set shape ( result , new shape ) return result", "predictions": ["unit output and unit output ."], "references": ["calls fn appropriately reshaping its input x and output ."], "bleu": 0.16038842424444547, "rouge_l": 0.3588235294117647}
{"id": 296, "code": "def call and reshape output ( self , fn , event shape list = None , static event shape list = None , extra kwargs = None ) : with tf . control dependencies ( self . runtime assertions ) : if event shape list is None : event shape list = [ self . event shape tensor ( ) ] if static event shape list is None : static event shape list = [ self . event shape ] new shape = tf . concat ( [ self . batch shape unexpanded ] + event shape list , axis = 0 ) result = tf . reshape ( fn ( * * extra kwargs ) if extra kwargs else fn ( ) , new shape ) if ( tensorshape util . rank ( self . batch shape ) is not None and tensorshape util . rank ( self . event shape ) is not None ) : event shape = tf . Tensor Shape ( [ ] ) for rss in static event shape list : event shape = tensorshape util . concatenate ( event shape , rss ) static shape = tensorshape util . concatenate ( self . batch shape , event shape ) tensorshape util . set shape ( result , static shape ) return result", "predictions": ["common convolution convolution convolution convolution ."], "references": ["calls fn and appropriately reshapes its output ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 297, "code": "def validate sample arg ( self , x ) : with tf . name scope ( \"validate sample arg\" ) : x ndims = ( tf . rank ( x ) if tensorshape util . rank ( x . shape ) is None else tensorshape util . rank ( x . shape ) ) event ndims = ( tf . size ( input = self . event shape tensor ( ) ) if tensorshape util . rank ( self . event shape ) is None else tensorshape util . rank ( self . event shape ) ) batch ndims = ( tf . size ( input = self . batch shape unexpanded ) if tensorshape util . rank ( self . batch shape ) is None else tensorshape util . rank ( self . batch shape ) ) expected batch event ndims = batch ndims + event ndims if ( isinstance ( x ndims , int ) and isinstance ( expected batch event ndims , int ) ) : if x ndims < expected batch event ndims : raise Not Implemented Error ( \"Broadcasting is not supported; too few batch and event dims \" \"(expected at least {}, saw {}).\" . format ( expected batch event ndims , x ndims ) ) ndims assertion = [ ] elif self . validate args : ndims assertion = [ assert util . assert greater equal ( x ndims , expected batch event ndims , message = ( \"Broadcasting is not supported; too few \" \"batch and event dims.\" ) , name = \"assert batch and event ndims large enough\" ) , ] if ( tensorshape util . is fully defined ( self . batch shape ) and tensorshape util . is fully defined ( self . event shape ) ) : expected batch event shape = np . int32 ( tensorshape util . concatenate ( self . batch shape , self . event shape ) ) else : expected batch event shape = tf . concat ( [ self . batch shape tensor ( ) , self . event shape tensor ( ) , ] , axis = 0 ) sample ndims = x ndims - expected batch event ndims if isinstance ( sample ndims , int ) : sample ndims = max ( sample ndims , 0 ) if ( isinstance ( sample ndims , int ) and tensorshape util . is fully defined ( x . shape [ sample ndims : ] ) ) : actual batch event shape = np . int32 ( x . shape [ sample ndims : ] ) else : sample ndims = tf . maximum ( sample ndims , 0 ) actual batch event shape = tf . shape ( input = x ) [ sample ndims : ] if ( isinstance ( expected batch event shape , np . ndarray ) and isinstance ( actual batch event shape , np . ndarray ) ) : if any ( expected batch event shape != actual batch event shape ) : raise Not Implemented Error ( \"Broadcasting is not supported; \" \"unexpected batch and event shape \" \"(expected {}, saw {}).\" . format ( expected batch event shape , actual batch event shape ) ) runtime assertions = ndims assertion elif self . validate args : with tf . control dependencies ( ndims assertion ) : shape assertion = assert util . assert equal ( expected batch event shape , actual batch event shape , message = ( \"Broadcasting is not supported; \" \"unexpected batch and event shape.\" ) , name = \"assert batch and event shape same\" ) runtime assertions = [ shape assertion ] else : runtime assertions = [ ] return runtime assertions", "predictions": ["make sure that the statistic of the summary of the concat are valid kwargs kwargs kwargs kwargs kwargs ."], "references": ["helper which validates sample arg e . g . input to log_prob ."], "bleu": 0.06439931429457924, "rouge_l": 0.06468716861081655}
{"id": 298, "code": "def maybe assert valid sample ( self , counts ) : if not self . validate args : return counts counts = distribution util . embed check nonnegative integer form ( counts ) return distribution util . with dependencies ( [ assert util . assert less equal ( counts , self . total count , message = \"counts are not less than or equal to n.\" ) , ] , counts )", "predictions": ["to to to to to to validate a tensor in the form in the like form in the like object"], "references": ["check counts for proper shape values then return tensor version ."], "bleu": 0.06108557268562171, "rouge_l": 0.06808035714285715}
{"id": 299, "code": "def flat sample distributions ( self , sample shape = ( ) , seed = None , value = None ) : ds = [ ] values out = [ ] seed = seed stream . Seed Stream ( 'Joint Distribution Coroutine' , seed ) gen = self . model ( ) index = 0 d = next ( gen ) try : while True : actual distribution = d . distribution if isinstance ( d , self . Root ) else d ds . append ( actual distribution ) if ( value is not None and len ( value ) > index and value [ index ] is not None ) : seed ( ) next value = value [ index ] else : next value = actual distribution . sample ( sample shape = sample shape if isinstance ( d , self . Root ) else ( ) , seed = seed ( ) ) values out . append ( next value ) index += 1 d = gen . send ( next value ) except Stop Iteration : pass return ds , values out", "predictions": ["pdf at at the end of the at the end of the at the end of the at the at the at the at the at the specified at the specified"], "references": ["executes model creating both samples and distributions ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 300, "code": "def newsgroups dataset ( directory , split name , num words , shuffle and repeat ) : data = np . load ( download ( directory , FILE TEMPLATE . format ( split = split name ) ) ) data = data [ : - 1 ] num documents = data . shape [ 0 ] indices = np . array ( [ ( row idx , column idx ) for row idx , row in enumerate ( data ) for column idx in row ] ) sparse matrix = scipy . sparse . coo matrix ( ( np . ones ( indices . shape [ 0 ] ) , ( indices [ : , 0 ] , indices [ : , 1 ] ) ) , shape = ( num documents , num words ) , dtype = np . float32 ) sparse matrix = sparse matrix . tocsr ( ) dataset = tf . data . Dataset . range ( num documents ) if shuffle and repeat : dataset = dataset . shuffle ( num documents ) . repeat ( ) def get row py func ( idx ) : def get row python ( idx py ) : return np . squeeze ( np . array ( sparse matrix [ idx py ] . todense ( ) ) , axis = 0 ) py func = tf . compat . v1 . py func ( get row python , [ idx ] , tf . float32 , stateful = False ) py func . set shape ( ( num words , ) ) return py func dataset = dataset . map ( get row py func ) return dataset", "predictions": ["name of the effective sample sample compat compat compat compat compat compat compat compat compat compat compat compat ."], "references": ["20 newsgroups as a tf . data . dataset ."], "bleu": 0.06439931429457924, "rouge_l": 0.07305389221556886}
{"id": 301, "code": "def build fake input fns ( batch size ) : num words = 1000 vocabulary = [ str ( i ) for i in range ( num words ) ] random sample = np . random . randint ( 10 , size = ( batch size , num words ) ) . astype ( np . float32 ) def train input fn ( ) : dataset = tf . data . Dataset . from tensor slices ( random sample ) dataset = dataset . batch ( batch size ) . repeat ( ) return tf . compat . v1 . data . make one shot iterator ( dataset ) . get next ( ) def eval input fn ( ) : dataset = tf . data . Dataset . from tensor slices ( random sample ) dataset = dataset . batch ( batch size ) return tf . compat . v1 . data . make one shot iterator ( dataset ) . get next ( ) return train input fn , eval input fn , vocabulary", "predictions": ["potential function for the images of the images . . . . . . . . ."], "references": ["builds fake data for unit testing ."], "bleu": 0.07994607499472013, "rouge_l": 0.18020679468242246}
{"id": 302, "code": "def load bernoulli mnist dataset ( directory , split name ) : amat file = download ( directory , FILE TEMPLATE . format ( split = split name ) ) dataset = tf . data . Text Line Dataset ( amat file ) str to arr = lambda string : np . array ( [ c == b\"1\" for c in string . split ( ) ] ) def parser ( s ) : booltensor = tf . compat . v1 . py func ( str to arr , [ s ] , tf . bool ) reshaped = tf . reshape ( booltensor , [ 28 , 28 , 1 ] ) return tf . cast ( reshaped , dtype = tf . float32 ) , tf . constant ( 0 , tf . int32 ) return dataset . map ( parser )", "predictions": ["cast mnist to tensor reduce it s mnist reduce it ."], "references": ["returns hugo larochelle s binary static mnist tf . data . dataset ."], "bleu": 0.11294012253658708, "rouge_l": 0.24629878869448185}
{"id": 303, "code": "def build input pipeline ( data dir , batch size , heldout size , mnist type ) : if mnist type in [ Mnist Type . FAKE DATA , Mnist Type . THRESHOLD ] : if mnist type == Mnist Type . FAKE DATA : mnist data = build fake data ( ) else : mnist data = mnist . read data sets ( data dir ) training dataset = tf . data . Dataset . from tensor slices ( ( mnist data . train . images , np . int32 ( mnist data . train . labels ) ) ) heldout dataset = tf . data . Dataset . from tensor slices ( ( mnist data . validation . images , np . int32 ( mnist data . validation . labels ) ) ) elif mnist type == Mnist Type . BERNOULLI : training dataset = load bernoulli mnist dataset ( data dir , \"train\" ) heldout dataset = load bernoulli mnist dataset ( data dir , \"valid\" ) else : raise Value Error ( \"Unknown MNIST type.\" ) training batches = training dataset . repeat ( ) . batch ( batch size ) training iterator = tf . compat . v1 . data . make one shot iterator ( training batches ) heldout frozen = ( heldout dataset . take ( heldout size ) . repeat ( ) . batch ( heldout size ) ) heldout iterator = tf . compat . v1 . data . make one shot iterator ( heldout frozen ) handle = tf . compat . v1 . placeholder ( tf . string , shape = [ ] ) feedable iterator = tf . compat . v1 . data . Iterator . from string handle ( handle , training batches . output types , training batches . output shapes ) images , labels = feedable iterator . get next ( ) images = tf . reshape ( images , shape = [ - 1 ] + IMAGE SHAPE ) if mnist type in [ Mnist Type . FAKE DATA , Mnist Type . THRESHOLD ] : images = tf . cast ( images > 0.5 , dtype = tf . int32 ) return images , labels , handle , training iterator , heldout iterator", "predictions": ["broadcast if if if if if if if if if not already set . . . then set . is not set . . . ."], "references": ["builds an iterator switching between train and heldout data ."], "bleu": 0.04668049023095243, "rouge_l": 0.060396039603960394}
{"id": 304, "code": "def as numpy dtype ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'as numpy dtype' ) : return dtype . as numpy dtype return dtype", "predictions": ["convert a self . to a numpy self . . . . . . . ."], "references": ["returns a np . dtype based on this dtype ."], "bleu": 0.09147827112247602, "rouge_l": 0.24078947368421053}
{"id": 305, "code": "def base dtype ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'base dtype' ) : return dtype . base dtype return dtype", "predictions": ["returns the merge dtype dtype for a given dtype equals equals ."], "references": ["returns a non - reference dtype based on this dtype ."], "bleu": 0.1367440667823257, "rouge_l": 0.3505747126436781}
{"id": 306, "code": "def is bool ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'is bool' ) : return dtype . is bool return np . dtype ( dtype ) . kind == 'b'", "predictions": ["return is a tuple of type type if it is tuple if it is not tuple if it is tuple if it is a tuple if it is tuple if it"], "references": ["returns whether this is a boolean data type ."], "bleu": 0.055177848898164926, "rouge_l": 0.1665150136487716}
{"id": 307, "code": "def is complex ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'is complex' ) : return dtype . is complex return np . issubdtype ( np . dtype ( dtype ) , np . complex )", "predictions": [". x posterior must be a block posterior posterior posterior"], "references": ["returns whether this is a complex floating point type ."], "bleu": 0.13950796967929133, "rouge_l": 0.1}
{"id": 308, "code": "def max ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'max' ) : return dtype . max use finfo = is floating ( dtype ) or is complex ( dtype ) return np . finfo ( dtype ) . max if use finfo else np . iinfo ( dtype ) . max", "predictions": ["maximum length of a none none if the none is not wrapped ."], "references": ["returns the maximum representable value in this data type ."], "bleu": 0.1135935489027116, "rouge_l": 0.1781021897810219}
{"id": 309, "code": "def name ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'name' ) : return dtype . name if hasattr ( dtype , ' name ' ) : return dtype . name return str ( dtype )", "predictions": ["returns the has has the corresponding u . . . . . . . . . . ."], "references": ["returns the string name for this dtype ."], "bleu": 0.09629943614188137, "rouge_l": 0.2479674796747967}
{"id": 310, "code": "def size ( dtype ) : dtype = tf . as dtype ( dtype ) if hasattr ( dtype , 'size' ) : return dtype . size return np . dtype ( dtype ) . itemsize", "predictions": ["returns the leapfrog leapfrog leapfrog for a given value current value current value ."], "references": ["returns the number of bytes to represent this dtype ."], "bleu": 0.1250076305588977, "rouge_l": 0.2577464788732394}
{"id": 311, "code": "def nelder mead one step ( current simplex , current objective values , objective function = None , dim = None , func tolerance = None , position tolerance = None , batch evaluate objective = False , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : with tf . compat . v1 . name scope ( name , 'nelder mead one step' ) : domain dtype = current simplex . dtype . base dtype order = tf . argsort ( current objective values , direction = 'ASCENDING' , stable = True ) ( best index , worst index , second worst index ) = order [ 0 ] , order [ - 1 ] , order [ - 2 ] worst vertex = current simplex [ worst index ] ( best objective value , worst objective value , second worst objective value ) = ( current objective values [ best index ] , current objective values [ worst index ] , current objective values [ second worst index ] ) face centroid = tf . reduce sum ( input tensor = current simplex , axis = 0 ) - worst vertex face centroid /= tf . cast ( dim , domain dtype ) reflected = face centroid + reflection * ( face centroid - worst vertex ) objective at reflected = objective function ( reflected ) num evaluations = 1 has converged = check convergence ( current simplex , current simplex [ best index ] , best objective value , worst objective value , func tolerance , position tolerance ) def converged fn ( ) : return ( True , current simplex , current objective values , 0 ) case0 = has converged , converged fn accept reflected = ( ( objective at reflected < second worst objective value ) & ( objective at reflected >= best objective value ) ) accept reflected fn = accept reflected fn ( current simplex , current objective values , worst index , reflected , objective at reflected ) case1 = accept reflected , accept reflected fn do expansion = objective at reflected < best objective value expansion fn = expansion fn ( objective function , current simplex , current objective values , worst index , reflected , objective at reflected , face centroid , expansion ) case2 = do expansion , expansion fn do outside contraction = ( ( objective at reflected < worst objective value ) & ( objective at reflected >= second worst objective value ) ) outside contraction fn = outside contraction fn ( objective function , current simplex , current objective values , face centroid , best index , worst index , reflected , objective at reflected , contraction , shrinkage , batch evaluate objective ) case3 = do outside contraction , outside contraction fn default fn = inside contraction fn ( objective function , current simplex , current objective values , face centroid , best index , worst index , worst objective value , contraction , shrinkage , batch evaluate objective ) ( converged , next simplex , next objective at simplex , case evals ) = prefer static . case ( [ case0 , case1 , case2 , case3 ] , default = default fn , exclusive = False ) next simplex . set shape ( current simplex . shape ) next objective at simplex . set shape ( current objective values . shape ) return ( converged , next simplex , next objective at simplex , num evaluations + case evals )", "predictions": ["log one of joint joint between the same simplex . . . . . . . and the same same ."], "references": ["a single iteration of the nelder mead algorithm ."], "bleu": 0.0690889519686715, "rouge_l": 0.21554770318021202}
{"id": 312, "code": "def accept reflected fn ( simplex , objective values , worst index , reflected , objective at reflected ) : def replace worst with reflected ( ) : next simplex = replace at index ( simplex , worst index , reflected ) next objective values = replace at index ( objective values , worst index , objective at reflected ) return False , next simplex , next objective values , 0 return replace worst with reflected", "predictions": ["random dtype of dtype in simplex"], "references": ["creates the condition function pair for a reflection to be accepted ."], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 313, "code": "def expansion fn ( objective function , simplex , objective values , worst index , reflected , objective at reflected , face centroid , expansion ) : def expand and maybe replace ( ) : \"\"\"Performs the expansion step.\"\"\" expanded = face centroid + expansion * ( reflected - face centroid ) expanded objective value = objective function ( expanded ) expanded is better = ( expanded objective value < objective at reflected ) accept expanded fn = lambda : ( expanded , expanded objective value ) accept reflected fn = lambda : ( reflected , objective at reflected ) next pt , next objective value = prefer static . cond ( expanded is better , accept expanded fn , accept reflected fn ) next simplex = replace at index ( simplex , worst index , next pt ) next objective at simplex = replace at index ( objective values , worst index , next objective value ) return False , next simplex , next objective at simplex , 1 return expand and maybe replace", "predictions": ["returns the expand of the expand objective"], "references": ["creates the condition function pair for an expansion ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 314, "code": "def outside contraction fn ( objective function , simplex , objective values , face centroid , best index , worst index , reflected , objective at reflected , contraction , shrinkage , batch evaluate objective ) : def contraction ( ) : \"\"\"Performs a contraction.\"\"\" contracted = face centroid + contraction * ( reflected - face centroid ) objective at contracted = objective function ( contracted ) is contracted acceptable = objective at contracted <= objective at reflected def accept contraction ( ) : next simplex = replace at index ( simplex , worst index , contracted ) objective at next simplex = replace at index ( objective values , worst index , objective at contracted ) return ( False , next simplex , objective at next simplex , 1 ) def reject contraction ( ) : return shrink towards best ( objective function , simplex , best index , shrinkage , batch evaluate objective ) return prefer static . cond ( is contracted acceptable , accept contraction , reject contraction ) return contraction", "predictions": ["nested nested convert using the convert objective"], "references": ["creates the condition function pair for an outside contraction ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 315, "code": "def shrink towards best ( objective function , simplex , best index , shrinkage , batch evaluate objective ) : best vertex = simplex [ best index ] shrunk simplex = best vertex + shrinkage * ( simplex - best vertex ) objective at shrunk simplex , evals = evaluate objective multiple ( objective function , shrunk simplex , batch evaluate objective ) return ( False , shrunk simplex , objective at shrunk simplex , evals )", "predictions": ["get the like objective for the given objective"], "references": ["shrinks the simplex around the best vertex ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 316, "code": "def replace at index ( x , index , replacement ) : x new = tf . concat ( [ x [ : index ] , tf . expand dims ( replacement , axis = 0 ) , x [ ( index + 1 ) : ] ] , axis = 0 ) return x new", "predictions": ["pack rows in index into replacement input input input input input input input input input input input input input ."], "references": ["replaces an element at supplied index ."], "bleu": 0.06760229884571738, "rouge_l": 0.1622340425531915}
{"id": 317, "code": "def prepare args with initial simplex ( objective function , initial simplex , objective at initial simplex , batch evaluate objective ) : initial simplex = tf . convert to tensor ( value = initial simplex ) num vertices = tf . shape ( input = initial simplex ) [ 0 ] dim = num vertices - 1 num evaluations = 0 if objective at initial simplex is None : objective at initial simplex , n evals = evaluate objective multiple ( objective function , initial simplex , batch evaluate objective ) num evaluations += n evals objective at initial simplex = tf . convert to tensor ( value = objective at initial simplex ) return ( dim , num vertices , initial simplex , objective at initial simplex , num evaluations )", "predictions": ["download objective and initial from initial ."], "references": ["evaluates the objective function at the specified initial simplex ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 318, "code": "def prepare args with initial vertex ( objective function , initial vertex , step sizes , objective at initial vertex , batch evaluate objective ) : dim = tf . size ( input = initial vertex ) num vertices = dim + 1 unit vectors along axes = tf . reshape ( tf . eye ( dim , dim , dtype = initial vertex . dtype . base dtype ) , tf . concat ( [ [ dim ] , tf . shape ( input = initial vertex ) ] , axis = 0 ) ) simplex face = initial vertex + step sizes * unit vectors along axes simplex = tf . concat ( [ tf . expand dims ( initial vertex , axis = 0 ) , simplex face ] , axis = 0 ) num evaluations = 0 if objective at initial vertex is None : objective at initial vertex = objective function ( initial vertex ) num evaluations += 1 objective at simplex face , num evals = evaluate objective multiple ( objective function , simplex face , batch evaluate objective ) num evaluations += num evals objective at simplex = tf . concat ( [ tf . expand dims ( objective at initial vertex , axis = 0 ) , objective at simplex face ] , axis = 0 ) return ( dim , num vertices , simplex , objective at simplex , num evaluations )", "predictions": [". . . . . . . . . . . . . fake astype astype ."], "references": ["constructs a standard axes aligned simplex ."], "bleu": 0.07223943354597204, "rouge_l": 0.09010339734121123}
{"id": 319, "code": "def build input pipeline ( mnist data , batch size , heldout size ) : training dataset = tf . data . Dataset . from tensor slices ( ( mnist data . train . images , np . int32 ( mnist data . train . labels ) ) ) training batches = training dataset . shuffle ( 50000 , reshuffle each iteration = True ) . repeat ( ) . batch ( batch size ) training iterator = tf . compat . v1 . data . make one shot iterator ( training batches ) heldout dataset = tf . data . Dataset . from tensor slices ( ( mnist data . validation . images , np . int32 ( mnist data . validation . labels ) ) ) heldout frozen = ( heldout dataset . take ( heldout size ) . repeat ( ) . batch ( heldout size ) ) heldout iterator = tf . compat . v1 . data . make one shot iterator ( heldout frozen ) handle = tf . compat . v1 . placeholder ( tf . string , shape = [ ] ) feedable iterator = tf . compat . v1 . data . Iterator . from string handle ( handle , training batches . output types , training batches . output shapes ) images , labels = feedable iterator . get next ( ) return images , labels , handle , training iterator , heldout iterator", "predictions": ["build the input pipeline pipeline mnist and fn mnist mnist mnist mnist mnist mnist mnist mnist mnist ."], "references": ["build an iterator switching between train and heldout data ."], "bleu": 0.08097785064266204, "rouge_l": 0.2259259259259259}
{"id": 320, "code": "def build fake data ( num examples = 10 ) : class Dummy ( object ) : pass num examples = 10 mnist data = Dummy ( ) mnist data . train = Dummy ( ) mnist data . train . images = np . float32 ( np . random . randn ( num examples , * IMAGE SHAPE ) ) mnist data . train . labels = np . int32 ( np . random . permutation ( np . arange ( num examples ) ) ) mnist data . train . num examples = num examples mnist data . validation = Dummy ( ) mnist data . validation . images = np . float32 ( np . random . randn ( num examples , * IMAGE SHAPE ) ) mnist data . validation . labels = np . int32 ( np . random . permutation ( np . arange ( num examples ) ) ) mnist data . validation . num examples = num examples return mnist data", "predictions": ["build the fake data ."], "references": ["build fake mnist - style data for unit testing ."], "bleu": 0.1262909976406414, "rouge_l": 0.5030927835051546}
{"id": 321, "code": "def get config ( self ) : return { 'initializers' : [ tf . compat . v2 . initializers . serialize ( tf . keras . initializers . get ( init ) ) for init in self . initializers ] , 'sizes' : self . sizes , 'validate args' : self . validate args , }", "predictions": ["returns the dictionary of parameters to use for storage ."], "references": ["returns initializer configuration as a json - serializable dict ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 322, "code": "def from config ( cls , config ) : return cls ( * * { 'initializers' : [ tf . compat . v2 . initializers . deserialize ( init ) for init in config . get ( 'initializers' , [ ] ) ] , 'sizes' : config . get ( 'sizes' , [ ] ) , 'validate args' : config . get ( 'validate args' , False ) , } )", "predictions": ["loads the configuration from a dictionary ."], "references": ["instantiates an initializer from a configuration dictionary ."], "bleu": 0.2789001430384383, "rouge_l": 0.5269978401727862}
{"id": 323, "code": "def matmul ( a , b , transpose a = False , transpose b = False , adjoint a = False , adjoint b = False , a is sparse = False , b is sparse = False , name = None ) : if a is sparse or b is sparse : raise Not Implemented Error ( 'Numpy backend does not support sparse matmul.' ) if transpose a or adjoint a : a = matrix transpose ( a , conjugate = adjoint a ) if transpose b or adjoint b : b = matrix transpose ( b , conjugate = adjoint b ) return np . matmul ( a , b )", "predictions": ["adjoint backend for a backend backend ."], "references": ["numpy matmul wrapper ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 324, "code": "def std var helper ( self , statistic , statistic name , statistic ndims , df factor fn ) : df = tf . reshape ( self . df , tf . concat ( [ tf . shape ( input = self . df ) , tf . ones ( [ statistic ndims ] , dtype = tf . int32 ) ] , - 1 ) ) df = broadcast to shape ( df , tf . shape ( input = statistic ) ) denom = tf . where ( df > 2. , df - 2. , tf . ones like ( df ) ) statistic = statistic * df factor fn ( df / denom ) inf = dtype util . as numpy dtype ( self . dtype ) ( np . inf ) result where defined = tf . where ( df > 2. , statistic , tf . fill ( tf . shape ( input = statistic ) , inf , name = \"inf\" ) ) if self . allow nan stats : nan = dtype util . as numpy dtype ( self . dtype ) ( np . nan ) return tf . where ( df > 1. , result where defined , tf . fill ( tf . shape ( input = statistic ) , nan , name = \"nan\" ) ) else : with tf . control dependencies ( [ assert util . assert less ( tf . cast ( 1. , self . dtype ) , df , message = statistic name + \" not defined for components of df <= 1\" ) , ] ) : return tf . identity ( result where defined )", "predictions": ["helper function for passing the statistic of the statistic ."], "references": ["helper to compute stddev covariance and variance ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 325, "code": "def pick scalar condition ( pred , cond true , cond false ) : pred = tf . get static value ( tf . convert to tensor ( value = pred ) ) if pred is None : return tf . where ( pred , cond true , cond false ) return cond true if pred else cond false", "predictions": ["picks a scalar condition ."], "references": ["convenience function which chooses the condition based on the predicate ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 326, "code": "def finish log prob for one fiber ( self , y , x , ildj , event ndims , * * distribution kwargs ) : x = self . maybe rotate dims ( x , rotate right = True ) log prob = self . distribution . log prob ( x , * * distribution kwargs ) if self . is maybe event override : log prob = tf . reduce sum ( input tensor = log prob , axis = self . reduce event indices ) log prob += tf . cast ( ildj , log prob . dtype ) if self . is maybe event override and isinstance ( event ndims , int ) : tensorshape util . set shape ( log prob , tf . broadcast static shape ( tensorshape util . with rank at least ( y . shape , 1 ) [ : - event ndims ] , self . batch shape ) ) return log prob", "predictions": ["rotate the log for a one - dimensional log event ."], "references": ["finish computation of log_prob on one element of the inverse image ."], "bleu": 0.12368857073777001, "rouge_l": 0.17256011315417258}
{"id": 327, "code": "def finish prob for one fiber ( self , y , x , ildj , event ndims , * * distribution kwargs ) : x = self . maybe rotate dims ( x , rotate right = True ) prob = self . distribution . prob ( x , * * distribution kwargs ) if self . is maybe event override : prob = tf . reduce prod ( input tensor = prob , axis = self . reduce event indices ) prob *= tf . exp ( tf . cast ( ildj , prob . dtype ) ) if self . is maybe event override and isinstance ( event ndims , int ) : tensorshape util . set shape ( prob , tf . broadcast static shape ( tensorshape util . with rank at least ( y . shape , 1 ) [ : - event ndims ] , self . batch shape ) ) return prob", "predictions": ["rotate the event for a one - hot event ."], "references": ["finish computation of prob on one element of the inverse image ."], "bleu": 0.12273680279953825, "rouge_l": 0.1788856304985337}
{"id": 328, "code": "def maybe rotate dims ( self , x , rotate right = False ) : needs rotation const = tf . get static value ( self . needs rotation ) if needs rotation const is not None and not needs rotation const : return x ndims = prefer static . rank ( x ) n = ( ndims - self . rotate ndims ) if rotate right else self . rotate ndims perm = prefer static . concat ( [ prefer static . range ( n , ndims ) , prefer static . range ( 0 , n ) ] , axis = 0 ) return tf . transpose ( a = x , perm = perm )", "predictions": ["rotate the coefficients of the rotation matrix ."], "references": ["helper which rolls left event_dims left or right event_dims right ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 329, "code": "def apply single step ( dist , params event ndims , slices , params overrides ) : if len ( slices ) == 1 and slices [ 0 ] == Ellipsis : override dict = { } else : override dict = slice params to dict ( dist , params event ndims , slices ) override dict . update ( params overrides ) parameters = dict ( dist . parameters , * * override dict ) new dist = type ( dist ) ( * * parameters ) return new dist", "predictions": ["apply a single step to a single step ."], "references": ["applies a single slicing step to dist returning a new instance ."], "bleu": 0.18204651199034363, "rouge_l": 0.5570776255707762}
{"id": 330, "code": "def apply slice sequence ( dist , params event ndims , slice overrides seq ) : for slices , overrides in slice overrides seq : dist = apply single step ( dist , params event ndims , slices , overrides ) return dist", "predictions": ["apply a slice to a slice ."], "references": ["applies a sequence of slice or copy - with - overrides operations to dist ."], "bleu": 0.07448668213629092, "rouge_l": 0.3412587412587413}
{"id": 331, "code": "def num cols ( x ) : if tf . compat . dimension value ( x . shape [ - 1 ] ) is not None : return tf . compat . dimension value ( x . shape [ - 1 ] ) return tf . shape ( input = x ) [ - 1 ]", "predictions": ["the number of cols in x ."], "references": ["returns number of cols in a given tensor ."], "bleu": 0.40661103887968814, "rouge_l": 0.6112224448897796}
{"id": 332, "code": "def prefer static ( original fn , static fn ) : original spec = tf inspect . getfullargspec ( original fn ) static spec = tf inspect . getfullargspec ( static fn ) if original spec != static spec : raise Value Error ( 'Arg specs do not match: original={}, static={}, fn={}' . format ( original spec , static spec , original fn ) ) @ decorator . decorator def wrap ( wrapped fn , * args , * * kwargs ) : del wrapped fn [ args , kwargs ] , all static = maybe get static args ( [ args , kwargs ] ) if all static : return static fn ( * args , * * kwargs ) return original fn ( * args , * * kwargs ) return wrap ( original fn )", "predictions": ["wrapper for static static specs ."], "references": ["wraps original_fn preferring to call static_fn when inputs are static ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 333, "code": "def copy docstring ( original fn , new fn ) : original spec = tf inspect . getfullargspec ( original fn ) new spec = tf inspect . getfullargspec ( new fn ) if original spec != new spec : raise Value Error ( 'Arg specs do not match: original={}, new={}, fn={}' . format ( original spec , new spec , original fn ) ) @ decorator . decorator def wrap ( wrapped fn , * args , * * kwargs ) : del wrapped fn return new fn ( * args , * * kwargs ) return wrap ( original fn )", "predictions": ["copy docstring into tf ."], "references": ["wraps new_fn with the doc of original_fn ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 334, "code": "def get static predicate ( pred ) : if pred in { 0 , 1 } : pred value = bool ( pred ) elif isinstance ( pred , bool ) : pred value = pred elif isinstance ( pred , tf . Tensor ) : pred value = tf . get static value ( pred ) if pred value is None : pred value = c api . TF Try Evaluate Constant wrapper ( pred . graph . c graph , pred . as tf output ( ) ) else : raise Type Error ( '`pred` must be a Tensor, or a Python bool, or 1 or 0. ' 'Found instead: {}' . format ( pred ) ) return pred value", "predictions": ["returns the static predicate predicate ."], "references": ["helper function for statically evaluating predicates in cond ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 335, "code": "def rank from shape ( shape tensor fn , tensorshape = None ) : if tensorshape is None : shape tensor = ( shape tensor fn ( ) if callable ( shape tensor fn ) else shape tensor fn ) if ( hasattr ( shape tensor , 'shape' ) and hasattr ( shape tensor . shape , 'num elements' ) ) : ndims = tensorshape util . num elements ( shape tensor . shape ) else : ndims = len ( shape tensor ) ndims fn = lambda : tf . size ( input = shape tensor ) else : ndims = tensorshape util . rank ( tensorshape ) ndims fn = lambda : tf . size ( input = shape tensor fn ( ) if callable ( shape tensor fn ) else shape tensor fn ) return ndims fn ( ) if ndims is None else ndims", "predictions": ["returns the rank of the given shape ."], "references": ["computes rank given a tensor s shape ."], "bleu": 0.239802967618271, "rouge_l": 0.5}
{"id": 336, "code": "def name scope ( self , name = None , default name = None , values = None ) : with tf . compat . v1 . name scope ( self . name ) : with tf . compat . v1 . name scope ( name , default name , values = values or [ ] ) as scope : yield scope", "predictions": ["returns a tf scope scope for the given name ."], "references": ["helper function to standardize op scope ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 337, "code": "def embed check nonnegative integer form ( x , name = \"embed check nonnegative integer form\" ) : with tf . name scope ( name ) : x = tf . convert to tensor ( value = x , name = \"x\" ) assertions = [ assert util . assert non negative ( x , message = \"'{}' must be non-negative.\" . format ( x ) ) , ] if not dtype util . is integer ( x . dtype ) : assertions += [ assert integer form ( x , message = \"'{}' cannot contain fractional components.\" . format ( x ) ) , ] return with dependencies ( assertions , x )", "predictions": ["check if x is nonnegative ."], "references": ["assert x is a non - negative tensor and optionally of integers ."], "bleu": 0.09728049676725326, "rouge_l": 0.29611650485436897}
{"id": 338, "code": "def is known unsigned by dtype ( dt ) : return { tf . bool : True , tf . uint8 : True , tf . uint16 : True , } . get ( dt . base dtype , False )", "predictions": ["return true if dt is known by dtype ."], "references": ["helper returning true if dtype is known to be unsigned ."], "bleu": 0.20344044854715337, "rouge_l": 0.4911433172302737}
{"id": 339, "code": "def is known signed by dtype ( dt ) : return { tf . float16 : True , tf . float32 : True , tf . float64 : True , tf . int8 : True , tf . int16 : True , tf . int32 : True , tf . int64 : True , } . get ( dt . base dtype , False )", "predictions": ["return boolean if dt is known by dtype ."], "references": ["helper returning true if dtype is known to be signed ."], "bleu": 0.1768796183625973, "rouge_l": 0.3929146537842191}
{"id": 340, "code": "def largest integer by dtype ( dt ) : if not is known dtype ( dt ) : raise Type Error ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) if dt . is floating : return int ( 2 ** ( np . finfo ( dt . as numpy dtype ) . nmant + 1 ) ) if dt . is integer : return np . iinfo ( dt . as numpy dtype ) . max if dt . base dtype == tf . bool : return int ( 1 ) raise Type Error ( \"Unrecognized dtype: {}\" . format ( dt . name ) )", "predictions": ["return the integer integer by dtype ."], "references": ["helper returning the largest integer exactly representable by dtype ."], "bleu": 0.24924249970265266, "rouge_l": 0.5700934579439253}
{"id": 341, "code": "def smallest integer by dtype ( dt ) : if not is known dtype ( dt ) : raise Type Error ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) if is known unsigned by dtype ( dt ) : return 0 return - 1 * largest integer by dtype ( dt )", "predictions": ["return the integer type of a given dtype ."], "references": ["helper returning the smallest integer exactly representable by dtype ."], "bleu": 0.18885888592159467, "rouge_l": 0.41709401709401706}
{"id": 342, "code": "def is integer like by dtype ( dt ) : if not is known dtype ( dt ) : raise Type Error ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) return dt . is integer or dt . base dtype == tf . bool", "predictions": ["return true if dt is a integer dtype ."], "references": ["helper returning true if dtype . is_integer or is bool ."], "bleu": 0.1957494756053795, "rouge_l": 0.3929146537842191}
{"id": 343, "code": "def gen new seed ( seed , salt ) : if seed is None : return None string = ( str ( seed ) + salt ) . encode ( \"utf-8\" ) return int ( hashlib . md5 ( string ) . hexdigest ( ) [ : 8 ] , 16 ) & 0x7FFFFFFF", "predictions": ["generate a new seed seed ."], "references": ["generate a new seed from the given seed and salt ."], "bleu": 0.29063333849923007, "rouge_l": 0.6703296703296703}
{"id": 344, "code": "def dimension size ( x , axis ) : s = tf . compat . dimension value ( tensorshape util . with rank at least ( x . shape , np . abs ( axis ) ) [ axis ] ) if s is not None : return s return tf . shape ( input = x ) [ axis ]", "predictions": ["size of x ."], "references": ["returns the size of a specific dimension ."], "bleu": 0.18693159143202892, "rouge_l": 0.47164948453608246}
{"id": 345, "code": "def maybe validate rightmost transposed ndims ( rightmost transposed ndims , validate args , name = None ) : with tf . name scope ( name or 'maybe validate rightmost transposed ndims' ) : assertions = [ ] if not dtype util . is integer ( rightmost transposed ndims . dtype ) : raise Type Error ( '`rightmost transposed ndims` must be integer type.' ) if tensorshape util . rank ( rightmost transposed ndims . shape ) is not None : if tensorshape util . rank ( rightmost transposed ndims . shape ) != 0 : raise Value Error ( '`rightmost transposed ndims` must be a scalar, ' 'saw rank: {}.' . format ( tensorshape util . rank ( rightmost transposed ndims . shape ) ) ) elif validate args : assertions += [ assert util . assert rank ( rightmost transposed ndims , 0 ) ] rightmost transposed ndims = tf . get static value ( rightmost transposed ndims ) msg = '`rightmost transposed ndims` must be non-negative.' if rightmost transposed ndims is not None : if rightmost transposed ndims < 0 : raise Value Error ( msg [ : - 1 ] + ', saw: {}.' . format ( rightmost transposed ndims ) ) elif validate args : assertions += [ assert util . assert non negative ( rightmost transposed ndims , message = msg ) ] return assertions", "predictions": ["validate that the transposed is correctly representable ."], "references": ["checks that rightmost_transposed_ndims is valid ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 346, "code": "def maybe validate perm ( perm , validate args , name = None ) : with tf . name scope ( name or 'maybe validate perm' ) : assertions = [ ] if not dtype util . is integer ( perm . dtype ) : raise Type Error ( '`perm` must be integer type' ) msg = '`perm` must be a vector.' if tensorshape util . rank ( perm . shape ) is not None : if tensorshape util . rank ( perm . shape ) != 1 : raise Value Error ( msg [ : - 1 ] + ', saw rank: {}.' . format ( tensorshape util . rank ( perm . shape ) ) ) elif validate args : assertions += [ assert util . assert rank ( perm , 1 , message = msg ) ] perm = tf . get static value ( perm ) msg = '`perm` must be a valid permutation vector.' if perm is not None : if not np . all ( np . arange ( np . size ( perm ) ) == np . sort ( perm ) ) : raise Value Error ( msg [ : - 1 ] + ', saw: {}.' . format ( perm ) ) elif validate args : assertions += [ assert util . assert equal ( tf . sort ( perm ) , tf . range ( tf . size ( input = perm ) ) , message = msg ) ] return assertions", "predictions": ["validate that a perm is valid ."], "references": ["checks that perm is valid ."], "bleu": 0.5410822690539396, "rouge_l": 0.7800511508951408}
{"id": 347, "code": "def event shape ( self , shape , static perm to shape ) : rightmost = tf . get static value ( self . rightmost transposed ndims ) if tensorshape util . rank ( shape ) is None or rightmost is None : return tf . Tensor Shape ( None ) if tensorshape util . rank ( shape ) < rightmost : raise Value Error ( 'Invalid shape: min event ndims={} but got {}' . format ( rightmost , shape ) ) perm = tf . get static value ( self . perm , partial = True ) if perm is None : return shape [ : tensorshape util . rank ( shape ) - rightmost ] . concatenate ( [ None ] * int ( rightmost ) ) if sum ( p is None for p in perm ) == 1 : present = np . argsort ( [ - 1 if p is None else p for p in perm ] ) for i , p in enumerate ( present [ 1 : ] ) : if i != p : perm = [ i if p is None else p for p in perm ] break return shape [ : tensorshape util . rank ( shape ) - rightmost ] . concatenate ( static perm to shape ( shape [ tensorshape util . rank ( shape ) - rightmost : ] , perm ) )", "predictions": ["return the shape of the event at the given shape ."], "references": ["helper for _forward and _inverse_event_shape ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 348, "code": "def check equal shape ( name , static shape , dynamic shape , static target shape , dynamic target shape = None ) : static target shape = tf . Tensor Shape ( static target shape ) if tensorshape util . is fully defined ( static shape ) and tensorshape util . is fully defined ( static target shape ) : if static shape != static target shape : raise Value Error ( \"{}: required shape {} but found {}\" . format ( name , static target shape , static shape ) ) return None else : if dynamic target shape is None : if tensorshape util . is fully defined ( static target shape ) : dynamic target shape = tensorshape util . as list ( static target shape ) else : raise Value Error ( \"{}: cannot infer target shape: no dynamic shape \" \"specified and static shape {} is not fully defined\" . format ( name , static target shape ) ) return assert util . assert equal ( dynamic shape , dynamic target shape , message = ( \"{}: required shape {}\" . format ( name , static target shape ) ) )", "predictions": ["check that static shape is equal to static shape ."], "references": ["check that source and target shape match statically if possible ."], "bleu": 0.1705647399369684, "rouge_l": 0.37770897832817335}
{"id": 349, "code": "def kalman transition ( filtered mean , filtered cov , transition matrix , transition noise ) : predicted mean = propagate mean ( filtered mean , transition matrix , transition noise ) predicted cov = propagate cov ( filtered cov , transition matrix , transition noise ) return predicted mean , predicted cov", "predictions": ["internal function to calculate the kalman transition of the kalman transition"], "references": ["propagate a filtered distribution through a transition model ."], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 350, "code": "def propagate mean ( mean , linop , dist ) : return linop . matmul ( mean ) + dist . mean ( ) [ ... , tf . newaxis ]", "predictions": ["propagate mean of mean mean of mean ."], "references": ["propagate a mean through linear gaussian transformation ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 351, "code": "def propagate cov ( cov , linop , dist ) : return linop . matmul ( linop . matmul ( cov ) , adjoint arg = True ) + dist . covariance ( )", "predictions": ["compute covariance of covariance"], "references": ["propagate covariance through linear gaussian transformation ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 352, "code": "def joint sample n ( self , n , seed = None ) : with tf . name scope ( \"sample n joint\" ) : stream = seed stream . Seed Stream ( seed , salt = \"Linear Gaussian State Space Model sample n joint\" ) sample and batch shape = distribution util . prefer static value ( tf . concat ( [ [ n ] , self . batch shape tensor ( ) ] , axis = 0 ) ) with tf . control dependencies ( self . runtime assertions ) : initial latent = self . initial state prior . sample ( sample shape = augment sample shape ( self . initial state prior , sample and batch shape , self . validate args ) , seed = stream ( ) ) initial latent = initial latent [ ... , tf . newaxis ] initial observation matrix = ( self . get observation matrix for timestep ( self . initial step ) ) initial observation noise = ( self . get observation noise for timestep ( self . initial step ) ) initial observation pred = initial observation matrix . matmul ( initial latent ) initial observation = ( initial observation pred + initial observation noise . sample ( sample shape = augment sample shape ( initial observation noise , sample and batch shape , self . validate args ) , seed = stream ( ) ) [ ... , tf . newaxis ] ) sample step = build kalman sample step ( self . get transition matrix for timestep , self . get transition noise for timestep , self . get observation matrix for timestep , self . get observation noise for timestep , full sample and batch shape = sample and batch shape , stream = stream , validate args = self . validate args ) ( latents , observations ) = tf . scan ( sample step , elems = tf . range ( self . initial step + 1 , self . final step ) , initializer = ( initial latent , initial observation ) ) latents = tf . concat ( [ initial latent [ tf . newaxis , ... ] , latents ] , axis = 0 ) observations = tf . concat ( [ initial observation [ tf . newaxis , ... ] , observations ] , axis = 0 ) latents = tf . squeeze ( latents , - 1 ) latents = distribution util . move dimension ( latents , 0 , - 2 ) observations = tf . squeeze ( observations , - 1 ) observations = distribution util . move dimension ( observations , 0 , - 2 ) return latents , observations", "predictions": ["creates data for build observation observation pass through initial pass pass pass to initial pass pass to initial"], "references": ["draw a joint sample from the prior over latents and observations ."], "bleu": 0.057259987315337726, "rouge_l": 0.0}
{"id": 353, "code": "def log normalization ( self ) : event dim = tf . compat . dimension value ( self . event shape [ 0 ] ) if event dim is None : raise Value Error ( 'v MF  log normalizer currently only supports ' 'statically known event shape' ) safe conc = tf . where ( self . concentration > 0 , self . concentration , tf . ones like ( self . concentration ) ) safe lognorm = ( ( event dim / 2 - 1 ) * tf . math . log ( safe conc ) - ( event dim / 2 ) * np . log ( 2 * np . pi ) - tf . math . log ( bessel ive ( event dim / 2 - 1 , safe conc ) ) - tf . abs ( safe conc ) ) log nsphere surface area = ( np . log ( 2. ) + ( event dim / 2 ) * np . log ( np . pi ) - tf . math . lgamma ( tf . cast ( event dim / 2 , self . dtype ) ) ) return tf . where ( self . concentration > 0 , - safe lognorm , log nsphere surface area * tf . ones like ( safe lognorm ) )", "predictions": ["get the get config for the get return value of the get return value of the get return value of the get the get ."], "references": ["computes the log - normalizer of the distribution ."], "bleu": 0.07265857755970445, "rouge_l": 0.2571127502634352}
{"id": 354, "code": "def maybe assert valid sample ( self , samples ) : if not self . validate args : return samples with tf . control dependencies ( [ assert util . assert near ( 1. , tf . linalg . norm ( tensor = samples , axis = - 1 ) , message = 'samples must be unit length' ) , assert util . assert equal ( tf . shape ( input = samples ) [ - 1 : ] , self . event shape tensor ( ) , message = ( 'samples must have innermost dimension matching that of ' '`self.mean direction`' ) ) , ] ) : return tf . identity ( samples )", "predictions": ["config if cls is valid and identity ."], "references": ["check counts for proper shape values then return tensor version ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 355, "code": "def mode ( self ) : return ( self . mean direction + tf . zeros like ( self . concentration ) [ ... , tf . newaxis ] )", "predictions": ["transpose the mode mode mode mode mode"], "references": ["the mode of the von mises - fisher distribution is the mean direction ."], "bleu": 0.08993236413460196, "rouge_l": 0.17967599410898377}
{"id": 356, "code": "def rotate ( self , samples ) : event dim = ( tf . compat . dimension value ( self . event shape [ 0 ] ) or self . event shape tensor ( ) [ 0 ] ) basis = tf . concat ( [ [ 1. ] , tf . zeros ( [ event dim - 1 ] , dtype = self . dtype ) ] , axis = 0 ) , u = tf . nn . l2 normalize ( basis - self . mean direction , axis = - 1 ) return samples - 2 * tf . reduce sum ( input tensor = samples * u , axis = - 1 , keepdims = True ) * u", "predictions": ["std the self fn fn ."], "references": ["applies a householder rotation to samples ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 357, "code": "def sample 3d ( self , n , seed = None ) : seed = seed stream . Seed Stream ( seed , salt = 'von mises fisher 3d' ) u shape = tf . concat ( [ [ n ] , self . batch shape tensor ( ) ] , axis = 0 ) z = tf . random . uniform ( u shape , seed = seed ( ) , dtype = self . dtype ) safe conc = tf . where ( self . concentration > 0 , self . concentration , tf . ones like ( self . concentration ) ) safe z = tf . where ( z > 0 , z , tf . ones like ( z ) ) safe u = 1 + tf . reduce logsumexp ( input tensor = [ tf . math . log ( safe z ) , tf . math . log1p ( - safe z ) - 2 * safe conc ] , axis = 0 ) / safe conc u = tf . where ( self . concentration > tf . zeros like ( safe u ) , safe u , 2 * z - 1 ) u = tf . where ( tf . equal ( z , 0 ) , - tf . ones like ( u ) , u ) if not self . allow nan stats : u = tf . debugging . check numerics ( u , 'u in  sample 3d' ) return u [ ... , tf . newaxis ]", "predictions": ["pick scalar from scalar and cond tf tf tf tf tf tf tf tf tf tf tf tf tf tf tf tf tf tf tf tf tf tf tf tf tf"], "references": ["specialized inversion sampler for 3d ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 358, "code": "def remove dict keys with value ( dict , val ) : return { k : v for k , v in dict . items ( ) if v is not val }", "predictions": ["finish a log log for a log log kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs ."], "references": ["removes dict keys which have have self as value ."], "bleu": 0.06439931429457924, "rouge_l": 0.07305389221556886}
{"id": 359, "code": "def recursively replace dict for pretty dict ( x ) : if isinstance ( x , dict ) : return Pretty Dict ( { k : recursively replace dict for pretty dict ( v ) for k , v in x . items ( ) } ) if ( isinstance ( x , collections . Sequence ) and not isinstance ( x , six . string types ) ) : args = ( recursively replace dict for pretty dict ( x ) for x in x ) is named tuple = ( isinstance ( x , tuple ) and hasattr ( x , \" asdict\" ) and hasattr ( x , \" fields\" ) ) return type ( x ) ( * args ) if is named tuple else type ( x ) ( args ) if isinstance ( x , collections . Mapping ) : return type ( x ) ( * * { k : recursively replace dict for pretty dict ( v ) for k , v in x . items ( ) } ) return x", "predictions": ["prob a for finish for finish ."], "references": ["recursively replace dict s with _prettydict ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 360, "code": "def get samples ( dist , z , n , seed ) : with tf . compat . v1 . name scope ( 'get samples' , values = [ z , n ] ) : if ( n is None ) == ( z is None ) : raise Value Error ( 'Must specify exactly one of arguments \"n\" and \"z\".  Found: ' 'n = %s, z = %s' % ( n , z ) ) if n is not None : return dist . sample ( n , seed = seed ) else : return tf . convert to tensor ( value = z , name = 'z' )", "predictions": ["maybe x and right rotate rotate needs to right needs to right needs to right needs"], "references": ["check args and return samples ."], "bleu": 0.07692375026049747, "rouge_l": 0.09902597402597402}
{"id": 361, "code": "def is namedtuple like ( x ) : try : for fn in x . fields : = getattr ( x , fn ) return True except Attribute Error : return False", "predictions": ["1 - point step"], "references": ["helper which returns true if input is collections . namedtuple - like ."], "bleu": 0.037870374782798366, "rouge_l": 0.1073943661971831}
{"id": 362, "code": "def make name ( super name , default super name , sub name ) : name = super name if super name is not None else default super name if sub name is not None : name += ' ' + sub name return name", "predictions": ["apply slice to default ."], "references": ["helper which makes a str name ; useful for tf . compat . v1 . name_scope ."], "bleu": 0.02476709768976917, "rouge_l": 0.08276797829036635}
{"id": 363, "code": "def choose base case ( is accepted , accepted , rejected , name = None ) : def expand is accepted like ( x ) : \"\"\"Helper to expand `is accepted` like the shape of some input arg.\"\"\" with tf . compat . v1 . name scope ( 'expand is accepted like' ) : expand shape = tf . concat ( [ tf . shape ( input = is accepted ) , tf . ones ( [ tf . rank ( x ) - tf . rank ( is accepted ) ] , dtype = tf . int32 ) , ] , axis = 0 ) multiples = tf . concat ( [ tf . ones ( [ tf . rank ( is accepted ) ] , dtype = tf . int32 ) , tf . shape ( input = x ) [ tf . rank ( is accepted ) : ] , ] , axis = 0 ) m = tf . tile ( tf . reshape ( is accepted , expand shape ) , multiples ) m . set shape ( m . shape . merge with ( x . shape ) ) return m def where ( accepted , rejected ) : if accepted is rejected : return accepted accepted = tf . convert to tensor ( value = accepted , name = 'accepted' ) rejected = tf . convert to tensor ( value = rejected , name = 'rejected' ) r = tf . where ( expand is accepted like ( accepted ) , accepted , rejected ) r . set shape ( r . shape . merge with ( accepted . shape . merge with ( rejected . shape ) ) ) return r with tf . compat . v1 . name scope ( name , 'choose' , values = [ is accepted , accepted , rejected ] ) : if not is list like ( accepted ) : return where ( accepted , rejected ) return [ ( choose ( is accepted , a , r , name = name ) if is namedtuple like ( a ) else where ( a , r ) ) for a , r in zip ( accepted , rejected ) ]", "predictions": ["num cols of the cols in the cols . . . . . . . . ."], "references": ["helper to choose which expand_dims is_accepted and applies tf . where ."], "bleu": 0.07994607499472013, "rouge_l": 0.1423570595099183}
{"id": 364, "code": "def choose ( is accepted , accepted , rejected , name = None ) : if not is namedtuple like ( accepted ) : return choose base case ( is accepted , accepted , rejected , name = name ) if not isinstance ( accepted , type ( rejected ) ) : raise Type Error ( 'Type of `accepted` ({}) must be identical to ' 'type of `rejected` ({})' . format ( type ( accepted ) . name , type ( rejected ) . name ) ) return type ( accepted ) ( * * dict ( [ ( fn , choose ( is accepted , getattr ( accepted , fn ) , getattr ( rejected , fn ) , name = name ) ) for fn in accepted . fields ] ) )", "predictions": ["returns the type object for the given namedtuple del del del del del"], "references": ["helper which expand_dims is_accepted then applies tf . where ."], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 365, "code": "def value and gradients ( fn , fn arg list , result = None , grads = None , name = None ) : with tf . compat . v1 . name scope ( name , 'value and gradients' , [ fn arg list , result , grads ] ) : def convert to tensor ( x , name ) : ctt = lambda x : x if x is None else tf . convert to tensor ( value = x , name = name ) return [ ctt ( x ) for x in x ] if is list like ( x ) else ctt ( x ) fn arg list = ( list ( fn arg list ) if is list like ( fn arg list ) else [ fn arg list ] ) fn arg list = convert to tensor ( fn arg list , 'fn arg' ) if result is None : result = fn ( * fn arg list ) if grads is None and tf . executing eagerly ( ) : fn arg list = [ 0 + x for x in fn arg list ] result = convert to tensor ( result , 'fn result' ) if grads is not None : grads = convert to tensor ( grads , 'fn grad' ) return result , grads if is list like ( result ) and len ( result ) == len ( fn arg list ) : def fn slice ( i ) : \"\"\"Needed to prevent `cell-var-from-loop` pylint warning.\"\"\" return lambda x : fn ( * ( fn arg list [ : i ] + [ x ] + fn arg list [ i + 1 : ] ) ) grads = [ tfp math value and gradients ( fn slice ( i ) , fn arg list [ i ] ) [ 1 ] for i in range ( len ( result ) ) ] else : , grads = tfp math value and gradients ( fn , fn arg list ) return result , grads", "predictions": ["convert function to gradients docstring docstring docstring raise error on tfp raise ."], "references": ["helper to maybe_call_fn_and_grads ."], "bleu": 0.10571070857151538, "rouge_l": 0.2601279317697228}
{"id": 366, "code": "def maybe call fn and grads ( fn , fn arg list , result = None , grads = None , check non none grads = True , name = None ) : with tf . compat . v1 . name scope ( name , 'maybe call fn and grads' , [ fn arg list , result , grads ] ) : fn arg list = ( list ( fn arg list ) if is list like ( fn arg list ) else [ fn arg list ] ) result , grads = value and gradients ( fn , fn arg list , result , grads ) if not all ( r . dtype . is floating for r in ( result if is list like ( result ) else [ result ] ) ) : raise Type Error ( 'Function result must be a `Tensor` with `float` ' '`dtype`.' ) if len ( fn arg list ) != len ( grads ) : raise Value Error ( 'Function args must be in one-to-one correspondence ' 'with grads.' ) if check non none grads and any ( g is None for g in grads ) : raise Value Error ( 'Encountered `None` gradient.\\n' '  fn arg list: {}\\n' '  grads: {}' . format ( fn arg list , grads ) ) return result , grads", "predictions": ["wrapper for the function arguments that can be used by the { pred } . . ."], "references": ["calls fn and computes the gradient of the result wrt args_list ."], "bleu": 0.0859076483566362, "rouge_l": 0.21353558926487748}
{"id": 367, "code": "def maybe check valid shape ( shape , validate args ) : if not dtype util . is integer ( shape . dtype ) : raise Type Error ( '{} dtype ({}) should be `int`-like.' . format ( shape , dtype util . name ( shape . dtype ) ) ) assertions = [ ] message = '`{}` rank should be <= 1.' if tensorshape util . rank ( shape . shape ) is not None : if tensorshape util . rank ( shape . shape ) > 1 : raise Value Error ( message . format ( shape ) ) elif validate args : assertions . append ( assert util . assert less ( tf . rank ( shape ) , 2 , message = message . format ( shape ) ) ) shape = tf . get static value ( shape ) message = '`{}` elements must have at most one `-1`.' if shape is not None : if sum ( shape == - 1 ) > 1 : raise Value Error ( message . format ( shape ) ) elif validate args : assertions . append ( assert util . assert less ( tf . reduce sum ( input tensor = tf . cast ( tf . equal ( shape , - 1 ) , tf . int32 ) ) , 2 , message = message . format ( shape ) ) ) message = '`{}` elements must be either positive integers or `-1`.' if shape is not None : if np . any ( shape < - 1 ) : raise Value Error ( message . format ( shape ) ) elif validate args : assertions . append ( assert util . assert greater ( shape , - 2 , message = message . format ( shape ) ) ) return assertions", "predictions": ["from shape and shape ."], "references": ["check that a shape tensor is int - type and otherwise sane ."], "bleu": 0.06554932163900559, "rouge_l": 0.3086003372681282}
{"id": 368, "code": "def maybe assert valid sample ( self , x ) : if not self . validate args : return x return distribution util . with dependencies ( [ assert util . assert positive ( x , message = \"sample must be positive\" ) , assert util . assert less ( x , 1. , message = \"sample must be less than `1`.\" ) , ] , x )", "predictions": ["scope if self tf is valid tf tf tf tf tf tf tf tf tf tf tf tf ."], "references": ["checks the validity of a sample ."], "bleu": 0.06439931429457924, "rouge_l": 0.08390646492434663}
{"id": 369, "code": "def converged any ( converged , failed ) : return ( tf . reduce any ( input tensor = converged ) | tf . reduce all ( input tensor = failed ) )", "predictions": ["return check that check that check that check that check check that check that form form check that form form form form check check and form form form form = check"], "references": ["condition to stop when any batch member converges or all have failed ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 370, "code": "def update position ( state , position delta , next objective , next gradient , grad tolerance , f relative tolerance , x tolerance ) : failed = state . failed | ~ tf . math . is finite ( next objective ) | ~ tf . reduce all ( input tensor = tf . math . is finite ( next gradient ) , axis = - 1 ) next position = state . position + position delta converged = ~ failed & check convergence ( state . position , next position , state . objective value , next objective , next gradient , grad tolerance , f relative tolerance , x tolerance ) return update fields ( state , converged = state . converged | converged , failed = failed , position = next position , objective value = next objective , objective gradient = next gradient )", "predictions": ["is the default is the same known to the target } } } } } } } } } } } } } } } }"], "references": ["updates the state advancing its position by a given position_delta ."], "bleu": 0.04668049023095243, "rouge_l": 0.05831739961759082}
{"id": 371, "code": "def check convergence ( current position , next position , current objective , next objective , next gradient , grad tolerance , f relative tolerance , x tolerance ) : grad converged = norm ( next gradient , dims = 1 ) <= grad tolerance x converged = norm ( next position - current position , dims = 1 ) <= x tolerance f converged = ( norm ( next objective - current objective , dims = 0 ) <= f relative tolerance * current objective ) return grad converged | x converged | f converged", "predictions": ["is the known known known known known known known known dtype"], "references": ["checks if the algorithm satisfies the convergence criteria ."], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 372, "code": "def get field ( kernel results , field name ) : if hasattr ( kernel results , field name ) : return getattr ( kernel results , field name ) if hasattr ( kernel results , 'accepted results' ) : return getattr ( kernel results . accepted results , field name ) raise Type Error ( 'Cannot extract %s from %s' % ( field name , kernel results ) )", "predictions": ["returns the integer integer integer by name"], "references": ["field_name from kernel_results or kernel_results . accepted_results ."], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 373, "code": "def variance scale term ( self ) : c0 = self . total concentration [ ... , tf . newaxis ] return tf . sqrt ( ( 1. + c0 / self . total count [ ... , tf . newaxis ] ) / ( 1. + c0 ) )", "predictions": ["smallest integer by smallest integer not including smallest not including tf not not not in tf not not ."], "references": ["helper to _covariance and _variance which computes a shared scale ."], "bleu": 0.06439931429457924, "rouge_l": 0.07003444316877153}
{"id": 374, "code": "def maybe assert valid concentration ( self , concentration , validate args ) : if not validate args : return concentration concentration = distribution util . embed check categorical event shape ( concentration ) return distribution util . with dependencies ( [ assert util . assert positive ( concentration , message = \"Concentration parameter must be positive.\" ) , ] , concentration )", "predictions": ["integer if by by by by by by by by by by by by by by by by by by by by by by by by by by by by by"], "references": ["checks the validity of the concentration parameter ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 375, "code": "def maybe assert valid sample ( self , counts ) : if not self . validate args : return counts counts = distribution util . embed check nonnegative integer form ( counts ) return distribution util . with dependencies ( [ assert util . assert equal ( self . total count , tf . reduce sum ( input tensor = counts , axis = - 1 ) , message = \"counts last-dimension must sum to `self.total count`\" ) , ] , counts )", "predictions": ["new sample if the counts is seed seed string ."], "references": ["check counts for proper shape values then return tensor version ."], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 376, "code": "def forward log det jacobian fn ( bijector ) : if not mcmc util . is list like ( bijector ) : bijector = [ bijector ] def fn ( transformed state parts , event ndims ) : return sum ( [ b . forward log det jacobian ( sp , event ndims = e ) for b , e , sp in zip ( bijector , event ndims , transformed state parts ) ] ) return fn", "predictions": ["dimension function for size of size det"], "references": ["makes a function which applies a list of bijectors log_det_jacobian s ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 377, "code": "def forward transform fn ( bijector ) : if not mcmc util . is list like ( bijector ) : bijector = [ bijector ] def fn ( transformed state parts ) : return [ b . forward ( sp ) for b , sp in zip ( bijector , transformed state parts ) ] return fn", "predictions": ["maybe validate validate the validate state"], "references": ["makes a function which applies a list of bijectors forward s ."], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 378, "code": "def inverse transform fn ( bijector ) : if not mcmc util . is list like ( bijector ) : bijector = [ bijector ] def fn ( state parts ) : return [ b . inverse ( sp ) for b , sp in zip ( bijector , state parts ) ] return fn", "predictions": ["maybe validate of the validate validate the validate and return"], "references": ["makes a function which applies a list of bijectors inverse s ."], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 379, "code": "def val where ( cond , tval , fval ) : if isinstance ( tval , tf . Tensor ) : return tf . where ( cond , tval , fval ) elif isinstance ( tval , tuple ) : cls = type ( tval ) return cls ( * ( val where ( cond , t , f ) for t , f in zip ( tval , fval ) ) ) else : raise Exception ( Type Error )", "predictions": ["convert a shape to a = value tf tf tf tf tf tf tf tf tf tf tf tf tf tf tf tf tf ."], "references": ["like tf . where but works on namedtuples ."], "bleu": 0.06394766688900896, "rouge_l": 0.1285563751317176}
{"id": 380, "code": "def secant2 inner ( value and gradients function , initial args , val 0 , val c , f lim , sufficient decrease param , curvature param ) : update result = update ( value and gradients function , initial args . left , initial args . right , val c , f lim , active = initial args . active ) active = initial args . active & ~ update result . failed failed = initial args . failed | update result . failed val left = val where ( active , update result . left , initial args . left ) val right = val where ( active , update result . right , initial args . right ) updated left = active & tf . equal ( val left . x , val c . x ) updated right = active & tf . equal ( val right . x , val c . x ) is new = updated left | updated right next c = tf . where ( updated left , secant ( initial args . left , val left ) , val c . x ) next c = tf . where ( updated right , secant ( initial args . right , val right ) , next c ) in range = ( val left . x <= next c ) & ( next c <= val right . x ) needs extra eval = tf . reduce any ( input tensor = in range & is new ) num evals = initial args . num evals + update result . num evals num evals = num evals + tf . cast ( needs extra eval , num evals . dtype ) next args = Secant2Result ( active = active & in range , converged = initial args . converged , failed = failed , num evals = num evals , left = val left , right = val right ) def apply inner update ( ) : next val c = prefer static . cond ( needs extra eval , ( lambda : value and gradients function ( next c ) ) , ( lambda : val c ) ) return secant2 inner update ( value and gradients function , next args , val 0 , next val c , f lim , sufficient decrease param , curvature param ) return prefer static . cond ( tf . reduce any ( input tensor = next args . active ) , apply inner update , lambda : next args )", "predictions": ["check for equal value name and gradients . . . . . . . . . . . . . ."], "references": ["helper function for secant square ."], "bleu": 0.06429451441231726, "rouge_l": 0.16464237516869096}
{"id": 381, "code": "def secant2 inner update ( value and gradients function , initial args , val 0 , val c , f lim , sufficient decrease param , curvature param ) : new failed = initial args . active & ~ is finite ( val c ) active = initial args . active & ~ new failed failed = initial args . failed | new failed found wolfe = active & satisfies wolfe ( val 0 , val c , f lim , sufficient decrease param , curvature param ) val left = val where ( found wolfe , val c , initial args . left ) val right = val where ( found wolfe , val c , initial args . right ) converged = initial args . converged | found wolfe active = active & ~ found wolfe def apply update ( ) : update result = update ( value and gradients function , val left , val right , val c , f lim , active = active ) return Secant2Result ( active = tf . zeros like ( active ) , converged = converged , failed = failed | update result . failed , num evals = initial args . num evals + update result . num evals , left = update result . left , right = update result . right ) def default ( ) : return Secant2Result ( active = active , converged = converged , failed = failed , num evals = initial args . num evals , left = val left , right = val right ) return prefer static . cond ( tf . reduce any ( input tensor = active ) , apply update , default )", "predictions": ["kalman update of static and . ."], "references": ["helper function for secant - square step ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 382, "code": "def bisect ( value and gradients function , initial args , f lim ) : def loop cond ( curr ) : return ~ tf . reduce all ( input tensor = curr . stopped ) def loop body ( curr ) : \"\"\"Narrow down interval to satisfy opposite slope conditions.\"\"\" mid = value and gradients function ( ( curr . left . x + curr . right . x ) / 2 ) failed = ( curr . failed | ~ is finite ( mid ) | tf . equal ( mid . x , curr . left . x ) | tf . equal ( mid . x , curr . right . x ) ) to update = ~ ( curr . stopped | failed ) update left = ( mid . df < 0 ) & ( mid . f <= f lim ) left = val where ( to update & update left , mid , curr . left ) right = val where ( to update & ~ update left , mid , curr . right ) stopped = curr . stopped | failed | ( right . df >= 0 ) return [ Intermediate Result ( iteration = curr . iteration , stopped = stopped , failed = failed , num evals = curr . num evals + 1 , left = left , right = right ) ] return tf . while loop ( cond = loop cond , body = loop body , loop vars = [ initial args ] ) [ 0 ]", "predictions": ["propagate value and and and ."], "references": ["actual implementation of bisect given initial_args in a _bracketresult ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 383, "code": "def prepare args ( target log prob fn , state , step size , target log prob = None , grads target log prob = None , maybe expand = False , state gradients are stopped = False ) : state parts = list ( state ) if mcmc util . is list like ( state ) else [ state ] state parts = [ tf . convert to tensor ( value = s , name = 'current state' ) for s in state parts ] if state gradients are stopped : state parts = [ tf . stop gradient ( x ) for x in state parts ] target log prob , grads target log prob = mcmc util . maybe call fn and grads ( target log prob fn , state parts , target log prob , grads target log prob ) step sizes = ( list ( step size ) if mcmc util . is list like ( step size ) else [ step size ] ) step sizes = [ tf . convert to tensor ( value = s , name = 'step size' , dtype = target log prob . dtype ) for s in step sizes ] if len ( step sizes ) == 1 : step sizes *= len ( state parts ) if len ( state parts ) != len ( step sizes ) : raise Value Error ( 'There should be exactly one `step size` or it should ' 'have same length as `current state`.' ) def maybe flatten ( x ) : return x if maybe expand or mcmc util . is list like ( state ) else x [ 0 ] return [ maybe flatten ( state parts ) , maybe flatten ( step sizes ) , target log prob , grads target log prob , ]", "predictions": ["propagate cov cov parameters to list of cov log ."], "references": ["helper which processes input args to meet list - like assumptions ."], "bleu": 0.12273680279953825, "rouge_l": 0.2683284457478006}
{"id": 384, "code": "def bootstrap results ( self , init state ) : kernel results = self . impl . bootstrap results ( init state ) if self . step size update fn is not None : step size assign = self . step size update fn ( self . step size , None ) kernel results = kernel results . replace ( extra = Hamiltonian Monte Carlo Extra Kernel Results ( step size assign = step size assign ) ) return kernel results", "predictions": ["bootstrap the results of the step ."], "references": ["creates initial previous_kernel_results using a supplied state ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 385, "code": "def resnet block ( x , filters , kernel , stride , kernel posterior fn ) : x = tf . keras . layers . Batch Normalization ( ) ( x ) x = tf . keras . layers . Activation ( 'relu' ) ( x ) if stride != 1 or filters != x . shape [ 1 ] : shortcut = projection shortcut ( x , filters , stride , kernel posterior fn ) else : shortcut = x x = tfp . layers . Convolution2D Flipout ( filters , kernel , strides = stride , padding = 'same' , kernel posterior fn = kernel posterior fn ) ( x ) x = tf . keras . layers . Batch Normalization ( ) ( x ) x = tf . keras . layers . Activation ( 'relu' ) ( x ) x = tfp . layers . Convolution2D Flipout ( filters , kernel , strides = 1 , padding = 'same' , kernel posterior fn = kernel posterior fn ) ( x ) x = tf . keras . layers . add ( [ x , shortcut ] ) return x", "predictions": ["a stack of convolution convolution ."], "references": ["network block for resnet ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 386, "code": "def deep exponential family ( data size , feature size , units , shape ) : w2 = ed . Gamma ( 0.1 , 0.3 , sample shape = [ units [ 2 ] , units [ 1 ] ] , name = \"w2\" ) w1 = ed . Gamma ( 0.1 , 0.3 , sample shape = [ units [ 1 ] , units [ 0 ] ] , name = \"w1\" ) w0 = ed . Gamma ( 0.1 , 0.3 , sample shape = [ units [ 0 ] , feature size ] , name = \"w0\" ) z2 = ed . Gamma ( 0.1 , 0.1 , sample shape = [ data size , units [ 2 ] ] , name = \"z2\" ) z1 = ed . Gamma ( shape , shape / tf . matmul ( z2 , w2 ) , name = \"z1\" ) z0 = ed . Gamma ( shape , shape / tf . matmul ( z1 , w1 ) , name = \"z0\" ) x = ed . Poisson ( tf . matmul ( z0 , w0 ) , name = \"x\" ) return x", "predictions": ["deep family of convolution ."], "references": ["a multi - layered topic model over a documents - by - terms matrix ."], "bleu": 0.0369481680224917, "rouge_l": 0.09172932330827067}
{"id": 387, "code": "def trainable positive deterministic ( shape , min loc = 1e-3 , name = None ) : with tf . compat . v1 . variable scope ( None , default name = \"trainable positive deterministic\" ) : unconstrained loc = tf . compat . v1 . get variable ( \"unconstrained loc\" , shape ) loc = tf . maximum ( tf . nn . softplus ( unconstrained loc ) , min loc ) rv = ed . Deterministic ( loc = loc , name = name ) return rv", "predictions": ["wrapper for trainable trainable ."], "references": ["learnable deterministic distribution over positive reals ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 388, "code": "def trainable gamma ( shape , min concentration = 1e-3 , min scale = 1e-5 , name = None ) : with tf . compat . v1 . variable scope ( None , default name = \"trainable gamma\" ) : unconstrained concentration = tf . compat . v1 . get variable ( \"unconstrained concentration\" , shape , initializer = tf . compat . v1 . initializers . random normal ( mean = 0.5 , stddev = 0.1 ) ) unconstrained scale = tf . compat . v1 . get variable ( \"unconstrained scale\" , shape , initializer = tf . compat . v1 . initializers . random normal ( stddev = 0.1 ) ) concentration = tf . maximum ( tf . nn . softplus ( unconstrained concentration ) , min concentration ) rate = tf . maximum ( 1. / tf . nn . softplus ( unconstrained scale ) , 1. / min scale ) rv = ed . Gamma ( concentration = concentration , rate = rate , name = name ) return rv", "predictions": ["example trainable gamma gamma ."], "references": ["learnable gamma via concentration and scale parameterization ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 389, "code": "def registered kl ( type a , type b ) : hierarchy a = tf inspect . getmro ( type a ) hierarchy b = tf inspect . getmro ( type b ) dist to children = None kl fn = None for mro to a , parent a in enumerate ( hierarchy a ) : for mro to b , parent b in enumerate ( hierarchy b ) : candidate dist = mro to a + mro to b candidate kl fn = DIVERGENCES . get ( ( parent a , parent b ) , None ) if not kl fn or ( candidate kl fn and candidate dist < dist to children ) : dist to children = candidate dist kl fn = candidate kl fn return kl fn", "predictions": ["returns the kl kl between two strings ."], "references": ["get the kl function registered for classes a and b ."], "bleu": 0.15587146574232644, "rouge_l": 0.3070469798657718}
{"id": 390, "code": "def read image ( filepath ) : im bytes = tf . io . read file ( filepath ) im = tf . image . decode image ( im bytes , channels = CHANNELS ) im = tf . image . convert image dtype ( im , tf . float32 ) return im", "predictions": ["read an image from a tf . image ."], "references": ["returns an image tensor ."], "bleu": 0.19960198807747329, "rouge_l": 0.4518518518518518}
{"id": 391, "code": "def download sprites ( ) : filepath = os . path . join ( FLAGS . data dir , DATA SPRITES DIR ) if not tf . io . gfile . exists ( filepath ) : if not tf . io . gfile . exists ( FLAGS . data dir ) : tf . io . gfile . makedirs ( FLAGS . data dir ) zip name = \"{}.zip\" . format ( filepath ) urllib . request . urlretrieve ( DATA SPRITES URL , zip name ) with zipfile . Zip File ( zip name , \"r\" ) as zip file : zip file . extractall ( FLAGS . data dir ) tf . io . gfile . remove ( zip name ) return filepath", "predictions": ["download and extract tf files from temporary directory ."], "references": ["downloads the sprites data and returns the saved filepath ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 392, "code": "def create character ( skin , hair , top , pants ) : dtype = skin . dtype hair mask = tf . cast ( hair [ ... , - 1 : ] <= 0 , dtype ) top mask = tf . cast ( top [ ... , - 1 : ] <= 0 , dtype ) pants mask = tf . cast ( pants [ ... , - 1 : ] <= 0 , dtype ) char = ( skin * hair mask ) + hair char = ( char * top mask ) + top char = ( char * pants mask ) + pants return char", "predictions": ["create a character from the top of the top - level pants ."], "references": ["creates a character sprite from a set of attribute sprites ."], "bleu": 0.14949751774990683, "rouge_l": 0.42302357836338417}
{"id": 393, "code": "def create random seq ( character , action metadata , direction , length = 8 ) : start = tf . random . uniform ( [ ] , maxval = action metadata [ 1 ] , dtype = tf . int32 ) return create seq ( character , action metadata , direction , length , start )", "predictions": ["create a random character ."], "references": ["creates a random sequence ."], "bleu": 0.3860973950960897, "rouge_l": 0.6}
{"id": 394, "code": "def maybe validate distributions ( distributions , dtype override , validate args ) : assertions = [ ] if not is iterable ( distributions ) or not distributions : raise Value Error ( '`distributions` must be a list of one or more ' 'distributions.' ) if dtype override is None : dts = [ dtype util . base dtype ( d . dtype ) for d in distributions if d . dtype is not None ] if dts [ 1 : ] != dts [ : - 1 ] : raise Type Error ( 'Distributions must have same dtype; found: {}.' . format ( set ( dtype util . name ( dt ) for dt in dts ) ) ) for d in distributions : if tensorshape util . rank ( d . event shape ) is not None : if tensorshape util . rank ( d . event shape ) != 1 : raise Value Error ( '`Distribution` must be vector variate, ' 'found event nimds: {}.' . format ( tensorshape util . rank ( d . event shape ) ) ) elif validate args : assertions . append ( assert util . assert equal ( 1 , tf . size ( input = d . event shape tensor ( ) ) , message = '`Distribution` must be vector variate.' ) ) batch shapes = [ d . batch shape for d in distributions ] if all ( tensorshape util . is fully defined ( b ) for b in batch shapes ) : if batch shapes [ 1 : ] != batch shapes [ : - 1 ] : raise Value Error ( 'Distributions must have the same `batch shape`; ' 'found: {}.' . format ( batch shapes ) ) elif validate args : batch shapes = [ tensorshape util . as list ( d . batch shape ) if tensorshape util . is fully defined ( d . batch shape ) else d . batch shape tensor ( ) for d in distributions ] assertions . extend ( assert util . assert equal ( b1 , b2 , message = 'Distribution `batch shape`s must be identical.' ) for b1 , b2 in zip ( batch shapes [ 1 : ] , batch shapes [ : - 1 ] ) ) return assertions", "predictions": ["validate that the distributions is correctly representable ."], "references": ["checks that distributions satisfies all assumptions ."], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 395, "code": "def build input pipeline ( x train , x test , y train , y test , batch size , valid size ) : x train = x train . astype ( \"float32\" ) x test = x test . astype ( \"float32\" ) x train /= 255 x test /= 255 y train = y train . flatten ( ) y test = y test . flatten ( ) if FLAGS . subtract pixel mean : x train mean = np . mean ( x train , axis = 0 ) x train -= x train mean x test -= x train mean print ( \"x train shape:\" + str ( x train . shape ) ) print ( str ( x train . shape [ 0 ] ) + \" train samples\" ) print ( str ( x test . shape [ 0 ] ) + \" test samples\" ) training dataset = tf . data . Dataset . from tensor slices ( ( x train , np . int32 ( y train ) ) ) training batches = training dataset . shuffle ( 50000 , reshuffle each iteration = True ) . repeat ( ) . batch ( batch size ) training iterator = tf . compat . v1 . data . make one shot iterator ( training batches ) heldout dataset = tf . data . Dataset . from tensor slices ( ( x test , np . int32 ( y test ) ) ) heldout batches = heldout dataset . repeat ( ) . batch ( valid size ) heldout iterator = tf . compat . v1 . data . make one shot iterator ( heldout batches ) handle = tf . compat . v1 . placeholder ( tf . string , shape = [ ] ) feedable iterator = tf . compat . v1 . data . Iterator . from string handle ( handle , training batches . output types , training batches . output shapes ) images , labels = feedable iterator . get next ( ) return images , labels , handle , training iterator , heldout iterator", "predictions": ["build the input pipeline ."], "references": ["build an iterator switching between train and heldout data ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 396, "code": "def build fake data ( ) : num examples = 10 x train = np . random . rand ( num examples , * IMAGE SHAPE ) . astype ( np . float32 ) y train = np . random . permutation ( np . arange ( num examples ) ) . astype ( np . int32 ) x test = np . random . rand ( num examples , * IMAGE SHAPE ) . astype ( np . float32 ) y test = np . random . permutation ( np . arange ( num examples ) ) . astype ( np . int32 ) return ( x train , y train ) , ( x test , y test )", "predictions": ["build the fake data from the fake dataset ."], "references": ["build fake cifar10 - style data for unit testing ."], "bleu": 0.15881076016027915, "rouge_l": 0.41709401709401706}
{"id": 397, "code": "def sort tensor ( tensor ) : sorted , = tf . nn . top k ( tensor , k = tf . shape ( input = tensor ) [ - 1 ] ) sorted . set shape ( tensor . shape ) return sorted", "predictions": ["sort tensor according to top of tensor ."], "references": ["use top_k to sort a tensor along the last dimension ."], "bleu": 0.13859150907108325, "rouge_l": 0.3070469798657718}
{"id": 398, "code": "def assert ndims statically ( x , expect ndims = None , expect ndims at least = None , expect static = False ) : ndims = x . shape . ndims if ndims is None : if expect static : raise Value Error ( 'Expected static ndims. Found: {}' . format ( x ) ) return if expect ndims is not None and ndims != expect ndims : raise Value Error ( 'ndims must be {}.  Found: {}' . format ( expect ndims , ndims ) ) if expect ndims at least is not None and ndims < expect ndims at least : raise Value Error ( 'ndims must be at least {}. Found {}' . format ( expect ndims at least , ndims ) )", "predictions": ["check that x is valid ."], "references": ["assert that tensor x has expected number of dimensions ."], "bleu": 0.13487005099534619, "rouge_l": 0.3588235294117647}
{"id": 399, "code": "def batch gather with broadcast ( params , indices , axis ) : leading bcast shape = tf . broadcast dynamic shape ( tf . shape ( input = params ) [ : axis ] , tf . shape ( input = indices ) [ : - 1 ] ) params += tf . zeros ( tf . concat ( ( leading bcast shape , tf . shape ( input = params ) [ axis : ] ) , axis = 0 ) , dtype = params . dtype ) indices += tf . zeros ( tf . concat ( ( leading bcast shape , tf . shape ( input = indices ) [ - 1 : ] ) , axis = 0 ) , dtype = indices . dtype ) return tf . compat . v1 . batch gather ( params , indices )", "predictions": ["batch gather with broadcast ."], "references": ["like batch_gather but broadcasts to the left of axis ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 400, "code": "def broadcast cat event and params ( event , params , base dtype ) : if dtype util . is integer ( event . dtype ) : pass elif dtype util . is floating ( event . dtype ) : event = tf . cast ( event , dtype = tf . int32 ) else : raise Type Error ( \"`value` should have integer `dtype` or \" \"`self.dtype` ({})\" . format ( base dtype ) ) shape known statically = ( tensorshape util . rank ( params . shape ) is not None and tensorshape util . is fully defined ( params . shape [ : - 1 ] ) and tensorshape util . is fully defined ( event . shape ) ) if not shape known statically or params . shape [ : - 1 ] != event . shape : params *= tf . ones like ( event [ ... , tf . newaxis ] , dtype = params . dtype ) params shape = tf . shape ( input = params ) [ : - 1 ] event *= tf . ones ( params shape , dtype = event . dtype ) if tensorshape util . rank ( params . shape ) is not None : tensorshape util . set shape ( event , params . shape [ : - 1 ] ) return event , params", "predictions": ["broadcast cat and params and params ."], "references": ["broadcasts the event or distribution parameters ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 401, "code": "def broadcast event and samples ( event , samples , event ndims ) : samples shape = tf . concat ( [ tf . shape ( input = samples ) [ : - event ndims - 1 ] , tf . shape ( input = samples ) [ tf . rank ( samples ) - event ndims : ] ] , axis = 0 ) event *= tf . ones ( samples shape , dtype = event . dtype ) event = tf . expand dims ( event , axis = - event ndims - 1 ) samples *= tf . ones like ( event , dtype = samples . dtype ) return event , samples", "predictions": ["broadcast event and samples ."], "references": ["broadcasts the event or samples ."], "bleu": 0.31610981104846864, "rouge_l": 0.5366568914956013}
{"id": 402, "code": "def update inv hessian ( prev state , next state ) : should update = ~ next state . converged & ~ next state . failed gradient delta = next state . objective gradient - prev state . objective gradient position delta = next state . position - prev state . position normalization factor = tf . reduce sum ( input tensor = gradient delta * position delta , axis = - 1 ) should update = should update & ~ tf . equal ( normalization factor , 0 ) def do update inv hessian ( ) : next inv hessian = bfgs inv hessian update ( gradient delta , position delta , normalization factor , prev state . inverse hessian estimate ) return bfgs utils . update fields ( next state , inverse hessian estimate = tf . where ( should update , next inv hessian , prev state . inverse hessian estimate ) ) return prefer static . cond ( tf . reduce any ( input tensor = should update ) , do update inv hessian , lambda : next state )", "predictions": ["update the hessian of the hessian of the hessian ."], "references": ["update the bgfs state by computing the next inverse hessian estimate ."], "bleu": 0.16153071659734697, "rouge_l": 0.44721407624633425}
{"id": 403, "code": "def get initial state ( value and gradients function , initial position , num correction pairs , tolerance ) : init args = bfgs utils . get initial state args ( value and gradients function , initial position , tolerance ) empty queue = make empty queue for ( num correction pairs , initial position ) init args . update ( position deltas = empty queue , gradient deltas = empty queue ) return L Bfgs Optimizer Results ( * * init args )", "predictions": ["returns the initial state of the initial state of the value and gradients ."], "references": ["create lbfgsoptimizerresults with initial state of search procedure ."], "bleu": 0.17395797375642236, "rouge_l": 0.3620178041543027}
{"id": 404, "code": "def von mises cdf series ( x , concentration , num terms , dtype ) : num terms = tf . cast ( num terms , dtype = dtype ) def loop body ( n , rn , drn dconcentration , vn , dvn dconcentration ) : \"\"\"One iteration of the series loop.\"\"\" denominator = 2. * n / concentration + rn ddenominator dk = - 2. * n / concentration ** 2 + drn dconcentration rn = 1. / denominator drn dconcentration = - ddenominator dk / denominator ** 2 multiplier = tf . sin ( n * x ) / n + vn vn = rn * multiplier dvn dconcentration = ( drn dconcentration * multiplier + rn * dvn dconcentration ) n -= 1. return n , rn , drn dconcentration , vn , dvn dconcentration ( , , , vn , dvn dconcentration ) = tf . while loop ( cond = lambda n , * : n > 0. , body = loop body , loop vars = ( num terms , tf . zeros like ( x , name = \"rn\" ) , tf . zeros like ( x , name = \"drn dconcentration\" ) , tf . zeros like ( x , name = \"vn\" ) , tf . zeros like ( x , name = \"dvn dconcentration\" ) , ) , ) cdf = .5 + x / ( 2. * np . pi ) + vn / np . pi dcdf dconcentration = dvn dconcentration / np . pi cdf clipped = tf . clip by value ( cdf , 0. , 1. ) dcdf dconcentration *= tf . cast ( ( cdf >= 0. ) & ( cdf <= 1. ) , dtype ) return cdf clipped , dcdf dconcentration", "predictions": ["cdf cdf series of cdf cdf ."], "references": ["computes the von mises cdf and its derivative via series expansion ."], "bleu": 0.1081377510275021, "rouge_l": 0.30148270181219106}
{"id": 405, "code": "def von mises cdf normal ( x , concentration , dtype ) : def cdf func ( concentration ) : \"\"\"A helper function that is passed to value and gradient.\"\"\" z = ( ( np . sqrt ( 2. / np . pi ) / tf . math . bessel i0e ( concentration ) ) * tf . sin ( .5 * x ) ) z2 = z ** 2 z3 = z2 * z z4 = z2 ** 2 c = 24. * concentration c1 = 56. xi = z - z3 / ( ( c - 2. * z2 - 16. ) / 3. - ( z4 + ( 7. / 4. ) * z2 + 167. / 2. ) / ( c - c1 - z2 + 3. ) ) ** 2 distrib = normal . Normal ( tf . cast ( 0. , dtype ) , tf . cast ( 1. , dtype ) ) return distrib . cdf ( xi ) return value and gradient ( cdf func , concentration )", "predictions": ["von cdf cdf cdf ."], "references": ["computes the von mises cdf and its derivative via normal approximation ."], "bleu": 0.08006212224540951, "rouge_l": 0.3285457809694794}
{"id": 406, "code": "def get initial args ( objective function , initial population , initial position , population size , population stddev , max iterations , func tolerance , position tolerance , differential weight , crossover prob , seed ) : was iterable = False if initial position is not None : initial position , was iterable = ensure list ( initial position ) if initial population is not None : initial population , was iterable = ensure list ( initial population ) population = get starting population ( initial population , initial position , population size , population stddev , seed = seed ) differential weight = tf . convert to tensor ( value = differential weight , dtype = population [ 0 ] . dtype . base dtype ) crossover prob = tf . convert to tensor ( value = crossover prob ) population values = objective function ( * population ) if max iterations is not None : max iterations = tf . convert to tensor ( value = max iterations ) func tolerance = tf . convert to tensor ( value = func tolerance , dtype = population values . dtype . base dtype ) position tolerance = tf . convert to tensor ( value = position tolerance , dtype = population [ 0 ] . dtype . base dtype ) return ( was iterable , population , population values , max iterations , func tolerance , position tolerance , differential weight , crossover prob )", "predictions": ["helper function to get initial args for the population ."], "references": ["processes initial args ."], "bleu": 0.17827531042796255, "rouge_l": 0.4644670050761421}
{"id": 407, "code": "def find best in population ( population , values ) : best value = tf . math . reduce min ( input tensor = values ) best index = tf . where ( tf . math . equal ( values , best value ) ) [ 0 , 0 ] return ( [ population part [ best index ] for population part in population ] , best value )", "predictions": ["return the best best in population ."], "references": ["finds the population member with the lowest value ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 408, "code": "def check convergence ( population , population values , func tolerance , position tolerance ) : value range = tf . math . abs ( tf . math . reduce max ( input tensor = population values ) - tf . math . reduce min ( input tensor = population values ) ) value converged = value range <= func tolerance half tol = position tolerance / 2 def part converged ( part ) : return tf . math . reduce max ( input tensor = tf . math . abs ( part - part [ 0 ] ) ) <= half tol x converged = tf . math . reduce all ( input tensor = [ part converged ( part ) for part in population ] ) return value converged | x converged", "predictions": ["check if population is convergence ."], "references": ["checks whether the convergence criteria have been met ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 409, "code": "def get tol ( tol , dtype , validate args ) : if tol is None : return tf . convert to tensor ( value = 0 , dtype = dtype ) tol = tf . convert to tensor ( value = tol , dtype = dtype ) if validate args : tol = distribution util . with dependencies ( [ assert util . assert non negative ( tol , message = \"Argument 'tol' must be non-negative\" ) ] , tol ) return tol", "predictions": ["convert a tf . tensor to a tensor ."], "references": ["gets a tensor of type dtype 0 if tol is none validation optional ."], "bleu": 0.11452232344544261, "rouge_l": 0.2510288065843621}
{"id": 410, "code": "def build input pipeline ( train images , batch size ) : training dataset = tf . data . Dataset . from tensor slices ( train images ) training batches = training dataset . shuffle ( 50000 , reshuffle each iteration = True ) . repeat ( ) . batch ( batch size ) training iterator = tf . compat . v1 . data . make one shot iterator ( training batches ) images = training iterator . get next ( ) return images", "predictions": ["build the input pipeline ."], "references": ["build an iterator over training batches ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 411, "code": "def hat integral inverse ( self , x ) : x = tf . cast ( x , self . power . dtype ) t = self . power - 1. return tf . math . expm1 ( - ( tf . math . log ( t ) + tf . math . log ( x ) ) / t )", "predictions": ["hat the inverse of the integral ."], "references": ["inverse function of _hat_integral ."], "bleu": 0.22089591134157885, "rouge_l": 0.5154929577464789}
{"id": 412, "code": "def lu reconstruct assertions ( lower upper , perm , validate args ) : assertions = [ ] message = 'Input `lower upper` must have at least 2 dimensions.' if lower upper . shape . ndims is not None : if lower upper . shape . ndims < 2 : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert rank at least ( lower upper , rank = 2 , message = message ) ) message = '`rank(lower upper)` must equal `rank(perm) + 1`' if lower upper . shape . ndims is not None and perm . shape . ndims is not None : if lower upper . shape . ndims != perm . shape . ndims + 1 : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert rank ( lower upper , rank = tf . rank ( perm ) + 1 , message = message ) ) message = '`lower upper` must be square.' if lower upper . shape [ : - 2 ] . is fully defined ( ) : if lower upper . shape [ - 2 ] != lower upper . shape [ - 1 ] : raise Value Error ( message ) elif validate args : m , n = tf . split ( tf . shape ( input = lower upper ) [ - 2 : ] , num or size splits = 2 ) assertions . append ( tf . compat . v1 . assert equal ( m , n , message = message ) ) return assertions", "predictions": ["reconstruct the assertions for the assertions ."], "references": ["returns list of assertions related to lu_reconstruct assumptions ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 413, "code": "def lu solve assertions ( lower upper , perm , rhs , validate args ) : assertions = lu reconstruct assertions ( lower upper , perm , validate args ) message = 'Input `rhs` must have at least 2 dimensions.' if rhs . shape . ndims is not None : if rhs . shape . ndims < 2 : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert rank at least ( rhs , rank = 2 , message = message ) ) message = '`lower upper.shape[-1]` must equal `rhs.shape[-1]`.' if ( tf . compat . dimension value ( lower upper . shape [ - 1 ] ) is not None and tf . compat . dimension value ( rhs . shape [ - 2 ] ) is not None ) : if lower upper . shape [ - 1 ] != rhs . shape [ - 2 ] : raise Value Error ( message ) elif validate args : assertions . append ( tf . compat . v1 . assert equal ( tf . shape ( input = lower upper ) [ - 1 ] , tf . shape ( input = rhs ) [ - 2 ] , message = message ) ) return assertions", "predictions": ["solve assertions for assertions ."], "references": ["returns list of assertions related to lu_solve assumptions ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 414, "code": "def maybe validate matrix ( a , validate args ) : assertions = [ ] if not a . dtype . is floating : raise Type Error ( 'Input `a` must have `float`-like `dtype` ' '(saw {}).' . format ( a . dtype . name ) ) if a . shape . ndims is not None : if a . shape . ndims < 2 : raise Value Error ( 'Input `a` must have at least 2 dimensions ' '(saw: {}).' . format ( a . shape . ndims ) ) elif validate args : assertions . append ( tf . compat . v1 . assert rank at least ( a , rank = 2 , message = 'Input `a` must have at least 2 dimensions.' ) ) return assertions", "predictions": ["validate a matrix a floating matrix ."], "references": ["checks that input is a float matrix ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 415, "code": "def gen slices ( num blocks , n in , n out , mask type = MASK EXCLUSIVE ) : slices = [ ] col = 0 d in = n in // num blocks d out = n out // num blocks row = d out if mask type == MASK EXCLUSIVE else 0 for in range ( num blocks ) : row slice = slice ( row , None ) col slice = slice ( col , col + d in ) slices . append ( [ row slice , col slice ] ) col += d in row += d out return slices", "predictions": ["generate slices for a list of slices"], "references": ["generate the slices for building an autoregressive mask ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 416, "code": "def gen mask ( num blocks , n in , n out , mask type = MASK EXCLUSIVE , dtype = tf . float32 ) : mask = np . zeros ( [ n out , n in ] , dtype = dtype . as numpy dtype ( ) ) slices = gen slices ( num blocks , n in , n out , mask type = mask type ) for [ row slice , col slice ] in slices : mask [ row slice , col slice ] = 1 return mask", "predictions": ["generate results for the step of the slices size size size size size size size size of the slices size of the slices size of the slices ."], "references": ["generate the mask for building an autoregressive dense layer ."], "bleu": 0.05442133807846856, "rouge_l": 0.17264150943396223}
{"id": 417, "code": "def create input order ( input size , input order = \"left-to-right\" ) : if isinstance ( input order , six . string types ) : if input order == \"left-to-right\" : return np . arange ( start = 1 , stop = input size + 1 ) elif input order == \"right-to-left\" : return np . arange ( start = input size , stop = 0 , step = - 1 ) elif input order == \"random\" : ret = np . arange ( start = 1 , stop = input size + 1 ) np . random . shuffle ( ret ) return ret elif np . all ( np . sort ( input order ) == np . arange ( 1 , input size + 1 ) ) : return np . array ( input order ) raise Value Error ( \"Invalid input order: '{}'.\" . format ( input order ) )", "predictions": ["resnet the block of the block order"], "references": ["returns a degree vectors for the input ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 418, "code": "def create masks ( degrees ) : return [ inp [ : , np . newaxis ] <= out for inp , out in zip ( degrees [ : - 1 ] , degrees [ 1 : ] ) ] + [ degrees [ - 1 ] [ : , np . newaxis ] < degrees [ 0 ] ]", "predictions": ["deep exponential exponential exponential from degrees"], "references": ["returns a list of binary mask matrices enforcing autoregressivity ."], "bleu": 0.09536752763778475, "rouge_l": 0.0}
{"id": 419, "code": "def make masked initializer ( mask , initializer ) : initializer = tf . keras . initializers . get ( initializer ) def masked initializer ( shape , dtype = None , partition info = None ) : if partition info is None : x = initializer ( shape , dtype ) else : x = initializer ( shape , dtype , partition info ) return tf . cast ( mask , x . dtype ) * x return masked initializer", "predictions": ["creates a positive positive deterministic deterministic name name name name name name name name name ."], "references": ["returns a masked version of the given initializer ."], "bleu": 0.08513012360883544, "rouge_l": 0.16850828729281767}
{"id": 420, "code": "def build ( self , input shape ) : if self . event shape is None : self . event shape = [ tf . compat . dimension value ( input shape [ - 1 ] ) ] self . event size = self . event shape [ - 1 ] self . event ndims = len ( self . event shape ) if input shape [ - 1 ] != self . event shape [ - 1 ] : raise Value Error ( \"Invalid final dimension of `input shape`. \" \"Expected `{!r}`, but got `{!r}`\" . format ( self . event shape [ - 1 ] , input shape [ - 1 ] ) ) self . input order = create input order ( self . event size , self . input order param ) self . masks = create masks ( create degrees ( input size = self . event size , hidden units = self . hidden units , input order = self . input order , hidden degrees = self . hidden degrees ) ) # self . masks [ - 1 ] = np . reshape ( np . tile ( self . masks [ - 1 ] [ ... , tf . newaxis ] , [ 1 , 1 , self . params ] ) , [ self . masks [ - 1 ] . shape [ 0 ] , self . event size * self . params ] ) self . network = tf . keras . Sequential ( [ tf . keras . layers . Input Layer ( ( self . event size , ) , dtype = self . dtype ) ] ) layer output sizes = self . hidden units + [ self . event size * self . params ] for k in range ( len ( self . masks ) ) : self . network . add ( tf . keras . layers . Dense ( layer output sizes [ k ] , kernel initializer = make masked initializer ( self . masks [ k ] , self . kernel initializer ) , kernel constraint = make masked constraint ( self . masks [ k ] ) , activation = self . activation if k + 1 < len ( self . masks ) else None , use bias = self . use bias , * * self . kwargs ) ) super ( Autoregressive Layer , self ) . build ( input shape )", "predictions": ["trainable the initializers event ."], "references": ["see tfkl . layer . build ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 421, "code": "def call ( self , x ) : with tf . compat . v2 . name scope ( self . name or \"Autoregressive Layer call\" ) : x = tf . convert to tensor ( value = x , dtype = self . dtype , name = \"x\" ) input shape = tf . shape ( input = x ) if tensorshape util . rank ( x . shape ) == 1 : x = x [ tf . newaxis , ... ] return tf . reshape ( self . network ( x ) , tf . concat ( [ input shape , [ self . params ] ] , axis = 0 ) )", "predictions": ["registered the input tensor ."], "references": ["see tfkl . layer . call ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 422, "code": "def zero dimensional mvndiag ( dtype ) : dummy mvndiag = tfd . Multivariate Normal Diag ( scale diag = tf . ones ( [ 0 ] , dtype = dtype ) ) dummy mvndiag . covariance = lambda : dummy mvndiag . variance ( ) [ ... , tf . newaxis ] return dummy mvndiag", "predictions": ["returns the mvndiag covariance covariance ."], "references": ["build a zero - dimensional mvndiag object ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 423, "code": "def observe timeseries fn ( timeseries ) : def observation noise fn ( t ) : current slice = timeseries [ ... , t , : ] return tfd . Multivariate Normal Diag ( loc = current slice , scale diag = tf . zeros like ( current slice ) ) return observation noise fn", "predictions": ["download the = = 0 - 1 if the sprites is not none if not ."], "references": ["build an observation_noise_fn that observes a tensor timeseries ."], "bleu": 0.07692375026049747, "rouge_l": 0.08425414364640883}
{"id": 424, "code": "def params to weights ( self , global scale variance , global scale noncentered , local scale variances , local scales noncentered , weights noncentered ) : global scale = ( global scale noncentered * tf . sqrt ( global scale variance ) * self . weights prior scale ) local scales = local scales noncentered * tf . sqrt ( local scale variances ) return weights noncentered * local scales * global scale [ ... , tf . newaxis ]", "predictions": ["returns the weights create the weights and top - level weights - level weights - value for the given pants - value - pants pants - value - of the given"], "references": ["build regression weights from model parameters ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 425, "code": "def depth ( g ) : def explore ( v ) : if v . depth < 0 : v . depth = ( ( 1 + max ( [ - 1 ] + [ explore ( annotated graph [ u ] ) for u in v . parents ] ) ) if v . parents else 0 ) return v . depth annotated graph = { k : Node ( k , v ) for k , v in g . items ( ) } for v in annotated graph . values ( ) : explore ( v ) return annotated graph", "predictions": ["return create an annotated graph"], "references": ["computes the number of edges on longest path from node to root ."], "bleu": 0.04635036983311895, "rouge_l": 0.0}
{"id": 426, "code": "def best order ( g ) : def explore ( u ) : \"\"\"Recursive function to ascend up through unvisited dependencies.\"\"\" if u . depth < 0 : return if not u . parents : result . append ( ( u . name , u . parents ) ) u . depth = - 1 return b = ( u . name , [ ] ) result . append ( b ) u . depth = - 1 d = 0 for v in sorted ( ( g . get ( p ) for p in u . parents ) , key = lambda v : v . depth ) : n0 = len ( result ) explore ( v ) n1 = len ( result ) b [ 1 ] . extend ( [ ' ' ] * d + [ v . name ] ) d = n1 - n0 - 1 g = depth ( g ) result = [ ] for u in sorted ( g . values ( ) , key = lambda v : v . depth , reverse = True ) : explore ( u ) return tuple ( reversed ( result ) )", "predictions": ["return the maybe validate validate the maybe g ."], "references": ["creates tuple of str tuple - str pairs representing resolved & sorted dag ."], "bleu": 0.08097951150551161, "rouge_l": 0.08367626886145405}
{"id": 427, "code": "def prob chain rule flatten ( named makers ) : def make ( dist fn , args ) : if args is None : return lambda * : dist fn if not args : return lambda * : dist fn ( ) def fn ( * xs ) : kwargs = dict ( zip ( args , reversed ( xs [ - len ( args ) : ] ) ) ) kwargs . pop ( ' ' , None ) return dist fn ( * * kwargs ) return fn named makers = convert to dict ( named makers ) g = { k : ( None if distribution util . is distribution instance ( v ) else joint distribution sequential . get required args ( v ) ) for k , v in named makers . items ( ) } g = best order ( g ) dist fn name , dist fn args = zip ( * g ) dist fn args = tuple ( None if a is None else tuple ( a ) for a in dist fn args ) dist fn wrapped = tuple ( make ( named makers [ name ] , parents ) for ( name , parents ) in g ) dist fn = tuple ( named makers . get ( n ) for n in dist fn name ) return dist fn , dist fn wrapped , dist fn args , dist fn name", "predictions": ["create a function that creates a input pipeline pipeline ."], "references": ["creates lists of callables suitable for jdseq ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 428, "code": "def build ( self , model ) : if not is dict like ( model ) : raise Type Error ( '`model` must be convertible to `dict` (saw: {}).' . format ( type ( model ) . name ) ) [ self . dist fn , self . dist fn wrapped , self . dist fn args , self . dist fn name , ] = prob chain rule flatten ( model )", "predictions": ["build the model for the given model permutation permutation ."], "references": ["creates dist_fn dist_fn_wrapped dist_fn_args dist_fn_name ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 429, "code": "def build is last day of season ( num steps per season ) : num steps per cycle = np . sum ( num steps per season ) changepoints = np . cumsum ( np . ravel ( num steps per season ) ) - 1 def is last day of season ( t ) : t = dist util . maybe get static value ( t ) if t is not None : step in cycle = t % num steps per cycle return any ( step in cycle == changepoints ) else : step in cycle = tf . math . floormod ( t , num steps per cycle ) return tf . reduce any ( input tensor = tf . equal ( step in cycle , changepoints ) ) return is last day of season", "predictions": ["sort the sorted day of of of"], "references": ["build utility method to compute whether the season is changing ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 430, "code": "def build seasonal transition matrix ( num seasons , is last day of season , dtype , basis change matrix = None , basis change matrix inv = None ) : with tf . compat . v1 . name scope ( 'build seasonal transition matrix' ) : seasonal permutation = np . concatenate ( [ np . arange ( 1 , num seasons ) , [ 0 ] ] , axis = 0 ) seasonal permutation matrix = tf . constant ( np . eye ( num seasons ) [ seasonal permutation ] , dtype = dtype ) if basis change matrix is not None : seasonal permutation matrix = tf . matmul ( basis change matrix , tf . matmul ( seasonal permutation matrix , basis change matrix inv ) ) identity matrix = tf . eye ( tf . shape ( input = seasonal permutation matrix ) [ - 1 ] , dtype = dtype ) def seasonal transition matrix ( t ) : return tf . linalg . Linear Operator Full Matrix ( matrix = dist util . pick scalar condition ( is last day of season ( t ) , seasonal permutation matrix , identity matrix ) ) return seasonal transition matrix", "predictions": ["builds the seasonal matrix matrix matrix matrix for the given season . . . . . . . . . . . . . . . . . . . ."], "references": ["build a function computing transitions for a seasonal effect model ."], "bleu": 0.046398855339878003, "rouge_l": 0.10418445772843724}
{"id": 431, "code": "def build seasonal transition noise ( drift scale , num seasons , is last day of season ) : drift scale diag = tf . stack ( [ tf . zeros like ( drift scale ) ] * ( num seasons - 1 ) + [ drift scale ] , axis = - 1 ) def seasonal transition noise ( t ) : noise scale diag = dist util . pick scalar condition ( is last day of season ( t ) , drift scale diag , tf . zeros like ( drift scale diag ) ) return tfd . Multivariate Normal Diag ( loc = tf . zeros ( num seasons , dtype = drift scale . dtype ) , scale diag = noise scale diag ) return seasonal transition noise", "predictions": ["batch gather with gather with gather = 1 ."], "references": ["build the transition noise model for a seasonalstatespacemodel ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 432, "code": "def build constrained seasonal transition noise ( drift scale , num seasons , is last day of season ) : # # # drift scale tril nonzeros = tf . concat ( [ tf . ones ( [ num seasons - 1 , 1 ] , dtype = drift scale . dtype ) , tf . zeros ( [ num seasons - 1 , num seasons - 2 ] , dtype = drift scale . dtype ) ] , axis = - 1 ) drift scale tril = ( drift scale tril nonzeros * drift scale [ ... , tf . newaxis , tf . newaxis ] / num seasons ) def seasonal transition noise ( t ) : noise scale tril = dist util . pick scalar condition ( is last day of season ( t ) , drift scale tril , tf . zeros like ( drift scale tril ) ) return tfd . Multivariate Normal Tri L ( loc = tf . zeros ( num seasons - 1 , dtype = drift scale . dtype ) , scale tril = noise scale tril ) return seasonal transition noise", "predictions": ["broadcast the cat params for the drift and util else none else ."], "references": ["build transition noise distribution for a constrainedseasonalssm ."], "bleu": 0.10571070857151538, "rouge_l": 0.19902120717781402}
{"id": 433, "code": "def optimize ( self ) : jmodel = call Java Func ( self . value . optimize ) from bigdl . nn . layer import Layer return Layer . of ( jmodel )", "predictions": ["broadcast the input input input"], "references": ["do an optimization ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 434, "code": "def get end trigger ( options ) : if options . end Trigger Type . lower ( ) == \"epoch\" : return Max Epoch ( options . end Trigger Num ) else : return Max Iteration ( options . end Trigger Num )", "predictions": ["converged hessian hessian to inv"], "references": ["when to end the optimization based on input option ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 435, "code": "def validate optimizer ( optimizer , test data , options ) : optimizer . set validation ( batch size = options . batch Size , val rdd = test data , trigger = Every Epoch ( ) , val method = [ Top1Accuracy ( ) ] ) optimizer . set checkpoint ( Every Epoch ( ) , options . checkpoint Path )", "predictions": ["get initial initial initial initial initial initial value"], "references": ["set validation and checkpoint for distributed optimizer ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 436, "code": "def value ( self ) : if not hasattr ( self , \" value\" ) and self . path is not None : self . value = self . load ( self . path ) return self . value", "predictions": ["body body from the = = value"], "references": ["return the broadcasted value"], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 437, "code": "def call Big Dl Func ( bigdl type , name , * args ) : gateway = get gateway ( ) error = Exception ( \"Cannot find function: %s\" % name ) for jinvoker in Java Creator . instance ( bigdl type , gateway ) . value : try : api = getattr ( jinvoker , name ) result = call Java Func ( api , * args ) except Exception as e : error = e if \"does not exist\" not in str ( e ) : raise e else : return result raise error", "predictions": ["von mises cdf cdf"], "references": ["call api in pythonbigdl"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 438, "code": "def py2java ( gateway , obj ) : if isinstance ( obj , RDD ) : obj = to java object rdd ( obj ) elif isinstance ( obj , Data Frame ) : obj = obj . jdf elif isinstance ( obj , Spark Context ) : obj = obj . jsc elif isinstance ( obj , ( list , tuple ) ) : obj = List Converter ( ) . convert ( [ py2java ( gateway , x ) for x in obj ] , gateway . gateway client ) elif isinstance ( obj , dict ) : result = { } for ( key , value ) in obj . items ( ) : result [ key ] = py2java ( gateway , value ) obj = Map Converter ( ) . convert ( result , gateway . gateway client ) elif isinstance ( obj , Java Value ) : obj = obj . value elif isinstance ( obj , Java Object ) : pass elif isinstance ( obj , ( int , long , float , bool , bytes , unicode ) ) : pass else : data = bytearray ( Pickle Serializer ( ) . dumps ( obj ) ) obj = gateway . jvm . org . apache . spark . bigdl . api . python . Big DL Ser De . loads ( data ) return obj", "predictions": ["convert an object into json - ld objects ."], "references": ["convert python object into java"], "bleu": 0.19960198807747329, "rouge_l": 0.4518518518518518}
{"id": 439, "code": "def get label ( self ) : label = call Big Dl Func ( self . bigdl type , \"image Feature To Label Tensor\" , self . value ) return label . to ndarray ( )", "predictions": ["returns the best best best best ndarray"], "references": ["get label as ndarray from imagefeature"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 440, "code": "def read parquet ( cls , path , sc , bigdl type = \"float\" ) : return Distributed Image Frame ( jvalue = call Big Dl Func ( bigdl type , \"read Parquet\" , path , sc ) )", "predictions": ["check if the convergence is in the convergence ."], "references": ["read parquet file as distributedimageframe"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 441, "code": "def write parquet ( cls , path , output , sc , partition num = 1 , bigdl type = \"float\" ) : return call Big Dl Func ( bigdl type , \"write Parquet\" , path , output , sc , partition num )", "predictions": ["get the parquet for the given validate ."], "references": ["write imageframe as parquet file"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 442, "code": "def get image ( self , float key = \"floats\" , to chw = True ) : return self . image frame . get image ( float key , to chw )", "predictions": ["build an input input input input input data type"], "references": ["get image from imageframe"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 443, "code": "def get image ( self , float key = \"floats\" , to chw = True ) : tensors = call Big Dl Func ( self . bigdl type , \"local Image Frame To Image Tensor\" , self . value , float key , to chw ) return map ( lambda tensor : tensor . to ndarray ( ) , tensors )", "predictions": ["returns the integral integral of the given integral"], "references": ["get image list from imageframe"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 444, "code": "def get label ( self ) : tensor rdd = call Big Dl Func ( self . bigdl type , \"distributed Image Frame To Label Tensor Rdd\" , self . value ) return tensor rdd . map ( lambda tensor : tensor . to ndarray ( ) )", "predictions": ["returns the reconstruct reconstruct from the bigdl"], "references": ["get label rdd from imageframe"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 445, "code": "def get predict ( self , key = \"predict\" ) : predicts = call Big Dl Func ( self . bigdl type , \"distributed Image Frame To Predict\" , self . value , key ) return predicts . map ( lambda predict : ( predict [ 0 ] , predict [ 1 ] . to ndarray ( ) ) if predict [ 1 ] else ( predict [ 0 ] , None ) )", "predictions": ["get the solve solve"], "references": ["get prediction rdd from imageframe"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 446, "code": "def save keras definition ( keras model , path ) : model json = keras model . to json ( ) with open ( path , \"w\" ) as json file : json file . write ( model json )", "predictions": ["maybe validate and floating matrix ."], "references": ["save a keras model definition to json with given path"], "bleu": 0.09536752763778475, "rouge_l": 0.0}
{"id": 447, "code": "def build keras model ( ) : from keras . models import Sequential from keras . layers import Dense , Dropout , Activation , Flatten from keras . layers import Convolution2D , Max Pooling2D keras model = Sequential ( ) keras model . add ( Convolution2D ( 32 , 3 , 3 , border mode = 'valid' , input shape = input shape ) ) keras model . add ( Activation ( 'relu' ) ) keras model . add ( Convolution2D ( 32 , 3 , 3 ) ) keras model . add ( Activation ( 'relu' ) ) keras model . add ( Max Pooling2D ( pool size = ( 2 , 2 ) ) ) keras model . add ( Dropout ( 0.25 ) ) keras model . add ( Flatten ( ) ) keras model . add ( Dense ( 128 ) ) keras model . add ( Activation ( 'relu' ) ) keras model . add ( Dropout ( 0.5 ) ) keras model . add ( Dense ( 10 ) ) keras model . add ( Activation ( 'softmax' ) ) return keras model", "predictions": ["gen model model model in slices in slices in slices in slices in slices in slices in slices in slices in slices in slices in slices in slices in slices in"], "references": ["define a convnet model in keras 1 . 2 . 2"], "bleu": 0.0513487742994337, "rouge_l": 0.10418445772843724}
{"id": 448, "code": "def training ( self , is training = True ) : if is training : call Java Func ( self . value . training ) else : call Java Func ( self . value . evaluate ) return self", "predictions": ["training the training of the training training"], "references": ["set this layer in the training mode or in predition mode if is_training = false"], "bleu": 0.07796037894057231, "rouge_l": 0.17062937062937064}
{"id": 449, "code": "def build keras model ( ) : from keras . models import Sequential from keras . layers import Dense , Dropout , Activation from keras . layers import Embedding from keras . layers import LSTM from keras . layers import Convolution1D , Max Pooling1D keras model = Sequential ( ) keras model . add ( Embedding ( 20000 , 128 , input length = 100 ) ) keras model . add ( Dropout ( 0.25 ) ) keras model . add ( Convolution1D ( nb filter = 64 , filter length = 5 , border mode = 'valid' , activation = 'relu' , subsample length = 1 ) ) keras model . add ( Max Pooling1D ( pool length = 4 ) ) keras model . add ( LSTM ( 70 ) ) keras model . add ( Dense ( 1 ) ) keras model . add ( Activation ( 'sigmoid' ) ) return keras model", "predictions": ["build a keras model ."], "references": ["define a recurrent convolutional model in keras 1 . 2 . 2"], "bleu": 0.08465538735236156, "rouge_l": 0.3285457809694794}
{"id": 450, "code": "def get bigdl classpath ( ) : if os . getenv ( \"BIGDL CLASSPATH\" ) : return os . environ [ \"BIGDL CLASSPATH\" ] jar dir = os . path . abspath ( file + \"/../../\" ) jar paths = glob . glob ( os . path . join ( jar dir , \"share/lib/*.jar\" ) ) if jar paths : assert len ( jar paths ) == 1 , \"Expecting one jar: %s\" % len ( jar paths ) return jar paths [ 0 ] return \"\"", "predictions": ["return the classpath classpath"], "references": ["get and return the jar path for bigdl if exists ."], "bleu": 0.08217262444082543, "rouge_l": 0.24596774193548387}
{"id": 451, "code": "def is spark below 2 2 ( ) : import pyspark if ( hasattr ( pyspark , \"version\" ) ) : full version = pyspark . version . version parts = full version . split ( \".\" ) spark version = parts [ 0 ] + \".\" + parts [ 1 ] if ( compare version ( spark version , \"2.2\" ) >= 0 ) : return False return True", "predictions": ["return true if the spark version is below 2 . 0 ."], "references": ["check if spark version is below 2 . 2"], "bleu": 0.48633831680799433, "rouge_l": 0.6842948717948718}
{"id": 452, "code": "def attention ( inputs , state , att size , mask , scope = \"attention\" ) : with tf . variable scope ( scope ) : u = tf . concat ( [ tf . tile ( tf . expand dims ( state , axis = 1 ) , [ 1 , tf . shape ( inputs ) [ 1 ] , 1 ] ) , inputs ] , axis = 2 ) logits = tf . layers . dense ( tf . layers . dense ( u , att size , activation = tf . nn . tanh ) , 1 , use bias = False ) logits = softmax mask ( tf . squeeze ( logits , [ 2 ] ) , mask ) att weights = tf . expand dims ( tf . nn . softmax ( logits ) , axis = 2 ) res = tf . reduce sum ( att weights * inputs , axis = 1 ) return res , logits", "predictions": ["compute attention attention ."], "references": ["computes weighted sum of inputs conditioned on state"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 453, "code": "def summary gradient updates ( grads , opt , lr ) : vars grads = { } for v in tf . trainable variables ( ) : vars grads [ v . name ] = [ v , None , None ] for g , v in grads : vars grads [ v . name ] [ 1 ] = g vars grads [ v . name ] [ 2 ] = opt . get slot ( v , 'accumulator' ) ret = [ ] for vname , ( v , g , a ) in vars grads . items ( ) : if g is None : continue if isinstance ( g , tf . Indexed Slices ) : updates = lr * g . values if a is not None : updates /= tf . sqrt ( tf . gather ( a , g . indices ) ) else : updates = lr * g if a is not None : updates /= tf . sqrt ( a ) values norm = tf . sqrt ( tf . reduce sum ( v * v ) ) + 1.0e-7 updates norm = tf . sqrt ( tf . reduce sum ( updates * updates ) ) ret . append ( tf . summary . scalar ( 'UPDATE/' + vname . replace ( \":\" , \" \" ) , updates norm / values norm ) ) return ret", "predictions": ["updates the summary of the summary of the variables of the variables ."], "references": ["get summary ops for the magnitude of gradient updates"], "bleu": 0.12011055432195765, "rouge_l": 0.2819722650231125}
{"id": 454, "code": "def dump weights ( tf save dir , outfile , options ) : def get outname ( tf name ) : outname = re . sub ( ':0$' , '' , tf name ) outname = outname . lstrip ( 'lm/' ) outname = re . sub ( '/rnn/' , '/RNN/' , outname ) outname = re . sub ( '/multi rnn cell/' , '/Multi RNN Cell/' , outname ) outname = re . sub ( '/cell ' , '/Cell' , outname ) outname = re . sub ( '/lstm cell/' , '/LSTM Cell/' , outname ) if '/RNN/' in outname : if 'projection' in outname : outname = re . sub ( 'projection/kernel' , 'W P 0' , outname ) else : outname = re . sub ( '/kernel' , '/W 0' , outname ) outname = re . sub ( '/bias' , '/B' , outname ) return outname ckpt file = tf . train . latest checkpoint ( tf save dir ) config = tf . Config Proto ( allow soft placement = True ) with tf . Graph ( ) . as default ( ) : with tf . Session ( config = config ) as sess : with tf . variable scope ( 'lm' ) : Language Model ( options , False ) loader = tf . train . Saver ( ) loader . restore ( sess , ckpt file ) with h5py . File ( outfile , 'w' ) as fout : for v in tf . trainable variables ( ) : if v . name . find ( 'softmax' ) >= 0 : continue outname = get outname ( v . name ) shape = v . get shape ( ) . as list ( ) dset = fout . create dataset ( outname , shape , dtype = 'float32' ) values = sess . run ( [ v ] ) [ 0 ] dset [ ... ] = values", "predictions": ["dump the weights of the rnn weights ."], "references": ["dump the trained weights from a model to a hdf5 file ."], "bleu": 0.14544785215055717, "rouge_l": 0.3860759493670886}
{"id": 455, "code": "def read data by config ( config : dict ) : dataset config = config . get ( 'dataset' , None ) if dataset config : config . pop ( 'dataset' ) ds type = dataset config [ 'type' ] if ds type == 'classification' : reader = { 'class name' : 'basic classification reader' } iterator = { 'class name' : 'basic classification iterator' } config [ 'dataset reader' ] = { * * dataset config , * * reader } config [ 'dataset iterator' ] = { * * dataset config , * * iterator } else : raise Exception ( \"Unsupported dataset type: {}\" . format ( ds type ) ) try : reader config = dict ( config [ 'dataset reader' ] ) except Key Error : raise Config Error ( \"No dataset reader is provided in the JSON config.\" ) reader = get model ( reader config . pop ( 'class name' ) ) ( ) data path = reader config . pop ( 'data path' , '' ) if isinstance ( data path , list ) : data path = [ expand path ( x ) for x in data path ] else : data path = expand path ( data path ) return reader . read ( data path , * * reader config )", "predictions": ["read data from a config file ."], "references": ["read data by dataset_reader from specified config ."], "bleu": 0.25201472805660513, "rouge_l": 0.6587473002159828}
{"id": 456, "code": "def train evaluate model from config ( config : Union [ str , Path , dict ] , iterator : Union [ Data Learning Iterator , Data Fitting Iterator ] = None , * , to train : bool = True , evaluation targets : Optional [ Iterable [ str ] ] = None , to validate : Optional [ bool ] = None , download : bool = False , start epoch num : Optional [ int ] = None , recursive : bool = False ) -> Dict [ str , Dict [ str , float ] ] : config = parse config ( config ) if download : deep download ( config ) if to train and recursive : for subconfig in get all elems from json ( config [ 'chainer' ] , 'config path' ) : log . info ( f'Training \"{subconfig}\"' ) train evaluate model from config ( subconfig , download = False , recursive = True ) import packages ( config . get ( 'metadata' , { } ) . get ( 'imports' , [ ] ) ) if iterator is None : try : data = read data by config ( config ) except Config Error as e : to train = False log . warning ( f'Skipping training. {e.message}' ) else : iterator = get iterator from config ( config , data ) if 'train' not in config : log . warning ( 'Train config is missing. Populating with default values' ) train config = config . get ( 'train' ) if start epoch num is not None : train config [ 'start epoch num' ] = start epoch num if 'evaluation targets' not in train config and ( 'validate best' in train config or 'test best' in train config ) : log . warning ( '\"validate best\" and \"test best\" parameters are deprecated.' ' Please, use \"evaluation targets\" list instead' ) train config [ 'evaluation targets' ] = [ ] if train config . pop ( 'validate best' , True ) : train config [ 'evaluation targets' ] . append ( 'valid' ) if train config . pop ( 'test best' , True ) : train config [ 'evaluation targets' ] . append ( 'test' ) trainer class = get model ( train config . pop ( 'class name' , 'nn trainer' ) ) trainer = trainer class ( config [ 'chainer' ] , * * train config ) if to train : trainer . train ( iterator ) res = { } if iterator is not None : if to validate is not None : if evaluation targets is None : log . warning ( '\"to validate\" parameter is deprecated and will be removed in future versions.' ' Please, use \"evaluation targets\" list instead' ) evaluation targets = [ 'test' ] if to validate : evaluation targets . append ( 'valid' ) else : log . warn ( 'Both \"evaluation targets\" and \"to validate\" parameters are specified.' ' \"to validate\" is deprecated and will be ignored' ) res = trainer . evaluate ( iterator , evaluation targets , print reports = True ) trainer . get chainer ( ) . destroy ( ) res = { k : v [ 'metrics' ] for k , v in res . items ( ) } return res", "predictions": ["train model from config file ."], "references": ["make training and evaluation of the model described in corresponding configuration file ."], "bleu": 0.09728049676725326, "rouge_l": 0.29611650485436897}
{"id": 457, "code": "def load ( self ) -> None : if self . load path . exists ( ) : path = str ( self . load path . resolve ( ) ) log . info ( '[loading model from {}]' . format ( path ) ) self . net . load ( path )", "predictions": ["load the model from the filesystem ."], "references": ["checks existence of the model file loads the model if the file exists"], "bleu": 0.1114789227233716, "rouge_l": 0.2846034214618974}
{"id": 458, "code": "def build ( self ) : word inputs = kl . Input ( shape = ( None , MAX WORD LENGTH + 2 ) , dtype = \"int32\" ) inputs = [ word inputs ] word outputs = self . build word cnn ( word inputs ) if len ( self . word vectorizers ) > 0 : additional word inputs = [ kl . Input ( shape = ( None , input dim ) , dtype = \"float32\" ) for input dim , dense dim in self . word vectorizers ] inputs . extend ( additional word inputs ) additional word embeddings = [ kl . Dense ( dense dim ) ( additional word inputs [ i ] ) for i , ( , dense dim ) in enumerate ( self . word vectorizers ) ] word outputs = kl . Concatenate ( ) ( [ word outputs ] + additional word embeddings ) outputs , lstm outputs = self . build basic network ( word outputs ) compile args = { \"optimizer\" : ko . nadam ( lr = 0.002 , clipnorm = 5.0 ) , \"loss\" : \"categorical crossentropy\" , \"metrics\" : [ \"accuracy\" ] } self . model = Model ( inputs , outputs ) self . model . compile ( * * compile args ) if self . verbose > 0 : self . model . summary ( print fn = log . info ) return self", "predictions": ["builds the basic word parameters ."], "references": ["builds the network using keras ."], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 459, "code": "def build word cnn ( self , inputs ) : inputs = kl . Lambda ( kb . one hot , arguments = { \"num classes\" : self . symbols number } , output shape = lambda x : tuple ( x ) + ( self . symbols number , ) ) ( inputs ) char embeddings = kl . Dense ( self . char embeddings size , use bias = False ) ( inputs ) conv outputs = [ ] self . char output dim = 0 for window size , filters number in zip ( self . char window size , self . char filters ) : curr output = char embeddings curr filters number = ( min ( self . char filter multiple * window size , 200 ) if filters number is None else filters number ) for in range ( self . char conv layers - 1 ) : curr output = kl . Conv2D ( curr filters number , ( 1 , window size ) , padding = \"same\" , activation = \"relu\" , data format = \"channels last\" ) ( curr output ) if self . conv dropout > 0.0 : curr output = kl . Dropout ( self . conv dropout ) ( curr output ) curr output = kl . Conv2D ( curr filters number , ( 1 , window size ) , padding = \"same\" , activation = \"relu\" , data format = \"channels last\" ) ( curr output ) conv outputs . append ( curr output ) self . char output dim += curr filters number if len ( conv outputs ) > 1 : conv output = kl . Concatenate ( axis = - 1 ) ( conv outputs ) else : conv output = conv outputs [ 0 ] highway input = kl . Lambda ( kb . max , arguments = { \"axis\" : - 2 } ) ( conv output ) if self . intermediate dropout > 0.0 : highway input = kl . Dropout ( self . intermediate dropout ) ( highway input ) for i in range ( self . char highway layers - 1 ) : highway input = Highway ( activation = \"relu\" ) ( highway input ) if self . highway dropout > 0.0 : highway input = kl . Dropout ( self . highway dropout ) ( highway input ) highway output = Highway ( activation = \"relu\" ) ( highway input ) return highway output", "predictions": ["build the word cnn ."], "references": ["builds word - level network"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 460, "code": "def main ( ) : args = parser . parse args ( ) path = get settings path ( ) if args . default : if populate settings dir ( force = True ) : print ( f'Populated {path} with default settings files' ) else : print ( f'{path} is already a default settings directory' ) else : print ( f'Current Deep Pavlov settings path: {path}' )", "predictions": ["main entry point for the application ."], "references": ["deeppavlov console configuration utility ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 461, "code": "def graph wrap ( func , graph ) : @ wraps ( func ) def wrapped ( * args , * * kwargs ) : with graph . as default ( ) : return func ( * args , * * kwargs ) return wrapped", "predictions": ["wrap a graph in the graph ."], "references": ["constructs function encapsulated in the graph ."], "bleu": 0.5169731539571706, "rouge_l": 0.5714285714285714}
{"id": 462, "code": "def keras wrap ( func , graph , session ) : import keras . backend as K @ wraps ( func ) def wrapped ( * args , * * kwargs ) : with graph . as default ( ) : K . set session ( session ) return func ( * args , * * kwargs ) return wrapped", "predictions": ["keras decorator for keras ."], "references": ["constructs function encapsulated in the graph and the session ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 463, "code": "def prettify metrics ( metrics : List [ Tuple [ str , float ] ] , precision : int = 4 ) -> Ordered Dict : prettified metrics = Ordered Dict ( ) for key , value in metrics : value = round ( value , precision ) prettified metrics [ key ] = value return prettified metrics", "predictions": ["return a list of metrics for the given precision ."], "references": ["prettifies the dictionary of metrics ."], "bleu": 0.18850319022747347, "rouge_l": 0.3927038626609442}
{"id": 464, "code": "def load ( self , exclude scopes : tuple = ( 'Optimizer' , ) ) -> None : if not hasattr ( self , 'sess' ) : raise Runtime Error ( 'Your Tensor Flow model {} must' ' have sess attribute!' . format ( self . class . name ) ) path = str ( self . load path . resolve ( ) ) if tf . train . checkpoint exists ( path ) : log . info ( '[loading model from {}]' . format ( path ) ) var list = self . get saveable variables ( exclude scopes ) saver = tf . train . Saver ( var list ) saver . restore ( self . sess , path )", "predictions": ["load the model from the given path ."], "references": ["load model parameters from self . load_path"], "bleu": 0.20164945583740668, "rouge_l": 0.5398230088495575}
{"id": 465, "code": "def save ( self , exclude scopes : tuple = ( 'Optimizer' , ) ) -> None : if not hasattr ( self , 'sess' ) : raise Runtime Error ( 'Your Tensor Flow model {} must' ' have sess attribute!' . format ( self . class . name ) ) path = str ( self . save path . resolve ( ) ) log . info ( '[saving model to {}]' . format ( path ) ) var list = self . get saveable variables ( exclude scopes ) saver = tf . train . Saver ( var list ) saver . save ( self . sess , path )", "predictions": ["save the model to the given path ."], "references": ["save model parameters to self . save_path"], "bleu": 0.20164945583740668, "rouge_l": 0.5398230088495575}
{"id": 466, "code": "def print number of parameters ( ) : log . info ( 'Number of parameters: ' ) variables = tf . trainable variables ( ) blocks = defaultdict ( int ) for var in variables : block name = var . name . split ( '/' ) [ 0 ] number of parameters = np . prod ( var . get shape ( ) . as list ( ) ) blocks [ block name ] += number of parameters for block name , cnt in blocks . items ( ) : log . info ( \"{} - {}.\" . format ( block name , cnt ) ) total num parameters = np . sum ( list ( blocks . values ( ) ) ) log . info ( 'Total number of parameters equal {}' . format ( total num parameters ) )", "predictions": ["print the number of parameters of parameters: ."], "references": ["print number of * trainable * parameters in the network"], "bleu": 0.19546825878823415, "rouge_l": 0.43571428571428567}
{"id": 467, "code": "def search ( self , word , d , allow spaces = True , return cost = True ) : if not all ( ( c in self . alphabet or ( c == \" \" and self . allow spaces ) ) for c in word ) : return [ ] return self . trie search ( word , d , allow spaces = allow spaces , return cost = return cost )", "predictions": ["search for words in given word ."], "references": ["finds all dictionary words in d - window from word"], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 468, "code": "def make default operation costs ( self , allow spaces = False ) : self . operation costs = dict ( ) self . operation costs [ \"\" ] = { c : 1.0 for c in list ( self . alphabet ) + [ ' ' ] } for a in self . alphabet : current costs = { c : 1.0 for c in self . alphabet } current costs [ a ] = 0.0 current costs [ \"\" ] = 1.0 if allow spaces : current costs [ \" \" ] = 1.0 self . operation costs [ a ] = current costs for a , b in itertools . permutations ( self . alphabet , 2 ) : self . operation costs [ a + b ] = { b + a : 1.0 } if allow spaces : self . operation costs [ \" \" ] = { c : 1.0 for c in self . alphabet } self . operation costs [ \" \" ] [ \"\" ] = 1.0", "predictions": ["make the default operation for the operation ."], "references": ["sets 1 . 0 cost for every replacement insertion deletion and transposition"], "bleu": 0.10764345432696364, "rouge_l": 0.09651898734177215}
{"id": 469, "code": "def start timer ( self ) -> None : self . timer = Timer ( self . config [ 'conversation lifetime' ] , self . self destruct callback ) self . timer . start ( )", "predictions": ["starts the timer ."], "references": ["initiates self - destruct timer ."], "bleu": 0.2868106410131918, "rouge_l": 0.3860759493670886}
{"id": 470, "code": "def build model ( config : Union [ str , Path , dict ] , mode : str = 'infer' , load trained : bool = False , download : bool = False , serialized : Optional [ bytes ] = None ) -> Chainer : config = parse config ( config ) if serialized : serialized : list = pickle . loads ( serialized ) if download : deep download ( config ) import packages ( config . get ( 'metadata' , { } ) . get ( 'imports' , [ ] ) ) model config = config [ 'chainer' ] model = Chainer ( model config [ 'in' ] , model config [ 'out' ] , model config . get ( 'in y' ) ) for component config in model config [ 'pipe' ] : if load trained and ( 'fit on' in component config or 'in y' in component config ) : try : component config [ 'load path' ] = component config [ 'save path' ] except Key Error : log . warning ( 'No \"save path\" parameter for the {} component, so \"load path\" will not be renewed' . format ( component config . get ( 'class name' , component config . get ( 'ref' , 'UNKNOWN' ) ) ) ) if serialized and 'in' in component config : component serialized = serialized . pop ( 0 ) else : component serialized = None component = from params ( component config , mode = mode , serialized = component serialized ) if 'in' in component config : c in = component config [ 'in' ] c out = component config [ 'out' ] in y = component config . get ( 'in y' , None ) main = component config . get ( 'main' , False ) model . append ( component , c in , c out , in y , main ) return model", "predictions": ["build a serialized model from a config object ."], "references": ["build and return the model described in corresponding configuration file ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 471, "code": "def interact model ( config : Union [ str , Path , dict ] ) -> None : model = build model ( config ) while True : args = [ ] for in x in model . in x : args . append ( ( input ( '{}::' . format ( in x ) ) , ) ) if args [ - 1 ] [ 0 ] in { 'exit' , 'stop' , 'quit' , 'q' } : return pred = model ( * args ) if len ( model . out params ) > 1 : pred = zip ( * pred ) print ( '>>' , * pred )", "predictions": ["create a model from a config file ."], "references": ["start interaction with the model described in corresponding configuration file ."], "bleu": 0.15587146574232644, "rouge_l": 0.3070469798657718}
{"id": 472, "code": "def predict on stream ( config : Union [ str , Path , dict ] , batch size : int = 1 , file path : Optional [ str ] = None ) -> None : if file path is None or file path == '-' : if sys . stdin . isatty ( ) : raise Runtime Error ( 'To process data from terminal please use interact mode' ) f = sys . stdin else : f = open ( file path , encoding = 'utf8' ) model : Chainer = build model ( config ) args count = len ( model . in x ) while True : batch = list ( ( l . strip ( ) for l in islice ( f , batch size * args count ) ) ) if not batch : break args = [ ] for i in range ( args count ) : args . append ( batch [ i : : args count ] ) res = model ( * args ) if len ( model . out params ) == 1 : res = [ res ] for res in zip ( * res ) : res = json . dumps ( res , ensure ascii = False ) print ( res , flush = True ) if f is not sys . stdin : f . close ( )", "predictions": ["predict the data on a stream ."], "references": ["make a prediction with the component described in corresponding configuration file ."], "bleu": 0.1081377510275021, "rouge_l": 0.20098846787479407}
{"id": 473, "code": "def fn from str ( name : str ) -> Callable [ ... , Any ] : try : module name , fn name = name . split ( ':' ) except Value Error : raise Config Error ( 'Expected function description in a `module.submodules:function name` form, but got `{}`' . format ( name ) ) return getattr ( importlib . import module ( module name ) , fn name )", "predictions": ["return the module description of the given module ."], "references": ["returns a function object with the name given in string ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 474, "code": "def register metric ( metric name : str ) -> Callable [ ... , Any ] : def decorate ( fn ) : fn name = fn . module + ':' + fn . name if metric name in REGISTRY and REGISTRY [ metric name ] != fn name : log . warning ( '\"{}\" is already registered as a metric name, the old function will be ignored' . format ( metric name ) ) REGISTRY [ metric name ] = fn name return fn return decorate", "predictions": ["decorator to register a metric ."], "references": ["decorator for metric registration ."], "bleu": 0.2626909894424158, "rouge_l": 0.5545454545454546}
{"id": 475, "code": "def get metric by name ( name : str ) -> Callable [ ... , Any ] : if name not in REGISTRY : raise Config Error ( f'\"{name}\" is not registered as a metric' ) return fn from str ( REGISTRY [ name ] )", "predictions": ["get a metric by name ."], "references": ["returns a metric callable with a corresponding name ."], "bleu": 0.22172045047934616, "rouge_l": 0.5147679324894514}
{"id": 476, "code": "def read requirements ( ) : reqs path = os . path . join ( location , 'requirements.txt' ) with open ( reqs path , encoding = 'utf8' ) as f : reqs = [ line . strip ( ) for line in f if not line . strip ( ) . startswith ( '#' ) ] names = [ ] links = [ ] for req in reqs : if '://' in req : links . append ( req ) else : names . append ( req ) return { 'install requires' : names , 'dependency links' : links }", "predictions": ["read requirements from a pip pip file ."], "references": ["parses requirements from requirements . txt"], "bleu": 0.22679164443904004, "rouge_l": 0.43990384615384615}
{"id": 477, "code": "def export2hub ( weight file , hub dir , options ) : spec = make module spec ( options , str ( weight file ) ) try : with tf . Graph ( ) . as default ( ) : module = hub . Module ( spec ) with tf . Session ( ) as sess : sess . run ( tf . global variables initializer ( ) ) if hub dir . exists ( ) : shutil . rmtree ( hub dir ) module . export ( str ( hub dir ) , sess ) finally : pass", "predictions": ["run the given weight in the given weight ."], "references": ["exports a tf - hub module"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 478, "code": "def main ( ) : args = parser . parse args ( ) run ms bot framework server ( agent generator = make agent , app id = args . ms id , app secret = args . ms secret , stateful = True )", "predictions": ["main function for the bot ."], "references": ["parse parameters and run ms bot framework"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 479, "code": "def check gpu existence ( ) : global gpu available if gpu available is None : sess config = tf . Config Proto ( ) sess config . gpu options . allow growth = True try : with tf . Session ( config = sess config ) : device list = device lib . list local devices ( ) gpu available = any ( device . device type == 'GPU' for device in device list ) except Attribute Error as e : log . warning ( f'Got an Attribute Error `{e}`, assuming documentation building' ) gpu available = False return gpu available", "predictions": ["check if documentation is available ."], "references": ["r return true if at least one gpu is available"], "bleu": 0.16038842424444547, "rouge_l": 0.3588235294117647}
{"id": 480, "code": "def parse config property ( item : T , variables : Dict [ str , Union [ str , Path , float , bool , None ] ] ) -> T : if isinstance ( item , str ) : return item . format ( * * variables ) elif isinstance ( item , list ) : return [ parse config property ( item , variables ) for item in item ] elif isinstance ( item , dict ) : return { k : parse config property ( v , variables ) for k , v in item . items ( ) } else : return item", "predictions": ["training a config self . into a dict of variables . ."], "references": ["recursively apply config s variables values to its property"], "bleu": 0.11498759556447223, "rouge_l": 0.19551282051282048}
{"id": 481, "code": "def parse config ( config : Union [ str , Path , dict ] ) -> dict : if isinstance ( config , ( str , Path ) ) : config = read json ( find config ( config ) ) variables = { 'DEEPPAVLOV PATH' : os . getenv ( f'DP DEEPPAVLOV PATH' , Path ( file ) . parent . parent . parent ) } for name , value in config . get ( 'metadata' , { } ) . get ( 'variables' , { } ) . items ( ) : env name = f'DP {name}' if env name in os . environ : value = os . getenv ( env name ) variables [ name ] = value . format ( * * variables ) return parse config property ( config , variables )", "predictions": ["build a dictionary from the config dictionary 128 128 128 ."], "references": ["read config s variables and apply their values to all its properties"], "bleu": 0.10400927574124633, "rouge_l": 0.08628005657708629}
{"id": 482, "code": "def expand path ( path : Union [ str , Path ] ) -> Path : return Path ( path ) . expanduser ( ) . resolve ( )", "predictions": ["get the path of a path environ environ environ environ environ environ environ environ environ environ"], "references": ["convert relative paths to absolute with resolving user directory ."], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 483, "code": "def from params ( params : Dict , mode : str = 'infer' , serialized : Any = None , * * kwargs ) -> Component : config params = { k : resolve ( v ) for k , v in params . items ( ) } if 'ref' in config params : try : component = refs [ config params [ 'ref' ] ] if serialized is not None : component . deserialize ( serialized ) return component except Key Error : e = Config Error ( 'Component with id \"{id}\" was referenced but not initialized' . format ( id = config params [ 'ref' ] ) ) log . exception ( e ) raise e elif 'config path' in config params : from deeppavlov . core . commands . infer import build model refs = refs . copy ( ) refs . clear ( ) config = parse config ( expand path ( config params [ 'config path' ] ) ) model = build model ( config , serialized = serialized ) refs . clear ( ) refs . update ( refs ) try : refs [ config params [ 'id' ] ] = model except Key Error : pass return model cls name = config params . pop ( 'class name' , None ) if not cls name : e = Config Error ( 'Component config has no `class name` nor `ref` fields' ) log . exception ( e ) raise e cls = get model ( cls name ) config params = { k : init param ( v , mode ) for k , v in config params . items ( ) } try : spec = inspect . getfullargspec ( cls ) if 'mode' in spec . args + spec . kwonlyargs or spec . varkw is not None : kwargs [ 'mode' ] = mode component = cls ( * * dict ( config params , * * kwargs ) ) try : refs [ config params [ 'id' ] ] = component except Key Error : pass except Exception : log . exception ( \"Exception in {}\" . format ( cls ) ) raise if serialized is not None : component . deserialize ( serialized ) return component", "predictions": ["create a new component from a split split spark compare ."], "references": ["builds and returns the component from corresponding dictionary of parameters ."], "bleu": 0.16108992769687397, "rouge_l": 0.2727272727272727}
{"id": 484, "code": "def run ( self ) -> None : while True : request = self . input queue . get ( ) response = self . handle request ( request ) self . output queue . put ( response )", "predictions": ["attention the job ."], "references": ["thread run method implementation ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 485, "code": "def refresh valid certs ( self ) -> None : self . timer = Timer ( REFRESH VALID CERTS PERIOD SECS , self . refresh valid certs ) self . timer . start ( ) expired certificates = [ ] for valid cert url , valid cert in self . valid certificates . items ( ) : valid cert : Validated Cert = valid cert cert expiration time : datetime = valid cert . expiration timestamp if datetime . utcnow ( ) > cert expiration time : expired certificates . append ( valid cert url ) for expired cert url in expired certificates : del self . valid certificates [ expired cert url ] log . info ( f'Validation period of {expired cert url} certificate expired' )", "predictions": ["summary variables that are not variables that are not gradient . . . . . . ."], "references": ["conducts cleanup of periodical certificates with expired validation ."], "bleu": 0.07223943354597204, "rouge_l": 0.0814419225634179}
{"id": 486, "code": "def cls from str ( name : str ) -> type : try : module name , cls name = name . split ( ':' ) except Value Error : raise Config Error ( 'Expected class description in a `module.submodules:Class Name` form, but got `{}`' . format ( name ) ) return getattr ( importlib . import module ( module name ) , cls name )", "predictions": ["in string returns a dotted class class name name name name name name name name name name name name name name name name name name name name name name name name"], "references": ["returns a class object with the name given as a string ."], "bleu": 0.06106432774355542, "rouge_l": 0.20215410107705056}
{"id": 487, "code": "def get model ( name : str ) -> type : if name not in REGISTRY : if ':' not in name : raise Config Error ( \"Model {} is not registered.\" . format ( name ) ) return cls from str ( name ) return cls from str ( REGISTRY [ name ] )", "predictions": ["read a data data data from the database . . ."], "references": ["returns a registered class object with the name given in the string ."], "bleu": 0.11294012253658708, "rouge_l": 0.24629878869448185}
{"id": 488, "code": "def list jobs ( self ) : res = h2o . api ( \"GET /3/Jobs\" ) table = [ [ \"type\" ] , [ \"dest\" ] , [ \"description\" ] , [ \"status\" ] ] for job in res [ \"jobs\" ] : job dest = job [ \"dest\" ] table [ 0 ] . append ( self . translate job type ( job dest [ \"type\" ] ) ) table [ 1 ] . append ( job dest [ \"name\" ] ) table [ 2 ] . append ( job [ \"description\" ] ) table [ 3 ] . append ( job [ \"status\" ] ) return table", "predictions": ["train the evaluate evaluate evaluate the to the system str str str str str"], "references": ["list all jobs performed by the cluster ."], "bleu": 0.08839374326825923, "rouge_l": 0.09561128526645768}
{"id": 489, "code": "def list timezones ( self ) : from h2o . expr import Expr Node return h2o . H2O Frame . expr ( expr = Expr Node ( \"list Time Zones\" ) ) . frame ( )", "predictions": ["returns a load - serializable load the timezones ."], "references": ["return the list of all known timezones ."], "bleu": 0.19960198807747329, "rouge_l": 0.35672514619883033}
{"id": 490, "code": "def summary ( self , key , column = \"C1\" , timeout Secs = 10 , * * kwargs ) : params dict = { } h2o methods . check params update kwargs ( params dict , kwargs , 'summary' , True ) result = self . do json request ( '3/Frames.json/%s/columns/%s/summary' % ( key , column ) , timeout = timeout Secs , params = params dict ) h2o sandbox . check sandbox for errors ( ) return result", "predictions": ["requests a build build build build"], "references": ["return the summary for a single column for a single frame in the h2o cluster ."], "bleu": 0.041721848418993325, "rouge_l": 0.0840220385674931}
{"id": 491, "code": "def delete frame ( self , key , ignore Missing Key = True , timeout Secs = 60 , * * kwargs ) : assert key is not None , '\"key\" parameter is null' result = self . do json request ( '/3/Frames.json/' + key , cmd = 'delete' , timeout = timeout Secs ) if not ignore Missing Key and 'f00b4r' in result : raise Value Error ( 'Frame key not found: ' + key ) return result", "predictions": ["build a word word from the redis server x x x"], "references": ["delete a frame on the h2o cluster given its key ."], "bleu": 0.12605968092174913, "rouge_l": 0.18181818181818182}
{"id": 492, "code": "def compute model metrics ( self , model , frame , timeout Secs = 60 , * * kwargs ) : assert model is not None , '\"model\" parameter is null' assert frame is not None , '\"frame\" parameter is null' models = self . models ( key = model , timeout Secs = timeout Secs ) assert models is not None , \"/Models REST call failed\" assert models [ 'models' ] [ 0 ] [ 'model id' ] [ 'name' ] == model , \"/Models/{0} returned Model {1} rather than Model {2}\" . format ( model , models [ 'models' ] [ 0 ] [ 'key' ] [ 'name' ] , model ) frames = self . frames ( key = frame ) assert frames is not None , \"/Frames/{0} REST call failed\" . format ( frame ) print \"frames:\" , dump json ( frames ) result = self . do json request ( '/3/Model Metrics.json/models/' + model + '/frames/' + frame , cmd = 'post' , timeout = timeout Secs ) mm = result [ 'model metrics' ] [ 0 ] verboseprint ( \"model metrics: \" + repr ( mm ) ) h2o sandbox . check sandbox for errors ( ) return mm", "predictions": ["main method for the model command"], "references": ["score a model on the h2o cluster on the given frame and return only the model metrics ."], "bleu": 0.0393440467350332, "rouge_l": 0.15288220551378442}
{"id": 493, "code": "def delete model ( self , key , ignore Missing Key = True , timeout Secs = 60 , * * kwargs ) : assert key is not None , '\"key\" parameter is null' result = self . do json request ( '/3/Models.json/' + key , cmd = 'delete' , timeout = timeout Secs ) if not ignore Missing Key and 'f00b4r' in result : raise Value Error ( 'Model key not found: ' + key ) verboseprint ( \"delete model result:\" , dump json ( result ) ) return result", "predictions": ["graph a wrap wrap wrap it in the redis wrap"], "references": ["delete a model on the h2o cluster given its key ."], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 494, "code": "def tabulate ( self , tablefmt = \"simple\" , rollups = False , rows = 10 ) : if not self . is valid ( ) : self . fill ( rows = rows ) d = collections . Ordered Dict ( ) if rollups : col = next ( iter ( viewvalues ( self . data ) ) ) lrows = len ( col [ 'data' ] ) d [ \"\" ] = [ \"type\" , \"mins\" , \"mean\" , \"maxs\" , \"sigma\" , \"zeros\" , \"missing\" ] + list ( map ( str , range ( lrows ) ) ) for k , v in viewitems ( self . data ) : x = v [ 'data' ] t = v [ \"type\" ] if t == \"enum\" : domain = v [ 'domain' ] x = [ \"\" if math . isnan ( idx ) else domain [ int ( idx ) ] for idx in x ] elif t == \"time\" : x = [ \"\" if math . isnan ( z ) else time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( z / 1000 ) ) for z in x ] if rollups : mins = v [ 'mins' ] [ 0 ] if v [ 'mins' ] and v [ \"type\" ] != \"enum\" else None maxs = v [ 'maxs' ] [ 0 ] if v [ 'maxs' ] and v [ \"type\" ] != \"enum\" else None #Cross check type with mean and sigma. Set to None if of type enum. if v [ 'type' ] == \"enum\" : v [ 'mean' ] = v [ 'sigma' ] = v [ 'zero count' ] = None x = [ v [ 'type' ] , mins , v [ 'mean' ] , maxs , v [ 'sigma' ] , v [ 'zero count' ] , v [ 'missing count' ] ] + x d [ k ] = x return tabulate . tabulate ( d , headers = \"keys\" , tablefmt = tablefmt )", "predictions": ["returns a keras keras keras keras data"], "references": ["pretty tabulated string of all the cached data and column names"], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 495, "code": "def run instances ( count , ec2 config , region , wait For SSH = True , tags = None ) : ec2params = inheritparams ( ec2 config , EC2 API RUN INSTANCE ) ec2params . setdefault ( 'min count' , count ) ec2params . setdefault ( 'max count' , count ) reservation = None conn = ec2 connect ( region ) try : reservation = conn . run instances ( * * ec2params ) log ( 'Reservation: {0}' . format ( reservation . id ) ) log ( 'Waiting for {0} EC2 instances {1} to come up, this can take 1-2 minutes.' . format ( len ( reservation . instances ) , reservation . instances ) ) start = time . time ( ) time . sleep ( 1 ) for instance in reservation . instances : while instance . update ( ) == 'pending' : time . sleep ( 1 ) h2o cmd . dot ( ) if not instance . state == 'running' : raise Exception ( '\\033[91m[ec2] Error waiting for running state. Instance is in state {0}.\\033[0m' . format ( instance . state ) ) log ( 'Instances started in {0} seconds' . format ( time . time ( ) - start ) ) log ( 'Instances: ' ) for inst in reservation . instances : log ( \"   {0} ({1}) : public ip: {2}, private ip: {3}\" . format ( inst . public dns name , inst . id , inst . ip address , inst . private ip address ) ) if wait For SSH : wait for ssh ( [ i . private ip address for i in reservation . instances ] ) try : if tags : conn . create tags ( [ i . id for i in reservation . instances ] , tags ) except : warn ( 'Something wrong during tagging instances. Exceptions IGNORED!' ) print sys . exc info ( ) pass return reservation except : print \"\\033[91m Unexpected error\\033[0m :\" , sys . exc info ( ) if reservation : terminate reservation ( reservation , region ) raise", "predictions": ["prettify ec2 to prettify metrics"], "references": ["create a new reservation for count instances"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 496, "code": "def terminate instances ( instances , region ) : if not instances : return conn = ec2 connect ( region ) log ( \"Terminating instances {0}.\" . format ( instances ) ) conn . terminate instances ( instances ) log ( \"Done\" )", "predictions": ["load ec2 instances and load them raise exception if not already running raise an error raise exception"], "references": ["terminate all the instances given by its ids"], "bleu": 0.07223943354597204, "rouge_l": 0.08555399719495091}
{"id": 497, "code": "def stop instances ( instances , region ) : if not instances : return conn = ec2 connect ( region ) log ( \"Stopping instances {0}.\" . format ( instances ) ) conn . stop instances ( instances ) log ( \"Done\" )", "predictions": ["save the ec2 instances instance raise an error if not already running"], "references": ["stop all the instances given by its ids"], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 498, "code": "def start instances ( instances , region ) : if not instances : return conn = ec2 connect ( region ) log ( \"Starting instances {0}.\" . format ( instances ) ) conn . start instances ( instances ) log ( \"Done\" )", "predictions": ["print list of number of number of number of number of number of number"], "references": ["start all the instances given by its ids"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 499, "code": "def reboot instances ( instances , region ) : if not instances : return conn = ec2 connect ( region ) log ( \"Rebooting instances {0}.\" . format ( instances ) ) conn . reboot instances ( instances ) log ( \"Done\" )", "predictions": ["search for an ec2 ec2"], "references": ["reboot all the instances given by its ids"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 500, "code": "def wait for ssh ( ips , port = 22 , skip Alive = True , requiredsuccess = 3 ) : log ( 'Waiting for SSH on following hosts: {0}' . format ( ips ) ) for ip in ips : if not skip Alive or not ssh live ( ip , port ) : log ( 'Waiting for SSH on instance {0}...' . format ( ip ) ) count = 0 while count < requiredsuccess : if ssh live ( ip , port ) : count += 1 else : count = 0 time . sleep ( 1 ) h2o cmd . dot ( )", "predictions": ["make sure that the if operation is operation"], "references": ["wait for ssh service to appear on given hosts"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 501, "code": "def join ( self ) : self . future = False self . job . poll ( ) model key = self . job . dest key self . job = None model json = h2o . api ( \"GET /%d/Models/%s\" % ( self . rest version , model key ) ) [ \"models\" ] [ 0 ] self . resolve model ( model key , model json )", "predictions": ["start a running = stopped ."], "references": ["wait until job s completion ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 502, "code": "def signal handler ( signum , stackframe ) : global g runner global g handling signal if g handling signal : return g handling signal = True print ( \"\" ) print ( \"----------------------------------------------------------------------\" ) print ( \"\" ) print ( \"SIGNAL CAUGHT (\" + str ( signum ) + \").  TEARING DOWN CLOUDS.\" ) print ( \"\" ) print ( \"----------------------------------------------------------------------\" ) g runner . terminate ( )", "predictions": ["handles build build if it is not already running"], "references": ["helper function to handle caught signals ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 503, "code": "def wipe output dir ( ) : print ( \"Wiping output directory.\" ) try : if os . path . exists ( g output dir ) : shutil . rmtree ( str ( g output dir ) ) except OS Error as e : print ( \"ERROR: Removing output directory %s failed: \" % g output dir ) print ( \"       (errno {0}): {1}\" . format ( e . errno , e . strerror ) ) print ( \"\" ) sys . exit ( 1 )", "predictions": ["delete the model directory"], "references": ["clear the output directory ."], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 504, "code": "def get ip ( self ) : if len ( self . client nodes ) > 0 : node = self . client nodes [ 0 ] else : node = self . nodes [ 0 ] return node . get ip ( )", "predictions": ["file on the client"], "references": ["return an ip to use to talk to this cluster ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 505, "code": "def get port ( self ) : if len ( self . client nodes ) > 0 : node = self . client nodes [ 0 ] else : node = self . nodes [ 0 ] return node . get port ( )", "predictions": ["returns the from the client"], "references": ["return a port to use to talk to this cluster ."], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 506, "code": "def determine vec size ( self ) : first column = self . pre trained . types [ self . pre trained . columns [ 0 ] ] if first column != 'string' : raise H2O Value Error ( \"First column of given pre trained model %s is required to be a String\" , self . pre trained . frame id ) if list ( self . pre trained . types . values ( ) ) . count ( 'string' ) > 1 : raise H2O Value Error ( \"There are multiple columns in given pre trained model %s with a String type.\" , self . pre trained . frame id ) self . vec size = self . pre trained . dim [ 1 ] - 1", "predictions": ["register the first size to the first"], "references": ["determines vec_size for a pre - trained model after basic model verification ."], "bleu": 0.06628576403773602, "rouge_l": 0.0}
{"id": 507, "code": "def get lambda source code ( lambda fn , src ) : def gen lambdas ( ) : def gen ( ) : yield src + \"\\n\" g = gen ( ) step = 0 tokens = [ ] for tok in tokenize . generate tokens ( getattr ( g , \"next\" , getattr ( g , \" next \" , None ) ) ) : if step == 0 : if tok [ 0 ] == tokenize . NAME and tok [ 1 ] == \"lambda\" : step = 1 tokens = [ tok ] level = 0 elif step == 1 : if tok [ 0 ] == tokenize . NAME : tokens . append ( tok ) step = 2 else : step = 0 elif step == 2 : if tok [ 0 ] == tokenize . OP and tok [ 1 ] == \":\" : tokens . append ( tok ) step = 3 else : step = 0 elif step == 3 : if level == 0 and ( tok [ 0 ] == tokenize . OP and tok [ 1 ] in \",)\" or tok [ 0 ] == tokenize . ENDMARKER ) : yield tokenize . untokenize ( tokens ) . strip ( ) step = 0 else : tokens . append ( tok ) if tok [ 0 ] == tokenize . OP : if tok [ 1 ] in \"[({\" : level += 1 if tok [ 1 ] in \"])}\" : level -= 1 assert not tokens actual code = lambda fn . code . co code for lambda src in gen lambdas ( ) : try : fn = eval ( lambda src , globals ( ) , locals ( ) ) if fn . code . co code == actual code : return lambda src . split ( \":\" , 1 ) [ 1 ] . strip ( ) except Exception : pass return \"<lambda>\"", "predictions": ["get the by by metric name"], "references": ["attempt to find the source code of the lambda_fn within the string src ."], "bleu": 0.05822753005110548, "rouge_l": 0.09327217125382263}
{"id": 508, "code": "def name ( self , src = None ) : res = [ get type name ( tt , src ) for tt in self . types ] if len ( res ) == 2 and \"None\" in res : res . remove ( \"None\" ) return \"?\" + res [ 0 ] else : return \" | \" . join ( res )", "predictions": ["returns the read read read read - only read - only read - only - only read read - only read read - safe read - safe read - only -"], "references": ["return string representing the name of this type ."], "bleu": 0.03901663112717908, "rouge_l": 0.055505004549590536}
{"id": 509, "code": "def check ( self , var ) : return all ( check type ( var , tt ) for tt in self . types )", "predictions": ["check check of all str str"], "references": ["return true if the variable matches this type and false otherwise ."], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 510, "code": "def name ( self , src = None ) : return \" & \" . join ( get type name ( tt , src ) for tt in self . types )", "predictions": ["returns the main main main main main main main main main loop framework framework framework framework framework framework framework framework framework"], "references": ["return string representing the name of this type ."], "bleu": 0.05809665204409193, "rouge_l": 0.071849234393404}
{"id": 511, "code": "def check ( self , var ) : return not any ( check type ( var , tt ) for tt in self . types )", "predictions": ["check if the input is is is is is is is is is is is is is is is is is is is is is is is is is is is"], "references": ["return true if the variable does not match any of the types and false otherwise ."], "bleu": 0.0513487742994337, "rouge_l": 0.09030347890451518}
{"id": 512, "code": "def name ( self , src = None ) : if len ( self . types ) > 1 : return \"!(%s)\" % str ( \"|\" . join ( get type name ( tt , src ) for tt in self . types ) ) else : return \"!\" + get type name ( self . types [ 0 ] , src )", "predictions": ["returns the name of the source ."], "references": ["return string representing the name of this type ."], "bleu": 0.27470644934024185, "rouge_l": 0.48897795591182364}
{"id": 513, "code": "def check ( self , var ) : return isinstance ( var , tuple ) and all ( check type ( t , self . element type ) for t in var )", "predictions": ["check if var is a tuple of var ."], "references": ["return true if the variable matches this type and false otherwise ."], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 514, "code": "def check ( self , var ) : if not isinstance ( var , dict ) : return False if any ( key not in self . types for key in var ) : return False for key , ktype in viewitems ( self . types ) : val = var . get ( key , None ) if not check type ( val , ktype ) : return False return True", "predictions": ["check if variable is a dict"], "references": ["return true if the variable matches this type and false otherwise ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 515, "code": "def name ( self , src = None ) : return \"{%s}\" % \", \" . join ( \"%s: %s\" % ( key , get type name ( ktype , src ) ) for key , ktype in viewitems ( self . types ) )", "predictions": ["return the name of this source ."], "references": ["return string representing the name of this type ."], "bleu": 0.42258673683841264, "rouge_l": 0.7334669338677354}
{"id": 516, "code": "def check ( self , var ) : return ( isinstance ( var , int type ) and ( self . lower bound is None or var >= self . lower bound ) and ( self . upper bound is None or var <= self . upper bound ) )", "predictions": ["check that variable is of type ."], "references": ["return true if the variable matches the specified type ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 517, "code": "def name ( self , src = None ) : if self . upper bound is None and self . lower bound is None : return \"int\" if self . upper bound is None : if self . lower bound == 1 : return \"int>0\" return \"int\u2265%d\" % s lf. l ower bound if self . lower bound is None : return \"int\u2264%d\" % s lf. u pper bound return \"int[%d\u2026%d]\" % ( e lf. l ower bound,  s lf. u pper bound)", "predictions": ["return the name of the upper - bound name ."], "references": ["return string representing the name of this type ."], "bleu": 0.25965358893403384, "rouge_l": 0.5313588850174217}
{"id": 518, "code": "def check ( self , var ) : return ( isinstance ( var , num type ) and ( self . lower bound is None or var >= self . lower bound ) and ( self . upper bound is None or var <= self . upper bound ) )", "predictions": ["check if variable is in variable ."], "references": ["return true if the variable matches the specified type ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 519, "code": "def check ( self , var ) : if self . class is None : self . init ( ) return self . class and self . checker ( var , self . class )", "predictions": ["check if this object is a checker ."], "references": ["return true if the variable matches this type and false otherwise ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 520, "code": "def check ( self , var ) : if not isinstance ( var , str type ) : return False return enum mangle ( var ) in self . consts", "predictions": ["check if a string is a enum value"], "references": ["check whether the provided value is a valid enum constant ."], "bleu": 0.17250013293422076, "rouge_l": 0.4093959731543625}
{"id": 521, "code": "def get config ( ) : self = H2O Config Reader . get instance ( ) if not self . config loaded : self . read config ( ) return self . config", "predictions": ["get the configuration from the config file"], "references": ["retrieve the config as a dictionary of key - value pairs ."], "bleu": 0.11967409389919142, "rouge_l": 0.20098846787479407}
{"id": 522, "code": "def read config ( self ) : self . config loaded = True conf = [ ] for f in self . candidate log files ( ) : if os . path . isfile ( f ) : self . logger . info ( \"Reading config file %s\" % f ) section rx = re . compile ( r\"^\\[(\\w+)\\]$\" ) keyvalue rx = re . compile ( r\"^(\\w+:)?([\\w.]+)\\s*=(.*)$\" ) with io . open ( f , \"rt\" , encoding = \"utf-8\" ) as config file : section name = None for lineno , line in enumerate ( config file ) : line = line . strip ( ) if line == \"\" or line . startswith ( \"#\" ) : continue m1 = section rx . match ( line ) if m1 : section name = m1 . group ( 1 ) continue m2 = keyvalue rx . match ( line ) if m2 : lng = m2 . group ( 1 ) key = m2 . group ( 2 ) val = m2 . group ( 3 ) . strip ( ) if lng and lng . lower ( ) != \"py:\" : continue if section name : key = section name + \".\" + key if key in H2O Config Reader . allowed config keys : conf . append ( ( key , val ) ) else : self . logger . error ( \"Key %s is not a valid config key\" % key ) continue self . logger . error ( \"Syntax error in config file line %d: %s\" % ( lineno , line ) ) self . config = dict ( conf ) return", "predictions": ["read the configuration from the config file ."], "references": ["find and parse config file storing all variables in self . _config ."], "bleu": 0.12139281957861149, "rouge_l": 0.2739520958083832}
{"id": 523, "code": "def candidate log files ( ) : relpath = \".h2oconfig\" prevpath = None while True : abspath = os . path . abspath ( relpath ) if abspath == prevpath : break prevpath = abspath relpath = \"../\" + relpath yield abspath yield os . path . expanduser ( \"~/.h2oconfig\" )", "predictions": ["return a list of all log files in the candidate directory ."], "references": ["return possible locations for the . h2oconfig file one at a time ."], "bleu": 0.12020484516681697, "rouge_l": 0.23828125000000006}
{"id": 524, "code": "def recalculate model parameters ( self , now ) : time until end = self . estimate progress completion time ( now ) - now assert time until end >= 0 , \"Estimated progress completion cannot be in the past.\" x real = self . get real progress ( ) if x real == 1 : t0 , x0 , v0 , ve = now , 1 , 0 , 0 else : x0 , v0 = self . compute progress at time ( now ) t0 = now if x0 >= 1 : t0 , x0 , v0 = self . t0 , self . x0 , self . v0 time until end += now - t0 z = self . BETA * time until end max speed = ( 1 - x real ** 2 ) / self . FINISH DELAY ve = v0 + ( self . BETA * ( 1 - x0 ) - v0 * z ) / ( z - 1 + math . exp ( - z ) ) if ve < 0 : v0 = self . BETA * ( 1 - x0 ) / ( 1 - math . exp ( - z ) ) ve = 0 if ve > max speed : ve = max speed self . t0 , self . x0 , self . v0 , self . ve = t0 , x0 , v0 , ve", "predictions": ["recalculate the parameters of the model parameters ."], "references": ["compute t0 x0 v0 ve ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 525, "code": "def draw ( self , txt , final = False ) : if not self . file mode : sys . stdout . write ( \"\\r\" ) sys . stdout . write ( txt ) if final and not isinstance ( self . widget , Hidden Widget ) : sys . stdout . write ( \"\\n\" ) else : if not self . file mode : sys . stdout . write ( \"\\r\" ) sys . stdout . flush ( )", "predictions": ["draw the file on the terminal"], "references": ["print the rendered string to the stdout ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 526, "code": "def render ( self , progress , width = None , status = None ) : results = [ widget . render ( progress , width = self . widget lengths [ i ] , status = status ) for i , widget in enumerate ( self . widgets ) ] if self . file mode : res = \"\" for i , result in enumerate ( results ) : res += result . rendered if result . length < self . widget lengths [ i ] and progress < 1 : break res += \" \" if i < len ( results ) - 1 else \"\" rendered str = res [ len ( self . rendered ) : ] self . rendered = res else : rendered str = \" \" . join ( r . rendered for r in results ) if self . to render : rendered str = self . to render + rendered str self . to render = None next progress = min ( r . next progress for r in results ) next time = min ( r . next time for r in results ) return Render Result ( rendered str , next progress = next progress , next time = next time )", "predictions": ["render the html representation of the element ."], "references": ["render the widget ."], "bleu": 0.22679164443904004, "rouge_l": 0.5319767441860466}
{"id": 527, "code": "def compute widget sizes ( self ) : wl = [ 0 ] * len ( self . widgets ) flex count = 0 for i , widget in enumerate ( self . widgets ) : if isinstance ( widget , Progress Bar Flexible Widget ) : flex count += 1 else : wl [ i ] = widget . render ( 1 ) . length remaining width = self . width - sum ( wl ) remaining width -= len ( self . widgets ) - 1 if remaining width < 10 * flex count : if self . file mode : remaining width = 10 * flex count else : widget0 = self . widgets [ 0 ] if isinstance ( widget0 , PBW String ) and remaining width + widget0 . render ( 0 ) . length >= 10 * flex count : remaining width += widget0 . render ( 0 ) . length + 1 self . to render = widget0 . render ( 0 ) . rendered + \"\\n\" self . widgets = self . widgets [ 1 : ] if remaining width < 10 * flex count : self . file mode = True remaining width = 10 * flex count remaining width = max ( remaining width , 10 * flex count ) for i , widget in enumerate ( self . widgets ) : if isinstance ( widget , Progress Bar Flexible Widget ) : target length = int ( remaining width / flex count ) result = widget . render ( 1 , target length ) wl [ i ] = result . length remaining width -= result . length flex count -= 1 return wl", "predictions": ["compute the widget sizes ."], "references": ["initial rendering stage done in order to compute widths of all widgets ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 528, "code": "def get terminal size ( ) : if not sys . stdout . isatty ( ) : return 80 try : import subprocess ret = subprocess . check output ( [ \"stty\" , \"size\" ] ) . strip ( ) . split ( \" \" ) if len ( ret ) == 2 : return int ( ret [ 1 ] ) except : pass try : from termios import TIOCGWINSZ from fcntl import ioctl from struct import unpack res = unpack ( \"hh\" , ioctl ( sys . stdout , TIOCGWINSZ , b\"1234\" ) ) return int ( res [ 1 ] ) except : pass return int ( os . environ . get ( \"COLUMNS\" , 80 ) )", "predictions": ["return the terminal size of the terminal"], "references": ["find current stdout s width in characters ."], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 529, "code": "def render ( self , progress , width = None , status = None ) : if width <= 3 : return Render Result ( ) bar width = width - 2 n chars = int ( progress * bar width + 0.001 ) endf , endl = self . bar ends if self . file mode : out = endf out += self . bar symbols [ - 1 ] * n chars out += endl if progress == 1 else \"\" if status : out += \" (%s)\" % status next progress = ( n chars + 1 ) / bar width rendered len = len ( out ) else : frac chars = int ( ( progress * bar width - n chars ) * len ( self . bar symbols ) ) out = endf out += self . bar symbols [ - 1 ] * n chars out += self . bar symbols [ frac chars - 1 ] if frac chars > 0 else \"\" rendered len = len ( out ) if status : out += colorama . Fore . RED + \" (\" + status + \")\" + colorama . Style . RESET ALL rendered len += 3 + len ( status ) out += \" \" * ( width - 1 - rendered len ) out += endl next progress = ( n chars + ( frac chars + 1 ) / len ( self . bar symbols ) ) / bar width rendered len += max ( 0 , width - 1 - rendered len ) + 1 return Render Result ( rendered = out , length = rendered len , next progress = next progress )", "predictions": ["render the html representation of the file ."], "references": ["render the widget ."], "bleu": 0.22679164443904004, "rouge_l": 0.5319767441860466}
{"id": 530, "code": "def set encoding ( self , encoding ) : self . bar ends = \"[]\" self . bar symbols = \"#\" if not encoding : return s1 = \"\\u258F\\u258E\\u258D\\u258C\\u258B\\u258A\\u2589\\u2588\" s2 = \"\\u258C\\u2588\" s3 = \"\\u2588\" if self . file mode : s1 = s2 = None assert len ( s3 ) == 1 for s in ( s1 , s2 , s3 ) : if s is None : continue try : s . encode ( encoding ) self . bar ends = \"||\" self . bar symbols = s return except Unicode Encode Error : pass except Lookup Error : print ( \"Warning: unknown encoding %s\" % encoding )", "predictions": ["set the encoding of the file ."], "references": ["inform the widget about the encoding of the underlying character stream ."], "bleu": 0.26488231194950745, "rouge_l": 0.5024711696869852}
{"id": 531, "code": "def render ( self , progress , width = None , status = None ) : current pct = int ( progress * 100 + 0.1 ) return Render Result ( rendered = \"%3d%%\" % current pct , next progress = ( current pct + 1 ) / 100 )", "predictions": ["render the rendered rendered progress bar ."], "references": ["render the widget ."], "bleu": 0.2626909894424158, "rouge_l": 0.5736677115987461}
{"id": 532, "code": "def refresh ( self ) : self . ex . cache . flush ( ) self . frame ( fill cache = True )", "predictions": ["refresh the cache from the cache ."], "references": ["reload frame information from the backend h2o server ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 533, "code": "def structure ( self ) : df = self . as data frame ( use pandas = False ) cn = df . pop ( 0 ) nr = self . nrow nc = self . ncol width = max ( [ len ( c ) for c in cn ] ) isfactor = self . isfactor ( ) numlevels = self . nlevels ( ) lvls = self . levels ( ) print ( \"H2O Frame: '{}' \\n Dimensions: {} obs. of {} variables\" . format ( self . frame id , nr , nc ) ) for i in range ( nc ) : print ( \"$ {} {}: \" . format ( cn [ i ] , ' ' * ( width - max ( 0 , len ( cn [ i ] ) ) ) ) , end = ' ' ) if isfactor [ i ] : nl = numlevels [ i ] print ( \"Factor w/ {} level(s) {} \" . format ( nl , '\"' + '\",\"' . join ( lvls [ i ] ) + '\"' ) , end = '\\n' ) else : print ( \"num {}\" . format ( \" \" . join ( it [ 0 ] if it else \"nan\" for it in h2o . as list ( self [ : 10 , i ] , False ) [ 1 : ] ) ) )", "predictions": ["print the data in a readable format"], "references": ["compactly display the internal structure of an h2oframe ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 534, "code": "def parse text ( text ) : assert isinstance ( text , str type ) , \"`text` parameter should be a string, got %r\" % type ( text ) gen = iter ( text . splitlines ( True ) ) readline = gen . next if hasattr ( gen , \"next\" ) else gen . next return Code ( tokenize ( readline ) )", "predictions": ["parse text into a text ."], "references": ["parse code from a string of text ."], "bleu": 0.236682065782701, "rouge_l": 0.5570776255707762}
{"id": 535, "code": "def parse file ( filename ) : assert isinstance ( filename , str type ) , \"`filename` parameter should be a string, got %r\" % type ( filename ) with open ( filename , \"rt\" , encoding = \"utf-8\" ) as f : return Code ( tokenize ( f . readline ) )", "predictions": ["parse a file into a string ."], "references": ["parse the provided file and return code object ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 536, "code": "def move ( self , drow , dcol = 0 ) : self . start row += drow self . start col += dcol self . end row += drow self . end col += dcol", "predictions": ["move drow to beginning of drow"], "references": ["move the token by drow rows and dcol columns ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 537, "code": "def unparse ( self ) : ut = Untokenizer ( start row = self . tokens [ 0 ] . start row ) self . unparse ( ut ) return ut . result ( )", "predictions": ["return the row of the row ."], "references": ["convert the parsed representation back into the source code ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 538, "code": "def centers ( self ) : o = self . model json [ \"output\" ] cvals = o [ \"centers\" ] . cell values centers = [ list ( cval [ 1 : ] ) for cval in cvals ] return centers", "predictions": ["returns a list of centers centers centers centers"], "references": ["the centers for the kmeans model ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 539, "code": "def centers std ( self ) : o = self . model json [ \"output\" ] cvals = o [ \"centers std\" ] . cell values centers std = [ list ( cval [ 1 : ] ) for cval in cvals ] centers std = [ list ( x ) for x in zip ( * centers std ) ] return centers std", "predictions": ["standard centers of the centers of the cell"], "references": ["the standardized centers for the kmeans model ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 540, "code": "def version check ( ) : from . init import version as ver pkg ci = h2oconn . cluster if not ci : raise H2O Connection Error ( \"Connection not initialized. Did you run h2o.connect()?\" ) ver h2o = ci . version if ver pkg == \"SUBST PROJECT VERSION\" : ver pkg = \"UNKNOWN\" if str ( ver h2o ) != str ( ver pkg ) : branch name h2o = ci . branch name build number h2o = ci . build number if build number h2o is None or build number h2o == \"unknown\" : raise H2O Connection Error ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"Upgrade H2O and h2o-Python to latest stable version - \" \"http://h2o-release.s3.amazonaws.com/h2o/latest stable.html\" \"\" . format ( ver h2o , ver pkg ) ) elif build number h2o == \"99999\" : raise H2O Connection Error ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"This is a developer build, please contact your developer.\" \"\" . format ( ver h2o , ver pkg ) ) else : raise H2O Connection Error ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"Install the matching h2o-Python version from - \" \"http://h2o-release.s3.amazonaws.com/h2o/{2}/{3}/index.html.\" \"\" . format ( ver h2o , ver pkg , branch name h2o , build number h2o ) ) if ci . build too old : print ( \"Warning: Your H2O cluster version is too old ({})! Please download and install the latest \" \"version from http://h2o.ai/download/\" . format ( ci . build age ) )", "predictions": ["check if the version of the cluster is running ."], "references": ["used to verify that h2o - python module and the h2o server are compatible with each other ."], "bleu": 0.06268497150209275, "rouge_l": 0.1358574610244989}
{"id": 541, "code": "def load dataset ( relative path ) : assert is type ( relative path , str ) h2o dir = os . path . split ( file ) [ 0 ] for possible file in [ os . path . join ( h2o dir , relative path ) , os . path . join ( h2o dir , \"h2o data\" , relative path ) , os . path . join ( h2o dir , \"h2o data\" , relative path + \".csv\" ) ] : if os . path . exists ( possible file ) : return upload file ( possible file ) raise H2O Value Error ( \"Data file %s cannot be found\" % relative path )", "predictions": ["load a dataset from the specified path ."], "references": ["imports a data file within the h2o_data folder ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 542, "code": "def check frame id ( frame id ) : if frame id is None : return if frame id . strip ( ) == \"\" : raise H2O Value Error ( \"Frame id cannot be an empty string: %r\" % frame id ) for i , ch in enumerate ( frame id ) : if ch == \"$\" and i == 0 : continue if ch not in id allowed characters : raise H2O Value Error ( \"Character '%s' is illegal in frame id: %s\" % ( ch , frame id ) ) if re . match ( r\"-?[0-9]\" , frame id ) : raise H2O Value Error ( \"Frame id cannot start with a number: %s\" % frame id )", "predictions": ["check that the id is allowed"], "references": ["check that the provided frame id is valid in rapids language ."], "bleu": 0.17986472826920394, "rouge_l": 0.5240549828178694}
{"id": 543, "code": "def slice is normalized ( s ) : return ( s . start is not None and s . stop is not None and s . step is not None and s . start <= s . stop )", "predictions": ["return true if s is a normalized normalized normalized normalized step ."], "references": ["return true if slice s in normalized form ."], "bleu": 0.2224246939793677, "rouge_l": 0.5865384615384615}
{"id": 544, "code": "def deprecated ( message ) : from traceback import extract stack assert message , \"`message` argument in @deprecated is required.\" def deprecated decorator ( fun ) : def decorator invisible ( * args , * * kwargs ) : stack = extract stack ( ) assert len ( stack ) >= 2 and stack [ - 1 ] [ 2 ] == \"decorator invisible\" , \"Got confusing stack... %r\" % stack print ( \"[WARNING] in %s line %d:\" % ( stack [ - 2 ] [ 0 ] , stack [ - 2 ] [ 1 ] ) ) print ( \"    >>> %s\" % ( stack [ - 2 ] [ 3 ] or \"????\" ) ) print ( \"        ^^^^ %s\" % message ) return fun ( * args , * * kwargs ) decorator invisible . doc = message decorator invisible . name = fun . name decorator invisible . module = fun . module decorator invisible . deprecated = True return decorator invisible return deprecated decorator", "predictions": ["str name of name name name name name name name name and name"], "references": ["the decorator to mark deprecated functions ."], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 545, "code": "def join ( self ) : self . future = False self . job . poll ( ) self . job = None", "predictions": ["check the tuple of the job . . . . . . . . . . . ."], "references": ["wait until grid finishes computing ."], "bleu": 0.06809398432036522, "rouge_l": 0.09159159159159158}
{"id": 546, "code": "def summary ( self , header = True ) : table = [ ] for model in self . models : model summary = model . model json [ \"output\" ] [ \"model summary\" ] r values = list ( model summary . cell values [ 0 ] ) r values [ 0 ] = model . model id table . append ( r values ) print ( ) if header : print ( 'Grid Summary:' ) print ( ) H2O Display ( table , [ 'Model Id' ] + model summary . col header [ 1 : ] , numalign = \"left\" , stralign = \"left\" )", "predictions": ["prints the check of the given var"], "references": ["print a detailed summary of the explored models ."], "bleu": 0.18370727471078332, "rouge_l": 0.24448897795591182}
{"id": 547, "code": "def show ( self ) : hyper combos = itertools . product ( * list ( self . hyper params . values ( ) ) ) if not self . models : c values = [ [ idx + 1 , list ( val ) ] for idx , val in enumerate ( hyper combos ) ] print ( H2O Two Dim Table ( col header = [ 'Model' , 'Hyperparameters: [' + ', ' . join ( list ( self . hyper params . keys ( ) ) ) + ']' ] , table header = 'Grid Search of Model ' + self . model . class . name , cell values = c values ) ) else : print ( self . sorted metric table ( ) )", "predictions": ["name of the = = 0 return the metric"], "references": ["print models sorted by metric ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 548, "code": "def parse ( self ) : f = open ( self . parse log path , \"r\" ) self . parse2 ( f ) f . close ( )", "predictions": ["check and check the int of the int"], "references": ["parse file specified by constructor ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 549, "code": "def log start transaction ( self , endpoint , data , json , files , params ) : self . requests counter += 1 if not self . is logging : return msg = \"\\n---- %d --------------------------------------------------------\\n\" % self . requests counter msg += \"[%s] %s\\n\" % ( time . strftime ( \"%H:%M:%S\" ) , endpoint ) if params is not None : msg += \"     params: {%s}\\n\" % \", \" . join ( \"%s:%s\" % item for item in viewitems ( params ) ) if data is not None : msg += \"     body: {%s}\\n\" % \", \" . join ( \"%s:%s\" % item for item in viewitems ( data ) ) if json is not None : import json as j msg += \"     json: %s\\n\" % j . dumps ( json ) if files is not None : msg += \"     file: %s\\n\" % \", \" . join ( f . name for f in viewvalues ( files ) ) self . log message ( msg + \"\\n\" )", "predictions": ["name of the self is done"], "references": ["log the beginning of an api request ."], "bleu": 0.17516432701748888, "rouge_l": 0.13926940639269406}
{"id": 550, "code": "def log end transaction ( self , start time , response ) : if not self . is logging : return elapsed time = int ( ( time . time ( ) - start time ) * 1000 ) msg = \"<<< HTTP %d %s   (%d ms)\\n\" % ( response . status code , response . reason , elapsed time ) if \"Content-Type\" in response . headers : msg += \"    Content-Type: %s\\n\" % response . headers [ \"Content-Type\" ] msg += response . text self . log message ( msg + \"\\n\\n\" )", "predictions": ["check the response and check for the response"], "references": ["log response from an api request ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 551, "code": "def print ( self , msg , flush = False , end = \"\\n\" ) : if self . verbose : print2 ( msg , end = end , flush = flush )", "predictions": ["check if var is not and flush"], "references": ["helper function to print connection status messages when in verbose mode ."], "bleu": 0.07646493705380929, "rouge_l": 0.0}
{"id": 552, "code": "def normalize enum constant ( s ) : if s . islower ( ) : return s if s . isupper ( ) : return s . lower ( ) return \"\" . join ( ch if ch . islower ( ) else \" \" + ch . lower ( ) for ch in s ) . strip ( \" \" )", "predictions": ["check for self not a string not aligned to a string not not unicode not not a string not not not a string not unicode ."], "references": ["return enum constant s converted to a canonical snake - case ."], "bleu": 0.0660161823828377, "rouge_l": 0.1691312384473198}
{"id": 553, "code": "def default params ( self ) : params = { } for p in self . parms : params [ p ] = self . parms [ p ] [ \"default value\" ] return params", "predictions": ["get the get parameters of this object"], "references": ["dictionary of the default parameters of the model ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 554, "code": "def actual params ( self ) : params to select = { \"model id\" : \"name\" , \"response column\" : \"column name\" , \"training frame\" : \"name\" , \"validation frame\" : \"name\" } params = { } for p in self . parms : if p in params to select . keys ( ) : params [ p ] = self . parms [ p ] [ \"actual value\" ] . get ( params to select [ p ] , None ) else : params [ p ] = self . parms [ p ] [ \"actual value\" ] return params", "predictions": ["read parameters from this object"], "references": ["dictionary of actual parameters of the model ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 555, "code": "def show ( self ) : if self . future : self . job . poll once ( ) return if self . model json is None : print ( \"No model trained yet\" ) return if self . model id is None : print ( \"This H2O Estimator has been removed.\" ) return model = self . model json [ \"output\" ] print ( \"Model Details\" ) print ( \"=============\" ) print ( self . class . name , \": \" , self . model json [ \"algo full name\" ] ) print ( \"Model Key: \" , self . id ) self . summary ( ) print ( ) tm = model [ \"training metrics\" ] if tm : tm . show ( ) vm = model [ \"validation metrics\" ] if vm : vm . show ( ) xm = model [ \"cross validation metrics\" ] if xm : xm . show ( ) xms = model [ \"cross validation metrics summary\" ] if xms : xms . show ( ) if \"scoring history\" in model and model [ \"scoring history\" ] : model [ \"scoring history\" ] . show ( ) if \"variable importances\" in model and model [ \"variable importances\" ] : model [ \"variable importances\" ] . show ( )", "predictions": ["poll the model model model"], "references": ["print innards of model without regards to type ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 556, "code": "def gbm ( interactive = True , echo = True , testing = False ) : def demo body ( go ) : go ( ) h2o . init ( ) go ( ) prostate = h2o . load dataset ( \"prostate\" ) go ( ) prostate . describe ( ) go ( ) train , test = prostate . split frame ( ratios = [ 0.70 ] ) go ( ) train [ \"CAPSULE\" ] = train [ \"CAPSULE\" ] . asfactor ( ) test [ \"CAPSULE\" ] = test [ \"CAPSULE\" ] . asfactor ( ) go ( ) from h2o . estimators import H2O Gradient Boosting Estimator prostate gbm = H2O Gradient Boosting Estimator ( distribution = \"bernoulli\" , ntrees = 10 , max depth = 8 , min rows = 10 , learn rate = 0.2 ) prostate gbm . train ( x = [ \"AGE\" , \"RACE\" , \"PSA\" , \"VOL\" , \"GLEASON\" ] , y = \"CAPSULE\" , training frame = train ) go ( ) prostate gbm . show ( ) go ( ) predictions = prostate gbm . predict ( test ) predictions . show ( ) go ( ) from h2o . tree import H2O Tree , H2O Node tree = H2O Tree ( prostate gbm , 0 , \"0\" ) len ( tree ) tree . left children tree . right children tree . root node . show ( ) go ( ) performance = prostate gbm . model performance ( test ) performance . show ( ) run demo ( demo body , interactive , echo , testing )", "predictions": ["run the end of the estimate progress bar progress bar progress"], "references": ["gbm model demo ."], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 557, "code": "def deeplearning ( interactive = True , echo = True , testing = False ) : def demo body ( go ) : go ( ) h2o . init ( ) go ( ) prostate = h2o . load dataset ( \"prostate\" ) go ( ) prostate . describe ( ) go ( ) train , test = prostate . split frame ( ratios = [ 0.70 ] ) go ( ) train [ \"CAPSULE\" ] = train [ \"CAPSULE\" ] . asfactor ( ) test [ \"CAPSULE\" ] = test [ \"CAPSULE\" ] . asfactor ( ) go ( ) from h2o . estimators import H2O Deep Learning Estimator prostate dl = H2O Deep Learning Estimator ( activation = \"Tanh\" , hidden = [ 10 , 10 , 10 ] , epochs = 10000 ) prostate dl . train ( x = list ( set ( prostate . col names ) - { \"ID\" , \"CAPSULE\" } ) , y = \"CAPSULE\" , training frame = train ) go ( ) prostate dl . show ( ) go ( ) predictions = prostate dl . predict ( test ) predictions . show ( ) go ( ) performance = prostate dl . model performance ( test ) performance . show ( ) run demo ( demo body , interactive , echo , testing )", "predictions": ["run the not - based applications"], "references": ["deep learning model demo ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 558, "code": "def glm ( interactive = True , echo = True , testing = False ) : def demo body ( go ) : go ( ) h2o . init ( ) go ( ) prostate = h2o . load dataset ( \"prostate\" ) go ( ) prostate . describe ( ) go ( ) train , test = prostate . split frame ( ratios = [ 0.70 ] ) go ( ) train [ \"CAPSULE\" ] = train [ \"CAPSULE\" ] . asfactor ( ) test [ \"CAPSULE\" ] = test [ \"CAPSULE\" ] . asfactor ( ) go ( ) from h2o . estimators import H2O Generalized Linear Estimator prostate glm = H2O Generalized Linear Estimator ( family = \"binomial\" , alpha = [ 0.5 ] ) prostate glm . train ( x = [ \"AGE\" , \"RACE\" , \"PSA\" , \"VOL\" , \"GLEASON\" ] , y = \"CAPSULE\" , training frame = train ) go ( ) prostate glm . show ( ) go ( ) predictions = prostate glm . predict ( test ) predictions . show ( ) go ( ) performance = prostate glm . model performance ( test ) performance . show ( ) run demo ( demo body , interactive , echo , testing )", "predictions": ["run the . py file"], "references": ["glm model demo ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 559, "code": "def as data frame ( self ) : if can use pandas ( ) : import pandas pandas . options . display . max colwidth = 70 return pandas . Data Frame ( self . cell values , columns = self . col header ) return self", "predictions": ["returns the widget widget as a 0 - 1 sizes"], "references": ["convert to a python data frame ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 560, "code": "def show ( self , header = True ) : if header and self . table header : print ( self . table header + \":\" , end = ' ' ) if self . table description : print ( self . table description ) print ( ) table = copy . deepcopy ( self . cell values ) nr = 0 if is list of lists ( table ) : nr = len ( table ) if nr > 20 : trunc table = [ ] trunc table += [ v for v in table [ : 5 ] ] trunc table . append ( [ \"---\" ] * len ( table [ 0 ] ) ) trunc table += [ v for v in table [ ( nr - 5 ) : ] ] table = trunc table H2O Display ( table , self . col header , numalign = \"left\" , stralign = \"left\" ) if nr > 20 and can use pandas ( ) : print ( '\\n See the whole table with table.as data frame()' )", "predictions": ["get the whole return return the whole return the return return value of the return return"], "references": ["print the contents of this table ."], "bleu": 0.08513012360883544, "rouge_l": 0.1871165644171779}
{"id": 561, "code": "def jar paths ( ) : own jar = os . getenv ( \"H2O JAR PATH\" , \"\" ) if own jar != \"\" : if not os . path . isfile ( own jar ) : raise H2O Startup Error ( \"Environment variable H2O JAR PATH is set to '%d' but file does not exists, unset environment variable or provide valid path to h2o.jar file.\" % own jar ) yield own jar cwd chunks = os . path . abspath ( \".\" ) . split ( os . path . sep ) for i in range ( len ( cwd chunks ) , 0 , - 1 ) : if cwd chunks [ i - 1 ] == \"h2o-3\" : yield os . path . sep . join ( cwd chunks [ : i ] + [ \"build\" , \"h2o.jar\" ] ) backend dir = os . path . split ( os . path . realpath ( file ) ) [ 0 ] yield os . path . join ( backend dir , \"bin\" , \"h2o.jar\" ) prefix1 = prefix2 = sys . prefix if prefix1 . startswith ( os . path . sep + \"Library\" ) : prefix2 = os . path . join ( \"\" , \"System\" , prefix1 ) elif prefix1 . startswith ( os . path . sep + \"System\" ) : prefix2 = prefix1 [ len ( os . path . join ( \"\" , \"System\" ) ) : ] yield os . path . join ( prefix1 , \"h2o jar\" , \"h2o.jar\" ) yield os . path . join ( os . path . abspath ( os . sep ) , \"usr\" , \"local\" , \"h2o jar\" , \"h2o.jar\" ) yield os . path . join ( prefix1 , \"local\" , \"h2o jar\" , \"h2o.jar\" ) yield os . path . join ( get config var ( \"userbase\" ) , \"h2o jar\" , \"h2o.jar\" ) yield os . path . join ( prefix2 , \"h2o jar\" , \"h2o.jar\" )", "predictions": ["list all render paths paths paths"], "references": ["produce potential paths for an h2o . jar executable ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 562, "code": "def csv dict writer ( f , fieldnames , * * kwargs ) : import csv if \"delimiter\" in kwargs : kwargs [ \"delimiter\" ] = str ( kwargs [ \"delimiter\" ] ) return csv . Dict Writer ( f , fieldnames , * * kwargs )", "predictions": ["s2 set of set of set values from set to set as set ."], "references": ["equivalent of csv . dictwriter but allows delimiter to be a unicode string on py2 ."], "bleu": 0.09112487712571068, "rouge_l": 0.1976241900647948}
{"id": 563, "code": "def path2uri ( self , dirpath ) : relpath = dirpath . replace ( self . root path , self . package name ) if relpath . startswith ( os . path . sep ) : relpath = relpath [ 1 : ] return relpath . replace ( os . path . sep , '.' )", "predictions": ["return the path of the int . . ."], "references": ["convert directory path to uri"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 564, "code": "def parse module ( self , uri ) : filename = self . uri2path ( uri ) if filename is None : return ( [ ] , [ ] ) f = open ( filename , 'rt' ) functions , classes = self . parse lines ( f ) f . close ( ) return functions , classes", "predictions": ["refresh the functions and return the functions"], "references": ["parse module defined in * uri *"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 565, "code": "def parse lines ( self , linesource ) : functions = [ ] classes = [ ] for line in linesource : if line . startswith ( 'def ' ) and line . count ( '(' ) : name = self . get object name ( line ) if not name . startswith ( ' ' ) : functions . append ( name ) elif line . startswith ( 'class ' ) : name = self . get object name ( line ) if not name . startswith ( ' ' ) : classes . append ( name ) else : pass functions . sort ( ) classes . sort ( ) return functions , classes", "predictions": ["structure from the df"], "references": ["parse lines of text for functions and classes"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 566, "code": "def to list ( self ) : return [ [ int ( self . table . cell values [ 0 ] [ 1 ] ) , int ( self . table . cell values [ 0 ] [ 2 ] ) ] , [ int ( self . table . cell values [ 1 ] [ 1 ] ) , int ( self . table . cell values [ 1 ] [ 2 ] ) ] ]", "predictions": ["return the parameter as a text ."], "references": ["convert this confusion matrix into a 2x2 plain list of values ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 567, "code": "def locate files ( root dir ) : all files = [ ] root dir = os . path . abspath ( root dir ) for dir name , subdirs , files in os . walk ( root dir ) : for f in files : if f . endswith ( \".py\" ) : all files . append ( os . path . join ( dir name , f ) ) return all files", "predictions": ["f - specific file parse all file file file file file file file names in the given filename parameter dir"], "references": ["find all python files in the given directory and all subfolders ."], "bleu": 0.12021577610863723, "rouge_l": 0.26180257510729615}
{"id": 568, "code": "def main ( ) : for filename in locate files ( ROOT DIR ) : print ( \"Processing %s\" % filename ) with open ( filename , \"rt\" ) as f : tokens = list ( tokenize . generate tokens ( f . readline ) ) text1 = tokenize . untokenize ( tokens ) ntokens = normalize tokens ( tokens ) text2 = tokenize . untokenize ( ntokens ) assert text1 == text2", "predictions": ["move all tokens to a file ."], "references": ["executed when script is run as - is ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 569, "code": "def generate schema ( class name , schema ) : has map = False for field in schema [ \"fields\" ] : if field [ \"type\" ] . startswith ( \"Map\" ) : has map = True superclass = schema [ \"superclass\" ] if superclass == \"Iced\" : superclass = \"Object\" yield \"/**\" yield \" * This file is auto-generated by h2o-3/h2o-bindings/bin/gen csharp.py\" yield \" * Copyright 2016 H2O.ai;  Apache License Version 2.0 (see LICENSE for details)\" yield \" */\" yield \"namespace ai.h2o\" yield \"{\" yield \"  using System;\" yield \"  using System.Collections.Generic;\" if has map else None yield \"\" yield \"  public class {name}: {super} {{\" . format ( name = class name , super = superclass ) for field in schema [ \"fields\" ] : if field [ \"name\" ] == \" meta\" : continue csharp type = translate type ( field [ \"type\" ] , field [ \"schema name\" ] ) yield \"    /// <summary>\" yield bi . wrap ( field [ \"help\" ] , \"    ///   \" ) yield \"    /// </summary>\" yield \"    public {type} {name} {{ get; set; }}\" . format ( type = csharp type , name = field [ \"name\" ] ) yield \"\" yield \"  }\" yield \"}\"", "predictions": ["unparse public for public ."], "references": ["generate c# declaration file for a schema ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 570, "code": "def available ( ) : builder json = h2o . api ( \"GET /3/Model Builders\" , data = { \"algo\" : \"deepwater\" } ) visibility = builder json [ \"model builders\" ] [ \"deepwater\" ] [ \"visibility\" ] if visibility == \"Experimental\" : print ( \"Cannot build a Deep Water model - no backend found.\" ) return False else : return True", "predictions": ["check if the current backend is centers"], "references": ["returns true if a deep water model can be built or false otherwise ."], "bleu": 0.06833381956448398, "rouge_l": 0.08983799705449189}
{"id": 571, "code": "def endpoint groups ( ) : groups = defaultdict ( list ) for e in endpoints ( ) : groups [ e [ \"class name\" ] ] . append ( e ) return groups", "predictions": ["returns a list of all centers std std"], "references": ["return endpoints grouped by the class which handles them ."], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 572, "code": "def update site forward ( apps , schema editor ) : Site = apps . get model ( \"sites\" , \"Site\" ) Site . objects . update or create ( id = settings . SITE ID , defaults = { \"domain\" : \"{{cookiecutter.domain name}}\" , \"name\" : \"{{cookiecutter.project name}}\" , } , )", "predictions": ["version of version for version of default check ci ci ci ci ci ci ci ci ci ci ci ci ci ci ci ci ci ci ci ci ci ci ci"], "references": ["set site domain and name ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 573, "code": "def json data ( self , data = None ) : if data is None : data = { } data . update ( self . default data ) return json . dumps ( data )", "predictions": [". load dataset dataset"], "references": ["adds the default_data to data and dumps it to a json ."], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 574, "code": "def comment user ( self , user id , amount = None ) : if not self . check user ( user id , filter closed acc = True ) : return False self . logger . info ( \"Going to comment user %s's feed:\" % user id ) user id = self . convert to user id ( user id ) medias = self . get user medias ( user id , is comment = True ) if not medias : self . logger . info ( \"None medias received: account is closed or medias have been filtered.\" ) return False return self . comment medias ( medias [ : amount ] )", "predictions": ["cannot be a check if the frame is closed"], "references": ["comments last user_id s medias"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 575, "code": "def get credentials ( username = None ) : while not check secret ( ) : pass while True : try : with open ( SECRET FILE , \"r\" ) as f : lines = [ line . strip ( ) . split ( \":\" , 2 ) for line in f . readlines ( ) ] except Value Error : msg = 'Problem with opening `{}`, will remove the file.' raise Exception ( msg . format ( SECRET FILE ) ) if username is not None : for login , password in lines : if login == username . strip ( ) : return login , password print ( \"Which account do you want to use? (Type number)\" ) for ind , ( login , password ) in enumerate ( lines ) : print ( \"%d: %s\" % ( ind + 1 , login ) ) print ( \"%d: %s\" % ( 0 , \"add another account.\" ) ) print ( \"%d: %s\" % ( - 1 , \"delete all accounts.\" ) ) try : ind = int ( sys . stdin . readline ( ) ) if ind == 0 : add credentials ( ) continue elif ind == - 1 : delete credentials ( ) check secret ( ) continue elif 0 <= ind - 1 < len ( lines ) : return lines [ ind - 1 ] except Exception : print ( \"Wrong input, enter the number of the account to use.\" )", "predictions": ["slice the is used to slice the account is"], "references": ["returns login and password stored in secret . txt ."], "bleu": 0.10620315618312248, "rouge_l": 0.0}
{"id": 576, "code": "def like user ( self , user id , amount = None , filtration = True ) : if filtration : if not self . check user ( user id ) : return False self . logger . info ( \"Liking user %s's feed:\" % user id ) user id = self . convert to user id ( user id ) medias = self . get user medias ( user id , filtration = filtration ) if not medias : self . logger . info ( \"None medias received: account is closed or medias have been filtered.\" ) return False return self . like medias ( medias [ : amount ] )", "predictions": ["like a user can be executed when a user is closed ."], "references": ["likes last user_id s medias"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 577, "code": "def like hashtag ( self , hashtag , amount = None ) : self . logger . info ( \"Going to like media with hashtag #%s.\" % hashtag ) medias = self . get total hashtag medias ( hashtag , amount ) return self . like medias ( medias )", "predictions": ["like hashtag but for media ."], "references": ["likes last medias from hashtag"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 578, "code": "def check not bot ( self , user id ) : self . small delay ( ) user id = self . convert to user id ( user id ) if not user id : return False if user id in self . whitelist : return True if user id in self . blacklist : return False user info = self . get user info ( user id ) if not user info : return True skipped = self . skipped file if \"following count\" in user info and user info [ \"following count\" ] > self . max following to block : msg = 'following count > bot.max following to block, skipping!' self . console print ( msg , 'red' ) skipped . append ( user id ) return False if search stop words in user ( self , user info ) : msg = '`bot.search stop words in user` found in user, skipping!' skipped . append ( user id ) return False return True", "predictions": ["check if a user can be not be not bot ."], "references": ["filter bot from real users ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 579, "code": "def get uri ( self , request ) : protocol = request . protocol override if request . protocol override else self . protocol protocol = protocol . lower ( ) port = HTTP PORT if protocol == 'http' else HTTPS PORT return protocol + '://' + request . host + ':' + str ( port ) + request . path", "predictions": ["return the uri from the request ."], "references": ["return the target uri for the request ."], "bleu": 0.37040566269530983, "rouge_l": 0.7904967602591793}
{"id": 580, "code": "def get connection ( self , request ) : protocol = request . protocol override if request . protocol override else self . protocol protocol = protocol . lower ( ) target host = request . host connection = Requests Connection ( target host , protocol , self . request session , self . timeout ) proxy host = self . proxy host proxy port = self . proxy port if self . proxy host : headers = None if self . proxy user and self . proxy password : auth = base64 . b64encode ( \"{0}:{1}\" . format ( self . proxy user , self . proxy password ) . encode ( ) ) headers = { 'Proxy-Authorization' : 'Basic {0}' . format ( auth . decode ( ) ) } connection . set tunnel ( proxy host , int ( proxy port ) , headers ) return connection", "predictions": ["return the connection string to be used in the request"], "references": ["create connection for the request ."], "bleu": 0.17827531042796255, "rouge_l": 0.3927038626609442}
{"id": 581, "code": "def perform request ( self , request ) : connection = self . get connection ( request ) try : connection . putrequest ( request . method , request . path ) self . send request headers ( connection , request . headers ) self . send request body ( connection , request . body ) if DEBUG REQUESTS and request . body : print ( 'request:' ) try : print ( request . body ) except : pass resp = connection . getresponse ( ) status = int ( resp . status ) message = resp . reason respheaders = resp . getheaders ( ) for i , value in enumerate ( respheaders ) : respheaders [ i ] = ( value [ 0 ] . lower ( ) , value [ 1 ] ) respbody = None if resp . length is None : respbody = resp . read ( ) elif resp . length > 0 : respbody = resp . read ( resp . length ) if DEBUG RESPONSES and respbody : print ( 'response:' ) try : print ( respbody ) except : pass response = HTTP Response ( status , resp . reason , respheaders , respbody ) if status == 307 : new url = urlparse ( dict ( respheaders ) [ 'location' ] ) request . host = new url . hostname request . path = new url . path request . path , request . query = self . update request uri query ( request ) return self . perform request ( request ) if status >= 300 : raise HTTP Error ( status , message , respheaders , respbody ) return response finally : connection . close ( )", "predictions": ["perform a request on the api ."], "references": ["sends request to cloud service server and return the response ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 582, "code": "def get authorization server ( self ) : value = '' for key in [ 'authorization uri' , 'authorization' ] : value = self . get value ( key ) or '' if value : break return value", "predictions": ["get the authorization server server value"], "references": ["returns the uri for the authorization server if present otherwise empty string ."], "bleu": 0.12802833376249098, "rouge_l": 0.29611650485436897}
{"id": 583, "code": "def validate request uri ( self , uri ) : if not uri : raise Value Error ( 'request uri cannot be empty' ) uri = parse . urlparse ( uri ) if not uri . netloc : raise Value Error ( 'request uri must be an absolute URI' ) if uri . scheme . lower ( ) not in [ 'http' , 'https' ] : raise Value Error ( 'request uri must be HTTP or HTTPS' ) return uri . netloc", "predictions": ["validate that the request uri is valid ."], "references": ["extracts the host authority from the given uri ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 584, "code": "def set timeout ( self , timeout in seconds ) : timeout in ms = int ( timeout in seconds * 1000 ) Win Http Request . Set Timeouts ( self , 0 , timeout in ms , timeout in ms , timeout in ms )", "predictions": ["sets the timeout on the given timeout ."], "references": ["sets up the timeout for the request ."], "bleu": 0.25098621243978964, "rouge_l": 0.625}
{"id": 585, "code": "def set request header ( self , name , value ) : name = BSTR ( name ) value = BSTR ( value ) Win Http Request . Set Request Header ( self , name , value )", "predictions": ["sets the request header"], "references": ["sets the request header ."], "bleu": 0.7788007830714049, "rouge_l": 0.8714285714285713}
{"id": 586, "code": "def get all response headers ( self ) : bstr headers = c void p ( ) Win Http Request . Get All Response Headers ( self , byref ( bstr headers ) ) bstr headers = ctypes . cast ( bstr headers , c wchar p ) headers = bstr headers . value Sys Free String ( bstr headers ) return headers", "predictions": ["get the response headers ."], "references": ["gets back all response headers ."], "bleu": 0.41602390756021224, "rouge_l": 0.5366568914956013}
{"id": 587, "code": "def send ( self , request = None ) : if request is None : var empty = VARIANT . create empty ( ) Win Http Request . Send ( self , var empty ) else : request = VARIANT . create safearray from str ( request ) Win Http Request . Send ( self , request )", "predictions": ["send a request to the server ."], "references": ["sends the request body ."], "bleu": 0.22089591134157885, "rouge_l": 0.34366197183098596}
{"id": 588, "code": "def status ( self ) : status = c long ( ) Win Http Request . Status ( self , byref ( status ) ) return int ( status . value )", "predictions": ["return the status of the server ."], "references": ["gets status of response ."], "bleu": 0.2626909894424158, "rouge_l": 0.5154929577464789}
{"id": 589, "code": "def status text ( self ) : bstr status text = c void p ( ) Win Http Request . Status Text ( self , byref ( bstr status text ) ) bstr status text = ctypes . cast ( bstr status text , c wchar p ) status text = bstr status text . value Sys Free String ( bstr status text ) return status text", "predictions": ["return the status of the status of the status ."], "references": ["gets status text of response ."], "bleu": 0.14991106946711685, "rouge_l": 0.3927038626609442}
{"id": 590, "code": "def response body ( self ) : var respbody = VARIANT ( ) Win Http Request . Response Body ( self , byref ( var respbody ) ) if var respbody . is safearray of bytes ( ) : respbody = var respbody . str from safearray ( ) return respbody else : return ''", "predictions": ["return the response body ."], "references": ["gets response body as a safearray and converts the safearray to str ."], "bleu": 0.08242390832192466, "rouge_l": 0.3086003372681282}
{"id": 591, "code": "def set client certificate ( self , certificate ) : certificate = BSTR ( certificate ) Win Http Request . Set Client Certificate ( self , certificate )", "predictions": ["sets the client certificate certificate ."], "references": ["sets client certificate for the request ."], "bleu": 0.29264632611424674, "rouge_l": 0.6069651741293532}
{"id": 592, "code": "def set tunnel ( self , host , port ) : url = host if port : url = url + u':' + port var host = VARIANT . create bstr from str ( url ) var empty = VARIANT . create empty ( ) Win Http Request . Set Proxy ( self , HTTPREQUEST PROXYSETTING PROXY , var host , var empty )", "predictions": ["set the tunnel tunnel from the given host ."], "references": ["sets up the host and the port for the http connect tunnelling ."], "bleu": 0.11379288211086455, "rouge_l": 0.26406926406926406}
{"id": 593, "code": "def set tunnel ( self , host , port = None , headers = None ) : self . httprequest . set tunnel ( unicode ( host ) , unicode ( str ( port ) ) )", "predictions": ["set the tunnel tunnel ."], "references": ["sets up the host and the port for the http connect tunnelling ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 594, "code": "def putrequest ( self , method , uri ) : protocol = unicode ( self . protocol + '://' ) url = protocol + self . host + unicode ( uri ) self . httprequest . set timeout ( self . timeout ) self . httprequest . open ( unicode ( method ) , url ) if self . cert file is not None : self . httprequest . set client certificate ( unicode ( self . cert file ) )", "predictions": ["send a client to the server ."], "references": ["connects to host and sends the request ."], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 595, "code": "def putheader ( self , name , value ) : if sys . version info < ( 3 , ) : name = str ( name ) . decode ( 'utf-8' ) value = str ( value ) . decode ( 'utf-8' ) self . httprequest . set request header ( name , value )", "predictions": ["set the value of the httprequest request ."], "references": ["sends the headers of request ."], "bleu": 0.239802967618271, "rouge_l": 0.5865384615384615}
{"id": 596, "code": "def send ( self , request body ) : if not request body : self . httprequest . send ( ) else : self . httprequest . send ( request body )", "predictions": ["send the request to the server ."], "references": ["sends request body ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 597, "code": "def getresponse ( self ) : status = self . httprequest . status ( ) status text = self . httprequest . status text ( ) resp headers = self . httprequest . get all response headers ( ) fixed headers = [ ] for resp header in resp headers . split ( '\\n' ) : if ( resp header . startswith ( '\\t' ) or resp header . startswith ( ' ' ) ) and fixed headers : fixed headers [ - 1 ] += resp header else : fixed headers . append ( resp header ) headers = [ ] for resp header in fixed headers : if ':' in resp header : pos = resp header . find ( ':' ) headers . append ( ( resp header [ : pos ] . lower ( ) , resp header [ pos + 1 : ] . strip ( ) ) ) body = self . httprequest . response body ( ) length = len ( body ) return Response ( status , status text , length , headers , body )", "predictions": ["returns the response body body ."], "references": ["gets the response and generates the _response object"], "bleu": 0.20830666398386113, "rouge_l": 0.2785388127853881}
{"id": 598, "code": "def get readable id ( id name , id prefix to skip ) : pos = id name . find ( '//' ) if pos != - 1 : pos += 2 if id prefix to skip : pos = id name . find ( id prefix to skip , pos ) if pos != - 1 : pos += len ( id prefix to skip ) pos = id name . find ( '/' , pos ) if pos != - 1 : return id name [ pos + 1 : ] return id name", "predictions": ["return the id of the readable id ."], "references": ["simplified an id to be more friendly for us people"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 599, "code": "def get serialization name ( element name ) : known = KNOWN SERIALIZATION XFORMS . get ( element name ) if known is not None : return known if element name . startswith ( 'x ms ' ) : return element name . replace ( ' ' , '-' ) if element name . endswith ( ' id' ) : element name = element name . replace ( ' id' , 'ID' ) for name in [ 'content ' , 'last modified' , 'if ' , 'cache control' ] : if element name . startswith ( name ) : element name = element name . replace ( ' ' , '- ' ) return '' . join ( name . capitalize ( ) for name in element name . split ( ' ' ) )", "predictions": ["return the serialization name for the given element ."], "references": ["converts a python name into a serializable name"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 600, "code": "def get entry properties from node ( entry , include id , id prefix to skip = None , use title as id = False ) : properties = { } etag = entry . get Attribute NS ( METADATA NS , 'etag' ) if etag : properties [ 'etag' ] = etag for updated in Minidom Xml To Object . get child nodes ( entry , 'updated' ) : properties [ 'updated' ] = updated . first Child . node Value for name in Minidom Xml To Object . get children from path ( entry , 'author' , 'name' ) : if name . first Child is not None : properties [ 'author' ] = name . first Child . node Value if include id : if use title as id : for title in Minidom Xml To Object . get child nodes ( entry , 'title' ) : properties [ 'name' ] = title . first Child . node Value else : for id in Minidom Xml To Object . get child nodes ( entry , 'id' ) : properties [ 'name' ] = get readable id ( id . first Child . node Value , id prefix to skip ) return properties", "predictions": ["returns the properties of an entry"], "references": ["get properties from entry xml"], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 601, "code": "def parse response body from xml node ( node , return type ) : return obj = return type ( ) Minidom Xml To Object . fill data to return object ( node , return obj ) return return obj", "predictions": ["parse the response body from an xml node ."], "references": ["parse the xml and fill all the data into a class of return_type"], "bleu": 0.1279808802469055, "rouge_l": 0.26406926406926406}
{"id": 602, "code": "def fill instance child ( xmldoc , element name , return type ) : xmlelements = Minidom Xml To Object . get child nodes ( xmldoc , get serialization name ( element name ) ) if not xmlelements : return None return obj = return type ( ) Minidom Xml To Object . fill data to return object ( xmlelements [ 0 ] , return obj ) return return obj", "predictions": ["fill the serialization object with the given type"], "references": ["converts a child of the current dom element to the specified type ."], "bleu": 0.10207878682119532, "rouge_l": 0.2739520958083832}
{"id": 603, "code": "def build package from pr number ( gh token , sdk id , pr number , output folder , * , with comment = False ) : con = Github ( gh token ) repo = con . get repo ( sdk id ) sdk pr = repo . get pull ( pr number ) package names = { f . filename . split ( '/' ) [ 0 ] for f in sdk pr . get files ( ) if f . filename . startswith ( \"azure\" ) } absolute output folder = Path ( output folder ) . resolve ( ) with tempfile . Temporary Directory ( ) as temp dir , manage git folder ( gh token , Path ( temp dir ) / Path ( \"sdk\" ) , sdk id , pr number = pr number ) as sdk folder : for package name in package names : LOGGER . debug ( \"Build {}\" . format ( package name ) ) execute simple command ( [ \"python\" , \"./build package.py\" , \"--dest\" , str ( absolute output folder ) , package name ] , cwd = sdk folder ) LOGGER . debug ( \"Build finished: {}\" . format ( package name ) ) if with comment : files = [ f . name for f in absolute output folder . iterdir ( ) ] comment message = None dashboard = Dashboard Commentable Object ( sdk pr , \"(message created by the CI based on PR content)\" ) try : installation message = build installation message ( sdk pr ) download message = build download message ( sdk pr , files ) comment message = installation message + \"\\n\\n\" + download message dashboard . create comment ( comment message ) except Exception : LOGGER . critical ( \"Unable to do PR comment:\\n%s\" , comment message )", "predictions": ["build a package from the pr number of pr sdk ."], "references": ["will clone the given pr branch and vuild the package with the given name ."], "bleu": 0.09956647337521526, "rouge_l": 0.22453987730061348}
{"id": 604, "code": "def extract api version from code ( function ) : try : srccode = inspect . getsource ( function ) try : ast tree = ast . parse ( srccode ) except Indentation Error : ast tree = ast . parse ( 'with 0:\\n' + srccode ) api version visitor = Api Version Extractor ( ) api version visitor . visit ( ast tree ) return api version visitor . api version except Exception : raise", "predictions": ["extract the version from the api code ."], "references": ["will extract from __code__ the api version . should be use if you use this is an operation group with no constant api_version ."], "bleu": 0.03530186082224557, "rouge_l": 0.2866541353383459}
{"id": 605, "code": "def build receiver ( self ) : self . handler . message handler = self . handler . receiver type ( self . handler . session , self . handler . remote address , self . handler . name , on message received = self . handler . message received , name = 'receiver-link-{}' . format ( uuid . uuid4 ( ) ) , debug = self . handler . debug trace , prefetch = self . handler . prefetch , max message size = self . handler . max message size , properties = self . handler . link properties , error policy = self . handler . error policy , encoding = self . handler . encoding ) if self . mode != Receive Settle Mode . Peek Lock : self . handler . message handler . send settle mode = constants . Sender Settle Mode . Settled self . handler . message handler . receive settle mode = constants . Receiver Settle Mode . Receive And Delete self . handler . message handler . settle mode = constants . Receiver Settle Mode . Receive And Delete self . handler . message handler . open ( )", "predictions": ["build the receiver ."], "references": ["this is a temporary patch pending a fix in uamqp ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 606, "code": "def parse response for async op ( response ) : if response is None : return None result = Asynchronous Operation Result ( ) if response . headers : for name , value in response . headers : if name . lower ( ) == 'x-ms-request-id' : result . request id = value return result", "predictions": ["parse the response for the response ."], "references": ["extracts request id from response header ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 607, "code": "def update management header ( self , request , x ms version ) : if request . method in [ 'PUT' , 'POST' , 'MERGE' , 'DELETE' ] : request . headers . append ( ( 'Content-Length' , str ( len ( request . body ) ) ) ) request . headers . append ( ( 'x-ms-version' , x ms version or self . x ms version ) ) if not request . method in [ 'GET' , 'HEAD' ] : for name , in request . headers : if 'content-type' == name . lower ( ) : break else : request . headers . append ( ( 'Content-Type' , self . content type ) ) return request . headers", "predictions": ["updates the request header header ."], "references": ["add additional headers for management ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 608, "code": "def get regions ( self ) : response = self . perform get ( self . get path ( 'services/service Bus/Regions/' , None ) , None ) return Minidom Xml To Object . convert response to feeds ( response , Service Bus Management Xml Serializer . xml to region )", "predictions": ["returns the list of user user user user user user user user user user user user user user user user user user user user user user user user user user user"], "references": ["get list of available service bus regions ."], "bleu": 0.0513487742994337, "rouge_l": 0.1147695202257761}
{"id": 609, "code": "def list namespaces ( self ) : response = self . perform get ( self . get path ( 'services/service Bus/Namespaces/' , None ) , None ) return Minidom Xml To Object . convert response to feeds ( response , Service Bus Management Xml Serializer . xml to namespace )", "predictions": ["returns a list of all namespaces namespaces namespaces . . . . . ."], "references": ["list the service bus namespaces defined on the account ."], "bleu": 0.10511846841633776, "rouge_l": 0.2577464788732394}
{"id": 610, "code": "def create ( env dir , system site packages = False , clear = False , symlinks = False , with pip = False , prompt = None ) : builder = Extended Env Builder ( system site packages = system site packages , clear = clear , symlinks = symlinks , with pip = with pip , prompt = prompt ) builder . create ( env dir ) return builder . context", "predictions": ["check if a new system is set blacklist blacklist blacklist blacklist blacklist blacklist blacklist blacklist blacklist blacklist blacklist ."], "references": ["create a virtual environment in a directory ."], "bleu": 0.0712695567709093, "rouge_l": 0.1598951507208388}
{"id": 611, "code": "def list databases ( self , name ) : response = self . perform get ( self . get list databases path ( name ) , None ) return Minidom Xml To Object . parse service resources response ( response , Database )", "predictions": ["returns a list of all uri for the specified return"], "references": ["list the sql databases defined on the specified server name"], "bleu": 0.17827531042796255, "rouge_l": 0.3}
{"id": 612, "code": "def validate challenge ( self , challenge ) : bearer string = 'Bearer ' if not challenge : raise Value Error ( 'Challenge cannot be empty' ) challenge = challenge . strip ( ) if not challenge . startswith ( bearer string ) : raise Value Error ( 'Challenge is not Bearer' ) return challenge [ len ( bearer string ) : ]", "predictions": ["get the connection from the connection proxy proxy proxy proxy proxy proxy proxy proxy proxy proxy proxy proxy ."], "references": ["verifies that the challenge is a bearer challenge and returns the key = value pairs ."], "bleu": 0.07658412276041004, "rouge_l": 0.1741198858230257}
{"id": 613, "code": "def list queues ( self ) : request = HTTP Request ( ) request . method = 'GET' request . host = self . get host ( ) request . path = '/$Resources/Queues' request . path , request . query = self . httpclient . update request uri query ( request ) request . headers = self . update service bus header ( request ) response = self . perform request ( request ) return E Tree Xml To Object . convert response to feeds ( response , convert etree element to queue )", "predictions": ["returns a list of all request request request request request get all available request names ."], "references": ["enumerates the queues in the service namespace ."], "bleu": 0.07692375026049747, "rouge_l": 0.08866279069767442}
{"id": 614, "code": "def send event ( self , hub name , message , device id = None , broker properties = None ) : validate not none ( 'hub name' , hub name ) request = HTTP Request ( ) request . method = 'POST' request . host = self . get host ( ) if device id : request . path = '/{0}/publishers/{1}/messages?api-version=2014-01' . format ( hub name , device id ) else : request . path = '/{0}/messages?api-version=2014-01' . format ( hub name ) if broker properties : request . headers . append ( ( 'Broker Properties' , str ( broker properties ) ) ) request . body = get request body ( message ) request . path , request . query = self . httpclient . update request uri query ( request ) request . headers = self . update service bus header ( request ) self . perform request ( request )", "predictions": ["get an authorization authorization value"], "references": ["sends a new message event to an event hub ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 615, "code": "def update service bus header ( self , request ) : if request . method in [ 'PUT' , 'POST' , 'MERGE' , 'DELETE' ] : request . headers . append ( ( 'Content-Length' , str ( len ( request . body ) ) ) ) if not request . method in [ 'GET' , 'HEAD' ] : for name , in request . headers : if name . lower ( ) == 'content-type' : break else : request . headers . append ( ( 'Content-Type' , 'application/atom+xml;type=entry;charset=utf-8' ) ) self . authentication . sign request ( request , self . httpclient ) return request . headers", "predictions": ["validate the uri uri and sign the request uri"], "references": ["add additional headers for service bus ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 616, "code": "def get authorization ( self , request , httpclient ) : return 'WRAP access token=\"' + self . get token ( request . host , request . path , httpclient ) + '\"'", "predictions": ["return the timeout for the given in the in - memory token . . . ."], "references": ["return the signed string with token ."], "bleu": 0.12729922658368398, "rouge_l": 0.3742331288343558}
{"id": 617, "code": "def token is expired ( self , token ) : time pos begin = token . find ( 'Expires On=' ) + len ( 'Expires On=' ) time pos end = token . find ( '&' , time pos begin ) token expire time = int ( token [ time pos begin : time pos end ] ) time now = time . mktime ( time . localtime ( ) ) return ( token expire time - time now ) < 30", "predictions": ["request if set is header . ."], "references": ["check if token expires or not ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 618, "code": "def add headers ( self , request ) : if self . custom properties : for name , value in self . custom properties . items ( ) : request . headers . append ( ( name , self . serialize escaped properties value ( value ) ) ) request . headers . append ( ( 'Content-Type' , self . type ) ) if self . broker properties : if hasattr ( self . broker properties , 'items' ) : broker properties = { name : self . serialize basic properties value ( value ) for name , value in self . broker properties . items ( ) } broker properties = json . dumps ( broker properties ) else : broker properties = self . broker properties request . headers . append ( ( 'Broker Properties' , str ( broker properties ) ) ) return request . headers", "predictions": ["get all c void from the self = value = 0 = 1 = 1 = 0 = 1 = 1 = 0 = 1 = 1 = 1 = 1"], "references": ["add addtional headers to request for message request ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 619, "code": "def as batch body ( self ) : if sys . version info >= ( 3 , ) and isinstance ( self . body , bytes ) : body = self . body . decode ( 'utf-8' ) else : body = self . body result = { 'Body' : body } if self . custom properties : result [ 'User Properties' ] = { name : self . serialize basic properties value ( value ) for name , value in self . custom properties . items ( ) } if self . broker properties : result [ 'Broker Properties' ] = { name : self . serialize basic properties value ( value ) for name , value in self . broker properties . items ( ) } return result", "predictions": ["returns the batch self if available if not already present if not ."], "references": ["return the current message as expected by batch body format"], "bleu": 0.10571070857151538, "rouge_l": 0.1781021897810219}
{"id": 620, "code": "def general error handler ( http error ) : message = str ( http error ) if http error . respbody is not None : message += '\\n' + http error . respbody . decode ( 'utf-8-sig' ) raise Azure Http Error ( message , http error . status )", "predictions": ["simple error self . error"], "references": ["simple error handler for azure ."], "bleu": 0.31610981104846864, "rouge_l": 0.5366568914956013}
{"id": 621, "code": "def handle redirect ( self , r , * * kwargs ) : if r . is redirect : self . thread local . auth attempted = False", "predictions": ["text for text ."], "references": ["reset auth_attempted on redirects ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 622, "code": "def use ( self , profile ) : if not isinstance ( profile , ( Known Profiles , Profile Definition ) ) : raise Value Error ( \"Can only set as default a Profile Definition or a Known Profiles\" ) type ( self ) . profile = profile", "predictions": ["setter for the profile attribute"], "references": ["define a new default profile ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 623, "code": "def build config ( config : Dict [ str , Any ] ) -> Dict [ str , str ] : result = config . copy ( ) is stable = result . pop ( \"is stable\" , False ) if is stable : result [ \"classifier\" ] = \"Development Status :: 5 - Production/Stable\" else : result [ \"classifier\" ] = \"Development Status :: 4 - Beta\" package name = result [ \"package name\" ] result [ \"package nspkg\" ] = result . pop ( \"package nspkg\" , package name [ : package name . rindex ( '-' ) ] + \"-nspkg\" ) result [ 'is arm' ] = result . pop ( \"is arm\" , True ) result [ 'need msrestazure' ] = result . pop ( \"need msrestazure\" , True ) package parts = result [ \"package nspkg\" ] [ : - len ( '-nspkg' ) ] . split ( '-' ) result [ 'nspkg names' ] = [ \".\" . join ( package parts [ : i + 1 ] ) for i in range ( len ( package parts ) ) ] result [ 'init names' ] = [ \"/\" . join ( package parts [ : i + 1 ] ) + \"/ init .py\" for i in range ( len ( package parts ) ) ] return result", "predictions": ["builds the client client to use for the site . . ."], "references": ["will build the actual config for jinja2 based on sdk config ."], "bleu": 0.1235622127262679, "rouge_l": 0.25}
{"id": 624, "code": "def get entry properties from element ( element , include id , id prefix to skip = None , use title as id = False ) : properties = { } etag = element . attrib . get ( make etree ns attr name ( etree entity feed namespaces [ 'm' ] , 'etag' ) , None ) if etag is not None : properties [ 'etag' ] = etag updated = element . findtext ( './atom:updated' , '' , etree entity feed namespaces ) if updated : properties [ 'updated' ] = updated author name = element . findtext ( './atom:author/atom:name' , '' , etree entity feed namespaces ) if author name : properties [ 'author' ] = author name if include id : if use title as id : title = element . findtext ( './atom:title' , '' , etree entity feed namespaces ) if title : properties [ 'name' ] = title else : element id = element . findtext ( './atom:id' , '' , etree entity feed namespaces ) if element id : properties [ 'name' ] = get readable id ( element id , id prefix to skip ) return properties", "predictions": ["return an tunnel properties properties from an element element"], "references": ["get properties from element tree element"], "bleu": 0.21105340631872635, "rouge_l": 0.5532879818594103}
{"id": 625, "code": "def parse response body from xml node ( node , return type ) : return obj = return type ( ) E Tree Xml To Object . fill data to return object ( node , return obj ) return return obj", "predictions": ["set the tunnel body body self . from the xml"], "references": ["parse the xml and fill all the data into a class of return_type"], "bleu": 0.13206959826272413, "rouge_l": 0.16991643454039}
{"id": 626, "code": "def fill instance child ( xmldoc , element name , return type ) : element = xmldoc . find ( get serialization name ( element name ) ) if element is None : return None return obj = return type ( ) E Tree Xml To Object . fill data to return object ( element , return obj ) return return obj", "predictions": ["fill an instance self ."], "references": ["converts a child of the current dom element to the specified type ."], "bleu": 0.05512018958855254, "rouge_l": 0.10286677908937607}
{"id": 627, "code": "def terminal width ( value ) : if isinstance ( value , bytes ) : value = value . decode ( \"utf8\" , \"ignore\" ) return sum ( map ( get width , map ( ord , value ) ) )", "predictions": ["= the terminal width width"], "references": ["returns the width of the string it would be when displayed ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 628, "code": "def get cut prefix ( value , max len ) : should convert = isinstance ( value , bytes ) if should convert : value = value . decode ( \"utf8\" , \"ignore\" ) for i in range ( len ( value ) ) : if terminal width ( value [ i : ] ) <= max len : break return value [ i : ] . encode ( \"utf8\" , \"ignore\" ) if should convert else value [ i : ]", "predictions": ["httprequest self . to httprequest"], "references": ["drops characters by unicode not by bytes ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 629, "code": "def print inplace ( msg ) : term width = get terminal size ( ) . columns spacing = term width - terminal width ( msg ) if is win32 : spacing -= 1 sys . stderr . write ( \"\\r{0}\" . format ( msg ) ) sys . stderr . write ( \" \" * max ( 0 , spacing ) ) sys . stderr . flush ( )", "predictions": ["print the resp message to stdout get the terminal get ."], "references": ["clears out the previous line and prints a new one ."], "bleu": 0.12605968092174913, "rouge_l": 0.18181818181818182}
{"id": 630, "code": "def format filesize ( size ) : for suffix in ( \"bytes\" , \"KB\" , \"MB\" , \"GB\" , \"TB\" ) : if size < 1024.0 : if suffix in ( \"GB\" , \"TB\" ) : return \"{0:3.2f} {1}\" . format ( size , suffix ) else : return \"{0:3.1f} {1}\" . format ( size , suffix ) size /= 1024.0", "predictions": ["get the readable readable readable string for the given size 2 2 2 - 16 - 16 - 1 - 16 - 16 - 16 - 16 - 16 - 16"], "references": ["formats the file size into a human readable format ."], "bleu": 0.046398855339878003, "rouge_l": 0.10748898678414096}
{"id": 631, "code": "def format time ( elapsed ) : hours = int ( elapsed / ( 60 * 60 ) ) minutes = int ( ( elapsed % ( 60 * 60 ) ) / 60 ) seconds = int ( elapsed % 60 ) rval = \"\" if hours : rval += \"{0}h\" . format ( hours ) if elapsed > 60 : rval += \"{0}m\" . format ( minutes ) rval += \"{0}s\" . format ( seconds ) return rval", "predictions": ["get serialization formatted serialization"], "references": ["formats elapsed seconds into a human readable format ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 632, "code": "def create status line ( * * params ) : max size = get terminal size ( ) . columns - 1 for fmt in PROGRESS FORMATS : status = fmt . format ( * * params ) if len ( status ) <= max size : break return status", "predictions": ["get a entry properties from the = value"], "references": ["creates a status line with appropriate size ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 633, "code": "def close ( self ) : if not self . closed : log . debug ( \"Closing worker thread\" ) self . closed = True if self . wait : self . wait . set ( )", "predictions": ["parse the obj . . . . . . . ."], "references": ["shuts down the thread ."], "bleu": 0.12605968092174913, "rouge_l": 0.2681318681318681}
{"id": 634, "code": "def close ( self ) : if not self . closed : log . debug ( \"Closing writer thread\" ) self . closed = True self . reader . buffer . close ( ) self . executor . shutdown ( wait = False ) if concurrent . futures . thread . threads queues : concurrent . futures . thread . threads queues . clear ( )", "predictions": ["fill the . ."], "references": ["shuts down the thread ."], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 635, "code": "def put ( self , segment ) : if self . closed : return if segment is not None : future = self . executor . submit ( self . fetch , segment , retries = self . retries ) else : future = None self . queue ( self . futures , ( segment , future ) )", "predictions": ["build or update the number of changes in the = = true"], "references": ["adds a segment to the download pool and write queue ."], "bleu": 0.10390302174233558, "rouge_l": 0.08764367816091953}
{"id": 636, "code": "def queue ( self , queue , value ) : while not self . closed : try : queue . put ( value , block = True , timeout = 1 ) return except queue . Full : continue", "predictions": ["extract data from a extract extract it from a extract extract the extract try to the extract ."], "references": ["puts a value into a queue but aborts if this thread is closed ."], "bleu": 0.08097785064266204, "rouge_l": 0.19182389937106917}
{"id": 637, "code": "def pkcs7 decode ( padded Data , key Size = 16 ) : val = ord ( padded Data [ - 1 : ] ) if val > key Size : raise Stream Error ( \"Input is not padded or padding is corrupt, got padding size of {0}\" . format ( val ) ) return padded Data [ : - val ]", "predictions": ["decodes a string from a . . build string mode mode mode mode mode mode mode mode mode mode mode mode"], "references": ["remove the pkcs#7 padding"], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 638, "code": "def prepend www ( url ) : parsed = urlparse ( url ) if parsed . netloc . split ( \".\" ) [ 0 ] != \"www\" : return parsed . scheme + \"://www.\" + parsed . netloc + parsed . path else : return url", "predictions": ["parse response lower case and name ."], "references": ["changes google . com to www . google . com"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 639, "code": "def json ( cls , res , * args , * * kwargs ) : if res . encoding is None : res . encoding = cls . determine json encoding ( res . content [ : 4 ] ) return parse json ( res . text , * args , * * kwargs )", "predictions": ["headers for update data from update"], "references": ["parses json from a response ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 640, "code": "def xml ( cls , res , * args , * * kwargs ) : return parse xml ( res . text , * args , * * kwargs )", "predictions": ["parse an xml string into an xml object ."], "references": ["parses xml from a response ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 641, "code": "def get streams ( self ) : token = self . login ( self . get option ( \"username\" ) , self . get option ( \"password\" ) ) m = self . url re . match ( self . url ) scode = m and m . group ( \"scode\" ) or self . get option ( \"station code\" ) res = self . session . http . get ( self . guide url , params = dict ( token = token ) ) channels = Ordered Dict ( ) for t in itertags ( res . text , \"a\" ) : if t . attributes . get ( 'cs' ) : channels [ t . attributes . get ( 'cs' ) . lower ( ) ] = t . attributes . get ( 'title' ) . replace ( \"Watch \" , \"\" ) . strip ( ) if not scode : log . error ( \"Station code not provided, use --ustvnow-station-code.\" ) log . info ( \"Available stations are: \\n{0} \" . format ( '\\n' . join ( '    {0} ({1})' . format ( c , n ) for c , n in channels . items ( ) ) ) ) return if scode in channels : log . debug ( \"Finding streams for: {0}\" , channels . get ( scode ) ) r = self . session . http . get ( self . stream url , params = { \"scode\" : scode , \"token\" : token , \"br n\" : \"Firefox\" , \"br v\" : \"52\" , \"br d\" : \"desktop\" } , headers = { \"User-Agent\" : useragents . FIREFOX } ) data = self . session . http . json ( r ) return HLS Stream . parse variant playlist ( self . session , data [ \"stream\" ] ) else : log . error ( \"Invalid station-code: {0}\" , scode )", "predictions": ["get all streams streams from the service ."], "references": ["finds the streams from tvcatchup . com ."], "bleu": 0.239802967618271, "rouge_l": 0.375}
{"id": 642, "code": "def login ( self ) : email = self . get option ( \"email\" ) password = self . get option ( \"password\" ) if email and password : res = self . session . http . get ( self . login url ) csrf match = self . csrf re . search ( res . text ) token = csrf match and csrf match . group ( 1 ) self . logger . debug ( \"Attempting login as {0} (token={1})\" , email , token ) res = self . session . http . post ( self . login url , data = dict ( login = email , password = password , csrfmiddlewaretoken = token ) , allow redirects = False , raise for status = False , headers = { \"Referer\" : self . login url } ) if res . status code != 302 : self . logger . error ( \"Failed to login to Live Edu account: {0}\" , email )", "predictions": ["authenticate against the login option ."], "references": ["attempt a login to liveedu . tv"], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 643, "code": "def output stream http ( plugin , initial streams , external = False , port = 0 ) : global output if not external : if not args . player : console . exit ( \"The default player (VLC) does not seem to be \" \"installed. You must specify the path to a player \" \"executable with --player.\" ) title = create title ( plugin ) server = create http server ( ) player = output = Player Output ( args . player , args = args . player args , filename = server . url , quiet = not args . verbose player , title = title ) try : log . info ( \"Starting player: {0}\" , args . player ) if player : player . open ( ) except OS Error as err : console . exit ( \"Failed to start player: {0} ({1})\" , args . player , err ) else : server = create http server ( host = None , port = port ) player = None log . info ( \"Starting server, access with one of:\" ) for url in server . urls : log . info ( \" \" + url ) for req in iter http requests ( server , player ) : user agent = req . headers . get ( \"User-Agent\" ) or \"unknown player\" log . info ( \"Got HTTP request from {0}\" . format ( user agent ) ) stream fd = prebuffer = None while not stream fd and ( not player or player . running ) : try : streams = initial streams or fetch streams ( plugin ) initial streams = None for stream name in ( resolve stream name ( streams , s ) for s in args . stream ) : if stream name in streams : stream = streams [ stream name ] break else : log . info ( \"Stream not available, will re-fetch \" \"streams in 10 sec\" ) sleep ( 10 ) continue except Plugin Error as err : log . error ( u\"Unable to fetch new streams: {0}\" , err ) continue try : log . info ( \"Opening stream: {0} ({1})\" , stream name , type ( stream ) . shortname ( ) ) stream fd , prebuffer = open stream ( stream ) except Stream Error as err : log . error ( \"{0}\" , err ) if stream fd and prebuffer : log . debug ( \"Writing stream to player\" ) read stream ( stream fd , server , prebuffer ) server . close ( True ) player . close ( ) server . close ( )", "predictions": ["start a new player stream ."], "references": ["continuously output the stream over http ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 644, "code": "def output stream passthrough ( plugin , stream ) : global output title = create title ( plugin ) filename = '\"{0}\"' . format ( stream to url ( stream ) ) output = Player Output ( args . player , args = args . player args , filename = filename , call = True , quiet = not args . verbose player , title = title ) try : log . info ( \"Starting player: {0}\" , args . player ) output . open ( ) except OS Error as err : console . exit ( \"Failed to start player: {0} ({1})\" , args . player , err ) return False return True", "predictions": ["start a passthrough stream ."], "references": ["prepares a filename to be passed to the player ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 645, "code": "def output stream ( plugin , stream ) : global output success open = False for i in range ( args . retry open ) : try : stream fd , prebuffer = open stream ( stream ) success open = True break except Stream Error as err : log . error ( \"Try {0}/{1}: Could not open stream {2} ({3})\" , i + 1 , args . retry open , stream , err ) if not success open : console . exit ( \"Could not open stream {0}, tried {1} times, exiting\" , stream , args . retry open ) output = create output ( plugin ) try : output . open ( ) except ( IO Error , OS Error ) as err : if isinstance ( output , Player Output ) : console . exit ( \"Failed to start player: {0} ({1})\" , args . player , err ) else : console . exit ( \"Failed to open output: {0} ({1})\" , args . output , err ) with closing ( output ) : log . debug ( \"Writing stream to output\" ) read stream ( stream fd , output , prebuffer ) return True", "predictions": ["start a stream of the given stream ."], "references": ["open stream create output and finally write the stream to output ."], "bleu": 0.1223065774797558, "rouge_l": 0.3860759493670886}
{"id": 646, "code": "def read stream ( stream , output , prebuffer , chunk size = 8192 ) : is player = isinstance ( output , Player Output ) is http = isinstance ( output , HTTP Server ) is fifo = is player and output . namedpipe show progress = isinstance ( output , File Output ) and output . fd is not stdout and sys . stdout . isatty ( ) show record progress = hasattr ( output , \"record\" ) and isinstance ( output . record , File Output ) and output . record . fd is not stdout and sys . stdout . isatty ( ) stream iterator = chain ( [ prebuffer ] , iter ( partial ( stream . read , chunk size ) , b\"\" ) ) if show progress : stream iterator = progress ( stream iterator , prefix = os . path . basename ( args . output ) ) elif show record progress : stream iterator = progress ( stream iterator , prefix = os . path . basename ( args . record ) ) try : for data in stream iterator : if is win32 and is fifo : output . player . poll ( ) if output . player . returncode is not None : log . info ( \"Player closed\" ) break try : output . write ( data ) except IO Error as err : if is player and err . errno in ACCEPTABLE ERRNO : log . info ( \"Player closed\" ) elif is http and err . errno in ACCEPTABLE ERRNO : log . info ( \"HTTP connection closed\" ) else : console . exit ( \"Error when writing to output: {0}, exiting\" , err ) break except IO Error as err : console . exit ( \"Error when reading from stream: {0}, exiting\" , err ) finally : stream . close ( ) log . info ( \"Stream ended\" )", "predictions": ["read a stream from the stream ."], "references": ["reads data from stream and then writes it to the output ."], "bleu": 0.11434175042957104, "rouge_l": 0.30148270181219106}
{"id": 647, "code": "def fetch streams ( plugin ) : return plugin . streams ( stream types = args . stream types , sorting excludes = args . stream sorting excludes )", "predictions": ["fetch the streams of the plugin ."], "references": ["fetches streams using correct parameters ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 648, "code": "def resolve stream name ( streams , stream name ) : if stream name in STREAM SYNONYMS and stream name in streams : for name , stream in streams . items ( ) : if stream is streams [ stream name ] and name not in STREAM SYNONYMS : return name return stream name", "predictions": ["resolve the stream name from the stream name ."], "references": ["returns the real stream name of a synonym ."], "bleu": 0.21105340631872635, "rouge_l": 0.4444444444444444}
{"id": 649, "code": "def print plugins ( ) : pluginlist = list ( streamlink . get plugins ( ) . keys ( ) ) pluginlist formatted = \", \" . join ( sorted ( pluginlist ) ) if console . json : console . msg json ( pluginlist ) else : console . msg ( \"Loaded plugins: {0}\" , pluginlist formatted )", "predictions": ["print plugins of all plugins ."], "references": ["outputs a list of all plugins streamlink has loaded ."], "bleu": 0.2231931376573339, "rouge_l": 0.47843137254901963}
{"id": 650, "code": "def load plugins ( dirs ) : dirs = [ os . path . expanduser ( d ) for d in dirs ] for directory in dirs : if os . path . isdir ( directory ) : streamlink . load plugins ( directory ) else : log . warning ( \"Plugin path {0} does not exist or is not \" \"a directory!\" , directory )", "predictions": ["load all plugins in the directory"], "references": ["attempts to load plugins from a list of directories ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 651, "code": "def setup http session ( ) : if args . http proxy : streamlink . set option ( \"http-proxy\" , args . http proxy ) if args . https proxy : streamlink . set option ( \"https-proxy\" , args . https proxy ) if args . http cookie : streamlink . set option ( \"http-cookies\" , dict ( args . http cookie ) ) if args . http header : streamlink . set option ( \"http-headers\" , dict ( args . http header ) ) if args . http query param : streamlink . set option ( \"http-query-params\" , dict ( args . http query param ) ) if args . http ignore env : streamlink . set option ( \"http-trust-env\" , False ) if args . http no ssl verify : streamlink . set option ( \"http-ssl-verify\" , False ) if args . http disable dh : streamlink . set option ( \"http-disable-dh\" , True ) if args . http ssl cert : streamlink . set option ( \"http-ssl-cert\" , args . http ssl cert ) if args . http ssl cert crt key : streamlink . set option ( \"http-ssl-cert\" , tuple ( args . http ssl cert crt key ) ) if args . http timeout : streamlink . set option ( \"http-timeout\" , args . http timeout ) if args . http cookies : streamlink . set option ( \"http-cookies\" , args . http cookies ) if args . http headers : streamlink . set option ( \"http-headers\" , args . http headers ) if args . http query params : streamlink . set option ( \"http-query-params\" , args . http query params )", "predictions": ["setup the http session object ."], "references": ["sets the global http settings such as proxy and headers ."], "bleu": 0.1141650334026257, "rouge_l": 0.33516483516483514}
{"id": 652, "code": "def setup plugins ( extra plugin dir = None ) : if os . path . isdir ( PLUGINS DIR ) : load plugins ( [ PLUGINS DIR ] ) if extra plugin dir : load plugins ( extra plugin dir )", "predictions": ["setup plugins from extra plugin dir ."], "references": ["loads any additional plugins ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 653, "code": "def setup plugin args ( session , parser ) : plugin args = parser . add argument group ( \"Plugin options\" ) for pname , plugin in session . plugins . items ( ) : defaults = { } for parg in plugin . arguments : plugin args . add argument ( parg . argument name ( pname ) , * * parg . options ) defaults [ parg . dest ] = parg . default plugin . options = Plugin Options ( defaults )", "predictions": ["add any plugin args to the parser ."], "references": ["sets streamlink plugin options ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 654, "code": "def setup plugin options ( session , plugin ) : pname = plugin . module required = Ordered Dict ( { } ) for parg in plugin . arguments : if parg . options . get ( \"help\" ) != argparse . SUPPRESS : if parg . required : required [ parg . name ] = parg value = getattr ( args , parg . namespace dest ( pname ) ) session . set plugin option ( pname , parg . dest , value ) if parg . required or value : try : for rparg in plugin . arguments . requires ( parg . name ) : required [ rparg . name ] = rparg except Runtime Error : console . logger . error ( \"{0} plugin has a configuration error and the arguments \" \"cannot be parsed\" . format ( pname ) ) break if required : for req in required . values ( ) : if not session . get plugin option ( pname , req . dest ) : prompt = req . prompt or \"Enter {0} {1}\" . format ( pname , req . name ) session . set plugin option ( pname , req . dest , console . askpass ( prompt + \": \" ) if req . sensitive else console . ask ( prompt + \": \" ) )", "predictions": ["ask the plugin options ."], "references": ["sets streamlink plugin options ."], "bleu": 0.5081327481546147, "rouge_l": 0.6}
{"id": 655, "code": "def log current versions ( ) : if logger . root . is Enabled For ( logging . DEBUG ) : if sys . platform == \"darwin\" : os version = \"mac OS {0}\" . format ( platform . mac ver ( ) [ 0 ] ) elif sys . platform . startswith ( \"win\" ) : os version = \"{0} {1}\" . format ( platform . system ( ) , platform . release ( ) ) else : os version = platform . platform ( ) log . debug ( \"OS:         {0}\" . format ( os version ) ) log . debug ( \"Python:     {0}\" . format ( platform . python version ( ) ) ) log . debug ( \"Streamlink: {0}\" . format ( streamlink version ) ) log . debug ( \"Requests({0}), Socks({1}), Websocket({2})\" . format ( requests . version , socks version , websocket version ) )", "predictions": ["log current versions of the current versions ."], "references": ["show current installed versions"], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 656, "code": "def get stream id ( self , text ) : m = self . image re . search ( text ) if m : return m . group ( \"stream id\" )", "predictions": ["return the stream id of the stream ."], "references": ["try to find a stream_id"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 657, "code": "def get iframe ( self , text ) : m = self . iframe re . search ( text ) if m : return self . session . streams ( m . group ( \"url\" ) )", "predictions": ["get the iframe of the iframe"], "references": ["fallback if no stream_id was found before"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 658, "code": "def startswith ( string ) : def starts with ( value ) : validate ( text , value ) if not value . startswith ( string ) : raise Value Error ( \"'{0}' does not start with '{1}'\" . format ( value , string ) ) return True return starts with", "predictions": ["check that a string is a valid string ."], "references": ["checks if the string value starts with another string ."], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 659, "code": "def endswith ( string ) : def ends with ( value ) : validate ( text , value ) if not value . endswith ( string ) : raise Value Error ( \"'{0}' does not end with '{1}'\" . format ( value , string ) ) return True return ends with", "predictions": ["check that a string is a valid string ."], "references": ["checks if the string value ends with another string ."], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 660, "code": "def contains ( string ) : def contains str ( value ) : validate ( text , value ) if string not in value : raise Value Error ( \"'{0}' does not contain '{1}'\" . format ( value , string ) ) return True return contains str", "predictions": ["check if a string contains a string ."], "references": ["checks if the string value contains another string ."], "bleu": 0.22149455506955362, "rouge_l": 0.5820610687022901}
{"id": 661, "code": "def url ( * * attributes ) : def check url ( value ) : validate ( text , value ) parsed = urlparse ( value ) if not parsed . netloc : raise Value Error ( \"'{0}' is not a valid URL\" . format ( value ) ) for name , schema in attributes . items ( ) : if not hasattr ( parsed , name ) : raise Value Error ( \"Invalid URL attribute '{0}'\" . format ( name ) ) try : validate ( schema , getattr ( parsed , name ) ) except Value Error as err : raise Value Error ( \"Unable to validate URL attribute '{0}': {1}\" . format ( name , err ) ) return True if attributes . get ( \"scheme\" ) == \"http\" : attributes [ \"scheme\" ] = any ( \"http\" , \"https\" ) return check url", "predictions": ["validates that a url is valid ."], "references": ["parses an url and validates its attributes ."], "bleu": 0.19148978368719022, "rouge_l": 0.2634989200863931}
{"id": 662, "code": "def xml find ( xpath ) : def xpath find ( value ) : validate ( ET . iselement , value ) value = value . find ( xpath ) if value is None : raise Value Error ( \"X Path '{0}' did not return an element\" . format ( xpath ) ) return validate ( ET . iselement , value ) return transform ( xpath find )", "predictions": ["find the xml element for the given xpath ."], "references": ["find a xml element via xpath ."], "bleu": 0.24446151121745052, "rouge_l": 0.639412997903564}
{"id": 663, "code": "def xml findall ( xpath ) : def xpath findall ( value ) : validate ( ET . iselement , value ) return value . findall ( xpath ) return transform ( xpath findall )", "predictions": ["return an xml string that represents an xml string ."], "references": ["find a list of xml elements via xpath ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 664, "code": "def dologin ( self , email , password , emailauth = \"\" , emailsteamid = \"\" , captchagid = \"-1\" , captcha text = \"\" , twofactorcode = \"\" ) : epassword , rsatimestamp = self . encrypt password ( email , password ) login data = { 'username' : email , \"password\" : epassword , \"emailauth\" : emailauth , \"loginfriendlyname\" : \"Streamlink\" , \"captchagid\" : captchagid , \"captcha text\" : captcha text , \"emailsteamid\" : emailsteamid , \"rsatimestamp\" : rsatimestamp , \"remember login\" : True , \"donotcache\" : self . donotcache , \"twofactorcode\" : twofactorcode } res = self . session . http . post ( self . dologin url , data = login data ) resp = self . session . http . json ( res , schema = self . dologin schema ) if not resp [ u\"success\" ] : if resp . get ( u\"captcha needed\" ) : captchagid = resp [ u\"captcha gid\" ] log . error ( \"Captcha result required, open this URL to see the captcha: {}\" . format ( self . captcha url . format ( captchagid ) ) ) try : captcha text = self . input ask ( \"Captcha text\" ) except Fatal Plugin Error : captcha text = None if not captcha text : return False else : if resp . get ( u\"emailauth needed\" ) : if not emailauth : try : emailauth = self . input ask ( \"Email auth code required\" ) except Fatal Plugin Error : emailauth = None if not emailauth : return False else : raise Steam Login Failed ( \"Email auth key error\" ) if resp . get ( u\"requires twofactor\" ) : try : twofactorcode = self . input ask ( \"Two factor auth code required\" ) except Fatal Plugin Error : twofactorcode = None if not twofactorcode : return False if resp . get ( u\"message\" ) : raise Steam Login Failed ( resp [ u\"message\" ] ) return self . dologin ( email , password , emailauth = emailauth , emailsteamid = resp . get ( u\"emailsteamid\" , u\"\" ) , captcha text = captcha text , captchagid = captchagid , twofactorcode = twofactorcode ) elif resp . get ( \"login complete\" ) : return True else : log . error ( \"Something when wrong when logging in to Steam\" ) return False", "predictions": ["encrypt the user with the given email address"], "references": ["logs in to steam"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 665, "code": "def get stream id ( self , html ) : stream id = stream id pattern . search ( html ) if not stream id : self . logger . error ( \"Failed to extract stream id.\" ) return stream id . group ( \"stream id\" )", "predictions": ["extract the stream id from the stream ."], "references": ["returns the stream_id contained in the html ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 666, "code": "def login ( self , username , password ) : self . logger . debug ( 'login ...' ) res = self . session . http . get ( self . login url ) input list = self . input re . findall ( res . text ) if not input list : raise Plugin Error ( 'Missing input data on login website.' ) data = { } for input data in input list : try : input name = self . name re . search ( input data ) . group ( 1 ) except Attribute Error : continue try : input value = self . value re . search ( input data ) . group ( 1 ) except Attribute Error : input value = '' data [ input name ] = input value login data = { 'ctl00$Login1$User Name' : username , 'ctl00$Login1$Password' : password , 'ctl00$Login1$Login Button.x' : '0' , 'ctl00$Login1$Login Button.y' : '0' } data . update ( login data ) res = self . session . http . post ( self . login url , data = data ) for cookie in self . session . http . cookies : self . session attributes . set ( cookie . name , cookie . value , expires = 3600 * 24 ) if self . session attributes . get ( 'ASP.NET Session Id' ) and self . session attributes . get ( '.abportail1' ) : self . logger . debug ( 'New session data' ) self . set expires time cache ( ) return True else : self . logger . error ( 'Failed to login, check your username/password' ) return False", "predictions": ["login to the input list"], "references": ["login and update cached cookies"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 667, "code": "def output Char Formatter ( c ) : #TODO 2: allow hex only output if 32 < c < 127 : return chr ( c ) elif c == 10 : return '\\\\n' elif c == 13 : return '\\\\r' elif c == 32 : return '\" \"' else : return '\\\\x{:02x}' . format ( c )", "predictions": ["return string representation of c numbers ."], "references": ["show character in readable format"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 668, "code": "def output Formatter ( s ) : result = '' def format Sub String ( s ) : for c in s : if c == 32 : yield ' ' else : yield output Char Formatter ( c ) if len ( result ) < 200 : return '' . join ( format Sub String ( s ) ) else : return '' . join ( format Sub String ( s [ : 100 ] ) ) + '...' + '' . join ( format Sub String ( s [ - 100 : ] ) )", "predictions": ["yield a string of characters from a string ."], "references": ["show string or char ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 669, "code": "def read Bytes ( self , n ) : if self . pos & 7 : raise Value Error ( 'read Bytes: need byte boundary' ) result = self . data [ self . pos >> 3 : ( self . pos >> 3 ) + n ] self . pos += 8 * n return result", "predictions": ["read n bytes from the buffer ."], "references": ["read n bytes from the stream on a byte boundary ."], "bleu": 0.39931601353061863, "rouge_l": 0.6409807355516637}
{"id": 670, "code": "def show Code ( self , width = 80 ) : #make table of all symbols with binary strings symbol Strings = [ ( self . bit Pattern ( s . index ) , self . mnemonic ( s . index ) ) for s in self ] #determine column widths the way Lisp programmers do it left Col Width , right Col Width = map ( max , map ( map , repeat ( len ) , zip ( * symbol Strings ) ) ) colwidth = left Col Width + right Col Width columns = 81 // ( colwidth + 2 ) rows = - ( - len ( symbol Strings ) // columns ) def justify ( bs ) : b , s = bs return b . rjust ( left Col Width ) + ':' + s . ljust ( right Col Width ) for i in range ( rows ) : print ( ' ' . join ( map ( justify , symbol Strings [ i : : rows ] ) ) . rstrip ( ) )", "predictions": ["print the column widths of the table"], "references": ["show all words of the code in a nice format ."], "bleu": 0.1380518455178974, "rouge_l": 0.2136602451838879}
{"id": 671, "code": "def read Tuple ( self , stream ) : length , symbol = self . decode Peek ( stream . peek ( self . max Length ) ) stream . pos += length return length , symbol", "predictions": ["reads a single length from the stream ."], "references": ["read symbol from stream . returns symbol length ."], "bleu": 0.2116253761537182, "rouge_l": 0.34923664122137404}
{"id": 672, "code": "def value ( self , index , extra ) : lower , upper = self . span ( index ) value = lower + ( extra or 0 ) if value > upper : raise Value Error ( 'value: extra out of range' ) return value", "predictions": ["return the xml xml xml xml string ."], "references": ["override if you don t define value0 and extratable"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 673, "code": "def value ( self , index , extra ) : index = index if index == 0 : return 1 , 0 if index <= self . RLEMAX : return ( 1 << index ) + extra , 0 return 1 , index - self . RLEMAX", "predictions": ["return get the get value of the given index"], "references": ["give count and value ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 674, "code": "def mnemonic ( self , index ) : i , c , d0 = self . split Symbol ( index ) i Lower , = i . code . span ( i . index ) i Extra = i . extra Bits ( ) c Lower , = c . code . span ( c . index ) c Extra = c . extra Bits ( ) return 'I{}{}{}C{}{}{}{}' . format ( i Lower , '+' if i Extra else '' , 'x' * i Extra if i Extra < 6 else '[{}*x]' . format ( i Extra ) , c Lower , '+' if c Extra else '' , 'x' * c Extra if c Extra < 6 else '[{}*x]' . format ( c Extra ) , '&D=0' if d0 else '' )", "predictions": ["returns the login login login login login"], "references": ["make a nice mnemonic"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 675, "code": "def compile Actions ( self ) : import re self . action List = actions = [ None ] * 121 #Action 73, which is too long, looks like this when expanded: actions [ 73 ] = \"b' the '+w+b' of the '\" #find out what the columns are action Lines = self . action Table . splitlines ( ) colon Positions = [ m . start ( ) for m in re . finditer ( ':' , action Lines [ 1 ] ) ] + [ 100 ] columns = [ ( colon Positions [ i ] - 3 , colon Positions [ i + 1 ] - 3 ) for i in range ( len ( colon Positions ) - 1 ) ] for line in self . action Table . splitlines ( keepends = False ) : for start , end in columns : action = line [ start : end ] #skip empty actions if not action or action . isspace ( ) : continue #chop it up, and check if the colon is properly placed index , colon , action = action [ : 3 ] , action [ 3 ] , action [ 4 : ] assert colon == ':' #remove filler spaces at right action = action . rstrip ( ) #replace space symbols action = action . replace ( ' ' , ' ' ) w Pos = action . index ( 'w' ) #add quotes around left string when present #translation: any pattern from beginning, up to #(but not including) a + following by a w later on action = re . sub ( r\"^(.*)(?=\\+[U(]*w)\" , r\"b'\\1'\" , action ) #add quotes around right string when present #translation: anything with a w in it, followed by a + #and a pattern up to the end #(there is no variable lookbehind assertion, #so we have to copy the pattern) action = re . sub ( r\"(w[[:\\-1\\]).U]*)\\+(.*)$\" , r\"\\1+b'\\2'\" , action ) #expand shortcut for uppercase All action = action . replace ( \".U\" , \".upper()\" ) #store action actions [ int ( index ) ] = action", "predictions": ["output the to the"], "references": ["build the action table from the text above"], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 676, "code": "def do Action ( self , w , action ) : #set environment for the Upper Case First U = self . upper Case1 return eval ( self . action List [ action ] , locals ( ) )", "predictions": ["get the action associated with the given action . ."], "references": ["perform the proper action"], "bleu": 0.13950796967929133, "rouge_l": 0.3096446700507614}
{"id": 677, "code": "def process Stream ( self ) : print ( 'addr  hex{:{}s}binary context explanation' . format ( '' , self . width - 10 ) ) print ( 'Stream header' . center ( 60 , '-' ) ) self . window Size = self . verbose Read ( Window Size Alphabet ( ) ) print ( 'Metablock header' . center ( 60 , '=' ) ) self . ISLAST = False self . output = bytearray ( ) while not self . ISLAST : self . ISLAST = self . verbose Read ( Bool Code ( 'LAST' , description = \"Last block\" ) ) if self . ISLAST : if self . verbose Read ( Bool Code ( 'EMPTY' , description = \"Empty block\" ) ) : break if self . metablock Length ( ) : continue if not self . ISLAST and self . uncompressed ( ) : continue print ( 'Block type descriptors' . center ( 60 , '-' ) ) self . number Of Block Types = { } self . current Block Counts = { } self . block Type Codes = { } self . block Count Codes = { } for block Type in ( L , I , D ) : self . block Type ( block Type ) print ( 'Distance code parameters' . center ( 60 , '-' ) ) self . NPOSTFIX , self . NDIRECT = self . verbose Read ( Distance Param Alphabet ( ) ) self . read Literal Context Modes ( ) print ( 'Context maps' . center ( 60 , '-' ) ) self . cmaps = { } #keep the number of each kind of prefix tree for the last loop number Of Trees = { I : self . number Of Block Types [ I ] } for block Type in ( L , D ) : number Of Trees [ block Type ] = self . context Map ( block Type ) print ( 'Prefix code lists' . center ( 60 , '-' ) ) self . prefix Codes = { } for block Type in ( L , I , D ) : self . read Prefix Array ( block Type , number Of Trees [ block Type ] ) self . metablock ( )", "predictions": ["output the from 'stream"], "references": ["process a brotli stream ."], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 678, "code": "def uncompressed ( self ) : ISUNCOMPRESSED = self . verbose Read ( Bool Code ( 'UNCMPR' , description = 'Is uncompressed?' ) ) if ISUNCOMPRESSED : self . verbose Read ( Filler Alphabet ( stream Pos = self . stream . pos ) ) print ( 'Uncompressed data:' ) self . output += self . stream . read Bytes ( self . MLEN ) print ( output Formatter ( self . output [ - self . MLEN : ] ) ) return ISUNCOMPRESSED", "predictions": ["not thread - safe"], "references": ["if true handle uncompressed data"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 679, "code": "def block Type ( self , kind ) : NBLTYPES = self . verbose Read ( Type Count Alphabet ( 'BT#' + kind [ 0 ] . upper ( ) , description = '{} block types' . format ( kind ) , ) ) self . number Of Block Types [ kind ] = NBLTYPES if NBLTYPES >= 2 : self . block Type Codes [ kind ] = self . read Prefix Code ( Block Type Alphabet ( 'BT' + kind [ 0 ] . upper ( ) , NBLTYPES ) ) self . block Count Codes [ kind ] = self . read Prefix Code ( Block Count Alphabet ( 'BC' + kind [ 0 ] . upper ( ) ) ) block Count = self . verbose Read ( self . block Count Codes [ kind ] ) else : block Count = 1 << 24 self . current Block Counts [ kind ] = block Count", "predictions": ["standard out the fetch value for the given fetch = value ."], "references": ["read block type switch descriptor for given kind of blocktype ."], "bleu": 0.1235622127262679, "rouge_l": 0.2629310344827586}
{"id": 680, "code": "def IMTF ( v ) : #mtf is initialized virtually with range(infinity) mtf = [ ] for i , vi in enumerate ( v ) : #get old value from mtf. If never seen, take virtual value try : value = mtf . pop ( vi ) except Index Error : value = vi #put value at front mtf . insert ( 0 , value ) #replace transformed value v [ i ] = value", "predictions": ["remove all vi in a group"], "references": ["in place inverse move to front transform ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 681, "code": "def read Prefix Array ( self , kind , number Of Trees ) : prefixes = [ ] for i in range ( number Of Trees ) : if kind == L : alphabet = Literal Alphabet ( i ) elif kind == I : alphabet = Insert And Copy Alphabet ( i ) elif kind == D : alphabet = Distance Alphabet ( i , NPOSTFIX = self . NPOSTFIX , NDIRECT = self . NDIRECT ) self . read Prefix Code ( alphabet ) prefixes . append ( alphabet ) self . prefix Codes [ kind ] = prefixes", "predictions": ["print formatted formatted formatted formatted formatted formatted for a given list of = value"], "references": ["read prefix code array"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 682, "code": "def arrow table from vaex df ( ds , column names = None , selection = None , strings = True , virtual = False ) : names = [ ] arrays = [ ] for name , array in ds . to items ( column names = column names , selection = selection , strings = strings , virtual = virtual ) : names . append ( name ) arrays . append ( arrow array from numpy array ( array ) ) return pyarrow . Table . from arrays ( arrays , names )", "predictions": ["convert a pyarrow into an pyarrow plugins"], "references": ["implementation of dataset . to_arrow_table"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 683, "code": "def patch ( f ) : name = f . name Dataset . hidden [ name ] = f return f", "predictions": ["decorator to setup a function args args args args args args args args args args args ."], "references": ["adds method f to the dataset class"], "bleu": 0.07223943354597204, "rouge_l": 0.09010339734121123}
{"id": 684, "code": "def graphviz ( self , dot = None ) : from graphviz import Graph , Digraph node = self . graph ( ) dot = dot or Digraph ( comment = self . expression ) def walk ( node ) : if isinstance ( node , six . string types ) : dot . node ( node , node ) return node , node else : node repr , fname , fobj , deps = node node id = node repr dot . node ( node id , node repr ) for dep in deps : dep id , dep = walk ( dep ) dot . edge ( node id , dep id ) return node id , node walk ( node ) return dot", "predictions": ["return the setup of the setup ."], "references": ["return a graphviz . digraph object with a graph of the expression"], "bleu": 0.13597602315271134, "rouge_l": 0.30148270181219106}
{"id": 685, "code": "def from astropy table ( table ) : import vaex . file . other return vaex . file . other . Dataset Astropy Table ( table = table )", "predictions": ["create an instance from an plugin args ."], "references": ["create a vaex dataframe from an astropy table ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 686, "code": "def zeldovich ( dim = 2 , N = 256 , n = - 2.5 , t = None , scale = 1 , seed = None ) : import vaex . file return vaex . file . other . Zeldovich ( dim = dim , N = N , n = n , t = t , scale = scale )", "predictions": ["convert from in - place to in - place"], "references": ["creates a zeldovich dataframe ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 687, "code": "def vrange ( start , stop , step = 1 , dtype = 'f8' ) : from . column import Column Virtual Range return Column Virtual Range ( start , stop , step , dtype )", "predictions": ["os log for versions of versions ."], "references": ["creates a virtual column which is the equivalent of numpy . arange but uses 0 memory"], "bleu": 0.056829570481990416, "rouge_l": 0.16245006657789615}
{"id": 688, "code": "def open ( self , path ) : logger . debug ( \"open dataset: %r\" , path ) if path . startswith ( \"http\" ) or path . startswith ( \"ws\" ) : dataset = vaex . open ( path , thread mover = self . call in main thread ) else : dataset = vaex . open ( path ) self . add recently opened ( path ) self . dataset selector . add ( dataset ) return dataset", "predictions": ["get a dataset dataset dataset dataset ."], "references": ["add a dataset and add it to the ui"], "bleu": 0.18370727471078332, "rouge_l": 0.24448897795591182}
{"id": 689, "code": "def evaluate ( self , expression , i1 = None , i2 = None , out = None , selection = None , delay = False ) : expression = ensure strings from expressions ( expression ) result = self . server . call dataset ( \"evaluate\" , self , expression = expression , i1 = i1 , i2 = i2 , selection = selection , delay = delay ) return result", "predictions": ["get model from input expression . . . . . . . . . . ."], "references": ["basic support for evaluate at server at least to run some unittest do not expect this to work from strings"], "bleu": 0.05990827693966461, "rouge_l": 0.05446428571428571}
{"id": 690, "code": "def depending columns ( self , ds ) : depending = set ( ) for expression in self . expressions : expression = ds . expr ( expression ) depending |= expression . variables ( ) if self . previous selection : depending |= self . previous selection . depending columns ( ds ) return depending", "predictions": ["return a text of all raise an columns if any ."], "references": ["find all columns that this selection depends on for df ds"], "bleu": 0.12605968092174913, "rouge_l": 0.18181818181818182}
{"id": 691, "code": "def task ( self , task , progressbar = False ) : if self . delay : return self . executor . schedule ( task ) else : import vaex . utils callback = None try : if progressbar == True : def update ( fraction ) : bar . update ( fraction ) return True bar = vaex . utils . progressbar ( task . name ) callback = self . executor . signal progress . connect ( update ) elif progressbar : callback = self . executor . signal progress . connect ( progressbar ) result = self . executor . run ( task ) if progressbar == True : bar . finish ( ) sys . stdout . write ( '\\n' ) return result finally : if callback : self . executor . signal progress . disconnect ( callback )", "predictions": ["process a endswith endswith"], "references": ["helper function for returning tasks results result when immediate is true otherwise the task itself which is a promise"], "bleu": 0.008450022790166857, "rouge_l": 0.07780612244897958}
{"id": 692, "code": "def sort ( self , Ncol , order ) : self . emit ( Qt Core . SIGNAL ( \"layout About To Be Changed()\" ) ) if Ncol == 0 : print ( \"by name\" ) sortlist = list ( zip ( self . pairs , list ( range ( len ( self . pairs ) ) ) ) ) print ( sortlist ) sortlist . sort ( key = operator . itemgetter ( 0 ) ) print ( sortlist ) self . indices = list ( map ( operator . itemgetter ( 1 ) , sortlist ) ) print ( ( self . indices ) ) if Ncol == 1 : if None not in self . ranking : sortlist = list ( zip ( self . ranking , list ( range ( len ( self . pairs ) ) ) ) ) sortlist . sort ( key = operator . itemgetter ( 0 ) ) self . indices = list ( map ( operator . itemgetter ( 1 ) , sortlist ) ) else : self . indices = list ( range ( len ( self . pairs ) ) ) print ( ( self . indices ) ) if order == Qt Core . Qt . Descending Order : self . indices . reverse ( ) print ( ( self . indices ) ) self . emit ( Qt Core . SIGNAL ( \"layout Changed()\" ) )", "predictions": ["contains the ranking ."], "references": ["sort table by given column number ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 693, "code": "def wait ( self ) : logger . debug ( \"will wait for last plot to finish\" ) self . plot event = threading . Event ( ) self . queue update . wait ( ) self . queue replot . wait ( ) self . queue redraw . wait ( ) qt app = Qt Core . Q Core Application . instance ( ) sleep = 10 while not self . plot event . is set ( ) : logger . debug ( \"waiting for last plot to finish\" ) qt app . process Events ( ) Qt Test . Q Test . q Sleep ( sleep ) logger . debug ( \"waiting for plot finished\" )", "predictions": ["url to the raise an exception if any ."], "references": ["used for unittesting to make sure the plots are all done"], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 694, "code": "def os open ( document ) : osname = platform . system ( ) . lower ( ) if osname == \"darwin\" : os . system ( \"open \\\"\" + document + \"\\\"\" ) if osname == \"linux\" : cmd = \"xdg-open \\\"\" + document + \"\\\"&\" os . system ( cmd ) if osname == \"windows\" : os . system ( \"start \\\"\" + document + \"\\\"\" )", "predictions": ["find the xml file for xml ."], "references": ["open document by the default handler of the os could be a url opened by a browser a text file by an editor etc"], "bleu": 0.018123322676611493, "rouge_l": 0.11742059672762271}
{"id": 695, "code": "def write to ( f , mode ) : if hasattr ( f , 'write' ) : yield f else : f = open ( f , mode ) yield f f . close ( )", "predictions": ["xml to a file or file - like object ."], "references": ["flexible writing where f can be a filename or f object if filename closed after writing"], "bleu": 0.08227293930285365, "rouge_l": 0.22154963680387407}
{"id": 696, "code": "def split and combine mask ( arrays ) : masks = [ np . ma . getmaskarray ( block ) for block in arrays if np . ma . is Masked Array ( block ) ] arrays = [ block . data if np . ma . is Masked Array ( block ) else block for block in arrays ] mask = None if masks : mask = masks [ 0 ] . copy ( ) for other in masks [ 1 : ] : mask |= other return arrays , mask", "predictions": ["split and and and"], "references": ["combines all masks from a list of arrays and logically ors them into a single mask"], "bleu": 0.017888698387160718, "rouge_l": 0.09023668639053255}
{"id": 697, "code": "def nop ( self , expression , progress = False , delay = False ) : expression = ensure string from expression ( expression ) def map ( ar ) : pass def reduce ( a , b ) : pass return self . map reduce ( map , reduce , [ expression ] , delay = delay , progress = progress , name = 'nop' , to numpy = False )", "predictions": ["apply a get to an self return a new self return the new self ."], "references": ["evaluates expression and drop the result usefull for benchmarking since vaex is usually lazy"], "bleu": 0.08225964699966554, "rouge_l": 0.06939704209328781}
{"id": 698, "code": "def plot3d ( self , x , y , z , vx = None , vy = None , vz = None , vwhat = None , limits = None , grid = None , what = \"count(*)\" , shape = 128 , selection = [ None , True ] , f = None , vcount limits = None , smooth pre = None , smooth post = None , grid limits = None , normalize = \"normalize\" , colormap = \"afmhot\" , figure key = None , fig = None , lighting = True , level = [ 0.1 , 0.5 , 0.9 ] , opacity = [ 0.01 , 0.05 , 0.1 ] , level width = 0.1 , show = True , * * kwargs ) : import vaex . ext . ipyvolume cls = vaex . ext . ipyvolume . Plot Default plot3d = cls ( df = self , x = x , y = y , z = z , vx = vx , vy = vy , vz = vz , grid = grid , shape = shape , limits = limits , what = what , f = f , figure key = figure key , fig = fig , selection = selection , smooth pre = smooth pre , smooth post = smooth post , grid limits = grid limits , vcount limits = vcount limits , normalize = normalize , colormap = colormap , * * kwargs ) if show : plot3d . show ( ) return plot3d", "predictions": ["login to vaex the login data ."], "references": ["use at own risk requires ipyvolume"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 699, "code": "def dtype ( self , expression , internal = False ) : expression = ensure string from expression ( expression ) if expression in self . variables : return np . float64 ( 1 ) . dtype elif expression in self . columns . keys ( ) : column = self . columns [ expression ] data = column [ 0 : 1 ] dtype = data . dtype else : data = self . evaluate ( expression , 0 , 1 , filtered = False ) dtype = data . dtype if not internal : if dtype != str type : if dtype . kind in 'US' : return str type if dtype . kind == 'O' : if isinstance ( data [ 0 ] , six . string types ) : return str type return dtype", "predictions": ["chr the c - type of an c c { c } . }"], "references": ["return the numpy dtype for the given expression if not a column the first row will be evaluated to get the dtype ."], "bleu": 0.05143456019237098, "rouge_l": 0.10356536502546689}
{"id": 700, "code": "def remove virtual meta ( self ) : dir = self . get private dir ( create = True ) path = os . path . join ( dir , \"virtual meta.yaml\" ) try : if os . path . exists ( path ) : os . remove ( path ) if not os . listdir ( dir ) : os . rmdir ( dir ) except : logger . exception ( \"error while trying to remove %s or %s\" , path , dir )", "predictions": ["output virtual meta format format format format format format format format format format format format format format format format format format format format format format format format format format format format"], "references": ["removes the file with the virtual column etc it does not change the current virtual columns etc ."], "bleu": 0.03901663112717908, "rouge_l": 0.04286718200983837}
{"id": 701, "code": "def evaluate variable ( self , name ) : if isinstance ( self . variables [ name ] , six . string types ) : value = eval ( self . variables [ name ] , expression namespace , self . variables ) return value else : return self . variables [ name ]", "predictions": ["read a variable from the & = value"], "references": ["evaluates the variable given by name ."], "bleu": 0.17747405280050269, "rouge_l": 0.13495575221238937}
{"id": 702, "code": "def evaluate selection mask ( self , name = \"default\" , i1 = None , i2 = None , selection = None , cache = False ) : i1 = i1 or 0 i2 = i2 or len ( self ) scope = scopes . Block Scope Selection ( self , i1 , i2 , selection , cache = cache ) return scope . evaluate ( name )", "predictions": ["show the scope scope"], "references": ["internal use ignores the filter"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 703, "code": "def add column ( self , name , f or array ) : if isinstance ( f or array , ( np . ndarray , Column ) ) : data = ar = f or array if self . length original is None : self . length unfiltered = len ( data ) self . length original = len ( data ) self . index end = self . length unfiltered if len ( ar ) != self . length original ( ) : if self . filtered : if len ( self ) == len ( ar ) : raise Value Error ( \"Array is of length %s, while the length of the Data Frame is %s due to the filtering, the (unfiltered) length is %s.\" % ( len ( ar ) , len ( self ) , self . length unfiltered ( ) ) ) raise Value Error ( \"array is of length %s, while the length of the Data Frame is %s\" % ( len ( ar ) , self . length original ( ) ) ) self . columns [ name ] = f or array if name not in self . column names : self . column names . append ( name ) else : raise Value Error ( \"functions not yet implemented\" ) self . save assign expression ( name , Expression ( self , name ) )", "predictions": ["read a column from the index"], "references": ["add an in memory array as a column ."], "bleu": 0.17632778423526832, "rouge_l": 0.2573839662447257}
{"id": 704, "code": "def rename column ( self , name , new name , unique = False , store in state = True ) : new name = vaex . utils . find valid name ( new name , used = [ ] if not unique else list ( self ) ) data = self . columns . get ( name ) if data is not None : del self . columns [ name ] self . column names [ self . column names . index ( name ) ] = new name self . columns [ new name ] = data else : expression = self . virtual columns [ name ] del self . virtual columns [ name ] self . virtual columns [ new name ] = expression if store in state : self . renamed columns . append ( ( name , new name ) ) for d in [ self . ucds , self . units , self . descriptions ] : if name in d : d [ new name ] = d [ name ] del d [ name ] return new name", "predictions": ["rename a column in the state ."], "references": ["renames a column not this is only the in memory name this will not be reflected on disk"], "bleu": 0.05770453188915095, "rouge_l": 0.2223572296476306}
{"id": 705, "code": "def delete virtual column ( self , name ) : del self . virtual columns [ name ] self . signal column changed . emit ( self , name , \"delete\" )", "predictions": ["delete the column column ."], "references": ["deletes a virtual column from a dataframe ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 706, "code": "def delete variable ( self , name ) : del self . variables [ name ] self . signal variable changed . emit ( self , name , \"delete\" )", "predictions": ["deletes a variable from the variable"], "references": ["deletes a variable from a dataframe ."], "bleu": 0.5204069361525009, "rouge_l": 0.6069651741293532}
{"id": 707, "code": "def tail ( self , n = 10 ) : N = len ( self ) return self [ max ( 0 , N - n ) : min ( len ( self ) , N ) ]", "predictions": ["return the number of n n - th n - th n - 1"], "references": ["return a shallow copy a dataframe with the last n rows ."], "bleu": 0.10511846841633776, "rouge_l": 0.2340153452685422}
{"id": 708, "code": "def head and tail print ( self , n = 5 ) : from I Python import display display . display ( display . HTML ( self . head and tail table ( n ) ) )", "predictions": ["print the head and tail the table and tail the table ."], "references": ["display the first and last n elements of a dataframe ."], "bleu": 0.1235622127262679, "rouge_l": 0.2629310344827586}
{"id": 709, "code": "def set current row ( self , value ) : if ( value is not None ) and ( ( value < 0 ) or ( value >= len ( self ) ) ) : raise Index Error ( \"index %d out of range [0,%d]\" % ( value , len ( self ) ) ) self . current row = value self . signal pick . emit ( self , value )", "predictions": ["set the current row of the current row ."], "references": ["set the current row and emit the signal signal_pick ."], "bleu": 0.3823532128872633, "rouge_l": 0.6256410256410255}
{"id": 710, "code": "def selection undo ( self , name = \"default\" , executor = None ) : logger . debug ( \"undo\" ) executor = executor or self . executor assert self . selection can undo ( name = name ) selection history = self . selection histories [ name ] index = self . selection history indices [ name ] self . selection history indices [ name ] -= 1 self . signal selection changed . emit ( self ) logger . debug ( \"undo: selection history is %r, index is %r\" , selection history , self . selection history indices [ name ] )", "predictions": ["undo the selection of the selection ."], "references": ["undo selection for the name ."], "bleu": 0.23356898886410002, "rouge_l": 0.6240409207161125}
{"id": 711, "code": "def selection redo ( self , name = \"default\" , executor = None ) : logger . debug ( \"redo\" ) executor = executor or self . executor assert self . selection can redo ( name = name ) selection history = self . selection histories [ name ] index = self . selection history indices [ name ] next = selection history [ index + 1 ] self . selection history indices [ name ] += 1 self . signal selection changed . emit ( self ) logger . debug ( \"redo: selection history is %r, index is %r\" , selection history , index )", "predictions": ["redo the selection in the selection"], "references": ["redo selection for the name ."], "bleu": 0.2626909894424158, "rouge_l": 0.5}
{"id": 712, "code": "def selection can redo ( self , name = \"default\" ) : return ( self . selection history indices [ name ] + 1 ) < len ( self . selection histories [ name ] )", "predictions": ["return whether the selection selection can be redo ."], "references": ["can selection name be redone?"], "bleu": 0.16784459625186196, "rouge_l": 0.3012345679012346}
{"id": 713, "code": "def selection ( self , create selection , name , executor = None , execute fully = False ) : selection history = self . selection histories [ name ] previous index = self . selection history indices [ name ] current = selection history [ previous index ] if selection history else None selection = create selection ( current ) executor = executor or self . executor selection history . append ( selection ) self . selection history indices [ name ] += 1 del selection history [ self . selection history indices [ name ] : - 1 ] if 0 : if self . is local ( ) : if selection : result = vaex . promise . Promise . fulfilled ( None ) self . signal selection changed . emit ( self ) else : result = vaex . promise . Promise . fulfilled ( None ) self . signal selection changed . emit ( self ) else : self . signal selection changed . emit ( self ) result = vaex . promise . Promise . fulfilled ( None ) self . signal selection changed . emit ( self ) result = vaex . promise . Promise . fulfilled ( None ) logger . debug ( \"select selection history is %r, index is %r\" , selection history , self . selection history indices [ name ] ) return result", "predictions": ["return a selection of the selection"], "references": ["select_lasso and select almost share the same code"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 714, "code": "def find valid name ( self , initial name ) : return vaex . utils . find valid name ( initial name , used = self . get column names ( hidden = True ) )", "predictions": ["find the valid name for the given initial name ."], "references": ["finds a non - colliding name by optional postfixing"], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 715, "code": "def root nodes ( self ) : root nodes = [ ] leafes = [ ] def walk ( node ) : if isinstance ( node , six . string types ) : leafes . append ( node ) if node in root nodes : root nodes . remove ( node ) else : node repr , fname , fobj , deps = node if node repr in self . virtual columns : leafes . append ( node repr ) if node repr in root nodes : root nodes . remove ( node repr ) for dep in deps : walk ( dep ) for column in self . virtual columns . keys ( ) : if column not in leafes : root nodes . append ( column ) node = self [ column ] . graph ( ) node repr , fname , fobj , deps = node for dep in deps : walk ( dep ) return root nodes", "predictions": ["walk through all nodes and walk them ."], "references": ["returns a list of string which are the virtual columns that are not used in any other virtual column ."], "bleu": 0.03578247111355201, "rouge_l": 0.06630434782608696}
{"id": 716, "code": "def graphviz ( self , dot = None ) : from graphviz import Digraph dot = dot or Digraph ( comment = 'whole dataframe' ) root nodes = self . root nodes ( ) for column in root nodes : self [ column ] . graphviz ( dot = dot ) return dot", "predictions": ["returns graphviz nodes in graphviz form"], "references": ["return a graphviz . digraph object with a graph of all virtual columns"], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 717, "code": "def categorize ( self , column , labels = None , check = True ) : column = ensure string from expression ( column ) if check : vmin , vmax = self . minmax ( column ) if labels is None : N = int ( vmax + 1 ) labels = list ( map ( str , range ( N ) ) ) if ( vmax - vmin ) >= len ( labels ) : raise Value Error ( 'value of {} found, which is larger than number of labels {}' . format ( vmax , len ( labels ) ) ) self . categories [ column ] = dict ( labels = labels , N = len ( labels ) )", "predictions": ["categorize a column in the expression"], "references": ["mark column as categorical with given labels assuming zero indexing"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 718, "code": "def hstack ( self , other , prefix = None ) : assert len ( self ) == len ( other ) , \"does not make sense to horizontally stack Data Frames with different lengths\" for name in other . get column names ( ) : if prefix : new name = prefix + name else : new name = name self . add column ( new name , other . columns [ name ] )", "predictions": ["add a sense to the stack ."], "references": ["join the columns of the other dataframe to this one assuming the ordering is the same"], "bleu": 0.056829570481990416, "rouge_l": 0.16245006657789615}
{"id": 719, "code": "def patch ( f ) : name = f . name setattr ( Data Frame , name , f ) return f", "predictions": ["decorator to patch the function of a function ."], "references": ["adds method f to the dataframe class"], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 720, "code": "def as recarray ( self ) : dtype = [ ( k , v . dtype ) for k , v in self . dict . iteritems ( ) ] R = numpy . recarray ( len ( self . dict [ k ] ) , dtype = dtype ) for key in self . dict : R [ key ] = self . dict [ key ] return R", "predictions": ["returns the object as a dictionary"], "references": ["convert into numpy recordarray"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 721, "code": "def show versions ( ) : core deps = [ 'audioread' , 'numpy' , 'scipy' , 'sklearn' , 'joblib' , 'decorator' , 'six' , 'soundfile' , 'resampy' , 'numba' ] extra deps = [ 'numpydoc' , 'sphinx' , 'sphinx rtd theme' , 'sphinxcontrib.versioning' , 'sphinx-gallery' , 'pytest' , 'pytest-mpl' , 'pytest-cov' , 'matplotlib' ] print ( 'INSTALLED VERSIONS' ) print ( '------------------' ) print ( 'python: {}\\n' . format ( sys . version ) ) print ( 'librosa: {}\\n' . format ( version ) ) for dep in core deps : print ( '{}: {}' . format ( dep , get mod version ( dep ) ) ) print ( '' ) for dep in extra deps : print ( '{}: {}' . format ( dep , get mod version ( dep ) ) ) pass", "predictions": ["print versions of available versions ."], "references": ["return the version information for all librosa dependencies ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 722, "code": "def adjust tuning ( input file , output file ) : print ( 'Loading ' , input file ) y , sr = librosa . load ( input file ) print ( 'Separating harmonic component ... ' ) y harm = librosa . effects . harmonic ( y ) print ( 'Estimating tuning ... ' ) tuning = librosa . estimate tuning ( y = y harm , sr = sr ) print ( '{:+0.2f} cents' . format ( 100 * tuning ) ) print ( 'Applying pitch-correction of {:+0.2f} cents' . format ( - 100 * tuning ) ) y tuned = librosa . effects . pitch shift ( y , sr , - tuning ) print ( 'Saving tuned audio to: ' , output file ) librosa . output . write wav ( output file , y tuned , sr )", "predictions": ["adjust the tuning audio audio audio audio audio files ."], "references": ["load audio estimate tuning apply pitch correction and save ."], "bleu": 0.14991106946711685, "rouge_l": 0.2}
{"id": 723, "code": "def cqt filter fft ( sr , fmin , n bins , bins per octave , tuning , filter scale , norm , sparsity , hop length = None , window = 'hann' ) : basis , lengths = filters . constant q ( sr , fmin = fmin , n bins = n bins , bins per octave = bins per octave , tuning = tuning , filter scale = filter scale , norm = norm , pad fft = True , window = window ) n fft = basis . shape [ 1 ] if ( hop length is not None and n fft < 2.0 ** ( 1 + np . ceil ( np . log2 ( hop length ) ) ) ) : n fft = int ( 2.0 ** ( 1 + np . ceil ( np . log2 ( hop length ) ) ) ) basis *= lengths [ : , np . newaxis ] / float ( n fft ) fft = get fftlib ( ) fft basis = fft . fft ( basis , n = n fft , axis = 1 ) [ : , : ( n fft // 2 ) + 1 ] fft basis = util . sparsify rows ( fft basis , quantile = sparsity ) return fft basis , n fft , lengths", "predictions": ["filter the fft of the fft basis ."], "references": ["generate the frequency domain constant - q filter basis ."], "bleu": 0.1867587389639562, "rouge_l": 0.3267857142857143}
{"id": 724, "code": "def trim stack ( cqt resp , n bins ) : max col = min ( x . shape [ 1 ] for x in cqt resp ) cqt resp = np . vstack ( [ x [ : , : max col ] for x in cqt resp ] [ : : - 1 ] ) return np . ascontiguousarray ( cqt resp [ - n bins : ] . T ) . T", "predictions": ["trim the stack in the stack"], "references": ["helper function to trim and stack a collection of cqt responses"], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 725, "code": "def cqt response ( y , n fft , hop length , fft basis , mode ) : D = stft ( y , n fft = n fft , hop length = hop length , window = 'ones' , pad mode = mode ) return fft basis . dot ( D )", "predictions": ["computes the fft response"], "references": ["compute the filter response with a target stft hop ."], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 726, "code": "def early downsample count ( nyquist , filter cutoff , hop length , n octaves ) : downsample count1 = max ( 0 , int ( np . ceil ( np . log2 ( audio . BW FASTEST * nyquist / filter cutoff ) ) - 1 ) - 1 ) num twos = num two factors ( hop length ) downsample count2 = max ( 0 , num twos - n octaves + 1 ) return min ( downsample count1 , downsample count2 )", "predictions": ["computes the count of the number of early between the given number of nyquist and twos ."], "references": ["compute the number of early downsampling operations"], "bleu": 0.20105373454060027, "rouge_l": 0.3604135893648449}
{"id": 727, "code": "def early downsample ( y , sr , hop length , res type , n octaves , nyquist , filter cutoff , scale ) : downsample count = early downsample count ( nyquist , filter cutoff , hop length , n octaves ) if downsample count > 0 and res type == 'kaiser fast' : downsample factor = 2 ** ( downsample count ) hop length //= downsample factor if len ( y ) < downsample factor : raise Parameter Error ( 'Input signal length={:d} is too short for ' '{:d}-octave CQT' . format ( len ( y ) , n octaves ) ) new sr = sr / float ( downsample factor ) y = audio . resample ( y , sr , new sr , res type = res type , scale = True ) if not scale : y *= np . sqrt ( downsample factor ) sr = new sr return y , sr , hop length", "predictions": ["downsample a signal for a early signal ."], "references": ["perform early downsampling on an audio signal if it applies ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 728, "code": "def check axes ( axes ) : if axes is None : import matplotlib . pyplot as plt axes = plt . gca ( ) elif not isinstance ( axes , Axes ) : raise Value Error ( \"`axes` must be an instance of matplotlib.axes.Axes. \" \"Found type(axes)={}\" . format ( type ( axes ) ) ) return axes", "predictions": ["check that the axes is an matplotlib axes ."], "references": ["check if axes is an instance of an axis object . if not use gca ."], "bleu": 0.13356214772572012, "rouge_l": 0.38077403245942576}
{"id": 729, "code": "def scale axes ( axes , ax type , which ) : kwargs = dict ( ) if which == 'x' : thresh = 'linthreshx' base = 'basex' scale = 'linscalex' scaler = axes . set xscale limit = axes . set xlim else : thresh = 'linthreshy' base = 'basey' scale = 'linscaley' scaler = axes . set yscale limit = axes . set ylim if ax type == 'mel' : mode = 'symlog' kwargs [ thresh ] = 1000.0 kwargs [ base ] = 2 elif ax type == 'log' : mode = 'symlog' kwargs [ base ] = 2 kwargs [ thresh ] = core . note to hz ( 'C2' ) kwargs [ scale ] = 0.5 elif ax type in [ 'cqt' , 'cqt hz' , 'cqt note' ] : mode = 'log' kwargs [ base ] = 2 elif ax type == 'tempo' : mode = 'log' kwargs [ base ] = 2 limit ( 16 , 480 ) else : return scaler ( mode , * * kwargs )", "predictions": ["scale the axes to the axes"], "references": ["set the axis scaling"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 730, "code": "def coord fft hz ( n , sr = 22050 , * * kwargs ) : n fft = 2 * ( n - 1 ) basis = core . fft frequencies ( sr = sr , n fft = n fft ) fmax = basis [ - 1 ] basis -= 0.5 * ( basis [ 1 ] - basis [ 0 ] ) basis = np . append ( np . maximum ( 0 , basis ) , [ fmax ] ) return basis", "predictions": ["fft - hz algorithm"], "references": ["get the frequencies for fft bins"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 731, "code": "def coord mel hz ( n , fmin = 0 , fmax = 11025.0 , * * kwargs ) : if fmin is None : fmin = 0 if fmax is None : fmax = 11025.0 basis = core . mel frequencies ( n , fmin = fmin , fmax = fmax ) basis [ 1 : ] -= 0.5 * np . diff ( basis ) basis = np . append ( np . maximum ( 0 , basis ) , [ fmax ] ) return basis", "predictions": ["r generate the basis of an n - qubit basis"], "references": ["get the frequencies for mel bins"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 732, "code": "def coord cqt hz ( n , fmin = None , bins per octave = 12 , * * kwargs ) : if fmin is None : fmin = core . note to hz ( 'C1' ) return core . cqt frequencies ( n + 1 , fmin = fmin / 2.0 ** ( 0.5 / bins per octave ) , bins per octave = bins per octave )", "predictions": ["return the n - th hz of the given n - 1 ."], "references": ["get cqt bin frequencies"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 733, "code": "def coord chroma ( n , bins per octave = 12 , * * kwargs ) : return np . linspace ( 0 , ( 12.0 * n ) / bins per octave , num = n + 1 , endpoint = True )", "predictions": ["r r r r r r r r r r ."], "references": ["get chroma bin numbers"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 734, "code": "def coord time ( n , sr = 22050 , hop length = 512 , * * kwargs ) : return core . frames to time ( np . arange ( n + 1 ) , sr = sr , hop length = hop length )", "predictions": ["return the time n n n - th time in a string"], "references": ["get time coordinates from frames"], "bleu": 0.10390302174233558, "rouge_l": 0.12708333333333333}
{"id": 735, "code": "def window ss fill ( x , win sq , n frames , hop length ) : n = len ( x ) n fft = len ( win sq ) for i in range ( n frames ) : sample = i * hop length x [ sample : min ( n , sample + n fft ) ] += win sq [ : max ( 0 , min ( n fft , n - sample ) ) ]", "predictions": ["fill the window with the window of frames"], "references": ["helper function for window sum - square calculation ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 736, "code": "def match interval overlaps ( query , intervals to , candidates ) : best score = - 1 best idx = - 1 for idx in candidates : score = jaccard ( query , intervals to [ idx ] ) if score > best score : best score , best idx = score , idx return best idx", "predictions": ["find the column overlaps overlaps overlaps ."], "references": ["find the best jaccard match from query to candidates"], "bleu": 0.18370727471078332, "rouge_l": 0.24448897795591182}
{"id": 737, "code": "def match intervals ( intervals from , intervals to , strict = True ) : start index = np . argsort ( intervals to [ : , 0 ] ) end index = np . argsort ( intervals to [ : , 1 ] ) start sorted = intervals to [ start index , 0 ] end sorted = intervals to [ end index , 1 ] search ends = np . searchsorted ( start sorted , intervals from [ : , 1 ] , side = 'right' ) search starts = np . searchsorted ( end sorted , intervals from [ : , 0 ] , side = 'left' ) output = np . empty ( len ( intervals from ) , dtype = numba . uint32 ) for i in range ( len ( intervals from ) ) : query = intervals from [ i ] after query = search ends [ i ] before query = search starts [ i ] candidates = set ( start index [ : after query ] ) & set ( end index [ before query : ] ) if len ( candidates ) > 0 : output [ i ] = match interval overlaps ( query , intervals to , candidates ) elif strict : raise Parameter Error else : dist before = np . inf dist after = np . inf if search starts [ i ] > 0 : dist before = query [ 0 ] - end sorted [ search starts [ i ] - 1 ] if search ends [ i ] + 1 < len ( intervals to ) : dist after = start sorted [ search ends [ i ] + 1 ] - query [ 1 ] if dist before < dist after : output [ i ] = end index [ search starts [ i ] - 1 ] else : output [ i ] = start index [ search ends [ i ] + 1 ] return output", "predictions": ["delete virtual virtual virtual virtual virtual virtual virtual virtual virtual virtual virtual virtual datasets ."], "references": ["numba - accelerated interval matching algorithm ."], "bleu": 0.08225964699966554, "rouge_l": 0.09728867623604465}
{"id": 738, "code": "def get files ( dir name , extensions ) : dir name = os . path . abspath ( os . path . expanduser ( dir name ) ) myfiles = set ( ) for sub ext in extensions : globstr = os . path . join ( dir name , '*' + os . path . extsep + sub ext ) myfiles |= set ( glob . glob ( globstr ) ) return myfiles", "predictions": ["delete all variable names in a directory . . . . . . ."], "references": ["helper function to get files in a single directory"], "bleu": 0.1250076305588977, "rouge_l": 0.271513353115727}
{"id": 739, "code": "def process arguments ( args ) : parser = argparse . Argument Parser ( description = 'Time stretching example' ) parser . add argument ( 'input file' , action = 'store' , help = 'path to the input file (wav, mp3, etc)' ) parser . add argument ( 'output file' , action = 'store' , help = 'path to the stretched output (wav)' ) parser . add argument ( '-s' , '--speed' , action = 'store' , type = float , default = 2.0 , required = False , help = 'speed' ) return vars ( parser . parse args ( args ) )", "predictions": ["tail the arguments arguments"], "references": ["argparse function to get the program parameters"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 740, "code": "def beat local score ( onset envelope , period ) : window = np . exp ( - 0.5 * ( np . arange ( - period , period + 1 ) * 32.0 / period ) ** 2 ) return scipy . signal . convolve ( normalize onsets ( onset envelope ) , window , 'same' )", "predictions": ["head tail tail for n - th and n - n signal"], "references": ["construct the local score for an onset envlope and given period"], "bleu": 0.11498759556447223, "rouge_l": 0.17528735632183906}
{"id": 741, "code": "def beat track dp ( localscore , period , tightness ) : backlink = np . zeros like ( localscore , dtype = int ) cumscore = np . zeros like ( localscore ) window = np . arange ( - 2 * period , - np . round ( period / 2 ) + 1 , dtype = int ) if tightness <= 0 : raise Parameter Error ( 'tightness must be strictly positive' ) txwt = - tightness * ( np . log ( - window / period ) ** 2 ) first beat = True for i , score i in enumerate ( localscore ) : z pad = np . maximum ( 0 , min ( - window [ 0 ] , len ( window ) ) ) candidates = txwt . copy ( ) candidates [ z pad : ] = candidates [ z pad : ] + cumscore [ window [ z pad : ] ] beat location = np . argmax ( candidates ) cumscore [ i ] = score i + candidates [ beat location ] if first beat and score i < 0.01 * localscore . max ( ) : backlink [ i ] = - 1 else : backlink [ i ] = window [ beat location ] first beat = False window = window + 1 return backlink , cumscore", "predictions": ["set the row of the current current value"], "references": ["core dynamic program for beat tracking"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 742, "code": "def last beat ( cumscore ) : maxes = util . localmax ( cumscore ) med score = np . median ( cumscore [ np . argwhere ( maxes ) ] ) return np . argwhere ( ( cumscore * maxes * 2 > med score ) ) . max ( )", "predictions": ["selection of the selection of the selection"], "references": ["get the last beat from the cumulative score array"], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 743, "code": "def conv3x3 ( in planes , out planes , dilation = 1 ) : return nn . Conv2d ( in planes , out planes , kernel size = 3 , padding = dilation , dilation = dilation )", "predictions": ["returns the logger with the self . self . self . self . self . self . self ."], "references": ["3x3 convolution with padding"], "bleu": 0.06439931429457924, "rouge_l": 0.09854604200323101}
{"id": 744, "code": "def average ( self , n = 0 ) : assert n >= 0 for key in self . val history : values = np . array ( self . val history [ key ] [ - n : ] ) nums = np . array ( self . n history [ key ] [ - n : ] ) avg = np . sum ( values * nums ) / np . sum ( nums ) self . output [ key ] = avg self . ready = True", "predictions": ["selection selection of the current value"], "references": ["average latest n values or all values"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 745, "code": "def scatter ( input , devices , streams = None ) : if streams is None : streams = [ None ] * len ( devices ) if isinstance ( input , list ) : chunk size = ( len ( input ) - 1 ) // len ( devices ) + 1 outputs = [ scatter ( input [ i ] , [ devices [ i // chunk size ] ] , [ streams [ i // chunk size ] ] ) for i in range ( len ( input ) ) ] return outputs elif isinstance ( input , torch . Tensor ) : output = input . contiguous ( ) stream = streams [ 0 ] if output . numel ( ) > 0 else None with torch . cuda . device ( devices [ 0 ] ) , torch . cuda . stream ( stream ) : output = output . cuda ( devices [ 0 ] , non blocking = True ) return output else : raise Exception ( 'Unknown type {}.' . format ( type ( input ) ) )", "predictions": ["selection else 0 - 1 . 0 is a list of create else none"], "references": ["scatters tensor across multiple gpus ."], "bleu": 0.08839374326825923, "rouge_l": 0.10777385159010601}
{"id": 746, "code": "def start ( self ) : if not self . is running : self . t start = time ( ) self . is running = True self . t last = time ( )", "predictions": ["find the timer ."], "references": ["start the timer ."], "bleu": 0.668740304976422, "rouge_l": 0.75}
{"id": 747, "code": "def scatter kwargs ( inputs , kwargs , target gpus , dim = 0 ) : inputs = scatter ( inputs , target gpus , dim ) if inputs else [ ] kwargs = scatter ( kwargs , target gpus , dim ) if kwargs else [ ] if len ( inputs ) < len ( kwargs ) : inputs . extend ( [ ( ) for in range ( len ( kwargs ) - len ( inputs ) ) ] ) elif len ( kwargs ) < len ( inputs ) : kwargs . extend ( [ { } for in range ( len ( inputs ) - len ( kwargs ) ) ] ) inputs = tuple ( inputs ) kwargs = tuple ( kwargs ) return inputs , kwargs", "predictions": ["root nodes with nodes and nodes"], "references": ["scatter with support for kwargs dictionary"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 748, "code": "async def json ( self , * , encoding : str = None , loads : JSON Decoder = DEFAULT JSON DECODER , content type : Optional [ str ] = 'application/json' ) -> Any : return await self . aws json ( encoding = encoding , loads = loads , content type = content type )", "predictions": ["decodes an aws instance into a json instance ."], "references": ["read and decodes json response ."], "bleu": 0.16784459625186196, "rouge_l": 0.4149659863945578}
{"id": 749, "code": "async def text ( self , * , encoding : Optional [ str ] = None , errors : str = 'strict' ) -> str : return await self . aws text ( encoding = encoding , errors = errors )", "predictions": ["vmax text text ."], "references": ["read response payload and decode ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 750, "code": "async def handle callback ( self , aws callback : typing . Coroutine , response ) : callback result = None try : callback result = await aws callback except Nothing Matched Error as e : self . logger . error ( f'<Item: {str(e).lower()}>' ) except Exception as e : self . logger . error ( f'<Callback[{aws callback. name }]: {e}' ) return callback result , response", "predictions": ["handle self . and process the result"], "references": ["process coroutine callback function"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 751, "code": "async def multiple request ( self , urls , is gather = False , * * kwargs ) : if is gather : resp results = await asyncio . gather ( * [ self . handle request ( self . request ( url = url , * * kwargs ) ) for url in urls ] , return exceptions = True ) for index , task result in enumerate ( resp results ) : if not isinstance ( task result , Runtime Error ) and task result : , response = task result response . index = index yield response else : for index , url in enumerate ( urls ) : , response = await self . handle request ( self . request ( url = url , * * kwargs ) ) response . index = index yield response", "predictions": ["generator for all multiple . ."], "references": ["for crawling multiple urls"], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 752, "code": "def request ( self , url : str , method : str = 'GET' , * , callback = None , encoding : typing . Optional [ str ] = None , headers : dict = None , metadata : dict = None , request config : dict = None , request session = None , * * kwargs ) : headers = headers or { } metadata = metadata or { } request config = request config or { } request session = request session or self . request session headers . update ( self . headers . copy ( ) ) request config . update ( self . request config . copy ( ) ) kwargs . update ( self . kwargs . copy ( ) ) return Request ( url = url , method = method , callback = callback , encoding = encoding , headers = headers , metadata = metadata , request config = request config , request session = request session , * * kwargs )", "predictions": ["make a as rest api as a as rest api dict"], "references": ["init a request class for crawling html"], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 753, "code": "async def start master ( self ) : for url in self . start urls : request ins = self . request ( url = url , callback = self . parse , metadata = self . metadata ) self . request queue . put nowait ( self . handle request ( request ins ) ) workers = [ asyncio . ensure future ( self . start worker ( ) ) for i in range ( self . worker numbers ) ] for worker in workers : self . logger . info ( f\"Worker started: {id(worker)}\" ) await self . request queue . join ( ) if not self . is async start : await self . stop ( SIGINT ) else : await self . cancel tasks ( )", "predictions": ["versions of the master workers . ."], "references": ["actually start crawling ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 754, "code": "def normalize task v2 ( task ) : result = dict ( ) mod arg parser = Module Args Parser ( task ) try : action , arguments , result [ 'delegate to' ] = mod arg parser . parse ( ) except Ansible Parser Error as e : try : task info = \"%s:%s\" % ( task [ FILENAME KEY ] , task [ LINE NUMBER KEY ] ) del task [ FILENAME KEY ] del task [ LINE NUMBER KEY ] except Key Error : task info = \"Unknown\" try : import pprint pp = pprint . Pretty Printer ( indent = 2 ) task pprint = pp . pformat ( task ) except Import Error : task pprint = task raise System Exit ( \"Couldn't parse task at %s (%s)\\n%s\" % ( task info , e . message , task pprint ) ) if ' uses shell' in arguments : action = 'shell' del ( arguments [ ' uses shell' ] ) for ( k , v ) in list ( task . items ( ) ) : if k in ( 'action' , 'local action' , 'args' , 'delegate to' ) or k == action : continue else : result [ k ] = v result [ 'action' ] = dict ( ansible module = action ) if ' raw params' in arguments : result [ 'action' ] [ ' ansible arguments ' ] = arguments [ ' raw params' ] . split ( ' ' ) del ( arguments [ ' raw params' ] ) else : result [ 'action' ] [ ' ansible arguments ' ] = list ( ) if 'argv' in arguments and not result [ 'action' ] [ ' ansible arguments ' ] : result [ 'action' ] [ ' ansible arguments ' ] = arguments [ 'argv' ] del ( arguments [ 'argv' ] ) result [ 'action' ] . update ( arguments ) return result", "predictions": ["adjust tuning v2 v2"], "references": ["ensures tasks have an action key and strings are converted to python objects"], "bleu": 0.03184506239916981, "rouge_l": 0.0}
{"id": 755, "code": "def wheel dist name ( self ) : return '-' . join ( ( safer name ( self . distribution . get name ( ) ) , safer version ( self . distribution . get version ( ) ) ) )", "predictions": ["n - wheel fft fft"], "references": ["return distribution full name with - replaced with _"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 756, "code": "def get archive basename ( self ) : impl tag , abi tag , plat tag = self . get tag ( ) archive basename = \"%s-%s-%s-%s\" % ( self . wheel dist name , impl tag , abi tag , plat tag ) return archive basename", "predictions": ["return basename basename min tag min min tag min min min min min min min min min tag"], "references": ["return archive name without extension"], "bleu": 0.06809398432036522, "rouge_l": 0.09682539682539681}
{"id": 757, "code": "def add requirements ( self , metadata path ) : additional = list ( self . setupcfg requirements ( ) ) if not additional : return pkg info = read pkg info ( metadata path ) if 'Provides-Extra' in pkg info or 'Requires-Dist' in pkg info : warnings . warn ( 'setup.cfg requirements overwrite values from setup.py' ) del pkg info [ 'Provides-Extra' ] del pkg info [ 'Requires-Dist' ] for k , v in additional : pkg info [ k ] = v write pkg info ( metadata path , pkg info )", "predictions": ["add response response ."], "references": ["add additional requirements from setup . cfg to file metadata_path"], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 758, "code": "def telemetry client ( self , value : Bot Telemetry Client ) -> None : if value is None : self . telemetry client = Null Telemetry Client ( ) else : self . telemetry client = value", "predictions": ["set private early early downsample = value ."], "references": ["sets the telemetry client for logging events ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 759, "code": "def create db and container ( self ) : db id = self . config . database container name = self . config . container self . db = self . get or create database ( self . client , db id ) self . container = self . get or create container ( self . client , container name )", "predictions": ["early the downsample and and"], "references": ["call the get or create methods ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 760, "code": "def get step name ( self , index : int ) -> str : step name = self . steps [ index ] . qualname if not step name or \">\" in step name : step name = f\"Step{index + 1}of{len(self. steps)}\" return step name", "predictions": ["an axes name name name"], "references": ["give the waterfall step a unique name"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 761, "code": "def c if ( self , classical , val ) : if not isinstance ( classical , Classical Register ) : raise Qiskit Error ( \"c if must be used with a classical register\" ) if val < 0 : raise Qiskit Error ( \"control value should be non-negative\" ) self . control = ( classical , val ) return self", "predictions": ["convert a string to a scale . instance ."], "references": ["add classical control on register classical and value val ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 762, "code": "def qasmif ( self , string ) : if self . control is None : return string return \"if(%s==%d) \" % ( self . control [ 0 ] . name , self . control [ 1 ] ) + string", "predictions": ["return a n - gram n - gram n - gram n - 1 22050 22050"], "references": ["print an if statement if needed ."], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 763, "code": "def u base ( self , theta , phi , lam , q ) : return self . append ( U Base ( theta , phi , lam ) , [ q ] , [ ] )", "predictions": ["default mel for mel ."], "references": ["apply u to q ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 764, "code": "def to matrix ( self ) : theta , phi , lam = self . params return numpy . array ( [ [ numpy . cos ( theta / 2 ) , - numpy . exp ( 1j * lam ) * numpy . sin ( theta / 2 ) ] , [ numpy . exp ( 1j * phi ) * numpy . sin ( theta / 2 ) , numpy . exp ( 1j * ( phi + lam ) ) * numpy . cos ( theta / 2 ) ] ] , dtype = complex )", "predictions": ["convert this vector to a matrix matrix ."], "references": ["return a numpy . array for the u3 gate ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 765, "code": "def exp fit fun ( x , a , tau , c ) : return a * np . exp ( - x / tau ) + c", "predictions": ["chroma chroma op ."], "references": ["function used to fit the exponential decay ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 766, "code": "def osc fit fun ( x , a , tau , f , phi , c ) : return a * np . exp ( - x / tau ) * np . cos ( 2 * np . pi * f * x + phi ) + c", "predictions": ["time - fun product for x"], "references": ["function used to fit the decay cosine ."], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 767, "code": "def trim ( image ) : background = PIL . Image . new ( image . mode , image . size , image . getpixel ( ( 0 , 0 ) ) ) diff = PIL . Image Chops . difference ( image , background ) diff = PIL . Image Chops . add ( diff , diff , 2.0 , - 100 ) bbox = diff . getbbox ( ) if bbox : image = image . crop ( bbox ) return image", "predictions": ["window an fill fill in the fill fill the fill fill the fill fill the fill in the fill"], "references": ["trim a pil image and remove white space ."], "bleu": 0.05415315253510895, "rouge_l": 0.0}
{"id": 768, "code": "def get gate span ( qregs , instruction ) : min index = len ( qregs ) max index = 0 for qreg in instruction . qargs : index = qregs . index ( qreg ) if index < min index : min index = index if index > max index : max index = index if instruction . cargs : return qregs [ min index : ] return qregs [ min index : max index + 1 ]", "predictions": ["return the span of the gate in the instruction"], "references": ["get the list of qubits drawing this gate would cover"], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 769, "code": "def is cptp ( self , atol = None , rtol = None ) : if self . data [ 1 ] is not None : return False if atol is None : atol = self . atol if rtol is None : rtol = self . rtol accum = 0j for op in self . data [ 0 ] : accum += np . dot ( np . transpose ( np . conj ( op ) ) , op ) return is identity matrix ( accum , rtol = rtol , atol = atol )", "predictions": ["return true if the identity is cptp false otherwise ."], "references": ["return true if completely - positive trace - preserving ."], "bleu": 0.24808415001701817, "rouge_l": 0.4}
{"id": 770, "code": "def conjugate ( self ) : kraus l , kraus r = self . data kraus l = [ k . conj ( ) for k in kraus l ] if kraus r is not None : kraus r = [ k . conj ( ) for k in kraus r ] return Kraus ( ( kraus l , kraus r ) , self . input dims ( ) , self . output dims ( ) )", "predictions": ["conjugate the input data ."], "references": ["return the conjugate of the quantumchannel ."], "bleu": 0.21763141204756337, "rouge_l": 0.48541114058355433}
{"id": 771, "code": "def transpose ( self ) : kraus l , kraus r = self . data kraus l = [ k . T for k in kraus l ] if kraus r is not None : kraus r = [ k . T for k in kraus r ] return Kraus ( ( kraus l , kraus r ) , input dims = self . output dims ( ) , output dims = self . input dims ( ) )", "predictions": ["transpose the transpose of the data ."], "references": ["return the transpose of the quantumchannel ."], "bleu": 0.5410822690539396, "rouge_l": 0.7142857142857143}
{"id": 772, "code": "def real ( self , nested scope = None ) : operation = self . children [ 0 ] . operation ( ) lhs = self . children [ 1 ] . real ( nested scope ) rhs = self . children [ 2 ] . real ( nested scope ) return operation ( lhs , rhs )", "predictions": ["return real operation of this operation ."], "references": ["return the correspond floating point number ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 773, "code": "def sym ( self , nested scope = None ) : operation = self . children [ 0 ] . operation ( ) lhs = self . children [ 1 ] . sym ( nested scope ) rhs = self . children [ 2 ] . sym ( nested scope ) return operation ( lhs , rhs )", "predictions": ["create a operation instance of the operation ."], "references": ["return the correspond symbolic number ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 774, "code": "def process custom unitary ( self , node ) : name = node . name if node . arguments is not None : args = self . process node ( node . arguments ) else : args = [ ] bits = [ self . process bit id ( node element ) for node element in node . bitlist . children ] if name in self . gates : gargs = self . gates [ name ] [ \"args\" ] gbits = self . gates [ name ] [ \"bits\" ] maxidx = max ( map ( len , bits ) ) for idx in range ( maxidx ) : self . arg stack . append ( { gargs [ j ] : args [ j ] for j in range ( len ( gargs ) ) } ) element = [ idx * x for x in [ len ( bits [ j ] ) > 1 for j in range ( len ( bits ) ) ] ] self . bit stack . append ( { gbits [ j ] : bits [ j ] [ element [ j ] ] for j in range ( len ( gbits ) ) } ) self . create dag op ( name , [ self . arg stack [ - 1 ] [ s ] . sym ( ) for s in gargs ] , [ self . bit stack [ - 1 ] [ s ] for s in gbits ] ) self . arg stack . pop ( ) self . bit stack . pop ( ) else : raise Qiskit Error ( \"internal error undefined gate:\" , \"line=%s\" % node . line , \"file=%s\" % node . file )", "predictions": ["process custom unitary unitary"], "references": ["process a custom unitary node ."], "bleu": 0.3081980909598119, "rouge_l": 0.5791139240506329}
{"id": 775, "code": "def process cnot ( self , node ) : id0 = self . process bit id ( node . children [ 0 ] ) id1 = self . process bit id ( node . children [ 1 ] ) if not ( len ( id0 ) == len ( id1 ) or len ( id0 ) == 1 or len ( id1 ) == 1 ) : raise Qiskit Error ( \"internal error: qreg size mismatch\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) maxidx = max ( [ len ( id0 ) , len ( id1 ) ] ) for idx in range ( maxidx ) : if len ( id0 ) > 1 and len ( id1 ) > 1 : self . dag . apply operation back ( CX Base ( ) , [ id0 [ idx ] , id1 [ idx ] ] , [ ] , self . condition ) elif len ( id0 ) > 1 : self . dag . apply operation back ( CX Base ( ) , [ id0 [ idx ] , id1 [ 0 ] ] , [ ] , self . condition ) else : self . dag . apply operation back ( CX Base ( ) , [ id0 [ 0 ] , id1 [ idx ] ] , [ ] , self . condition )", "predictions": ["process cnot nodes ."], "references": ["process a cnot gate node ."], "bleu": 0.25916266987614406, "rouge_l": 0.5791139240506329}
{"id": 776, "code": "def process measure ( self , node ) : id0 = self . process bit id ( node . children [ 0 ] ) id1 = self . process bit id ( node . children [ 1 ] ) if len ( id0 ) != len ( id1 ) : raise Qiskit Error ( \"internal error: reg size mismatch\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) for idx , idy in zip ( id0 , id1 ) : self . dag . apply operation back ( Measure ( ) , [ idx ] , [ idy ] , self . condition )", "predictions": ["apply measure of measure nodes in place"], "references": ["process a measurement node ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 777, "code": "def process if ( self , node ) : creg name = node . children [ 0 ] . name creg = self . dag . cregs [ creg name ] cval = node . children [ 1 ] . value self . condition = ( creg , cval ) self . process node ( node . children [ 2 ] ) self . condition = None", "predictions": ["process the creg if it is not defined ."], "references": ["process an if node ."], "bleu": 0.16784459625186196, "rouge_l": 0.4518518518518518}
{"id": 778, "code": "def qasm ( self , prec = 15 ) : return \"measure \" + self . children [ 0 ] . qasm ( prec ) + \" -> \" + self . children [ 1 ] . qasm ( prec ) + \";\"", "predictions": ["return the qasm for the receiver ."], "references": ["return the corresponding openqasm string ."], "bleu": 0.2626909894424158, "rouge_l": 0.4680306905370844}
{"id": 779, "code": "def to string ( self , indent ) : ind = indent * ' ' print ( ind , 'indexed id' , self . name , self . index )", "predictions": ["print the string representation of this element ."], "references": ["print with indent ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 780, "code": "def validate ( instance ) : try : = instance . schema . validate ( instance . to dict ( ) ) except Validation Error as ex : raise Model Validation Error ( ex . messages , ex . field names , ex . fields , ex . data , * * ex . kwargs )", "predictions": ["validate an instance of an instance of an instance"], "references": ["validate the internal representation of the instance ."], "bleu": 0.16784459625186196, "rouge_l": 0.35672514619883033}
{"id": 781, "code": "def validate after init ( init method ) : @ wraps ( init method ) def decorated ( self , * * kwargs ) : try : = self . shallow schema . validate ( kwargs ) except Validation Error as ex : raise Model Validation Error ( ex . messages , ex . field names , ex . fields , ex . data , * * ex . kwargs ) from None init method ( self , * * kwargs ) return decorated", "predictions": ["check if the method is valid ."], "references": ["add validation after instantiation ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 782, "code": "def qft ( circ , q , n ) : for j in range ( n ) : for k in range ( j ) : circ . cu1 ( math . pi / float ( 2 ** ( j - k ) ) , q [ j ] , q [ k ] ) circ . h ( q [ j ] )", "predictions": ["draw n times between q and q"], "references": ["n - qubit qft on q in circ ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 783, "code": "def random unitary matrix ( dim , seed = None ) : warnings . warn ( 'The random unitary matrix() function in qiskit.tools.qi has been ' 'deprecated and will be removed in the future. Instead use ' 'the function in qiskit.quantum info.random' , Deprecation Warning ) return random . random unitary ( dim , seed ) . data", "predictions": ["returns a random unitary matrix ."], "references": ["deprecated in 0 . 8 +"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 784, "code": "def random density matrix ( length , rank = None , method = 'Hilbert-Schmidt' , seed = None ) : warnings . warn ( 'The random density matrix() function in qiskit.tools.qi has been ' 'deprecated and will be removed in the future. Instead use ' 'the function in qiskit.quantum info.random' , Deprecation Warning ) return random . random density matrix ( length , rank , method , seed )", "predictions": ["returns a random density matrix ."], "references": ["deprecated in 0 . 8 +"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 785, "code": "def u3 ( self , theta , phi , lam , q ) : return self . append ( U3Gate ( theta , phi , lam ) , [ q ] , [ ] )", "predictions": ["compute the u3 - th u3 of the quaternion ."], "references": ["apply u3 to q ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 786, "code": "def check type ( self , value , attr , data ) : root value = super ( Instruction Parameter , self ) . check type ( value , attr , data ) if is collection ( value ) : = [ super ( Instruction Parameter , self ) . check type ( item , attr , data ) for item in value ] return root value", "predictions": ["check that the value is of the type and type ."], "references": ["customize check_type for handling containers ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 787, "code": "def check range ( self , j ) : if isinstance ( j , int ) : if j < 0 or j >= self . size : raise Qiskit Index Error ( \"register index out of range\" ) elif isinstance ( j , slice ) : if j . start < 0 or j . stop >= self . size or ( j . step is not None and j . step <= 0 ) : raise Qiskit Index Error ( \"register index slice out of range\" )", "predictions": ["check if input is valid"], "references": ["check that j is a valid index into self ."], "bleu": 0.11943865131127647, "rouge_l": 0.37731958762886597}
{"id": 788, "code": "def to string ( self , indent ) : ind = indent * ' ' if self . root : print ( ind , self . type , '---' , self . root ) else : print ( ind , self . type ) indent = indent + 3 ind = indent * ' ' for children in self . children : if children is None : print ( \"OOPS! type of parent is\" , type ( self ) ) print ( self . children ) if isinstance ( children , str ) : print ( ind , children ) elif isinstance ( children , int ) : print ( ind , str ( children ) ) elif isinstance ( children , float ) : print ( ind , str ( children ) ) else : children . to string ( indent )", "predictions": ["print the tree representation of this node ."], "references": ["print with indent ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 789, "code": "def is square matrix ( mat ) : mat = np . array ( mat ) if mat . ndim != 2 : return False shape = mat . shape return shape [ 0 ] == shape [ 1 ]", "predictions": ["return true if mat is a square matrix ."], "references": ["test if an array is a square matrix ."], "bleu": 0.537284965911771, "rouge_l": 0.6666666666666666}
{"id": 790, "code": "def is diagonal matrix ( mat , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT mat = np . array ( mat ) if mat . ndim != 2 : return False return np . allclose ( mat , np . diag ( np . diagonal ( mat ) ) , rtol = rtol , atol = atol )", "predictions": ["return true if mat is a diagonal matrix ."], "references": ["test if an array is a diagonal matrix"], "bleu": 0.4111336169005197, "rouge_l": 0.594541910331384}
{"id": 791, "code": "def is symmetric matrix ( op , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT mat = np . array ( op ) if mat . ndim != 2 : return False return np . allclose ( mat , mat . T , rtol = rtol , atol = atol )", "predictions": ["return true if op is a symmetric matrix ."], "references": ["test if an array is a symmetrix matrix"], "bleu": 0.21105340631872635, "rouge_l": 0.4756335282651072}
{"id": 792, "code": "def is hermitian matrix ( mat , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT mat = np . array ( mat ) if mat . ndim != 2 : return False return np . allclose ( mat , np . conj ( mat . T ) , rtol = rtol , atol = atol )", "predictions": ["return true if the hermitian is a hermitian matrix ."], "references": ["test if an array is a hermitian matrix"], "bleu": 0.3672056269893592, "rouge_l": 0.5669144981412639}
{"id": 793, "code": "def is positive semidefinite matrix ( mat , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT if not is hermitian matrix ( mat , rtol = rtol , atol = atol ) : return False vals = np . linalg . eigvalsh ( mat ) for v in vals : if v < - atol : return False return True", "predictions": ["return true if mat is a positive positive matrix ."], "references": ["test if a matrix is positive semidefinite"], "bleu": 0.16590387014219712, "rouge_l": 0.36454183266932266}
{"id": 794, "code": "def is identity matrix ( mat , ignore phase = False , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT mat = np . array ( mat ) if mat . ndim != 2 : return False if ignore phase : theta = np . angle ( mat [ 0 , 0 ] ) mat = np . exp ( - 1j * theta ) * mat iden = np . eye ( len ( mat ) ) return np . allclose ( mat , iden , rtol = rtol , atol = atol )", "predictions": ["return true if the identity is a identity matrix ."], "references": ["test if an array is an identity matrix ."], "bleu": 0.25965358893403384, "rouge_l": 0.5313588850174217}
{"id": 795, "code": "def is unitary matrix ( mat , rtol = RTOL DEFAULT , atol = ATOL DEFAULT ) : if atol is None : atol = ATOL DEFAULT if rtol is None : rtol = RTOL DEFAULT mat = np . array ( mat ) mat = np . conj ( mat . T ) . dot ( mat ) return is identity matrix ( mat , ignore phase = False , rtol = rtol , atol = atol )", "predictions": ["return true if the matrix is a unitary matrix ."], "references": ["test if an array is a unitary matrix ."], "bleu": 0.47987820666906633, "rouge_l": 0.6376306620209059}
{"id": 796, "code": "def run ( self , dag ) : swaps = dag . op nodes ( Swap Gate ) for swap in swaps : final successor = [ ] for successor in dag . successors ( swap ) : final successor . append ( successor . type == 'out' or ( successor . type == 'op' and successor . op . name == 'measure' ) ) if all ( final successor ) : swap qargs = swap . qargs measure layer = DAG Circuit ( ) for qreg in dag . qregs . values ( ) : measure layer . add qreg ( qreg ) for creg in dag . cregs . values ( ) : measure layer . add creg ( creg ) for successor in dag . successors ( swap ) : if successor . type == 'op' and successor . op . name == 'measure' : dag . remove op node ( successor ) old measure qarg = successor . qargs [ 0 ] new measure qarg = swap qargs [ swap qargs . index ( old measure qarg ) - 1 ] measure layer . apply operation back ( Measure ( ) , [ new measure qarg ] , [ successor . cargs [ 0 ] ] ) dag . extend back ( measure layer ) dag . remove op node ( swap ) return dag", "predictions": ["run the dag layer ."], "references": ["return a new circuit that has been optimized ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 797, "code": "def to choi ( rep , data , input dim , output dim ) : if rep == 'Choi' : return data if rep == 'Operator' : return from operator ( 'Choi' , data , input dim , output dim ) if rep == 'Super Op' : return superop to choi ( data , input dim , output dim ) if rep == 'Kraus' : return kraus to choi ( data , input dim , output dim ) if rep == 'Chi' : return chi to choi ( data , input dim , output dim ) if rep == 'PTM' : data = ptm to superop ( data , input dim , output dim ) return superop to choi ( data , input dim , output dim ) if rep == 'Stinespring' : return stinespring to choi ( data , input dim , output dim ) raise Qiskit Error ( 'Invalid Quantum Channel {}' . format ( rep ) )", "predictions": ["convert data to choi ."], "references": ["transform a quantumchannel to the choi representation ."], "bleu": 0.1781815298791261, "rouge_l": 0.44309927360774815}
{"id": 798, "code": "def to superop ( rep , data , input dim , output dim ) : if rep == 'Super Op' : return data if rep == 'Operator' : return from operator ( 'Super Op' , data , input dim , output dim ) if rep == 'Choi' : return choi to superop ( data , input dim , output dim ) if rep == 'Kraus' : return kraus to superop ( data , input dim , output dim ) if rep == 'Chi' : data = chi to choi ( data , input dim , output dim ) return choi to superop ( data , input dim , output dim ) if rep == 'PTM' : return ptm to superop ( data , input dim , output dim ) if rep == 'Stinespring' : return stinespring to superop ( data , input dim , output dim ) raise Qiskit Error ( 'Invalid Quantum Channel {}' . format ( rep ) )", "predictions": ["convert a data to a superop"], "references": ["transform a quantumchannel to the superop representation ."], "bleu": 0.18822631894109965, "rouge_l": 0.4178082191780822}
{"id": 799, "code": "def to kraus ( rep , data , input dim , output dim ) : if rep == 'Kraus' : return data if rep == 'Stinespring' : return stinespring to kraus ( data , input dim , output dim ) if rep == 'Operator' : return from operator ( 'Kraus' , data , input dim , output dim ) if rep != 'Choi' : data = to choi ( rep , data , input dim , output dim ) return choi to kraus ( data , input dim , output dim )", "predictions": ["convert data to kraus s ."], "references": ["transform a quantumchannel to the kraus representation ."], "bleu": 0.18822631894109965, "rouge_l": 0.4178082191780822}
{"id": 800, "code": "def to chi ( rep , data , input dim , output dim ) : if rep == 'Chi' : return data check nqubit dim ( input dim , output dim ) if rep == 'Operator' : return from operator ( 'Chi' , data , input dim , output dim ) if rep != 'Choi' : data = to choi ( rep , data , input dim , output dim ) return choi to chi ( data , input dim , output dim )", "predictions": ["convert data to gate ."], "references": ["transform a quantumchannel to the chi representation ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 801, "code": "def to ptm ( rep , data , input dim , output dim ) : if rep == 'PTM' : return data check nqubit dim ( input dim , output dim ) if rep == 'Operator' : return from operator ( 'PTM' , data , input dim , output dim ) if rep != 'Super Op' : data = to superop ( rep , data , input dim , output dim ) return superop to ptm ( data , input dim , output dim )", "predictions": ["convert atol to ptm ."], "references": ["transform a quantumchannel to the ptm representation ."], "bleu": 0.1781815298791261, "rouge_l": 0.44309927360774815}
{"id": 802, "code": "def to stinespring ( rep , data , input dim , output dim ) : if rep == 'Stinespring' : return data if rep == 'Operator' : return from operator ( 'Stinespring' , data , input dim , output dim ) if rep != 'Kraus' : data = to kraus ( rep , data , input dim , output dim ) return kraus to stinespring ( data , input dim , output dim )", "predictions": ["convert kraus kraus kraus to stinespring ."], "references": ["transform a quantumchannel to the stinespring representation ."], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 803, "code": "def to operator ( rep , data , input dim , output dim ) : if rep == 'Operator' : return data if rep == 'Stinespring' : return stinespring to operator ( data , input dim , output dim ) if rep != 'Kraus' : data = to kraus ( rep , data , input dim , output dim ) return kraus to operator ( data , input dim , output dim )", "predictions": ["convert a single operator to an operator ."], "references": ["transform a quantumchannel to the operator representation ."], "bleu": 0.20164945583740668, "rouge_l": 0.5}
{"id": 804, "code": "def from operator ( rep , data , input dim , output dim ) : if rep == 'Operator' : return data if rep == 'Super Op' : return np . kron ( np . conj ( data ) , data ) if rep == 'Choi' : vec = np . ravel ( data , order = 'F' ) return np . outer ( vec , np . conj ( vec ) ) if rep == 'Kraus' : return ( [ data ] , None ) if rep == 'Stinespring' : return ( data , None ) if rep == 'Chi' : check nqubit dim ( input dim , output dim ) data = from operator ( 'Choi' , data , input dim , output dim ) return choi to chi ( data , input dim , output dim ) if rep == 'PTM' : check nqubit dim ( input dim , output dim ) data = from operator ( 'Super Op' , data , input dim , output dim ) return superop to ptm ( data , input dim , output dim ) raise Qiskit Error ( 'Invalid Quantum Channel {}' . format ( rep ) )", "predictions": ["create an 1 - d operator from the operator operator"], "references": ["transform operator representation to other representation ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 805, "code": "def stinespring to operator ( data , input dim , output dim ) : trace dim = data [ 0 ] . shape [ 0 ] // output dim if data [ 1 ] is not None or trace dim != 1 : raise Qiskit Error ( 'Channel cannot be converted to Operator representation' ) return data [ 0 ]", "predictions": ["convert nested self to self . self . self lhs"], "references": ["transform stinespring representation to operator representation ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 806, "code": "def superop to choi ( data , input dim , output dim ) : shape = ( output dim , output dim , input dim , input dim ) return reshuffle ( data , shape )", "predictions": ["convert process self . unitary to unitary ."], "references": ["transform superop representation to choi representation ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 807, "code": "def choi to superop ( data , input dim , output dim ) : shape = ( input dim , output dim , input dim , output dim ) return reshuffle ( data , shape )", "predictions": ["convert process data to superop ."], "references": ["transform choi to superop representation ."], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 808, "code": "def kraus to choi ( data , input dim , output dim ) : choi = 0 kraus l , kraus r = data if kraus r is None : for i in kraus l : vec = i . ravel ( order = 'F' ) choi += np . outer ( vec , vec . conj ( ) ) else : for i , j in zip ( kraus l , kraus r ) : choi += np . outer ( i . ravel ( order = 'F' ) , j . ravel ( order = 'F' ) . conj ( ) ) return choi", "predictions": ["convert process data to choi"], "references": ["transform kraus representation to choi representation ."], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 809, "code": "def choi to kraus ( data , input dim , output dim , atol = ATOL DEFAULT ) : if is hermitian matrix ( data , atol = atol ) : w , v = la . eigh ( data ) if len ( w [ w < - atol ] ) == 0 : kraus = [ ] for val , vec in zip ( w , v . T ) : if abs ( val ) > atol : k = np . sqrt ( val ) * vec . reshape ( ( output dim , input dim ) , order = 'F' ) kraus . append ( k ) if not kraus : kraus . append ( np . zeros ( ( output dim , input dim ) , dtype = complex ) ) return ( kraus , None ) mat u , svals , mat vh = la . svd ( data ) kraus l = [ ] kraus r = [ ] for val , vec l , vec r in zip ( svals , mat u . T , mat vh . conj ( ) ) : kraus l . append ( np . sqrt ( val ) * vec l . reshape ( ( output dim , input dim ) , order = 'F' ) ) kraus r . append ( np . sqrt ( val ) * vec r . reshape ( ( output dim , input dim ) , order = 'F' ) ) return ( kraus l , kraus r )", "predictions": ["convert process data to kraus"], "references": ["transform choi representation to kraus representation ."], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 810, "code": "def stinespring to kraus ( data , input dim , output dim ) : kraus pair = [ ] for stine in data : if stine is None : kraus pair . append ( None ) else : trace dim = stine . shape [ 0 ] // output dim iden = np . eye ( output dim ) kraus = [ ] for j in range ( trace dim ) : vec = np . zeros ( trace dim ) vec [ j ] = 1 kraus . append ( np . kron ( iden , vec [ None , : ] ) . dot ( stine ) ) kraus pair . append ( kraus ) return tuple ( kraus pair )", "predictions": ["convert a trace object to a tuple tuple"], "references": ["transform stinespring representation to kraus representation ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 811, "code": "def stinespring to choi ( data , input dim , output dim ) : trace dim = data [ 0 ] . shape [ 0 ] // output dim stine l = np . reshape ( data [ 0 ] , ( output dim , trace dim , input dim ) ) if data [ 1 ] is None : stine r = stine l else : stine r = np . reshape ( data [ 1 ] , ( output dim , trace dim , input dim ) ) return np . reshape ( np . einsum ( 'i Aj,k Al->jilk' , stine l , stine r . conj ( ) ) , 2 * [ input dim * output dim ] )", "predictions": ["convert to choi . choi"], "references": ["transform stinespring representation to choi representation ."], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 812, "code": "def kraus to stinespring ( data , input dim , output dim ) : stine pair = [ None , None ] for i , kraus in enumerate ( data ) : if kraus is not None : num kraus = len ( kraus ) stine = np . zeros ( ( output dim * num kraus , input dim ) , dtype = complex ) for j , mat in enumerate ( kraus ) : vec = np . zeros ( num kraus ) vec [ j ] = 1 stine += np . kron ( mat , vec [ : , None ] ) stine pair [ i ] = stine return tuple ( stine pair )", "predictions": ["convert validate data to instance"], "references": ["transform kraus representation to stinespring representation ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 813, "code": "def kraus to superop ( data , input dim , output dim ) : kraus l , kraus r = data superop = 0 if kraus r is None : for i in kraus l : superop += np . kron ( np . conj ( i ) , i ) else : for i , j in zip ( kraus l , kraus r ) : superop += np . kron ( np . conj ( j ) , i ) return superop", "predictions": ["convert validate to init"], "references": ["transform kraus representation to superop representation ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 814, "code": "def chi to choi ( data , input dim , output dim ) : num qubits = int ( np . log2 ( input dim ) ) return transform from pauli ( data , num qubits )", "predictions": ["convert chi to choi"], "references": ["transform chi representation to a choi representation ."], "bleu": 0.1571901051328651, "rouge_l": 0.47164948453608246}
{"id": 815, "code": "def choi to chi ( data , input dim , output dim ) : num qubits = int ( np . log2 ( input dim ) ) return transform to pauli ( data , num qubits )", "predictions": ["convert a random matrix to an matrix ."], "references": ["transform choi representation to the chi representation ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 816, "code": "def reravel ( mat1 , mat2 , shape1 , shape2 ) : left dims = shape1 [ : 2 ] + shape2 [ : 2 ] right dims = shape1 [ 2 : ] + shape2 [ 2 : ] tensor shape = left dims + right dims final shape = ( np . product ( left dims ) , np . product ( right dims ) ) data = np . kron ( mat1 , mat2 ) data = np . reshape ( np . transpose ( np . reshape ( data , tensor shape ) , ( 0 , 2 , 1 , 3 , 4 , 6 , 5 , 7 ) ) , final shape ) return data", "predictions": ["return - . algorithm"], "references": ["reravel two bipartite matrices ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 817, "code": "def transform from pauli ( data , num qubits ) : basis mat = np . array ( [ [ 1 , 0 , 0 , 1 ] , [ 0 , 1 , 1j , 0 ] , [ 0 , 1 , - 1j , 0 ] , [ 1 , 0j , 0 , - 1 ] ] , dtype = complex ) cob = basis mat for in range ( num qubits - 1 ) : dim = int ( np . sqrt ( len ( cob ) ) ) cob = np . reshape ( np . transpose ( np . reshape ( np . kron ( basis mat , cob ) , ( 2 , 2 , dim , dim , 4 , dim * dim ) ) , ( 0 , 2 , 1 , 3 , 4 , 5 ) ) , ( 4 * dim * dim , 4 * dim * dim ) ) return np . dot ( np . dot ( cob , data ) , cob . conj ( ) . T ) / 2 ** num qubits", "predictions": ["u3 the theta from from from from the self ."], "references": ["change of basis of bipartite matrix represenation ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 818, "code": "def check nqubit dim ( input dim , output dim ) : if input dim != output dim : raise Qiskit Error ( 'Not an n-qubit channel: input dim' + ' ({}) != output dim ({})' . format ( input dim , output dim ) ) num qubits = int ( np . log2 ( input dim ) ) if 2 ** num qubits != input dim : raise Qiskit Error ( 'Not an n-qubit channel: input dim != 2 ** n' )", "predictions": ["check that the input input is correct"], "references": ["return true if dims correspond to an n - qubit channel ."], "bleu": 0.07646493705380929, "rouge_l": 0.0}
{"id": 819, "code": "def hide tick lines and labels ( axis ) : for item in axis . get ticklines ( ) + axis . get ticklabels ( ) : item . set visible ( False )", "predictions": ["check for range lines self 0 0 0"], "references": ["set visible property of ticklines and ticklabels of an axis to false"], "bleu": 0.08179133792443427, "rouge_l": 0.0}
{"id": 820, "code": "def clear ( self ) : self . points = [ ] self . vectors = [ ] self . point style = [ ] self . annotations = [ ]", "predictions": ["clears the current ."], "references": ["resets bloch sphere data sets to empty ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 821, "code": "def render ( self , title = '' ) : if self . rendered : self . axes . clear ( ) self . rendered = True if not self . ext fig : self . fig = plt . figure ( figsize = self . figsize ) if not self . ext axes : self . axes = Axes3D ( self . fig , azim = self . view [ 0 ] , elev = self . view [ 1 ] ) if self . background : self . axes . clear ( ) self . axes . set xlim3d ( - 1.3 , 1.3 ) self . axes . set ylim3d ( - 1.3 , 1.3 ) self . axes . set zlim3d ( - 1.3 , 1.3 ) else : self . plot axes ( ) self . axes . set axis off ( ) self . axes . set xlim3d ( - 0.7 , 0.7 ) self . axes . set ylim3d ( - 0.7 , 0.7 ) self . axes . set zlim3d ( - 0.7 , 0.7 ) self . axes . grid ( False ) self . plot back ( ) self . plot points ( ) self . plot vectors ( ) self . plot front ( ) self . plot axes labels ( ) self . plot annotations ( ) self . axes . set title ( title , fontsize = self . font size , y = 1.08 )", "predictions": ["renders the html representation of the element"], "references": ["render the bloch sphere and its data sets in on given figure and axes ."], "bleu": 0.059237077985967744, "rouge_l": 0.08531468531468532}
{"id": 822, "code": "def plot front ( self ) : u angle = np . linspace ( - np . pi , 0 , 25 ) v angle = np . linspace ( 0 , np . pi , 25 ) x dir = np . outer ( np . cos ( u angle ) , np . sin ( v angle ) ) y dir = np . outer ( np . sin ( u angle ) , np . sin ( v angle ) ) z dir = np . outer ( np . ones ( u angle . shape [ 0 ] ) , np . cos ( v angle ) ) self . axes . plot surface ( x dir , y dir , z dir , rstride = 2 , cstride = 2 , color = self . sphere color , linewidth = 0 , alpha = self . sphere alpha ) self . axes . plot wireframe ( x dir , y dir , z dir , rstride = 5 , cstride = 5 , color = self . frame color , alpha = self . frame alpha ) self . axes . plot ( 1.0 * np . cos ( u angle ) , 1.0 * np . sin ( u angle ) , zs = 0 , zdir = 'z' , lw = self . frame width , color = self . frame color ) self . axes . plot ( 1.0 * np . cos ( u angle ) , 1.0 * np . sin ( u angle ) , zs = 0 , zdir = 'x' , lw = self . frame width , color = self . frame color )", "predictions": ["is the diagonal of the diagonal diagonal"], "references": ["front half of sphere"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 823, "code": "def show ( self , title = '' ) : self . render ( title = title ) if self . fig : plt . show ( self . fig )", "predictions": ["atol of the widget"], "references": ["display bloch sphere and corresponding data sets ."], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 824, "code": "def two qubit kak ( unitary matrix , verify gate sequence = False ) : warnings . warn ( \"two qubit kak function is now accessible under \" \"qiskit.quantum info.synthesis\" , Deprecation Warning ) return synthesis . two qubit kak ( unitary matrix )", "predictions": ["deprecated alias for is matrix matrix"], "references": ["deprecated after 0 . 8"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 825, "code": "def top ( self ) : ret = self . top format % self . top connect . center ( self . width , self . top pad ) if self . right fill : ret = ret . ljust ( self . right fill , self . top pad ) if self . left fill : ret = ret . rjust ( self . left fill , self . top pad ) ret = ret . center ( self . layer width , self . top bck ) return ret", "predictions": ["returns the is is only the is suitable for this vals . . ."], "references": ["constructs the top line of the element"], "bleu": 0.09782375748961449, "rouge_l": 0.2026578073089701}
{"id": 826, "code": "def mid ( self ) : ret = self . mid format % self . mid content . center ( self . width , self . mid padding ) if self . right fill : ret = ret . ljust ( self . right fill , self . mid padding ) if self . left fill : ret = ret . rjust ( self . left fill , self . mid padding ) ret = ret . center ( self . layer width , self . mid bck ) return ret", "predictions": ["is the is called when a is updated ."], "references": ["constructs the middle line of the element"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 827, "code": "def bot ( self ) : ret = self . bot format % self . bot connect . center ( self . width , self . bot pad ) if self . right fill : ret = ret . ljust ( self . right fill , self . bot pad ) if self . left fill : ret = ret . rjust ( self . left fill , self . bot pad ) ret = ret . center ( self . layer width , self . bot bck ) return ret", "predictions": ["the is the is a string that represents the is created ."], "references": ["constructs the bottom line of the element"], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 828, "code": "def length ( self ) : return max ( len ( self . top ) , len ( self . mid ) , len ( self . bot ) )", "predictions": ["maximum run of the maximum number of rows ."], "references": ["returns the length of the element including the box around ."], "bleu": 0.15982877755018768, "rouge_l": 0.2946859903381642}
{"id": 829, "code": "def label for box ( instruction ) : label = instruction . name . capitalize ( ) params = Text Drawing . params for label ( instruction ) if params : label += \"(%s)\" % ',' . join ( params ) return label", "predictions": ["returns a to to a to a to a string"], "references": ["creates the label for a box ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 830, "code": "def latex ( self , prec = 15 , nested scope = None ) : if not nested scope : return \"\\textrm{\" + self . name + \"}\" else : if self . name not in nested scope [ - 1 ] : raise Node Exception ( \"Expected local parameter name: \" , \"name=%s, \" % self . name , \"line=%s, \" % self . line , \"file=%s\" % self . file ) else : return nested scope [ - 1 ] [ self . name ] . latex ( prec , nested scope [ 0 : - 1 ] )", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["return the correspond math mode latex string ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 831, "code": "def sym ( self , nested scope = None ) : if not nested scope or self . name not in nested scope [ - 1 ] : raise Node Exception ( \"Expected local parameter name: \" , \"name=%s, line=%s, file=%s\" % ( self . name , self . line , self . file ) ) else : return nested scope [ - 1 ] [ self . name ] . sym ( nested scope [ 0 : - 1 ] )", "predictions": ["return the data in the nested . . ."], "references": ["return the correspond symbolic number ."], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 832, "code": "def real ( self , nested scope = None ) : if not nested scope or self . name not in nested scope [ - 1 ] : raise Node Exception ( \"Expected local parameter name: \" , \"name=%s, line=%s, file=%s\" % ( self . name , self . line , self . file ) ) else : return nested scope [ - 1 ] [ self . name ] . real ( nested scope [ 0 : - 1 ] )", "predictions": ["real scope in real scope ."], "references": ["return the correspond floating point number ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 833, "code": "def attach ( self , instruction , qargs , cargs ) : self . append ( instruction , qargs , cargs )", "predictions": ["attach an item to the list ."], "references": ["deprecated after 0 . 8"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 834, "code": "def check dups ( self , qubits ) : squbits = set ( qubits ) if len ( squbits ) != len ( qubits ) : raise Qiskit Error ( \"duplicate qubit arguments\" )", "predictions": ["check that this qubit is available"], "references": ["raise exception if list of qubits contains duplicates ."], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 835, "code": "def check qargs ( self , qargs ) : if not all ( isinstance ( i , tuple ) and isinstance ( i [ 0 ] , Quantum Register ) and isinstance ( i [ 1 ] , int ) for i in qargs ) : raise Qiskit Error ( \"qarg not (Quantum Register, int) tuple\" ) if not all ( self . has register ( i [ 0 ] ) for i in qargs ) : raise Qiskit Error ( \"register not in this circuit\" ) for qubit in qargs : qubit [ 0 ] . check range ( qubit [ 1 ] )", "predictions": ["check that qargs is valid"], "references": ["raise exception if a qarg is not in this circuit or bad format ."], "bleu": 0.04512859433163675, "rouge_l": 0.09697933227344992}
{"id": 836, "code": "def check cargs ( self , cargs ) : if not all ( isinstance ( i , tuple ) and isinstance ( i [ 0 ] , Classical Register ) and isinstance ( i [ 1 ] , int ) for i in cargs ) : raise Qiskit Error ( \"carg not (Classical Register, int) tuple\" ) if not all ( self . has register ( i [ 0 ] ) for i in cargs ) : raise Qiskit Error ( \"register not in this circuit\" ) for clbit in cargs : clbit [ 0 ] . check range ( clbit [ 1 ] )", "predictions": ["check if cargs is valid"], "references": ["raise exception if clbit is not in this circuit or bad format ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 837, "code": "def check compatible regs ( self , rhs ) : list1 = self . qregs + self . cregs list2 = rhs . qregs + rhs . cregs for element1 in list1 : for element2 in list2 : if element2 . name == element1 . name : if element1 != element2 : raise Qiskit Error ( \"circuits are not compatible\" )", "predictions": ["check if compatible regs are compatible with the same name"], "references": ["raise exception if the circuits are defined on incompatible registers"], "bleu": 0.14991106946711685, "rouge_l": 0.2}
{"id": 838, "code": "def qasm ( self ) : string temp = self . header + \"\\n\" string temp += self . extension lib + \"\\n\" for register in self . qregs : string temp += register . qasm ( ) + \"\\n\" for register in self . cregs : string temp += register . qasm ( ) + \"\\n\" for instruction , qargs , cargs in self . data : if instruction . name == 'measure' : qubit = qargs [ 0 ] clbit = cargs [ 0 ] string temp += \"%s %s[%d] -> %s[%d];\\n\" % ( instruction . qasm ( ) , qubit [ 0 ] . name , qubit [ 1 ] , clbit [ 0 ] . name , clbit [ 1 ] ) else : string temp += \"%s %s;\\n\" % ( instruction . qasm ( ) , \",\" . join ( [ \"%s[%d]\" % ( j [ 0 ] . name , j [ 1 ] ) for j in qargs + cargs ] ) ) return string temp", "predictions": ["return a string representing the string ."], "references": ["return openqasm string ."], "bleu": 0.2626909894424158, "rouge_l": 0.5736677115987461}
{"id": 839, "code": "def bind parameter ( self , parameter , value ) : for ( instr , param index ) in self . parameter table [ parameter ] : instr . params [ param index ] = value", "predictions": ["bind a parameter to its index ."], "references": ["assigns a parameter value to matching instructions in - place ."], "bleu": 0.15685718045401453, "rouge_l": 0.4273204903677758}
{"id": 840, "code": "def score step ( step ) : return len ( [ g for g in step [ 'gates mapped' ] if len ( g . qargs ) == 2 ] ) - 3 * step [ 'swaps added' ]", "predictions": ["return the score of a step in the step ."], "references": ["count the mapped two - qubit gates less the number of added swaps ."], "bleu": 0.10625354116793678, "rouge_l": 0.24270557029177717}
{"id": 841, "code": "def transform gate for layout ( gate , layout ) : mapped op node = deepcopy ( [ n for n in gate [ 'graph' ] . nodes ( ) if n . type == 'op' ] [ 0 ] ) device qreg = Quantum Register ( len ( layout . get physical bits ( ) ) , 'q' ) mapped qargs = [ ( device qreg , layout [ a ] ) for a in mapped op node . qargs ] mapped op node . qargs = mapped op node . op . qargs = mapped qargs mapped op node . pop ( 'name' ) return mapped op node", "predictions": ["transforms gate for gate layout"], "references": ["return op implementing a virtual gate on given layout ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 842, "code": "def swap ops from edge ( edge , layout ) : device qreg = Quantum Register ( len ( layout . get physical bits ( ) ) , 'q' ) qreg edge = [ ( device qreg , i ) for i in edge ] return [ DAG Node ( { 'op' : Swap Gate ( ) , 'qargs' : qreg edge , 'cargs' : [ ] , 'type' : 'op' } ) ]", "predictions": ["swap ops from edge to list of ops ."], "references": ["generate list of ops to implement a swap gate along a coupling edge ."], "bleu": 0.1792371224986277, "rouge_l": 0.3347050754458162}
{"id": 843, "code": "def physical qubits ( self ) : if self . qubit list is None : self . qubit list = sorted ( [ pqubit for pqubit in self . graph . nodes ] ) return self . qubit list", "predictions": ["the physical qubits of this physical ."], "references": ["returns a sorted list of physical_qubits"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 844, "code": "def cu1 ( self , theta , ctl , tgt ) : return self . append ( Cu1Gate ( theta ) , [ ctl , tgt ] , [ ] )", "predictions": ["add comparison to the image"], "references": ["apply cu1 from ctl to tgt with angle theta ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 845, "code": "def inverse ( self ) : for index , instruction in enumerate ( self . instructions ) : self . instructions [ index ] = instruction . inverse ( ) return self", "predictions": ["inverse of the inverse of this instruction ."], "references": ["invert all instructions ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 846, "code": "def q if ( self , * qregs ) : for gate in self . instructions : gate . q if ( * qregs ) return self", "predictions": ["return the q if any of the instructions is not instructions ."], "references": ["add controls to all instructions ."], "bleu": 0.1367440667823257, "rouge_l": 0.2364341085271318}
{"id": 847, "code": "def c if ( self , classical , val ) : for gate in self . instructions : gate . c if ( classical , val ) return self", "predictions": ["get the c representation of a given value ."], "references": ["add classical control register to all instructions ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 848, "code": "def initialize ( self , params , qubits ) : if isinstance ( qubits , Quantum Register ) : qubits = qubits [ : ] else : qubits = convert to bits ( [ qubits ] , [ qbit for qreg in self . qregs for qbit in qreg ] ) [ 0 ] return self . append ( Initialize ( params ) , qubits )", "predictions": ["stores the list of qubits"], "references": ["apply initialize to circuit ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 849, "code": "def is virtual ( value ) : return value is None or isinstance ( value , tuple ) and len ( value ) == 2 and isinstance ( value [ 0 ] , Register ) and isinstance ( value [ 1 ] , int )", "predictions": ["return true if value is a virtual virtual type ."], "references": ["checks if value has the format of a virtual qubit"], "bleu": 0.2086130724305753, "rouge_l": 0.4}
{"id": 850, "code": "def copy ( self ) : layout copy = type ( self ) ( ) layout copy . p2v = self . p2v . copy ( ) layout copy . v2p = self . v2p . copy ( ) return layout copy", "predictions": ["returns a copy of this instance ."], "references": ["returns a copy of a layout instance ."], "bleu": 0.5154486831107657, "rouge_l": 0.7904967602591793}
{"id": 851, "code": "def ccx ( self , ctl1 , ctl2 , tgt ) : return self . append ( Toffoli Gate ( ) , [ ctl1 , ctl2 , tgt ] , [ ] )", "predictions": ["append a ccx to a image"], "references": ["apply toffoli to from ctl1 and ctl2 to tgt ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 852, "code": "def u2 ( self , phi , lam , q ) : return self . append ( U2Gate ( phi , lam ) , [ q ] , [ ] )", "predictions": ["u2 expression to call this expression ."], "references": ["apply u2 to q ."], "bleu": 0.22089591134157885, "rouge_l": 0.5154929577464789}
{"id": 853, "code": "def to matrix ( self ) : isqrt2 = 1 / numpy . sqrt ( 2 ) phi , lam = self . params phi , lam = float ( phi ) , float ( lam ) return numpy . array ( [ [ isqrt2 , - numpy . exp ( 1j * lam ) * isqrt2 ] , [ numpy . exp ( 1j * phi ) * isqrt2 , numpy . exp ( 1j * ( phi + lam ) ) * isqrt2 ] ] , dtype = complex )", "predictions": ["return the matrix representation of this complex matrix ."], "references": ["return a numpy . array for the u3 gate ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 854, "code": "def is cptp ( self , atol = None , rtol = None ) : if atol is None : atol = self . atol if rtol is None : rtol = self . rtol if self . data [ 1 ] is not None : return False check = np . dot ( np . transpose ( np . conj ( self . data [ 0 ] ) ) , self . data [ 0 ] ) return is identity matrix ( check , rtol = self . rtol , atol = self . atol )", "predictions": ["return true if the identity is cptp false otherwise ."], "references": ["return true if completely - positive trace - preserving ."], "bleu": 0.24808415001701817, "rouge_l": 0.4}
{"id": 855, "code": "def conjugate ( self ) : stine l = np . conjugate ( self . data [ 0 ] ) stine r = None if self . data [ 1 ] is not None : stine r = np . conjugate ( self . data [ 1 ] ) return Stinespring ( ( stine l , stine r ) , self . input dims ( ) , self . output dims ( ) )", "predictions": ["conjugate of the vector"], "references": ["return the conjugate of the quantumchannel ."], "bleu": 0.3158905525406873, "rouge_l": 0.5198863636363635}
{"id": 856, "code": "def transpose ( self ) : din , dout = self . dim dtr = self . data [ 0 ] . shape [ 0 ] // dout stine = [ None , None ] for i , mat in enumerate ( self . data ) : if mat is not None : stine [ i ] = np . reshape ( np . transpose ( np . reshape ( mat , ( dout , dtr , din ) ) , ( 2 , 1 , 0 ) ) , ( din * dtr , dout ) ) return Stinespring ( tuple ( stine ) , input dims = self . output dims ( ) , output dims = self . input dims ( ) )", "predictions": ["transpose the transpose of the data ."], "references": ["return the transpose of the quantumchannel ."], "bleu": 0.5410822690539396, "rouge_l": 0.7142857142857143}
{"id": 857, "code": "def to operator ( self ) : from qiskit . quantum info . operators . operator import Operator return Operator ( self . to matrix ( ) )", "predictions": ["return this operator as a operator ."], "references": ["convert to operator object ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 858, "code": "def to instruction ( self ) : from qiskit . circuit import Quantum Circuit , Quantum Register from qiskit . extensions . standard import Id Gate , X Gate , Y Gate , Z Gate gates = { 'I' : Id Gate ( ) , 'X' : X Gate ( ) , 'Y' : Y Gate ( ) , 'Z' : Z Gate ( ) } label = self . to label ( ) n qubits = self . numberofqubits qreg = Quantum Register ( n qubits ) circuit = Quantum Circuit ( qreg , name = 'Pauli:{}' . format ( label ) ) for i , pauli in enumerate ( reversed ( label ) ) : circuit . append ( gates [ pauli ] , [ qreg [ i ] ] ) return circuit . to instruction ( )", "predictions": ["convert this circuit to a instruction ."], "references": ["convert to pauli circuit instruction ."], "bleu": 0.29071536848410967, "rouge_l": 0.6240409207161125}
{"id": 859, "code": "def validate initial statevector ( self ) : if self . initial statevector is None : return length = len ( self . initial statevector ) required dim = 2 ** self . number of qubits if length != required dim : raise Basic Aer Error ( 'initial statevector is incorrect length: ' + '{} != {}' . format ( length , required dim ) )", "predictions": ["validate that the initial statevector is valid ."], "references": ["validate an initial statevector"], "bleu": 0.22679164443904004, "rouge_l": 0.5319767441860466}
{"id": 860, "code": "def set options ( self , qobj config = None , backend options = None ) : self . initial statevector = self . DEFAULT OPTIONS [ \"initial statevector\" ] self . chop threshold = self . DEFAULT OPTIONS [ \"chop threshold\" ] if backend options is None : backend options = { } if 'initial statevector' in backend options : self . initial statevector = np . array ( backend options [ 'initial statevector' ] , dtype = complex ) elif hasattr ( qobj config , 'initial statevector' ) : self . initial statevector = np . array ( qobj config . initial statevector , dtype = complex ) if self . initial statevector is not None : norm = np . linalg . norm ( self . initial statevector ) if round ( norm , 12 ) != 1 : raise Basic Aer Error ( 'initial statevector is not normalized: ' + 'norm {} != 1' . format ( norm ) ) if 'chop threshold' in backend options : self . chop threshold = backend options [ 'chop threshold' ] elif hasattr ( qobj config , 'chop threshold' ) : self . chop threshold = qobj config . chop threshold", "predictions": ["set the options for the given backend ."], "references": ["set the backend options for all experiments in a qobj"], "bleu": 0.21632118787624222, "rouge_l": 0.43571428571428567}
{"id": 861, "code": "def initialize statevector ( self ) : if self . initial statevector is None : self . statevector = np . zeros ( 2 ** self . number of qubits , dtype = complex ) self . statevector [ 0 ] = 1 else : self . statevector = self . initial statevector . copy ( ) self . statevector = np . reshape ( self . statevector , self . number of qubits * [ 2 ] )", "predictions": ["initialize the statevector - statevector statevector"], "references": ["set the initial statevector for simulation"], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 862, "code": "def get statevector ( self ) : vec = np . reshape ( self . statevector , 2 ** self . number of qubits ) vec = np . stack ( [ vec . real , vec . imag ] , axis = 1 ) vec [ abs ( vec ) < self . chop threshold ] = 0.0 return vec", "predictions": ["return the statevector vector"], "references": ["return the current statevector in json result spec format"], "bleu": 0.14558246978804804, "rouge_l": 0.43160377358490565}
{"id": 863, "code": "def validate ( self , qobj ) : n qubits = qobj . config . n qubits max qubits = self . configuration ( ) . n qubits if n qubits > max qubits : raise Basic Aer Error ( 'Number of qubits {} ' . format ( n qubits ) + 'is greater than maximum ({}) ' . format ( max qubits ) + 'for \"{}\".' . format ( self . name ( ) ) ) for experiment in qobj . experiments : name = experiment . header . name if experiment . config . memory slots == 0 : logger . warning ( 'No classical registers in circuit \"%s\", ' 'counts will be empty.' , name ) elif 'measure' not in [ op . name for op in experiment . instructions ] : logger . warning ( 'No measurements in circuit \"%s\", ' 'classical register will remain all zeros.' , name )", "predictions": ["validate the measurements of the experiment ."], "references": ["semantic validations of the qobj which cannot be done via schemas ."], "bleu": 0.1285981829222983, "rouge_l": 0.30148270181219106}
{"id": 864, "code": "def validate initial unitary ( self ) : if self . initial unitary is None : return shape = np . shape ( self . initial unitary ) required shape = ( 2 ** self . number of qubits , 2 ** self . number of qubits ) if shape != required shape : raise Basic Aer Error ( 'initial unitary is incorrect shape: ' + '{} != 2 ** {}' . format ( shape , required shape ) )", "predictions": ["real validation for the initial . . . ."], "references": ["validate an initial unitary matrix"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 865, "code": "def set options ( self , qobj config = None , backend options = None ) : self . initial unitary = self . DEFAULT OPTIONS [ \"initial unitary\" ] self . chop threshold = self . DEFAULT OPTIONS [ \"chop threshold\" ] if backend options is None : backend options = { } if 'initial unitary' in backend options : self . initial unitary = np . array ( backend options [ 'initial unitary' ] , dtype = complex ) elif hasattr ( qobj config , 'initial unitary' ) : self . initial unitary = np . array ( qobj config . initial unitary , dtype = complex ) if self . initial unitary is not None : shape = np . shape ( self . initial unitary ) if len ( shape ) != 2 or shape [ 0 ] != shape [ 1 ] : raise Basic Aer Error ( \"initial unitary is not a square matrix\" ) iden = np . eye ( len ( self . initial unitary ) ) u dagger u = np . dot ( self . initial unitary . T . conj ( ) , self . initial unitary ) norm = np . linalg . norm ( u dagger u - iden ) if round ( norm , 10 ) != 0 : raise Basic Aer Error ( \"initial unitary is not unitary\" ) if 'chop threshold' in backend options : self . chop threshold = backend options [ 'chop threshold' ] elif hasattr ( qobj config , 'chop threshold' ) : self . chop threshold = qobj config . chop threshold", "predictions": ["attach the options options to the initial . ."], "references": ["set the backend options for all experiments in a qobj"], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 866, "code": "def initialize unitary ( self ) : self . validate initial unitary ( ) if self . initial unitary is None : self . unitary = np . eye ( 2 ** self . number of qubits , dtype = complex ) else : self . unitary = self . initial unitary . copy ( ) self . unitary = np . reshape ( self . unitary , self . number of qubits * [ 2 , 2 ] )", "predictions": ["check the dups for the dups"], "references": ["set the initial unitary for simulation"], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 867, "code": "def get unitary ( self ) : unitary = np . reshape ( self . unitary , 2 * [ 2 ** self . number of qubits ] ) unitary = np . stack ( ( unitary . real , unitary . imag ) , axis = - 1 ) unitary [ abs ( unitary ) < self . chop threshold ] = 0.0 return unitary", "predictions": ["returns the unitary of the unitary not in this 1 not including the unitary not yet yet not ."], "references": ["return the current unitary in json result spec format"], "bleu": 0.07658412276041004, "rouge_l": 0.2290362953692115}
{"id": 868, "code": "def is bit ( obj ) : if isinstance ( obj , tuple ) and len ( obj ) == 2 : if isinstance ( obj [ 0 ] , Register ) and isinstance ( obj [ 1 ] , int ) and obj [ 1 ] < len ( obj [ 0 ] ) : return True return False", "predictions": ["returns true if object is a bit object"], "references": ["determine if obj is a bit"], "bleu": 0.3155984539112945, "rouge_l": 0.5865384615384615}
{"id": 869, "code": "def to matrix ( self ) : lam = self . params [ 0 ] lam = float ( lam ) return numpy . array ( [ [ 1 , 0 ] , [ 0 , numpy . exp ( 1j * lam ) ] ] , dtype = complex )", "predictions": ["convert the complex to a complex compatible representation ."], "references": ["return a numpy . array for the u3 gate ."], "bleu": 0.15019394384099988, "rouge_l": 0.20854700854700853}
{"id": 870, "code": "def real ( self , nested scope = None ) : op = self . children [ 0 ] . name expr = self . children [ 1 ] dispatch = { 'sin' : sympy . sin , 'cos' : sympy . cos , 'tan' : sympy . tan , 'asin' : sympy . asin , 'acos' : sympy . acos , 'atan' : sympy . atan , 'exp' : sympy . exp , 'ln' : sympy . log , 'sqrt' : sympy . sqrt } if op in dispatch : arg = expr . real ( nested scope ) return dispatch [ op ] ( arg ) else : raise Node Exception ( \"internal error: undefined external\" )", "predictions": ["return the qasm operator for the given nested . . . . . . . . . . . . . . . . . . ."], "references": ["return the correspond floating point number ."], "bleu": 0.06352047085618948, "rouge_l": 0.19741100323624594}
{"id": 871, "code": "def rzz ( self , theta , qubit1 , qubit2 ) : return self . append ( RZZ Gate ( theta ) , [ qubit1 , qubit2 ] , [ ] )", "predictions": ["add a vector to the image"], "references": ["apply rzz to circuit ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 872, "code": "def cswap ( self , ctl , tgt1 , tgt2 ) : return self . append ( Fredkin Gate ( ) , [ ctl , tgt1 , tgt2 ] , [ ] )", "predictions": ["add a score to the image"], "references": ["apply fredkin to circuit ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 873, "code": "def select best remaining cx ( self ) : candidates = [ ] for gate in self . gate list : chk1 = gate [ 0 ] in self . available hw qubits chk2 = gate [ 1 ] in self . available hw qubits if chk1 and chk2 : candidates . append ( gate ) best reliab = 0 best item = None for item in candidates : if self . gate cost [ item ] > best reliab : best reliab = self . gate cost [ item ] best item = item return best item", "predictions": ["returns the gate for the gate type"], "references": ["select best remaining cnot in the hardware for the next program edge ."], "bleu": 0.1114789227233716, "rouge_l": 0.2846034214618974}
{"id": 874, "code": "def select best remaining qubit ( self , prog qubit ) : reliab store = { } for hw qubit in self . available hw qubits : reliab = 1 for n in self . prog graph . neighbors ( prog qubit ) : if n in self . prog2hw : reliab *= self . swap costs [ self . prog2hw [ n ] ] [ hw qubit ] reliab *= self . readout errors [ hw qubit ] reliab store [ hw qubit ] = reliab max reliab = 0 best hw qubit = None for hw qubit in reliab store : if reliab store [ hw qubit ] > max reliab : max reliab = reliab store [ hw qubit ] best hw qubit = hw qubit return best hw qubit", "predictions": ["swap the ops to the ops with the given prog bits bits"], "references": ["select the best remaining hardware qubit for the next program qubit ."], "bleu": 0.11498759556447223, "rouge_l": 0.16666666666666666}
{"id": 875, "code": "def run ( self , dag ) : self . initialize backend prop ( ) num qubits = self . create program graph ( dag ) if num qubits > len ( self . swap graph ) : raise Transpiler Error ( 'Number of qubits greater than device.' ) for end1 , end2 , in sorted ( self . prog graph . edges ( data = True ) , key = lambda x : x [ 2 ] [ 'weight' ] , reverse = True ) : self . pending program edges . append ( ( end1 , end2 ) ) while self . pending program edges : edge = self . select next edge ( ) q1 mapped = edge [ 0 ] in self . prog2hw q2 mapped = edge [ 1 ] in self . prog2hw if ( not q1 mapped ) and ( not q2 mapped ) : best hw edge = self . select best remaining cx ( ) self . prog2hw [ edge [ 0 ] ] = best hw edge [ 0 ] self . prog2hw [ edge [ 1 ] ] = best hw edge [ 1 ] self . available hw qubits . remove ( best hw edge [ 0 ] ) self . available hw qubits . remove ( best hw edge [ 1 ] ) elif not q1 mapped : best hw qubit = self . select best remaining qubit ( edge [ 0 ] ) self . prog2hw [ edge [ 0 ] ] = best hw qubit self . available hw qubits . remove ( best hw qubit ) else : best hw qubit = self . select best remaining qubit ( edge [ 1 ] ) self . prog2hw [ edge [ 1 ] ] = best hw qubit self . available hw qubits . remove ( best hw qubit ) new edges = [ x for x in self . pending program edges if not ( x [ 0 ] in self . prog2hw and x [ 1 ] in self . prog2hw ) ] self . pending program edges = new edges for qid in self . qarg to id . values ( ) : if qid not in self . prog2hw : self . prog2hw [ qid ] = self . available hw qubits [ 0 ] self . available hw qubits . remove ( self . prog2hw [ qid ] ) layout = Layout ( ) for q in dag . qubits ( ) : pid = self . qarg to id ( q ) hwid = self . prog2hw [ pid ] layout [ ( q [ 0 ] , q [ 1 ] ) ] = hwid self . property set [ 'layout' ] = layout", "predictions": ["physical the dag of the dag ."], "references": ["main run method for the noise adaptive layout ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 876, "code": "def inverse ( self ) : self . data = [ gate . inverse ( ) for gate in reversed ( self . data ) ] self . inverse flag = not self . inverse flag return self", "predictions": ["returns the inverse of the return return the inverse . . . . . . . . . . . . . . . . ."], "references": ["invert this gate ."], "bleu": 0.04668049023095243, "rouge_l": 0.07682619647355164}
{"id": 877, "code": "def q if ( self , * qregs ) : self . data = [ gate . q if ( qregs ) for gate in self . data ] return self", "predictions": ["return if of q"], "references": ["add controls to this gate ."], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 878, "code": "def c if ( self , classical , val ) : self . data = [ gate . c if ( classical , val ) for gate in self . data ] return self", "predictions": ["get the q value of the given instructions for the given instructions for the given instructions for the given instructions for the given instructions for a given instructions for a given"], "references": ["add classical control register ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 879, "code": "def is unitary ( self , atol = None , rtol = None ) : if atol is None : atol = self . atol if rtol is None : rtol = self . rtol return is unitary matrix ( self . data , rtol = rtol , atol = atol )", "predictions": ["instructions of a if this is a if it is if it is if it is if it is if it is if it is if it is if it is"], "references": ["return true if operator is a unitary matrix ."], "bleu": 0.055177848898164926, "rouge_l": 0.1665150136487716}
{"id": 880, "code": "def conjugate ( self ) : return Operator ( np . conj ( self . data ) , self . input dims ( ) , self . output dims ( ) )", "predictions": ["initialize the functional of the vector if any ."], "references": ["return the conjugate of the operator ."], "bleu": 0.21105340631872635, "rouge_l": 0.5115303983228512}
{"id": 881, "code": "def transpose ( self ) : return Operator ( np . transpose ( self . data ) , self . input dims ( ) , self . output dims ( ) )", "predictions": ["is the is a numpy array"], "references": ["return the transpose of the operator ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 882, "code": "def shape ( self ) : return tuple ( reversed ( self . output dims ( ) ) ) + tuple ( reversed ( self . input dims ( ) ) )", "predictions": ["layout of the size of this vector ."], "references": ["return the tensor shape of the matrix operator"], "bleu": 0.21105340631872638, "rouge_l": 0.25}
{"id": 883, "code": "def format state ( self , state ) : state = np . array ( state ) shape = state . shape ndim = state . ndim if ndim > 2 : raise Qiskit Error ( 'Input state is not a vector or matrix.' ) if ndim == 2 : if shape [ 1 ] != 1 and shape [ 1 ] != shape [ 0 ] : raise Qiskit Error ( 'Input state is not a vector or matrix.' ) if shape [ 1 ] == 1 : state = np . reshape ( state , shape [ 0 ] ) return state", "predictions": ["format a state state state"], "references": ["format input state so it is statevector or density matrix"], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 884, "code": "def instruction to operator ( cls , instruction ) : if isinstance ( instruction , Quantum Circuit ) : instruction = instruction . to instruction ( ) op = Operator ( np . eye ( 2 ** instruction . num qubits ) ) op . append instruction ( instruction ) return op", "predictions": ["convert an u2 u2 to an u2 self . ."], "references": ["convert a quantumcircuit or instruction to an operator ."], "bleu": 0.18850319022747347, "rouge_l": 0.42508710801393734}
{"id": 885, "code": "def append instruction ( self , obj , qargs = None ) : if isinstance ( obj , Instruction ) : mat = None if hasattr ( obj , 'to matrix' ) : try : mat = obj . to matrix ( ) except Qiskit Error : pass if mat is not None : op = self . compose ( mat , qargs = qargs ) self . data = op . data else : if obj . definition is None : raise Qiskit Error ( 'Cannot apply Instruction: {}' . format ( obj . name ) ) for instr , qregs , cregs in obj . definition : if cregs : raise Qiskit Error ( 'Cannot apply instruction with classical registers: {}' . format ( instr . name ) ) new qargs = [ tup [ 1 ] for tup in qregs ] self . append instruction ( instr , qargs = new qargs ) else : raise Qiskit Error ( 'Input is not an instruction.' )", "predictions": ["to to to to to add an matrix"], "references": ["update the current operator by apply an instruction ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 886, "code": "def real ( self , nested scope = None ) : operation = self . children [ 0 ] . operation ( ) expr = self . children [ 1 ] . real ( nested scope ) return operation ( expr )", "predictions": ["return is is is is is is is is is is is is is is is the is is is is is is is is is is is is is is"], "references": ["return the correspond floating point number ."], "bleu": 0.04317900023606586, "rouge_l": 0.11879259980525803}
{"id": 887, "code": "def sym ( self , nested scope = None ) : operation = self . children [ 0 ] . operation ( ) expr = self . children [ 1 ] . sym ( nested scope ) return operation ( expr )", "predictions": ["is the data of the data conjugate"], "references": ["return the correspond symbolic number ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 888, "code": "def separate bitstring ( bitstring , creg sizes ) : substrings = [ ] running index = 0 for , size in reversed ( creg sizes ) : substrings . append ( bitstring [ running index : running index + size ] ) running index += size return ' ' . join ( substrings )", "predictions": ["transpose bitstring to bitstring"], "references": ["separate a bitstring according to the registers defined in the result header ."], "bleu": 0.041910459064397936, "rouge_l": 0.2147887323943662}
{"id": 889, "code": "def bit string index ( text ) : n = len ( text ) k = text . count ( \"1\" ) if text . count ( \"0\" ) != n - k : raise Visualization Error ( \"s must be a string of 0 and 1\" ) ones = [ pos for pos , char in enumerate ( text ) if char == \"1\" ] return lex index ( n , k , ones )", "predictions": ["return operator for to to to to to return"], "references": ["return the index of a string of 0s and 1s ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 890, "code": "def bit string index ( s ) : n = len ( s ) k = s . count ( \"1\" ) if s . count ( \"0\" ) != n - k : raise Visualization Error ( \"s must be a string of 0 and 1\" ) ones = [ pos for pos , char in enumerate ( s ) if char == \"1\" ] return lex index ( n , k , ones )", "predictions": ["i i have the to the to the to the to the to the to the to the to the to the to the to the to the to the to"], "references": ["return the index of a string of 0s and 1s ."], "bleu": 0.03901663112717908, "rouge_l": 0.05209222886421862}
{"id": 891, "code": "def op ( self ) : if 'type' not in self . data dict or self . data dict [ 'type' ] != 'op' : raise Qiskit Error ( \"The node %s is not an op node\" % ( str ( self ) ) ) return self . data dict . get ( 'op' )", "predictions": ["the qubits of the qubits as a string ."], "references": ["returns the instruction object corresponding to the op for the node else none"], "bleu": 0.10015045110931886, "rouge_l": 0.17604617604617603}
{"id": 892, "code": "def run ( self , dag ) : diagonal 1q gates = ( RZ Gate , Z Gate , T Gate , S Gate , Tdg Gate , Sdg Gate , U1Gate ) diagonal 2q gates = ( Cz Gate , Crz Gate , Cu1Gate , RZZ Gate ) nodes to remove = set ( ) for measure in dag . op nodes ( Measure ) : predecessor = dag . quantum predecessors ( measure ) [ 0 ] if predecessor . type == 'op' and isinstance ( predecessor . op , diagonal 1q gates ) : nodes to remove . add ( predecessor ) if predecessor . type == 'op' and isinstance ( predecessor . op , diagonal 2q gates ) : successors = dag . quantum successors ( predecessor ) if all ( [ s . type == 'op' and isinstance ( s . op , Measure ) for s in successors ] ) : nodes to remove . add ( predecessor ) for node to remove in nodes to remove : dag . remove op node ( node to remove ) return dag", "predictions": ["set up the dag backend list of nodes objects"], "references": ["return a new circuit that has been optimized ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 893, "code": "def to string ( self , indent ) : ind = indent * ' ' print ( ind , 'qreg' ) self . children [ 0 ] . to string ( indent + 3 )", "predictions": ["np string initialize a string instance"], "references": ["print the node data with indent ."], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 894, "code": "def remove all ops named ( self , opname ) : for n in self . named nodes ( opname ) : self . remove op node ( n )", "predictions": ["removes all from all self ** registered ** self ** self ** number ** self ** self ** ** self ** self ** self ** self ** self ** ** self"], "references": ["remove all operation nodes with the given name ."], "bleu": 0.03901663112717908, "rouge_l": 0.055505004549590536}
{"id": 895, "code": "def add qreg ( self , qreg ) : if not isinstance ( qreg , Quantum Register ) : raise DAG Circuit Error ( \"not a Quantum Register instance.\" ) if qreg . name in self . qregs : raise DAG Circuit Error ( \"duplicate register %s\" % qreg . name ) self . qregs [ qreg . name ] = qreg for j in range ( qreg . size ) : self . add wire ( ( qreg , j ) )", "predictions": ["validate a qreg object"], "references": ["add all wires in a quantum register ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 896, "code": "def add creg ( self , creg ) : if not isinstance ( creg , Classical Register ) : raise DAG Circuit Error ( \"not a Classical Register instance.\" ) if creg . name in self . cregs : raise DAG Circuit Error ( \"duplicate register %s\" % creg . name ) self . cregs [ creg . name ] = creg for j in range ( creg . size ) : self . add wire ( ( creg , j ) )", "predictions": ["add a creg object to the ontology ."], "references": ["add all wires in a classical register ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 897, "code": "def extend back ( self , dag , edge map = None ) : edge map = edge map or { } for qreg in dag . qregs . values ( ) : if qreg . name not in self . qregs : self . add qreg ( Quantum Register ( qreg . size , qreg . name ) ) edge map . update ( [ ( qbit , qbit ) for qbit in qreg if qbit not in edge map ] ) for creg in dag . cregs . values ( ) : if creg . name not in self . cregs : self . add creg ( Classical Register ( creg . size , creg . name ) ) edge map . update ( [ ( cbit , cbit ) for cbit in creg if cbit not in edge map ] ) self . compose back ( dag , edge map )", "predictions": ["extend all back to the dag of the dag ."], "references": ["add dag at the end of self using edge_map ."], "bleu": 0.15851165692617156, "rouge_l": 0.3}
{"id": 898, "code": "def named nodes ( self , * names ) : named nodes = [ ] for node in self . multi graph . nodes ( ) : if node . type == 'op' and node . op . name in names : named nodes . append ( node ) return named nodes", "predictions": ["return a named named nodes that are not the same as a list of named names ."], "references": ["get the set of op nodes with the given name ."], "bleu": 0.09083627868206415, "rouge_l": 0.22289890377588306}
{"id": 899, "code": "def two Q gates ( self ) : two q gates = [ ] for node in self . gate nodes ( ) : if len ( node . qargs ) == 2 : two q gates . append ( node ) return two q gates", "predictions": ["returns a list of two nodes that are two ."], "references": ["get list of 2 - qubit gates . ignore snapshot barriers and the like ."], "bleu": 0.10812944164434664, "rouge_l": 0.23164556962025318}
{"id": 900, "code": "def predecessors ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling predecessors() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] return self . multi graph . predecessors ( node )", "predictions": ["return predecessors if node is predecessors ."], "references": ["returns list of the predecessors of a node as dagnodes ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 901, "code": "def ancestors ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling ancestors() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] return nx . ancestors ( self . multi graph , node )", "predictions": ["return the ancestors of this node ."], "references": ["returns set of the ancestors of a node as dagnodes ."], "bleu": 0.21606281467072083, "rouge_l": 0.5341506129597198}
{"id": 902, "code": "def remove ancestors of ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling remove ancestors of() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] anc = nx . ancestors ( self . multi graph , node ) for anc node in anc : if anc node . type == \"op\" : self . remove op node ( anc node )", "predictions": ["removes ancestors of a node from the graph ."], "references": ["remove all of the ancestor operation nodes of node ."], "bleu": 0.15881076016027915, "rouge_l": 0.31282051282051276}
{"id": 903, "code": "def remove descendants of ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling remove descendants of() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] desc = nx . descendants ( self . multi graph , node ) for desc node in desc : if desc node . type == \"op\" : self . remove op node ( desc node )", "predictions": ["remove descendants of a node ."], "references": ["remove all of the descendant operation nodes of node ."], "bleu": 0.16959011078459055, "rouge_l": 0.47843137254901963}
{"id": 904, "code": "def remove nonancestors of ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling remove nonancestors of() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] anc = nx . ancestors ( self . multi graph , node ) comp = list ( set ( self . multi graph . nodes ( ) ) - set ( anc ) ) for n in comp : if n . type == \"op\" : self . remove op node ( n )", "predictions": ["removes a nonancestors of a node from the graph ."], "references": ["remove all of the non - ancestors operation nodes of node ."], "bleu": 0.12977836824680314, "rouge_l": 0.2683284457478006}
{"id": 905, "code": "def remove nondescendants of ( self , node ) : if isinstance ( node , int ) : warnings . warn ( 'Calling remove nondescendants of() with a node id is deprecated,' ' use a DAG Node instead' , Deprecation Warning , 2 ) node = self . id to node [ node ] dec = nx . descendants ( self . multi graph , node ) comp = list ( set ( self . multi graph . nodes ( ) ) - set ( dec ) ) for n in comp : if n . type == \"op\" : self . remove op node ( n )", "predictions": ["removes a nondescendants of a node from the graph ."], "references": ["remove all of the non - descendants operation nodes of node ."], "bleu": 0.12977836824680314, "rouge_l": 0.2683284457478006}
{"id": 906, "code": "def multigraph layers ( self ) : predecessor count = dict ( ) cur layer = [ node for node in self . input map . values ( ) ] yield cur layer next layer = [ ] while cur layer : for node in cur layer : for successor in self . multi graph . successors ( node ) : multiplicity = self . multi graph . number of edges ( node , successor ) if successor in predecessor count : predecessor count [ successor ] -= multiplicity else : predecessor count [ successor ] = self . multi graph . in degree ( successor ) - multiplicity if predecessor count [ successor ] == 0 : next layer . append ( successor ) del predecessor count [ successor ] yield next layer cur layer = next layer next layer = [ ]", "predictions": ["return an iterator over all multigraph layers"], "references": ["yield layers of the multigraph ."], "bleu": 0.20556680845025982, "rouge_l": 0.15601023017902813}
{"id": 907, "code": "def properties ( self ) : summary = { \"size\" : self . size ( ) , \"depth\" : self . depth ( ) , \"width\" : self . width ( ) , \"bits\" : self . num cbits ( ) , \"factors\" : self . num tensor factors ( ) , \"operations\" : self . count ops ( ) } return summary", "predictions": ["returns a dictionary of all factors properties ."], "references": ["return a dictionary of circuit properties ."], "bleu": 0.3549481056010052, "rouge_l": 0.6747787610619468}
{"id": 908, "code": "def pauli prep gates ( circuit , qreg , op ) : bas , proj = op if bas not in [ 'X' , 'Y' , 'Z' ] : raise Qiskit Error ( \"There's no X, Y or Z basis for this Pauli \" \"preparation\" ) if bas == \"X\" : if proj == 1 : circuit . u2 ( np . pi , np . pi , qreg ) else : circuit . u2 ( 0. , np . pi , qreg ) elif bas == \"Y\" : if proj == 1 : circuit . u2 ( - 0.5 * np . pi , np . pi , qreg ) else : circuit . u2 ( 0.5 * np . pi , np . pi , qreg ) elif bas == \"Z\" and proj == 1 : circuit . u3 ( np . pi , 0. , np . pi , qreg )", "predictions": ["prepare the gates for a pauli pauli gates ."], "references": ["add state preparation gates to a circuit ."], "bleu": 0.16784459625186196, "rouge_l": 0.35672514619883033}
{"id": 909, "code": "def pauli meas gates ( circuit , qreg , op ) : if op not in [ 'X' , 'Y' , 'Z' ] : raise Qiskit Error ( \"There's no X, Y or Z basis for this Pauli \" \"measurement\" ) if op == \"X\" : circuit . u2 ( 0. , np . pi , qreg ) elif op == \"Y\" : circuit . u2 ( 0. , 0.5 * np . pi , qreg )", "predictions": ["apply pauli gates to pauli gates ."], "references": ["add state measurement gates to a circuit ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 910, "code": "def sic prep gates ( circuit , qreg , op ) : bas , proj = op if bas != 'S' : raise Qiskit Error ( 'Not in SIC basis!' ) theta = - 2 * np . arctan ( np . sqrt ( 2 ) ) if proj == 1 : circuit . u3 ( theta , np . pi , 0.0 , qreg ) elif proj == 2 : circuit . u3 ( theta , np . pi / 3 , 0.0 , qreg ) elif proj == 3 : circuit . u3 ( theta , - np . pi / 3 , 0.0 , qreg )", "predictions": ["prepare the gates for a sic gates ."], "references": ["add state preparation gates to a circuit ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 911, "code": "def projector ( op list , basis ) : ret = 1 for op in op list : label , eigenstate = op ret = np . kron ( basis [ label ] [ eigenstate ] , ret ) return ret", "predictions": ["return the projector of a basis basis"], "references": ["returns a projectors ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 912, "code": "def run ( self , dag ) : resets = dag . op nodes ( Reset ) for reset in resets : predecessor = next ( dag . predecessors ( reset ) ) if predecessor . type == 'in' : dag . remove op node ( reset ) return dag", "predictions": ["run a dag ."], "references": ["return a new circuit that has been optimized ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 913, "code": "def cu3 ( self , theta , phi , lam , ctl , tgt ) : return self . append ( Cu3Gate ( theta , phi , lam ) , [ ctl , tgt ] , [ ] )", "predictions": ["add comparison to gate ."], "references": ["apply cu3 from ctl to tgt with angle theta phi lam ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 914, "code": "def build bell circuit ( ) : q = Quantum Register ( 2 ) c = Classical Register ( 2 ) qc = Quantum Circuit ( q , c ) qc . h ( q [ 0 ] ) qc . cx ( q [ 0 ] , q [ 1 ] ) qc . measure ( q , c ) return qc", "predictions": ["build the bell circuit circuit ."], "references": ["returns a circuit putting 2 qubits in the bell state ."], "bleu": 0.1435549295013305, "rouge_l": 0.33516483516483514}
{"id": 915, "code": "def drive ( self ) -> Drive Channel : if self . drives : return self . drives [ 0 ] else : raise Pulse Error ( \"No drive channels in q[%d]\" % self . index )", "predictions": ["the drive in this drive ."], "references": ["return the primary drive channel of this qubit ."], "bleu": 0.16847111051295394, "rouge_l": 0.5147679324894514}
{"id": 916, "code": "def control ( self ) -> Control Channel : if self . controls : return self . controls [ 0 ] else : raise Pulse Error ( \"No control channels in q[%d]\" % self . index )", "predictions": ["the control control channels ."], "references": ["return the primary control channel of this qubit ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 917, "code": "def measure ( self ) -> Measure Channel : if self . measures : return self . measures [ 0 ] else : raise Pulse Error ( \"No measurement channels in q[%d]\" % self . index )", "predictions": ["measure of the measurement in this measurement ."], "references": ["return the primary measure channel of this qubit ."], "bleu": 0.1862539773562041, "rouge_l": 0.465648854961832}
{"id": 918, "code": "def acquire ( self ) -> Acquire Channel : if self . acquires : return self . acquires [ 0 ] else : raise Pulse Error ( \"No acquire channels in q[%d]\" % self . index )", "predictions": ["acquire the channels in the pool ."], "references": ["return the primary acquire channel of this qubit ."], "bleu": 0.16599826150636804, "rouge_l": 0.24448897795591182}
{"id": 919, "code": "def input state ( circ , q , n ) : for j in range ( n ) : circ . h ( q [ j ] ) circ . u1 ( math . pi / float ( 2 ** ( j ) ) , q [ j ] ) . inverse ( )", "predictions": ["n state of the state of the state of the state"], "references": ["n - qubit input state for qft that produces output 1 ."], "bleu": 0.11510518494396255, "rouge_l": 0.17256011315417258}
{"id": 920, "code": "def unset qiskit logger ( ) : qiskit logger = logging . get Logger ( 'qiskit' ) for handler in qiskit logger . handlers : qiskit logger . remove Handler ( handler )", "predictions": ["remove qiskit from the logger ."], "references": ["remove the handlers for the qiskit logger ."], "bleu": 0.24771976691208875, "rouge_l": 0.5570776255707762}
{"id": 921, "code": "def input ( self , data ) : self . data = data self . lexer . input ( data )", "predictions": ["set the lexer for this lexer ."], "references": ["set the input text data ."], "bleu": 0.2626909894424158, "rouge_l": 0.4680306905370844}
{"id": 922, "code": "def pop ( self ) : self . lexer = self . stack . pop ( ) self . filename = self . lexer . qasm file self . lineno = self . lexer . qasm line", "predictions": ["remove the current line from the stack ."], "references": ["pop a ply lexer off the stack ."], "bleu": 0.2984745896009823, "rouge_l": 0.375}
{"id": 923, "code": "def push ( self , filename ) : self . lexer . qasm file = self . filename self . lexer . qasm line = self . lineno self . stack . append ( self . lexer ) self . mklexer ( filename )", "predictions": ["push a new lexer to the stack ."], "references": ["push a ply lexer on the stack to parse filename ."], "bleu": 0.2051383854242905, "rouge_l": 0.6140939597315436}
{"id": 924, "code": "def get bound method ( self , instruction ) : try : return self . bound instructions [ type ( instruction ) ] except Key Error : raise Pulse Error ( 'Qobj conversion method for %s is not found.' % instruction )", "predictions": ["return the bound method for the given instruction ."], "references": ["get conversion method for instruction ."], "bleu": 0.23356898886410005, "rouge_l": 0.5532879818594103}
{"id": 925, "code": "def verify declared bit ( self , obj ) : if obj . name not in self . current symtab : raise Qasm Error ( \"Cannot find symbol '\" + obj . name + \"' in argument list for gate, line\" , str ( obj . line ) , 'file' , obj . file ) sym = self . current symtab [ obj . name ] if not ( sym . type == 'id' and sym . is bit ) : raise Qasm Error ( \"Bit\" , obj . name , 'is not declared as a bit in the gate.' )", "predictions": ["verify that the declared bit is valid"], "references": ["verify a qubit id against the gate prototype ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 926, "code": "def verify exp list ( self , obj ) : # if obj . children is not None : for children in obj . children : if isinstance ( children , node . Id ) : if children . name in self . external functions : continue if children . name not in self . current symtab : raise Qasm Error ( \"Argument '\" + children . name + \"' in expression cannot be \" + \"found, line\" , str ( children . line ) , \"file\" , children . file ) else : if hasattr ( children , \"children\" ) : self . verify exp list ( children )", "predictions": ["check if an exp is valid"], "references": ["verify each expression in a list ."], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 927, "code": "def verify as gate ( self , obj , bitlist , arglist = None ) : if obj . name not in self . global symtab : raise Qasm Error ( \"Cannot find gate definition for '\" + obj . name + \"', line\" , str ( obj . line ) , 'file' , obj . file ) g sym = self . global symtab [ obj . name ] if not ( g sym . type == 'gate' or g sym . type == 'opaque' ) : raise Qasm Error ( \"'\" + obj . name + \"' is used as a gate \" + \"or opaque call but the symbol is neither;\" + \" it is a '\" + g sym . type + \"' line\" , str ( obj . line ) , 'file' , obj . file ) if g sym . n bits ( ) != bitlist . size ( ) : raise Qasm Error ( \"Gate or opaque call to '\" + obj . name + \"' uses\" , str ( bitlist . size ( ) ) , \"qubits but is declared for\" , str ( g sym . n bits ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file ) if arglist : if g sym . n args ( ) != arglist . size ( ) : raise Qasm Error ( \"Gate or opaque call to '\" + obj . name + \"' uses\" , str ( arglist . size ( ) ) , \"qubits but is declared for\" , str ( g sym . n args ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file ) else : if g sym . n args ( ) > 0 : raise Qasm Error ( \"Gate or opaque call to '\" + obj . name + \"' has no arguments but is declared for\" , str ( g sym . n args ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file )", "predictions": ["verify that the gate is valid"], "references": ["verify a user defined gate call ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 928, "code": "def verify reg ( self , obj , object type ) : if obj . name not in self . global symtab : raise Qasm Error ( 'Cannot find definition for' , object type , \"'\" + obj . name + \"'\" , 'at line' , str ( obj . line ) , 'file' , obj . file ) g sym = self . global symtab [ obj . name ] if g sym . type != object type : raise Qasm Error ( \"Type for '\" + g sym . name + \"' should be '\" + object type + \"' but was found to be '\" + g sym . type + \"'\" , \"line\" , str ( obj . line ) , \"file\" , obj . file ) if obj . type == 'indexed id' : bound = g sym . index ndx = obj . index if ndx < 0 or ndx >= bound : raise Qasm Error ( \"Register index for '\" + g sym . name + \"' out of bounds. Index is\" , str ( ndx ) , \"bound is 0 <= index <\" , str ( bound ) , \"at line\" , str ( obj . line ) , \"file\" , obj . file )", "predictions": ["add an object to the global"], "references": ["verify a register ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 929, "code": "def verify reg list ( self , obj , object type ) : for children in obj . children : self . verify reg ( children , object type )", "predictions": ["extend an object on a given object"], "references": ["verify a list of registers ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 930, "code": "def get tokens ( self ) : try : while True : token = self . lexer . token ( ) if not token : break yield token except Qasm Error as e : print ( 'Exception tokenizing qasm file:' , e . msg )", "predictions": ["named nodes from the for the for the for the = = 0 . ."], "references": ["returns a generator of the tokens ."], "bleu": 0.09103526405546068, "rouge_l": 0.1945773524720893}
{"id": 931, "code": "def parse debug ( self , val ) : if val is True : self . parse deb = True elif val is False : self . parse deb = False else : raise Qasm Error ( \"Illegal debug value '\" + str ( val ) + \"' must be True or False.\" )", "predictions": ["two - level two values ."], "references": ["set the parse_deb field ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 932, "code": "def parse ( self , data ) : self . parser . parse ( data , lexer = self . lexer , debug = self . parse deb ) if self . qasm is None : raise Qasm Error ( \"Uncaught exception in parser; \" + \"see previous messages for details.\" ) return self . qasm", "predictions": ["predecessors to the first use of the parser if it exists if not already ."], "references": ["parse some data ."], "bleu": 0.08225964699966554, "rouge_l": 0.11753371868978806}
{"id": 933, "code": "def get tokens ( self ) : if self . filename : with open ( self . filename ) as ifile : self . data = ifile . read ( ) with Qasm Parser ( self . filename ) as qasm p : return qasm p . get tokens ( )", "predictions": ["ancestors the file contents of the file if necessary if possible if not in the file if not ."], "references": ["returns a generator of the tokens ."], "bleu": 0.09107438368292149, "rouge_l": 0.2517193947730399}
{"id": 934, "code": "def parse ( self ) : if self . filename : with open ( self . filename ) as ifile : self . data = ifile . read ( ) with Qasm Parser ( self . filename ) as qasm p : qasm p . parse debug ( False ) return qasm p . parse ( self . data )", "predictions": ["remove the with the with the content of the file node node node node node node node node node node node node node node node node from the file node node"], "references": ["parse the data ."], "bleu": 0.03901663112717908, "rouge_l": 0.06637649619151251}
{"id": 935, "code": "def crz ( self , theta , ctl , tgt ) : return self . append ( Crz Gate ( theta ) , [ ctl , tgt ] , [ ] )", "predictions": ["add a message to the image"], "references": ["apply crz from ctl to tgt with angle theta ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 936, "code": "def qasm ( self , prec = 15 ) : string = \"gate \" + self . name if self . arguments is not None : string += \"(\" + self . arguments . qasm ( prec ) + \")\" string += \" \" + self . bitlist . qasm ( prec ) + \"\\n\" string += \"{\\n\" + self . body . qasm ( prec ) + \"}\" return string", "predictions": ["return the arguments ."], "references": ["return the corresponding openqasm string ."], "bleu": 0.3081980909598119, "rouge_l": 0.5791139240506329}
{"id": 937, "code": "def backend widget ( backend ) : config = backend . configuration ( ) . to dict ( ) props = backend . properties ( ) . to dict ( ) name = widgets . HTML ( value = \"<h4>{name}</h4>\" . format ( name = backend . name ( ) ) , layout = widgets . Layout ( ) ) n qubits = config [ 'n qubits' ] qubit count = widgets . HTML ( value = \"<h5><b>{qubits}</b></h5>\" . format ( qubits = n qubits ) , layout = widgets . Layout ( justify content = 'center' ) ) cmap = widgets . Output ( layout = widgets . Layout ( min width = '250px' , max width = '250px' , max height = '250px' , min height = '250px' , justify content = 'center' , align items = 'center' , margin = '0px 0px 0px 0px' ) ) with cmap : cmap fig = plot gate map ( backend , plot directed = False , label qubits = False ) if cmap fig is not None : display ( cmap fig ) plt . close ( cmap fig ) pending = generate jobs pending widget ( ) is oper = widgets . HTML ( value = \"<h5></h5>\" , layout = widgets . Layout ( justify content = 'center' ) ) least busy = widgets . HTML ( value = \"<h5></h5>\" , layout = widgets . Layout ( justify content = 'center' ) ) t1 units = props [ 'qubits' ] [ 0 ] [ 0 ] [ 'unit' ] avg t1 = round ( sum ( [ q [ 0 ] [ 'value' ] for q in props [ 'qubits' ] ] ) / n qubits , 1 ) t1 widget = widgets . HTML ( value = \"<h5>{t1} {units}</h5>\" . format ( t1 = avg t1 , units = t1 units ) , layout = widgets . Layout ( ) ) t2 units = props [ 'qubits' ] [ 0 ] [ 1 ] [ 'unit' ] avg t2 = round ( sum ( [ q [ 1 ] [ 'value' ] for q in props [ 'qubits' ] ] ) / n qubits , 1 ) t2 widget = widgets . HTML ( value = \"<h5>{t2} {units}</h5>\" . format ( t2 = avg t2 , units = t2 units ) , layout = widgets . Layout ( ) ) out = widgets . V Box ( [ name , cmap , qubit count , pending , least busy , is oper , t1 widget , t2 widget ] , layout = widgets . Layout ( display = 'inline-flex' , flex flow = 'column' , align items = 'center' ) ) out . is alive = True return out", "predictions": ["create a remove widget widget widget"], "references": ["creates a backend widget ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 938, "code": "def generate jobs pending widget ( ) : pbar = widgets . Int Progress ( value = 0 , min = 0 , max = 50 , description = '' , orientation = 'horizontal' , layout = widgets . Layout ( max width = '180px' ) ) pbar . style . bar color = '#71cddd' pbar current = widgets . Label ( value = str ( pbar . value ) , layout = widgets . Layout ( min width = 'auto' ) ) pbar max = widgets . Label ( value = str ( pbar . max ) , layout = widgets . Layout ( min width = 'auto' ) ) def on max change ( change ) : pbar max . value = str ( change [ 'new' ] ) def on val change ( change ) : pbar current . value = str ( change [ 'new' ] ) pbar . observe ( on max change , names = 'max' ) pbar . observe ( on val change , names = 'value' ) jobs widget = widgets . H Box ( [ pbar current , pbar , pbar max ] , layout = widgets . Layout ( max width = '250px' , min width = '250px' , justify content = 'center' ) ) return jobs widget", "predictions": ["generate layers for the layers of the layers cur cur cur cur cur cur cur cur cur cur cur cur cur cur cur cur cur cur cur cur cur cur cur"], "references": ["generates a jobs_pending progress bar widget ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 939, "code": "def bipartite shape ( self ) : return ( self . input dim , self . output dim , self . input dim , self . output dim )", "predictions": ["the shape of the properties of the dimension { properties } ."], "references": ["return the shape for bipartite matrix"], "bleu": 0.1367440667823257, "rouge_l": 0.2364341085271318}
{"id": 940, "code": "def conjugate ( self ) : return Choi ( np . conj ( self . data ) , self . input dims ( ) , self . output dims ( ) )", "predictions": ["pauli value of the vector"], "references": ["return the conjugate of the quantumchannel ."], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 941, "code": "def transpose ( self ) : d in , d out = self . dim data = np . reshape ( self . data , ( d in , d out , d in , d out ) ) data = np . transpose ( data , ( 1 , 0 , 3 , 2 ) ) data = np . reshape ( data , ( d in * d out , d in * d out ) ) return Choi ( data , input dims = self . output dims ( ) , output dims = self . input dims ( ) )", "predictions": ["pauli pauli correlation correlation correlation correlation correlation if any ."], "references": ["return the transpose of the quantumchannel ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 942, "code": "def load schemas and validators ( ) : schema base path = os . path . join ( os . path . dirname ( file ) , '../..' ) for name , path in DEFAULT SCHEMA PATHS . items ( ) : load schema ( os . path . join ( schema base path , path ) , name ) get validator ( name )", "predictions": ["sic all prep gates gates = validators"], "references": ["load all default schemas into _schemas ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 943, "code": "def qasm ( self , prec = 15 ) : return \",\" . join ( [ self . children [ j ] . qasm ( prec ) for j in range ( self . size ( ) ) ] )", "predictions": ["return the projector projector for this ast ."], "references": ["return the corresponding openqasm string ."], "bleu": 0.22679164443904004, "rouge_l": 0.43990384615384615}
{"id": 944, "code": "def qasm ( self , prec = 15 ) : string = \"\" for children in self . children : string += \"  \" + children . qasm ( prec ) + \"\\n\" return string", "predictions": ["returns a string with the current run reset reset reset ."], "references": ["return the corresponding openqasm string ."], "bleu": 0.1354599427337814, "rouge_l": 0.2484725050916497}
{"id": 945, "code": "def calls ( self ) : lst = [ ] for children in self . children : if children . type == \"custom unitary\" : lst . append ( children . name ) return lst", "predictions": ["returns a list of all children ."], "references": ["return a list of custom gate names in this gate body ."], "bleu": 0.17895451045590982, "rouge_l": 0.40197693574958815}
{"id": 946, "code": "def qasm ( self , prec = 15 ) : if self . value == pi : return \"pi\" return ccode ( self . value , precision = prec )", "predictions": ["return - . build build for a given h"], "references": ["return the corresponding openqasm string ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 947, "code": "def conjugate ( self ) : return Super Op ( np . conj ( self . data ) , self . input dims ( ) , self . output dims ( ) )", "predictions": ["drive of the vector ."], "references": ["return the conjugate of the quantumchannel ."], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 948, "code": "def transpose ( self ) : return Super Op ( np . transpose ( self . data ) , input dims = self . output dims ( ) , output dims = self . input dims ( ) )", "predictions": ["control correlation correlation correlation ."], "references": ["return the transpose of the quantumchannel ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 949, "code": "def compose subsystem ( self , other , qargs , front = False ) : input dims = list ( self . input dims ( ) ) output dims = list ( self . output dims ( ) ) if front : num indices = len ( self . input dims ( ) ) shift = 2 * len ( self . output dims ( ) ) right mul = True for pos , qubit in enumerate ( qargs ) : input dims [ qubit ] = other . input dims [ pos ] else : num indices = len ( self . output dims ( ) ) shift = 0 right mul = False for pos , qubit in enumerate ( qargs ) : output dims [ qubit ] = other . output dims [ pos ] tensor = np . reshape ( self . data , self . shape ) mat = np . reshape ( other . data , other . shape ) indices = [ 2 * num indices - 1 - qubit for qubit in qargs ] + [ num indices - 1 - qubit for qubit in qargs ] final shape = [ np . product ( output dims ) ** 2 , np . product ( input dims ) ** 2 ] data = np . reshape ( self . einsum matmul ( tensor , mat , indices , shift , right mul ) , final shape ) return Super Op ( data , input dims , output dims )", "predictions": ["measure the input of the input"], "references": ["return the composition channel ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 950, "code": "def instruction to superop ( cls , instruction ) : if isinstance ( instruction , Quantum Circuit ) : instruction = instruction . to instruction ( ) op = Super Op ( np . eye ( 4 ** instruction . num qubits ) ) op . append instruction ( instruction ) return op", "predictions": ["convert an acquire acquire acquire to an acquire acquire 0 0 0 0 0 0 to an acquire 0 0 0 0 0 0 0 0 0 0 0 0 0"], "references": ["convert a quantumcircuit or instruction to a superop ."], "bleu": 0.04317900023606586, "rouge_l": 0.11101000909918107}
{"id": 951, "code": "def append instruction ( self , obj , qargs = None ) : if isinstance ( obj , Instruction ) : chan = None if obj . name == 'reset' : chan = Super Op ( np . array ( [ [ 1 , 0 , 0 , 1 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] ] ) ) if obj . name == 'kraus' : kraus = obj . params dim = len ( kraus [ 0 ] ) chan = Super Op ( to superop ( 'Kraus' , ( kraus , None ) , dim , dim ) ) elif hasattr ( obj , 'to matrix' ) : try : kraus = [ obj . to matrix ( ) ] dim = len ( kraus [ 0 ] ) chan = Super Op ( to superop ( 'Kraus' , ( kraus , None ) , dim , dim ) ) except Qiskit Error : pass if chan is not None : op = self . compose ( chan , qargs = qargs ) self . data = op . data else : if obj . definition is None : raise Qiskit Error ( 'Cannot apply Instruction: {}' . format ( obj . name ) ) for instr , qregs , cregs in obj . definition : if cregs : raise Qiskit Error ( 'Cannot apply instruction with classical registers: {}' . format ( instr . name ) ) new qargs = [ tup [ 1 ] for tup in qregs ] self . append instruction ( instr , qargs = new qargs ) else : raise Qiskit Error ( 'Input is not an instruction.' )", "predictions": ["input an state on the state of an state"], "references": ["update the current operator by apply an instruction ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 952, "code": "def run ( self , dag ) : final op types = [ 'measure' , 'barrier' ] final ops = [ ] for candidate node in dag . named nodes ( * final op types ) : is final op = True for , child successors in dag . bfs successors ( candidate node ) : if any ( suc . type == 'op' and suc . name not in final op types for suc in child successors ) : is final op = False break if is final op : final ops . append ( candidate node ) if not final ops : return dag barrier layer = DAG Circuit ( ) for qreg in dag . qregs . values ( ) : barrier layer . add qreg ( qreg ) for creg in dag . cregs . values ( ) : barrier layer . add creg ( creg ) final qubits = set ( final op . qargs [ 0 ] for final op in final ops ) barrier layer . apply operation back ( Barrier ( len ( final qubits ) ) , list ( final qubits ) , [ ] ) ordered final nodes = [ node for node in dag . topological op nodes ( ) if node in set ( final ops ) ] for final node in ordered final nodes : barrier layer . apply operation back ( final node . op , final node . qargs , final node . cargs ) for final op in final ops : dag . remove op node ( final op ) dag . extend back ( barrier layer ) adjacent pass = Merge Adjacent Barriers ( ) return adjacent pass . run ( dag )", "predictions": ["unset all handlers in a dag . . ."], "references": ["return a circuit with a barrier before last measurements ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 953, "code": "def unitary ( self , obj , qubits , label = None ) : if isinstance ( qubits , Quantum Register ) : qubits = qubits [ : ] return self . append ( Unitary Gate ( obj , label = label ) , qubits , [ ] )", "predictions": ["add an input object to the backend"], "references": ["apply u2 to q ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 954, "code": "def define ( self ) : if self . num qubits == 1 : q = Quantum Register ( 1 , \"q\" ) angles = euler angles 1q ( self . to matrix ( ) ) self . definition = [ ( U3Gate ( * angles ) , [ q [ 0 ] ] , [ ] ) ] if self . num qubits == 2 : self . definition = two qubit kak ( self . to matrix ( ) )", "predictions": ["defines the angles angles"], "references": ["calculate a subcircuit that implements this unitary ."], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 955, "code": "def atol ( self , atol ) : max tol = self . class . MAX TOL if atol < 0 : raise Qiskit Error ( \"Invalid atol: must be non-negative.\" ) if atol > max tol : raise Qiskit Error ( \"Invalid atol: must be less than {}.\" . format ( max tol ) ) self . class . ATOL = atol", "predictions": ["set the atol: object qasm qasm qasm qasm qasm qasm qasm qasm qasm qasm qasm qasm"], "references": ["set the absolute tolerence parameter for float comparisons ."], "bleu": 0.10123734869668824, "rouge_l": 0.16850828729281767}
{"id": 956, "code": "def rtol ( self , rtol ) : max tol = self . class . MAX TOL if rtol < 0 : raise Qiskit Error ( \"Invalid rtol: must be non-negative.\" ) if rtol > max tol : raise Qiskit Error ( \"Invalid rtol: must be less than {}.\" . format ( max tol ) ) self . class . RTOL = rtol", "predictions": ["setter for a description of the receiver ."], "references": ["set the relative tolerence parameter for float comparisons ."], "bleu": 0.16829946711936866, "rouge_l": 0.232824427480916}
{"id": 957, "code": "def input dims ( self , qargs = None ) : if qargs is None : return self . input dims return tuple ( self . input dims [ i ] for i in qargs )", "predictions": ["not yet yet yet"], "references": ["return tuple of input dimension for specified subsystems ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 958, "code": "def output dims ( self , qargs = None ) : if qargs is None : return self . output dims return tuple ( self . output dims [ i ] for i in qargs )", "predictions": ["children of the verify dimension of the verify ."], "references": ["return tuple of output dimension for specified subsystems ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 959, "code": "def copy ( self ) : return self . class ( self . data , self . input dims ( ) , self . output dims ( ) )", "predictions": ["return a verify verify that the same as a verify model obj obj obj obj obj obj ."], "references": ["make a copy of current operator ."], "bleu": 0.07535838128770536, "rouge_l": 0.17378917378917377}
{"id": 960, "code": "def automatic dims ( cls , dims , size ) : if dims is None : dims = size elif np . product ( dims ) != size : raise Qiskit Error ( \"dimensions do not match size.\" ) if isinstance ( dims , ( int , np . integer ) ) : num qubits = int ( np . log2 ( dims ) ) if 2 ** num qubits == size : return num qubits * ( 2 , ) return ( dims , ) return tuple ( dims )", "predictions": ["automatic dimensions to zero or more dimensions ."], "references": ["check if input dimension corresponds to qubit subsystems ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 961, "code": "def deserialize ( self , value , attr , data ) : try : return super ( ) . deserialize ( value , attr , data ) except Validation Error as ex : if 'deserialization schema selector' in ex . messages [ 0 ] : ex . messages [ 0 ] = 'Cannot find a valid schema among the choices' raise", "predictions": ["deserialize string value to a string ."], "references": ["override _deserialize for customizing the exception raised ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 962, "code": "def serialize ( self , value , key , obj ) : try : return super ( ) . serialize ( value , key , obj ) except Type Error as ex : if 'serialization schema selector' in str ( ex ) : raise Validation Error ( 'Data from an invalid schema' ) raise", "predictions": ["serialize the value and raise it if necessary ."], "references": ["override _serialize for customizing the exception raised ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 963, "code": "def inverse ( self ) : return Snapshot ( self . num qubits , self . num clbits , self . params [ 0 ] , self . params [ 1 ] )", "predictions": ["inverse of this hyperparameters ."], "references": ["special case . return self ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 964, "code": "def is unitary ( self , atol = None , rtol = None ) : try : op = self . to operator ( ) return op . is unitary ( atol = atol , rtol = rtol ) except Qiskit Error : return False", "predictions": ["return true if this operator is unitary ."], "references": ["return true if quantumchannel is a unitary channel ."], "bleu": 0.3029563540905917, "rouge_l": 0.6984732824427481}
{"id": 965, "code": "def to operator ( self ) : mat = to operator ( self . rep , self . data , * self . dim ) return Operator ( mat , self . input dims ( ) , self . output dims ( ) )", "predictions": ["convert the promise to an operator ."], "references": ["try to convert channel to a unitary representation operator ."], "bleu": 0.18094495256969623, "rouge_l": 0.45607476635514016}
{"id": 966, "code": "def format state ( self , state , density matrix = False ) : state = np . array ( state ) shape = state . shape ndim = state . ndim if ndim > 2 : raise Qiskit Error ( 'Input state is not a vector or matrix.' ) if ndim == 2 : if shape [ 1 ] != 1 and shape [ 1 ] != shape [ 0 ] : raise Qiskit Error ( 'Input state is not a vector or matrix.' ) if shape [ 1 ] == 1 : state = np . reshape ( state , shape [ 0 ] ) if density matrix and ndim == 1 : state = np . outer ( state , np . transpose ( np . conj ( state ) ) ) return state", "predictions": ["format a vector state ."], "references": ["format input state so it is statevector or density matrix"], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 967, "code": "def init transformer ( cls , data ) : if isinstance ( data , Quantum Channel ) : return data if hasattr ( data , 'to quantumchannel' ) : return data . to channel ( ) if hasattr ( data , 'to channel' ) : return data . to channel ( ) return Operator ( data )", "predictions": ["initialize instance from data ."], "references": ["convert input into a quantumchannel subclass object or operator object"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 968, "code": "def parse time ( self , date string , settings ) : date string = PATTERN . sub ( '' , date string ) date string = re . sub ( r'\\b(?:ago|in)\\b' , '' , date string ) try : return time parser ( date string ) except : pass", "predictions": ["parse time string into time"], "references": ["attemps to parse time part of date strings like 1 day ago 2 pm"], "bleu": 0.059392570240942286, "rouge_l": 0.19395866454689983}
{"id": 969, "code": "def read config ( self ) : self . threads = self . cfg [ \"threads\" ] or str ( int ( multiprocessing . cpu count ( ) / 2 ) + 1 ) self . phantom modules path = self . cfg [ \"phantom modules path\" ] self . additional libs = ' ' . join ( self . cfg [ \"additional libs\" ] ) self . answ log level = self . cfg [ \"writelog\" ] if self . answ log level . lower ( ) in [ '0' , 'false' ] : self . answ log level = 'none' elif self . answ log level . lower ( ) in [ '1' , 'true' ] : self . answ log level = 'all' self . timeout = parse duration ( self . cfg [ \"timeout\" ] ) if self . timeout > 120000 : logger . warning ( \"You've set timeout over 2 minutes.\" \" Are you a functional tester?\" ) self . answ log = self . core . mkstemp ( \".log\" , \"answ \" ) self . core . add artifact file ( self . answ log ) self . core . add artifact file ( self . phout file ) self . core . add artifact file ( self . stat log ) self . phantom log = self . core . mkstemp ( \".log\" , \"phantom \" ) self . core . add artifact file ( self . phantom log ) main stream = Stream Config ( self . core , len ( self . streams ) , self . phout file , self . answ log , self . answ log level , self . timeout , self . cfg , True ) self . streams . append ( main stream ) for section in self . multi ( ) : self . streams . append ( Stream Config ( self . core , len ( self . streams ) , self . phout file , self . answ log , self . answ log level , self . timeout , section ) ) for stream in self . streams : stream . read config ( ) if any ( stream . ssl for stream in self . streams ) : self . additional libs += ' ssl io benchmark method stream transport ssl'", "predictions": ["read the configuration from the configuration file ."], "references": ["read phantom tool specific options"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 970, "code": "def compose config ( self ) : streams config = '' stat benchmarks = '' for stream in self . streams : streams config += stream . compose config ( ) if not stream . is main : stat benchmarks += \" \" + \"benchmark io%s\" % stream . sequence no kwargs = { } kwargs [ 'threads' ] = self . threads kwargs [ 'phantom log' ] = self . phantom log kwargs [ 'stat log' ] = self . stat log kwargs [ 'benchmarks block' ] = streams config kwargs [ 'stat benchmarks' ] = stat benchmarks kwargs [ 'additional libs' ] = self . additional libs kwargs [ 'phantom modules path' ] = self . phantom modules path filename = self . core . mkstemp ( \".conf\" , \"phantom \" ) self . core . add artifact file ( filename ) logger . debug ( \"Generating phantom config: %s\" , filename ) template str = resource string ( name , \"config/phantom.conf.tpl\" ) tpl = string . Template ( template str ) config = tpl . substitute ( kwargs ) with open ( filename , 'w' ) as conffile : conffile . write ( config ) return filename", "predictions": ["compose the sequence of the sequence to the sequence ."], "references": ["generate phantom tool run config"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 971, "code": "def get info ( self ) : result = copy . copy ( self . streams [ 0 ] ) result . stat log = self . stat log result . steps = [ ] result . ammo file = '' result . rps schedule = None result . ammo count = 0 result . duration = 0 result . instances = 0 result . loadscheme = [ ] result . loop count = 0 for stream in self . streams : sec no = 0 logger . debug ( \"Steps: %s\" , stream . stepper wrapper . steps ) for item in stream . stepper wrapper . steps : for x in range ( 0 , item [ 1 ] ) : if len ( result . steps ) > sec no : result . steps [ sec no ] [ 0 ] += item [ 0 ] else : result . steps . append ( [ item [ 0 ] , 1 ] ) sec no += 1 if result . rps schedule : result . rps schedule = [ ] else : result . rps schedule = stream . stepper wrapper . loadscheme if result . loadscheme : result . loadscheme = '' else : result . loadscheme = '' if result . loop count : result . loop count = u'0' else : result . loop count = stream . stepper wrapper . loop count result . ammo file += '{} ' . format ( stream . stepper wrapper . ammo file ) result . ammo count += stream . stepper wrapper . ammo count result . duration = max ( result . duration , stream . stepper wrapper . duration ) result . instances += stream . instances if not result . ammo count : raise Value Error ( \"Total ammo count cannot be zero\" ) return result", "predictions": ["return the info from the ammo ."], "references": ["get merged info about phantom conf"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 972, "code": "def expand time ( str time , default unit = 's' , multiplier = 1 ) : parser = re . compile ( r'(\\d+)([a-z A-Z]*)' ) parts = parser . findall ( str time ) result = 0.0 for value , unit in parts : value = int ( value ) unit = unit . lower ( ) if unit == '' : unit = default unit if unit == 'ms' : result += value * 0.001 continue elif unit == 's' : result += value continue elif unit == 'm' : result += value * 60 continue elif unit == 'h' : result += value * 60 * 60 continue elif unit == 'd' : result += value * 60 * 60 * 24 continue elif unit == 'w' : result += value * 60 * 60 * 24 * 7 continue else : raise Value Error ( \"String contains unsupported unit %s: %s\" % ( unit , str time ) ) return int ( result * multiplier )", "predictions": ["expand a time time string to a string ."], "references": ["helper for above functions"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 973, "code": "def pid exists ( pid ) : if pid < 0 : return False try : os . kill ( pid , 0 ) except OS Error as exc : logging . debug ( \"No process[%s]: %s\" , exc . errno , exc ) return exc . errno == errno . EPERM else : p = psutil . Process ( pid ) return p . status != psutil . STATUS ZOMBIE", "predictions": ["check if a pid exists ."], "references": ["check whether pid exists in the current process table ."], "bleu": 0.16959011078459055, "rouge_l": 0.47843137254901963}
{"id": 974, "code": "def read config ( self ) : self . log . info ( \"Configuring Stepper Wrapper...\" ) self . ammo file = self . get option ( self . OPTION AMMOFILE ) self . ammo type = self . get option ( 'ammo type' ) if self . ammo file : self . ammo file = os . path . expanduser ( self . ammo file ) self . loop limit = self . get option ( self . OPTION LOOP ) self . ammo limit = self . get option ( \"ammo limit\" ) self . load profile = Load Profile ( * * self . get option ( 'load profile' ) ) self . instances = int ( self . get option ( self . OPTION INSTANCES LIMIT , '1000' ) ) self . uris = self . get option ( \"uris\" , [ ] ) while '' in self . uris : self . uris . remove ( '' ) self . headers = self . get option ( \"headers\" ) self . http ver = self . get option ( \"header http\" ) self . autocases = self . get option ( \"autocases\" ) self . enum ammo = self . get option ( \"enum ammo\" ) self . use caching = self . get option ( \"use caching\" ) self . file cache = self . get option ( 'file cache' ) cache dir = self . get option ( \"cache dir\" ) or self . core . artifacts base dir self . cache dir = os . path . expanduser ( cache dir ) self . force stepping = self . get option ( \"force stepping\" ) if self . get option ( self . OPTION LOAD ) [ self . OPTION LOAD TYPE ] == 'stpd file' : self . stpd = self . get option ( self . OPTION LOAD ) [ self . OPTION SCHEDULE ] self . chosen cases = self . get option ( \"chosen cases\" ) . split ( ) if self . chosen cases : self . log . info ( \"chosen cases LIMITS: %s\" , self . chosen cases )", "predictions": ["read the configuration from the ammo ."], "references": ["stepper part of reading options"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 975, "code": "def prepare stepper ( self ) : def publish info ( stepper info ) : info . status . publish ( 'loadscheme' , stepper info . loadscheme ) info . status . publish ( 'loop count' , stepper info . loop count ) info . status . publish ( 'steps' , stepper info . steps ) info . status . publish ( 'duration' , stepper info . duration ) info . status . ammo count = stepper info . ammo count info . status . publish ( 'instances' , stepper info . instances ) self . core . publish ( 'stepper' , 'loadscheme' , stepper info . loadscheme ) self . core . publish ( 'stepper' , 'loop count' , stepper info . loop count ) self . core . publish ( 'stepper' , 'steps' , stepper info . steps ) self . core . publish ( 'stepper' , 'duration' , stepper info . duration ) self . core . publish ( 'stepper' , 'ammo count' , stepper info . ammo count ) self . core . publish ( 'stepper' , 'instances' , stepper info . instances ) return stepper info if not self . stpd : self . stpd = self . get stpd filename ( ) if self . use caching and not self . force stepping and os . path . exists ( self . stpd ) and os . path . exists ( self . si filename ( ) ) : self . log . info ( \"Using cached stpd-file: %s\" , self . stpd ) stepper info = self . read cached options ( ) if self . instances and self . load profile . is rps ( ) : self . log . info ( \"rps schedule is set. Overriding cached instances param from config: %s\" , self . instances ) stepper info = stepper info . replace ( instances = self . instances ) publish info ( stepper info ) else : if ( self . force stepping and os . path . exists ( self . si filename ( ) ) ) : os . remove ( self . si filename ( ) ) self . make stpd file ( ) stepper info = info . status . get info ( ) self . write cached options ( stepper info ) else : self . log . info ( \"Using specified stpd-file: %s\" , self . stpd ) stepper info = publish info ( self . read cached options ( ) ) self . ammo count = stepper info . ammo count self . duration = stepper info . duration self . loop count = stepper info . loop count self . loadscheme = stepper info . loadscheme self . steps = stepper info . steps if stepper info . instances : self . instances = stepper info . instances", "predictions": ["publish stepper to stepper ."], "references": ["generate test data if necessary"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 976, "code": "def get stpd filename ( self ) : if self . use caching : sep = \"|\" hasher = hashlib . md5 ( ) hashed str = \"cache version 6\" + sep + ';' . join ( self . load profile . schedule ) + sep + str ( self . loop limit ) hashed str += sep + str ( self . ammo limit ) + sep + ';' . join ( self . load profile . schedule ) + sep + str ( self . autocases ) hashed str += sep + \";\" . join ( self . uris ) + sep + \";\" . join ( self . headers ) + sep + self . http ver + sep + \";\" . join ( self . chosen cases ) hashed str += sep + str ( self . enum ammo ) + sep + str ( self . ammo type ) if self . load profile . is instances ( ) : hashed str += sep + str ( self . instances ) if self . ammo file : opener = resource . get opener ( self . ammo file ) hashed str += sep + opener . hash else : if not self . uris : raise Runtime Error ( \"Neither ammofile nor uris specified\" ) hashed str += sep + ';' . join ( self . uris ) + sep + ';' . join ( self . headers ) self . log . debug ( \"stpd-hash source: %s\" , hashed str ) hasher . update ( hashed str . encode ( 'utf8' ) ) if not os . path . exists ( self . cache dir ) : os . makedirs ( self . cache dir ) stpd = self . cache dir + '/' + os . path . basename ( self . ammo file ) + \" \" + hasher . hexdigest ( ) + \".stpd\" else : stpd = os . path . realpath ( \"ammo.stpd\" ) self . log . debug ( \"Generated cache file name: %s\" , stpd ) return stpd", "predictions": ["returns the filename of the stpd ."], "references": ["choose the name for stepped data file"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 977, "code": "def read cached options ( self ) : self . log . debug ( \"Reading cached stepper info: %s\" , self . si filename ( ) ) with open ( self . si filename ( ) , 'r' ) as si file : si = info . Stepper Info ( * * json . load ( si file ) ) return si", "predictions": ["read the cached options from the cached file ."], "references": ["read stepper info from json"], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 978, "code": "def write cached options ( self , si ) : self . log . debug ( \"Saving stepper info: %s\" , self . si filename ( ) ) with open ( self . si filename ( ) , 'w' ) as si file : json . dump ( si . asdict ( ) , si file , indent = 4 )", "predictions": ["write the cached options to the stepper file ."], "references": ["write stepper info to json"], "bleu": 0.16784459625186196, "rouge_l": 0.3012345679012346}
{"id": 979, "code": "def make stpd file ( self ) : self . log . info ( \"Making stpd-file: %s\" , self . stpd ) stepper = Stepper ( self . core , rps schedule = self . load profile . schedule if self . load profile . is rps ( ) else None , http ver = self . http ver , ammo file = self . ammo file , instances schedule = self . load profile . schedule if self . load profile . is instances ( ) else None , instances = self . instances , loop limit = self . loop limit , ammo limit = self . ammo limit , uris = self . uris , headers = [ header . strip ( '[]' ) for header in self . headers ] , autocases = self . autocases , enum ammo = self . enum ammo , ammo type = self . ammo type , chosen cases = self . chosen cases , use cache = self . use caching ) with open ( self . stpd , 'w' , self . file cache ) as os : stepper . write ( os )", "predictions": ["make the stpd file for the stpd ."], "references": ["stpd generation using stepper class"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 980, "code": "def create ( rps schedule ) : if len ( rps schedule ) > 1 : lp = Composite ( [ Step Factory . produce ( step config ) for step config in rps schedule ] ) else : lp = Step Factory . produce ( rps schedule [ 0 ] ) info . status . publish ( 'duration' , lp . get duration ( ) / 1000 ) info . status . publish ( 'steps' , lp . get rps list ( ) ) info . status . lp len = len ( lp ) return lp", "predictions": ["create a list of rps ."], "references": ["create load plan as defined in schedule . publish info about its duration ."], "bleu": 0.06443935473636557, "rouge_l": 0.18654434250764526}
{"id": 981, "code": "def rps at ( self , t ) : if 0 <= t <= self . duration : return self . minrps + float ( self . maxrps - self . minrps ) * t / self . duration else : return 0", "predictions": ["return the rps at the given point t ."], "references": ["return rps for second t"], "bleu": 0.16784459625186196, "rouge_l": 0.4518518518518518}
{"id": 982, "code": "def execute ( self , cmd ) : self . log . info ( \"Executing: %s\" , cmd ) retcode = execute ( cmd , shell = True , poll period = 0.1 , catch out = self . catch out ) [ 0 ] if retcode : raise Runtime Error ( \"Subprocess returned %s\" % retcode ) return retcode", "predictions": ["execute a command and return the output ."], "references": ["execute and check exit code"], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 983, "code": "def publish ( self , key , value ) : self . log . debug ( \"Publishing status: %s/%s: %s\" , self . class . name , key , value ) self . core . publish ( self . class . name , key , value )", "predictions": ["publish a key to the main window ."], "references": ["publish value to status"], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 984, "code": "def count matched codes ( codes regex , codes dict ) : total = 0 for code , count in codes dict . items ( ) : if codes regex . match ( str ( code ) ) : total += count return total", "predictions": ["return the total number of matched codes that match the given codes ."], "references": ["helper to aggregate codes by mask"], "bleu": 0.09552040806823771, "rouge_l": 0.11275415896487984}
{"id": 985, "code": "def stop ( self ) : self . quit . set ( ) while sorted ( [ self . pool [ i ] . is alive ( ) for i in xrange ( len ( self . pool ) ) ] ) [ - 1 ] : time . sleep ( 1 ) try : while not self . task queue . empty ( ) : self . task queue . get ( timeout = 0.1 ) self . task queue . close ( ) self . feeder . join ( ) except Exception as ex : logger . info ( ex )", "predictions": ["stop the task ."], "references": ["say the workers to finish their jobs and quit ."], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 986, "code": "def feed ( self ) : self . plan = Stpd Reader ( self . stpd filename ) if self . cached stpd : self . plan = list ( self . plan ) for task in self . plan : if self . quit . is set ( ) : logger . info ( \"Stop feeding: gonna quit\" ) return while True : try : self . task queue . put ( task , timeout = 1 ) break except Full : if self . quit . is set ( ) or self . workers finished : return else : continue workers count = self . instances logger . info ( \"Feeded all data. Publishing %d killer tasks\" % ( workers count ) ) retry delay = 1 for in range ( 5 ) : try : [ self . task queue . put ( None , timeout = 1 ) for in xrange ( 0 , workers count ) ] break except Full : logger . debug ( \"Couldn't post killer tasks\" \" because queue is full. Retrying in %ss\" , retry delay ) time . sleep ( retry delay ) retry delay *= 2 try : logger . info ( \"Waiting for workers\" ) map ( lambda x : x . join ( ) , self . pool ) logger . info ( \"All workers exited.\" ) self . workers finished = True except ( Keyboard Interrupt , System Exit ) : self . task queue . close ( ) self . results . close ( ) self . quit . set ( ) logger . info ( \"Going to quit. Waiting for workers\" ) map ( lambda x : x . join ( ) , self . pool ) self . workers finished = True", "predictions": ["feed all workers to the queue"], "references": ["a feeder that runs in distinct thread in main process ."], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 987, "code": "def worker ( self ) : logger . debug ( \"Init shooter process\" ) try : self . gun . setup ( ) except Exception : logger . exception ( \"Couldn't initialize gun. Exit shooter process\" ) return while not self . quit . is set ( ) : try : task = self . task queue . get ( timeout = 1 ) if not task : logger . debug ( \"Got killer task.\" ) break timestamp , missile , marker = task planned time = self . start time + ( timestamp / 1000.0 ) delay = planned time - time . time ( ) if delay > 0 : time . sleep ( delay ) try : with self . instance counter . get lock ( ) : self . instance counter . value += 1 self . gun . shoot ( missile , marker ) finally : with self . instance counter . get lock ( ) : self . instance counter . value -= 1 except ( Keyboard Interrupt , System Exit ) : break except Empty : if self . quit . is set ( ) : logger . debug ( \"Empty queue. Exiting process\" ) return except Full : logger . warning ( \"Couldn't put to result queue because it's full\" ) except Exception : logger . exception ( \"Bfg shoot exception\" ) try : self . gun . teardown ( ) except Exception : logger . exception ( \"Couldn't finalize gun. Exit shooter process\" ) return logger . debug ( \"Exit shooter process\" )", "predictions": ["the main loop ."], "references": ["a worker that does actual jobs"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 988, "code": "def green worker ( self ) : while not self . quit . is set ( ) : try : task = self . green queue . get ( timeout = 1 ) timestamp , missile , marker = task planned time = self . start time + ( timestamp / 1000.0 ) delay = planned time - time . time ( ) if delay > 0 : time . sleep ( delay ) try : with self . instance counter . get lock ( ) : self . instance counter . value += 1 self . gun . shoot ( missile , marker ) finally : with self . instance counter . get lock ( ) : self . instance counter . value -= 1 self . free threads count += 1 except ( Keyboard Interrupt , System Exit ) : break except Empty : continue except Full : logger . warning ( \"Couldn't put to result queue because it's full\" ) except Exception : logger . exception ( \"Bfg shoot exception\" )", "predictions": ["green for the worker"], "references": ["a worker that does actual jobs"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 989, "code": "def add user options ( self ) : if self . options . get ( 'user options' , None ) : self . core . apply shorthand options ( self . options [ 'user options' ] )", "predictions": ["add user options ."], "references": ["override config options with user specified options"], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 990, "code": "def configure ( self , options ) : self . options = options if self . options . get ( 'lock dir' , None ) : self . core . set option ( self . core . SECTION , \"lock dir\" , self . options [ 'lock dir' ] ) if self . options . get ( 'ignore lock' , None ) : self . core . set option ( self . core . SECTION , 'ignore lock' , self . options [ 'ignore lock' ] ) while True : try : self . core . get lock ( ) break except Exception as exc : if self . options . get ( 'lock fail' , None ) : raise Runtime Error ( \"Lock file present, cannot continue\" ) self . log . info ( \"Couldn't get lock. Will retry in 5 seconds... (%s)\" , str ( exc ) ) time . sleep ( 5 ) configs = self . get default configs ( ) if self . options . get ( 'config' , None ) : configs . append ( self . options [ 'config' ] ) self . core . load configs ( configs ) self . add user options ( ) self . core . load plugins ( ) if self . options . get ( 'ignore lock' , None ) : self . core . set option ( self . core . SECTION , self . IGNORE LOCKS , \"1\" )", "predictions": ["configure the core core core ."], "references": ["make preparations before running tank"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 991, "code": "def collect data ( self , end = False ) : data = get nowait from queue ( self . results ) stats = get nowait from queue ( self . stats results ) logger . debug ( \"Data timestamps: %s\" % [ d . get ( 'ts' ) for d in data ] ) logger . debug ( \"Stats timestamps: %s\" % [ d . get ( 'ts' ) for d in stats ] ) for item in data : ts = item [ 'ts' ] if ts in self . stat cache : data item = item stat item = self . stat cache . pop ( ts ) self . notify listeners ( data item , stat item ) else : self . data cache [ ts ] = item for item in stats : ts = item [ 'ts' ] if ts in self . data cache : data item = self . data cache . pop ( ts ) stat item = item self . notify listeners ( data item , stat item ) else : self . stat cache [ ts ] = item if end and len ( self . data cache ) > 0 : logger . info ( 'Timestamps without stats:' ) for ts , data item in sorted ( self . data cache . items ( ) , key = lambda i : i [ 0 ] ) : logger . info ( ts ) self . notify listeners ( data item , Stats Reader . stats item ( ts , 0 , 0 ) )", "predictions": ["collect data from queue"], "references": ["collect data cache it and send to listeners"], "bleu": 0.1739594473063345, "rouge_l": 0.31443298969072164}
{"id": 992, "code": "def notify listeners ( self , data , stats ) : for listener in self . listeners : listener . on aggregated data ( data , stats )", "predictions": ["automatic dims dims on the given size . . . . . . . . . ."], "references": ["notify all listeners about aggregate data and stats"], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 993, "code": "def clean markup ( self , orig str ) : for val in [ self . YELLOW , self . RED , self . RESET , self . CYAN , self . BG MAGENTA , self . WHITE , self . BG GREEN , self . GREEN , self . BG BROWN , self . RED DARK , self . MAGENTA , self . BG CYAN ] : orig str = orig str . replace ( val , '' ) return orig str", "predictions": ["deserialize the markup of the markup except it is valid"], "references": ["clean markup from string"], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 994, "code": "def uninstall ( self ) : if self . session : logger . info ( 'Waiting monitoring data...' ) self . session . terminate ( ) self . session . wait ( ) self . session = None log filename = \"agent {host}.log\" . format ( host = \"localhost\" ) data filename = \"agent {host}.rawdata\" . format ( host = \"localhost\" ) try : logger . info ( 'Saving monitoring artefacts from localhost' ) copyfile ( self . workdir + \"/ agent.log\" , log filename ) copyfile ( self . workdir + \"/monitoring.rawdata\" , data filename ) logger . info ( 'Deleting temp directory: %s' , self . workdir ) rmtree ( self . workdir ) except Exception : logger . error ( \"Exception while uninstalling agent\" , exc info = True ) logger . info ( \"Removing agent from: localhost\" ) return log filename , data filename", "predictions": ["serialize super an super agent obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj"], "references": ["remove agent s files from remote host"], "bleu": 0.05809665204409193, "rouge_l": 0.0785070785070785}
{"id": 995, "code": "def uninstall ( self ) : log filename = \"agent {host}.log\" . format ( host = self . host ) data filename = \"agent {host}.rawdata\" . format ( host = self . host ) try : if self . session : self . session . send ( \"stop\\n\" ) self . session . close ( ) self . session = None except Base Exception : logger . warning ( 'Unable to correctly stop monitoring agent - session is broken. Pay attention to agent log (%s).' , log filename , exc info = True ) else : try : self . ssh . get file ( os . path . join ( self . path [ 'AGENT REMOTE FOLDER' ] , \" agent.log\" ) , log filename ) self . ssh . get file ( os . path . join ( self . path [ 'AGENT REMOTE FOLDER' ] , \"monitoring.rawdata\" ) , data filename ) self . ssh . rm r ( self . path [ 'AGENT REMOTE FOLDER' ] ) except Exception : logger . error ( \"Unable to get agent artefacts\" , exc info = True ) self . kill agent ( ) return log filename , data filename", "predictions": ["inverse the monitoring agent agent"], "references": ["remove agent s files from remote host"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 996, "code": "def add jmeter components ( self , jmx , jtl , variables ) : logger . debug ( \"Original JMX: %s\" , os . path . realpath ( jmx ) ) with open ( jmx , 'r' ) as src jmx : source lines = src jmx . readlines ( ) try : closing = source lines . pop ( - 1 ) if \"Work Bench Gui\" in source lines [ - 5 ] : logger . info ( \"Work Bench checkbox enabled...bypassing\" ) last string count = 6 else : last string count = 2 while last string count > 0 : closing = source lines . pop ( - 1 ) + closing last string count -= 1 logger . debug ( \"Closing statement: %s\" , closing ) except Exception as exc : raise Runtime Error ( \"Failed to find the end of JMX XML: %s\" % exc ) udv tpl = resource string ( name , 'config/jmeter var template.xml' ) udv set = [ ] for var name , var value in variables . iteritems ( ) : udv set . append ( udv tpl % ( var name , var name , var value ) ) udv = \"\\n\" . join ( udv set ) if self . jmeter ver >= 2.13 : save connect = '<connect Time>true</connect Time>' else : save connect = '' if self . ext log in [ 'errors' , 'all' ] : level map = { 'errors' : 'true' , 'all' : 'false' } tpl resource = 'jmeter writer ext.xml' tpl args = { 'jtl' : self . jtl file , 'udv' : udv , 'ext log' : self . ext log file , 'ext level' : level map [ self . ext log ] , 'save connect' : save connect } else : tpl resource = 'jmeter writer.xml' tpl args = { 'jtl' : self . jtl file , 'udv' : udv , 'save connect' : save connect } tpl = resource string ( name , 'config/' + tpl resource ) try : new jmx = self . core . mkstemp ( '.jmx' , 'modified ' , os . path . dirname ( os . path . realpath ( jmx ) ) ) except OS Error as exc : logger . debug ( \"Can't create modified jmx near original: %s\" , exc ) new jmx = self . core . mkstemp ( '.jmx' , 'modified ' ) logger . debug ( \"Modified JMX: %s\" , new jmx ) with open ( new jmx , \"wb\" ) as fh : fh . write ( '' . join ( source lines ) ) fh . write ( tpl % tpl args ) fh . write ( closing ) return new jmx", "predictions": ["is the unitary components ."], "references": ["genius idea by alexey lavrenyuk"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 997, "code": "def terminate ( self ) : if self . stderr file : self . stderr file . close ( ) if not self . process : return waitfor = time . time ( ) + PROCESS KILL TIMEOUT while time . time ( ) < waitfor : try : self . process . terminate ( ) except Environment Error as e : if e . errno != errno . ESRCH : LOGGER . warning ( \"Failed to terminate process '{}': {}\" . format ( self . cmd , e ) ) return time . sleep ( 0.1 ) try : self . process . kill ( ) except Environment Error as e : if e . errno != errno . ESRCH : LOGGER . warning ( \"Failed to kill process '{}': {}\" . format ( self . cmd , e ) ) return", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["gracefull termination of running process"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 998, "code": "def read data ( self , lines ) : results = [ ] for line in lines : timestamp , rps , instances = line . split ( \"\\t\" ) curr ts = int ( float ( timestamp ) ) if self . last ts < curr ts : self . last ts = curr ts results . append ( self . stats item ( self . last ts , float ( rps ) , float ( instances ) ) ) return results", "predictions": ["format state into a list of instances objects"], "references": ["parse lines and return stats"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 999, "code": "def create criterion ( self , criterion str ) : parsed = criterion str . split ( \"(\" ) type str = parsed [ 0 ] . strip ( ) . lower ( ) parsed [ 1 ] = parsed [ 1 ] . split ( \")\" ) [ 0 ] . strip ( ) for criterion class in self . custom criterions : if criterion class . get type string ( ) == type str : return criterion class ( self , parsed [ 1 ] ) raise Value Error ( \"Unsupported autostop criterion type: %s\" % criterion str )", "predictions": ["init the transformer class class"], "references": ["instantiate criterion from config string"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1000, "code": "def getconfig ( self , filename , target hint ) : try : tree = self . parse xml ( filename ) except IO Error as exc : logger . error ( \"Error loading config: %s\" , exc ) raise Runtime Error ( \"Can't read monitoring config %s\" % filename ) hosts = tree . findall ( 'Host' ) config = [ ] for host in hosts : host config = self . get host config ( host , target hint ) config . append ( host config ) return config", "predictions": ["load monitoring from the host file"], "references": ["prepare config data ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1001, "code": "def check disk ( self ) : cmd = \"sh -c \\\"df --no-sync -m -P -l -x fuse -x tmpfs -x devtmpfs -x davfs -x nfs \" cmd += self . core . artifacts base dir cmd += \" | tail -n 1 | awk '{print \\$4}' \\\"\" res = execute ( cmd , True , 0.1 , True ) logging . debug ( \"Result: %s\" , res ) if not len ( res [ 1 ] ) : self . log . debug ( \"No disk usage info: %s\" , res [ 2 ] ) return disk free = res [ 1 ] self . log . debug ( \"Disk free space: %s/%s\" , disk free . strip ( ) , self . disk limit ) if int ( disk free . strip ( ) ) < self . disk limit : raise Runtime Error ( \"Not enough local resources: disk space less than %s MB in %s: %s MB\" % ( self . disk limit , self . core . artifacts base dir , int ( disk free . strip ( ) ) ) )", "predictions": ["read path usage and warning if not available phantom phantom phantom phantom phantom phantom phantom phantom phantom phantom"], "references": ["raise exception on disk space exceeded"], "bleu": 0.057259987315337726, "rouge_l": 0.0}
{"id": 1002, "code": "def check mem ( self ) : mem free = psutil . virtual memory ( ) . available / 2 ** 20 self . log . debug ( \"Memory free: %s/%s\" , mem free , self . mem limit ) if mem free < self . mem limit : raise Runtime Error ( \"Not enough resources: free memory less \" \"than %s MB: %s MB\" % ( self . mem limit , mem free ) )", "predictions": ["compose the streams for the streams stat stat stat stat stat stat stat stat stat stat stat stat stat stat stat stat stat stat stat stat stat stat stat stat stat"], "references": ["raise exception on ram exceeded"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1003, "code": "def get terminal size ( ) : default size = ( 30 , 120 ) env = os . environ def ioctl gwinsz ( file d ) : try : sizes = struct . unpack ( 'hh' , fcntl . ioctl ( file d , termios . TIOCGWINSZ , '1234' ) ) except Exception : sizes = default size return sizes sizes = ioctl gwinsz ( 0 ) or ioctl gwinsz ( 1 ) or ioctl gwinsz ( 2 ) if not sizes : try : file d = os . open ( os . ctermid ( ) , os . O RDONLY ) sizes = ioctl gwinsz ( file d ) os . close ( file d . fileno ( ) ) except Exception : pass if not sizes : try : sizes = ( env [ 'LINES' ] , env [ 'COLUMNS' ] ) except Exception : sizes = default size return int ( sizes [ 1 ] ) , int ( sizes [ 0 ] )", "predictions": ["get the info from the info file . ."], "references": ["gets width and height of terminal viewport"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1004, "code": "def get right line ( self , widget output ) : right line = '' if widget output : right line = widget output . pop ( 0 ) if len ( right line ) > self . right panel width : right line plain = self . markup . clean markup ( right line ) if len ( right line plain ) > self . right panel width : right line = right line [ : self . right panel width ] + self . markup . RESET return right line", "predictions": ["returns the time line of the time series"], "references": ["gets next line for right panel"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 1005, "code": "def truncate ( self , line arr , max width ) : def is space ( chunk ) : return all ( [ True if i == ' ' else False for i in chunk ] ) def is empty ( chunks , markups ) : result = [ ] for chunk in chunks : if chunk in markups : result . append ( True ) elif is space ( chunk ) : result . append ( True ) else : result . append ( False ) return all ( result ) left = max width result = '' markups = self . markup . get markup vars ( ) for num , chunk in enumerate ( line arr ) : if chunk in markups : result += chunk else : if left > 0 : if len ( chunk ) <= left : result += chunk left -= len ( chunk ) else : leftover = ( chunk [ left : ] , ) + line arr [ num + 1 : ] was cut = not is empty ( leftover , markups ) if was cut : result += chunk [ : left - 1 ] + self . markup . RESET + u'\\u2026' else : result += chunk [ : left ] left = 0 return result", "predictions": ["pid all chunks in a given line"], "references": ["cut tuple of line chunks according to it s wisible lenght"], "bleu": 0.1160873020151595, "rouge_l": 0.10683012259194395}
{"id": 1006, "code": "def render screen ( self ) : self . term width , self . term height = get terminal size ( ) self . log . debug ( \"Terminal size: %sx%s\" , self . term width , self . term height ) self . right panel width = int ( ( self . term width - len ( self . RIGHT PANEL SEPARATOR ) ) * ( float ( self . info panel percent ) / 100 ) ) - 1 if self . right panel width > 0 : self . left panel width = self . term width - self . right panel width - len ( self . RIGHT PANEL SEPARATOR ) - 2 else : self . right panel width = 0 self . left panel width = self . term width - 1 self . log . debug ( \"Left/right panels width: %s/%s\" , self . left panel width , self . right panel width ) widget output = [ ] if self . right panel width : widget output = [ ] self . log . debug ( \"There are %d info widgets\" % len ( self . info widgets ) ) for index , widget in sorted ( self . info widgets . iteritems ( ) , key = lambda item : ( item [ 1 ] . get index ( ) , item [ 0 ] ) ) : self . log . debug ( \"Rendering info widget #%s: %s\" , index , widget ) widget out = widget . render ( self ) . strip ( ) if widget out : widget output += widget out . split ( \"\\n\" ) widget output += [ \"\" ] left lines = self . render left panel ( ) self . log . debug ( \"Composing final screen output\" ) output = [ ] for line no in range ( 1 , self . term height ) : line = \" \" if line no > 1 and left lines : left line = left lines . pop ( 0 ) left line plain = self . markup . clean markup ( left line ) left line += ( ' ' * ( self . left panel width - len ( left line plain ) ) ) line += left line else : line += ' ' * self . left panel width if self . right panel width : line += self . markup . RESET line += self . markup . WHITE line += self . RIGHT PANEL SEPARATOR line += self . markup . RESET right line = self . get right line ( widget output ) line += right line output . append ( line ) return self . markup . new line . join ( output ) + self . markup . new line", "predictions": ["read the config in the == panel ."], "references": ["main method to render screen view"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1007, "code": "def add info widget ( self , widget ) : index = widget . get index ( ) while index in self . info widgets . keys ( ) : index += 1 self . info widgets [ widget . get index ( ) ] = widget", "predictions": ["prepare the stepper widget"], "references": ["add widget string to right panel of the screen"], "bleu": 0.11392443929712959, "rouge_l": 0.14386792452830188}
{"id": 1008, "code": "def fill rectangle ( self , prepared ) : result = [ ] width = max ( [ self . clean len ( line ) for line in prepared ] ) for line in prepared : spacer = ' ' * ( width - self . clean len ( line ) ) result . append ( line + ( self . screen . markup . RESET , spacer ) ) return ( width , result )", "predictions": ["get the rectangle = value for each profile hasher hasher hasher hasher hasher hasher hasher ."], "references": ["right - pad lines of block to equal width"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 1009, "code": "def clean len ( self , line ) : if isinstance ( line , basestring ) : return len ( self . screen . markup . clean markup ( line ) ) elif isinstance ( line , tuple ) or isinstance ( line , list ) : markups = self . screen . markup . get markup vars ( ) length = 0 for i in line : if i not in markups : length += len ( i ) return length", "predictions": ["cleans the markup and returns the info of the markup"], "references": ["calculate wisible length of string"], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 1010, "code": "def add info widget ( self , widget ) : if not self . screen : self . log . debug ( \"No screen instance to add widget\" ) else : self . screen . add info widget ( widget )", "predictions": ["write cached debug options"], "references": ["add right panel widget"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 1011, "code": "def clean markup ( self , orig str ) : for val in self . get markup vars ( ) : orig str = orig str . replace ( val , '' ) return orig str", "predictions": ["remove markup characters from a string"], "references": ["clean markup from string"], "bleu": 0.2626909894424158, "rouge_l": 0.6224489795918368}
{"id": 1012, "code": "def make writer request ( self , params = None , json = None , http method = \"POST\" , trace = False ) : request = requests . Request ( http method , self . writer url , params = params , json = json , headers = { 'User-Agent' : self . user agent } ) ids = id gen ( str ( uuid . uuid4 ( ) ) ) network timeouts = self . network timeouts ( ) maintenance timeouts = self . maintenance timeouts ( ) while True : try : response = self . send single request ( request , ids . next ( ) , trace = trace ) return response except ( Timeout , Connection Error , Protocol Error ) : logger . warn ( traceback . format exc ( ) ) try : timeout = next ( network timeouts ) logger . warn ( \"Network error, will retry in %ss...\" % timeout ) time . sleep ( timeout ) continue except Stop Iteration : raise self . Network Error ( ) except self . Under Maintenance as e : try : timeout = next ( maintenance timeouts ) logger . warn ( \"Writer is under maintenance, will retry in %ss...\" % timeout ) time . sleep ( timeout ) continue except Stop Iteration : raise e", "predictions": ["create a writer request request request"], "references": ["send request to writer service ."], "bleu": 0.24446151121745047, "rouge_l": 0.16666666666666666}
{"id": 1013, "code": "def load plugins ( self ) : logger . info ( \"Loading plugins...\" ) for ( plugin name , plugin path , plugin cfg ) in self . config . plugins : logger . debug ( \"Loading plugin %s from %s\" , plugin name , plugin path ) if plugin path == \"yandextank.plugins.Overload\" : logger . warning ( \"Deprecated plugin name: 'yandextank.plugins.Overload'\\n\" \"There is a new generic plugin now.\\n\" \"Correcting to 'yandextank.plugins.Data Uploader overload'\" ) plugin path = \"yandextank.plugins.Data Uploader overload\" try : plugin = il . import module ( plugin path ) except Import Error : logger . warning ( 'Plugin name %s path %s import error' , plugin name , plugin path ) logger . debug ( 'Plugin name %s path %s import error' , plugin name , plugin path , exc info = True ) raise try : instance = getattr ( plugin , 'Plugin' ) ( self , cfg = plugin cfg , name = plugin name ) except Attribute Error : logger . warning ( 'Plugin %s classname should be `Plugin`' , plugin name ) raise else : self . register plugin ( self . PLUGIN PREFIX + plugin name , instance ) logger . debug ( \"Plugin instances: %s\" , self . plugins )", "predictions": ["register duration from the duration ."], "references": ["tells core to take plugin options and instantiate plugin classes"], "bleu": 0.09536752763778475, "rouge_l": 0.0}
{"id": 1014, "code": "def get plugin of type ( self , plugin class ) : logger . debug ( \"Searching for plugin: %s\" , plugin class ) matches = [ plugin for plugin in self . plugins . values ( ) if isinstance ( plugin , plugin class ) ] if matches : if len ( matches ) > 1 : logger . debug ( \"More then one plugin of type %s found. Using first one.\" , plugin class ) return matches [ - 1 ] else : raise Key Error ( \"Requested plugin type not found: %s\" % plugin class )", "predictions": ["return plugin by type . . . . . . ."], "references": ["retrieve a plugin of desired class keyerror raised otherwise"], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 1015, "code": "def get plugins of type ( self , plugin class ) : logger . debug ( \"Searching for plugins: %s\" , plugin class ) matches = [ plugin for plugin in self . plugins . values ( ) if isinstance ( plugin , plugin class ) ] if matches : return matches else : raise Key Error ( \"Requested plugin type not found: %s\" % plugin class )", "predictions": ["publish plugins self . . . ."], "references": ["retrieve a list of plugins of desired class keyerror raised otherwise"], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 1016, "code": "def collect file ( self , filename , keep original = False ) : dest = self . artifacts dir + '/' + os . path . basename ( filename ) logger . debug ( \"Collecting file: %s to %s\" , filename , dest ) if not filename or not os . path . exists ( filename ) : logger . warning ( \"File not found to collect: %s\" , filename ) return if os . path . exists ( dest ) : logger . warning ( \"File already exists: %s\" , dest ) return if keep original : shutil . copy ( filename , self . artifacts dir ) else : shutil . move ( filename , self . artifacts dir ) os . chmod ( dest , 0o644 )", "predictions": ["count the contents of a matched matched matched matched matched matched ."], "references": ["move or copy single file to artifacts dir"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 1017, "code": "def add artifact file ( self , filename , keep original = False ) : if filename : logger . debug ( \"Adding artifact file to collect (keep=%s): %s\" , keep original , filename ) self . artifact files [ filename ] = keep original", "predictions": ["stop an artifact self alive alive alive alive alive alive alive alive alive alive alive ."], "references": ["add file to be stored as result artifact on post - process phase"], "bleu": 0.07692375026049747, "rouge_l": 0.07027649769585254}
{"id": 1018, "code": "def load files ( self , configs ) : logger . debug ( \"Reading configs: %s\" , configs ) config filenames = [ resource . resource filename ( config ) for config in configs ] try : self . config . read ( config filenames ) except Exception as ex : logger . error ( \"Can't load configs: %s\" , ex ) raise ex", "predictions": ["feed files to list of files ."], "references": ["read configs set into storage"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1019, "code": "def flush ( self , filename = None ) : if not filename : filename = self . file if filename : with open ( filename , 'w' ) as handle : self . config . write ( handle )", "predictions": ["worker the initialize initialize try to a file"], "references": ["flush current stat to file"], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 1020, "code": "def get options ( self , section , prefix = '' ) : res = [ ] try : for option in self . config . options ( section ) : if not prefix or option . find ( prefix ) == 0 : res += [ ( option [ len ( prefix ) : ] , self . config . get ( section , option ) ) ] except Config Parser . No Section Error as ex : logger . warning ( \"No section: %s\" , ex ) logger . debug ( \"Section: [%s] prefix: '%s' options:\\n%s\" , section , prefix , res ) return res", "predictions": ["green method to green worker worker"], "references": ["get options list with requested prefix"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1021, "code": "def find sections ( self , prefix ) : res = [ ] for section in self . config . sections ( ) : if section . startswith ( prefix ) : res . append ( section ) return res", "predictions": ["add all user user user user user user user user user user user user user user user apply ."], "references": ["return sections with specified prefix"], "bleu": 0.05415315253510895, "rouge_l": 0.0}
{"id": 1022, "code": "def decode stat data ( self , chunk ) : for date str , statistics in chunk . iteritems ( ) : date obj = datetime . datetime . strptime ( date str . split ( \".\" ) [ 0 ] , '%Y-%m-%d %H:%M:%S' ) chunk date = int ( time . mktime ( date obj . timetuple ( ) ) ) instances = 0 for benchmark name , benchmark in statistics . iteritems ( ) : if not benchmark name . startswith ( \"benchmark io\" ) : continue for method , meth obj in benchmark . iteritems ( ) : if \"mmtasks\" in meth obj : instances += meth obj [ \"mmtasks\" ] [ 2 ] offset = chunk date - 1 - self . start time reqps = 0 if 0 <= offset < len ( self . phantom info . steps ) : reqps = self . phantom info . steps [ offset ] [ 0 ] yield self . stats item ( chunk date - 1 , instances , reqps )", "predictions": ["configure the stat self core core core core core core core core core core core core core core core core core core core core core core core core core core core"], "references": ["return all items found in this chunk"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1023, "code": "def prepare ( self ) : agent configs = [ ] if self . config : agent configs = self . config manager . getconfig ( self . config , self . default target ) for config in agent configs : if config [ 'host' ] in [ 'localhost' , '127.0.0.1' , '::1' ] : client = self . clients [ 'localhost' ] ( config , self . old style configs , kill old = self . kill old ) else : client = self . clients [ 'ssh' ] ( config , self . old style configs , timeout = 5 , kill old = self . kill old ) logger . debug ( 'Installing monitoring agent. Host: %s' , client . host ) agent config , startup config , customs script = client . install ( ) if agent config : self . agents . append ( client ) self . artifact files . append ( agent config ) if startup config : self . artifact files . append ( startup config ) if customs script : self . artifact files . append ( customs script )", "predictions": ["collect the without without any without the without the without the d - from the d - 50 . . . . . ."], "references": ["prepare for monitoring - install agents etc"], "bleu": 0.050661968099322066, "rouge_l": 0.07159624413145539}
{"id": 1024, "code": "def poll ( self ) : start time = time . time ( ) for agent in self . agents : for collect in agent . reader : if not collect : return 0 for chunk in collect : ts , prepared results = chunk if self . load start time and int ( ts ) >= self . load start time : ready to send = { \"timestamp\" : int ( ts ) , \"data\" : { self . hash hostname ( agent . host ) : { \"comment\" : agent . config . comment , \"metrics\" : prepared results } } } self . collected data . append ( ready to send ) logger . debug ( 'Polling/decoding agents data took: %.2fms' , ( time . time ( ) - start time ) * 1000 ) collected data length = len ( self . collected data ) if not self . first data received and self . collected data : self . first data received = True logger . info ( \"Monitoring received first data.\" ) else : self . send collected data ( ) return collected data length", "predictions": ["poll agents data from the network ."], "references": ["poll agents for data"], "bleu": 0.2626909894424158, "rouge_l": 0.5736677115987461}
{"id": 1025, "code": "def send collected data ( self ) : data = self . collected data self . collected data = [ ] for listener in self . listeners : listener . monitoring data ( copy . deepcopy ( data ) )", "predictions": ["send collected collected data to server ."], "references": ["sends pending data set to listeners"], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 1026, "code": "def decode agents data ( self , block ) : collect = [ ] if block : for chunk in block . split ( '\\n' ) : try : if chunk : prepared results = { } jsn = json . loads ( chunk ) for ts , values in jsn . iteritems ( ) : for key , value in values . iteritems ( ) : try : key group , key name = key . split ( ' ' ) [ 0 ] . split ( '-' ) [ 0 ] , ' ' . join ( key . split ( ' ' ) [ 1 : ] ) except : key group , key name = key . split ( ' ' ) [ 0 ] , ' ' . join ( key . split ( ' ' ) [ 1 : ] ) if key group in decoder . diff metrics . keys ( ) : if key name in decoder . diff metrics [ key group ] : decoded key = decoder . find common names ( key ) if self . prev check : try : value = jsn [ ts ] [ key ] - self . prev check [ key ] except Key Error : logger . debug ( 'There is no diff value for metric %s.\\n' 'Timestamp: %s. Is it initial data?' , key , ts , exc info = True ) value = 0 prepared results [ decoded key ] = value else : decoded key = decoder . find common names ( key ) prepared results [ decoded key ] = value else : decoded key = decoder . find common names ( key ) prepared results [ decoded key ] = value self . prev check = jsn [ ts ] collect . append ( ( ts , prepared results ) ) except Value Error : logger . error ( 'Telegraf agent send trash to output: %s' , chunk ) logger . debug ( 'Telegraf agent data block w/ trash: %s' , exc info = True ) return [ ] except Base Exception : logger . error ( 'Exception trying to parse agent data: %s' , chunk , exc info = True ) return [ ] if collect : return collect", "predictions": ["decode agents from a block block ."], "references": ["decode agents jsons count diffs"], "bleu": 0.24446151121745047, "rouge_l": 0.34366197183098596}
{"id": 1027, "code": "async def close ( self ) : if self . ws is not None : await self . ws . close ( ) if self . polygon is not None : await self . polygon . close ( )", "predictions": ["close the polygon polygon polygon ."], "references": ["close any of open connections"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 1028, "code": "def submit order ( self , symbol , qty , side , type , time in force , limit price = None , stop price = None , client order id = None ) : params = { 'symbol' : symbol , 'qty' : qty , 'side' : side , 'type' : type , 'time in force' : time in force , } if limit price is not None : params [ 'limit price' ] = limit price if stop price is not None : params [ 'stop price' ] = stop price if client order id is not None : params [ 'client order id' ] = client order id resp = self . post ( '/orders' , params ) return Order ( resp )", "predictions": ["submit an order to the specified symbol ."], "references": ["request a new order"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 1029, "code": "def get position ( self , symbol ) : resp = self . get ( '/positions/{}' . format ( symbol ) ) return Position ( resp )", "predictions": ["return the position for the specified symbol"], "references": ["get an open position"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 1030, "code": "def list assets ( self , status = None , asset class = None ) : params = { 'status' : status , 'assert class' : asset class , } resp = self . get ( '/assets' , params ) return [ Asset ( o ) for o in resp ]", "predictions": ["returns a list of all available assets ."], "references": ["get a list of assets"], "bleu": 0.3155984539112945, "rouge_l": 0.6421052631578947}
{"id": 1031, "code": "def construct event logger ( event record callback ) : check . callable param ( event record callback , 'event record callback' ) return construct single handler logger ( 'event-logger' , DEBUG , Structured Logger Handler ( lambda logger message : event record callback ( construct event record ( logger message ) ) ) , )", "predictions": ["create a logger logger ."], "references": ["callback receives a stream of event_records"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 1032, "code": "def construct json event logger ( json path ) : check . str param ( json path , 'json path' ) return construct single handler logger ( \"json-event-record-logger\" , DEBUG , Json Event Logger Handler ( json path , lambda record : construct event record ( Structured Logger Message ( name = record . name , message = record . msg , level = record . levelno , meta = record . dagster meta , record = record , ) ) , ) , )", "predictions": ["create a json event logger from a json string ."], "references": ["record a stream of event records to json"], "bleu": 0.14991106946711685, "rouge_l": 0.34014869888475835}
{"id": 1033, "code": "def format config for graphql ( config ) : def format config subdict ( config , current indent = 0 ) : check . dict param ( config , 'config' , key type = str ) printer = Indenting String Io Printer ( indent level = 2 , current indent = current indent ) printer . line ( '{' ) n elements = len ( config ) for i , key in enumerate ( sorted ( config , key = lambda x : x [ 0 ] ) ) : value = config [ key ] with printer . with indent ( ) : formatted value = ( format config item ( value , current indent = printer . current indent ) . lstrip ( ' ' ) . rstrip ( '\\n' ) ) printer . line ( '{key}: {formatted value}{comma}' . format ( key = key , formatted value = formatted value , comma = ',' if i != n elements - 1 else '' , ) ) printer . line ( '}' ) return printer . read ( ) def format config sublist ( config , current indent = 0 ) : printer = Indenting String Io Printer ( indent level = 2 , current indent = current indent ) printer . line ( '[' ) n elements = len ( config ) for i , value in enumerate ( config ) : with printer . with indent ( ) : formatted value = ( format config item ( value , current indent = printer . current indent ) . lstrip ( ' ' ) . rstrip ( '\\n' ) ) printer . line ( '{formatted value}{comma}' . format ( formatted value = formatted value , comma = ',' if i != n elements - 1 else '' ) ) printer . line ( ']' ) return printer . read ( ) def format config item ( config , current indent = 0 ) : printer = Indenting String Io Printer ( indent level = 2 , current indent = current indent ) if isinstance ( config , dict ) : return format config subdict ( config , printer . current indent ) elif isinstance ( config , list ) : return format config sublist ( config , printer . current indent ) elif isinstance ( config , bool ) : return repr ( config ) . lower ( ) else : return repr ( config ) . replace ( '\\'' , '\"' ) check . dict param ( config , 'config' , key type = str ) if not isinstance ( config , dict ) : check . failed ( 'Expected a dict to format as config, got: {item}' . format ( item = repr ( config ) ) ) return format config subdict ( config )", "predictions": ["format the config for json ."], "references": ["this recursive descent thing formats a config dict for graphql ."], "bleu": 0.1141650334026257, "rouge_l": 0.33516483516483514}
{"id": 1034, "code": "def execute pipeline through queue ( repository info , pipeline name , solid subset , environment dict , run id , message queue , reexecution config , step keys to execute , ) : message queue . put ( Process Started Sentinel ( os . getpid ( ) ) ) run config = Run Config ( run id , event callback = message queue . put , executor config = In Process Executor Config ( raise on error = False ) , reexecution config = reexecution config , step keys to execute = step keys to execute , ) repository container = Repository Container ( repository info ) if repository container . repo error : message queue . put ( Multiprocessing Error ( serializable error info from exc info ( repository container . repo error ) ) ) return try : result = execute pipeline ( repository container . repository . get pipeline ( pipeline name ) . build sub pipeline ( solid subset ) , environment dict , run config = run config , ) return result except : error info = serializable error info from exc info ( sys . exc info ( ) ) message queue . put ( Multiprocessing Error ( error info ) ) finally : message queue . put ( Multiprocessing Done ( ) ) message queue . close ( )", "predictions": ["execute a pipeline in the queue ."], "references": ["execute pipeline using message queue as a transport"], "bleu": 0.2024757945132846, "rouge_l": 0.3952483801295896}
{"id": 1035, "code": "def join ( self ) : while True : with self . processes lock : if not self . processes and self . processing semaphore . locked ( ) : return True gevent . sleep ( 0.1 )", "predictions": ["join the processes if it s stopped ."], "references": ["waits until all there are no processes enqueued ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 1036, "code": "def build ( self , pipeline def , artifacts persisted ) : deps = { step . key : set ( ) for step in self . steps } for step in self . steps : for step input in step . step inputs : deps [ step . key ] . add ( step input . prev output handle . step key ) step dict = { step . key : step for step in self . steps } return Execution Plan ( pipeline def , step dict , deps , artifacts persisted )", "predictions": ["build a dictionary of artifacts from the pipeline ."], "references": ["builds the execution plan ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 1037, "code": "def construct publish comands ( additional steps = None , nightly = False ) : publish commands = ( [ 'rm -rf dist' ] + ( additional steps if additional steps else [ ] ) + [ 'python setup.py sdist bdist wheel{nightly}' . format ( nightly = ' --nightly' if nightly else '' ) , 'twine upload dist/*' , ] ) return publish commands", "predictions": ["construct the comands from the comands steps ."], "references": ["get the shell commands we ll use to actually build and publish a package to pypi ."], "bleu": 0.05761738913376924, "rouge_l": 0.1502463054187192}
{"id": 1038, "code": "def block ( self , text , prefix = '' ) : wrapper = Text Wrapper ( width = self . line length - len ( self . current indent str ) , initial indent = prefix , subsequent indent = prefix , break long words = False , break on hyphens = False , ) for line in wrapper . wrap ( text ) : self . line ( line )", "predictions": ["wrap the given text with the given prefix ."], "references": ["automagically wrap a block of text ."], "bleu": 0.16784459625186196, "rouge_l": 0.38364779874213834}
{"id": 1039, "code": "def define shared fields ( ) : clustering fields = Field ( List ( String ) , description = , is optional = True , ) create disposition = Field ( BQ Create Disposition , description = , is optional = True , ) destination encryption configuration = Field ( String , description = , is optional = True , ) schema update options = Field ( List ( BQ Schema Update Option ) , description = , is optional = True , ) time partitioning = Field ( Dict ( fields = { 'expiration ms' : Field ( Int , description = , is optional = True , ) , 'field' : Field ( String , description = , is optional = True , ) , 'require partition filter' : Field ( Bool , description = , is optional = True , ) , } ) , description = 'Specifies time-based partitioning for the destination table.' , is optional = True , ) write disposition = Field ( BQ Write Disposition , description = , is optional = True , ) return { 'clustering fields' : clustering fields , 'create disposition' : create disposition , 'destination encryption configuration' : destination encryption configuration , 'schema update options' : schema update options , 'time partitioning' : time partitioning , 'write disposition' : write disposition , }", "predictions": ["define shared fields for shared fields ."], "references": ["the following fields are shared between both queryjobconfig and loadjobconfig ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 1040, "code": "def mkdir p ( newdir , mode = 0o777 ) : try : os . makedirs ( newdir , mode ) except OS Error as err : if err . errno != errno . EEXIST or not os . path . isdir ( newdir ) : raise", "predictions": ["create directory if it doesn t exist ."], "references": ["the missing mkdir - p functionality in os ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 1041, "code": "def success ( self ) : any success = False for step event in itertools . chain ( self . input expectations , self . output expectations , self . transforms ) : if step event . event type == Dagster Event Type . STEP FAILURE : return False if step event . event type == Dagster Event Type . STEP SUCCESS : any success = True return any success", "predictions": ["return true if any of the input step has been success ."], "references": ["whether the solid execution was successful"], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 1042, "code": "def skipped ( self ) : return all ( [ step event . event type == Dagster Event Type . STEP SKIPPED for step event in itertools . chain ( self . input expectations , self . output expectations , self . transforms ) ] )", "predictions": ["return a list of all skipped transforms ."], "references": ["whether the solid execution was skipped"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 1043, "code": "def failure data ( self ) : for result in itertools . chain ( self . input expectations , self . output expectations , self . transforms ) : if result . event type == Dagster Event Type . STEP FAILURE : return result . step failure data", "predictions": ["return the failure data ."], "references": ["returns the failing step s data that happened during this solid s execution if any"], "bleu": 0.040889869516541145, "rouge_l": 0.18345864661654135}
{"id": 1044, "code": "def is valid dataset ( config value ) : return re . match ( r'^' + RE PROJECT + r'\\.' + RE DS TABLE + r'$|^' + RE DS TABLE + r'$' , config value , )", "predictions": ["check if a config value is valid"], "references": ["datasets must be of form project . dataset or dataset"], "bleu": 0.10175282441454783, "rouge_l": 0.0}
{"id": 1045, "code": "def is valid table ( config value ) : return re . match ( r'^' + RE PROJECT + r'\\.' + RE DS TABLE + r'\\.' + RE DS TABLE + r'$|^' + RE DS TABLE + r'\\.' + RE DS TABLE + r'$' , config value , )", "predictions": ["return true if config is a valid table ."], "references": ["tables must be of form project . dataset . table or dataset . table"], "bleu": 0.08961856124931385, "rouge_l": 0.1673525377229081}
{"id": 1046, "code": "def coalesce execution steps ( execution plan ) : solid order = coalesce solid order ( execution plan ) steps = defaultdict ( list ) for solid name , solid steps in itertools . groupby ( execution plan . topological steps ( ) , lambda x : x . solid name ) : steps [ solid name ] += list ( solid steps ) return Ordered Dict ( [ ( solid name , steps [ solid name ] ) for solid name in solid order ] )", "predictions": ["coalesce solid steps steps into solid steps ."], "references": ["groups execution steps by solid in topological order of the solids ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 1047, "code": "def create cursor ( self , name = None ) : return Cursor ( self . client connection , self . connection , self . djongo connection )", "predictions": ["creates a new cursor ."], "references": ["returns an active connection cursor to the database ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 1048, "code": "def close ( self ) : if self . connection : with self . wrap database errors : self . connection . client . close ( )", "predictions": ["close the database ."], "references": ["closes the client connection to the database ."], "bleu": 0.24601580968354606, "rouge_l": 0.47164948453608246}
{"id": 1049, "code": "def make mdl ( model , model dict ) : for field name in model dict : field = model . meta . get field ( field name ) model dict [ field name ] = field . to python ( model dict [ field name ] ) return model ( * * model dict )", "predictions": ["make a model from a model dict ."], "references": ["builds an instance of model from the model_dict ."], "bleu": 0.20014292374951972, "rouge_l": 0.34923664122137404}
{"id": 1050, "code": "def formfield ( self , * * kwargs ) : defaults = { 'form class' : Array Form Field , 'model container' : self . model container , 'model form class' : self . model form class , 'name' : self . attname , 'mdl form kw l' : self . model form kwargs l } defaults . update ( kwargs ) return super ( ) . formfield ( * * defaults )", "predictions": ["returns field s form ."], "references": ["returns the formfield for the array ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 1051, "code": "def apply rel filters ( self , queryset ) : queryset . add hints ( instance = self . instance ) if self . db : queryset = queryset . using ( self . db ) queryset = queryset . filter ( * * self . core filters ) return queryset", "predictions": ["filter the queryset with a queryset ."], "references": ["filter the queryset for the instance this manager is bound to ."], "bleu": 0.17895451045590982, "rouge_l": 0.40197693574958815}
{"id": 1052, "code": "def calc c ( self , a1 , a2 , r1 , r2 ) : if r1 == 0.0 and r2 == 0.0 : return a1 , a2 div = 1 / ( r1 + r2 ) c1 = ( a1 * r2 + a2 * r1 ) * div c2 = ( a1 * r1 + a2 * r2 ) * div return c1 , c2", "predictions": ["calculate the c - two - greedy distribution"], "references": ["compute the functions c1 and c2"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 1053, "code": "def clear ( self ) : self . reg = np . zeros ( ( self . m , ) , dtype = np . int8 )", "predictions": ["clears the contents of the instance ."], "references": ["reset the current hyperloglog to empty ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 1054, "code": "def index ( self ) : for i , hashtable in enumerate ( self . hashtables ) : self . sorted hashtables [ i ] = [ H for H in hashtable . keys ( ) ] self . sorted hashtables [ i ] . sort ( )", "predictions": ["get the index of all hashtable in the order"], "references": ["index all the keys added so far and make them searchable ."], "bleu": 0.12026590852507517, "rouge_l": 0.2785388127853881}
{"id": 1055, "code": "async def close ( self ) : async with self . lock : for t in self . hashtables : await t . close ( ) if self . keys is not None : await self . keys . close ( ) self . initialized = False", "predictions": ["close all keys ."], "references": ["cleanup client resources and disconnect from asyncminhashlsh storage ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 1056, "code": "def parse scoped selector ( scoped selector ) : if scoped selector [ 0 ] == '%' : if scoped selector . endswith ( '.value' ) : err str = '{} is invalid cannot use % and end with .value' raise Value Error ( err str . format ( scoped selector ) ) scoped selector = scoped selector [ 1 : ] + '/macro.value' scope selector list = scoped selector . rsplit ( '/' , 1 ) scope = '' . join ( scope selector list [ : - 1 ] ) selector = scope selector list [ - 1 ] return scope , selector", "predictions": ["poll for scoped self agents agents and self agents agents agents ."], "references": ["parse scoped selector ."], "bleu": 0.11498759556447223, "rouge_l": 0.27477477477477474}
{"id": 1057, "code": "def advance one line ( self ) : current line = self . current token . line number while current line == self . current token . line number : self . current token = Config Parser . Token ( * next ( self . token generator ) )", "predictions": ["send a collected data data data to the = data data"], "references": ["advances to next line ."], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 1058, "code": "def augment exception message and reraise ( exception , message ) : class Exception Proxy ( type ( exception ) ) : \"\"\"Acts as a proxy for an exception with an augmented message.\"\"\" module = type ( exception ) . module def init ( self ) : pass def getattr ( self , attr name ) : return getattr ( exception , attr name ) def str ( self ) : return str ( exception ) + message Exception Proxy . name = type ( exception ) . name proxy = Exception Proxy ( ) if six . PY3 : Exception Proxy . qualname = type ( exception ) . qualname six . raise from ( proxy . with traceback ( exception . traceback ) , None ) else : six . reraise ( proxy , None , sys . exc info ( ) [ 2 ] )", "predictions": ["decode the agents and and and and and"], "references": ["reraises exception appending message to its string representation ."], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 1059, "code": "def markdownify operative config str ( self , string ) : def process ( line ) : \"\"\"Convert a single line to markdown format.\"\"\" if not line . startswith ( '#' ) : return '    ' + line line = line [ 2 : ] if line . startswith ( '====' ) : return '' if line . startswith ( 'None' ) : return if line . endswith ( ':' ) : return + line return line output lines = [ ] for line in string . splitlines ( ) : procd line = process ( line ) if procd line is not None : output lines . append ( procd line ) return '\\n' . join ( output lines )", "predictions": ["convert close to markdown"], "references": ["convert an operative config string to markdown format ."], "bleu": 0.14558246978804804, "rouge_l": 0.43160377358490565}
{"id": 1060, "code": "def after create session ( self , session = None , coord = None ) : config str = config . operative config str ( ) if not tf . gfile . Is Directory ( self . output dir ) : tf . gfile . Make Dirs ( self . output dir ) global step val = 0 if session is not None : global step = tf . train . get global step ( ) if global step is not None : global step val = session . run ( global step ) filename = '%s-%s.gin' % ( self . base name , global step val ) config path = os . path . join ( self . output dir , filename ) with tf . gfile . G File ( config path , 'w' ) as f : f . write ( config str ) if self . summarize config : md config str = self . markdownify operative config str ( config str ) summary metadata = summary pb2 . Summary Metadata ( ) summary metadata . plugin data . plugin name = 'text' summary metadata . plugin data . content = b'{}' text tensor = tf . make tensor proto ( md config str ) summary = summary pb2 . Summary ( ) summary . value . add ( tag = 'gin/' + self . base name , tensor = text tensor , metadata = summary metadata ) if not self . summary writer : self . summary writer = tf . summary . File Writer Cache . get ( self . output dir ) self . summary writer . add summary ( summary , global step val ) self . summary writer . flush ( )", "predictions": ["submit the = 0 to order to order price price price price price price price price price price price price price"], "references": ["writes out gin s operative config and maybe adds a summary of it ."], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 1061, "code": "def find class construction fn ( cls ) : for base in type . mro ( cls ) : if ' init ' in base . dict : return base . init if ' new ' in base . dict : return base . new", "predictions": ["get the construction construction construction construction"], "references": ["find the first __init__ or __new__ method in the given class s mro ."], "bleu": 0.05822753005110548, "rouge_l": 0.09327217125382263}
{"id": 1062, "code": "def ensure wrappability ( fn ) : if isinstance ( fn , ( type ( object . init ) , type ( object . call ) ) ) : wrappable fn = lambda * args , * * kwargs : fn ( * args , * * kwargs ) wrappable fn . name = fn . name wrappable fn . doc = fn . doc wrappable fn . module = '' wrappable fn . wrapped = fn return wrappable fn return fn", "predictions": ["list of assets to be called from assets . . ."], "references": ["make sure fn can be wrapped cleanly by functools . wraps ."], "bleu": 0.12368857073777001, "rouge_l": 0.25884016973125884}
{"id": 1063, "code": "def get cached arg spec ( fn ) : arg spec = ARG SPEC CACHE . get ( fn ) if arg spec is None : arg spec fn = inspect . getfullargspec if six . PY3 else inspect . getargspec try : arg spec = arg spec fn ( fn ) except Type Error : arg spec = arg spec fn ( fn . call ) ARG SPEC CACHE [ fn ] = arg spec return arg spec", "predictions": ["construct the event spec spec spec spec spec"], "references": ["gets cached argspec for fn ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1064, "code": "def get supplied positional parameter names ( fn , args ) : arg spec = get cached arg spec ( fn ) return arg spec . args [ : len ( args ) ]", "predictions": ["construct the json - event names names"], "references": ["returns the names of the supplied arguments to the given function ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 1065, "code": "def get all positional parameter names ( fn ) : arg spec = get cached arg spec ( fn ) args = arg spec . args if arg spec . defaults : args = args [ : - len ( arg spec . defaults ) ] return args", "predictions": ["format config names names names"], "references": ["returns the names of all positional arguments to the given function ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 1066, "code": "def parse value ( value ) : if not isinstance ( value , six . string types ) : raise Value Error ( 'value ({}) should be a string type.' . format ( value ) ) return config parser . Config Parser ( value , Parser Delegate ( ) ) . parse value ( )", "predictions": ["execute a pipeline pipeline subset subset"], "references": ["parse and return a single gin value ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 1067, "code": "def iterate flattened values ( value ) : if isinstance ( value , six . string types ) : yield value return if isinstance ( value , collections . Mapping ) : value = collections . Values View ( value ) if isinstance ( value , collections . Iterable ) : for nested value in value : for nested nested value in iterate flattened values ( nested value ) : yield nested nested value yield value", "predictions": ["processing a value into a list of flattened objects ."], "references": ["provides an iterator over all values in a nested structure ."], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 1068, "code": "def get all matches ( self , partial selector ) : matching selectors = self . matching selectors ( partial selector ) return [ self . selector map [ selector ] for selector in matching selectors ]", "predictions": ["build all self . selector ."], "references": ["returns all values matching partial_selector as a list ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 1069, "code": "def sp search query ( query ) : result = [ ] for ( field , values ) in query . items ( ) : field = SEARCH FIELD MAP . get ( field , field ) if field is None : continue for value in values : if field == 'year' : value = transform year ( value ) if value is not None : result . append ( '%s:%d' % ( field , value ) ) elif field == 'any' : result . append ( '\"%s\"' % value ) else : result . append ( '%s:\"%s\"' % ( field , value ) ) return ' ' . join ( result )", "predictions": ["construct a publish query from a query query"], "references": ["translate a mopidy search query to a spotify search query"], "bleu": 0.157044754112095, "rouge_l": 0.43571428571428567}
{"id": 1070, "code": "def parse retry after ( self , response ) : value = response . headers . get ( 'Retry-After' ) if not value : seconds = 0 elif re . match ( r'^\\s*[0-9]+\\s*$' , value ) : seconds = int ( value ) else : date tuple = email . utils . parsedate ( value ) if date tuple is None : seconds = 0 else : seconds = time . mktime ( date tuple ) - time . time ( ) return max ( 0 , seconds )", "predictions": ["block for retry self ."], "references": ["parse retry - after header from response if it is set ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 1071, "code": "def set default headers ( self , * args , * * kwargs ) : self . set header ( 'Access-Control-Allow-Origin' , '*' ) self . set header ( 'Access-Control-Allow-Headers' , 'Origin, X-Requested-With, Content-Type, Accept' ) self . set header ( 'Access-Control-Allow-Methods' , 'GET, HEAD, PUT, POST, DELETE' )", "predictions": ["sets the shared fields to be used in the request"], "references": ["set the default headers for all requests ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 1072, "code": "def prepare ( self ) : host = self . request . headers . get ( 'Host' , None ) if host is not None and host in self . hosts : return raise tornado . web . HTTP Error ( 403 )", "predictions": ["mkdir the mode and raise the request ."], "references": ["validate host header ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 1073, "code": "def start ( self ) : self . service info = Service Info ( ' webthing. tcp.local.' , '{}. webthing. tcp.local.' . format ( self . name ) , address = socket . inet aton ( get ip ( ) ) , port = self . port , properties = { 'path' : '/' , } , server = '{}.local.' . format ( socket . gethostname ( ) ) ) self . zeroconf = Zeroconf ( ) self . zeroconf . register service ( self . service info ) self . server . listen ( self . port ) tornado . ioloop . IO Loop . current ( ) . start ( )", "predictions": ["success the connection to the server any required clients any that can be started"], "references": ["start listening for incoming connections ."], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 1074, "code": "def start ( self ) : self . status = 'pending' self . thing . action notify ( self ) self . perform action ( ) self . finish ( )", "predictions": ["skipped the . ."], "references": ["start performing the action ."], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 1075, "code": "def finish ( self ) : self . status = 'completed' self . time completed = timestamp ( ) self . thing . action notify ( self )", "predictions": ["mark this event as finished . . . ."], "references": ["finish performing the action ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 1076, "code": "def update ( self , * * fields ) : self . for write = True if django . VERSION >= ( 2 , 0 ) : query = self . query . chain ( Update Query ) else : query = self . query . clone ( Update Query ) query . annotations = None query . add update values ( fields ) connection = django . db . connections [ self . db ] compiler = Postgres Returning Update Compiler ( query , connection , self . db ) with transaction . atomic ( using = self . db , savepoint = False ) : rows = compiler . execute sql ( CURSOR ) self . result cache = None for row in rows : signals . update . send ( self . model , pk = row [ 0 ] ) return len ( rows )", "predictions": ["is the django a single query"], "references": ["updates all rows that match the filter ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 1077, "code": "def on model save ( sender , * * kwargs ) : created , instance = kwargs [ 'created' ] , kwargs [ 'instance' ] if created : signals . create . send ( sender , pk = instance . pk ) else : signals . update . send ( sender , pk = instance . pk )", "predictions": ["update object when saving signals"], "references": ["when a model gets created or updated ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1078, "code": "def on model delete ( sender , * * kwargs ) : instance = kwargs [ 'instance' ] signals . delete . send ( sender , pk = instance . pk )", "predictions": ["steps steps that don t have a value"], "references": ["when a model gets deleted ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 1079, "code": "def resolve expression ( self , * args , * * kwargs ) : result = dict ( ) for key , value in self . value . items ( ) : if hasattr ( value , 'resolve expression' ) : result [ key ] = value . resolve expression ( * args , * * kwargs ) else : result [ key ] = value return H Store Value ( result )", "predictions": ["create a . cursor from the given cursor . ."], "references": ["resolves expressions inside the dictionary ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 1080, "code": "def as sql ( self , compiler , connection ) : qn = compiler . quote name unless alias return \"%s.%s->'%s'\" % ( qn ( self . alias ) , qn ( self . target . column ) , self . hstore key ) , [ ]", "predictions": ["return the sql as a sql"], "references": ["compiles this expression into sql ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 1081, "code": "def relabeled clone ( self , relabels ) : return self . class ( relabels . get ( self . alias , self . alias ) , self . target , self . hstore key , self . output field )", "predictions": ["clone clone instance for make sure this instance exists"], "references": ["gets a re - labeled clone of this expression ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 1082, "code": "def as sql ( self , compiler , connection ) : sql , params = super ( ) . as sql ( compiler , connection ) return 'EXTRACT(epoch FROM {})' . format ( sql ) , params", "predictions": ["returns sql s sql instance"], "references": ["compiles this expression into sql ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 1083, "code": "def create model ( self , model ) : for field in model . meta . local fields : if not isinstance ( field , H Store Field ) : continue self . add field ( model , field )", "predictions": ["apply the instance of the given rel add it to the model add them to the model add it if it was created add it ."], "references": ["ran when a new model is created ."], "bleu": 0.05551277111446364, "rouge_l": 0.19509594882729217}
{"id": 1084, "code": "def delete model ( self , model ) : for field in model . meta . local fields : if not isinstance ( field , H Store Field ) : continue self . remove field ( model , field )", "predictions": ["calc a c { tablename } . instance . }"], "references": ["ran when a model is being deleted ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 1085, "code": "def alter db table ( self , model , old db table , new db table ) : for field in model . meta . local fields : if not isinstance ( field , H Store Field ) : continue for key in self . iterate required keys ( field ) : self . rename hstore required ( old db table , new db table , field , field , key )", "predictions": ["run the hstore self . in the db self ."], "references": ["ran when the name of a model is changed ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 1086, "code": "def add field ( self , model , field ) : for key in self . iterate required keys ( field ) : self . create hstore required ( model . meta . db table , field , key )", "predictions": ["index field for all sorted keys ."], "references": ["ran when a field is added to a model ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 1087, "code": "def remove field ( self , model , field ) : for key in self . iterate required keys ( field ) : self . drop hstore required ( model . meta . db table , field , key )", "predictions": ["def close close close close model in the model"], "references": ["ran when a field is removed from a model ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 1088, "code": "def alter field ( self , model , old field , new field , strict = False ) : is old field hstore = isinstance ( old field , H Store Field ) is new field hstore = isinstance ( new field , H Store Field ) if not is old field hstore and not is new field hstore : return old required = getattr ( old field , 'required' , [ ] ) or [ ] new required = getattr ( new field , 'required' , [ ] ) or [ ] if str ( old field . column ) != str ( new field . column ) : for key in self . iterate required keys ( old field ) : self . rename hstore required ( model . meta . db table , model . meta . db table , old field , new field , key ) for key in old required : if key not in new required : self . drop hstore required ( model . meta . db table , old field , key ) for key in new required : if key not in old required : self . create hstore required ( model . meta . db table , new field , key )", "predictions": ["alter the hstore field of the model and rename it ."], "references": ["ran when the configuration on a field changed ."], "bleu": 0.1354599427337814, "rouge_l": 0.3055091819699499}
{"id": 1089, "code": "def create hstore required ( self , table name , field , key ) : name = self . required constraint name ( table name , field , key ) sql = self . sql hstore required create . format ( name = self . quote name ( name ) , table = self . quote name ( table name ) , field = self . quote name ( field . column ) , key = key ) self . execute ( sql )", "predictions": ["create a hstore field in the model ."], "references": ["creates a required constraint for the specified hstore key ."], "bleu": 0.157044754112095, "rouge_l": 0.3267857142857143}
{"id": 1090, "code": "def drop hstore required ( self , table name , field , key ) : name = self . required constraint name ( table name , field , key ) sql = self . sql hstore required drop . format ( table = self . quote name ( table name ) , name = self . quote name ( name ) ) self . execute ( sql )", "predictions": ["remove hstore from the table ."], "references": ["drops a required constraint for the specified hstore key ."], "bleu": 0.13487005099534619, "rouge_l": 0.23921568627450981}
{"id": 1091, "code": "def create sql ( self , model , schema editor , using = '' ) : if django . VERSION >= ( 2 , 0 ) : statement = super ( ) . create sql ( model , schema editor , using ) statement . template = self . sql create index statement . parts [ 'condition' ] = self . condition return statement else : sql create index = self . sql create index sql parameters = { * * Index . get sql create template values ( self , model , schema editor , using ) , 'condition' : self . condition } return sql create index % sql parameters", "predictions": ["create sql and index index for the given model ."], "references": ["creates the actual sql used when applying the migration ."], "bleu": 0.14991106946711685, "rouge_l": 0.3}
{"id": 1092, "code": "def create command ( text , commands ) : class Custom Command ( Base Command ) : description = text def run ( self ) : for cmd in commands : subprocess . check call ( cmd ) return Custom Command", "predictions": ["create command from text ."], "references": ["creates a custom setup . py command ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 1093, "code": "def create model ( self , model ) : super ( ) . create model ( model ) for mixin in self . post processing mixins : mixin . create model ( model )", "predictions": ["create the model model ."], "references": ["ran when a new model is created ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 1094, "code": "def delete model ( self , model ) : for mixin in self . post processing mixins : mixin . delete model ( model ) super ( ) . delete model ( model )", "predictions": ["delete all processing processing processing models ."], "references": ["ran when a model is being deleted ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1095, "code": "def alter db table ( self , model , old db table , new db table ) : super ( Schema Editor , self ) . alter db table ( model , old db table , new db table ) for mixin in self . post processing mixins : mixin . alter db table ( model , old db table , new db table )", "predictions": ["run the alter table ."], "references": ["ran when the name of a model is changed ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 1096, "code": "def add field ( self , model , field ) : super ( Schema Editor , self ) . add field ( model , field ) for mixin in self . post processing mixins : mixin . add field ( model , field )", "predictions": ["add a field to the model ."], "references": ["ran when a field is added to a model ."], "bleu": 0.20958712452883563, "rouge_l": 0.5700934579439253}
{"id": 1097, "code": "def remove field ( self , model , field ) : for mixin in self . post processing mixins : mixin . remove field ( model , field ) super ( Schema Editor , self ) . remove field ( model , field )", "predictions": ["remove field from model ."], "references": ["ran when a field is removed from a model ."], "bleu": 0.1501861529550426, "rouge_l": 0.5030927835051546}
{"id": 1098, "code": "def alter field ( self , model , old field , new field , strict = False ) : super ( Schema Editor , self ) . alter field ( model , old field , new field , strict ) for mixin in self . post processing mixins : mixin . alter field ( model , old field , new field , strict )", "predictions": ["alter the model field for the model ."], "references": ["ran when the configuration on a field changed ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 1099, "code": "def form returning ( self ) : qn = self . connection . ops . quote name return ' RETURNING %s' % qn ( self . query . model . meta . pk . attname )", "predictions": ["return the form to be used in the query ."], "references": ["builds the returning part of the query ."], "bleu": 0.24808415001701817, "rouge_l": 0.4535315985130111}
{"id": 1100, "code": "def as sql ( self , return id = False ) : queries = [ self . rewrite insert ( sql , params , return id ) for sql , params in super ( ) . as sql ( ) ] return queries", "predictions": ["returns the sql queries as sql ."], "references": ["builds the sql insert statement ."], "bleu": 0.2626909894424158, "rouge_l": 0.4680306905370844}
{"id": 1101, "code": "def alter db table ( self , model , old db table , new db table ) : for field in model . meta . local fields : if not isinstance ( field , H Store Field ) : continue for keys in self . iterate uniqueness keys ( field ) : self . rename hstore unique ( old db table , new db table , field , field , keys )", "predictions": ["run the unique table table ."], "references": ["ran when the name of a model is changed ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 1102, "code": "def add field ( self , model , field ) : for keys in self . iterate uniqueness keys ( field ) : self . create hstore unique ( model , field , keys )", "predictions": ["add a unique field to the model ."], "references": ["ran when a field is added to a model ."], "bleu": 0.19546825878823415, "rouge_l": 0.5446428571428571}
{"id": 1103, "code": "def remove field ( self , model , field ) : for keys in self . iterate uniqueness keys ( field ) : self . drop hstore unique ( model , field , keys )", "predictions": ["remove a field from the model ."], "references": ["ran when a field is removed from a model ."], "bleu": 0.20958712452883563, "rouge_l": 0.5700934579439253}
{"id": 1104, "code": "def alter field ( self , model , old field , new field , strict = False ) : is old field hstore = isinstance ( old field , H Store Field ) is new field hstore = isinstance ( new field , H Store Field ) if not is old field hstore and not is new field hstore : return old uniqueness = getattr ( old field , 'uniqueness' , [ ] ) or [ ] new uniqueness = getattr ( new field , 'uniqueness' , [ ] ) or [ ] if str ( old field . column ) != str ( new field . column ) : for keys in self . iterate uniqueness keys ( old field ) : self . rename hstore unique ( model . meta . db table , model . meta . db table , old field , new field , keys ) for keys in old uniqueness : if keys not in new uniqueness : self . drop hstore unique ( model , old field , self . compose keys ( keys ) ) for keys in new uniqueness : if keys not in old uniqueness : self . create hstore unique ( model , new field , self . compose keys ( keys ) )", "predictions": ["this method runs the hstore field of a model"], "references": ["ran when the configuration on a field changed ."], "bleu": 0.16784459625186196, "rouge_l": 0.2222222222222222}
{"id": 1105, "code": "def create hstore unique ( self , model , field , keys ) : name = self . unique constraint name ( model . meta . db table , field , keys ) columns = [ '(%s->\\'%s\\')' % ( field . column , key ) for key in keys ] sql = self . sql hstore unique create . format ( name = self . quote name ( name ) , table = self . quote name ( model . meta . db table ) , columns = ',' . join ( columns ) ) self . execute ( sql )", "predictions": ["create unique hstore for the given model"], "references": ["creates a unique constraint for the specified hstore keys ."], "bleu": 0.18094495256969623, "rouge_l": 0.34205607476635513}
{"id": 1106, "code": "def drop hstore unique ( self , model , field , keys ) : name = self . unique constraint name ( model . meta . db table , field , keys ) sql = self . sql hstore unique drop . format ( name = self . quote name ( name ) ) self . execute ( sql )", "predictions": ["drop hstore and execute hstore ."], "references": ["drops a unique constraint for the specified hstore keys ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 1107, "code": "def as sql ( self , compiler , connection ) -> Tuple [ str , List [ Any ] ] : sql , params = super ( ) . as sql ( compiler , connection ) qn = compiler . quote name unless alias extra conditions = ' AND ' . join ( [ '{}.{} = %s' . format ( qn ( self . table name ) , qn ( field . column ) ) for field , value in self . extra conditions ] ) for , value in self . extra conditions : params . append ( value ) rewritten sql = sql . replace ( ')' , ' AND {})' . format ( extra conditions ) ) return rewritten sql , params", "predictions": ["return sql as sql ."], "references": ["compiles this join into a sql string ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 1108, "code": "def select ( self , board ) : if self . unexplored : i = random . randrange ( len ( self . unexplored ) ) pos = self . unexplored [ i ] self . unexplored [ i ] = self . unexplored [ len ( self . unexplored ) - 1 ] self . unexplored . pop ( ) return pos elif self . bestchild : return self . bestchild . pos else : return PASS", "predictions": ["select a random board"], "references": ["select move ; unexplored children first then according to uct value"], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 1109, "code": "def random playout ( self , board ) : for x in range ( MAXMOVES ) : if board . finished : break board . move ( board . random move ( ) )", "predictions": ["move a random playout to the given board ."], "references": ["random play until both players pass"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 1110, "code": "def Get Domain ( self ) : return ( self . knots [ self . degree - 1 ] , self . knots [ len ( self . knots ) - self . degree ] )", "predictions": ["get the knots object for this degree ."], "references": ["returns the domain of the b - spline"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 1111, "code": "def parse posts ( self , raw posts ) : parsed posts = self . parse json ( raw posts ) for post id in parsed posts [ 'order' ] : yield parsed posts [ 'posts' ] [ post id ]", "predictions": ["parse posts from the json response ."], "references": ["parse posts and returns in order ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 1112, "code": "def posts ( self , channel , page = None ) : entrypoint = self . RCHANNELS + '/' + channel + '/' + self . RPOSTS params = { self . PPER PAGE : self . max items } if page is not None : params [ self . PPAGE ] = page response = self . fetch ( entrypoint , params ) return response", "predictions": ["fetch posts for a specific channel ."], "references": ["fetch the history of a channel ."], "bleu": 0.2777619034011791, "rouge_l": 0.5714285714285714}
{"id": 1113, "code": "def user ( self , user ) : entrypoint = self . RUSERS + '/' + user response = self . fetch ( entrypoint , None ) return response", "predictions": ["fetch a user by user ."], "references": ["fetch user data ."], "bleu": 0.2626909894424158, "rouge_l": 0.6224489795918368}
{"id": 1114, "code": "def pre init ( self ) : if not self . parsed args . mboxes path : base path = os . path . expanduser ( '~/.perceval/mailinglists/' ) dirpath = os . path . join ( base path , self . parsed args . url ) else : dirpath = self . parsed args . mboxes path setattr ( self . parsed args , 'dirpath' , dirpath )", "predictions": ["pre - init init directory"], "references": ["initialize mailing lists directory path"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 1115, "code": "def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , archive = True ) parser . parser . add argument ( 'url' , help = \"URL of the RSS feed\" ) return parser", "predictions": ["setup setup parser ."], "references": ["returns the rss argument parser ."], "bleu": 0.2868106410131918, "rouge_l": 0.3860759493670886}
{"id": 1116, "code": "def fetch merge requests ( self , from date ) : merges groups = self . client . merges ( from date = from date ) for raw merges in merges groups : merges = json . loads ( raw merges ) for merge in merges : merge id = merge [ 'iid' ] if self . blacklist ids and merge id in self . blacklist ids : logger . warning ( \"Skipping blacklisted merge request %s\" , merge id ) continue merge full raw = self . client . merge ( merge id ) merge full = json . loads ( merge full raw ) self . init merge extra fields ( merge full ) merge full [ 'notes data' ] = self . get merge notes ( merge id ) merge full [ 'award emoji data' ] = self . get award emoji ( Git Lab Client . MERGES , merge id ) merge full [ 'versions data' ] = self . get merge versions ( merge id ) yield merge full", "predictions": ["fetch all requests from the given date"], "references": ["fetch the merge requests"], "bleu": 0.22089591134157885, "rouge_l": 0.3824451410658307}
{"id": 1117, "code": "def issues ( self , from date = None ) : payload = { 'state' : 'all' , 'order by' : 'updated at' , 'sort' : 'asc' , 'per page' : PER PAGE } if from date : payload [ 'updated after' ] = from date . isoformat ( ) return self . fetch items ( Git Lab Client . ISSUES , payload )", "predictions": ["list all issues for a specific date ."], "references": ["get the issues from pagination"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 1118, "code": "def merges ( self , from date = None ) : payload = { 'state' : 'all' , 'order by' : 'updated at' , 'sort' : 'asc' , 'view' : 'simple' , 'per page' : PER PAGE } if from date : payload [ 'updated after' ] = from date . isoformat ( ) return self . fetch items ( Git Lab Client . MERGES , payload )", "predictions": ["list all items that are not provided ."], "references": ["get the merge requests from pagination"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1119, "code": "def merge ( self , merge id ) : path = urijoin ( self . base url , Git Lab Client . PROJECTS , self . owner + '%2F' + self . repository , Git Lab Client . MERGES , merge id ) response = self . fetch ( path ) return response . text", "predictions": ["merge a merge merge with the given id ."], "references": ["get the merge full data"], "bleu": 0.15619699684601276, "rouge_l": 0.1506172839506173}
{"id": 1120, "code": "def merge versions ( self , merge id ) : payload = { 'order by' : 'updated at' , 'sort' : 'asc' , 'per page' : PER PAGE } path = urijoin ( Git Lab Client . MERGES , str ( merge id ) , Git Lab Client . VERSIONS ) return self . fetch items ( path , payload )", "predictions": ["alter field field with alter model . . . . . . . . . . ."], "references": ["get the merge versions from pagination"], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 1121, "code": "def merge version ( self , merge id , version id ) : path = urijoin ( self . base url , Git Lab Client . PROJECTS , self . owner + '%2F' + self . repository , Git Lab Client . MERGES , merge id , Git Lab Client . VERSIONS , version id ) response = self . fetch ( path ) return response . text", "predictions": ["create a create hstore hstore ."], "references": ["get merge version detail"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1122, "code": "def notes ( self , item type , item id ) : payload = { 'order by' : 'updated at' , 'sort' : 'asc' , 'per page' : PER PAGE } path = urijoin ( item type , str ( item id ) , Git Lab Client . NOTES ) return self . fetch items ( path , payload )", "predictions": ["fetch an drop an self . ."], "references": ["get the notes from pagination"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1123, "code": "def emojis ( self , item type , item id ) : payload = { 'order by' : 'updated at' , 'sort' : 'asc' , 'per page' : PER PAGE } path = urijoin ( item type , str ( item id ) , Git Lab Client . EMOJI ) return self . fetch items ( path , payload )", "predictions": ["{ % } for a specific item"], "references": ["get emojis from pagination"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1124, "code": "def note emojis ( self , item type , item id , note id ) : payload = { 'order by' : 'updated at' , 'sort' : 'asc' , 'per page' : PER PAGE } path = urijoin ( item type , str ( item id ) , Git Lab Client . NOTES , str ( note id ) , Git Lab Client . EMOJI ) return self . fetch items ( path , payload )", "predictions": ["create an commands for a specific commands . ."], "references": ["get emojis of a note"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 1125, "code": "def fetch items ( self , path , payload ) : page = 0 last page = None url next = urijoin ( self . base url , Git Lab Client . PROJECTS , self . owner + '%2F' + self . repository , path ) logger . debug ( \"Get Git Lab paginated items from \" + url next ) response = self . fetch ( url next , payload = payload ) items = response . text page += 1 if 'last' in response . links : last url = response . links [ 'last' ] [ 'url' ] last page = last url . split ( '&page=' ) [ 1 ] . split ( '&' ) [ 0 ] last page = int ( last page ) logger . debug ( \"Page: %i/%i\" % ( page , last page ) ) while items : yield items items = None if 'next' in response . links : url next = response . links [ 'next' ] [ 'url' ] response = self . fetch ( url next , payload = payload ) page += 1 items = response . text logger . debug ( \"Page: %i/%i\" % ( page , last page ) )", "predictions": ["create model model from path"], "references": ["return the items from gitlab api using links pagination"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 1126, "code": "def init rate limit ( self ) : url = urijoin ( self . base url , 'projects' , self . owner + '%2F' + self . repository ) try : response = super ( ) . fetch ( url ) self . update rate limit ( response ) except requests . exceptions . HTTP Error as error : if error . response . status code == 401 : raise error else : logger . warning ( \"Rate limit not initialized: %s\" , error )", "predictions": ["delete the model limit limit ."], "references": ["initialize rate limit information"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 1127, "code": "def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , token auth = True , archive = True ) group = parser . parser . add argument group ( 'Git Lab arguments' ) group . add argument ( '--enterprise-url' , dest = 'base url' , help = \"Base URL for Git Lab Enterprise instance\" ) group . add argument ( '--sleep-for-rate' , dest = 'sleep for rate' , action = 'store true' , help = \"sleep for getting more rate\" ) group . add argument ( '--min-rate-to-sleep' , dest = 'min rate to sleep' , default = MIN RATE LIMIT , type = int , help = ) group . add argument ( '--blacklist-ids' , dest = 'blacklist ids' , nargs = '*' , type = int , help = \"Ids of items that must not be retrieved.\" ) group . add argument ( '--max-retries' , dest = 'max retries' , default = MAX RETRIES , type = int , help = \"number of API call retries\" ) group . add argument ( '--sleep-time' , dest = 'sleep time' , default = DEFAULT SLEEP TIME , type = int , help = \"sleeping time between API call retries\" ) parser . parser . add argument ( 'owner' , help = \"Git Lab owner\" ) parser . parser . add argument ( 'repository' , help = \"Git Lab repository\" ) return parser", "predictions": ["alter the db table table . ."], "references": ["returns the gitlab argument parser ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 1128, "code": "def channel info ( self , channel ) : resource = self . RCHANNEL INFO params = { self . PCHANNEL : channel , } response = self . fetch ( resource , params ) return response", "predictions": ["list all add information for a add add add a add add add add add add to a add add add add a add add add add add add add add"], "references": ["fetch information about a channel ."], "bleu": 0.04317900023606586, "rouge_l": 0.12310797174571139}
{"id": 1129, "code": "def history ( self , channel , oldest = None , latest = None ) : resource = self . RCHANNEL HISTORY params = { self . PCHANNEL : channel , self . PCOUNT : self . max items } if oldest is not None : params [ self . POLDEST ] = oldest if latest is not None : params [ self . PLATEST ] = latest response = self . fetch ( resource , params ) return response", "predictions": ["list available remove remove a specific model"], "references": ["fetch the history of a channel ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 1130, "code": "def user ( self , user id ) : resource = self . RUSER INFO params = { self . PUSER : user id } response = self . fetch ( resource , params ) return response", "predictions": ["gets a alter by id"], "references": ["fetch user info ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1131, "code": "def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , token auth = True , archive = True ) action = parser . parser . option string actions [ '--api-token' ] action . required = True group = parser . parser . add argument group ( 'Slack arguments' ) group . add argument ( '--max-items' , dest = 'max items' , type = int , default = MAX ITEMS , help = \"Maximum number of items requested on the same query\" ) parser . parser . add argument ( 'channel' , help = \"Slack channel identifier\" ) return parser", "predictions": ["form the command line options name name name name name name name name name name name name name name name name name name name name name name name name to use"], "references": ["returns the slack argument parser ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 1132, "code": "def logout ( self ) : params = { self . PLOGOUT : '1' } self . call ( self . CGI LOGIN , params ) self . close http session ( ) logger . debug ( \"Bugzilla user logged out from %s\" , self . base url )", "predictions": ["closes the user and as logged"], "references": ["logout from the server ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 1133, "code": "def metadata ( self ) : params = { self . PCTYPE : self . CTYPE XML } response = self . call ( self . CGI BUG , params ) return response", "predictions": ["if the meta data is a list of all the alter new alter new alter new one ."], "references": ["get metadata information in xml format ."], "bleu": 0.06809398432036522, "rouge_l": 0.08689458689458689}
{"id": 1134, "code": "def events ( self , group , from date = DEFAULT DATETIME ) : date = datetime to utc ( from date ) date = date . strftime ( \"since:%Y-%m-%d T%H:%M:%S.000Z\" ) resource = urijoin ( group , self . REVENTS ) fixed params = '?' + self . PFIELDS + '=' + ',' . join ( self . VEVENT FIELDS ) fixed params += '&' + self . PSTATUS + '=' + ',' . join ( self . VSTATUS ) resource += fixed params params = { self . PORDER : self . VUPDATED , self . PSCROLL : date , self . PPAGE : self . max items } try : for page in self . fetch ( resource , params ) : yield page except requests . exceptions . HTTP Error as error : if error . response . status code == 410 : msg = \"Group is no longer accessible: {}\" . format ( error ) raise Repository Error ( cause = msg ) else : raise error", "predictions": ["retrieve all add add add add add add add add add add add add add add add add add add add add add add add add add add add add add"], "references": ["fetch the events pages of a given group ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1135, "code": "def comments ( self , group , event id ) : resource = urijoin ( group , self . REVENTS , event id , self . RCOMMENTS ) params = { self . PPAGE : self . max items } for page in self . fetch ( resource , params ) : yield page", "predictions": ["list all remove remove remove remove the remove remove remove the model iterate"], "references": ["fetch the comments of a given event ."], "bleu": 0.09552040806823771, "rouge_l": 0.09951060358890701}
{"id": 1136, "code": "def rsvps ( self , group , event id ) : resource = urijoin ( group , self . REVENTS , event id , self . RRSVPS ) fixed params = '?' + self . PFIELDS + '=' + ',' . join ( self . VRSVP FIELDS ) fixed params += '&' + self . PRESPONSE + '=' + ',' . join ( self . VRESPONSE ) resource += fixed params params = { self . PPAGE : self . max items } for page in self . fetch ( resource , params ) : yield page", "predictions": ["rename an model ."], "references": ["fetch the rsvps of a given event ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 1137, "code": "def parse reviews ( raw data ) : items raw = \"[\" + raw data . replace ( \"\\n\" , \",\" ) + \"]\" items raw = items raw . replace ( \",]\" , \"]\" ) items = json . loads ( items raw ) reviews = [ ] for item in items : if 'project' in item . keys ( ) : reviews . append ( item ) return reviews", "predictions": ["create hstore from raw self name name name name name name name name name name name name name name name name name name name name name name name name name name"], "references": ["parse a gerrit reviews list ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1138, "code": "def version ( self ) : if self . version : return self . version cmd = self . gerrit cmd + \" %s \" % ( Gerrit Client . CMD VERSION ) logger . debug ( \"Getting version: %s\" % ( cmd ) ) raw data = self . execute ( cmd ) raw data = str ( raw data , \"UTF-8\" ) logger . debug ( \"Gerrit version: %s\" % ( raw data ) ) m = re . match ( Gerrit Client . VERSION REGEX , raw data ) if not m : cause = \"Invalid gerrit version %s\" % raw data raise Backend Error ( cause = cause ) try : mayor = int ( m . group ( 1 ) ) minor = int ( m . group ( 2 ) ) except Exception : cause = \"Gerrit client could not determine the server version.\" raise Backend Error ( cause = cause ) self . version = [ mayor , minor ] return self . version", "predictions": ["get the drop - drop drop - drop - drop - drop - drop - drop - drop - drop - drop raw"], "references": ["return the gerrit server version ."], "bleu": 0.05291907393644996, "rouge_l": 0.07711757269279393}
{"id": 1139, "code": "def reviews ( self , last item , filter = None ) : cmd = self . get gerrit cmd ( last item , filter ) logger . debug ( \"Getting reviews with command: %s\" , cmd ) raw data = self . execute ( cmd ) raw data = str ( raw data , \"UTF-8\" ) return raw data", "predictions": ["perform a gerrit command"], "references": ["get the reviews starting from last_item ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 1140, "code": "def next retrieve group item ( self , last item = None , entry = None ) : next item = None gerrit version = self . version if gerrit version [ 0 ] == 2 and gerrit version [ 1 ] > 9 : if last item is None : next item = 0 else : next item = last item elif gerrit version [ 0 ] == 2 and gerrit version [ 1 ] == 9 : cause = \"Gerrit 2.9.0 does not support pagination\" raise Backend Error ( cause = cause ) else : if entry is not None : next item = entry [ 'sort Key' ] return next item", "predictions": ["return the select item item"], "references": ["return the item to start from in next reviews group ."], "bleu": 0.11629030063732083, "rouge_l": 0.35124760076775424}
{"id": 1141, "code": "def execute from archive ( self , cmd ) : cmd = self . sanitize for archive ( cmd ) response = self . archive . retrieve ( cmd , None , None ) if isinstance ( response , Runtime Error ) : raise response return response", "predictions": ["random archive command from by archive ."], "references": ["execute gerrit command against the archive"], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 1142, "code": "def execute from remote ( self , cmd ) : result = None retries = 0 while retries < self . MAX RETRIES : try : result = subprocess . check output ( cmd , shell = True ) break except subprocess . Called Process Error as ex : logger . error ( \"gerrit cmd %s failed: %s\" , cmd , ex ) time . sleep ( self . RETRY WAIT * retries ) retries += 1 if result is None : result = Runtime Error ( cmd + \" failed \" + str ( self . MAX RETRIES ) + \" times. Giving up!\" ) if self . archive : cmd = self . sanitize for archive ( cmd ) self . archive . store ( cmd , None , None , result ) if isinstance ( result , Runtime Error ) : raise result return result", "predictions": ["execute from from from"], "references": ["execute gerrit command with retry if it fails"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 1143, "code": "def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , archive = True ) group = parser . parser . add argument group ( 'Gerrit arguments' ) group . add argument ( '--user' , dest = 'user' , help = \"Gerrit ssh user\" ) group . add argument ( '--max-reviews' , dest = 'max reviews' , type = int , default = MAX REVIEWS , help = \"Max number of reviews per ssh query.\" ) group . add argument ( '--blacklist-reviews' , dest = 'blacklist reviews' , nargs = '*' , help = \"Wrong reviews that must not be retrieved.\" ) group . add argument ( '--disable-host-key-check' , dest = 'disable host key check' , action = 'store true' , help = \"Don't check remote host identity\" ) group . add argument ( '--ssh-port' , dest = 'port' , default = PORT , type = int , help = \"Set SSH port of the Gerrit server\" ) parser . parser . add argument ( 'hostname' , help = \"Hostname of the Gerrit server\" ) return parser", "predictions": ["parse command - line arguments for the plugin for the program for the group for the program for the program for the program for the plugin for the given date for"], "references": ["returns the gerrit argument parser ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 1144, "code": "def fetch issue data ( self , issue id ) : raw issue = self . client . issue ( issue id ) issue = json . loads ( raw issue ) return issue", "predictions": ["posts an issue by issue"], "references": ["get data associated to an issue"], "bleu": 0.2941733261715515, "rouge_l": 0.3577712609970674}
{"id": 1145, "code": "def fetch issue attachments ( self , issue id ) : for attachments raw in self . client . issue collection ( issue id , \"attachments\" ) : attachments = json . loads ( attachments raw ) for attachment in attachments [ 'entries' ] : yield attachment", "predictions": ["user - compute self response from issue"], "references": ["get attachments of an issue"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1146, "code": "def fetch issue messages ( self , issue id ) : for messages raw in self . client . issue collection ( issue id , \"messages\" ) : messages = json . loads ( messages raw ) for msg in messages [ 'entries' ] : msg [ 'owner data' ] = self . fetch user data ( '{OWNER}' , msg [ 'owner link' ] ) yield msg", "predictions": ["pre - order messages for a given init"], "references": ["get messages of an issue"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 1147, "code": "def fetch issue activities ( self , issue id ) : for activities raw in self . client . issue collection ( issue id , \"activity\" ) : activities = json . loads ( activities raw ) for act in activities [ 'entries' ] : act [ 'person data' ] = self . fetch user data ( '{PERSON}' , act [ 'person link' ] ) yield act", "predictions": ["setup cmd parser for a given cmd id ."], "references": ["get activities on an issue"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1148, "code": "def fetch user data ( self , tag type , user link ) : user name = self . client . user name ( user link ) user = { } if not user name : return user user raw = self . client . user ( user name ) user = json . loads ( user raw ) return user", "predictions": ["fetch merge requests from merge link"], "references": ["get data associated to an user"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1149, "code": "def issues ( self , start = None ) : payload = self . build payload ( size = self . items per page , operation = True , startdate = start ) path = self . get url project ( ) return self . fetch items ( path = path , payload = payload )", "predictions": ["fetch data from the api payload payload payload payload payload payload payload payload payload payload payload payload payload payload payload payload"], "references": ["get the issues from pagination"], "bleu": 0.06429451441231726, "rouge_l": 0.08652482269503546}
{"id": 1150, "code": "def user ( self , user name ) : user = None if user name in self . users : return self . users [ user name ] url user = self . get url ( \"~\" + user name ) logger . info ( \"Getting info for %s\" % ( url user ) ) try : raw user = self . send request ( url user ) user = raw user except requests . exceptions . HTTP Error as e : if e . response . status code in [ 404 , 410 ] : logger . warning ( \"Data is not available - %s\" , url user ) user = '{}' else : raise e self . users [ user name ] = user return user", "predictions": ["get merges merges data from device"], "references": ["get the user data by url"], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 1151, "code": "def issue ( self , issue id ) : path = urijoin ( \"bugs\" , str ( issue id ) ) url issue = self . get url ( path ) raw text = self . send request ( url issue ) return raw text", "predictions": ["return a merge request ."], "references": ["get the issue data by its id"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 1152, "code": "def issue collection ( self , issue id , collection name ) : path = urijoin ( \"bugs\" , str ( issue id ) , collection name ) url collection = self . get url ( path ) payload = { 'ws.size' : self . items per page , 'ws.start' : 0 , 'order by' : 'date last updated' } raw items = self . fetch items ( path = url collection , payload = payload ) return raw items", "predictions": ["fetch an issue collection for a specific issue ."], "references": ["get a collection list of a given issue"], "bleu": 0.16784459625186196, "rouge_l": 0.35672514619883033}
{"id": 1153, "code": "def fetch items ( self , path , payload ) : page = 0 url next = path fetch data = True while fetch data : logger . debug ( \"Fetching page: %i\" , page ) try : raw content = self . send request ( url next , payload ) content = json . loads ( raw content ) except requests . exceptions . HTTP Error as e : if e . response . status code in [ 410 ] : logger . warning ( \"Data is not available - %s\" , url next ) raw content = '{\"total size\": 0, \"start\": 0, \"entries\": []}' content = json . loads ( raw content ) else : raise e if 'next collection link' in content : url next = content [ 'next collection link' ] payload = None else : fetch data = False yield raw content page += 1", "predictions": ["fetch items from path"], "references": ["return the items from launchpad api using pagination"], "bleu": 0.1739594473063345, "rouge_l": 0.31443298969072164}
{"id": 1154, "code": "def find group id ( self ) : group subscriptions = self . subscriptions ( self . auth ) for subscriptions in group subscriptions : for sub in subscriptions : if sub [ 'group name' ] == self . group name : return sub [ 'group id' ] msg = \"Group id not found for group name %s\" % self . group name raise Backend Error ( cause = msg )", "predictions": ["find the subscriptions id of the group ."], "references": ["find the id of a group given its name by iterating on the list of subscriptions"], "bleu": 0.10980266522628493, "rouge_l": 0.3930412371134021}
{"id": 1155, "code": "def fetch ( self , url , payload ) : r = requests . get ( url , params = payload , auth = self . auth , verify = self . verify ) try : r . raise for status ( ) except requests . exceptions . HTTP Error as e : raise e return r", "predictions": ["do a fetch of the given url and save it to the api"], "references": ["fetch requests from groupsio api"], "bleu": 0.10571070857151538, "rouge_l": 0.24158415841584158}
{"id": 1156, "code": "def pre init ( self ) : if not self . parsed args . mboxes path : base path = os . path . expanduser ( '~/.perceval/mailinglists/' ) dirpath = os . path . join ( base path , GROUPSIO URL , 'g' , self . parsed args . group name ) else : dirpath = self . parsed args . mboxes path setattr ( self . parsed args , 'dirpath' , dirpath )", "predictions": ["initialize the pre - init directory ."], "references": ["initialize mailing lists directory path"], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 1157, "code": "def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , token auth = True ) action = parser . parser . option string actions [ '--api-token' ] action . required = True group = parser . parser . add argument group ( 'Groupsio arguments' ) group . add argument ( '--mboxes-path' , dest = 'mboxes path' , help = \"Path where mbox files will be stored\" ) group . add argument ( '--no-verify' , dest = 'verify' , action = 'store false' , help = \"Value 'True' enable SSL verification\" ) parser . parser . add argument ( 'group name' , help = \"Name of the group on Groups.io\" ) return parser", "predictions": ["setup the plugin parser ."], "references": ["returns the groupsio argument parser ."], "bleu": 0.31610981104846864, "rouge_l": 0.5366568914956013}
{"id": 1158, "code": "def set auth arguments ( self , basic auth = True , token auth = False ) : group = self . parser . add argument group ( 'authentication arguments' ) if basic auth : group . add argument ( '-u' , '--backend-user' , dest = 'user' , help = \"backend user\" ) group . add argument ( '-p' , '--backend-password' , dest = 'password' , help = \"backend password\" ) if token auth : group . add argument ( '-t' , '--api-token' , dest = 'api token' , help = \"backend authentication token / API key\" )", "predictions": ["set arguments for basic auth ."], "references": ["activate authentication arguments parsing"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 1159, "code": "def set archive arguments ( self ) : group = self . parser . add argument group ( 'archive arguments' ) group . add argument ( '--archive-path' , dest = 'archive path' , default = None , help = \"directory path to the archives\" ) group . add argument ( '--no-archive' , dest = 'no archive' , action = 'store true' , help = \"do not archive data\" ) group . add argument ( '--fetch-archive' , dest = 'fetch archive' , action = 'store true' , help = \"fetch data from the archives\" ) group . add argument ( '--archived-since' , dest = 'archived since' , default = '1970-01-01' , help = \"retrieve items archived since the given date\" )", "predictions": ["set arguments for the archive ."], "references": ["activate archive arguments parsing"], "bleu": 0.24446151121745047, "rouge_l": 0.2074829931972789}
{"id": 1160, "code": "def set output arguments ( self ) : group = self . parser . add argument group ( 'output arguments' ) group . add argument ( '-o' , '--output' , type = argparse . File Type ( 'w' ) , dest = 'outfile' , default = sys . stdout , help = \"output file\" ) group . add argument ( '--json-line' , dest = 'json line' , action = 'store true' , help = \"produce a JSON line for each output item\" )", "predictions": ["set the output arguments for this group ."], "references": ["activate output arguments parsing"], "bleu": 0.21105340631872638, "rouge_l": 0.3546511627906977}
{"id": 1161, "code": "def initialize archive ( self ) : if 'archive path' not in self . parsed args : manager = None elif self . parsed args . no archive : manager = None else : if not self . parsed args . archive path : archive path = os . path . expanduser ( ARCHIVES DEFAULT PATH ) else : archive path = self . parsed args . archive path manager = Archive Manager ( archive path ) self . archive manager = manager", "predictions": ["initialize the manager archive ."], "references": ["initialize archive based on the parsed parameters"], "bleu": 0.21763141204756337, "rouge_l": 0.32360742705570295}
{"id": 1162, "code": "def fetch and parse messages ( self , mailing list , from date ) : from date = datetime to utc ( from date ) nmsgs , imsgs , tmsgs = ( 0 , 0 , 0 ) for mbox in mailing list . mboxes : tmp path = None try : tmp path = self . copy mbox ( mbox ) for message in self . parse mbox ( tmp path ) : tmsgs += 1 if not self . validate message ( message ) : imsgs += 1 continue dt = str to datetime ( message [ M Box . DATE FIELD ] ) if dt < from date : logger . debug ( \"Message %s sent before %s; skipped\" , message [ 'unixfrom' ] , str ( from date ) ) tmsgs -= 1 continue message = self . casedict to dict ( message ) nmsgs += 1 logger . debug ( \"Message %s parsed\" , message [ 'unixfrom' ] ) yield message except ( OS Error , EOF Error ) as e : logger . warning ( \"Ignoring %s mbox due to: %s\" , mbox . filepath , str ( e ) ) except Exception as e : if tmp path and os . path . exists ( tmp path ) : os . remove ( tmp path ) raise e finally : if tmp path and os . path . exists ( tmp path ) : os . remove ( tmp path ) logger . info ( \"Done. %s/%s messages fetched; %s ignored\" , nmsgs , tmsgs , imsgs )", "predictions": ["fetch messages from mailing and parse them into mailing"], "references": ["fetch and parse the messages from a mailing list"], "bleu": 0.25406637407730737, "rouge_l": 0.4444444444444444}
{"id": 1163, "code": "def copy mbox ( self , mbox ) : tmp path = tempfile . mktemp ( prefix = 'perceval ' ) with mbox . container as f in : with open ( tmp path , mode = 'wb' ) as f out : for l in f in : f out . write ( l ) return tmp path", "predictions": ["copy the mbox to the new location"], "references": ["copy the contents of a mbox to a temporary file"], "bleu": 0.20024850746991507, "rouge_l": 0.45607476635514016}
{"id": 1164, "code": "def validate message ( self , message ) : if self . MESSAGE ID FIELD not in message : logger . warning ( \"Field 'Message-ID' not found in message %s; ignoring\" , message [ 'unixfrom' ] ) return False if not message [ self . MESSAGE ID FIELD ] : logger . warning ( \"Field 'Message-ID' is empty in message %s; ignoring\" , message [ 'unixfrom' ] ) return False if self . DATE FIELD not in message : logger . warning ( \"Field 'Date' not found in message %s; ignoring\" , message [ 'unixfrom' ] ) return False if not message [ self . DATE FIELD ] : logger . warning ( \"Field 'Date' is empty in message %s; ignoring\" , message [ 'unixfrom' ] ) return False try : str to datetime ( message [ self . DATE FIELD ] ) except Invalid Date Error : logger . warning ( \"Invalid date %s in message %s; ignoring\" , message [ self . DATE FIELD ] , message [ 'unixfrom' ] ) return False return True", "predictions": ["validate a message against the given message"], "references": ["check if the given message has the mandatory fields"], "bleu": 0.2598013194025897, "rouge_l": 0.3667334669338677}
{"id": 1165, "code": "def get message ( self , key ) : start , stop = self . lookup ( key ) self . file . seek ( start ) from line = self . file . readline ( ) . replace ( mailbox . linesep , b'' ) string = self . file . read ( stop - self . file . tell ( ) ) msg = self . message factory ( string . replace ( mailbox . linesep , b'\\n' ) ) try : msg . set from ( from line [ 5 : ] . decode ( 'ascii' ) ) return msg except Unicode Decode Error : pass try : msg . set from ( from line [ 5 : ] . decode ( 'utf-8' ) ) except Unicode Decode Error : msg . set from ( from line [ 5 : ] . decode ( 'iso-8859-1' ) ) return msg", "predictions": ["get a message from the file ."], "references": ["return a message representation or raise a keyerror ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 1166, "code": "def pre init ( self ) : if self . parsed args . git log : git path = self . parsed args . git log elif not self . parsed args . git path : base path = os . path . expanduser ( '~/.perceval/repositories/' ) processed uri = self . parsed args . uri . lstrip ( '/' ) git path = os . path . join ( base path , processed uri ) + '-git' else : git path = self . parsed args . git path setattr ( self . parsed args , 'gitpath' , git path )", "predictions": ["pre - init git git git git git git git git ."], "references": ["initialize repositories directory path"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 1167, "code": "def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , to date = True ) group = parser . parser . add argument group ( 'Git arguments' ) group . add argument ( '--branches' , dest = 'branches' , nargs = '+' , type = str , default = None , help = \"Fetch commits only from these branches\" ) exgroup = group . add mutually exclusive group ( ) exgroup . add argument ( '--git-path' , dest = 'git path' , help = \"Path where the Git repository will be cloned\" ) exgroup . add argument ( '--git-log' , dest = 'git log' , help = \"Path to the Git log file\" ) exgroup fetch = group . add mutually exclusive group ( ) exgroup fetch . add argument ( '--latest-items' , dest = 'latest items' , action = 'store true' , help = \"Fetch latest commits added to the repository\" ) exgroup fetch . add argument ( '--no-update' , dest = 'no update' , action = 'store true' , help = \"Fetch all commits without updating the repository\" ) parser . parser . add argument ( 'uri' , help = \"URI of the Git log repository\" ) return parser", "predictions": ["setup the command line parser ."], "references": ["returns the git argument parser ."], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 1168, "code": "def parse ( self ) : for line in self . stream : line = line . rstrip ( '\\n' ) parsed = False self . nline += 1 while not parsed : parsed = self . handlers [ self . state ] ( line ) if self . state == self . COMMIT and self . commit : commit = self . build commit ( ) logger . debug ( \"Commit %s parsed\" , commit [ 'commit' ] ) yield commit if self . commit : commit = self . build commit ( ) logger . debug ( \"Commit %s parsed\" , commit [ 'commit' ] ) yield commit", "predictions": ["parse the stream ."], "references": ["parse the git log stream ."], "bleu": 0.36064528799877893, "rouge_l": 0.7721518987341772}
{"id": 1169, "code": "def fetch pack ( self ) : def prepare refs ( refs ) : return [ ref . hash . encode ( 'utf-8' ) for ref in refs if not ref . refname . endswith ( '^{}' ) ] def determine wants ( refs ) : remote refs = prepare refs ( self . discover refs ( remote = True ) ) local refs = prepare refs ( self . discover refs ( ) ) wants = [ ref for ref in remote refs if ref not in local refs ] return wants client , repo path = dulwich . client . get transport and path ( self . uri ) repo = dulwich . repo . Repo ( self . dirpath ) fd = io . Bytes IO ( ) local refs = self . discover refs ( ) graph walker = Graph Walker ( local refs ) result = client . fetch pack ( repo path , determine wants , graph walker , fd . write ) refs = [ Git Ref ( ref hash . decode ( 'utf-8' ) , ref name . decode ( 'utf-8' ) ) for ref name , ref hash in result . refs . items ( ) ] if len ( fd . getvalue ( ) ) > 0 : fd . seek ( 0 ) pack = repo . object store . add thin pack ( fd . read , None ) pack name = pack . name ( ) . decode ( 'utf-8' ) else : pack name = None return ( pack name , refs )", "predictions": ["discover a remote repo for the current repo ."], "references": ["fetch changes and store them in a pack ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 1170, "code": "def read commits from pack ( self , packet name ) : filepath = 'objects/pack/pack-' + packet name cmd verify pack = [ 'git' , 'verify-pack' , '-v' , filepath ] outs = self . exec ( cmd verify pack , cwd = self . dirpath , env = self . gitenv ) outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) lines = [ line . split ( ' ' ) for line in outs . split ( '\\n' ) ] commits = [ parts [ 0 ] for parts in lines if parts [ 1 ] == 'commit' ] commits . reverse ( ) return commits", "predictions": ["read commits from the packet ."], "references": ["read the commits of a pack ."], "bleu": 0.23512037509993022, "rouge_l": 0.45522388059701485}
{"id": 1171, "code": "def update references ( self , refs ) : new refs = [ ref . refname for ref in refs ] for old ref in self . discover refs ( ) : if not old ref . refname . startswith ( 'refs/heads/' ) : continue if old ref . refname in new refs : continue self . update ref ( old ref , delete = True ) for new ref in refs : refname = new ref . refname if refname . endswith ( '^{}' ) : logger . debug ( \"Annotated tag %s ignored for updating in sync process\" , refname ) continue elif not refname . startswith ( 'refs/heads/' ) and not refname . startswith ( 'refs/tags/' ) : logger . debug ( \"Reference %s not needed; ignored for updating in sync process\" , refname ) continue else : self . update ref ( new ref ) cmd = [ 'git' , 'remote' , 'prune' , 'origin' ] self . exec ( cmd , cwd = self . dirpath , env = self . gitenv )", "predictions": ["discover what references do not include the references"], "references": ["update references removing old ones ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 1172, "code": "def discover refs ( self , remote = False ) : if remote : cmd refs = [ 'git' , 'ls-remote' , '-h' , '-t' , '--exit-code' , 'origin' ] sep = '\\t' ignored error codes = [ 2 ] else : if self . is empty ( ) : raise Empty Repository Error ( repository = self . uri ) cmd refs = [ 'git' , 'show-ref' , '--heads' , '--tags' ] sep = ' ' ignored error codes = [ 1 ] outs = self . exec ( cmd refs , cwd = self . dirpath , env = self . gitenv , ignored error codes = ignored error codes ) outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) outs = outs . split ( '\\n' ) if outs else [ ] refs = [ ] for line in outs : data = line . split ( sep ) ref = Git Ref ( data [ 0 ] , data [ 1 ] ) refs . append ( ref ) return refs", "predictions": ["discover list of refs ."], "references": ["get the current list of local or remote refs ."], "bleu": 0.16620830006469267, "rouge_l": 0.5030927835051546}
{"id": 1173, "code": "def update ref ( self , ref , delete = False ) : cmd = [ 'git' , 'update-ref' ] if delete : cmd . extend ( [ '-d' , ref . refname ] ) action = 'deleted' else : cmd . extend ( [ ref . refname , ref . hash ] ) action = 'updated to %s' % ref . hash try : self . exec ( cmd , cwd = self . dirpath , env = self . gitenv ) except Repository Error as e : logger . warning ( \"Git %s ref could not be %s during sync process in %s (%s); skipped\" , ref . refname , action , self . uri , self . dirpath ) else : logger . debug ( \"Git %s ref %s in %s (%s)\" , ref . refname , action , self . uri , self . dirpath )", "predictions": ["update a reference to the reference reference"], "references": ["update a reference ."], "bleu": 0.345720784641941, "rouge_l": 0.5736677115987461}
{"id": 1174, "code": "def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , token auth = True , archive = True ) action = parser . parser . option string actions [ '--api-token' ] action . required = True group = parser . parser . add argument group ( 'Twitter arguments' ) group . add argument ( '--max-items' , dest = 'max items' , type = int , default = MAX ITEMS , help = \"Maximum number of items requested on the same query\" ) group . add argument ( '--no-entities' , dest = 'include entities' , action = 'store false' , help = \" Exclude entities node\" ) group . add argument ( '--geo-code' , dest = 'geocode' , help = \"Select tweets by users located at latitude,longitude,radius\" ) group . add argument ( '--lang' , dest = 'lang' , help = \"Select tweets to the given language in ISO 639-1 code\" ) group . add argument ( '--tweets-type' , dest = 'tweets type' , default = TWEET TYPE MIXED , help = \"Type of tweets returned. Default is 'mixed', others are 'recent' and 'popular'\" ) group . add argument ( '--sleep-for-rate' , dest = 'sleep for rate' , action = 'store true' , help = \"sleep for getting more rate\" ) group . add argument ( '--min-rate-to-sleep' , dest = 'min rate to sleep' , default = MIN RATE LIMIT , type = int , help = \"sleep until reset when the rate limit reaches this value\" ) group . add argument ( '--sleep-time' , dest = 'sleep time' , default = SLEEP TIME , type = int , help = \"minimun sleeping time to avoid too many request exception\" ) parser . parser . add argument ( 'query' , help = \"Search query including operators, max 500 chars\" ) return parser", "predictions": ["setup the cmd parser ."], "references": ["returns the twitter argument parser ."], "bleu": 0.31610981104846864, "rouge_l": 0.5366568914956013}
{"id": 1175, "code": "def parse hits ( self , hit raw ) : bs result = bs4 . Beautiful Soup ( hit raw , 'html.parser' ) hit string = bs result . find ( \"div\" , id = \"result Stats\" ) . text hit string = hit string . replace ( ',' , u'' ) hit string = hit string . replace ( '.' , u'' ) fetched on = datetime utcnow ( ) . timestamp ( ) id args = self . keywords [ : ] id args . append ( str ( fetched on ) ) hits json = { 'fetched on' : fetched on , 'id' : uuid ( * id args ) , 'keywords' : self . keywords , 'type' : 'google Search Hits' } if not hit string : logger . warning ( \"No hits for %s\" , self . keywords ) hits json [ 'hits' ] = 0 return hits json str hits = re . search ( r'\\d+' , hit string ) . group ( 0 ) hits = int ( str hits ) hits json [ 'hits' ] = hits return hits json", "predictions": ["parse hits json hit into a dict ."], "references": ["parse the hits returned by the google search api"], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 1176, "code": "def hits ( self , keywords ) : if len ( keywords ) == 1 : query str = keywords [ 0 ] else : query str = ' ' . join ( [ k for k in keywords ] ) logger . info ( \"Fetching hits for '%s'\" , query str ) params = { 'q' : query str } req = self . fetch ( GOOGLE SEARCH URL , payload = params ) return req . text", "predictions": ["fetch hits data from api ."], "references": ["fetch information about a list of keywords ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 1177, "code": "def fetch pull requests ( self , from date , to date ) : raw pulls = self . client . pulls ( from date = from date ) for raw pull in raw pulls : pull = json . loads ( raw pull ) if str to datetime ( pull [ 'updated at' ] ) > to date : return self . init extra pull fields ( pull ) for field in TARGET PULL FIELDS : if not pull [ field ] : continue if field == 'user' : pull [ field + ' data' ] = self . get user ( pull [ field ] [ 'login' ] ) elif field == 'merged by' : pull [ field + ' data' ] = self . get user ( pull [ field ] [ 'login' ] ) elif field == 'review comments' : pull [ field + ' data' ] = self . get pull review comments ( pull [ 'number' ] ) elif field == 'requested reviewers' : pull [ field + ' data' ] = self . get pull requested reviewers ( pull [ 'number' ] ) elif field == 'commits' : pull [ field + ' data' ] = self . get pull commits ( pull [ 'number' ] ) yield pull", "predictions": ["fetch requests from pull"], "references": ["fetch the pull requests"], "bleu": 0.42728700639623407, "rouge_l": 0.5}
{"id": 1178, "code": "def fetch repo info ( self ) : raw repo = self . client . repo ( ) repo = json . loads ( raw repo ) fetched on = datetime utcnow ( ) repo [ 'fetched on' ] = fetched on . timestamp ( ) yield repo", "predictions": ["fetch repo info from the current repo ."], "references": ["get repo info about stars watchers and forks"], "bleu": 0.21105340631872638, "rouge_l": 0.25}
{"id": 1179, "code": "def get issue comment reactions ( self , comment id , total count ) : reactions = [ ] if total count == 0 : return reactions group reactions = self . client . issue comment reactions ( comment id ) for raw reactions in group reactions : for reaction in json . loads ( raw reactions ) : reaction [ 'user data' ] = self . get user ( reaction [ 'user' ] [ 'login' ] ) reactions . append ( reaction ) return reactions", "predictions": ["get reactions for a issue"], "references": ["get reactions on issue comments"], "bleu": 0.3860973950960897, "rouge_l": 0.6}
{"id": 1180, "code": "def get pull requested reviewers ( self , pr number ) : requested reviewers = [ ] group requested reviewers = self . client . pull requested reviewers ( pr number ) for raw requested reviewers in group requested reviewers : group requested reviewers = json . loads ( raw requested reviewers ) for requested reviewer in group requested reviewers [ 'users' ] : user data = self . get user ( requested reviewer [ 'login' ] ) requested reviewers . append ( user data ) return requested reviewers", "predictions": ["return all requested reviewers requested by reviewers ."], "references": ["get pull request requested reviewers"], "bleu": 0.21105340631872638, "rouge_l": 0.32105263157894737}
{"id": 1181, "code": "def get pull commits ( self , pr number ) : hashes = [ ] group pull commits = self . client . pull commits ( pr number ) for raw pull commits in group pull commits : for commit in json . loads ( raw pull commits ) : commit hash = commit [ 'sha' ] hashes . append ( commit hash ) return hashes", "predictions": ["get all commits for a given pr number"], "references": ["get pull request commit hashes"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 1182, "code": "def get pull review comments ( self , pr number ) : comments = [ ] group comments = self . client . pull review comments ( pr number ) for raw comments in group comments : for comment in json . loads ( raw comments ) : comment id = comment . get ( 'id' ) user = comment . get ( 'user' , None ) if not user : logger . warning ( \"Missing user info for %s\" , comment [ 'url' ] ) comment [ 'user data' ] = None else : comment [ 'user data' ] = self . get user ( user [ 'login' ] ) comment [ 'reactions data' ] = self . get pull review comment reactions ( comment id , comment [ 'reactions' ] [ 'total count' ] ) comments . append ( comment ) return comments", "predictions": ["get review comments for the given pr"], "references": ["get pull request review comments"], "bleu": 0.2626909894424158, "rouge_l": 0.5154929577464789}
{"id": 1183, "code": "def get pull review comment reactions ( self , comment id , total count ) : reactions = [ ] if total count == 0 : return reactions group reactions = self . client . pull review comment reactions ( comment id ) for raw reactions in group reactions : for reaction in json . loads ( raw reactions ) : reaction [ 'user data' ] = self . get user ( reaction [ 'user' ] [ 'login' ] ) reactions . append ( reaction ) return reactions", "predictions": ["pull review comment from reactions and returns list of reactions ."], "references": ["get pull review comment reactions"], "bleu": 0.22416933501922287, "rouge_l": 0.5362637362637362}
{"id": 1184, "code": "def get user ( self , login ) : user = { } if not login : return user user raw = self . client . user ( login ) user = json . loads ( user raw ) user orgs raw = self . client . user orgs ( login ) user [ 'organizations' ] = json . loads ( user orgs raw ) return user", "predictions": ["issue the collection of the collection ."], "references": ["get user and org data for the login"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1185, "code": "def issue reactions ( self , issue number ) : payload = { 'per page' : PER PAGE , 'direction' : 'asc' , 'sort' : 'updated' } path = urijoin ( \"issues\" , str ( issue number ) , \"reactions\" ) return self . fetch items ( path , payload )", "predictions": ["list all json json data for a fetch ."], "references": ["get reactions of an issue"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1186, "code": "def pull requested reviewers ( self , pr number ) : requested reviewers url = urijoin ( \"pulls\" , str ( pr number ) , \"requested reviewers\" ) return self . fetch items ( requested reviewers url , { } )", "predictions": ["find group group group group . . . ."], "references": ["get pull requested reviewers"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1187, "code": "def pull commits ( self , pr number ) : payload = { 'per page' : PER PAGE , } commit url = urijoin ( \"pulls\" , str ( pr number ) , \"commits\" ) return self . fetch items ( commit url , payload )", "predictions": ["fetch commits commits by payload as specified in the provider"], "references": ["get pull request commits"], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 1188, "code": "def pull review comments ( self , pr number ) : payload = { 'per page' : PER PAGE , 'direction' : 'asc' , 'sort' : 'updated' } comments url = urijoin ( \"pulls\" , str ( pr number ) , \"comments\" ) return self . fetch items ( comments url , payload )", "predictions": ["pre - init comments for a specific comments . . . . . . ."], "references": ["get pull request review comments"], "bleu": 0.08225964699966554, "rouge_l": 0.10990990990990988}
{"id": 1189, "code": "def pull review comment reactions ( self , comment id ) : payload = { 'per page' : PER PAGE , 'direction' : 'asc' , 'sort' : 'updated' } path = urijoin ( \"pulls\" , \"comments\" , str ( comment id ) , \"reactions\" ) return self . fetch items ( path , payload )", "predictions": ["setup a cmd parser for a specific parser group group group group group group ."], "references": ["get reactions of a review comment"], "bleu": 0.08225964699966554, "rouge_l": 0.10321489001692045}
{"id": 1190, "code": "def user ( self , login ) : user = None if login in self . users : return self . users [ login ] url user = urijoin ( self . base url , 'users' , login ) logging . info ( \"Getting info for %s\" % ( url user ) ) r = self . fetch ( url user ) user = r . text self . users [ login ] = user return user", "predictions": ["group set by self . ."], "references": ["get the user information and update the user cache"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 1191, "code": "def user orgs ( self , login ) : if login in self . users orgs : return self . users orgs [ login ] url = urijoin ( self . base url , 'users' , login , 'orgs' ) try : r = self . fetch ( url ) orgs = r . text except requests . exceptions . HTTP Error as error : if error . response . status code == 404 : logger . error ( \"Can't get github login orgs: %s\" , error ) orgs = '[]' else : raise error self . users orgs [ login ] = orgs return orgs", "predictions": ["argument for from from from from from from from from from from from from from from from from from from from from from from from from from from from from from"], "references": ["get the user public organizations"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1192, "code": "def get token rate limit ( self , token ) : rate url = urijoin ( self . base url , \"rate limit\" ) self . session . headers . update ( { 'Authorization' : 'token ' + token } ) remaining = 0 try : headers = super ( ) . fetch ( rate url ) . headers if self . rate limit header in headers : remaining = int ( headers [ self . rate limit header ] ) except requests . exceptions . HTTP Error as error : logger . warning ( \"Rate limit not initialized: %s\" , error ) return remaining", "predictions": ["set the arguments for the arguments arguments argument argument argument argument argument argument argument argument ."], "references": ["return token s remaining api points"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 1193, "code": "def get tokens rate limits ( self ) : remainings = [ 0 ] * self . n tokens arch = self . archive self . archive = None for idx , token in enumerate ( self . tokens ) : remainings [ idx ] = self . get token rate limit ( token ) self . archive = arch logger . debug ( \"Remaining API points: {}\" . format ( remainings ) ) return remainings", "predictions": ["return rate self manager manager manager manager manager manager manager manager manager manager manager manager manager manager manager manager manager manager manager manager manager manager manager manager manager manager manager manager"], "references": ["return array of all tokens remaining api points"], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 1194, "code": "def choose best api token ( self ) : if self . n tokens == 0 : return token idx = 0 if self . n tokens > 1 : remainings = self . get tokens rate limits ( ) token idx = remainings . index ( max ( remainings ) ) logger . debug ( \"Remaining API points: {}, choosen index: {}\" . format ( remainings , token idx ) ) self . current token = self . tokens [ token idx ] self . session . headers . update ( { 'Authorization' : 'token ' + self . current token } ) self . update current rate limit ( )", "predictions": ["fetch and fetch and debug parse limits limits"], "references": ["check all api tokens defined and choose one with most remaining api points"], "bleu": 0.08583768591139128, "rouge_l": 0.09131736526946108}
{"id": 1195, "code": "def need check tokens ( self ) : if self . n tokens <= 1 or self . rate limit is None : return False elif self . last rate limit checked is None : self . last rate limit checked = self . rate limit return True approaching limit = float ( self . min rate to sleep ) * ( 1.0 + TOKEN USAGE BEFORE SWITCH ) + 1 if self . rate limit <= approaching limit : self . last rate limit checked = self . rate limit return True ratio = float ( self . rate limit ) / float ( self . last rate limit checked ) if ratio < 1.0 - TOKEN USAGE BEFORE SWITCH : self . last rate limit checked = self . rate limit return True elif ratio > 1.0 : self . last rate limit checked = self . rate limit return False else : return False", "predictions": ["check if the prefix copy copy copy tokens"], "references": ["check if we need to switch github api tokens"], "bleu": 0.20014292374951972, "rouge_l": 0.34923664122137404}
{"id": 1196, "code": "def update current rate limit ( self ) : url = urijoin ( self . base url , \"rate limit\" ) try : arch = self . archive self . archive = None response = super ( ) . fetch ( url ) self . archive = arch self . update rate limit ( response ) self . last rate limit checked = self . rate limit except requests . exceptions . HTTP Error as error : if error . response . status code == 404 : logger . warning ( \"Rate limit not initialized: %s\" , error ) else : raise error", "predictions": ["validate the message self . self . . . . . . . . . . ."], "references": ["update rate limits data for the current token"], "bleu": 0.07223943354597204, "rouge_l": 0.08555399719495091}
{"id": 1197, "code": "def load metadata ( self ) : logger . debug ( \"Loading metadata infomation of archive %s\" , self . archive path ) cursor = self . db . cursor ( ) select stmt = \"SELECT origin, backend name, backend version, \" \"category, backend params, created on \" \"FROM \" + self . METADATA TABLE + \" \" \"LIMIT 1\" cursor . execute ( select stmt ) row = cursor . fetchone ( ) cursor . close ( ) if row : self . origin = row [ 0 ] self . backend name = row [ 1 ] self . backend version = row [ 2 ] self . category = row [ 3 ] self . backend params = pickle . loads ( row [ 4 ] ) self . created on = str to datetime ( row [ 5 ] ) else : logger . debug ( \"Metadata of archive %s was empty\" , self . archive path ) logger . debug ( \"Metadata of archive %s loaded\" , self . archive path )", "predictions": ["get the message message from the lookup table ."], "references": ["load metadata from the archive file"], "bleu": 0.18575057999133596, "rouge_l": 0.27664399092970515}
{"id": 1198, "code": "def count table rows ( self , table name ) : cursor = self . db . cursor ( ) select stmt = \"SELECT COUNT(*) FROM \" + table name try : cursor . execute ( select stmt ) row = cursor . fetchone ( ) except sqlite3 . Database Error as e : msg = \"invalid archive file; cause: %s\" % str ( e ) raise Archive Error ( cause = msg ) finally : cursor . close ( ) return row [ 0 ]", "predictions": ["pre - pre - pre - pre - pre - pre - pre - pre - pre - pre - pre - pre - pre - pre - pre - pre"], "references": ["fetch the number of rows in a table"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1199, "code": "def search archives ( self , origin , backend name , category , archived after ) : for archive path in self . search files ( ) : try : archive = Archive ( archive path ) except Archive Error : continue match = archive . origin == origin and archive . backend name == backend name and archive . category == category and archive . created on >= archived after if not match : continue yield archive path , archive . created on", "predictions": ["setup cmd files in given backend date date date date date date date date date date date"], "references": ["search archives using filters ."], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 1200, "code": "def search files ( self ) : for root , , files in os . walk ( self . dirpath ) : for filename in files : location = os . path . join ( root , filename ) yield location", "predictions": ["parse all files rstrip rstrip rstrip"], "references": ["retrieve the file paths stored under the base path ."], "bleu": 0.09536752763778475, "rouge_l": 0.0}
{"id": 1201, "code": "def repository ( self , owner , repository ) : url = urijoin ( self . base url , self . RREPOSITORY , owner , repository ) logger . debug ( \"Docker Hub client requests: %s\" , url ) response = self . fetch ( url ) return response . text", "predictions": ["do a fetch of a fetch"], "references": ["fetch information about a repository ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 1202, "code": "def get fields ( self ) : url = urijoin ( self . base url , self . RESOURCE , self . VERSION API , 'field' ) req = self . fetch ( url ) return req . text", "predictions": ["read the commits from the api ."], "references": ["retrieve all the fields available ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 1203, "code": "def get builds ( self , job name ) : if self . blacklist jobs and job name in self . blacklist jobs : logger . warning ( \"Not getting blacklisted job: %s\" , job name ) return payload = { 'depth' : self . detail depth } url build = urijoin ( self . base url , \"job\" , job name , \"api\" , \"json\" ) response = self . fetch ( url build , payload = payload ) return response . text", "predictions": ["update a refs by name"], "references": ["retrieve all builds from a job"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 1204, "code": "def setup cmd parser ( cls ) : parser = Backend Command Argument Parser ( cls . BACKEND . CATEGORIES , from date = True , token auth = True , archive = True ) group = parser . parser . add argument group ( 'Stack Exchange arguments' ) group . add argument ( '--site' , dest = 'site' , required = True , help = \"Stack Exchange site\" ) group . add argument ( '--tagged' , dest = 'tagged' , help = \"filter items by question Tag\" ) group . add argument ( '--max-questions' , dest = 'max questions' , type = int , default = MAX QUESTIONS , help = \"Maximum number of questions requested in the same query\" ) return parser", "predictions": ["discover the command line parser . . . ."], "references": ["returns the stackexchange argument parser ."], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 1205, "code": "def get max date ( self , reviews ) : max ts = 0 for review in reviews : ts = str to datetime ( review [ 'timestamp' ] ) ts = datetime to utc ( ts ) if ts . timestamp ( ) > max ts : max ts = ts . timestamp ( ) return max ts", "predictions": ["returns the ref date time of the time series"], "references": ["get the max date in unixtime format from reviews ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 1206, "code": "def get pages ( self , namespace , apcontinue = '' ) : params = { \"action\" : \"query\" , \"list\" : \"allpages\" , \"aplimit\" : self . limit , \"apnamespace\" : namespace , \"format\" : \"json\" } if apcontinue : params [ 'apcontinue' ] = apcontinue return self . call ( params )", "predictions": ["setup all cmd cmd cmd cmd . ."], "references": ["retrieve all pages from a namespace starting from apcontinue ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 1207, "code": "def get recent pages ( self , namespaces , rccontinue = '' ) : namespaces . sort ( ) params = { \"action\" : \"query\" , \"list\" : \"recentchanges\" , \"rclimit\" : self . limit , \"rcnamespace\" : \"|\" . join ( namespaces ) , \"rcprop\" : \"title|timestamp|ids\" , \"format\" : \"json\" } if rccontinue : params [ 'rccontinue' ] = rccontinue return self . call ( params )", "predictions": ["list all hits pages bs4 bs4"], "references": ["retrieve recent pages from all namespaces starting from rccontinue ."], "bleu": 0.1255107248036171, "rouge_l": 0.11960784313725491}
{"id": 1208, "code": "def retrieve archives ( self , from date ) : archives = [ ] candidates = self . list supybot archives ( ) for candidate in candidates : dt = self . parse date from filepath ( candidate ) if dt . date ( ) >= from date . date ( ) : archives . append ( ( dt , candidate ) ) else : logger . debug ( \"Archive %s stored before %s; skipped\" , candidate , str ( from date ) ) archives . sort ( key = lambda x : x [ 0 ] ) return [ archive [ 1 ] for archive in archives ]", "predictions": ["hits for all archives archives objects"], "references": ["retrieve the supybot archives after the given date"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 1209, "code": "def list supybot archives ( self ) : archives = [ ] for root , , files in os . walk ( self . dirpath ) : for filename in files : location = os . path . join ( root , filename ) archives . append ( location ) return archives", "predictions": ["return the requests requests for all pull requests for all pull for the files for all requests for the pull for the files for all requests for the pull - py"], "references": ["list the filepath of the archives stored in dirpath"], "bleu": 0.04317900023606586, "rouge_l": 0.11101000909918107}
{"id": 1210, "code": "def capabilities url ( self , service url ) : qs = [ ] if service url . find ( '?' ) != - 1 : qs = cgi . parse qsl ( service url . split ( '?' ) [ 1 ] ) params = [ x [ 0 ] for x in qs ] if 'service' not in params : qs . append ( ( 'service' , 'WFS' ) ) if 'request' not in params : qs . append ( ( 'request' , 'Get Capabilities' ) ) if 'version' not in params : qs . append ( ( 'version' , self . version ) ) urlqs = urlencode ( tuple ( qs ) ) return service url . split ( '?' ) [ 0 ] + '?' + urlqs", "predictions": ["returns the repo repo for the given service fetched fetched fetched fetched fetched fetched fetched fetched fetched ."], "references": ["return a capabilities url"], "bleu": 0.057259987315337726, "rouge_l": 0.0}
{"id": 1211, "code": "def parse result ( self ) : if self . result is not None : result = self . result . find ( nspv ( \"wml2:Measurement Timeseries\" ) ) self . result = Measurement Timeseries ( result )", "predictions": ["get the issue issue from the parser id id id id id id id id id id id id id id id id id id id id id id id id"], "references": ["parse the result element of the observation type"], "bleu": 0.04317900023606586, "rouge_l": 0.1147695202257761}
{"id": 1212, "code": "def complex input with reference ( ) : print ( \"\\ncomplex input with reference ...\" ) wps = Web Processing Service ( 'http://localhost:8094/wps' , verbose = verbose ) processid = 'wordcount' textdoc = Complex Data Input ( \"http://www.gutenberg.org/files/28885/28885-h/28885-h.htm\" ) inputs = [ ( \"text\" , textdoc ) ] outputs = [ ( \"output\" , True , 'some/mime-type' ) ] execution = wps . execute ( processid , inputs , output = outputs ) monitor Execution ( execution ) print ( 'percent complete' , execution . percent Completed ) print ( 'status message' , execution . status Message ) for output in execution . process Outputs : print ( 'identifier=%s, data Type=%s, data=%s, reference=%s' % ( output . identifier , output . data Type , output . data , output . reference ) )", "predictions": ["print data pull data data"], "references": ["use complexdatainput with a reference to a document"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 1213, "code": "def normalize ( s ) : if ( nonorm ) : return s . split ( ) try : s . split ( ) except : s = \" \" . join ( s ) for ( pattern , replace ) in normalize1 : s = re . sub ( pattern , replace , s ) s = xml . sax . saxutils . unescape ( s , { '&quot;' : '\"' } ) s = \" %s \" % s if not preserve case : s = s . lower ( ) return [ tok for tok in normalize3 . split ( s ) if tok and tok != ' ' ]", "predictions": ["get the hash of a string ."], "references": ["normalize and tokenize text . this is lifted from nist mteval - v11a . pl ."], "bleu": 0.05135131375181345, "rouge_l": 0.08122503328894808}
{"id": 1214, "code": "def erfcc ( x ) : z = abs ( x ) t = 1 / ( 1 + 0.5 * z ) r = t * math . exp ( - z * z - 1.26551223 + t * ( 1.00002368 + t * ( .37409196 + t * ( .09678418 + t * ( - .18628806 + t * ( .27886807 + t * ( - 1.13520398 + t * ( 1.48851587 + t * ( - .82215223 + t * .17087277 ) ) ) ) ) ) ) ) ) if ( x >= 0. ) : return r else : return 2. - r", "predictions": ["get the get point of review"], "references": ["complementary error function ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1215, "code": "def log calls ( func ) : def wrapper ( * args , * * kargs ) : call Str = \"%s(%s)\" % ( func . name , \", \" . join ( [ repr ( p ) for p in args ] + [ \"%s=%s\" % ( k , repr ( v ) ) for ( k , v ) in list ( kargs . items ( ) ) ] ) ) debug ( \">> %s\" , call Str ) ret = func ( * args , * * kargs ) debug ( \"<< %s: %s\" , call Str , repr ( ret ) ) return ret return wrapper", "predictions": ["get the function pull out of the function pull it from the given function if it s not already ."], "references": ["decorator to log function calls ."], "bleu": 0.06760229884571738, "rouge_l": 0.17039106145251398}
{"id": 1216, "code": "def synchronized ( func ) : func . lock = threading . Lock ( ) def synced func ( * args , * * kargs ) : with func . lock : return func ( * args , * * kargs ) return synced func", "predictions": ["decorator to make sure that a function is called ."], "references": ["decorator to synchronize function ."], "bleu": 0.18850319022747347, "rouge_l": 0.5674418604651164}
{"id": 1217, "code": "def message ( msg , * args ) : clear progress ( ) text = ( msg % args ) sys . stdout . write ( text + '\\n' )", "predictions": ["write a progress message to stdout ."], "references": ["program message output ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 1218, "code": "def tempfile get ( target ) : fn = '%s-%s.tmp' % ( target , '' . join ( random . Random ( ) . sample ( \"0123456789abcdefghijklmnopqrstuvwxyz\" , 15 ) ) ) TEMP FILES . add ( fn ) return fn", "predictions": ["get a random tempfile"], "references": ["get a temp filename for atomic download ."], "bleu": 0.1739594473063345, "rouge_l": 0.31443298969072164}
{"id": 1219, "code": "def tempfile set ( tempfile , target ) : if target : os . rename ( tempfile , target ) else : os . unlink ( tempfile ) if target in TEMP FILES : TEMP FILES . remove ( tempfile )", "predictions": ["set a tempfile s tempfile ."], "references": ["atomically rename and clean tempfile"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 1220, "code": "def clean tempfiles ( ) : for fn in TEMP FILES : if os . path . exists ( fn ) : os . unlink ( fn )", "predictions": ["remove all tempfiles created by the given path ."], "references": ["clean up temp files"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1221, "code": "def get fixed path ( self ) : pi = self . path . split ( PATH SEP ) fi = [ ] for p in pi : if '*' in p or '?' in p : break fi . append ( p ) return PATH SEP . join ( fi )", "predictions": ["return the fixed path of the fixed pi ."], "references": ["get the fixed part of the path without wildcard"], "bleu": 0.24446151121745052, "rouge_l": 0.4444444444444444}
{"id": 1222, "code": "def get legal params ( self , method ) : if method not in self . client . meta . method to api mapping : return [ ] api = self . client . meta . method to api mapping [ method ] shape = self . client . meta . service model . operation model ( api ) . input shape if shape is None : return [ ] return shape . members . keys ( )", "predictions": ["get the legal params for the given method ."], "references": ["given a api name list all legal parameters using boto3 service model ."], "bleu": 0.10761866342063775, "rouge_l": 0.17604617604617603}
{"id": 1223, "code": "def add options ( parser ) : for param , param type , param doc in Boto Client . EXTRA CLIENT PARAMS : parser . add option ( '--API-' + param , help = param doc , type = param type , dest = param )", "predictions": ["add options to parser ."], "references": ["add the whole list of api parameters into optparse ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 1224, "code": "def add task ( self , func name , * args , * * kargs ) : self . tasks . put ( ( func name , 0 , args , kargs ) )", "predictions": ["add a task to the queue ."], "references": ["utility function to add a single task into task queue"], "bleu": 0.18938334565508194, "rouge_l": 0.45607476635514016}
{"id": 1225, "code": "def join ( self ) : self . tasks . join ( ) for worker in self . workers : self . tasks . put ( None ) for worker in self . workers : worker . join ( ) worker . s3 = None", "predictions": ["join all workers ."], "references": ["utility function to wait all tasks to complete"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 1226, "code": "def processed ( self ) : self . processed tasks += 1 qsize = self . tasks . qsize ( ) if qsize > 0 : progress ( '[%d task(s) completed, %d remaining, %d thread(s)]' , self . processed tasks , qsize , len ( self . workers ) ) else : progress ( '[%d task(s) completed, %d thread(s)]' , self . processed tasks , len ( self . workers ) )", "predictions": ["processed the command ."], "references": ["increase the processed task counter and show progress message"], "bleu": 0.11392443929712959, "rouge_l": 0.14386792452830188}
{"id": 1227, "code": "def s3 keys from env ( ) : env = os . environ if S3 ACCESS KEY NAME in env and S3 SECRET KEY NAME in env : keys = ( env [ S3 ACCESS KEY NAME ] , env [ S3 SECRET KEY NAME ] ) debug ( \"read S3 keys from environment\" ) return keys else : return None", "predictions": ["return s3 keys from s3 env ."], "references": ["retrieve s3 access keys from the environment or none if not present ."], "bleu": 0.11787460936700446, "rouge_l": 0.3794712286158632}
{"id": 1228, "code": "def s3 keys from cmdline ( opt ) : if opt . access key != None and opt . secret key != None : keys = ( opt . access key , opt . secret key ) debug ( \"read S3 keys from commandline\" ) return keys else : return None", "predictions": ["return a list of s3 keys from s3 ."], "references": ["retrieve s3 access keys from the command line or none if not present ."], "bleu": 0.12109261383365659, "rouge_l": 0.3347050754458162}
{"id": 1229, "code": "def s3 keys from s3cfg ( opt ) : try : if opt . s3cfg != None : s3cfg path = \"%s\" % opt . s3cfg else : s3cfg path = \"%s/.s3cfg\" % os . environ [ \"HOME\" ] if not os . path . exists ( s3cfg path ) : return None config = Config Parser . Config Parser ( ) config . read ( s3cfg path ) keys = config . get ( \"default\" , \"access key\" ) , config . get ( \"default\" , \"secret key\" ) debug ( \"read S3 keys from %s file\" , s3cfg path ) return keys except Exception as e : info ( \"could not read S3 keys from %s file; skipping (%s)\" , s3cfg path , e ) return None", "predictions": ["return the s3 keys from the s3 s3cfg ."], "references": ["retrieve s3 access key settings from s3cmd s config file if present ; otherwise return none ."], "bleu": 0.07296176435240816, "rouge_l": 0.2186379928315412}
{"id": 1230, "code": "def init s3 keys ( opt ) : S3Handler . S3 KEYS = S3Handler . s3 keys from cmdline ( opt ) or S3Handler . s3 keys from env ( ) or S3Handler . s3 keys from s3cfg ( opt )", "predictions": ["initialize s3 keys from s3 keys ."], "references": ["initialize s3 access keys from environment variable or s3cfg config file ."], "bleu": 0.15749996500436228, "rouge_l": 0.5024711696869852}
{"id": 1231, "code": "def connect ( self ) : try : if S3Handler . S3 KEYS : self . s3 = Boto Client ( self . opt , S3Handler . S3 KEYS [ 0 ] , S3Handler . S3 KEYS [ 1 ] ) else : self . s3 = Boto Client ( self . opt ) except Exception as e : raise Retry Failure ( 'Unable to connect to s3: %s' % e )", "predictions": ["connect to s3 ."], "references": ["connect to s3 storage"], "bleu": 0.668740304976422, "rouge_l": 0.75}
{"id": 1232, "code": "def local walk ( self , basedir ) : result = [ ] for root , dirs , files in os . walk ( basedir ) : for f in files : result . append ( os . path . join ( root , f ) ) return result", "predictions": ["walk through all local files in a directory ."], "references": ["walk through local directories from root basedir"], "bleu": 0.19960198807747329, "rouge_l": 0.38364779874213834}
{"id": 1233, "code": "def put single file ( self , pool , source , target ) : if os . path . isdir ( source ) : if self . opt . recursive : for f in ( f for f in self . local walk ( source ) if not os . path . isdir ( f ) ) : target url = S3URL ( target ) joined path = os . path . normpath ( os . path . join ( target url . path , os . path . relpath ( f , source ) ) ) pool . upload ( f , S3URL . combine ( 's3' , target url . bucket , joined path ) ) else : message ( 'omitting directory \"%s\".' % source ) else : pool . upload ( source , target )", "predictions": ["copy a single file to the local directory"], "references": ["upload a single file or a directory by adding a task into queue"], "bleu": 0.1689276792789442, "rouge_l": 0.3652694610778443}
{"id": 1234, "code": "def create bucket ( self , source ) : s3url = S3URL ( source ) message ( 'Creating %s' , source ) if not self . opt . dry run : resp = self . s3 . create bucket ( Bucket = s3url . bucket ) if resp [ 'Response Metadata' ] [ \"HTTP Status Code\" ] == 200 : message ( 'Done.' ) else : raise Failure ( 'Unable to create bucket %s' % source )", "predictions": ["create a bucket on the s3 source ."], "references": ["use the create_bucket api to create a new bucket"], "bleu": 0.2116253761537182, "rouge_l": 0.34923664122137404}
{"id": 1235, "code": "def update privilege ( self , obj , target ) : if 'privilege' in obj [ 'Metadata' ] : os . chmod ( target , int ( obj [ 'Metadata' ] [ 'privilege' ] , 8 ) )", "predictions": ["set the target privilege of the target object ."], "references": ["get privileges from metadata of the source in s3 and apply them to target"], "bleu": 0.11452232344544261, "rouge_l": 0.2510288065843621}
{"id": 1236, "code": "def print files ( self , source ) : sources = self . source expand ( source ) for source in sources : s3url = S3URL ( source ) response = self . s3 . get object ( Bucket = s3url . bucket , Key = s3url . path ) message ( '%s' , response [ 'Body' ] . read ( ) )", "predictions": ["print all files in s3 ."], "references": ["print out a series of files"], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 1237, "code": "def get single file ( self , pool , source , target ) : if source [ - 1 ] == PATH SEP : if self . opt . recursive : basepath = S3URL ( source ) . path for f in ( f for f in self . s3walk ( source ) if not f [ 'is dir' ] ) : pool . download ( f [ 'name' ] , os . path . join ( target , os . path . relpath ( S3URL ( f [ 'name' ] ) . path , basepath ) ) ) else : message ( 'omitting directory \"%s\".' % source ) else : pool . download ( source , target )", "predictions": ["download a single file from the given source pool ."], "references": ["download a single file or a directory by adding a task into queue"], "bleu": 0.2599116052692159, "rouge_l": 0.33983286908078}
{"id": 1238, "code": "def cp single file ( self , pool , source , target , delete source ) : if source [ - 1 ] == PATH SEP : if self . opt . recursive : basepath = S3URL ( source ) . path for f in ( f for f in self . s3walk ( source ) if not f [ 'is dir' ] ) : pool . copy ( f [ 'name' ] , os . path . join ( target , os . path . relpath ( S3URL ( f [ 'name' ] ) . path , basepath ) ) , delete source = delete source ) else : message ( 'omitting directory \"%s\".' % source ) else : pool . copy ( source , target , delete source = delete source )", "predictions": ["cp a single file with the given target ."], "references": ["copy a single file or a directory by adding a task into queue"], "bleu": 0.16843231064773728, "rouge_l": 0.26406926406926406}
{"id": 1239, "code": "def del files ( self , source ) : src files = [ ] for obj in self . s3walk ( source ) : if not obj [ 'is dir' ] : src files . append ( obj [ 'name' ] ) pool = Thread Pool ( Thread Util , self . opt ) pool . batch delete ( src files ) pool . join ( )", "predictions": ["delete files in the pool which do not exist ."], "references": ["delete files on s3"], "bleu": 0.16590387014219712, "rouge_l": 0.3096446700507614}
{"id": 1240, "code": "def dsync files ( self , source , target ) : src s3 url = S3URL . is valid ( source ) dst s3 url = S3URL . is valid ( target ) source list = self . relative dir walk ( source ) if len ( source list ) == 0 or '.' in source list : raise Failure ( 'Sync command need to sync directory to directory.' ) sync list = [ ( os . path . join ( source , f ) , os . path . join ( target , f ) ) for f in source list ] pool = Thread Pool ( Thread Util , self . opt ) if src s3 url and not dst s3 url : for src , dest in sync list : pool . download ( src , dest ) elif not src s3 url and dst s3 url : for src , dest in sync list : pool . upload ( src , dest ) elif src s3 url and dst s3 url : for src , dest in sync list : pool . copy ( src , dest ) else : raise Invalid Argument ( 'Cannot sync two local directories.' ) pool . join ( ) if self . opt . delete removed : target list = self . relative dir walk ( target ) remove list = [ os . path . join ( target , f ) for f in ( set ( target list ) - set ( source list ) ) ] if S3URL . is valid ( target ) : pool = Thread Pool ( Thread Util , self . opt ) pool . batch delete ( remove list ) pool . join ( ) else : for f in remove list : try : os . unlink ( f ) message ( 'Delete %s' , f ) except : pass", "predictions": ["sync files in a directory to a given source directory ."], "references": ["sync directory to directory ."], "bleu": 0.1972940627795883, "rouge_l": 0.6703296703296704}
{"id": 1241, "code": "def file hash ( self , filename , block size = 2 ** 20 ) : m = hashlib . md5 ( ) with open ( filename , 'rb' ) as f : while True : data = f . read ( block size ) if not data : break m . update ( data ) return m . hexdigest ( )", "predictions": ["compute the md5 hash of a file ."], "references": ["calculate md5 hash code for a local file"], "bleu": 0.239802967618271, "rouge_l": 0.5}
{"id": 1242, "code": "def get md5 ( self ) : if self . md5 is None : self . md5 = self . file hash ( self . filename ) return self . md5", "predictions": ["get the md5 md5 of the file ."], "references": ["get or calculate md5 value of the local file ."], "bleu": 0.22482042087568346, "rouge_l": 0.6535714285714286}
{"id": 1243, "code": "def mkdirs ( self , target ) : path = os . path . dirname ( target ) if path and path != PATH SEP and not os . path . isdir ( path ) : try : os . makedirs ( path ) except OS Error as ose : if ose . errno != errno . EEXIST : raise Failure ( 'Unable to create directory (%s)' % ( path , ) )", "predictions": ["set the directory of a directory to a directory ."], "references": ["ensure all directories are created for a given target file ."], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 1244, "code": "def conditional ( self , result , obj ) : fileonly = ( self . opt . last modified before is not None ) or ( self . opt . last modified after is not None ) if obj [ 'is dir' ] : if not fileonly : result . append ( obj ) return if ( self . opt . last modified before is not None ) and obj [ 'last modified' ] >= self . opt . last modified before : return if ( self . opt . last modified after is not None ) and obj [ 'last modified' ] <= self . opt . last modified after : return result . append ( obj )", "predictions": ["conditional conditional objects for the object"], "references": ["check all file item with given conditions ."], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 1245, "code": "def get file privilege ( self , source ) : try : return str ( oct ( os . stat ( source ) . st mode ) [ - 3 : ] ) except Exception as e : raise Failure ( 'Could not get stat for %s, error message = %s' , source , e )", "predictions": ["return the privilege privilege of the given source ."], "references": ["get privileges of a local file"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 1246, "code": "def lookup ( self , s3url ) : try : return self . s3 . head object ( Bucket = s3url . bucket , Key = s3url . path ) except Boto Client . Client Error as e : if e . response [ 'Response Metadata' ] [ 'HTTP Status Code' ] == 404 : return None else : raise e", "predictions": ["looks up an object by s3url ."], "references": ["get the s3 object with the s3 url . return none if not exist ."], "bleu": 0.06555660318294844, "rouge_l": 0.17062937062937064}
{"id": 1247, "code": "def read file chunk ( self , source , pos , chunk ) : if chunk == 0 : return String IO ( ) data = None with open ( source , 'rb' ) as f : f . seek ( pos ) data = f . read ( chunk ) if not data : raise Failure ( 'Unable to read data from source: %s' % source ) return String IO ( data )", "predictions": ["read a file from the source file ."], "references": ["read local file chunk"], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 1248, "code": "def upload ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 ) : s3url = S3URL ( target ) obj = self . lookup ( s3url ) if not mpi : fsize = os . path . getsize ( source ) md5cache = Local MD5Cache ( source ) if self . opt . dry run : message ( '%s => %s' , source , target ) return elif self . opt . sync check and self . sync check ( md5cache , obj ) : message ( '%s => %s (synced)' , source , target ) return elif not self . opt . force and obj : raise Failure ( 'File already exists: %s' % target ) if fsize < self . opt . max singlepart upload size : data = self . read file chunk ( source , 0 , fsize ) self . s3 . put object ( Bucket = s3url . bucket , Key = s3url . path , Body = data , Metadata = { 'md5' : md5cache . get md5 ( ) , 'privilege' : self . get file privilege ( source ) } ) message ( '%s => %s' , source , target ) return response = self . s3 . create multipart upload ( Bucket = s3url . bucket , Key = s3url . path , Metadata = { 'md5' : md5cache . get md5 ( ) , 'privilege' : self . get file privilege ( source ) } ) upload id = response [ 'Upload Id' ] for args in self . get file splits ( upload id , source , target , fsize , self . opt . multipart split size ) : self . pool . upload ( * args ) return data = self . read file chunk ( source , pos , chunk ) response = self . s3 . upload part ( Bucket = s3url . bucket , Key = s3url . path , Upload Id = mpi . id , Body = data , Part Number = part ) if mpi . complete ( { 'E Tag' : response [ 'E Tag' ] , 'Part Number' : part } ) : try : self . s3 . complete multipart upload ( Bucket = s3url . bucket , Key = s3url . path , Upload Id = mpi . id , Multipart Upload = { 'Parts' : mpi . sorted parts ( ) } ) message ( '%s => %s' , source , target ) except Exception as e : message ( 'Unable to complete upload: %s' , str ( e ) ) self . s3 . abort multipart upload ( Bucket = s3url . bucket , Key = s3url . path , Upload Id = mpi . id ) raise Retry Failure ( 'Upload failed: Unable to complete upload %s.' % source )", "predictions": ["uploads a single source to the server . ."], "references": ["thread worker for upload operation ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 1249, "code": "def verify file size ( self , obj , downloaded file ) : file size = os . path . getsize ( downloaded file ) if int ( obj [ 'Content Length' ] ) != file size : raise Retry Failure ( 'Downloaded file size inconsistent: %s' % ( repr ( obj ) ) )", "predictions": ["message msg msg msg"], "references": ["verify the file size of the downloaded file ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 1250, "code": "def write file chunk ( self , target , pos , chunk , body ) : fd = os . open ( target , os . O CREAT | os . O WRONLY ) try : os . lseek ( fd , pos , os . SEEK SET ) data = body . read ( chunk ) num bytes written = os . write ( fd , data ) if ( num bytes written != len ( data ) ) : raise Retry Failure ( 'Number of bytes written inconsistent: %s != %s' % ( num bytes written , sys . getsizeof ( data ) ) ) finally : os . close ( fd )", "predictions": ["tempfile the given fn to the given fn random random bytes random random bytes random random random bytes random bytes random bytes random random bytes random bytes random random bytes random"], "references": ["write local file chunk"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1251, "code": "def download ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 ) : s3url = S3URL ( source ) obj = self . lookup ( s3url ) if obj is None : raise Failure ( 'The obj \"%s\" does not exists.' % ( s3url . path , ) ) if not mpi : if self . opt . dry run : message ( '%s => %s' , source , target ) return elif self . opt . sync check and self . sync check ( Local MD5Cache ( target ) , obj ) : message ( '%s => %s (synced)' , source , target ) return elif not self . opt . force and os . path . exists ( target ) : raise Failure ( 'File already exists: %s' % target ) fsize = int ( obj [ 'Content Length' ] ) if fsize < self . opt . max singlepart download size : mpi = Thread Util . Multipart Item ( tempfile get ( target ) ) mpi . total = 1 pos = 0 chunk = fsize else : for args in self . get file splits ( tempfile get ( target ) , source , target , fsize , self . opt . multipart split size ) : self . pool . download ( * args ) return tempfile = mpi . id if self . opt . recursive : self . mkdirs ( tempfile ) response = self . s3 . get object ( Bucket = s3url . bucket , Key = s3url . path , Range = 'bytes=%d-%d' % ( pos , pos + chunk - 1 ) ) self . write file chunk ( tempfile , pos , chunk , response [ 'Body' ] ) if mpi . complete ( { 'Part Number' : part } ) : try : self . update privilege ( obj , tempfile ) self . verify file size ( obj , tempfile ) tempfile set ( tempfile , target ) message ( '%s => %s' , source , target ) except Exception as e : tempfile set ( tempfile , None ) raise Failure ( 'Download Failure: %s, Source: %s.' % ( e . message , source ) )", "predictions": ["tempfile the given target ."], "references": ["thread worker for download operation ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 1252, "code": "def copy ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 , delete source = False ) : if self . opt . dry run : message ( '%s => %s' % ( source , target ) ) return source url = S3URL ( source ) target url = S3URL ( target ) if not mpi : obj = self . lookup ( source url ) fsize = int ( obj [ 'Content Length' ] ) if fsize < self . opt . max singlepart copy size : self . s3 . copy object ( Bucket = target url . bucket , Key = target url . path , Copy Source = { 'Bucket' : source url . bucket , 'Key' : source url . path } ) message ( '%s => %s' % ( source , target ) ) if delete source : self . delete ( source ) return response = self . s3 . create multipart upload ( Bucket = target url . bucket , Key = target url . path , Metadata = obj [ 'Metadata' ] ) upload id = response [ 'Upload Id' ] for args in self . get file splits ( upload id , source , target , fsize , self . opt . multipart split size ) : self . pool . copy ( * args , delete source = delete source ) return response = self . s3 . upload part copy ( Bucket = target url . bucket , Key = target url . path , Copy Source = { 'Bucket' : source url . bucket , 'Key' : source url . path } , Copy Source Range = 'bytes=%d-%d' % ( pos , pos + chunk - 1 ) , Upload Id = mpi . id , Part Number = part ) if mpi . complete ( { 'E Tag' : response [ 'Copy Part Result' ] [ 'E Tag' ] , 'Part Number' : part } ) : try : self . s3 . complete multipart upload ( Bucket = target url . bucket , Key = target url . path , Upload Id = mpi . id , Multipart Upload = { 'Parts' : mpi . sorted parts ( ) } ) if delete source : self . delete ( source ) message ( '%s => %s' % ( source , target ) ) except Exception as e : message ( 'Unable to complete upload: %s' , str ( e ) ) self . s3 . abort multipart upload ( Bucket = source url . bucket , Key = source url . path , Upload Id = mpi . id ) raise Retry Failure ( 'Copy failed: Unable to complete copy %s.' % source )", "predictions": ["clean up the source object with the given source ."], "references": ["copy a single file from source to target using boto s3 library ."], "bleu": 0.10335004586873166, "rouge_l": 0.16991643454039}
{"id": 1253, "code": "def delete ( self , source ) : s3url = S3URL ( source ) message ( 'Delete %s' , source ) if not self . opt . dry run : self . s3 . delete object ( Bucket = s3url . bucket , Key = s3url . path )", "predictions": ["get a file from the repository ."], "references": ["thread worker for download operation ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 1254, "code": "def run ( self , args ) : if len ( args ) == 0 : raise Invalid Argument ( 'No command provided' ) cmd = args [ 0 ] if cmd + ' handler' in Command Handler . dict : Command Handler . dict [ cmd + ' handler' ] ( self , args ) else : raise Invalid Argument ( 'Unknown command %s' % cmd )", "predictions": ["execute a to to the to be executed with the to the to the to the to the to the to the to the to execute to the to the to"], "references": ["main entry to handle commands . dispatch to individual command handler ."], "bleu": 0.04317900023606586, "rouge_l": 0.10107705053852528}
{"id": 1255, "code": "def ls handler ( self , args ) : if len ( args ) == 1 : self . pretty print ( self . s3handler ( ) . list buckets ( ) ) return self . validate ( 'cmd|s3' , args ) self . pretty print ( self . s3handler ( ) . s3walk ( args [ 1 ] ) )", "predictions": ["list available buckets buckets"], "references": ["handler for ls command"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 1256, "code": "def mb handler ( self , args ) : if len ( args ) == 1 : raise Invalid Argument ( 'No s3 bucketname provided' ) self . validate ( 'cmd|s3' , args ) self . s3handler ( ) . create bucket ( args [ 1 ] )", "predictions": ["handle s3 on s3 ."], "references": ["handler for mb command"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1257, "code": "def put handler ( self , args ) : if len ( args ) < 3 : raise Invalid Argument ( 'Invalid number of parameters' ) self . validate ( '|' . join ( [ 'cmd' ] + [ 'local' ] * ( len ( args ) - 2 ) + [ 's3' ] ) , args ) source = args [ 1 : - 1 ] target = args [ - 1 ] self . s3handler ( ) . put files ( source , target )", "predictions": ["join a number handler to the server . ."], "references": ["handler for put command"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 1258, "code": "def get handler ( self , args ) : if len ( args ) == 2 : args += [ '.' ] self . validate ( 'cmd|s3|local' , args ) source = args [ 1 ] target = args [ 2 ] self . s3handler ( ) . get files ( source , target )", "predictions": ["process the source ."], "references": ["handler for get command"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 1259, "code": "def cat handler ( self , args ) : self . validate ( 'cmd|s3' , args ) source = args [ 1 ] self . s3handler ( ) . print files ( source )", "predictions": ["process files file os ."], "references": ["handler for cat command"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1260, "code": "def dsync handler ( self , args ) : self . opt . recursive = True self . opt . sync check = True self . opt . force = True self . validate ( 'cmd|s3,local|s3,local' , args ) source = args [ 1 ] target = args [ 2 ] self . s3handler ( ) . dsync files ( source , target )", "predictions": ["handle s3 debug keys if needed ."], "references": ["handler for dsync command ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1261, "code": "def cp handler ( self , args ) : self . validate ( 'cmd|s3|s3' , args ) source = args [ 1 ] target = args [ 2 ] self . s3handler ( ) . cp files ( source , target )", "predictions": ["process s3 os ."], "references": ["handler for cp command"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 1262, "code": "def mv handler ( self , args ) : self . validate ( 'cmd|s3|s3' , args ) source = args [ 1 ] target = args [ 2 ] self . s3handler ( ) . cp files ( source , target , delete source = True )", "predictions": ["process files file ."], "references": ["handler for mv command"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 1263, "code": "def del handler ( self , args ) : self . validate ( 'cmd|s3' , args ) source = args [ 1 ] self . s3handler ( ) . del files ( source )", "predictions": ["delete a s3 s3 s3 s3 s3 ."], "references": ["handler for del command"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1264, "code": "def du handler ( self , args ) : for src , size in self . s3handler ( ) . size ( args [ 1 : ] ) : message ( '%s\\t%s' % ( size , src ) )", "predictions": ["handle incoming local walk of an incoming message for this user for the specified user for the specified user for the specified user for the specified user for this class for"], "references": ["handler for size command"], "bleu": 0.03901663112717908, "rouge_l": 0.06637649619151251}
{"id": 1265, "code": "def totalsize handler ( self , args ) : total size = 0 for src , size in self . s3handler ( ) . size ( args [ 1 : ] ) : total size += size message ( str ( total size ) )", "predictions": ["handle put single put single message . ."], "references": ["handler of total_size command"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1266, "code": "def match date ( self , value ) : m = self . REGEX DATE . search ( value ) date = datetime . datetime . utcnow ( ) . date ( ) if m : date = datetime . date ( int ( m . group ( 1 ) ) , int ( m . group ( 2 ) ) , int ( m . group ( 3 ) ) ) value = self . REGEX DATE . sub ( '' , value ) return ( date , value )", "predictions": ["create a bucket object"], "references": ["search for date information in the string"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 1267, "code": "def match time ( self , value ) : m = self . REGEX TIME . search ( value ) time = datetime . datetime . utcnow ( ) . time ( ) if m : time = datetime . time ( int ( m . group ( 1 ) ) , int ( m . group ( 2 ) ) ) value = self . REGEX TIME . sub ( '' , value ) return ( time , value )", "predictions": ["update privilege privilege privilege privilege value"], "references": ["search for time information in the string"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 1268, "code": "def match delta ( self , value ) : m = self . REGEX DELTA . search ( value ) delta = datetime . timedelta ( days = 0 ) if m : d = int ( m . group ( 1 ) ) if m . group ( 3 ) == 'ago' or m . group ( 3 ) == 'before' : d = - d if m . group ( 2 ) == 'minute' : delta = datetime . timedelta ( minutes = d ) elif m . group ( 2 ) == 'hour' : delta = datetime . timedelta ( hours = d ) elif m . group ( 2 ) == 'day' : delta = datetime . timedelta ( days = d ) elif m . group ( 2 ) == 'week' : delta = datetime . timedelta ( weeks = d ) value = self . REGEX DELTA . sub ( '' , value ) return ( delta , value )", "predictions": ["print files in files"], "references": ["search for timedelta information in the string"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 1269, "code": "def check dict ( self , opt , value ) : try : return json . loads ( value ) except : raise optparse . Option Value Error ( \"Option %s: invalid dict value: %r\" % ( opt , value ) )", "predictions": ["get the if pool is a if it is not a if it is not a if not a if it otherwise raise"], "references": ["take json as dictionary parameter"], "bleu": 0.04449945957170705, "rouge_l": 0.0}
{"id": 1270, "code": "def get from hub ( self , sid ) : cmd = '{ \"cmd\":\"read\",\"sid\":\"' + sid + '\"}' resp = self . send cmd ( cmd , \"read ack\" ) if int ( self . proto [ 0 : 1 ] ) == 1 else self . send cmd ( cmd , \"read rsp\" ) LOGGER . debug ( \"read ack << %s\" , resp ) return self . push data ( resp )", "predictions": ["cp single file single file ."], "references": ["get data from gateway"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1271, "code": "def push data ( self , data ) : if not validate data ( data ) : return False jdata = json . loads ( data [ 'data' ] ) if int ( self . proto [ 0 : 1 ] ) == 1 else list2map ( data [ 'params' ] ) if jdata is None : return False sid = data [ 'sid' ] for func in self . callbacks [ sid ] : func ( jdata , data ) return True", "predictions": ["del files from the in in the in in the in in the in - place"], "references": ["push data broadcasted from gateway to device"], "bleu": 0.07692375026049747, "rouge_l": 0.09355828220858894}
{"id": 1272, "code": "def get key ( self ) : init vector = bytes ( bytearray . fromhex ( '17996d093d28ddb3ba695a2e6f58562e' ) ) encryptor = Cipher ( algorithms . AES ( self . key . encode ( ) ) , modes . CBC ( init vector ) , backend = default backend ( ) ) . encryptor ( ) ciphertext = encryptor . update ( self . token . encode ( ) ) + encryptor . finalize ( ) if isinstance ( ciphertext , str ) : return '' . join ( '{:02x}' . format ( ord ( x ) ) for x in ciphertext ) return '' . join ( '{:02x}' . format ( x ) for x in ciphertext )", "predictions": ["get the files for the == files"], "references": ["get key using token from gateway"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 1273, "code": "def ensure log handler ( self ) : if log . handlers : return handler = logging . Stream Handler ( ) formatter = logging . Formatter ( '%(asctime)s %(levelname)-5.5s [%(name)s][%(thread Name)s] %(message)s' ) handler . set Formatter ( formatter ) log . add Handler ( handler )", "predictions": ["while we have a hash handler"], "references": ["if there s no log configuration set up a default handler ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 1274, "code": "def lambda function ( f ) : @ functools . wraps ( f ) def wrapper ( event , context ) : global CURRENT LAMBDA CONTEXT CURRENT LAMBDA CONTEXT = context try : result = f ( event , context ) return wait ( lambda : result ) except : cls , exc , trace = sys . exc info ( ) report exc info ( ( cls , exc , trace . tb next ) ) wait ( ) raise return wrapper", "predictions": ["decorator to require a get md5 md5 is available is a function is called is a md5 md5 ."], "references": ["decorator for making error handling on aws lambda easier"], "bleu": 0.06439931429457924, "rouge_l": 0.07634543178973717}
{"id": 1275, "code": "def create agent log ( ) : log file = SETTINGS [ 'agent.log file' ] if not log file . endswith ( '.rollbar' ) : log . error ( \"Provided agent log file does not end with .rollbar, which it must. \" \"Using default instead.\" ) log file = DEFAULTS [ 'agent.log file' ] retval = logging . get Logger ( 'rollbar agent' ) handler = logging . File Handler ( log file , 'a' , 'utf-8' ) formatter = logging . Formatter ( '%(message)s' ) handler . set Formatter ( formatter ) retval . add Handler ( handler ) retval . set Level ( logging . WARNING ) return retval", "predictions": ["create self . ."], "references": ["creates . rollbar log file for use with rollbar - agent"], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 1276, "code": "def add lambda context data ( data ) : global CURRENT LAMBDA CONTEXT context = CURRENT LAMBDA CONTEXT if context is None : return try : lambda data = { 'lambda' : { 'remaining time in millis' : context . get remaining time in millis ( ) , 'function name' : context . function name , 'function version' : context . function version , 'arn' : context . invoked function arn , 'request id' : context . aws request id , } } if 'custom' in data : data [ 'custom' ] = dict merge ( data [ 'custom' ] , lambda data ) else : data [ 'custom' ] = lambda data except Exception as e : log . exception ( \"Exception while adding lambda context data: %r\" , e ) finally : CURRENT LAMBDA CONTEXT = None", "predictions": ["conditional lambda self . data data to aws"], "references": ["attempts to add information from the lambda context if it exists"], "bleu": 0.12197601375336842, "rouge_l": 0.10234899328859062}
{"id": 1277, "code": "def add request data ( data , request ) : try : request data = build request data ( request ) except Exception as e : log . exception ( \"Exception while building request data for Rollbar payload: %r\" , e ) else : if request data : filter ip ( request data , SETTINGS [ 'capture ip' ] ) data [ 'request' ] = request data", "predictions": ["get file privilege privilege privilege privilege privilege privilege stat privilege"], "references": ["attempts to build request data ; if successful sets the request key on data ."], "bleu": 0.06429415067236427, "rouge_l": 0.0}
{"id": 1278, "code": "def check add locals ( frame , frame num , total frames ) : return any ( ( ( frame num == total frames - 1 ) , ( 'root' in SETTINGS and ( frame . get ( 'filename' ) or '' ) . lower ( ) . startswith ( ( SETTINGS [ 'root' ] or '' ) . lower ( ) ) ) ) )", "predictions": ["lookup if the frame is not a add"], "references": ["returns true if we should record local variables for the given frame ."], "bleu": 0.10207878682119532, "rouge_l": 0.2739520958083832}
{"id": 1279, "code": "def build server data ( ) : server data = { 'host' : socket . gethostname ( ) , 'pid' : os . getpid ( ) } argv = getattr ( sys , 'argv' , None ) if argv : server data [ 'argv' ] = argv for key in [ 'branch' , 'root' ] : if SETTINGS . get ( key ) : server data [ key ] = SETTINGS [ key ] return server data", "predictions": ["read file chunk chunk chunk from with return dict"], "references": ["returns a dictionary containing information about the server environment ."], "bleu": 0.10620315618312248, "rouge_l": 0.0}
{"id": 1280, "code": "def build payload ( data ) : for k , v in iteritems ( data ) : data [ k ] = transform ( v , key = ( k , ) ) payload = { 'access token' : SETTINGS [ 'access token' ] , 'data' : data } return payload", "predictions": ["build dict from data ."], "references": ["returns the full payload as a string ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1281, "code": "def main ( ) : rollbar . init ( 'ACCESS TOKEN' , environment = 'test' , handler = 'twisted' ) factory = protocol . Server Factory ( ) factory . protocol = Echo reactor . listen TCP ( 8000 , factory ) reactor . run ( )", "predictions": ["run the rollbar ."], "references": ["this runs the protocol on port 8000"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 1282, "code": "def decompose ( hangul letter ) : from . import checker if len ( hangul letter ) < 1 : raise Not Letter Exception ( '' ) elif not checker . is hangul ( hangul letter ) : raise Not Hangul Exception ( '' ) if hangul letter in CHO : return hangul letter , '' , '' if hangul letter in JOONG : return '' , hangul letter , '' if hangul letter in JONG : return '' , '' , hangul letter code = hangul index ( hangul letter ) cho , joong , jong = decompose index ( code ) if cho < 0 : cho = 0 try : return CHO [ cho ] , JOONG [ joong ] , JONG [ jong ] except : print ( \"%d / %d  / %d\" % ( cho , joong , jong ) ) print ( \"%s / %s \" % ( JOONG [ joong ] . encode ( \"utf8\" ) , JONG [ jong ] . encode ( 'utf8' ) ) ) raise Exception ( )", "predictions": ["decompose a hangul letter in the letter letter ."], "references": ["this function returns letters by decomposing the specified hangul letter ."], "bleu": 0.18702742554494436, "rouge_l": 0.2946859903381642}
{"id": 1283, "code": "def has jongsung ( letter ) : if len ( letter ) != 1 : raise Exception ( 'The target string must be one letter.' ) if not is hangul ( letter ) : raise Not Hangul Exception ( 'The target string must be Hangul' ) code = lt . hangul index ( letter ) return code % NUM JONG > 0", "predictions": ["check if a letter string is hangul"], "references": ["check whether this letter contains jongsung"], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 1284, "code": "def attach ( word , josa = EUN NEUN ) : last letter = word . strip ( ) [ - 1 ] try : , , letter jong = letter . decompose ( last letter ) except Not Hangul Exception : letter jong = letter . get substituent of ( last letter ) if letter jong in ( '' , josa [ 'except' ] ) : return word + josa [ 'has' ] return word + josa [ 'not' ]", "predictions": ["attach a word to the letter letter"], "references": ["add josa at the end of this word"], "bleu": 0.17820132316770915, "rouge_l": 0.13174946004319654}
{"id": 1285, "code": "def is inside except ( node ) : current = node while current and not isinstance ( current . parent , astroid . Except Handler ) : current = current . parent return current and current is current . parent . name", "predictions": ["return true if node is inside the current node ."], "references": ["returns true if node is inside the name of an except handler ."], "bleu": 0.43711747430744424, "rouge_l": 0.5947075208913649}
{"id": 1286, "code": "def is inside lambda ( node : astroid . node classes . Node NG ) -> bool : parent = node . parent while parent is not None : if isinstance ( parent , astroid . Lambda ) : return True parent = parent . parent return False", "predictions": ["return true if node is inside the parent lambda ."], "references": ["return true if given node is inside lambda"], "bleu": 0.3508439695638686, "rouge_l": 0.7936802973977695}
{"id": 1287, "code": "def get all elements ( node : astroid . node classes . Node NG ) -> Iterable [ astroid . node classes . Node NG ] : if isinstance ( node , ( astroid . Tuple , astroid . List ) ) : for child in node . elts : for e in get all elements ( child ) : yield e else : yield node", "predictions": ["return all elements of a node ."], "references": ["recursively returns all atoms in nested lists and tuples ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 1288, "code": "def is super ( node : astroid . node classes . Node NG ) -> bool : if getattr ( node , \"name\" , None ) == \"super\" and node . root ( ) . name == BUILTINS NAME : return True return False", "predictions": ["check if node is super ."], "references": ["return true if the node is referencing the super builtin function"], "bleu": 0.1435549295013305, "rouge_l": 0.4468864468864468}
{"id": 1289, "code": "def is error ( node : astroid . node classes . Node NG ) -> bool : for child node in node . get children ( ) : if isinstance ( child node , astroid . Raise ) : return True return False", "predictions": ["return true if the node is an error ."], "references": ["return true if the function does nothing but raising an exception"], "bleu": 0.3292100646487161, "rouge_l": 0.4911433172302737}
{"id": 1290, "code": "def is builtin object ( node : astroid . node classes . Node NG ) -> bool : return node and node . root ( ) . name == BUILTINS NAME", "predictions": ["return true if node is a builtin object ."], "references": ["returns true if the given node is an object from the __builtin__ module ."], "bleu": 0.14577145122121135, "rouge_l": 0.5020576131687242}
{"id": 1291, "code": "def is func decorator ( node : astroid . node classes . Node NG ) -> bool : parent = node . parent while parent is not None : if isinstance ( parent , astroid . Decorators ) : return True if parent . is statement or isinstance ( parent , ( astroid . Lambda , scoped nodes . Comprehension Scope , scoped nodes . List Comp ) , ) : break parent = parent . parent return False", "predictions": ["check if the node is decorator ."], "references": ["return true if the name is used in function decorator"], "bleu": 0.18094495256969623, "rouge_l": 0.45607476635514016}
{"id": 1292, "code": "def assign parent ( node : astroid . node classes . Node NG ) -> astroid . node classes . Node NG : while node and isinstance ( node , ( astroid . Assign Name , astroid . Tuple , astroid . List ) ) : node = node . parent return node", "predictions": ["assign the parent node to the astroid ."], "references": ["return the higher parent which is not an assignname tuple or list node"], "bleu": 0.10207878682119532, "rouge_l": 0.2739520958083832}
{"id": 1293, "code": "def check messages ( * messages : str ) -> Callable : def store messages ( func ) : func . checks msgs = messages return func return store messages", "predictions": ["check if messages are checks ."], "references": ["decorator to store messages that are handled by a checker method"], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 1294, "code": "def decorated with property ( node : astroid . Function Def ) -> bool : if not node . decorators : return False for decorator in node . decorators . nodes : if not isinstance ( decorator , astroid . Name ) : continue try : if is property decorator ( decorator ) : return True except astroid . Inference Error : pass return False", "predictions": ["check if the node is decorated with the given property ."], "references": ["detect if the given function node is decorated with a property ."], "bleu": 0.3959377364332706, "rouge_l": 0.6902404526166903}
{"id": 1295, "code": "def decorated with ( func : astroid . Function Def , qnames : Iterable [ str ] ) -> bool : decorators = func . decorators . nodes if func . decorators else [ ] for decorator node in decorators : try : if any ( i is not None and i . qname ( ) in qnames for i in decorator node . infer ( ) ) : return True except astroid . Inference Error : continue return False", "predictions": ["check if the function is decorated with the given function ."], "references": ["determine if the func node has a decorator with the qualified name qname ."], "bleu": 0.15020004628709785, "rouge_l": 0.39152759948652116}
{"id": 1296, "code": "def find try except wrapper node ( node : astroid . node classes . Node NG ) -> Union [ astroid . Except Handler , astroid . Try Except ] : current = node ignores = ( astroid . Except Handler , astroid . Try Except ) while current and not isinstance ( current . parent , ignores ) : current = current . parent if current and isinstance ( current . parent , ignores ) : return current . parent return None", "predictions": ["find the node in the astroid ."], "references": ["return the excepthandler or the tryexcept node in which the node is ."], "bleu": 0.13653323887370866, "rouge_l": 0.474339035769829}
{"id": 1297, "code": "def is from fallback block ( node : astroid . node classes . Node NG ) -> bool : context = find try except wrapper node ( node ) if not context : return False if isinstance ( context , astroid . Except Handler ) : other body = context . parent . body handlers = context . parent . handlers else : other body = itertools . chain . from iterable ( handler . body for handler in context . handlers ) handlers = context . handlers has fallback imports = any ( isinstance ( import node , ( astroid . Import From , astroid . Import ) ) for import node in other body ) ignores import error = except handlers ignores exception ( handlers , Import Error ) return ignores import error or has fallback imports", "predictions": ["return true if node is from fallback block ."], "references": ["check if the given node is from a fallback import block ."], "bleu": 0.24053181705015034, "rouge_l": 0.6499238964992391}
{"id": 1298, "code": "def is registered in singledispatch function ( node : astroid . Function Def ) -> bool : singledispatch qnames = ( \"functools.singledispatch\" , \"singledispatch.singledispatch\" , ) if not isinstance ( node , astroid . Function Def ) : return False decorators = node . decorators . nodes if node . decorators else [ ] for decorator in decorators : if not isinstance ( decorator , astroid . Call ) : continue func = decorator . func if not isinstance ( func , astroid . Attribute ) or func . attrname != \"register\" : continue try : func def = next ( func . expr . infer ( ) ) except astroid . Inference Error : continue if isinstance ( func def , astroid . Function Def ) : return decorated with ( func def , singledispatch qnames ) return False", "predictions": ["check if the function is registered in the singledispatch function ."], "references": ["check if the given function node is a singledispatch function ."], "bleu": 0.3264971028628052, "rouge_l": 0.7272727272727273}
{"id": 1299, "code": "def is postponed evaluation enabled ( node : astroid . node classes . Node NG ) -> bool : name = \"annotations\" module = node . root ( ) stmt = module . locals . get ( name ) return ( stmt and isinstance ( stmt [ 0 ] , astroid . Import From ) and stmt [ 0 ] . modname == \" future \" )", "predictions": ["return true if node is enabled ."], "references": ["check if the postponed evaluation of annotations is enabled"], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 1300, "code": "def repr tree defs ( data , indent str = None ) : lines = [ ] nodes = data . items ( ) for i , ( mod , ( sub , files ) ) in enumerate ( sorted ( nodes , key = lambda x : x [ 0 ] ) ) : if not files : files = \"\" else : files = \"(%s)\" % \",\" . join ( sorted ( files ) ) if indent str is None : lines . append ( \"%s %s\" % ( mod , files ) ) sub indent str = \"  \" else : lines . append ( r\"%s\\-%s %s\" % ( indent str , mod , files ) ) if i == len ( nodes ) - 1 : sub indent str = \"%s  \" % indent str else : sub indent str = \"%s| \" % indent str if sub : lines . append ( repr tree defs ( sub , sub indent str ) ) return \"\\n\" . join ( lines )", "predictions": ["pretty - print the tree of the data in a string ."], "references": ["return a string which represents imports as a tree"], "bleu": 0.14694106251955755, "rouge_l": 0.19551282051282048}
{"id": 1301, "code": "def visit import ( self , node ) : self . check reimport ( node ) self . check import as rename ( node ) modnode = node . root ( ) names = [ name for name , in node . names ] if len ( names ) >= 2 : self . add message ( \"multiple-imports\" , args = \", \" . join ( names ) , node = node ) for name in names : self . check deprecated module ( node , name ) self . check preferred module ( node , name ) imported module = self . get imported module ( node , name ) if isinstance ( node . parent , astroid . Module ) : self . check position ( node ) if isinstance ( node . scope ( ) , astroid . Module ) : self . record import ( node , imported module ) if imported module is None : continue self . check relative import ( modnode , node , imported module , name ) self . add imported module ( node , imported module . name )", "predictions": ["visit an import node ."], "references": ["triggered when an import statement is seen"], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 1302, "code": "def visit importfrom ( self , node ) : basename = node . modname imported module = self . get imported module ( node , basename ) self . check import as rename ( node ) self . check misplaced future ( node ) self . check deprecated module ( node , basename ) self . check preferred module ( node , basename ) self . check wildcard imports ( node , imported module ) self . check same line imports ( node ) self . check reimport ( node , basename = basename , level = node . level ) if isinstance ( node . parent , astroid . Module ) : self . check position ( node ) if isinstance ( node . scope ( ) , astroid . Module ) : self . record import ( node , imported module ) if imported module is None : return modnode = node . root ( ) self . check relative import ( modnode , node , imported module , basename ) for name , in node . names : if name != \"*\" : self . add imported module ( node , \"%s.%s\" % ( imported module . name , name ) ) else : self . add imported module ( node , imported module . name )", "predictions": ["visitor for functioncall ast node ."], "references": ["triggered when a from statement is seen"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 1303, "code": "def record import ( self , node , importedmodnode ) : if isinstance ( node , astroid . Import From ) : importedname = node . modname else : importedname = importedmodnode . name if importedmodnode else None if not importedname : importedname = node . names [ 0 ] [ 0 ] . split ( \".\" ) [ 0 ] if isinstance ( node , astroid . Import From ) and ( node . level or 0 ) >= 1 : importedname = \".\" + importedname self . imports stack . append ( ( node , importedname ) )", "predictions": ["add a astroid to the astroid"], "references": ["record the package node imports from"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 1304, "code": "def add imported module ( self , node , importedmodname ) : module file = node . root ( ) . file context name = node . root ( ) . name base = os . path . splitext ( os . path . basename ( module file ) ) [ 0 ] try : importedmodname = astroid . modutils . get module part ( importedmodname , module file ) except Import Error : pass if context name == importedmodname : self . add message ( \"import-self\" , node = node ) elif not astroid . modutils . is standard module ( importedmodname ) : if base != \" init \" and context name not in self . module pkg : self . module pkg [ context name ] = context name . rsplit ( \".\" , 1 ) [ 0 ] importedmodnames = self . stats [ \"dependencies\" ] . setdefault ( importedmodname , set ( ) ) if context name not in importedmodnames : importedmodnames . add ( context name ) self . import graph [ context name ] . add ( importedmodname ) if not self . linter . is message enabled ( \"cyclic-import\" , line = node . lineno ) : self . excluded edges [ context name ] . add ( importedmodname )", "predictions": ["add a module to the module ."], "references": ["notify an imported module used to analyze dependencies"], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 1305, "code": "def check deprecated module ( self , node , mod path ) : for mod name in self . config . deprecated modules : if mod path == mod name or mod path . startswith ( mod name + \".\" ) : self . add message ( \"deprecated-module\" , node = node , args = mod path )", "predictions": ["check if a node is deprecated"], "references": ["check if the module is deprecated"], "bleu": 0.3655552228545124, "rouge_l": 0.6666666666666666}
{"id": 1306, "code": "def check preferred module ( self , node , mod path ) : if mod path in self . preferred modules : self . add message ( \"preferred-module\" , node = node , args = ( self . preferred modules [ mod path ] , mod path ) , )", "predictions": ["check if a preferred module is not a preferred module ."], "references": ["check if the module has a preferred replacement"], "bleu": 0.1972940627795883, "rouge_l": 0.5417406749555951}
{"id": 1307, "code": "def report external dependencies ( self , sect , , dummy ) : dep info = make tree defs ( self . external dependencies info ( ) . items ( ) ) if not dep info : raise Empty Report Error ( ) tree str = repr tree defs ( dep info ) sect . append ( Verbatim Text ( tree str ) )", "predictions": ["report external dependencies of external dependencies ."], "references": ["return a verbatim layout for displaying dependencies"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 1308, "code": "def filter dependencies graph ( self , internal ) : graph = collections . defaultdict ( set ) for importee , importers in self . stats [ \"dependencies\" ] . items ( ) : for importer in importers : package = self . module pkg . get ( importer , importer ) is inside = importee . startswith ( package ) if is inside and internal or not is inside and not internal : graph [ importee ] . add ( importer ) return graph", "predictions": ["filter out the dependencies of the graph ."], "references": ["build the internal or the external depedency graph"], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 1309, "code": "def get default options ( ) : options = [ ] home = os . environ . get ( \"HOME\" , \"\" ) if home : rcfile = os . path . join ( home , RCFILE ) try : options = open ( rcfile ) . read ( ) . split ( ) except IO Error : pass return options", "predictions": ["return the default options ."], "references": ["read config file and return list of options"], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 1310, "code": "def insert default options ( ) : options = get default options ( ) options . reverse ( ) for arg in options : sys . argv . insert ( 1 , arg )", "predictions": ["insert default options ."], "references": ["insert default options to sys . argv"], "bleu": 0.3340135926488844, "rouge_l": 0.693181818181818}
{"id": 1311, "code": "def show attr ( self , node ) : visibility = get visibility ( getattr ( node , \"name\" , node ) ) return not self . mode & VIS MOD [ visibility ]", "predictions": ["show attribute attributes ."], "references": ["return true if the node should be treated"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 1312, "code": "def get callbacks ( self , node ) : klass = node . class methods = self . cache . get ( klass ) if methods is None : handler = self . handler kid = klass . name . lower ( ) e method = getattr ( handler , \"visit %s\" % kid , getattr ( handler , \"visit default\" , None ) ) l method = getattr ( handler , \"leave %s\" % kid , getattr ( handler , \"leave default\" , None ) ) self . cache [ klass ] = ( e method , l method ) else : e method , l method = methods return e method , l method", "predictions": ["returns the payload for the given node in this class in the cache"], "references": ["get callbacks from handler for the visited node"], "bleu": 0.1350862565735141, "rouge_l": 0.2985318107667211}
{"id": 1313, "code": "def visit ( self , node ) : if node in self . visited : return None self . visited [ node ] = 1 methods = self . get callbacks ( node ) if methods [ 0 ] is not None : methods [ 0 ] ( node ) if hasattr ( node , \"locals\" ) : for local node in node . values ( ) : self . visit ( local node ) if methods [ 1 ] is not None : return methods [ 1 ] ( node ) return None", "predictions": ["visitor for node ast node ast node ."], "references": ["launch the visit starting from the given node"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 1314, "code": "def visit call ( self , node ) : try : for inferred in node . func . infer ( ) : if inferred is astroid . Uninferable : continue elif inferred . root ( ) . name == OPEN MODULE : if getattr ( node . func , \"name\" , None ) in OPEN FILES : self . check open mode ( node ) elif inferred . root ( ) . name == UNITTEST CASE : self . check redundant assert ( node , inferred ) elif isinstance ( inferred , astroid . Class Def ) : if inferred . qname ( ) == THREADING THREAD : self . check bad thread instantiation ( node ) elif inferred . qname ( ) == SUBPROCESS POPEN : self . check for preexec fn in popen ( node ) elif isinstance ( inferred , astroid . Function Def ) : name = inferred . qname ( ) if name == COPY COPY : self . check shallow copy environ ( node ) elif name in ENV GETTERS : self . check env function ( node , inferred ) elif name == SUBPROCESS RUN and PY35 : self . check for check kw in run ( node ) self . check deprecated method ( node , inferred ) except astroid . Inference Error : return", "predictions": ["decompose an astroid . node . . ."], "references": ["visit a call node ."], "bleu": 0.21105340631872638, "rouge_l": 0.32105263157894737}
{"id": 1315, "code": "def check open mode ( self , node ) : try : mode arg = utils . get argument from call ( node , position = 1 , keyword = \"mode\" ) except utils . No Such Argument Error : return if mode arg : mode arg = utils . safe infer ( mode arg ) if isinstance ( mode arg , astroid . Const ) and not check mode str ( mode arg . value ) : self . add message ( \"bad-open-mode\" , node = node , args = mode arg . value )", "predictions": ["has the open mode mode"], "references": ["check that the mode argument of an open or file call is valid ."], "bleu": 0.053667245469253895, "rouge_l": 0.19395866454689983}
{"id": 1316, "code": "def handle message ( self , msg ) : self . messages . append ( { \"type\" : msg . category , \"module\" : msg . module , \"obj\" : msg . obj , \"line\" : msg . line , \"column\" : msg . column , \"path\" : msg . path , \"symbol\" : msg . symbol , \"message\" : html . escape ( msg . msg or \"\" , quote = False ) , \"message-id\" : msg . msg id , } )", "predictions": ["attach a message to the client . . ."], "references": ["manage message of different type and in the context of path ."], "bleu": 0.12026590852507517, "rouge_l": 0.2785388127853881}
{"id": 1317, "code": "def get title ( self , node ) : title = node . name if self . module names : title = \"%s.%s\" % ( node . root ( ) . name , title ) return title", "predictions": ["return inside the node"], "references": ["get title for objects"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 1318, "code": "def set default options ( self ) : self . module names = self . set option ( self . config . module names ) all ancestors = self . set option ( self . config . all ancestors ) all associated = self . set option ( self . config . all associated ) anc level , association level = ( 0 , 0 ) if all ancestors : anc level = - 1 if all associated : association level = - 1 if self . config . show ancestors is not None : anc level = self . config . show ancestors if self . config . show associated is not None : association level = self . config . show associated self . anc level , self . association level = anc level , association level", "predictions": ["is the inside the inside the inside the inside"], "references": ["set different default options with _default dictionary"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1319, "code": "def show node ( self , node ) : if self . config . show builtin : return True return node . root ( ) . name != BUILTINS NAME", "predictions": ["return all the nodes of a all the all nodes"], "references": ["true if builtins and not show_builtins"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1320, "code": "def add class ( self , node ) : self . linker . visit ( node ) self . classdiagram . add object ( self . get title ( node ) , node )", "predictions": ["is the current super super super"], "references": ["visit one class and add it to diagram"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 1321, "code": "def get ancestors ( self , node , level ) : if level == 0 : return for ancestor in node . ancestors ( recurs = False ) : if not self . show node ( ancestor ) : continue yield ancestor", "predictions": ["return all error error error error for a given astroid get ."], "references": ["return ancestor nodes of a class node"], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 1322, "code": "def get associated ( self , klass node , level ) : if level == 0 : return for association nodes in list ( klass node . instance attrs type . values ( ) ) + list ( klass node . locals type . values ( ) ) : for node in association nodes : if isinstance ( node , astroid . Instance ) : node = node . proxied if not ( isinstance ( node , astroid . Class Def ) and self . show node ( node ) ) : continue yield node", "predictions": ["return all builtin builtin root of a given astroid ."], "references": ["return associated nodes of a class node"], "bleu": 0.17827531042796255, "rouge_l": 0.36454183266932266}
{"id": 1323, "code": "def extract classes ( self , klass node , anc level , association level ) : if self . classdiagram . has node ( klass node ) or not self . show node ( klass node ) : return self . add class ( klass node ) for ancestor in self . get ancestors ( klass node , anc level ) : self . extract classes ( ancestor , anc level - 1 , association level ) for node in self . get associated ( klass node , association level ) : self . extract classes ( node , anc level , association level - 1 )", "predictions": ["is the first astroid of the given klass"], "references": ["extract recursively classes related to klass_node"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1324, "code": "def visit importfrom ( self , node ) : if self . pkgdiagram : self . pkgdiagram . add from depend ( node , node . modname )", "predictions": ["visitor for depend ast astroid ."], "references": ["visit astroid . importfrom and catch modules for package diagram"], "bleu": 0.16038842424444547, "rouge_l": 0.23921568627450981}
{"id": 1325, "code": "def has parent of type ( node , node type , statement ) : parent = node . parent while not isinstance ( parent , node type ) and statement . parent of ( parent ) : parent = parent . parent return isinstance ( parent , node type )", "predictions": ["return must be a messages of or none"], "references": ["check if the given node has a parent of the given type ."], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 1326, "code": "def is name used as variadic ( name , variadics ) : return any ( variadic . value == name or variadic . value . parent of ( name ) for variadic in variadics )", "predictions": ["return is any or not"], "references": ["check if the given name is used as a variadic argument ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 1327, "code": "def register ( linter ) : linter . register checker ( Type Checker ( linter ) ) linter . register checker ( Iterable Checker ( linter ) )", "predictions": ["decorated method to decorated ."], "references": ["required method to auto register this checker"], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 1328, "code": "def visit unaryop ( self , node ) : for error in node . type errors ( ) : self . add message ( \"invalid-unary-operand-type\" , args = str ( error ) , node = node )", "predictions": ["check if try is try to try to try to try to try to try to try to the try and add ."], "references": ["detect typeerrors for unary operands ."], "bleu": 0.05291907393644996, "rouge_l": 0.07711757269279393}
{"id": 1329, "code": "def interfaces ( node , herited = True , handler func = iface hdlr ) : try : implements = bases . Instance ( node ) . getattr ( \" implements \" ) [ 0 ] except exceptions . Not Found Error : return if not herited and implements . frame ( ) is not node : return found = set ( ) missing = False for iface in node classes . unpack infer ( implements ) : if iface is astroid . Uninferable : missing = True continue if iface not in found and handler func ( iface ) : found . add ( iface ) yield iface if missing : raise exceptions . Inference Error ( )", "predictions": ["return an iterator over all is is available in the given fallback = value = value"], "references": ["return an iterator on interfaces implemented by the given class node ."], "bleu": 0.1702602472176709, "rouge_l": 0.36658653846153844}
{"id": 1330, "code": "def project from files ( files , func wrapper = astroid wrapper , project name = \"no name\" , black list = ( \"CVS\" , ) ) : astroid manager = manager . Astroid Manager ( ) project = Project ( project name ) for something in files : if not os . path . exists ( something ) : fpath = modutils . file from modpath ( something . split ( \".\" ) ) elif os . path . isdir ( something ) : fpath = os . path . join ( something , \" init .py\" ) else : fpath = something ast = func wrapper ( astroid manager . ast from file , fpath ) if ast is None : continue project . path = project . path or ast . file project . add module ( ast ) base name = ast . name if ast . package and something . find ( \" init \" ) == - 1 : for fpath in modutils . get module files ( os . path . dirname ( ast . file ) , black list ) : ast = func wrapper ( astroid manager . ast from file , fpath ) if ast is None or ast . name == base name : continue project . add module ( ast ) return project", "predictions": ["return function to create a is a is a is is not a is passed to the function"], "references": ["return a project from a list of files or modules"], "bleu": 0.08097785064266204, "rouge_l": 0.2259259259259259}
{"id": 1331, "code": "def compute module ( self , context name , mod path ) : package dir = os . path . dirname ( self . project . path ) if context name == mod path : return 0 if modutils . is standard module ( mod path , ( package dir , ) ) : return 1 return 0", "predictions": ["is the module a module"], "references": ["return true if the module should be added to dependencies"], "bleu": 0.13218059591958078, "rouge_l": 0.2515463917525773}
{"id": 1332, "code": "def imported module ( self , node , mod path , relative ) : module = node . root ( ) context name = module . name if relative : mod path = \"%s.%s\" % ( \".\" . join ( context name . split ( \".\" ) [ : - 1 ] ) , mod path ) if self . compute module ( context name , mod path ) : if not hasattr ( module , \"depends\" ) : module . depends = [ ] mod paths = module . depends if mod path not in mod paths : mod paths . append ( mod path )", "predictions": ["key is repr of the tree"], "references": ["notify an imported module used to analyze dependencies"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 1333, "code": "def register ( linter ) : linter . register reporter ( Text Reporter ) linter . register reporter ( Parseable Text Reporter ) linter . register reporter ( VS Text Reporter ) linter . register reporter ( Colorized Text Reporter )", "predictions": ["visit the whole whole whole linter node node node node node node node node node node node node node node node node node node node node node node node node node"], "references": ["register the reporter classes with the linter ."], "bleu": 0.04317900023606586, "rouge_l": 0.1147695202257761}
{"id": 1334, "code": "def handle message ( self , msg ) : if msg . module not in self . modules : if msg . module : self . writeln ( \"************* Module %s\" % msg . module ) self . modules . add ( msg . module ) else : self . writeln ( \"************* \" ) self . write message ( msg )", "predictions": ["visit a importfrom importfrom importfrom"], "references": ["manage message of different type and in the context of path"], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 1335, "code": "def open graph ( self , * * args ) : self . stream . write ( \"%sgraph:{\\n\" % self . indent ) self . inc indent ( ) self . write attributes ( GRAPH ATTRS , * * args )", "predictions": ["record the import import import import"], "references": ["open a vcg graph"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1336, "code": "def edge ( self , from node , to node , edge type = \"\" , * * args ) : self . stream . write ( '%s%sedge: {sourcename:\"%s\" targetname:\"%s\"' % ( self . indent , edge type , from node , to node ) ) self . write attributes ( EDGE ATTRS , * * args ) self . stream . write ( \"}\\n\" )", "predictions": ["print an add add an add add add add a add add add add add add an add add add add add an add add add add add add add add"], "references": ["draw an edge from a node to another ."], "bleu": 0.04317900023606586, "rouge_l": 0.11101000909918107}
{"id": 1337, "code": "def write attributes ( self , attributes dict , * * args ) : for key , value in args . items ( ) : try : type = attributes dict [ key ] except Key Error : raise Exception ( % ( key , attributes dict . keys ( ) ) ) if not type : self . stream . write ( '%s%s:\"%s\"\\n' % ( self . indent , key , value ) ) elif type == 1 : self . stream . write ( \"%s%s:%s\\n\" % ( self . indent , key , int ( value ) ) ) elif value in type : self . stream . write ( \"%s%s:%s\\n\" % ( self . indent , key , value ) ) else : raise Exception ( % ( value , key , type ) )", "predictions": ["check if the deprecated deprecated is not none"], "references": ["write graph node or edge attributes"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1338, "code": "def register ( linter ) : linter . register checker ( String Format Checker ( linter ) ) linter . register checker ( String Constant Checker ( linter ) )", "predictions": ["check that the node is required"], "references": ["required method to auto register this checker"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 1339, "code": "def check new format ( self , node , func ) : # # if isinstance ( node . func , astroid . Attribute ) and not isinstance ( node . func . expr , astroid . Const ) : return if node . starargs or node . kwargs : return try : strnode = next ( func . bound . infer ( ) ) except astroid . Inference Error : return if not ( isinstance ( strnode , astroid . Const ) and isinstance ( strnode . value , str ) ) : return try : call site = Call Site . from call ( node ) except astroid . Inference Error : return try : fields , num args , manual pos = utils . parse format method string ( strnode . value ) except utils . Incomplete Format String : self . add message ( \"bad-format-string\" , node = node ) return positional arguments = call site . positional arguments named arguments = call site . keyword arguments named fields = { field [ 0 ] for field in fields if isinstance ( field [ 0 ] , str ) } if num args and manual pos : self . add message ( \"format-combined-specification\" , node = node ) return check args = False num args += sum ( 1 for field in named fields if field == \"\" ) if named fields : for field in named fields : if field and field not in named arguments : self . add message ( \"missing-format-argument-key\" , node = node , args = ( field , ) ) for field in named arguments : if field not in named fields : self . add message ( \"unused-format-string-argument\" , node = node , args = ( field , ) ) num args = num args or manual pos if positional arguments or num args : empty = any ( True for field in named fields if field == \"\" ) if named arguments or empty : check args = True else : check args = True if check args : num args = num args or manual pos if len ( positional arguments ) > num args : self . add message ( \"too-many-format-args\" , node = node ) elif len ( positional arguments ) < num args : self . add message ( \"too-few-format-args\" , node = node ) self . detect vacuous formatting ( node , positional arguments ) self . check new format specifiers ( node , fields , named arguments )", "predictions": ["report if the dependencies of the named is valid make sure it s not in the dependencies make sure that the dependencies make sure it s a external dependencies make sure"], "references": ["check the new string formatting ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 1340, "code": "def visit section ( self , layout ) : self . section += 1 self . writeln ( ) self . format children ( layout ) self . section -= 1 self . writeln ( )", "predictions": ["filter the dependencies of this dependencies . . . . . . . . . ."], "references": ["display a section as text"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 1341, "code": "def visit evaluationsection ( self , layout ) : self . section += 1 self . format children ( layout ) self . section -= 1 self . writeln ( )", "predictions": ["set the default value of this node . . . . . . . . . ."], "references": ["display an evaluation section as a text ."], "bleu": 0.07223943354597204, "rouge_l": 0.08555399719495091}
{"id": 1342, "code": "def visit table ( self , layout ) : table content = self . get table content ( layout ) cols width = [ 0 ] * len ( table content [ 0 ] ) for row in table content : for index , col in enumerate ( row ) : cols width [ index ] = max ( cols width [ index ] , len ( col ) ) self . default table ( layout , table content , cols width ) self . writeln ( )", "predictions": ["generates a default default default default default default default default default default default default default default default default default sys for the given layout ."], "references": ["display a table as text"], "bleu": 0.048589719316429775, "rouge_l": 0.07577639751552796}
{"id": 1343, "code": "def check symbol ( self , msgid , symbol ) : other message = self . messages definitions . get ( symbol ) if other message : self . raise duplicate msg id ( symbol , msgid , other message . msgid ) else : alternative msgid = None alternative message = self . alternative names . get ( symbol ) if alternative message : if alternative message . symbol == symbol : alternative msgid = alternative message . msgid else : for old msgid , old symbol in alternative message . old names : if old symbol == symbol : alternative msgid = old msgid break if msgid != alternative msgid : self . raise duplicate msg id ( symbol , msgid , alternative msgid )", "predictions": ["show a attr that was received from the attr"], "references": ["check that a symbol is not already used ."], "bleu": 0.15619699684601276, "rouge_l": 0.1111111111111111}
{"id": 1344, "code": "def help message ( self , msgids ) : for msgid in msgids : try : for message definition in self . get message definitions ( msgid ) : print ( message definition . format help ( checkerref = True ) ) print ( \"\" ) except Unknown Message Error as ex : print ( ex ) print ( \"\" ) continue", "predictions": ["prints a help message"], "references": ["display help messages for the given message identifiers"], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 1345, "code": "def list messages ( self ) : messages = sorted ( self . messages definitions . values ( ) , key = lambda m : m . msgid ) for message in messages : if not message . may be emitted ( ) : continue print ( message . format help ( checkerref = False ) ) print ( \"\" )", "predictions": ["print all messages in the terminal ."], "references": ["output full messages list documentation in rest format ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 1346, "code": "def builder inited ( app ) : base path = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( file ) ) ) ) ext path = os . path . join ( base path , \"pylint\" , \"extensions\" ) modules = [ ] doc files = { } for filename in os . listdir ( ext path ) : name , ext = os . path . splitext ( filename ) if name [ 0 ] == \" \" or name in DEPRECATED MODULES : continue if ext == \".py\" : modules . append ( \"pylint.extensions.%s\" % name ) elif ext == \".rst\" : doc files [ \"pylint.extensions.\" + name ] = os . path . join ( ext path , filename ) modules . sort ( ) if not modules : sys . exit ( \"No Pylint extensions found?\" ) linter = Py Linter ( ) linter . load plugin modules ( modules ) extensions doc = os . path . join ( base path , \"doc\" , \"technical reference\" , \"extensions.rst\" ) with open ( extensions doc , \"w\" ) as stream : stream . write ( \"Optional Pylint checkers in the extensions module\\n\" ) stream . write ( \"=================================================\\n\\n\" ) stream . write ( \"Pylint provides the following optional plugins:\\n\\n\" ) for module in modules : stream . write ( \"- :ref:`{}`\\n\" . format ( module ) ) stream . write ( \"\\n\" ) stream . write ( \"You can activate any or all of these extensions \" \"by adding a ``load-plugins`` line to the ``MASTER`` \" \"section of your ``.pylintrc``, for example::\\n\" ) stream . write ( \"\\n    load-plugins=pylint.extensions.docparams,\" \"pylint.extensions.docstyle\\n\\n\" ) by module = get plugins info ( linter , doc files ) for module , info in sorted ( by module . items ( ) ) : linter . print checker doc ( info [ \"name\" ] , info , stream = stream )", "predictions": ["builder for the inited inited ."], "references": ["output full documentation in rest format for all extension modules"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 1347, "code": "def cpu count ( ) -> int : sched getaffinity = getattr ( os , \"sched getaffinity\" , None ) if sched getaffinity : return len ( sched getaffinity ( 0 ) ) if multiprocessing : return multiprocessing . cpu count ( ) return 1", "predictions": ["return the number of cpu in the system"], "references": ["use sched_affinity if available for virtualized or containerized environments ."], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 1348, "code": "def report messages stats ( sect , stats , ) : if not stats [ \"by msg\" ] : raise exceptions . Empty Report Error ( ) in order = sorted ( [ ( value , msg id ) for msg id , value in stats [ \"by msg\" ] . items ( ) if not msg id . startswith ( \"I\" ) ] ) in order . reverse ( ) lines = ( \"message id\" , \"occurrences\" ) for value , msg id in in order : lines += ( msg id , str ( value ) ) sect . append ( report nodes . Table ( children = lines , cols = 2 , rheaders = 1 ) )", "predictions": ["report messages from the order of the messages"], "references": ["make messages type report"], "bleu": 0.17747405280050269, "rouge_l": 0.17732558139534885}
{"id": 1349, "code": "def python3 porting mode ( self ) : self . disable ( \"all\" ) self . enable ( \"python3\" ) if self . error mode : for msg id in self . checker messages ( \"python3\" ) : if msg id . startswith ( \"E\" ) : self . enable ( msg id ) else : self . disable ( msg id ) config parser = self . cfgfile parser if config parser . has option ( \"MESSAGES CONTROL\" , \"disable\" ) : value = config parser . get ( \"MESSAGES CONTROL\" , \"disable\" ) self . global set option ( \"disable\" , value ) self . python3 porting mode = True", "predictions": ["disable the porting mode ."], "references": ["disable all other checkers and enable python 3 warnings ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 1350, "code": "def get checkers ( self ) : return [ self ] + [ c for checkers in self . checkers . values ( ) for c in checkers if c is not self ]", "predictions": ["return the list of checkers in this collection ."], "references": ["return all available checkers as a list"], "bleu": 0.16784459625186196, "rouge_l": 0.2557651991614256}
{"id": 1351, "code": "def get checker names ( self ) : current checkers = self . get checkers ( ) return sorted ( { check . name for check in current checkers if check . name != \"master\" } )", "predictions": ["return a list of all checker names in this collection ."], "references": ["get all the checker names that this linter knows about ."], "bleu": 0.17827531042796255, "rouge_l": 0.45454545454545453}
{"id": 1352, "code": "def prepare checkers ( self ) : if not self . config . reports : self . disable reporters ( ) neededcheckers = [ self ] for checker in self . get checkers ( ) [ 1 : ] : messages = { msg for msg in checker . msgs if self . is message enabled ( msg ) } if messages or any ( self . report is enabled ( r [ 0 ] ) for r in checker . reports ) : neededcheckers . append ( checker ) neededcheckers = sorted ( neededcheckers , key = operator . attrgetter ( \"priority\" ) , reverse = True ) return neededcheckers", "predictions": ["prepare the checker s checkers checkers ."], "references": ["return checkers needed for activated messages and reports"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1353, "code": "def expand files ( self , modules ) : result , errors = utils . expand modules ( modules , self . config . black list , self . config . black list re ) for error in errors : message = modname = error [ \"mod\" ] key = error [ \"key\" ] self . set current module ( modname ) if key == \"fatal\" : message = str ( error [ \"ex\" ] ) . replace ( os . getcwd ( ) + os . sep , \"\" ) self . add message ( key , args = message ) return result", "predictions": ["expand all files in the current module ."], "references": ["get modules and errors from a list of modules and handle errors"], "bleu": 0.08179133792443427, "rouge_l": 0.0}
{"id": 1354, "code": "def check astroid module ( self , ast node , walker , rawcheckers , tokencheckers ) : try : tokens = utils . tokenize module ( ast node ) except tokenize . Token Error as ex : self . add message ( \"syntax-error\" , line = ex . args [ 1 ] [ 0 ] , args = ex . args [ 0 ] ) return None if not ast node . pure python : self . add message ( \"raw-checker-failed\" , args = ast node . name ) else : self . process tokens ( tokens ) if self . ignore file : return False self . file state . collect block lines ( self . msgs store , ast node ) for checker in rawcheckers : checker . process module ( ast node ) for checker in tokencheckers : checker . process tokens ( tokens ) walker . walk ( ast node ) return True", "predictions": ["check if the astroid is in the astroid ."], "references": ["check a module from its astroid representation ."], "bleu": 0.16784459625186196, "rouge_l": 0.35672514619883033}
{"id": 1355, "code": "def report evaluation ( self ) : previous stats = config . load results ( self . file state . base name ) if self . stats [ \"statement\" ] == 0 : return evaluation = self . config . evaluation try : note = eval ( evaluation , { } , self . stats ) except Exception as ex : msg = \"An exception occurred while rating: %s\" % ex else : self . stats [ \"global note\" ] = note msg = \"Your code has been rated at %.2f/10\" % note pnote = previous stats . get ( \"global note\" ) if pnote is not None : msg += \" (previous run: %.2f/10, %+.2f)\" % ( pnote , note - pnote ) if self . config . score : sect = report nodes . Evaluation Section ( msg ) self . reporter . display reports ( sect )", "predictions": ["report evaluation of evaluation evaluation ."], "references": ["make the global evaluation report"], "bleu": 0.24446151121745047, "rouge_l": 0.18484848484848485}
{"id": 1356, "code": "def cb generate config ( self , * args , * * kwargs ) : self . linter . generate config ( skipsections = ( \"COMMANDS\" , ) ) sys . exit ( 0 )", "predictions": ["generate the config ."], "references": ["optik callback for sample config file generation"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 1357, "code": "def cb generate manpage ( self , * args , * * kwargs ) : from pylint import pkginfo self . linter . generate manpage ( pkginfo ) sys . exit ( 0 )", "predictions": ["generate python code for the given python ."], "references": ["optik callback for sample config file generation"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 1358, "code": "def cb help message ( self , option , optname , value , parser ) : self . linter . msgs store . help message ( utils . splitstrip ( value ) ) sys . exit ( 0 )", "predictions": ["set the help message"], "references": ["optik callback for printing some help about a particular message"], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 1359, "code": "def cb full documentation ( self , option , optname , value , parser ) : self . linter . print full documentation ( ) sys . exit ( 0 )", "predictions": ["callback for printing a full documentation"], "references": ["optik callback for printing full documentation"], "bleu": 0.488923022434901, "rouge_l": 0.8333333333333334}
{"id": 1360, "code": "def cb list messages ( self , option , optname , value , parser ) : self . linter . msgs store . list messages ( ) sys . exit ( 0 )", "predictions": ["shows messages in list ."], "references": ["optik callback for printing available messages"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 1361, "code": "def normalize text ( text , line len = 80 , indent = \"\" ) : return \"\\n\" . join ( textwrap . wrap ( text , width = line len , initial indent = indent , subsequent indent = indent ) )", "predictions": ["normalize text by line ."], "references": ["wrap the text on the given line length ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 1362, "code": "def get module and frameid ( node ) : frame = node . frame ( ) module , obj = \"\" , [ ] while frame : if isinstance ( frame , Module ) : module = frame . name else : obj . append ( getattr ( frame , \"name\" , \"<lambda>\" ) ) try : frame = frame . parent . frame ( ) except Attribute Error : frame = None obj . reverse ( ) return module , \".\" . join ( obj )", "predictions": ["return the module and frameid of a module and frameid ."], "references": ["return the module name and the frame id in the module"], "bleu": 0.23462350320527994, "rouge_l": 0.45454545454545453}
{"id": 1363, "code": "def safe decode ( line , encoding , * args , * * kwargs ) : try : return line . decode ( encoding or sys . getdefaultencoding ( ) , * args , * * kwargs ) except Lookup Error : return line . decode ( sys . getdefaultencoding ( ) , * args , * * kwargs )", "predictions": ["decodes a line into a unicode string ."], "references": ["return decoded line from encoding or decode with default encoding"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 1364, "code": "def comment ( string ) : lines = [ line . strip ( ) for line in string . splitlines ( ) ] return + ( % linesep ) . join ( lines )", "predictions": ["return a comment of a string"], "references": ["return string as a comment"], "bleu": 0.3303164318013807, "rouge_l": 0.5545454545454546}
{"id": 1365, "code": "def format option value ( optdict , value ) : if isinstance ( value , ( list , tuple ) ) : value = \",\" . join ( format option value ( optdict , item ) for item in value ) elif isinstance ( value , dict ) : value = \",\" . join ( \"%s:%s\" % ( k , v ) for k , v in value . items ( ) ) elif hasattr ( value , \"match\" ) : value = value . pattern elif optdict . get ( \"type\" ) == \"yn\" : value = \"yes\" if value else \"no\" elif isinstance ( value , str ) and value . isspace ( ) : value = \"'%s'\" % value return value", "predictions": ["format the option value for the given option ."], "references": ["return the user input s value from a compiled value"], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 1366, "code": "def format section ( stream , section , options , doc = None ) : if doc : print ( comment ( doc ) , file = stream ) print ( \"[%s]\" % section , file = stream ) ini format ( stream , options )", "predictions": ["format a section into a json - serializable section ."], "references": ["format an options section using the ini format"], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 1367, "code": "def ini format ( stream , options ) : for optname , optdict , value in options : value = format option value ( optdict , value ) help opt = optdict . get ( \"help\" ) if help opt : help opt = normalize text ( help opt , line len = 79 , indent = ) print ( file = stream ) print ( help opt , file = stream ) else : print ( file = stream ) if value is None : print ( \"#%s=\" % optname , file = stream ) else : value = str ( value ) . strip ( ) if re . match ( r\"^([\\w-]+,)+[\\w-]+$\" , str ( value ) ) : separator = \"\\n \" + \" \" * len ( optname ) value = separator . join ( x + \",\" for x in str ( value ) . split ( \",\" ) ) value = value [ : - 1 ] print ( \"%s=%s\" % ( optname , value ) , file = stream )", "predictions": ["print the ini format of the stream ."], "references": ["format options using the ini format"], "bleu": 0.2984745896009823, "rouge_l": 0.43990384615384615}
{"id": 1368, "code": "def insert ( self , index , child ) : self . children . insert ( index , child ) child . parent = self", "predictions": ["insert a child at index"], "references": ["insert a child node"], "bleu": 0.5081327481546147, "rouge_l": 0.6802973977695167}
{"id": 1369, "code": "def append ( self , child ) : assert child not in self . parents ( ) V Node . append ( self , child )", "predictions": ["append child to child ."], "references": ["overridden to detect problems easily"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 1370, "code": "def parents ( self ) : assert self . parent is not self if self . parent is None : return [ ] return [ self . parent ] + self . parent . parents ( )", "predictions": ["returns the parents of this node ."], "references": ["return the ancestor nodes"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 1371, "code": "def collect block lines ( self , msgs store , module node ) : for msg , lines in self . module msgs state . items ( ) : self . raw module msgs state [ msg ] = lines . copy ( ) orig state = self . module msgs state . copy ( ) self . module msgs state = { } self . suppression mapping = { } self . effective max line number = module node . tolineno self . collect block lines ( msgs store , module node , orig state )", "predictions": ["collect all lines from a module ."], "references": ["walk the ast to collect block level options line numbers ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 1372, "code": "def enable report ( self , reportid ) : reportid = reportid . upper ( ) self . reports state [ reportid ] = True", "predictions": ["enable a report on the reportid"], "references": ["disable the report of the given id"], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 1373, "code": "def disable report ( self , reportid ) : reportid = reportid . upper ( ) self . reports state [ reportid ] = False", "predictions": ["disable a report of the specified reportid"], "references": ["disable the report of the given id"], "bleu": 0.3655552228545123, "rouge_l": 0.5714285714285714}
{"id": 1374, "code": "def register ( linter ) : linter . register checker ( Encoding Checker ( linter ) ) linter . register checker ( By Id Managed Messages Checker ( linter ) )", "predictions": ["register method to auto register this checker ."], "references": ["required method to auto register this checker"], "bleu": 0.7259795291154771, "rouge_l": 0.8097345132743362}
{"id": 1375, "code": "def process module ( self , module ) : managed msgs = Messages Handler Mix In . get by id managed msgs ( ) for ( mod name , msg id , msg symbol , lineno , is disabled ) in managed msgs : if mod name == module . name : if is disabled : txt = \"Id '{ident}' is used to disable '{symbol}' message emission\" . format ( ident = msg id , symbol = msg symbol ) else : txt = \"Id '{ident}' is used to enable '{symbol}' message emission\" . format ( ident = msg id , symbol = msg symbol ) self . add message ( \"use-symbolic-message-instead\" , line = lineno , args = txt ) Messages Handler Mix In . clear by id managed msgs ( )", "predictions": ["process a module message"], "references": ["inspect the source file to find messages activated or deactivated by id ."], "bleu": 0.03184506239916981, "rouge_l": 0.0}
{"id": 1376, "code": "def process module ( self , module ) : if module . file encoding : encoding = module . file encoding else : encoding = \"ascii\" with module . stream ( ) as stream : for lineno , line in enumerate ( stream ) : self . check encoding ( lineno + 1 , line , encoding )", "predictions": ["help for a message in a message for a message for a given message for its message for it for a given message for a message for a message for a"], "references": ["inspect the source file to find encoding problem"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1377, "code": "def process tokens ( self , tokens ) : if not self . config . notes : return comments = ( token info for token info in tokens if token info . type == tokenize . COMMENT ) for comment in comments : comment text = comment . string [ 1 : ] . lstrip ( ) disable option match = OPTION RGX . search ( comment text ) if disable option match : try : , value = disable option match . group ( 1 ) . split ( \"=\" , 1 ) values = [ val . strip ( ) . upper ( ) for val in value . split ( \",\" ) ] if set ( values ) & set ( self . config . notes ) : continue except Value Error : self . add message ( \"bad-inline-option\" , args = disable option match . group ( 1 ) . strip ( ) , line = comment . string , ) continue match = self . fixme pattern . search ( \"#\" + comment text . lower ( ) ) if match : note = match . group ( 1 ) self . add message ( \"fixme\" , col offset = comment . string . lower ( ) . index ( note . lower ( ) ) , args = comment text , line = comment . start [ 0 ] , )", "predictions": ["parse messages from the msgid msgid definitions definitions definitions definitions definitions definitions definitions definitions definitions definitions definitions definitions definitions definitions definitions definitions definitions definitions definitions definitions definitions definitions definitions definitions definitions"], "references": ["inspect the source to find fixme problems"], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 1378, "code": "def is from future import ( stmt , name ) : try : module = stmt . do import module ( stmt . modname ) except astroid . Astroid Building Exception : return None for local node in module . locals . get ( name , [ ] ) : if isinstance ( local node , astroid . Import From ) and local node . modname == FUTURE : return True return None", "predictions": ["return builder that is inited by app app file app file file file file file app file file file app app file file file file app file app file app file"], "references": ["check if the name is a future import from another module ."], "bleu": 0.03901663112717908, "rouge_l": 0.05053852526926264}
{"id": 1379, "code": "def in for else branch ( parent , stmt ) : return isinstance ( parent , astroid . For ) and any ( else stmt . parent of ( stmt ) or else stmt == stmt for else stmt in parent . orelse )", "predictions": ["getattr whether sched is cpu . ."], "references": ["returns true if stmt in inside the else branch for a parent for stmt ."], "bleu": 0.059237077985967744, "rouge_l": 0.08531468531468532}
{"id": 1380, "code": "def overridden method ( klass , name ) : try : parent = next ( klass . local attr ancestors ( name ) ) except ( Stop Iteration , Key Error ) : return None try : meth node = parent [ name ] except Key Error : return None if isinstance ( meth node , astroid . Function Def ) : return meth node return None", "predictions": ["order of the local messages ."], "references": ["get overridden method if any"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1381, "code": "def assigned locally ( name node ) : assign stmts = name node . scope ( ) . nodes of class ( astroid . Assign Name ) return any ( a . name == name node . name for a in assign stmts )", "predictions": ["checker is the python3 locally"], "references": ["checks if name_node has corresponding assign statement in same scope"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 1382, "code": "def visit global ( self , node ) : frame = node . frame ( ) if isinstance ( frame , astroid . Module ) : self . add message ( \"global-at-module-level\" , node = node ) return module = frame . root ( ) default message = True locals = node . scope ( ) . locals for name in node . names : try : assign nodes = module . getattr ( name ) except astroid . Not Found Error : assign nodes = [ ] not defined locally by import = not any ( isinstance ( local , astroid . node classes . Import ) for local in locals . get ( name , ( ) ) ) if not assign nodes and not defined locally by import : self . add message ( \"global-variable-not-assigned\" , args = name , node = node ) default message = False continue for anode in assign nodes : if ( isinstance ( anode , astroid . Assign Name ) and anode . name in module . special attributes ) : self . add message ( \"redefined-builtin\" , args = name , node = node ) break if anode . frame ( ) is module : break else : if not defined locally by import : self . add message ( \"global-variable-undefined\" , args = name , node = node ) default message = False if default message : self . add message ( \"global-statement\" , node = node )", "predictions": ["visitor for checkers ast node ."], "references": ["check names imported exists in the global scope"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 1383, "code": "def visit import ( self , node ) : if not self . analyse fallback blocks and utils . is from fallback block ( node ) : return for name , in node . names : parts = name . split ( \".\" ) try : module = next ( infer name module ( node , parts [ 0 ] ) ) except astroid . Resolve Error : continue self . check module attrs ( node , module , parts [ 1 : ] )", "predictions": ["triggered when a checker is seen ."], "references": ["check modules attribute accesses"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1384, "code": "def visit importfrom ( self , node ) : if not self . analyse fallback blocks and utils . is from fallback block ( node ) : return name parts = node . modname . split ( \".\" ) try : module = node . do import module ( name parts [ 0 ] ) except astroid . Astroid Building Exception : return module = self . check module attrs ( node , module , name parts [ 1 : ] ) if not module : return for name , in node . names : if name == \"*\" : continue self . check module attrs ( node , module , name . split ( \".\" ) )", "predictions": ["visitor for functioncall ast node node ast node ."], "references": ["check modules attribute accesses"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1385, "code": "def check metaclasses ( self , node ) : consumed = [ ] for child node in node . get children ( ) : if isinstance ( child node , astroid . Class Def ) : consumed . extend ( self . check classdef metaclasses ( child node , node ) ) for scope locals , name in consumed : scope locals . pop ( name , None )", "predictions": ["expand all files in the result list"], "references": ["update consumption analysis for metaclasses ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1386, "code": "def get packages ( directory , prefix ) : result = [ ] for package in os . listdir ( directory ) : absfile = join ( directory , package ) if isdir ( absfile ) : if exists ( join ( absfile , \" init .py\" ) ) : if prefix : result . append ( \"%s.%s\" % ( prefix , package ) ) else : result . append ( package ) result += get packages ( absfile , result [ - 1 ] ) return result", "predictions": ["check all astroid in a given prefix"], "references": ["return a list of subpackages for the given directory"], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 1387, "code": "def run ( self ) : install lib . install lib . run ( self ) if include dirs : for directory in include dirs : dest = join ( self . install dir , directory ) if sys . version info >= ( 3 , 0 ) : exclude = { \"invalid encoded data*\" , \"unknown encoding*\" } else : exclude = set ( ) shutil . rmtree ( dest , ignore errors = True ) shutil . copytree ( directory , dest , ignore = shutil . ignore patterns ( * exclude ) )", "predictions": ["report the contents of the script stats stats stats stats stats stats stats stats stats stats stats stats stats stats stats stats stats stats stats stats stats stats stats stats stats"], "references": ["overridden from install_lib class"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1388, "code": "def report similarities ( sect , stats , old stats ) : lines = [ \"\" , \"now\" , \"previous\" , \"difference\" ] lines += table lines from stats ( stats , old stats , ( \"nb duplicated lines\" , \"percent duplicated lines\" ) ) sect . append ( Table ( children = lines , cols = 4 , rheaders = 1 , cheaders = 1 ) )", "predictions": ["cb the generate generate generate generate the generate generate generate generate the generate generate generate . ."], "references": ["make a layout with some stats about duplication"], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 1389, "code": "def Run ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] from getopt import getopt s opts = \"hdi\" l opts = ( \"help\" , \"duplicates=\" , \"ignore-comments\" , \"ignore-imports\" , \"ignore-docstrings\" , ) min lines = 4 ignore comments = False ignore docstrings = False ignore imports = False opts , args = getopt ( argv , s opts , l opts ) for opt , val in opts : if opt in ( \"-d\" , \"--duplicates\" ) : min lines = int ( val ) elif opt in ( \"-h\" , \"--help\" ) : usage ( ) elif opt in ( \"-i\" , \"--ignore-comments\" ) : ignore comments = True elif opt in ( \"--ignore-docstrings\" , ) : ignore docstrings = True elif opt in ( \"--ignore-imports\" , ) : ignore imports = True if not args : usage ( 1 ) sim = Similar ( min lines , ignore comments , ignore docstrings , ignore imports ) for filename in args : with open ( filename ) as stream : sim . append stream ( filename , stream ) sim . run ( ) sys . exit ( 0 )", "predictions": ["parse command line options . ."], "references": ["standalone command line access point"], "bleu": 0.2907153684841096, "rouge_l": 0.3696969696969697}
{"id": 1390, "code": "def append stream ( self , streamid , stream , encoding = None ) : if encoding is None : readlines = stream . readlines else : readlines = decoding stream ( stream , encoding ) . readlines try : self . linesets . append ( Line Set ( streamid , readlines ( ) , self . ignore comments , self . ignore docstrings , self . ignore imports , ) ) except Unicode Decode Error : pass", "predictions": ["cb a help help help"], "references": ["append a file to search for similarities"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1391, "code": "def compute sims ( self ) : no duplicates = defaultdict ( list ) for num , lineset1 , idx1 , lineset2 , idx2 in self . iter sims ( ) : duplicate = no duplicates [ num ] for couples in duplicate : if ( lineset1 , idx1 ) in couples or ( lineset2 , idx2 ) in couples : couples . add ( ( lineset1 , idx1 ) ) couples . add ( ( lineset2 , idx2 ) ) break else : duplicate . append ( { ( lineset1 , idx1 ) , ( lineset2 , idx2 ) } ) sims = [ ] for num , ensembles in no duplicates . items ( ) : for couples in ensembles : sims . append ( ( num , couples ) ) sims . sort ( ) sims . reverse ( ) return sims", "predictions": ["cb the duplicate and returns a ensembles"], "references": ["compute similarities in appended files"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1392, "code": "def display sims ( self , sims ) : nb lignes dupliquees = 0 for num , couples in sims : print ( ) print ( num , \"similar lines in\" , len ( couples ) , \"files\" ) couples = sorted ( couples ) for lineset , idx in couples : print ( \"==%s:%s\" % ( lineset . name , idx ) ) for line in lineset . real lines [ idx : idx + num ] : print ( \"  \" , line . rstrip ( ) ) nb lignes dupliquees += num * ( len ( couples ) - 1 ) nb total lignes = sum ( [ len ( lineset ) for lineset in self . linesets ] ) print ( \"TOTAL lines=%s duplicates=%s percent=%.2f\" % ( nb total lignes , nb lignes dupliquees , nb lignes dupliquees * 100.0 / nb total lignes , ) )", "predictions": ["cb the total nb . ."], "references": ["display computed similarities on stdout"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1393, "code": "def find common ( self , lineset1 , lineset2 ) : lines1 = lineset1 . enumerate stripped lines2 = lineset2 . enumerate stripped find = lineset2 . find index1 = 0 min lines = self . min lines while index1 < len ( lineset1 ) : skip = 1 num = 0 for index2 in find ( lineset1 [ index1 ] ) : non blank = 0 for num , ( ( , line1 ) , ( , line2 ) ) in enumerate ( zip ( lines1 ( index1 ) , lines2 ( index2 ) ) ) : if line1 != line2 : if non blank > min lines : yield num , lineset1 , index1 , lineset2 , index2 skip = max ( skip , num ) break if line1 : non blank += 1 else : num += 1 if non blank > min lines : yield num , lineset1 , index1 , lineset2 , index2 skip = max ( skip , num ) index1 += skip", "predictions": ["normalize the text of the given len ."], "references": ["find similarities in the two given linesets"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 1394, "code": "def mk index ( self ) : index = defaultdict ( list ) for line no , line in enumerate ( self . stripped lines ) : if line : index [ line ] . append ( line no ) return index", "predictions": ["append the module to the module ."], "references": ["create the index for this set"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 1395, "code": "def definition equivalent to call ( definition , call ) : if definition . kwargs : same kw variadics = definition . kwargs in call . starred kws else : same kw variadics = not call . starred kws if definition . varargs : same args variadics = definition . varargs in call . starred args else : same args variadics = not call . starred args same kwonlyargs = all ( kw in call . kws for kw in definition . kwonlyargs ) same args = definition . args == call . args no additional kwarg arguments = True if call . kws : for keyword in call . kws : is arg = keyword in call . args is kwonly = keyword in definition . kwonlyargs if not is arg and not is kwonly : no additional kwarg arguments = False break return all ( ( same args , same kwonlyargs , same args variadics , same kw variadics , no additional kwarg arguments , ) )", "predictions": ["wrapper around line decode to to ."], "references": ["check if a definition signature is equivalent to a call ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 1396, "code": "def register ( linter ) : linter . register checker ( Class Checker ( linter ) ) linter . register checker ( Special Methods Checker ( linter ) )", "predictions": ["comment method to comment = auto - comment lines lines lines lines lines lines lines lines lines lines lines lines lines"], "references": ["required method to auto register this checker"], "bleu": 0.0821610732492254, "rouge_l": 0.23552123552123552}
{"id": 1397, "code": "def set accessed ( self , node ) : frame = node frame class ( node ) if frame is None : return self . scopes [ frame ] [ node . attrname ] . append ( node )", "predictions": ["format the option to be used in the node"], "references": ["set the given node as accessed ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 1398, "code": "def visit classdef ( self , node ) : self . check bases classes ( node ) if node . type == \"class\" and has known bases ( node ) : try : node . local attr ( \" init \" ) except astroid . Not Found Error : self . add message ( \"no-init\" , args = node , node = node ) self . check slots ( node ) self . check proper bases ( node ) self . check consistent mro ( node )", "predictions": ["visitor for functioncall ast options ."], "references": ["init visit variable _accessed"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1399, "code": "def check consistent mro ( self , node ) : try : node . mro ( ) except Inconsistent Mro Error : self . add message ( \"inconsistent-mro\" , args = node . name , node = node ) except Duplicate Bases Error : self . add message ( \"duplicate-bases\" , args = node . name , node = node ) except Not Implemented Error : pass", "predictions": ["ini mro for format mro ."], "references": ["detect that a class has a consistent mro or duplicate bases ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 1400, "code": "def visit functiondef ( self , node ) : if not node . is method ( ) : return self . check useless super delegation ( node ) klass = node . parent . frame ( ) self . meth could be func = True self . check first arg for type ( node , klass . type == \"metaclass\" ) if node . name == \" init \" : self . check init ( node ) return for overridden in klass . local attr ancestors ( node . name ) : try : meth node = overridden [ node . name ] except Key Error : continue if not isinstance ( meth node , astroid . Function Def ) : continue self . check signature ( node , meth node , \"overridden\" , klass ) break if node . decorators : for decorator in node . decorators . nodes : if isinstance ( decorator , astroid . Attribute ) and decorator . attrname in ( \"getter\" , \"setter\" , \"deleter\" , ) : return if isinstance ( decorator , astroid . Name ) : if decorator . name == \"property\" : return inferred = safe infer ( decorator ) if not inferred : return if isinstance ( inferred , astroid . Function Def ) : try : inferred = next ( inferred . infer call result ( inferred ) ) except astroid . Inference Error : return try : if ( isinstance ( inferred , ( astroid . Instance , astroid . Class Def ) ) and inferred . getattr ( \" get \" ) and inferred . getattr ( \" set \" ) ) : return except astroid . Attribute Inference Error : pass try : overridden = klass . instance attr ( node . name ) [ 0 ] overridden frame = overridden . frame ( ) if ( isinstance ( overridden frame , astroid . Function Def ) and overridden frame . type == \"method\" ) : overridden frame = overridden frame . parent . frame ( ) if isinstance ( overridden frame , astroid . Class Def ) and klass . is subtype of ( overridden frame . qname ( ) ) : args = ( overridden . root ( ) . name , overridden . fromlineno ) self . add message ( \"method-hidden\" , args = args , node = node ) except astroid . Not Found Error : pass", "predictions": ["check if python child is defined as argument parent parent parent parent parent parent parent parent ."], "references": ["check method arguments overriding"], "bleu": 0.07223943354597204, "rouge_l": 0.10720562390158171}
{"id": 1401, "code": "def check accessed members ( self , node , accessed ) : excs = ( \"Attribute Error\" , \"Exception\" , \"Base Exception\" ) for attr , nodes in accessed . items ( ) : try : node . local attr ( attr ) continue except astroid . Not Found Error : pass try : next ( node . instance attr ancestors ( attr ) ) continue except Stop Iteration : pass try : defstmts = node . instance attr ( attr ) except astroid . Not Found Error : pass else : defstmts = [ stmt for stmt in defstmts if stmt not in nodes ] if not defstmts : continue scope = defstmts [ 0 ] . scope ( ) defstmts = [ stmt for i , stmt in enumerate ( defstmts ) if i == 0 or stmt . scope ( ) is not scope ] if len ( defstmts ) == 1 : defstmt = defstmts [ 0 ] frame = defstmt . frame ( ) lno = defstmt . fromlineno for node in nodes : if ( node . frame ( ) is frame and node . fromlineno < lno and not astroid . are exclusive ( node . statement ( ) , defstmt , excs ) ) : self . add message ( \"access-member-before-definition\" , node = node , args = ( attr , lno ) , )", "predictions": ["append accessed self . to accessed"], "references": ["check that accessed members are defined"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 1402, "code": "def check signature ( self , method1 , refmethod , class type , cls ) : if not ( isinstance ( method1 , astroid . Function Def ) and isinstance ( refmethod , astroid . Function Def ) ) : self . add message ( \"method-check-failed\" , args = ( method1 , refmethod ) , node = method1 ) return instance = cls . instantiate class ( ) method1 = function to method ( method1 , instance ) refmethod = function to method ( refmethod , instance ) if method1 . args . args is None or refmethod . args . args is None : return if is attr private ( method1 . name ) : return if method1 . decorators : for decorator in method1 . decorators . nodes : if ( isinstance ( decorator , astroid . Attribute ) and decorator . attrname == \"setter\" ) : return if different parameters ( refmethod , method1 , dummy parameter regex = self . dummy rgx ) : self . add message ( \"arguments-differ\" , args = ( class type , method1 . name ) , node = method1 ) elif len ( method1 . args . defaults ) < len ( refmethod . args . defaults ) : self . add message ( \"signature-differs\" , args = ( class type , method1 . name ) , node = method1 )", "predictions": ["parents function to the method ."], "references": ["check that the signature of the two given methods match"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 1403, "code": "def is raising ( body : typing . List ) -> bool : for node in body : if isinstance ( node , astroid . Raise ) : return True return False", "predictions": ["return list of nodes that are block msgs msgs msgs"], "references": ["return true if the given statement node raise an exception"], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 1404, "code": "def visit tryexcept ( self , node ) : self . check try except raise ( node ) exceptions classes = [ ] nb handlers = len ( node . handlers ) for index , handler in enumerate ( node . handlers ) : if handler . type is None : if not is raising ( handler . body ) : self . add message ( \"bare-except\" , node = handler ) if index < ( nb handlers - 1 ) : msg = \"empty except clause should always appear last\" self . add message ( \"bad-except-order\" , node = node , args = msg ) elif isinstance ( handler . type , astroid . Bool Op ) : self . add message ( \"binary-op-exception\" , node = handler , args = handler . type . op ) else : try : excs = list ( annotated unpack infer ( handler . type ) ) except astroid . Inference Error : continue for part , exc in excs : if exc is astroid . Uninferable : continue if isinstance ( exc , astroid . Instance ) and utils . inherit from std ex ( exc ) : exc = exc . proxied self . check catching non exception ( handler , exc , part ) if not isinstance ( exc , astroid . Class Def ) : continue exc ancestors = [ anc for anc in exc . ancestors ( ) if isinstance ( anc , astroid . Class Def ) ] for previous exc in exceptions classes : if previous exc in exc ancestors : msg = \"%s is an ancestor class of %s\" % ( previous exc . name , exc . name , ) self . add message ( \"bad-except-order\" , node = handler . type , args = msg ) if ( exc . name in self . config . overgeneral exceptions and exc . root ( ) . name == utils . EXCEPTIONS MODULE and not is raising ( handler . body ) ) : self . add message ( \"broad-except\" , args = exc . name , node = handler . type ) if exc in exceptions classes : self . add message ( \"duplicate-except\" , args = exc . name , node = handler . type ) exceptions classes += [ exc for , exc in excs ]", "predictions": ["visitor for report ast node exceptions"], "references": ["check for empty except"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 1405, "code": "def visit functiondef ( self , node ) : if not node . is method ( ) : return klass = node . parent . frame ( ) for stmt in node . nodes of class ( astroid . Call ) : if node frame class ( stmt ) != node frame class ( node ) : continue expr = stmt . func if not isinstance ( expr , astroid . Attribute ) : continue call = expr . expr if not ( isinstance ( call , astroid . Call ) and isinstance ( call . func , astroid . Name ) and call . func . name == \"super\" ) : continue if not klass . newstyle and has known bases ( klass ) : continue else : if not call . args : if sys . version info [ 0 ] == 3 : continue else : self . add message ( \"missing-super-argument\" , node = call ) continue arg0 = call . args [ 0 ] if ( isinstance ( arg0 , astroid . Call ) and isinstance ( arg0 . func , astroid . Name ) and arg0 . func . name == \"type\" ) : self . add message ( \"bad-super-call\" , node = call , args = ( \"type\" , ) ) continue if ( len ( call . args ) >= 2 and isinstance ( call . args [ 1 ] , astroid . Name ) and call . args [ 1 ] . name == \"self\" and isinstance ( arg0 , astroid . Attribute ) and arg0 . attrname == \" class \" ) : self . add message ( \"bad-super-call\" , node = call , args = ( \"self. class \" , ) ) continue try : supcls = call . args and next ( call . args [ 0 ] . infer ( ) , None ) except astroid . Inference Error : continue if klass is not supcls : name = None if supcls : name = supcls . name elif call . args and hasattr ( call . args [ 0 ] , \"name\" ) : name = call . args [ 0 ] . name if name : self . add message ( \"bad-super-call\" , node = call , args = ( name , ) )", "predictions": ["visitor for functioncall ast node ast node"], "references": ["check use of super"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1406, "code": "def display reports ( self , layout ) : self . section = 0 if hasattr ( layout , \"report id\" ) : layout . children [ 0 ] . children [ 0 ] . data += \" (%s)\" % layout . report id self . display ( layout )", "predictions": ["register the . ."], "references": ["display results encapsulated in the layout tree"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 1407, "code": "def is typing namedtuple ( node : astroid . Class Def ) -> bool : for base in node . ancestors ( ) : if base . qname ( ) == TYPING NAMEDTUPLE : return True return False", "predictions": ["check if the node is module managed by the given node managed managed managed managed managed managed managed managed managed managed node managed managed managed managed by the astroid managed ."], "references": ["check if a class node is a typing . namedtuple class"], "bleu": 0.06757878745244751, "rouge_l": 0.26046114432109313}
{"id": 1408, "code": "def visit classdef ( self , node ) : nb parents = len ( list ( node . ancestors ( ) ) ) if nb parents > self . config . max parents : self . add message ( \"too-many-ancestors\" , node = node , args = ( nb parents , self . config . max parents ) , ) if len ( node . instance attrs ) > self . config . max attributes : self . add message ( \"too-many-instance-attributes\" , node = node , args = ( len ( node . instance attrs ) , self . config . max attributes ) , )", "predictions": ["add classdef to classdef ."], "references": ["check size of inheritance hierarchy and number of instance attributes"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 1409, "code": "def leave classdef ( self , node ) : my methods = sum ( 1 for method in node . mymethods ( ) if not method . name . startswith ( \" \" ) ) if my methods > self . config . max public methods : self . add message ( \"too-many-public-methods\" , node = node , args = ( my methods , self . config . max public methods ) , ) if ( node . type != \"class\" or is enum class ( node ) or is dataclass ( node ) or is typing namedtuple ( node ) ) : return all methods = count methods in class ( node ) if all methods < self . config . min public methods : self . add message ( \"too-few-public-methods\" , node = node , args = ( all methods , self . config . min public methods ) , )", "predictions": ["leave all methods that are public methods ."], "references": ["check number of public methods"], "bleu": 0.21105340631872638, "rouge_l": 0.32105263157894737}
{"id": 1410, "code": "def visit tryexcept ( self , node ) : branches = len ( node . handlers ) if node . orelse : branches += 1 self . inc branch ( node , branches ) self . inc all stmts ( branches )", "predictions": ["visitor for functiondeclaration ast node ."], "references": ["increments the branches counter"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1411, "code": "def visit if ( self , node ) : self . check boolean expressions ( node ) branches = 1 if node . orelse and ( len ( node . orelse ) > 1 or not isinstance ( node . orelse [ 0 ] , If ) ) : branches += 1 self . inc branch ( node , branches ) self . inc all stmts ( branches )", "predictions": ["visitor for functioncall ast node ."], "references": ["increments the branches counter and checks boolean expressions"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 1412, "code": "def visit while ( self , node ) : branches = 1 if node . orelse : branches += 1 self . inc branch ( node , branches )", "predictions": ["visitor for functiondeclaration ast node ."], "references": ["increments the branches counter"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1413, "code": "def check docstring ( self , node ) : docstring = node . doc if not docstring : return start line = node . lineno + 1 for idx , line in enumerate ( docstring . splitlines ( ) ) : self . check spelling ( \"wrong-spelling-in-docstring\" , line , start line + idx )", "predictions": ["check if the docstring is valid ."], "references": ["check the node has any spelling errors"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 1414, "code": "def register ( linter ) : linter . register checker ( Refactoring Checker ( linter ) ) linter . register checker ( Not Checker ( linter ) ) linter . register checker ( Recommandation Checker ( linter ) ) linter . register checker ( Len Checker ( linter ) )", "predictions": ["required method to register this checker ."], "references": ["required method to auto register this checker ."], "bleu": 0.5993954153807813, "rouge_l": 0.9222462203023757}
{"id": 1415, "code": "def check stop iteration inside generator ( self , node ) : frame = node . frame ( ) if not isinstance ( frame , astroid . Function Def ) or not frame . is generator ( ) : return if utils . node ignores exception ( node , Stop Iteration ) : return if not node . exc : return exc = utils . safe infer ( node . exc ) if exc is None or exc is astroid . Uninferable : return if self . check exception inherit from stopiteration ( exc ) : self . add message ( \"stop-iteration-return\" , node = node )", "predictions": ["check if the iteration is inside the iteration inside the iteration ."], "references": ["check if an exception of type stopiteration is raised inside a generator"], "bleu": 0.15537125692760353, "rouge_l": 0.3333333333333333}
{"id": 1416, "code": "def check exception inherit from stopiteration ( exc ) : stopiteration qname = \"{}.Stop Iteration\" . format ( utils . EXCEPTIONS MODULE ) return any ( class . qname ( ) == stopiteration qname for class in exc . mro ( ) )", "predictions": ["check if the exception is inherit inherit ."], "references": ["return true if the exception node in argument inherit from stopiteration"], "bleu": 0.21690743377623947, "rouge_l": 0.4093959731543625}
{"id": 1417, "code": "def check nested blocks ( self , node ) : if not isinstance ( node . scope ( ) , astroid . Function Def ) : return nested blocks = self . nested blocks [ : ] if node . parent == node . scope ( ) : self . nested blocks = [ node ] else : for ancestor node in reversed ( self . nested blocks ) : if ancestor node == node . parent : break self . nested blocks . pop ( ) if isinstance ( node , astroid . If ) and self . is actual elif ( node ) : if self . nested blocks : self . nested blocks . pop ( ) self . nested blocks . append ( node ) if len ( nested blocks ) > len ( self . nested blocks ) : self . emit nested blocks message if needed ( nested blocks )", "predictions": ["check if nested blocks are nested ."], "references": ["update and check the number of nested blocks"], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 1418, "code": "def check consider merging isinstance ( self , node ) : if node . op != \"or\" : return first args = self . duplicated isinstance types ( node ) for duplicated name , class names in first args . items ( ) : names = sorted ( name for name in class names ) self . add message ( \"consider-merging-isinstance\" , node = node , args = ( duplicated name , \", \" . join ( names ) ) , )", "predictions": ["check if the merging merging the merging merging duplicated ."], "references": ["check isinstance calls which can be merged together ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 1419, "code": "def visit for ( self , node ) : if not isinstance ( node . iter , astroid . Call ) : return if not self . is builtin ( node . iter . func , \"range\" ) : return if len ( node . iter . args ) == 2 and not is constant zero ( node . iter . args [ 0 ] ) : return if len ( node . iter . args ) > 2 : return if not isinstance ( node . iter . args [ - 1 ] , astroid . Call ) : return second func = node . iter . args [ - 1 ] . func if not self . is builtin ( second func , \"len\" ) : return len args = node . iter . args [ - 1 ] . args if not len args or len ( len args ) != 1 : return iterating object = len args [ 0 ] if not isinstance ( iterating object , astroid . Name ) : return scope = node . scope ( ) if iterating object . name == \"self\" and scope . name == \" iter \" : return for child in node . body : for subscript in child . nodes of class ( astroid . Subscript ) : if not isinstance ( subscript . value , astroid . Name ) : continue if not isinstance ( subscript . slice , astroid . Index ) : continue if not isinstance ( subscript . slice . value , astroid . Name ) : continue if subscript . slice . value . name != node . target . name : continue if iterating object . name != subscript . value . name : continue if subscript . value . scope ( ) != node . scope ( ) : continue self . add message ( \"consider-using-enumerate\" , node = node ) return", "predictions": ["return an astroid . for node as string"], "references": ["emit a convention whenever range and len are used for indexing ."], "bleu": 0.10764345432696364, "rouge_l": 0.09651898734177215}
{"id": 1420, "code": "def check graphviz available ( output format ) : try : subprocess . call ( [ \"dot\" , \"-V\" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) except OS Error : print ( \"The output format '%s' is currently not available.\\n\" \"Please install 'Graphviz' to have other output formats \" \"than 'dot' or 'vcg'.\" % output format ) sys . exit ( 32 )", "predictions": ["check if graphviz graphviz format is available ."], "references": ["check if we need graphviz for different output format"], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 1421, "code": "def run ( self , args ) : if not args : print ( self . help ( ) ) return 1 sys . path . insert ( 0 , os . getcwd ( ) ) try : project = project from files ( args , project name = self . config . project , black list = self . config . black list , ) linker = Linker ( project , tag = True ) handler = Diadefs Handler ( self . config ) diadefs = handler . get diadefs ( project , linker ) finally : sys . path . pop ( 0 ) if self . config . output format == \"vcg\" : writer . VCG Writer ( self . config ) . write ( diadefs ) else : writer . Dot Writer ( self . config ) . write ( diadefs ) return 0", "predictions": ["run the project ."], "references": ["checking arguments and run project"], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 1422, "code": "def visit tryexcept ( self , node ) : for handler in node . handlers : if handler . type is None : continue if isinstance ( handler . type , astroid . Bool Op ) : continue try : excs = list ( annotated unpack infer ( handler . type ) ) except astroid . Inference Error : continue handled in clause = [ ] for part , exc in excs : if exc is astroid . Uninferable : continue if isinstance ( exc , astroid . Instance ) and utils . inherit from std ex ( exc ) : exc = exc . proxied if not isinstance ( exc , astroid . Class Def ) : continue exc ancestors = [ anc for anc in exc . ancestors ( ) if isinstance ( anc , astroid . Class Def ) ] for prev part , prev exc in handled in clause : prev exc ancestors = [ anc for anc in prev exc . ancestors ( ) if isinstance ( anc , astroid . Class Def ) ] if exc == prev exc : self . add message ( \"overlapping-except\" , node = handler . type , args = \"%s and %s are the same\" % ( prev part . as string ( ) , part . as string ( ) ) , ) elif prev exc in exc ancestors or exc in prev exc ancestors : ancestor = part if exc in prev exc ancestors else prev part descendant = part if prev exc in exc ancestors else prev part self . add message ( \"overlapping-except\" , node = handler . type , args = \"%s is an ancestor class of %s\" % ( ancestor . as string ( ) , descendant . as string ( ) ) , ) handled in clause += [ ( part , exc ) ]", "predictions": ["visit all of the astroid ."], "references": ["check for empty except"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1423, "code": "def write packages ( self , diagram ) : for i , obj in enumerate ( sorted ( diagram . modules ( ) , key = lambda x : x . title ) ) : self . printer . emit node ( i , label = self . get title ( obj ) , shape = \"box\" ) obj . fig id = i for rel in diagram . get relationships ( \"depends\" ) : self . printer . emit edge ( rel . from object . fig id , rel . to object . fig id , * * self . pkg edges )", "predictions": ["write packages in diagram ."], "references": ["write a package diagram"], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 1424, "code": "def write classes ( self , diagram ) : for i , obj in enumerate ( sorted ( diagram . objects , key = lambda x : x . title ) ) : self . printer . emit node ( i , * * self . get values ( obj ) ) obj . fig id = i for rel in diagram . get relationships ( \"specialization\" ) : self . printer . emit edge ( rel . from object . fig id , rel . to object . fig id , * * self . inh edges ) for rel in diagram . get relationships ( \"implements\" ) : self . printer . emit edge ( rel . from object . fig id , rel . to object . fig id , * * self . imp edges ) for rel in diagram . get relationships ( \"association\" ) : self . printer . emit edge ( rel . from object . fig id , rel . to object . fig id , label = rel . name , * * self . association edges )", "predictions": ["write hierarchy from diagram to the given diagram ."], "references": ["write a class diagram"], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 1425, "code": "def set printer ( self , file name , basename ) : layout = dict ( rankdir = \"BT\" ) self . printer = Dot Backend ( basename , additional param = layout ) self . file name = file name", "predictions": ["set the printer of a file ."], "references": ["initialize dotwriter and add options for layout ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1426, "code": "def set printer ( self , file name , basename ) : self . graph file = open ( file name , \"w+\" ) self . printer = VCG Printer ( self . graph file ) self . printer . open graph ( title = basename , layoutalgorithm = \"dfs\" , late edge labels = \"yes\" , port sharing = \"no\" , manhattan edges = \"yes\" , ) self . printer . emit node = self . printer . node self . printer . emit edge = self . printer . edge", "predictions": ["set a printer for the given file ."], "references": ["initialize vcgwriter for a uml graph"], "bleu": 0.17747405280050269, "rouge_l": 0.14663461538461536}
{"id": 1427, "code": "def may be emitted ( self ) : if self . minversion is not None and self . minversion > sys . version info : return False if self . maxversion is not None and self . maxversion <= sys . version info : return False return True", "predictions": ["return whether or not this version is a may be run ."], "references": ["return true if message may be emitted using the current interpreter"], "bleu": 0.14694106251955755, "rouge_l": 0.2629310344827586}
{"id": 1428, "code": "def format help ( self , checkerref = False ) : desc = self . descr if checkerref : desc += \" This message belongs to the %s checker.\" % self . checker . name title = self . msg if self . symbol : msgid = \"%s (%s)\" % ( self . symbol , self . msgid ) else : msgid = self . msgid if self . minversion or self . maxversion : restr = [ ] if self . minversion : restr . append ( \"< %s\" % \".\" . join ( [ str ( n ) for n in self . minversion ] ) ) if self . maxversion : restr . append ( \">= %s\" % \".\" . join ( [ str ( n ) for n in self . maxversion ] ) ) restr = \" or \" . join ( restr ) if checkerref : desc += \" It can't be emitted when using Python %s.\" % restr else : desc += \" This message can't be emitted when using Python %s.\" % restr desc = normalize text ( \" \" . join ( desc . split ( ) ) , indent = \"  \" ) if title != \"%s\" : title = title . splitlines ( ) [ 0 ] return \":%s: *%s*\\n%s\" % ( msgid , title . rstrip ( \" \" ) , desc ) return \":%s:\\n%s\" % ( msgid , desc )", "predictions": ["format the help string for printing ."], "references": ["return the help string for the given message id"], "bleu": 0.38849358632832764, "rouge_l": 0.48897795591182364}
{"id": 1429, "code": "def get cycles ( graph dict , path , visited , result , vertice ) : if vertice in path : cycle = [ vertice ] for node in path [ : : - 1 ] : if node == vertice : break cycle . insert ( 0 , node ) start from = min ( cycle ) index = cycle . index ( start from ) cycle = cycle [ index : ] + cycle [ 0 : index ] if cycle not in result : result . append ( cycle ) return path . append ( vertice ) try : for node in graph dict [ vertice ] : if node not in visited : get cycles ( graph dict , path , visited , result , node ) visited . add ( node ) except Key Error : pass path . pop ( )", "predictions": ["get cycles of cycles from path ."], "references": ["recursive function doing the real work for get_cycles"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 1430, "code": "def get source ( self ) : if self . source is None : self . emit ( \"}\\n\" ) self . source = \"\\n\" . join ( self . lines ) del self . lines return self . source", "predictions": ["return the source source from the file ."], "references": ["returns self . _source"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 1431, "code": "def rest format section ( stream , section , options , doc = None ) : if section : print ( \"%s\\n%s\" % ( section , \"'\" * len ( section ) ) , file = stream ) if doc : print ( normalize text ( doc , line len = 79 , indent = \"\" ) , file = stream ) print ( file = stream ) for optname , optdict , value in options : help opt = optdict . get ( \"help\" ) print ( \":%s:\" % optname , file = stream ) if help opt : help opt = normalize text ( help opt , line len = 79 , indent = \"  \" ) print ( help opt , file = stream ) if value : value = str ( format option value ( optdict , value ) ) print ( file = stream ) print ( \"  Default: ``%s``\" % value . replace ( \"`` \" , \"```` ``\" ) , file = stream )", "predictions": ["print the rest of a rest rest section ."], "references": ["format an options section using as rest formatted output"], "bleu": 0.15619699684601276, "rouge_l": 0.1111111111111111}
{"id": 1432, "code": "def disable ( self , msgid , scope = \"package\" , line = None , ignore unknown = False ) : self . set msg status ( msgid , enable = False , scope = scope , line = line , ignore unknown = ignore unknown ) self . register by id managed msg ( msgid , line )", "predictions": ["disable this message ."], "references": ["don t output message of the given id"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 1433, "code": "def enable ( self , msgid , scope = \"package\" , line = None , ignore unknown = False ) : self . set msg status ( msgid , enable = True , scope = scope , line = line , ignore unknown = ignore unknown ) self . register by id managed msg ( msgid , line , is disabled = False )", "predictions": ["enable a message ."], "references": ["reenable message of the given id"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 1434, "code": "def print full documentation ( self , stream = None ) : if not stream : stream = sys . stdout print ( \"Pylint global options and switches\" , file = stream ) print ( \"----------------------------------\" , file = stream ) print ( \"\" , file = stream ) print ( \"Pylint provides global options and switches.\" , file = stream ) print ( \"\" , file = stream ) by checker = { } for checker in self . get checkers ( ) : if checker . name == \"master\" : if checker . options : for section , options in checker . options by section ( ) : if section is None : title = \"General options\" else : title = \"%s options\" % section . capitalize ( ) print ( title , file = stream ) print ( \"~\" * len ( title ) , file = stream ) rest format section ( stream , None , options ) print ( \"\" , file = stream ) else : name = checker . name try : by checker [ name ] [ \"options\" ] += checker . options and values ( ) by checker [ name ] [ \"msgs\" ] . update ( checker . msgs ) by checker [ name ] [ \"reports\" ] += checker . reports except Key Error : by checker [ name ] = { \"options\" : list ( checker . options and values ( ) ) , \"msgs\" : dict ( checker . msgs ) , \"reports\" : list ( checker . reports ) , } print ( \"Pylint checkers' options and switches\" , file = stream ) print ( \"-------------------------------------\" , file = stream ) print ( \"\" , file = stream ) print ( \"Pylint checkers can provide three set of features:\" , file = stream ) print ( \"\" , file = stream ) print ( \"* options that control their execution,\" , file = stream ) print ( \"* messages that they can raise,\" , file = stream ) print ( \"* reports that they can generate.\" , file = stream ) print ( \"\" , file = stream ) print ( \"Below is a list of all checkers and their features.\" , file = stream ) print ( \"\" , file = stream ) for checker , info in sorted ( by checker . items ( ) ) : self . print checker doc ( checker , info , stream = stream )", "predictions": ["print the full documentation of the checker ."], "references": ["output a full documentation in rest format"], "bleu": 0.21105340631872638, "rouge_l": 0.26991150442477874}
{"id": 1435, "code": "def get indent length ( line ) : result = 0 for char in line : if char == \" \" : result += 1 elif char == \"\\t\" : result += TAB LENGTH else : break return result", "predictions": ["get the length of a line in the line ."], "references": ["return the length of the indentation on the given token s line ."], "bleu": 0.2148213130469897, "rouge_l": 0.5097493036211699}
{"id": 1436, "code": "def get indent hint line ( bar positions , bad position ) : if not bar positions : return ( \"\" , \"\" ) bar positions = [ get indent length ( indent ) for indent in bar positions ] bad position = get indent length ( bad position ) delta message = \"\" markers = [ ( pos , \"|\" ) for pos in bar positions ] if len ( markers ) == 1 : expected position = markers [ 0 ] [ 0 ] delta = abs ( expected position - bad position ) direction = \"add\" if expected position > bad position else \"remove\" delta message = CONTINUATION HINT MESSAGE % ( direction , delta , \"s\" if delta > 1 else \"\" , ) markers . append ( ( bad position , \"^\" ) ) markers . sort ( ) line = [ \" \" ] * ( markers [ - 1 ] [ 0 ] + 1 ) for position , marker in markers : line [ position ] = marker return ( \"\" . join ( line ) , delta message )", "predictions": ["return a string with the hint line of the bar ."], "references": ["return a line with |s for each of the positions in the given lists ."], "bleu": 0.15177895722063425, "rouge_l": 0.44907975460122695}
{"id": 1437, "code": "def handle line start ( self , pos ) : if self . line start > - 1 : return check token position = pos if self . tokens . token ( pos ) == ASYNC TOKEN : check token position += 1 self . is block opener = ( self . tokens . token ( check token position ) in CONTINUATION BLOCK OPENERS ) self . line start = pos", "predictions": ["handle a line start of a line ."], "references": ["record the first non - junk token at the start of a line ."], "bleu": 0.27746570848392715, "rouge_l": 0.43323863636363635}
{"id": 1438, "code": "def get valid indentations ( self , idx ) : stack top = - 1 if ( self . tokens . token ( idx ) in ( \"}\" , \"for\" ) and self . cont stack [ - 1 ] . token == \":\" ) : stack top = - 2 indent = self . cont stack [ stack top ] if self . tokens . token ( idx ) in CLOSING BRACKETS : valid indentations = indent . valid outdent strings else : valid indentations = indent . valid continuation strings return indent , valid indentations . copy ( )", "predictions": ["get valid indentations strings ."], "references": ["returns the valid offsets for the token at the given position ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 1439, "code": "def continuation inside bracket ( self , bracket , position ) : indentation = self . tokens . line indent ( position ) token indent = self . tokens . token indent ( position ) next token indent = self . tokens . token indent ( position + 1 ) if ( self . is block opener and next token indent == indentation + self . block indent string ) : return Continued Indent ( CONTINUED BLOCK , bracket , position , Indentations ( token indent ) , Before Block Indentations ( next token indent , next token indent + self . continuation string ) , ) return Continued Indent ( CONTINUED , bracket , position , Indentations ( token indent , next token indent ) , Indentations ( next token indent ) , )", "predictions": ["continuation inside an inside the bracket bracket ."], "references": ["extracts indentation information for a continued indent ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 1440, "code": "def new line ( self , tokens , line end , line start ) : if last token on line is ( tokens , line end , \";\" ) : self . add message ( \"unnecessary-semicolon\" , line = tokens . start line ( line end ) ) line num = tokens . start line ( line start ) line = tokens . line ( line start ) if tokens . type ( line start ) not in JUNK TOKENS : self . lines [ line num ] = line . split ( \"\\n\" ) [ 0 ] self . check lines ( line , line num )", "predictions": ["check a visit classdef classdef ."], "references": ["a new line has been encountered process it if necessary"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 1441, "code": "def has valid type annotation ( self , tokens , i ) : if not self . inside brackets ( \"(\" ) : return False bracket level = 0 for token in tokens [ i - 1 : : - 1 ] : if token [ 1 ] == \":\" : return True if token [ 1 ] == \"(\" : return False if token [ 1 ] == \"]\" : bracket level += 1 elif token [ 1 ] == \"[\" : bracket level -= 1 elif token [ 1 ] == \",\" : if not bracket level : return False elif token [ 1 ] in ( \".\" , \"...\" ) : continue elif token [ 0 ] not in ( tokenize . NAME , tokenize . STRING , tokenize . NL ) : return False return False", "predictions": ["check if the type is classdef"], "references": ["extended check of pep - 484 type hint presence"], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 1442, "code": "def check equals spacing ( self , tokens , i ) : if self . has valid type annotation ( tokens , i ) : self . check space ( tokens , i , ( MUST , MUST ) ) elif self . inside brackets ( \"(\" ) or self . inside brackets ( \"lambda\" ) : self . check space ( tokens , i , ( MUST NOT , MUST NOT ) ) else : self . check space ( tokens , i , ( MUST , MUST ) )", "predictions": ["visit all stmts that can be strings ."], "references": ["check the spacing of a single equals sign ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 1443, "code": "def check surrounded by space ( self , tokens , i ) : self . check space ( tokens , i , ( MUST , MUST ) )", "predictions": ["visit all if the tokens are if the if the self expressions is not if not if not if not if not if not if not if not if it is"], "references": ["check that a binary operator is surrounded by exactly one space ."], "bleu": 0.03901663112717908, "rouge_l": 0.05053852526926264}
{"id": 1444, "code": "def visit default ( self , node ) : if not node . is statement : return if not node . root ( ) . pure python : return prev sibl = node . previous sibling ( ) if prev sibl is not None : prev line = prev sibl . fromlineno else : if ( isinstance ( node . parent , nodes . Try Finally ) and node in node . parent . finalbody ) : prev line = node . parent . body [ 0 ] . tolineno + 1 else : prev line = node . parent . statement ( ) . fromlineno line = node . fromlineno assert line , node if prev line == line and self . visited lines . get ( line ) != 2 : self . check multi statement line ( node , line ) return if line in self . visited lines : return try : tolineno = node . blockstart tolineno except Attribute Error : tolineno = node . tolineno assert tolineno , node lines = [ ] for line in range ( line , tolineno + 1 ) : self . visited lines [ line ] = 1 try : lines . append ( self . lines [ line ] . rstrip ( ) ) except Key Error : lines . append ( \"\" )", "predictions": ["visitor for functioncall ast node 1 ."], "references": ["check the node line number and check it if not yet done"], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 1445, "code": "def check multi statement line ( self , node , line ) : if isinstance ( node , nodes . With ) : return if isinstance ( node , nodes . Try Except ) and isinstance ( node . parent , nodes . Try Finally ) : return if ( isinstance ( node . parent , nodes . If ) and not node . parent . orelse and self . config . single line if stmt ) : return if ( isinstance ( node . parent , nodes . Class Def ) and len ( node . parent . body ) == 1 and self . config . single line class stmt ) : return self . add message ( \"multiple-statements\" , node = node ) self . visited lines [ line ] = 2", "predictions": ["check if docstring is valid return true if it is not valid return false otherwise ."], "references": ["check for lines containing multiple statements ."], "bleu": 0.08513012360883544, "rouge_l": 0.1871165644171779}
{"id": 1446, "code": "def check lines ( self , lines , i ) : max chars = self . config . max line length ignore long line = self . config . ignore long lines def check line ( line , i ) : if not line . endswith ( \"\\n\" ) : self . add message ( \"missing-final-newline\" , line = i ) else : stripped line = line . rstrip ( \"\\t\\n\\r\\v \" ) if not stripped line and EMPTY LINE in self . config . no space check : pass elif line [ len ( stripped line ) : ] not in ( \"\\n\" , \"\\r\\n\" ) : self . add message ( \"trailing-whitespace\" , line = i , col offset = len ( stripped line ) ) line = stripped line mobj = OPTION RGX . search ( line ) if mobj and \"=\" in line : front of equal , , back of equal = mobj . group ( 1 ) . partition ( \"=\" ) if front of equal . strip ( ) == \"disable\" : if \"line-too-long\" in { msg id . strip ( ) for msg id in back of equal . split ( \",\" ) } : return None line = line . rsplit ( \"#\" , 1 ) [ 0 ] . rstrip ( ) if len ( line ) > max chars and not ignore long line . search ( line ) : self . add message ( \"line-too-long\" , line = i , args = ( len ( line ) , max chars ) ) return i + 1 unsplit ends = { \"\\v\" , \"\\x0b\" , \"\\f\" , \"\\x0c\" , \"\\x1c\" , \"\\x1d\" , \"\\x1e\" , \"\\x85\" , \"\\u2028\" , \"\\u2029\" , } unsplit = [ ] for line in lines . splitlines ( True ) : if line [ - 1 ] in unsplit ends : unsplit . append ( line ) continue if unsplit : unsplit . append ( line ) line = \"\" . join ( unsplit ) unsplit = [ ] i = check line ( line , i ) if i is None : break if unsplit : check line ( \"\" . join ( unsplit ) , i )", "predictions": ["register all lines lines"], "references": ["check lines have less than a maximum number of characters"], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 1447, "code": "def check indent level ( self , string , expected , line num ) : indent = self . config . indent string if indent == \"\\\\t\" : indent = \"\\t\" level = 0 unit size = len ( indent ) while string [ : unit size ] == indent : string = string [ unit size : ] level += 1 suppl = \"\" while string and string [ 0 ] in \" \\t\" : if string [ 0 ] != indent [ 0 ] : if string [ 0 ] == \"\\t\" : args = ( \"tab\" , \"space\" ) else : args = ( \"space\" , \"tab\" ) self . add message ( \"mixed-indentation\" , args = args , line = line num ) return level suppl += string [ 0 ] string = string [ 1 : ] if level != expected or suppl : i type = \"spaces\" if indent [ 0 ] == \"\\t\" : i type = \"tabs\" self . add message ( \"bad-indentation\" , line = line num , args = ( level * unit size + len ( suppl ) , i type , expected * unit size ) , ) return None", "predictions": ["check the stop iteration"], "references": ["return the indent level of the string"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 1448, "code": "def is conditional import ( node ) : parent = node . parent return isinstance ( parent , ( astroid . Try Except , astroid . Except Handler , astroid . If , astroid . If Exp ) )", "predictions": ["= = parent * stopiteration *"], "references": ["checks if an import node is in the context of a conditional ."], "bleu": 0.0578433294533084, "rouge_l": 0.0}
{"id": 1449, "code": "def visit name ( self , node ) : found node , = node . lookup ( node . name ) if not is builtin ( found node ) : return if node . name not in self . bad builtins : return if node ignores exception ( node ) or isinstance ( find try except wrapper node ( node ) , astroid . Except Handler ) : return message = node . name . lower ( ) + \"-builtin\" self . add message ( message , node = node )", "predictions": ["check if the nested nested node is a nested nested node not defined"], "references": ["detect when a bad built - in is referenced ."], "bleu": 0.10571070857151538, "rouge_l": 0.08905109489051095}
{"id": 1450, "code": "def visit subscript ( self , node ) : try : for inferred in node . value . infer ( ) : if not isinstance ( inferred , astroid . Instance ) : continue if utils . inherit from std ex ( inferred ) : self . add message ( \"indexing-exception\" , node = node ) except astroid . Inference Error : return", "predictions": ["visitor for functioncall ast self op op op op op op op op op op ."], "references": ["look for indexing exceptions ."], "bleu": 0.08513012360883544, "rouge_l": 0.21034482758620687}
{"id": 1451, "code": "def visit attribute ( self , node ) : if node . attrname == \"xreadlines\" : self . add message ( \"xreadlines-attribute\" , node = node ) return exception message = \"message\" try : for inferred in node . expr . infer ( ) : if isinstance ( inferred , astroid . Instance ) and utils . inherit from std ex ( inferred ) : if node . attrname == exception message : if exception message in inferred . instance attrs : continue self . add message ( \"exception-message-attribute\" , node = node ) if isinstance ( inferred , astroid . Module ) : self . warn if deprecated ( node , inferred . name , { node . attrname } , report on modules = False ) except astroid . Inference Error : return", "predictions": ["visitor for node ast node not serializable by the code code not built not possible ."], "references": ["look for removed attributes"], "bleu": 0.07692375026049747, "rouge_l": 0.11213235294117647}
{"id": 1452, "code": "def visit excepthandler ( self , node ) : def is used in except block ( node ) : scope = node . scope ( ) current = node while ( current and current != scope and not isinstance ( current , astroid . Except Handler ) ) : current = current . parent return isinstance ( current , astroid . Except Handler ) and current . type != node if isinstance ( node . name , ( astroid . Tuple , astroid . List ) ) : self . add message ( \"unpacking-in-except\" , node = node ) return if not node . name : return scope = node . parent . scope ( ) scope names = scope . nodes of class ( astroid . Name , skip klass = astroid . Function Def ) scope names = list ( scope names ) potential leaked names = [ scope name for scope name in scope names if scope name . name == node . name . name and scope name . lineno > node . lineno and not is used in except block ( scope name ) ] reassignments for same name = { assign name . lineno for assign name in scope . nodes of class ( astroid . Assign Name , skip klass = astroid . Function Def ) if assign name . name == node . name . name } for leaked name in potential leaked names : if any ( node . lineno < elem < leaked name . lineno for elem in reassignments for same name ) : continue self . add message ( \"exception-escape\" , node = leaked name )", "predictions": ["visitor for functiondeclaration ast format format . ast . ."], "references": ["visit an except handler block and check for exception unpacking ."], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 1453, "code": "def find pylintrc ( ) : if os . path . exists ( \"pylintrc\" ) : return os . path . abspath ( \"pylintrc\" ) if os . path . exists ( \".pylintrc\" ) : return os . path . abspath ( \".pylintrc\" ) if os . path . isfile ( \" init .py\" ) : curdir = os . path . abspath ( os . getcwd ( ) ) while os . path . isfile ( os . path . join ( curdir , \" init .py\" ) ) : curdir = os . path . abspath ( os . path . join ( curdir , \"..\" ) ) if os . path . isfile ( os . path . join ( curdir , \"pylintrc\" ) ) : return os . path . join ( curdir , \"pylintrc\" ) if os . path . isfile ( os . path . join ( curdir , \".pylintrc\" ) ) : return os . path . join ( curdir , \".pylintrc\" ) if \"PYLINTRC\" in os . environ and os . path . exists ( os . environ [ \"PYLINTRC\" ] ) : pylintrc = os . environ [ \"PYLINTRC\" ] else : user home = os . path . expanduser ( \"~\" ) if user home in ( \"~\" , \"/root\" ) : pylintrc = \".pylintrc\" else : pylintrc = os . path . join ( user home , \".pylintrc\" ) if not os . path . isfile ( pylintrc ) : pylintrc = os . path . join ( user home , \".config\" , \"pylintrc\" ) if not os . path . isfile ( pylintrc ) : if os . path . isfile ( \"/etc/pylintrc\" ) : pylintrc = \"/etc/pylintrc\" else : pylintrc = None return pylintrc", "predictions": ["run the pylintrc if it exists if not ."], "references": ["search the pylint rc file and return its path if it find it else none"], "bleu": 0.10247907767191411, "rouge_l": 0.23921568627450981}
{"id": 1454, "code": "def register options provider ( self , provider , own group = True ) : assert provider . priority <= 0 , \"provider's priority can't be >= 0\" for i in range ( len ( self . options providers ) ) : if provider . priority > self . options providers [ i ] . priority : self . options providers . insert ( i , provider ) break else : self . options providers . append ( provider ) non group spec options = [ option for option in provider . options if \"group\" not in option [ 1 ] ] groups = getattr ( provider , \"option groups\" , ( ) ) if own group and non group spec options : self . add option group ( provider . name . upper ( ) , provider . doc , non group spec options , provider , ) else : for opt , optdict in non group spec options : self . add optik option ( provider , self . cmdline parser , opt , optdict ) for gname , gdoc in groups : gname = gname . upper ( ) goptions = [ option for option in provider . options if option [ 1 ] . get ( \"group\" , \"\" ) . upper ( ) == gname ] self . add option group ( gname , gdoc , goptions , provider )", "predictions": ["visit tryexcept tryexcept tryexcept tryexcept tryexcept with the tryexcept unpack unpack if needed if not present if not ."], "references": ["register an options provider"], "bleu": 0.05415315253510895, "rouge_l": 0.0}
{"id": 1455, "code": "def cb set provider option ( self , option , opt , value , parser ) : if opt . startswith ( \"--\" ) : opt = opt [ 2 : ] else : opt = self . short options [ opt [ 1 : ] ] if value is None : value = 1 self . global set option ( opt , value )", "predictions": ["set the provider self sorted by the provider attribute"], "references": ["optik callback for option setting"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1456, "code": "def global set option ( self , opt , value ) : self . all options [ opt ] . set option ( opt , value )", "predictions": ["classes for setting options"], "references": ["set option on the correct option provider"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 1457, "code": "def add help section ( self , title , description , level = 0 ) : group = optparse . Option Group ( self . cmdline parser , title = title . capitalize ( ) , description = description ) group . level = level self . maxlevel = max ( self . maxlevel , level ) self . cmdline parser . add option group ( group )", "predictions": ["set the printer section"], "references": ["add a dummy option section for help purpose"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 1458, "code": "def help ( self , level = 0 ) : self . cmdline parser . formatter . output level = level with patch optparse ( ) : return self . cmdline parser . format help ( )", "predictions": ["display the set of the set of the current = set of the set of set"], "references": ["return the usage string for available options"], "bleu": 0.07692375026049747, "rouge_l": 0.09355828220858894}
{"id": 1459, "code": "def load defaults ( self ) : for opt , optdict in self . options : action = optdict . get ( \"action\" ) if action != \"callback\" : if optdict is None : optdict = self . get option def ( opt ) default = optdict . get ( \"default\" ) self . set option ( opt , default , action , optdict )", "predictions": ["may be called when the command - line is loaded"], "references": ["initialize the provider using default values"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 1460, "code": "def option attrname ( self , opt , optdict = None ) : if optdict is None : optdict = self . get option def ( opt ) return optdict . get ( \"dest\" , opt . replace ( \"-\" , \" \" ) )", "predictions": ["return the command line options for this category"], "references": ["get the config attribute corresponding to opt"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 1461, "code": "def get option def ( self , opt ) : assert self . options for option in self . options : if option [ 0 ] == opt : return option [ 1 ] raise optparse . Option Error ( \"no such option %s in section %r\" % ( opt , self . name ) , opt )", "predictions": ["get the cycles for this start and cycles . . ."], "references": ["return the dictionary defining an option given its name"], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 1462, "code": "def visit module ( self , node ) : self . logging names = set ( ) logging mods = self . config . logging modules self . format style = self . config . logging format style self . logging modules = set ( logging mods ) self . from imports = { } for logging mod in logging mods : parts = logging mod . rsplit ( \".\" , 1 ) if len ( parts ) > 1 : self . from imports [ parts [ 0 ] ] = parts [ 1 ]", "predictions": ["get logging logging and logging logging is required ."], "references": ["clears any state left in this checker from last module checked ."], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 1463, "code": "def visit importfrom ( self , node ) : try : logging name = self . from imports [ node . modname ] for module , as name in node . names : if module == logging name : self . logging names . add ( as name or module ) except Key Error : pass", "predictions": ["visitor for or ast node ."], "references": ["checks to see if a module uses a non - python logging module ."], "bleu": 0.05822753005110548, "rouge_l": 0.09327217125382263}
{"id": 1464, "code": "def visit import ( self , node ) : for module , as name in node . names : if module in self . logging modules : self . logging names . add ( as name or module )", "predictions": ["visitor for as ast scope . ."], "references": ["checks to see if this module uses python s built - in logging ."], "bleu": 0.06833381956448398, "rouge_l": 0.08983799705449189}
{"id": 1465, "code": "def visit call ( self , node ) : def is logging name ( ) : return ( isinstance ( node . func , astroid . Attribute ) and isinstance ( node . func . expr , astroid . Name ) and node . func . expr . name in self . logging names ) def is logger class ( ) : try : for inferred in node . func . infer ( ) : if isinstance ( inferred , astroid . Bound Method ) : parent = inferred . proxied . parent if isinstance ( parent , astroid . Class Def ) and ( parent . qname ( ) == \"logging.Logger\" or any ( ancestor . qname ( ) == \"logging.Logger\" for ancestor in parent . ancestors ( ) ) ) : return True , inferred . proxied . name except astroid . exceptions . Inference Error : pass return False , None if is logging name ( ) : name = node . func . attrname else : result , name = is logger class ( ) if not result : return self . check log method ( node , name )", "predictions": ["enable an astroid . function . ."], "references": ["checks calls to logging methods ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 1466, "code": "def in loop ( node ) : parent = node . parent while parent is not None : if isinstance ( parent , ( astroid . For , astroid . List Comp , astroid . Set Comp , astroid . Dict Comp , astroid . Generator Exp , ) , ) : return True parent = parent . parent return False", "predictions": ["returns true if the node is print false otherwise ."], "references": ["return true if the node is inside a kind of for loop"], "bleu": 0.3780379795765553, "rouge_l": 0.44721407624633425}
{"id": 1467, "code": "def register ( linter ) : linter . register checker ( Basic Error Checker ( linter ) ) linter . register checker ( Basic Checker ( linter ) ) linter . register checker ( Name Checker ( linter ) ) linter . register checker ( Doc String Checker ( linter ) ) linter . register checker ( Pass Checker ( linter ) ) linter . register checker ( Comparison Checker ( linter ) )", "predictions": ["get the checker checker checker checker"], "references": ["required method to auto register this checker"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 1468, "code": "def visit starred ( self , node ) : if isinstance ( node . parent , astroid . Call ) : return if PY35 and isinstance ( node . parent , ( astroid . List , astroid . Tuple , astroid . Set , astroid . Dict ) ) : return stmt = node . statement ( ) if not isinstance ( stmt , astroid . Assign ) : return if stmt . value is node or stmt . value . parent of ( node ) : self . add message ( \"star-needs-assignment-target\" , node = node )", "predictions": ["get an astroid . bar bar ."], "references": ["check that a starred expression is used in an assignment target ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 1469, "code": "def check nonlocal and global ( self , node ) : def same scope ( current ) : return current . scope ( ) is node from iter = itertools . chain . from iterable nonlocals = set ( from iter ( child . names for child in node . nodes of class ( astroid . Nonlocal ) if same scope ( child ) ) ) if not nonlocals : return global vars = set ( from iter ( child . names for child in node . nodes of class ( astroid . Global ) if same scope ( child ) ) ) for name in nonlocals . intersection ( global vars ) : self . add message ( \"nonlocal-and-global\" , args = ( name , ) , node = node )", "predictions": ["handle line start and global ."], "references": ["check that a name is both nonlocal and global ."], "bleu": 0.21108303712651422, "rouge_l": 0.3588235294117647}
{"id": 1470, "code": "def visit unaryop ( self , node ) : if ( ( node . op in \"+-\" ) and isinstance ( node . operand , astroid . Unary Op ) and ( node . operand . op == node . op ) ) : self . add message ( \"nonexistent-operator\" , node = node , args = node . op * 2 )", "predictions": ["visitor for valid valid valid valid valid valid valid valid valid"], "references": ["check use of the non - existent ++ and -- operator operator"], "bleu": 0.08746102712394917, "rouge_l": 0.0}
{"id": 1471, "code": "def check else on loop ( self , node ) : if node . orelse and not loop exits early ( node ) : self . add message ( \"useless-else-on-loop\" , node = node , line = node . orelse [ 0 ] . lineno - 1 , )", "predictions": ["continuation continuation for a inside a loop loop ."], "references": ["check that any loop with an else clause has a break statement ."], "bleu": 0.10761866342063775, "rouge_l": 0.17604617604617603}
{"id": 1472, "code": "def check in loop ( self , node , node name ) : node = node . parent while node : if isinstance ( node , ( astroid . For , astroid . While ) ) : if node not in node . orelse : return if isinstance ( node , ( astroid . Class Def , astroid . Function Def ) ) : break if ( isinstance ( node , astroid . Try Finally ) and node in node . finalbody and isinstance ( node , astroid . Continue ) ) : self . add message ( \"continue-in-finally\" , node = node ) node = node . parent self . add message ( \"not-in-loop\" , node = node , args = node name )", "predictions": ["check if a node is in the astroid loop ."], "references": ["check that a node is inside a for or while loop"], "bleu": 0.23494428299484157, "rouge_l": 0.47213622291021673}
{"id": 1473, "code": "def open ( self ) : self . tryfinallys = [ ] self . stats = self . linter . add stats ( module = 0 , function = 0 , method = 0 , class = 0 )", "predictions": ["open the linter ."], "references": ["initialize visit variables and statistics"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 1474, "code": "def visit expr ( self , node ) : expr = node . value if isinstance ( expr , astroid . Const ) and isinstance ( expr . value , str ) : scope = expr . scope ( ) if isinstance ( scope , ( astroid . Class Def , astroid . Module , astroid . Function Def ) ) : if isinstance ( scope , astroid . Function Def ) and scope . name != \" init \" : pass else : sibling = expr . previous sibling ( ) if ( sibling is not None and sibling . scope ( ) is scope and isinstance ( sibling , ( astroid . Assign , astroid . Ann Assign ) ) ) : return self . add message ( \"pointless-string-statement\" , node = node ) return if isinstance ( expr , ( astroid . Yield , astroid . Await , astroid . Ellipsis , astroid . Call ) ) or ( isinstance ( node . parent , astroid . Try Except ) and node . parent . body == [ node ] ) : return if any ( expr . nodes of class ( astroid . Call ) ) : self . add message ( \"expression-not-assigned\" , node = node , args = expr . as string ( ) ) else : self . add message ( \"pointless-statement\" , node = node )", "predictions": ["return an astroid . expr node as string"], "references": ["check for various kind of statements without effect"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1475, "code": "def visit lambda ( self , node ) : if node . args . defaults : return call = node . body if not isinstance ( call , astroid . Call ) : return if isinstance ( node . body . func , astroid . Attribute ) and isinstance ( node . body . func . expr , astroid . Call ) : return call site = Call Site . from call ( call ) ordinary args = list ( node . args . args ) new call args = list ( self . filter vararg ( node , call . args ) ) if node . args . kwarg : if self . has variadic argument ( call . kwargs , node . args . kwarg ) : return if node . args . vararg : if self . has variadic argument ( call . starargs , node . args . vararg ) : return elif call . starargs : return if call . keywords : lambda kwargs = { keyword . name for keyword in node . args . defaults } if len ( lambda kwargs ) != len ( call site . keyword arguments ) : return if set ( call site . keyword arguments ) . difference ( lambda kwargs ) : return if len ( ordinary args ) != len ( new call args ) : return for arg , passed arg in zip ( ordinary args , new call args ) : if not isinstance ( passed arg , astroid . Name ) : return if arg . name != passed arg . name : return self . add message ( \"unnecessary-lambda\" , line = node . fromlineno , node = node )", "predictions": ["visitor for functioncall ast node ."], "references": ["check whether or not the lambda is suspicious"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 1476, "code": "def visit assert ( self , node ) : if ( node . fail is None and isinstance ( node . test , astroid . Tuple ) and len ( node . test . elts ) == 2 ) : self . add message ( \"assert-on-tuple\" , node = node )", "predictions": ["visitor for node ast node ."], "references": ["check the use of an assert statement on a tuple ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 1477, "code": "def visit dict ( self , node ) : keys = set ( ) for k , in node . items : if isinstance ( k , astroid . Const ) : key = k . value if key in keys : self . add message ( \"duplicate-key\" , node = node , args = key ) keys . add ( key )", "predictions": ["recursively add a dict of keys to the model ."], "references": ["check duplicate key in dictionary"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1478, "code": "def check reversed ( self , node ) : try : argument = utils . safe infer ( utils . get argument from call ( node , position = 0 ) ) except utils . No Such Argument Error : pass else : if argument is astroid . Uninferable : return if argument is None : if isinstance ( node . args [ 0 ] , astroid . Call ) : try : func = next ( node . args [ 0 ] . func . infer ( ) ) except astroid . Inference Error : return if getattr ( func , \"name\" , None ) == \"iter\" and utils . is builtin object ( func ) : self . add message ( \"bad-reversed-sequence\" , node = node ) return if isinstance ( argument , ( astroid . List , astroid . Tuple ) ) : return if isinstance ( argument , astroid . Instance ) : if argument . proxied . name == \"dict\" and utils . is builtin object ( argument . proxied ) : self . add message ( \"bad-reversed-sequence\" , node = node ) return if any ( ancestor . name == \"dict\" and utils . is builtin object ( ancestor ) for ancestor in argument . proxied . ancestors ( ) ) : try : argument . locals [ REVERSED PROTOCOL METHOD ] except Key Error : self . add message ( \"bad-reversed-sequence\" , node = node ) return if hasattr ( argument , \"getattr\" ) : for methods in REVERSED METHODS : for meth in methods : try : argument . getattr ( meth ) except astroid . Not Found Error : break else : break else : self . add message ( \"bad-reversed-sequence\" , node = node ) else : self . add message ( \"bad-reversed-sequence\" , node = node )", "predictions": ["check if the node is valid ."], "references": ["check that the argument to reversed is a sequence"], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 1479, "code": "def visit assignname ( self , node ) : self . check assign to new keyword violation ( node . name , node ) frame = node . frame ( ) assign type = node . assign type ( ) if isinstance ( assign type , astroid . Comprehension ) : self . check name ( \"inlinevar\" , node . name , node ) elif isinstance ( frame , astroid . Module ) : if isinstance ( assign type , astroid . Assign ) and not in loop ( assign type ) : if isinstance ( utils . safe infer ( assign type . value ) , astroid . Class Def ) : self . check name ( \"class\" , node . name , node ) else : if not redefines import ( node ) : self . check name ( \"const\" , node . name , node ) elif isinstance ( assign type , astroid . Except Handler ) : self . check name ( \"variable\" , node . name , node ) elif isinstance ( frame , astroid . Function Def ) : if node . name in frame and node . name not in frame . argnames ( ) : if not redefines import ( node ) : self . check name ( \"variable\" , node . name , node ) elif isinstance ( frame , astroid . Class Def ) : if not list ( frame . local attr ancestors ( node . name ) ) : self . check name ( \"class attribute\" , node . name , node )", "predictions": ["visitor for functioncall ast node ."], "references": ["check module level assigned names"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1480, "code": "def check name ( self , node type , name , node , confidence = interfaces . HIGH ) : def should exempt from invalid name ( node ) : if node type == \"variable\" : inferred = utils . safe infer ( node ) if isinstance ( inferred , astroid . Class Def ) : return True return False if utils . is inside except ( node ) : clobbering , = utils . clobber in except ( node ) if clobbering : return if name in self . config . good names : return if name in self . config . bad names : self . stats [ \"badname \" + node type ] += 1 self . add message ( \"blacklisted-name\" , node = node , args = name ) return regexp = self . name regexps [ node type ] match = regexp . match ( name ) if is multi naming match ( match , node type , confidence ) : name group = self . find name group ( node type ) bad name group = self . bad names . setdefault ( name group , { } ) warnings = bad name group . setdefault ( match . lastgroup , [ ] ) warnings . append ( ( node , node type , name , confidence ) ) if match is None and not should exempt from invalid name ( node ) : self . raise name warning ( node , node type , name , confidence )", "predictions": ["check if the confidence name is valid ."], "references": ["check for a name using the type s regexp"], "bleu": 0.16829946711936866, "rouge_l": 0.232824427480916}
{"id": 1481, "code": "def check docstring ( self , node type , node , report missing = True , confidence = interfaces . HIGH ) : docstring = node . doc if docstring is None : if not report missing : return lines = utils . get node last lineno ( node ) - node . lineno if node type == \"module\" and not lines : return max lines = self . config . docstring min length if node type != \"module\" and max lines > - 1 and lines < max lines : return self . stats [ \"undocumented \" + node type ] += 1 if ( node . body and isinstance ( node . body [ 0 ] , astroid . Expr ) and isinstance ( node . body [ 0 ] . value , astroid . Call ) ) : func = utils . safe infer ( node . body [ 0 ] . value . func ) if isinstance ( func , astroid . Bound Method ) and isinstance ( func . bound , astroid . Instance ) : if PY3K and func . bound . name == \"str\" : return if func . bound . name in ( \"str\" , \"unicode\" , \"bytes\" ) : return self . add message ( \"missing-docstring\" , node = node , args = ( node type , ) , confidence = confidence ) elif not docstring . strip ( ) : self . stats [ \"undocumented \" + node type ] += 1 self . add message ( \"empty-docstring\" , node = node , args = ( node type , ) , confidence = confidence )", "predictions": ["check if the docstring is valid ."], "references": ["check the node has a non empty docstring"], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 1482, "code": "def check literal comparison ( self , literal , node ) : nodes = ( astroid . List , astroid . Tuple , astroid . Dict , astroid . Set ) is other literal = isinstance ( literal , nodes ) is const = False if isinstance ( literal , astroid . Const ) : if isinstance ( literal . value , bool ) or literal . value is None : return is const = isinstance ( literal . value , ( bytes , str , int , float ) ) if is const or is other literal : self . add message ( \"literal-comparison\" , node = node )", "predictions": ["check that the literal is in the astroid ."], "references": ["check if we compare to a literal which is usually what we do not want to do ."], "bleu": 0.06528905536667996, "rouge_l": 0.27949599083619703}
{"id": 1483, "code": "def subgraph ( self , node , name , extra blocks = ( ) ) : if self . graph is None : self . graph = Path Graph ( node ) self . subgraph parse ( node , node , extra blocks ) self . graphs [ \"%s%s\" % ( self . classname , name ) ] = self . graph self . reset ( ) else : self . append node ( node ) self . subgraph parse ( node , node , extra blocks )", "predictions": ["set a subgraph ."], "references": ["create the subgraphs representing any if and for statements"], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 1484, "code": "def subgraph parse ( self , node , pathnode , extra blocks ) : loose ends = [ ] self . tail = node self . dispatch list ( node . body ) loose ends . append ( self . tail ) for extra in extra blocks : self . tail = node self . dispatch list ( extra . body ) loose ends . append ( self . tail ) if node . orelse : self . tail = node self . dispatch list ( node . orelse ) loose ends . append ( self . tail ) else : loose ends . append ( node ) if node : bottom = \"%s\" % self . bottom counter self . bottom counter += 1 for le in loose ends : self . graph . connect ( le , bottom ) self . tail = bottom", "predictions": ["parse a subgraph node ."], "references": ["parse the body and any else block of if and for statements"], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 1485, "code": "def add checker ( self , checker ) : vcids = set ( ) lcids = set ( ) visits = self . visit events leaves = self . leave events for member in dir ( checker ) : cid = member [ 6 : ] if cid == \"default\" : continue if member . startswith ( \"visit \" ) : v meth = getattr ( checker , member ) if self . is method enabled ( v meth ) : visits [ cid ] . append ( v meth ) vcids . add ( cid ) elif member . startswith ( \"leave \" ) : l meth = getattr ( checker , member ) if self . is method enabled ( l meth ) : leaves [ cid ] . append ( l meth ) lcids . add ( cid ) visit default = getattr ( checker , \"visit default\" , None ) if visit default : for cls in nodes . ALL NODE CLASSES : cid = cls . name . lower ( ) if cid not in vcids : visits [ cid ] . append ( visit default )", "predictions": ["add a checker to this checker ."], "references": ["walk to the checker s dir and collect visit and leave methods"], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 1486, "code": "def add relationship ( self , from object , to object , relation type , name = None ) : rel = Relationship ( from object , to object , relation type , name ) self . relationships . setdefault ( relation type , [ ] ) . append ( rel )", "predictions": ["add an relationship to the relationships ."], "references": ["create a relation ship"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1487, "code": "def get relationship ( self , from object , relation type ) : for rel in self . relationships . get ( relation type , ( ) ) : if rel . from object is from object : return rel raise Key Error ( relation type )", "predictions": ["return the relationship object for the given object ."], "references": ["return a relation ship or none"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 1488, "code": "def get attrs ( self , node ) : attrs = [ ] properties = [ ( n , m ) for n , m in node . items ( ) if isinstance ( m , astroid . Function Def ) and decorated with property ( m ) ] for node name , associated nodes in ( list ( node . instance attrs type . items ( ) ) + list ( node . locals type . items ( ) ) + properties ) : if not self . show attr ( node name ) : continue names = self . class names ( associated nodes ) if names : node name = \"%s : %s\" % ( node name , \", \" . join ( names ) ) attrs . append ( node name ) return sorted ( attrs )", "predictions": ["return a list of all attrs of the node ."], "references": ["return visible attributes possibly with class name"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 1489, "code": "def add object ( self , title , node ) : assert node not in self . nodes ent = Diagram Entity ( title , node ) self . nodes [ node ] = ent self . objects . append ( ent )", "predictions": ["add an object to the graph"], "references": ["create a diagram object"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 1490, "code": "def class names ( self , nodes ) : names = [ ] for node in nodes : if isinstance ( node , astroid . Instance ) : node = node . proxied if ( isinstance ( node , astroid . Class Def ) and hasattr ( node , \"name\" ) and not self . has node ( node ) ) : if node . name not in names : node name = node . name names . append ( node name ) return names", "predictions": ["return the names of all nodes in the astroid ."], "references": ["return class names if needed in diagram"], "bleu": 0.14991106946711685, "rouge_l": 0.36454183266932266}
{"id": 1491, "code": "def classes ( self ) : return [ o for o in self . objects if isinstance ( o . node , astroid . Class Def ) ]", "predictions": ["returns all classes of this node ."], "references": ["return all class nodes in the diagram"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 1492, "code": "def classe ( self , name ) : for klass in self . classes ( ) : if klass . node . name == name : return klass raise Key Error ( name )", "predictions": ["return the first child of the given name ."], "references": ["return a class by its name raise keyerror if not found"], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 1493, "code": "def extract relationships ( self ) : for obj in self . classes ( ) : node = obj . node obj . attrs = self . get attrs ( node ) obj . methods = self . get methods ( node ) if is interface ( node ) : obj . shape = \"interface\" else : obj . shape = \"class\" for par node in node . ancestors ( recurs = False ) : try : par obj = self . object from node ( par node ) self . add relationship ( obj , par obj , \"specialization\" ) except Key Error : continue for impl node in node . implements : try : impl obj = self . object from node ( impl node ) self . add relationship ( obj , impl obj , \"implements\" ) except Key Error : continue for name , values in list ( node . instance attrs type . items ( ) ) + list ( node . locals type . items ( ) ) : for value in values : if value is astroid . Uninferable : continue if isinstance ( value , astroid . Instance ) : value = value . proxied try : associated obj = self . object from node ( value ) self . add relationship ( associated obj , obj , \"association\" , name ) except Key Error : continue", "predictions": ["extract relationships from relationships ."], "references": ["extract relation ships between nodes in the diagram"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1494, "code": "def modules ( self ) : return [ o for o in self . objects if isinstance ( o . node , astroid . Module ) ]", "predictions": ["list of all modules in this module ."], "references": ["return all module nodes in the diagram"], "bleu": 0.19070828081828378, "rouge_l": 0.26991150442477874}
{"id": 1495, "code": "def module ( self , name ) : for mod in self . modules ( ) : if mod . node . name == name : return mod raise Key Error ( name )", "predictions": ["return the module with the given name ."], "references": ["return a module by its name raise keyerror if not found"], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 1496, "code": "def add from depend ( self , node , from module ) : mod name = node . root ( ) . name obj = self . module ( mod name ) if from module not in obj . node . depends : obj . node . depends . append ( from module )", "predictions": ["add an instance of the given depend to the depend module ."], "references": ["add dependencies created by from - imports"], "bleu": 0.10390302174233558, "rouge_l": 0.11050724637681159}
{"id": 1497, "code": "def extract relationships ( self ) : Class Diagram . extract relationships ( self ) for obj in self . classes ( ) : try : mod = self . object from node ( obj . node . root ( ) ) self . add relationship ( obj , mod , \"ownership\" ) except Key Error : continue for obj in self . modules ( ) : obj . shape = \"package\" for dep name in obj . node . depends : try : dep = self . get module ( dep name , obj . node ) except Key Error : continue self . add relationship ( obj , dep , \"depends\" )", "predictions": ["add all modules to the relationships ."], "references": ["extract relation ships between nodes in the diagram"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1498, "code": "def query ( self ) : if hasattr ( self . model , 'query' ) : return self . model . query else : return self . session . query ( self . model )", "predictions": ["query for model ."], "references": ["determines which method of getting the query object for use"], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 1499, "code": "def prepare request ( uri , headers = None , data = None , method = None ) : if headers is None : headers = { } if data and not method : method = 'POST' elif not method : method = 'GET' if method == 'GET' and data : uri = add params to uri ( uri , data ) data = None return uri , headers , data , method", "predictions": ["prepare request object uri ."], "references": ["make request parameters right ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 1500, "code": "def handle oauth1 response ( self , args ) : client = self . make client ( ) client . verifier = args . get ( 'oauth verifier' ) tup = session . get ( '%s oauthtok' % self . name ) if not tup : raise O Auth Exception ( 'Token not found, maybe you disabled cookie' , type = 'token not found' ) client . resource owner key = tup [ 0 ] client . resource owner secret = tup [ 1 ] uri , headers , data = client . sign ( self . expand url ( self . access token url ) , encode ( self . access token method ) ) headers . update ( self . access token headers ) resp , content = self . http request ( uri , headers , to bytes ( data , self . encoding ) , method = self . access token method ) data = parse response ( resp , content ) if resp . code not in ( 200 , 201 ) : raise O Auth Exception ( 'Invalid response from %s' % self . name , type = 'invalid response' , data = data ) return data", "predictions": ["make a response from the http response ."], "references": ["handles an oauth1 authorization response ."], "bleu": 0.21105340631872638, "rouge_l": 0.2932692307692307}
{"id": 1501, "code": "def handle oauth2 response ( self , args ) : client = self . make client ( ) remote args = { 'code' : args . get ( 'code' ) , 'client secret' : self . consumer secret , 'redirect uri' : session . get ( '%s oauthredir' % self . name ) } log . debug ( 'Prepare oauth2 remote args %r' , remote args ) remote args . update ( self . access token params ) headers = copy ( self . access token headers ) if self . access token method == 'POST' : headers . update ( { 'Content-Type' : 'application/x-www-form-urlencoded' } ) body = client . prepare request body ( * * remote args ) resp , content = self . http request ( self . expand url ( self . access token url ) , headers = headers , data = to bytes ( body , self . encoding ) , method = self . access token method , ) elif self . access token method == 'GET' : qs = client . prepare request body ( * * remote args ) url = self . expand url ( self . access token url ) url += ( '?' in url and '&' or '?' ) + qs resp , content = self . http request ( url , headers = headers , method = self . access token method , ) else : raise O Auth Exception ( 'Unsupported access token method: %s' % self . access token method ) data = parse response ( resp , content , content type = self . content type ) if resp . code not in ( 200 , 201 ) : raise O Auth Exception ( 'Invalid response from %s' % self . name , type = 'invalid response' , data = data ) return data", "predictions": ["gets the response from the http response ."], "references": ["handles an oauth2 authorization response ."], "bleu": 0.21105340631872638, "rouge_l": 0.2932692307692307}
{"id": 1502, "code": "def authorized response ( self , args = None ) : if args is None : args = request . args if 'oauth verifier' in args : data = self . handle oauth1 response ( args ) elif 'code' in args : data = self . handle oauth2 response ( args ) else : data = self . handle unknown response ( ) session . pop ( '%s oauthtok' % self . name , None ) session . pop ( '%s oauthredir' % self . name , None ) return data", "predictions": ["authorized a response from the client ."], "references": ["handles authorization response smartly ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 1503, "code": "def make client with token ( self , token ) : cached clients = getattr ( self , 'clients' , None ) hashed token = hash token ( self , token ) if cached clients and hashed token in cached clients : return cached clients [ hashed token ] client = self . make client ( token ) if cached clients : cached clients [ hashed token ] = client return client", "predictions": ["return a client with the specified token ."], "references": ["uses cached client or create new one with specific token ."], "bleu": 0.16481400866629634, "rouge_l": 0.4093959731543625}
{"id": 1504, "code": "def confirm authorization request ( self ) : server = self . server uri , http method , body , headers = extract params ( ) try : realms , credentials = server . get realms and credentials ( uri , http method = http method , body = body , headers = headers ) ret = server . create authorization response ( uri , http method , body , headers , realms , credentials ) log . debug ( 'Authorization successful.' ) return create response ( * ret ) except errors . O Auth1Error as e : return redirect ( e . in uri ( self . error uri ) ) except errors . Invalid Client Error as e : return redirect ( e . in uri ( self . error uri ) )", "predictions": ["handle the in - memory loop . . . . . . . . ."], "references": ["when consumer confirm the authrozation ."], "bleu": 0.09103526405546068, "rouge_l": 0.2064297800338409}
{"id": 1505, "code": "def require oauth ( self , * realms , * * kwargs ) : def wrapper ( f ) : @ wraps ( f ) def decorated ( * args , * * kwargs ) : for func in self . before request funcs : func ( ) if hasattr ( request , 'oauth' ) and request . oauth : return f ( * args , * * kwargs ) server = self . server uri , http method , body , headers = extract params ( ) try : valid , req = server . validate protected resource request ( uri , http method , body , headers , realms ) except Exception as e : log . warn ( 'Exception: %r' , e ) e . urlencoded = urlencode ( [ ( 'error' , 'unknown' ) ] ) e . status code = 400 return error response ( e ) for func in self . after request funcs : valid , req = func ( valid , req ) if not valid : return abort ( 401 ) req . user = req . access token . user request . oauth = req return f ( * args , * * kwargs ) return decorated return wrapper", "predictions": ["decorator to validate protected protected ."], "references": ["protect resource with specified scopes ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 1506, "code": "def get default realms ( self , client key , request ) : log . debug ( 'Get realms for %r' , client key ) if not request . client : request . client = self . clientgetter ( client key = client key ) client = request . client if hasattr ( client , 'default realms' ) : return client . default realms return [ ]", "predictions": ["gets the expr for the given client . . . . . . . . ."], "references": ["default realms of the client ."], "bleu": 0.10878661088699644, "rouge_l": 0.2970779220779221}
{"id": 1507, "code": "def get realms ( self , token , request ) : log . debug ( 'Get realms of %r' , token ) tok = request . request token or self . grantgetter ( token = token ) if not tok : return [ ] request . request token = tok if hasattr ( tok , 'realms' ) : return tok . realms or [ ] return [ ]", "predictions": ["visit the request node"], "references": ["realms for this request token ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 1508, "code": "def get redirect uri ( self , token , request ) : log . debug ( 'Get redirect uri of %r' , token ) tok = request . request token or self . grantgetter ( token = token ) return tok . redirect uri", "predictions": ["visit the assert uri uri and returns the assert uri"], "references": ["redirect uri for this request token ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 1509, "code": "def get rsa key ( self , client key , request ) : if not request . client : request . client = self . clientgetter ( client key = client key ) if hasattr ( request . client , 'rsa key' ) : return request . client . rsa key return None", "predictions": ["returns the dict of dict for the given client k k k k k k k k k k k k k k k k k k k k k k"], "references": ["retrieves a previously stored client provided rsa key ."], "bleu": 0.03901663112717908, "rouge_l": 0.055505004549590536}
{"id": 1510, "code": "def validate client key ( self , client key , request ) : log . debug ( 'Validate client key for %r' , client key ) if not request . client : request . client = self . clientgetter ( client key = client key ) if request . client : return True return False", "predictions": ["check if the reversed key should be used to access the reversed key = true"], "references": ["validates that supplied client key ."], "bleu": 0.08225964699966554, "rouge_l": 0.10321489001692045}
{"id": 1511, "code": "def validate request token ( self , client key , token , request ) : log . debug ( 'Validate request token %r for %r' , token , client key ) tok = request . request token or self . grantgetter ( token = token ) if tok and tok . client key == client key : request . request token = tok return True return False", "predictions": ["visit the assignname token token"], "references": ["validates request token is available for client ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1512, "code": "def validate access token ( self , client key , token , request ) : log . debug ( 'Validate access token %r for %r' , token , client key ) tok = request . access token or self . tokengetter ( client key = client key , token = token , ) if tok : request . access token = tok return True return False", "predictions": ["check if the name is valid ."], "references": ["validates access token is available for client ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 1513, "code": "def validate timestamp and nonce ( self , client key , timestamp , nonce , request , request token = None , access token = None ) : log . debug ( 'Validate timestamp and nonce %r' , client key ) nonce exists = self . noncegetter ( client key = client key , timestamp = timestamp , nonce = nonce , request token = request token , access token = access token ) if nonce exists : return False self . noncesetter ( client key = client key , timestamp = timestamp , nonce = nonce , request token = request token , access token = access token ) return True", "predictions": ["check if docstring and is valid doc doc doc doc doc doc doc doc doc doc doc doc doc doc doc doc doc doc doc doc doc doc doc doc doc"], "references": ["validate the timestamp and nonce is used or not ."], "bleu": 0.04317900023606586, "rouge_l": 0.10748898678414096}
{"id": 1514, "code": "def validate redirect uri ( self , client key , redirect uri , request ) : log . debug ( 'Validate redirect uri %r for %r' , redirect uri , client key ) if not request . client : request . client = self . clientgetter ( client key = client key ) if not request . client : return False if not request . client . redirect uris and redirect uri is None : return True request . redirect uri = redirect uri return redirect uri in request . client . redirect uris", "predictions": ["check if the request comparison comparison ."], "references": ["validate if the redirect_uri is allowed by the client ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 1515, "code": "def validate realms ( self , client key , token , request , uri = None , realms = None ) : log . debug ( 'Validate realms %r for %r' , realms , client key ) if request . access token : tok = request . access token else : tok = self . tokengetter ( client key = client key , token = token ) request . access token = tok if not tok : return False return set ( tok . realms ) . issuperset ( set ( realms ) )", "predictions": ["subgraph if the name is valid"], "references": ["check if the token has permission on those realms ."], "bleu": 0.14925824694560996, "rouge_l": 0.23921568627450981}
{"id": 1516, "code": "def validate verifier ( self , client key , token , verifier , request ) : log . debug ( 'Validate verifier %r for %r' , verifier , client key ) data = self . verifiergetter ( verifier = verifier , token = token ) if not data : return False if not hasattr ( data , 'user' ) : log . debug ( 'Verifier should has user attribute' ) return False request . user = data . user if hasattr ( data , 'client key' ) : return data . client key == client key return True", "predictions": ["subgraph that the parse extra key is valid ends ends ends ends ends with the given key ends ends ends ends"], "references": ["validate verifier exists ."], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 1517, "code": "def verify request token ( self , token , request ) : log . debug ( 'Verify request token %r' , token ) tok = request . request token or self . grantgetter ( token = token ) if tok : request . request token = tok return True return False", "predictions": ["add a checker token token to the checker set set set"], "references": ["verify if the request token is existed ."], "bleu": 0.12605968092174913, "rouge_l": 0.108348134991119}
{"id": 1518, "code": "def verify realms ( self , token , realms , request ) : log . debug ( 'Verify realms %r' , realms ) tok = request . request token or self . grantgetter ( token = token ) if not tok : return False request . request token = tok if not hasattr ( tok , 'realms' ) : return True return set ( tok . realms ) == set ( realms )", "predictions": ["add a relationship from the object"], "references": ["verify if the realms match the requested realms ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 1519, "code": "def confirm authorization request ( self ) : server = self . server scope = request . values . get ( 'scope' ) or '' scopes = scope . split ( ) credentials = dict ( client id = request . values . get ( 'client id' ) , redirect uri = request . values . get ( 'redirect uri' , None ) , response type = request . values . get ( 'response type' , None ) , state = request . values . get ( 'state' , None ) ) log . debug ( 'Fetched credentials from request %r.' , credentials ) redirect uri = credentials . get ( 'redirect uri' ) log . debug ( 'Found redirect uri %s.' , redirect uri ) uri , http method , body , headers = extract params ( ) try : ret = server . create authorization response ( uri , http method , body , headers , scopes , credentials ) log . debug ( 'Authorization successful.' ) return create response ( * ret ) except oauth2 . Fatal Client Error as e : log . debug ( 'Fatal client error %r' , e , exc info = True ) return self . on exception ( e , e . in uri ( self . error uri ) ) except oauth2 . O Auth2Error as e : log . debug ( 'O Auth2Error: %r' , e , exc info = True ) state = request . values . get ( 'state' ) if state and not e . state : e . state = state return self . on exception ( e , e . in uri ( redirect uri or self . error uri ) ) except Exception as e : log . exception ( e ) return self . on exception ( e , add params to uri ( self . error uri , { 'error' : str ( e ) } ) )", "predictions": ["get the relationship request . . ."], "references": ["when consumer confirm the authorization ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 1520, "code": "def require oauth ( self , * scopes ) : def wrapper ( f ) : @ wraps ( f ) def decorated ( * args , * * kwargs ) : for func in self . before request funcs : func ( ) if hasattr ( request , 'oauth' ) and request . oauth : return f ( * args , * * kwargs ) valid , req = self . verify request ( scopes ) for func in self . after request funcs : valid , req = func ( valid , req ) if not valid : if self . invalid response : return self . invalid response ( req ) return abort ( 401 ) request . oauth = req return f ( * args , * * kwargs ) return decorated return wrapper", "predictions": ["get a attrs from the attrs . . . . . . . . . ."], "references": ["protect resource with specified scopes ."], "bleu": 0.07692375026049747, "rouge_l": 0.09902597402597402}
{"id": 1521, "code": "def get default redirect uri ( self , client id , request , * args , * * kwargs ) : request . client = request . client or self . clientgetter ( client id ) redirect uri = request . client . default redirect uri log . debug ( 'Found default redirect uri %r' , redirect uri ) return redirect uri", "predictions": ["returns the object self = redirect = 0 = 1 = 0 = 1 = 1 = 0 = 1 = 0 = 1 = 1 = 1 = 0 ="], "references": ["default redirect_uri for the given client ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 1522, "code": "def get default scopes ( self , client id , request , * args , * * kwargs ) : request . client = request . client or self . clientgetter ( client id ) scopes = request . client . default scopes log . debug ( 'Found default scopes %r' , scopes ) return scopes", "predictions": ["returns the names of the names for the client if available if not set"], "references": ["default scopes for the given client ."], "bleu": 0.1250076305588977, "rouge_l": 0.30398671096345514}
{"id": 1523, "code": "def save authorization code ( self , client id , code , request , * args , * * kwargs ) : log . debug ( 'Persist authorization code %r for client %r' , code , client id ) request . client = request . client or self . clientgetter ( client id ) self . grantsetter ( client id , code , request , * args , * * kwargs ) return request . client . default redirect uri", "predictions": ["classes for saving authorization self . . ."], "references": ["persist the authorization code ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 1524, "code": "def save bearer token ( self , token , request , * args , * * kwargs ) : log . debug ( 'Save bearer token %r' , token ) self . tokensetter ( token , request , * args , * * kwargs ) return request . client . default redirect uri", "predictions": ["classe a bearer self node node node node node node node ."], "references": ["persist the bearer token ."], "bleu": 0.11498759556447223, "rouge_l": 0.25416666666666665}
{"id": 1525, "code": "def validate client id ( self , client id , request , * args , * * kwargs ) : log . debug ( 'Validate client %r' , client id ) client = request . client or self . clientgetter ( client id ) if client : request . client = client return True return False", "predictions": ["extract the relationships from the relationships attrs attrs attrs attrs attrs attrs attrs attrs attrs attrs"], "references": ["ensure client_id belong to a valid and active client ."], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 1526, "code": "def validate code ( self , client id , code , client , request , * args , * * kwargs ) : client = client or self . clientgetter ( client id ) log . debug ( 'Validate code for client %r and code %r' , client . client id , code ) grant = self . grantgetter ( client id = client . client id , code = code ) if not grant : log . debug ( 'Grant not found.' ) return False if hasattr ( grant , 'expires' ) and datetime . datetime . utcnow ( ) > grant . expires : log . debug ( 'Grant is expired.' ) return False request . state = kwargs . get ( 'state' ) request . user = grant . user request . scopes = grant . scopes return True", "predictions": ["modules must be called before the return trap"], "references": ["ensure the grant code is valid ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 1527, "code": "def validate scopes ( self , client id , scopes , client , request , * args , * * kwargs ) : if hasattr ( client , 'validate scopes' ) : return client . validate scopes ( scopes ) return set ( client . default scopes ) . issuperset ( set ( scopes ) )", "predictions": ["module scopes for all scopes . . ."], "references": ["ensure the client is authorized access to requested scopes ."], "bleu": 0.1643685581109115, "rouge_l": 0.21785714285714283}
{"id": 1528, "code": "def revoke token ( self , token , token type hint , request , * args , * * kwargs ) : if token type hint : tok = self . tokengetter ( * * { token type hint : token } ) else : tok = self . tokengetter ( access token = token ) if not tok : tok = self . tokengetter ( refresh token = token ) if tok : request . client id = tok . client id request . user = tok . user tok . delete ( ) return True msg = 'Invalid token supplied.' log . debug ( msg ) request . error message = msg return False", "predictions": ["add a from the client if it exists if it is not already in the client"], "references": ["revoke an access or refresh token ."], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 1529, "code": "def update qq api request data ( data = { } ) : defaults = { 'openid' : session . get ( 'qq openid' ) , 'access token' : session . get ( 'qq token' ) [ 0 ] , 'oauth consumer key' : QQ APP ID , } defaults . update ( data ) return defaults", "predictions": ["extract relationships from relationships mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod mod"], "references": ["update some required parameters for oauth2 . 0 api calls"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1530, "code": "def convert keys to string ( dictionary ) : if not isinstance ( dictionary , dict ) : return dictionary return dict ( ( str ( k ) , convert keys to string ( v ) ) for k , v in dictionary . items ( ) )", "predictions": ["query all keys in if necessary"], "references": ["recursively converts dictionary keys to strings ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 1531, "code": "def register to ( self , oauth , name = None , * * kwargs ) : kwargs = self . process kwargs ( name = ( name or self . default name ) , * * kwargs ) return oauth . remote app ( * * kwargs )", "predictions": ["prepare a headers request is not a remote is the remote"], "references": ["creates a remote app and registers it ."], "bleu": 0.14991106946711685, "rouge_l": 0.216696269982238}
{"id": 1532, "code": "def create ( self , oauth , * * kwargs ) : kwargs = self . process kwargs ( name = self . default name , register = False , * * kwargs ) return oauth . remote app ( * * kwargs )", "predictions": ["handle the self = true = false if it is not already created = false = false = false = false otherwise ."], "references": ["creates a remote app only ."], "bleu": 0.05291907393644996, "rouge_l": 0.07711757269279393}
{"id": 1533, "code": "def extract params ( ) : uri = get uri from request ( request ) http method = request . method headers = dict ( request . headers ) if 'wsgi.input' in headers : del headers [ 'wsgi.input' ] if 'wsgi.errors' in headers : del headers [ 'wsgi.errors' ] if request . authorization : headers [ 'Authorization' ] = request . authorization body = request . form . to dict ( ) return uri , http method , body , headers", "predictions": ["handle oauth2 oauth2 command line parameters"], "references": ["extract request params ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1534, "code": "def to bytes ( text , encoding = 'utf-8' ) : if not text : return text if not isinstance ( text , bytes type ) : text = text . encode ( encoding ) return text", "predictions": ["convert self to response ."], "references": ["make sure text is bytes type ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1535, "code": "def decode base64 ( text , encoding = 'utf-8' ) : text = to bytes ( text , encoding ) return to unicode ( base64 . b64decode ( text ) , encoding )", "predictions": ["make sure token is client . ."], "references": ["decode base64 string ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 1536, "code": "def create response ( headers , body , status ) : response = Response ( body or '' ) for k , v in headers . items ( ) : response . headers [ str ( k ) ] = v response . status code = status return response", "predictions": ["create a response object from the headers ."], "references": ["create response class for flask ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 1537, "code": "def get cached clients ( ) : if O Auth . state key not in current app . extensions : raise Runtime Error ( '%r is not initialized.' % current app ) state = current app . extensions [ O Auth . state key ] return state . cached clients", "predictions": ["get the cached clients for the current app ."], "references": ["gets the cached clients dictionary in current context ."], "bleu": 0.2907153684841096, "rouge_l": 0.5555555555555556}
{"id": 1538, "code": "def check exception ( self ) : for i in xrange ( self . iterations ) : cert = X509 ( ) try : cert . get pubkey ( ) except Error : pass", "predictions": ["check if any of the exception is valid ."], "references": ["call the method repeatedly such that it will raise an exception ."], "bleu": 0.12026590852507517, "rouge_l": 0.2785388127853881}
{"id": 1539, "code": "def check success ( self ) : small = xrange ( 3 ) for i in xrange ( self . iterations ) : key = P Key ( ) key . generate key ( TYPE DSA , 256 ) for i in small : cert = X509 ( ) cert . set pubkey ( key ) for i in small : cert . get pubkey ( )", "predictions": ["check if success of success ."], "references": ["call the method repeatedly such that it will return a pkey object ."], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 1540, "code": "def check load privatekey callback ( self ) : for i in xrange ( self . iterations * 10 ) : load privatekey ( FILETYPE PEM , self . ENCRYPTED PEM , lambda * args : \"hello, secret\" )", "predictions": ["check if all load secret\" are secret\""], "references": ["call the function with an encrypted pem and a passphrase callback ."], "bleu": 0.07646493705380929, "rouge_l": 0.0}
{"id": 1541, "code": "def bio to string ( bio ) : result buffer = ffi . new ( 'char**' ) buffer length = lib . BIO get mem data ( bio , result buffer ) return ffi . buffer ( result buffer [ 0 ] , buffer length ) [ : ]", "predictions": ["convert bio bio bio to string"], "references": ["copy the contents of an openssl bio object into a python byte string ."], "bleu": 0.06443935473636557, "rouge_l": 0.18654434250764526}
{"id": 1542, "code": "def print token factory ( col ) : def helper ( msg ) : style = style from dict ( { Token . Color : col , } ) tokens = [ ( Token . Color , msg ) ] print tokens ( tokens , style = style ) def helper no terminal ( msg ) : print ( msg ) if sys . stdout . isatty ( ) : return helper else : return helper no terminal", "predictions": ["print a token factory for a message ."], "references": ["internal helper to provide color names ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 1543, "code": "def get service metadata ( self ) : return { 'import labels as tags' : self . config . get ( 'import labels as tags' , False , asbool ) , 'label template' : self . config . get ( 'label template' , DEFAULT LABEL TEMPLATE ) , }", "predictions": ["returns the metadata for the service ."], "references": ["return extra config options to be passed to the trelloissue class"], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 1544, "code": "def issues ( self ) : for board in self . get boards ( ) : for lst in self . get lists ( board [ 'id' ] ) : listextra = dict ( boardname = board [ 'name' ] , listname = lst [ 'name' ] ) for card in self . get cards ( lst [ 'id' ] ) : issue = self . get issue for record ( card , extra = listextra ) issue . update extra ( { \"annotations\" : self . annotations ( card ) } ) yield issue", "predictions": ["return an iterable of issues for the current issue ."], "references": ["returns a list of dicts representing issues from a remote service ."], "bleu": 0.12273680279953825, "rouge_l": 0.2683284457478006}
{"id": 1545, "code": "def get comments ( self , card id ) : params = { 'filter' : 'comment Card' , 'member Creator fields' : 'username' } comments = self . api request ( \"/1/cards/{card id}/actions\" . format ( card id = card id ) , * * params ) for comment in comments : assert comment [ 'type' ] == 'comment Card' yield comment", "predictions": ["get all comments for a card"], "references": ["returns an iterator for the comments on a certain card ."], "bleu": 0.12071482560966854, "rouge_l": 0.33516483516483514}
{"id": 1546, "code": "def get issues ( self , repo , keys ) : key1 , key2 = keys key3 = key1 [ : - 1 ] url = self . base url + \"/api/0/\" + repo + \"/\" + key1 response = self . session . get ( url , params = dict ( status = 'Open' ) ) if not bool ( response ) : error = response . json ( ) code = error [ 'error code' ] if code == 'ETRACKERDISABLED' : return [ ] else : raise IO Error ( 'Failed to talk to %r %r' % ( url , error ) ) issues = [ ] for result in response . json ( ) [ key2 ] : idx = six . text type ( result [ 'id' ] ) result [ 'html url' ] = \"/\" . join ( [ self . base url , repo , key3 , idx ] ) issues . append ( ( repo , result ) ) return issues", "predictions": ["get issues for a given repo"], "references": ["grab all the issues"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 1547, "code": "def api url ( self , path , * * context ) : if self . host == 'github.com' : baseurl = \"https://api.github.com\" else : baseurl = \"https://{}/api/v3\" . format ( self . host ) return baseurl + path . format ( * * context )", "predictions": ["return the api url for the given path ."], "references": ["build the full url to the api endpoint"], "bleu": 0.21105340631872635, "rouge_l": 0.35672514619883033}
{"id": 1548, "code": "def getter ( self , url , subkey = None ) : kwargs = { } if 'basic' in self . auth : kwargs [ 'auth' ] = self . auth [ 'basic' ] results = [ ] link = dict ( next = url ) while 'next' in link : response = self . session . get ( link [ 'next' ] , * * kwargs ) if response . status code == 404 and 'token' in self . auth : log . warn ( \"A '404' from github may indicate an auth \" \"failure. Make sure both that your token is correct \" \"and that it has 'public repo' and not 'public \" \"access' rights.\" ) json res = self . json response ( response ) if subkey is not None : json res = json res [ subkey ] results += json res link = self . link field to dict ( response . headers . get ( 'link' , None ) ) return results", "predictions": ["perform http getter for the given url ."], "references": ["pagination utility . obnoxious ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 1549, "code": "def get owned repo issues ( self , tag ) : issues = { } for issue in self . client . get issues ( * tag . split ( '/' ) ) : issues [ issue [ 'url' ] ] = ( tag , issue ) return issues", "predictions": ["fetch issues issues by tag tag ."], "references": ["grab all the issues"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 1550, "code": "def get query ( self , query ) : issues = { } for issue in self . client . get query ( query ) : url = issue [ 'html url' ] try : repo = self . get repository from issue ( issue ) except Value Error as e : log . critical ( e ) else : issues [ url ] = ( repo , issue ) return issues", "predictions": ["get repository issues from issue"], "references": ["grab all issues matching a github query"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1551, "code": "def reqs ( self , tag ) : return [ ( tag , i ) for i in self . client . get pulls ( * tag . split ( '/' ) ) ]", "predictions": ["return a list of pulls ids for a tag ."], "references": ["grab all the pull requests"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1552, "code": "def aggregate issues ( conf , main section , debug ) : log . info ( \"Starting to aggregate remote issues.\" ) targets = aslist ( conf . get ( main section , 'targets' ) ) queue = multiprocessing . Queue ( ) log . info ( \"Spawning %i workers.\" % len ( targets ) ) processes = [ ] if debug : for target in targets : aggregate issues ( conf , main section , target , queue , conf . get ( target , 'service' ) ) else : for target in targets : proc = multiprocessing . Process ( target = aggregate issues , args = ( conf , main section , target , queue , conf . get ( target , 'service' ) ) ) proc . start ( ) processes . append ( proc ) time . sleep ( 1 ) currently running = len ( targets ) while currently running > 0 : issue = queue . get ( True ) if isinstance ( issue , tuple ) : completion type , args = issue if completion type == SERVICE FINISHED ERROR : target , e = args log . info ( \"Terminating workers\" ) for process in processes : process . terminate ( ) raise Runtime Error ( \"critical error in target '{}'\" . format ( target ) ) currently running -= 1 continue yield issue log . info ( \"Done aggregating remote issues.\" )", "predictions": ["aggregate issues from an issue ."], "references": ["return all issues from every target ."], "bleu": 0.2644358066258934, "rouge_l": 0.45522388059701485}
{"id": 1553, "code": "def get config or default ( self , key , default , as type = lambda x : x ) : if self . main config . has option ( self . main section , key ) : return as type ( self . main config . get ( self . main section , key ) ) return default", "predictions": ["return the config or default if it exists ."], "references": ["return a main config value or default if it does not exist ."], "bleu": 0.2832682524731647, "rouge_l": 0.6161616161616162}
{"id": 1554, "code": "def validate config ( cls , service config , target ) : if service config . has option ( target , 'only if assigned' ) : die ( \"[%s] has an 'only if assigned' option.  Should be \" \"'%s.only if assigned'.\" % ( target , cls . CONFIG PREFIX ) ) if service config . has option ( target , 'also unassigned' ) : die ( \"[%s] has an 'also unassigned' option.  Should be \" \"'%s.also unassigned'.\" % ( target , cls . CONFIG PREFIX ) ) if service config . has option ( target , 'default priority' ) : die ( \"[%s] has a 'default priority' option.  Should be \" \"'%s.default priority'.\" % ( target , cls . CONFIG PREFIX ) ) if service config . has option ( target , 'add tags' ) : die ( \"[%s] has an 'add tags' option.  Should be \" \"'%s.add tags'.\" % ( target , cls . CONFIG PREFIX ) )", "predictions": ["validate the configuration options ."], "references": ["validate generic options for a particular target"], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 1555, "code": "def include ( self , issue ) : only if assigned = self . config . get ( 'only if assigned' , None ) if only if assigned : owner = self . get owner ( issue ) include owners = [ only if assigned ] if self . config . get ( 'also unassigned' , None , asbool ) : include owners . append ( None ) return owner in include owners only if author = self . config . get ( 'only if author' , None ) if only if author : return self . get author ( issue ) == only if author return True", "predictions": ["include the specified issue"], "references": ["return true if the issue in question should be included"], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 1556, "code": "def oracle eval ( command ) : p = subprocess . Popen ( command , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) p . wait ( ) if p . returncode == 0 : return p . stdout . readline ( ) . strip ( ) . decode ( 'utf-8' ) else : die ( \"Error retrieving password: `{command}` returned '{error}'\" . format ( command = command , error = p . stderr . read ( ) . strip ( ) ) )", "predictions": ["runs a oracle command with the oracle ."], "references": ["retrieve password from the given command"], "bleu": 0.17747405280050269, "rouge_l": 0.14663461538461536}
{"id": 1557, "code": "def getint ( self , section , option ) : try : return super ( Bugwarrior Config Parser , self ) . getint ( section , option ) except Value Error : if self . get ( section , option ) == u'' : return None else : raise Value Error ( \"{section}.{option} must be an integer or empty.\" . format ( section = section , option = option ) )", "predictions": ["returns the getint object for the given option ."], "references": ["accepts both integers and empty values ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 1558, "code": "def get data ( self , url ) : return self . json response ( requests . get ( url , * * self . requests kwargs ) )", "predictions": ["do a get request"], "references": ["perform a request to the fully qualified url and return json ."], "bleu": 0.0538140946637381, "rouge_l": 0.22932330827067668}
{"id": 1559, "code": "def hamdist ( str1 , str2 ) : diffs = 0 for ch1 , ch2 in zip ( str1 , str2 ) : if ch1 != ch2 : diffs += 1 return diffs", "predictions": ["return the number of numbers of the given dimension ."], "references": ["count the # of differences between equal length strings str1 and str2"], "bleu": 0.11421946507590645, "rouge_l": 0.1788856304985337}
{"id": 1560, "code": "def fdrcorrection ( pvals , alpha = 0.05 ) : pvals = np . asarray ( pvals ) pvals sortind = np . argsort ( pvals ) pvals sorted = np . take ( pvals , pvals sortind ) ecdffactor = ecdf ( pvals sorted ) reject = pvals sorted <= ecdffactor * alpha if reject . any ( ) : rejectmax = max ( np . nonzero ( reject ) [ 0 ] ) reject [ : rejectmax ] = True pvals corrected raw = pvals sorted / ecdffactor pvals corrected = np . minimum . accumulate ( pvals corrected raw [ : : - 1 ] ) [ : : - 1 ] del pvals corrected raw pvals corrected [ pvals corrected > 1 ] = 1 pvals corrected = np . empty like ( pvals corrected ) pvals corrected [ pvals sortind ] = pvals corrected del pvals corrected reject = np . empty like ( reject ) reject [ pvals sortind ] = reject return reject , pvals corrected", "predictions": ["calculates the log - likelihood of the energy corrected corrected ."], "references": ["benjamini hocheberg fdr correction . inspired by statsmodels"], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 1561, "code": "def prepare argparser ( ) : description = \"%(prog)s -- Gene Set Enrichment Analysis in Python\" epilog = \"For command line options of each command, type: %(prog)s COMMAND -h\" argparser = ap . Argument Parser ( description = description , epilog = epilog ) argparser . add argument ( \"--version\" , action = \"version\" , version = \"%(prog)s \" + version ) subparsers = argparser . add subparsers ( dest = 'subcommand name' ) #help=\"sub-command help\") add gsea parser ( subparsers ) add prerank parser ( subparsers ) add singlesample parser ( subparsers ) add plot parser ( subparsers ) add enrichr parser ( subparsers ) add biomart parser ( subparsers ) return argparser", "predictions": ["setup setup parser ."], "references": ["prepare argparser object . new options will be added in this function first ."], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 1562, "code": "def add gsea parser ( subparsers ) : argparser gsea = subparsers . add parser ( \"gsea\" , help = \"Main GSE Apy Function: run GSE Apy instead of GSEA.\" ) group input = argparser gsea . add argument group ( \"Input files arguments\" ) group input . add argument ( \"-d\" , \"--data\" , dest = \"data\" , action = \"store\" , type = str , required = True , help = \"Input gene expression dataset file in txt format.Same with GSEA.\" ) group input . add argument ( \"-c\" , \"--cls\" , dest = \"cls\" , action = \"store\" , type = str , required = True , help = \"Input class vector (phenotype) file in CLS format. Same with GSEA.\" ) group input . add argument ( \"-g\" , \"--gmt\" , dest = \"gmt\" , action = \"store\" , type = str , required = True , help = \"Gene set database in GMT format. Same with GSEA.\" ) group input . add argument ( \"-t\" , \"--permu-type\" , action = \"store\" , dest = \"type\" , type = str , metavar = 'per Type' , choices = ( \"gene set\" , \"phenotype\" ) , default = \"gene set\" , help = \"Permutation type. Same with GSEA, choose from {'gene set', 'phenotype'}\" ) group output = argparser gsea . add argument group ( \"Output arguments\" ) add output option ( group output ) group opt = argparser gsea . add argument group ( \"GSEA advanced arguments\" ) group opt . add argument ( \"-n\" , \"--permu-num\" , dest = \"n\" , action = \"store\" , type = int , default = 1000 , metavar = 'nperm' , help = \"Number of random permutations. For calculating esnulls. Default: 1000\" ) group opt . add argument ( \"--min-size\" , dest = \"mins\" , action = \"store\" , type = int , default = 15 , metavar = 'int' , help = \"Min size of input genes presented in Gene Sets. Default: 15\" ) group opt . add argument ( \"--max-size\" , dest = \"maxs\" , action = \"store\" , type = int , default = 500 , metavar = 'int' , help = \"Max size of input genes presented in Gene Sets. Default: 500\" ) group opt . add argument ( \"-w\" , \"--weight\" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , help = 'Weighted score of rank metrics. For weighting input genes. Choose from {0, 1, 1.5, 2}. Default: 1' , ) group opt . add argument ( \"-m\" , \"--method\" , action = \"store\" , dest = \"method\" , type = str , metavar = '' , choices = ( \"signal to noise\" , \"t test\" , \"ratio of classes\" , \"diff of classes\" , \"log2 ratio of classes\" ) , default = \"log2 ratio of classes\" , help = ) group opt . add argument ( \"-a\" , \"--ascending\" , action = 'store true' , dest = 'ascending' , default = False , help = 'Rank metric sorting order. If the -a flag was chosen, then ascending equals to True. Default: False.' ) group opt . add argument ( \"-s\" , \"--seed\" , dest = \"seed\" , action = \"store\" , type = int , default = None , metavar = '' , help = \"Number of random seed. Default: None\" ) group opt . add argument ( \"-p\" , \"--threads\" , dest = \"threads\" , action = \"store\" , type = int , default = 1 , metavar = 'procs' , help = \"Number of Processes you are going to use. Default: 1\" ) return", "predictions": ["add gsea to the gsea parser ."], "references": ["add main function gsea argument parsers ."], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 1563, "code": "def add prerank parser ( subparsers ) : argparser prerank = subparsers . add parser ( \"prerank\" , help = \"Run GSE Apy Prerank tool on preranked gene list.\" ) prerank input = argparser prerank . add argument group ( \"Input files arguments\" ) prerank input . add argument ( \"-r\" , \"--rnk\" , dest = \"rnk\" , action = \"store\" , type = str , required = True , help = \"Ranking metric file in .rnk format. Same with GSEA.\" ) prerank input . add argument ( \"-g\" , \"--gmt\" , dest = \"gmt\" , action = \"store\" , type = str , required = True , help = \"Gene set database in GMT format. Same with GSEA.\" ) prerank input . add argument ( \"-l\" , \"--label\" , action = 'store' , nargs = 2 , dest = 'label' , metavar = ( 'pos' , 'neg' ) , type = str , default = ( 'Pos' , 'Neg' ) , help = \"The phenotype label argument need two parameters to define. Default: ('Pos','Neg')\" ) prerank output = argparser prerank . add argument group ( \"Output arguments\" ) add output option ( prerank output ) prerank opt = argparser prerank . add argument group ( \"GSEA advanced arguments\" ) prerank opt . add argument ( \"-n\" , \"--permu-num\" , dest = \"n\" , action = \"store\" , type = int , default = 1000 , metavar = 'nperm' , help = \"Number of random permutations. For calculating esnulls. Default: 1000\" ) prerank opt . add argument ( \"--min-size\" , dest = \"mins\" , action = \"store\" , type = int , default = 15 , metavar = 'int' , help = \"Min size of input genes presented in Gene Sets. Default: 15\" ) prerank opt . add argument ( \"--max-size\" , dest = \"maxs\" , action = \"store\" , type = int , default = 500 , metavar = 'int' , help = \"Max size of input genes presented in Gene Sets. Default: 500\" ) prerank opt . add argument ( \"-w\" , \"--weight\" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , help = 'Weighted score of rank metrics. For weighting input genes. Choose from {0, 1, 1.5, 2}. Default: 1' , ) prerank opt . add argument ( \"-a\" , \"--ascending\" , action = 'store true' , dest = 'ascending' , default = False , help = 'Rank metric sorting order. If the -a flag was chosen, then ascending equals to True. Default: False.' ) prerank opt . add argument ( \"-s\" , \"--seed\" , dest = \"seed\" , action = \"store\" , type = int , default = None , metavar = '' , help = \"Number of random seed. Default: None\" ) prerank opt . add argument ( \"-p\" , \"--threads\" , dest = \"threads\" , action = \"store\" , type = int , default = 1 , metavar = 'procs' , help = \"Number of Processes you are going to use. Default: 1\" ) return", "predictions": ["add options to the prerank ."], "references": ["add function prerank argument parsers ."], "bleu": 0.2626909894424158, "rouge_l": 0.5}
{"id": 1564, "code": "def add plot parser ( subparsers ) : argparser replot = subparsers . add parser ( \"replot\" , help = \"Reproduce GSEA desktop output figures.\" ) group replot = argparser replot . add argument group ( \"Input arguments\" ) group replot . add argument ( \"-i\" , \"--indir\" , action = \"store\" , dest = \"indir\" , required = True , metavar = 'GSEA dir' , help = \"The GSEA desktop results directroy that you want to reproduce the figure \" ) add output option ( group replot ) #add output group( argparser plot ) group replot . add argument ( \"-w\" , \"--weight\" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , help = 'Weighted score of rank metrics. Please Use the same value in GSEA. Choose from (0, 1, 1.5, 2),default: 1' , ) return", "predictions": ["add the plot for the gsea cli ."], "references": ["add function plot argument parsers ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 1565, "code": "def add enrichr parser ( subparsers ) : argparser enrichr = subparsers . add parser ( \"enrichr\" , help = \"Using Enrichr API to perform GO analysis.\" ) enrichr opt = argparser enrichr . add argument group ( \"Input arguments\" ) enrichr opt . add argument ( \"-i\" , \"--input-list\" , action = \"store\" , dest = \"gene list\" , type = str , required = True , metavar = 'I Ds' , help = \"Enrichr uses a list of gene names as input.\" ) enrichr opt . add argument ( \"-g\" , \"--gene-sets\" , action = \"store\" , dest = \"library\" , type = str , required = True , metavar = 'GMT' , help = \"Enrichr library name(s) required. Separate each name by comma.\" ) enrichr opt . add argument ( \"--org\" , \"--organism\" , action = \"store\" , dest = \"organism\" , type = str , default = '' , help = \"Enrichr supported organism name. Default: human. See here: https://amp.pharm.mssm.edu/mod Enrichr.\" ) enrichr opt . add argument ( \"--ds\" , \"--description\" , action = \"store\" , dest = \"descrip\" , type = str , default = 'enrichr' , metavar = 'STRING' , help = ) enrichr opt . add argument ( \"--cut\" , \"--cut-off\" , action = \"store\" , dest = \"thresh\" , metavar = 'float' , type = float , default = 0.05 , help = \"Adjust-Pval cutoff, used for generating plots. Default: 0.05.\" ) enrichr opt . add argument ( \"--bg\" , \"--background\" , action = \"store\" , dest = \"bg\" , default = 'hsapiens gene ensembl' , metavar = 'BGNUM' , help = \"Bio Mart Dataset name or Background total genes number. Default: None\" ) enrichr opt . add argument ( \"-t\" , \"--top-term\" , dest = \"term\" , action = \"store\" , type = int , default = 10 , metavar = 'int' , help = \"Numbers of top terms shown in the plot. Default: 10\" ) enrichr output = argparser enrichr . add argument group ( \"Output figure arguments\" ) add output option ( enrichr output ) return", "predictions": ["add options specific to the enrichr subcommand ."], "references": ["add function enrichr argument parsers ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 1566, "code": "def add biomart parser ( subparsers ) : argparser biomart = subparsers . add parser ( \"biomart\" , help = \"Using Bio Mart API to convert gene ids.\" ) biomart opt = argparser biomart . add argument group ( \"Input arguments\" ) biomart opt . add argument ( \"-f\" , \"--filter\" , action = 'store' , nargs = 2 , dest = 'filter' , required = True , metavar = ( 'NAME' , 'VALUE' ) , help = ) biomart opt . add argument ( \"-a\" , \"--attributes\" , action = \"store\" , dest = \"attrs\" , type = str , required = True , metavar = 'ATTR' , help = \"Which attribute(s) to retrieve. Separate each attr by comma.\" ) biomart opt . add argument ( \"-o\" , \"--ofile\" , dest = \"ofile\" , type = str , required = True , help = \"Output file name\" ) biomart opt . add argument ( \"-d\" , \"--dataset\" , action = \"store\" , dest = \"bg\" , type = str , default = 'hsapiens gene ensembl' , metavar = 'DATA' , help = \"Which dataset to use. Default: hsapiens gene ensembl\" ) biomart opt . add argument ( \"--host\" , action = \"store\" , dest = \"host\" , type = str , default = 'www.ensembl.org' , metavar = 'HOST' , help = \"Which host to use. Select from {'www.ensembl.org', 'asia.ensembl.org', 'useast.ensembl.org'}.\" ) biomart opt . add argument ( \"-m\" , \"--mart\" , action = \"store\" , dest = \"mart\" , type = str , metavar = 'MART' , default = \"ENSEMBL MART ENSEMBL\" , help = \"Which mart to use. Default: ENSEMBL MART ENSEMBL.\" ) biomart opt . add argument ( \"-v\" , \"--verbose\" , action = \"store true\" , default = False , dest = 'verbose' , help = \"Increase output verbosity, print out progress of your job\" , )", "predictions": ["add biomart to biomart ."], "references": ["add function biomart argument parsers ."], "bleu": 0.2658156069371863, "rouge_l": 0.5366568914956013}
{"id": 1567, "code": "def get marts ( self ) : mart names = pd . Series ( self . names , name = \"Name\" ) mart descriptions = pd . Series ( self . display Names , name = \"Description\" ) return pd . concat ( [ mart names , mart descriptions ] , axis = 1 )", "predictions": ["return a dataframe of marts marts ."], "references": ["get available marts and their names ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 1568, "code": "def get datasets ( self , mart = 'ENSEMBL MART ENSEMBL' ) : datasets = self . datasets ( mart , raw = True ) return pd . read csv ( String IO ( datasets ) , header = None , usecols = [ 1 , 2 ] , names = [ \"Name\" , \"Description\" ] , sep = \"\\t\" )", "predictions": ["v response in a list of response ."], "references": ["get available datasets from mart you ve selected"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1569, "code": "def get attributes ( self , dataset ) : attributes = self . attributes ( dataset ) attr = [ ( k , v [ 0 ] ) for k , v in attributes . items ( ) ] return pd . Data Frame ( attr , columns = [ \"Attribute\" , \"Description\" ] )", "predictions": ["return cached cached cached cached cached cached cached cached cached cached cached cached cached cached cached data ."], "references": ["get available attritbutes from dataset you ve selected"], "bleu": 0.057259987315337726, "rouge_l": 0.0}
{"id": 1570, "code": "def get filters ( self , dataset ) : filters = self . filters ( dataset ) filt = [ ( k , v [ 0 ] ) for k , v in filters . items ( ) ] return pd . Data Frame ( filt , columns = [ \"Filter\" , \"Description\" ] )", "predictions": ["return the dataset exception for the given dataset . . ."], "references": ["get available filters from dataset you ve selected"], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 1571, "code": "def prepare outdir ( self ) : self . outdir = self . outdir if self . outdir is None : self . tmpdir = Temporary Directory ( ) self . outdir = self . tmpdir . name elif isinstance ( self . outdir , str ) : mkdirs ( self . outdir ) else : raise Exception ( \"Error parsing outdir: %s\" % type ( self . outdir ) ) if isinstance ( self . gene sets , str ) : gset = os . path . split ( self . gene sets ) [ - 1 ] . lower ( ) . rstrip ( \".gmt\" ) elif isinstance ( self . gene sets , dict ) : gset = \"blank name\" else : raise Exception ( \"Error parsing gene sets parameter for gene sets\" ) logfile = os . path . join ( self . outdir , \"gseapy.%s.%s.log\" % ( self . module , gset ) ) return logfile", "predictions": ["check for all gene sets and return"], "references": ["create temp directory ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1572, "code": "def set cores ( self ) : cpu num = cpu count ( ) - 1 if self . processes > cpu num : cores = cpu num elif self . processes < 1 : cores = 1 else : cores = self . processes self . processes = int ( cores )", "predictions": ["check the number of * * load * iterations * iterations * iterations * iterations *"], "references": ["set cpu numbers to be used"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 1573, "code": "def load gmt ( self , gene list , gmt ) : if isinstance ( gmt , dict ) : genesets dict = gmt elif isinstance ( gmt , str ) : genesets dict = self . parse gmt ( gmt ) else : raise Exception ( \"Error parsing gmt parameter for gene sets\" ) subsets = list ( genesets dict . keys ( ) ) self . n genesets = len ( subsets ) for subset in subsets : subset list = genesets dict . get ( subset ) if isinstance ( subset list , set ) : subset list = list ( subset list ) genesets dict [ subset ] = subset list tag indicator = np . in1d ( gene list , subset list , assume unique = True ) tag len = tag indicator . sum ( ) if self . min size <= tag len <= self . max size : continue del genesets dict [ subset ] filsets num = len ( subsets ) - len ( genesets dict ) self . logger . info ( \"%04d gene sets have been filtered out when max size=%s and min size=%s\" % ( filsets num , self . max size , self . min size ) ) if filsets num == len ( subsets ) : self . logger . error ( \"No gene sets passed through filtering condition!!!, try new parameters again!\\n\" + \"Note: check gene name, gmt file format, or filtering size.\" ) sys . exit ( 0 ) self . gmtdct = genesets dict return genesets dict", "predictions": ["bio from a result result"], "references": ["load gene set dict"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1574, "code": "def get libraries ( self , database = '' ) : lib url = 'http://amp.pharm.mssm.edu/%s Enrichr/dataset Statistics' % database libs json = json . loads ( requests . get ( lib url ) . text ) libs = [ lib [ 'library Name' ] for lib in libs json [ 'statistics' ] ] return sorted ( libs )", "predictions": ["print token list from the database"], "references": ["return active enrichr library name . offical api"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 1575, "code": "def download libraries ( self , libname ) : self . logger . info ( \"Downloading and generating Enrichr library gene sets......\" ) s = retry ( 5 ) ENRICHR URL = 'http://amp.pharm.mssm.edu/Enrichr/gene Set Library' query string = '?mode=text&library Name=%s' response = s . get ( ENRICHR URL + query string % libname , timeout = None ) if not response . ok : raise Exception ( 'Error fetching enrichment results, check internet connection first.' ) mkdirs ( DEFAULT CACHE PATH ) genesets dict = { } outname = \"enrichr.%s.gmt\" % libname gmtout = open ( os . path . join ( DEFAULT CACHE PATH , outname ) , \"w\" ) for line in response . iter lines ( chunk size = 1024 , decode unicode = 'utf-8' ) : line = line . strip ( ) k = line . split ( \"\\t\" ) [ 0 ] v = list ( map ( lambda x : x . split ( \",\" ) [ 0 ] , line . split ( \"\\t\" ) [ 2 : ] ) ) genesets dict . update ( { k : v } ) outline = \"%s\\t\\t%s\\n\" % ( k , \"\\t\" . join ( v ) ) gmtout . write ( outline ) gmtout . close ( ) return genesets dict", "predictions": ["get service from libname"], "references": ["download enrichr libraries ."], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 1576, "code": "def heatmat ( self , df , classes , pheno pos , pheno neg ) : width = len ( classes ) if len ( classes ) >= 6 else 5 cls boo A = list ( map ( lambda x : True if x == pheno pos else False , classes ) ) cls boo B = list ( map ( lambda x : True if x == pheno neg else False , classes ) ) dat A = df . loc [ : , cls boo A ] dat B = df . loc [ : , cls boo B ] dat AB = pd . concat ( [ dat A , dat B ] , axis = 1 ) self . width = width self . heatmat = dat AB return", "predictions": ["convert a dataframe into a dataframe"], "references": ["only use for gsea heatmap"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1577, "code": "def save results ( self , zipdata , outdir , module , gmt , rank metric , permutation type ) : res = Ordered Dict ( ) for gs , gseale , ind , RES in zipdata : rdict = Ordered Dict ( ) rdict [ 'es' ] = gseale [ 0 ] rdict [ 'nes' ] = gseale [ 1 ] rdict [ 'pval' ] = gseale [ 2 ] rdict [ 'fdr' ] = gseale [ 3 ] rdict [ 'geneset size' ] = len ( gmt [ gs ] ) rdict [ 'matched size' ] = len ( ind ) #reformat gene list. genes = rank metric . index . values [ ind ] rdict [ 'genes' ] = \";\" . join ( [ str ( g ) . strip ( ) for g in genes ] ) if self . module != 'ssgsea' : if rdict [ 'es' ] > 0 : idx = RES . argmax ( ) ldg pos = list ( filter ( lambda x : x <= idx , ind ) ) elif rdict [ 'es' ] < 0 : idx = RES . argmin ( ) ldg pos = list ( filter ( lambda x : x >= idx , ind ) ) else : ldg pos = ind rdict [ 'ledge genes' ] = ';' . join ( list ( map ( str , rank metric . iloc [ ldg pos ] . index ) ) ) rdict [ 'RES' ] = RES rdict [ 'hits indices' ] = ind res [ gs ] = rdict self . results = res res df = pd . Data Frame . from dict ( res , orient = 'index' ) res df . index . name = 'Term' res df . drop ( [ 'RES' , 'hits indices' ] , axis = 1 , inplace = True ) res df . sort values ( by = [ 'fdr' , 'pval' ] , inplace = True ) self . res2d = res df if self . outdir is None : return out = os . path . join ( outdir , 'gseapy.{b}.{c}.report.csv' . format ( b = module , c = permutation type ) ) if self . module == 'ssgsea' : out = out . replace ( \".csv\" , \".txt\" ) with open ( out , 'a' ) as f : f . write ( ) f . write ( ) res df . to csv ( f , sep = '\\t' ) else : res df . to csv ( out ) return", "predictions": ["get all comments in a csv"], "references": ["reformat gsea results and save to txt"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 1578, "code": "def load data ( self , cls vec ) : if isinstance ( self . data , pd . Data Frame ) : exprs = self . data . copy ( ) if exprs . index . dtype == 'O' : exprs = exprs . reset index ( ) elif os . path . isfile ( self . data ) : if self . data . endswith ( \"gct\" ) : exprs = pd . read csv ( self . data , skiprows = 1 , comment = '#' , sep = \"\\t\" ) else : exprs = pd . read csv ( self . data , comment = '#' , sep = \"\\t\" ) else : raise Exception ( 'Error parsing gene expression Data Frame!' ) #drop duplicated gene names if exprs . iloc [ : , 0 ] . duplicated ( ) . sum ( ) > 0 : self . logger . warning ( \"Warning: dropping duplicated gene names, only keep the first values\" ) exprs . drop duplicates ( subset = exprs . columns [ 0 ] , inplace = True ) #drop duplicate gene names. if exprs . isnull ( ) . any ( ) . sum ( ) > 0 : self . logger . warning ( \"Warning: Input data contains NA, filled NA with 0\" ) exprs . dropna ( how = 'all' , inplace = True ) #drop rows with all N As exprs = exprs . fillna ( 0 ) exprs . set index ( keys = exprs . columns [ 0 ] , inplace = True ) df = exprs . select dtypes ( include = [ np . number ] ) df std = df . groupby ( by = cls vec , axis = 1 ) . std ( ) df = df [ ~ df std . isin ( [ 0 ] ) . any ( axis = 1 ) ] df = df + 0.00001 return df", "predictions": ["get the issues from the issues"], "references": ["pre - processed the data frame . new filtering methods will be implement here ."], "bleu": 0.04928854007377984, "rouge_l": 0.08840579710144927}
{"id": 1579, "code": "def run Samples Permu ( self , df , gmt = None ) : assert self . min size <= self . max size mkdirs ( self . outdir ) self . results On Samples = Ordered Dict ( ) outdir = self . outdir for name , ser in df . iteritems ( ) : self . outdir = os . path . join ( outdir , str ( name ) ) self . logger . info ( \"Run Sample: %s \" % name ) mkdirs ( self . outdir ) dat2 = ser . sort values ( ascending = self . ascending ) gsea results , hit ind , rank ES , subsets = gsea compute ( data = dat2 , n = self . permutation num , gmt = gmt , weighted score type = self . weighted score type , permutation type = 'gene set' , method = None , pheno pos = '' , pheno neg = '' , classes = None , ascending = self . ascending , processes = self . processes , seed = self . seed , single = True , scale = self . scale ) res zip = zip ( subsets , list ( gsea results ) , hit ind , rank ES ) self . save results ( zipdata = res zip , outdir = self . outdir , module = self . module , gmt = gmt , rank metric = dat2 , permutation type = \"gene sets\" ) self . results On Samples [ name ] = self . res2d . es if self . noplot : continue self . logger . info ( \"Plotting Sample: %s \\n\" % name ) self . plotting ( rank metric = dat2 , results = self . results , graph num = self . graph num , outdir = self . outdir , figsize = self . figsize , format = self . format ) self . save ( outdir ) return", "predictions": ["api method for the url of the dataframe == the results == the results == == self"], "references": ["single sample gsea workflow with permutation procedure"], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 1580, "code": "def save ( self , outdir ) : samples Raw ES = pd . Data Frame ( self . results On Samples ) samples Raw ES . index . name = 'Term|ES' samples NES = samples Raw ES / ( samples Raw ES . values . max ( ) - samples Raw ES . values . min ( ) ) samples NES = samples NES . copy ( ) samples NES . index . rename ( 'Term|NES' , inplace = True ) self . res2d = samples NES self . logger . info ( \"Congratulations. GSE Apy runs successfully................\\n\" ) if self . outdir is None : return out E Sfile = os . path . join ( outdir , \"gseapy.samples.raw.es.txt\" ) with open ( out E Sfile , 'a' ) as f : if self . scale : f . write ( ) f . write ( + 'as indicated by Barbie et al., 2009, online methods, pg. 2\\n' ) else : f . write ( ) f . write ( ) samples Raw ES . to csv ( f , sep = '\\t' ) out NE Sfile = os . path . join ( outdir , \"gseapy.samples.normalized.es.txt\" ) with open ( out NE Sfile , 'a' ) as f : f . write ( ) f . write ( ) samples NES . to csv ( f , sep = '\\t' ) return", "predictions": ["getter for converting samples to csv"], "references": ["save es and stats"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1581, "code": "def prepare outdir ( self ) : self . outdir = self . outdir if self . outdir is None : self . tmpdir = Temporary Directory ( ) self . outdir = self . tmpdir . name elif isinstance ( self . outdir , str ) : mkdirs ( self . outdir ) else : raise Exception ( \"Error parsing outdir: %s\" % type ( self . outdir ) ) logfile = os . path . join ( self . outdir , \"gseapy.%s.%s.log\" % ( self . module , self . descriptions ) ) return logfile", "predictions": ["return the path to the logfile directory . ."], "references": ["create temp directory ."], "bleu": 0.18575057999133596, "rouge_l": 0.3306233062330623}
{"id": 1582, "code": "def parse genesets ( self ) : enrichr library = self . get libraries ( ) if isinstance ( self . gene sets , list ) : gss = self . gene sets elif isinstance ( self . gene sets , str ) : gss = [ g . strip ( ) for g in self . gene sets . strip ( ) . split ( \",\" ) ] elif isinstance ( self . gene sets , dict ) : gss = [ self . gene sets ] else : raise Exception ( \"Error parsing enrichr libraries, please provided corrected one\" ) gss exist = [ ] for g in gss : if isinstance ( g , dict ) : gss exist . append ( g ) continue if isinstance ( g , str ) : if g in enrichr library : gss exist . append ( g ) continue if g . lower ( ) . endswith ( \".gmt\" ) and os . path . exists ( g ) : self . logger . info ( \"User Defined gene sets is given: %s\" % g ) with open ( g ) as genesets : g dict = { line . strip ( ) . split ( \"\\t\" ) [ 0 ] : line . strip ( ) . split ( \"\\t\" ) [ 2 : ] for line in genesets . readlines ( ) } gss exist . append ( g dict ) return gss exist", "predictions": ["get the list of query client from the . txt file"], "references": ["parse gene_sets input file type"], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 1583, "code": "def send genes ( self , gene list , url ) : payload = { 'list' : ( None , gene list ) , 'description' : ( None , self . descriptions ) } response = requests . post ( url , files = payload ) if not response . ok : raise Exception ( 'Error analyzing gene list' ) sleep ( 1 ) job id = json . loads ( response . text ) return job id", "predictions": ["reqs a gene list"], "references": ["send gene list to enrichr server"], "bleu": 0.2868106410131918, "rouge_l": 0.3860759493670886}
{"id": 1584, "code": "def check genes ( self , gene list , usr list id ) : response = requests . get ( 'http://amp.pharm.mssm.edu/Enrichr/view?user List Id=%s' % usr list id ) if not response . ok : raise Exception ( 'Error getting gene list back' ) returned L = json . loads ( response . text ) [ \"genes\" ] returned N = sum ( [ 1 for gene in gene list if gene in returned L ] ) self . logger . info ( '{} genes successfully recognized by Enrichr' . format ( returned N ) )", "predictions": ["aggregate issues list of issues"], "references": ["compare the genes sent and received to get successfully recognized genes"], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 1585, "code": "def run ( self ) : self . get organism ( ) genes list = self . parse genelists ( ) gss = self . parse genesets ( ) self . logger . info ( \"Connecting to Enrichr Server to get latest library names\" ) if len ( gss ) < 1 : sys . stderr . write ( \"Not validated Enrichr library name provided\\n\" ) sys . stdout . write ( \"Hint: use get library name() to view full list of supported names\" ) sys . exit ( 1 ) self . results = pd . Data Frame ( ) for g in gss : if isinstance ( g , dict ) : res = self . enrich ( g ) short ID , self . gs = str ( id ( g ) ) , \"CUSTOM%s\" % id ( g ) if res is None : self . logger . info ( \"No hits return, for gene set: Custom%s\" % short ID ) continue else : self . gs = str ( g ) self . logger . debug ( \"Start Enrichr using library: %s\" % ( self . gs ) ) self . logger . info ( 'Analysis name: %s, Enrichr Library: %s' % ( self . descriptions , self . gs ) ) short ID , res = self . get results ( genes list ) res . insert ( 0 , \"Gene set\" , self . gs ) self . results = self . results . append ( res , ignore index = True , sort = True ) self . res2d = res if self . outdir is None : continue self . logger . info ( 'Save file of enrichment results: Job Id:' + str ( short ID ) ) outfile = \"%s/%s.%s.%s.reports.txt\" % ( self . outdir , self . gs , self . descriptions , self . module ) self . res2d . to csv ( outfile , index = False , encoding = 'utf-8' , sep = \"\\t\" ) if not self . no plot : msg = barplot ( df = res , cutoff = self . cutoff , figsize = self . figsize , top term = self . top term , color = 'salmon' , title = self . gs , ofname = outfile . replace ( \"txt\" , self . format ) ) if msg is not None : self . logger . warning ( msg ) self . logger . info ( 'Done.\\n' ) if self . outdir is None : self . tmpdir . cleanup ( ) return", "predictions": ["view the hits library"], "references": ["run enrichr for one sample gene list but multi - libraries"], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 1586, "code": "def annulus hires ( script , radius = None , radius1 = None , radius2 = None , diameter = None , diameter1 = None , diameter2 = None , cir segments = 48 , rad segments = 1 , color = None ) : if radius is not None and diameter is None : if radius1 is None and diameter1 is None : radius1 = radius if radius2 is None and diameter2 is None : radius2 = 0 if diameter is not None : if radius1 is None and diameter1 is None : radius1 = diameter / 2 if radius2 is None and diameter2 is None : radius2 = 0 if diameter1 is not None : radius1 = diameter1 / 2 if diameter2 is not None : radius2 = diameter2 / 2 if radius1 is None : radius1 = 1 if radius2 is None : radius2 = 0 ring = ( radius1 - radius2 ) / rad segments for i in range ( 0 , rad segments ) : annulus ( script , radius1 = radius1 - i * ring , radius2 = radius1 - ( i + 1 ) * ring , cir segments = cir segments ) layers . join ( script , merge vert = True ) if color is not None : vert color . function ( script , color = color ) return None", "predictions": ["return a cls cls cls"], "references": ["create a cylinder with user defined number of segments"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 1587, "code": "def tube hires ( script , height = 1.0 , radius = None , radius1 = None , radius2 = None , diameter = None , diameter1 = None , diameter2 = None , cir segments = 32 , rad segments = 1 , height segments = 1 , center = False , simple bottom = False , color = None ) : if radius is not None and diameter is None : if radius1 is None and diameter1 is None : radius1 = radius if radius2 is None and diameter2 is None : radius2 = 0 if diameter is not None : if radius1 is None and diameter1 is None : radius1 = diameter / 2 if radius2 is None and diameter2 is None : radius2 = 0 if diameter1 is not None : radius1 = diameter1 / 2 if diameter2 is not None : radius2 = diameter2 / 2 if radius1 is None : radius1 = 1 if radius2 is None : radius2 = 0 annulus hires ( script , radius1 = radius1 , radius2 = radius2 , cir segments = cir segments , rad segments = rad segments ) transform . translate ( script , [ 0 , 0 , height ] ) if simple bottom : annulus ( script , radius1 = radius1 , radius2 = radius2 , cir segments = cir segments ) else : layers . duplicate ( script ) transform . translate ( script , [ 0 , 0 , - height ] ) transform . rotate ( script , 'x' , 180 ) cylinder open hires ( script , height , radius1 , cir segments = cir segments , height segments = height segments ) if radius2 != 0 : cylinder open hires ( script , height , radius2 , cir segments = cir segments , height segments = height segments , invert normals = True ) layers . join ( script ) clean . merge vert ( script , threshold = 0.00002 ) if center : transform . translate ( script , [ 0 , 0 , - height / 2 ] ) if color is not None : vert color . function ( script , color = color ) return None", "predictions": ["hires a script script"], "references": ["create a cylinder with user defined number of segments"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 1588, "code": "def save to file ( self , script file ) : if not self . filters : print ( 'WARNING: no filters to save to file!' ) script file descriptor = open ( script file , 'w' ) script file descriptor . write ( '' . join ( self . opening + self . filters + self . closing ) ) script file descriptor . close ( )", "predictions": ["oracle the current p - type to a file"], "references": ["save filter script to an mlx file"], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 1589, "code": "def per triangle ( script , sidedim = 0 , textdim = 1024 , border = 2 , method = 1 ) : filter xml = '' . join ( [ '  <filter name=\"Parametrization: Trivial Per-Triangle \">\\n' , '    <Param name=\"sidedim\"' , 'value=\"%d\"' % sidedim , 'description=\"Quads per line\"' , 'type=\"Rich Int\"' , 'tooltip=\"Indicates how many triangles have to be put on each line (every quad contains two triangles). Leave 0 for automatic calculation\"' , '/>\\n' , '    <Param name=\"textdim\"' , 'value=\"%d\"' % textdim , 'description=\"Texture Dimension (px)\"' , 'type=\"Rich Int\"' , 'tooltip=\"Gives an indication on how big the texture is\"' , '/>\\n' , '    <Param name=\"border\"' , 'value=\"%d\"' % border , 'description=\"Inter-Triangle border (px)\"' , 'type=\"Rich Int\"' , 'tooltip=\"Specifies how many pixels to be left between triangles in parametrization domain\"' , '/>\\n' , '    <Param name=\"method\"' , 'value=\"%d\"' % method , 'description=\"Method\"' , 'enum val0=\"Basic\"' , 'enum val1=\"Space-optimizing\"' , 'enum cardinality=\"2\"' , 'type=\"Rich Enum\"' , 'tooltip=\"Choose space optimizing to map smaller faces into smaller triangles in parametrizazion domain\"' '/>\\n' , '  </filter>\\n' ] ) util . write filter ( script , filter xml ) return None", "predictions": ["write per per ."], "references": ["trivial per - triangle parameterization"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 1590, "code": "def v multiply ( scalar , v1 ) : vector = [ ] for i , x in enumerate ( v1 ) : vector . append ( '(({})*({}))' . format ( scalar , v1 [ i ] ) ) return vector", "predictions": ["returns the return return the return return the return value of the return value of the return value of the return the return as a return value of the return value"], "references": ["multiply vector by scalar"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1591, "code": "def measure all ( fbasename = None , log = None , ml version = ml version ) : ml script1 file = 'TEMP3D measure g And T.mlx' if ml version == '1.3.4BETA' : file out = 'TEMP3D aabb.xyz' else : file out = None ml script1 = mlx . Filter Script ( file in = fbasename , file out = file out , ml version = ml version ) compute . measure geometry ( ml script1 ) compute . measure topology ( ml script1 ) ml script1 . save to file ( ml script1 file ) ml script1 . run script ( log = log , script file = ml script1 file ) geometry = ml script1 . geometry topology = ml script1 . topology if ml version == '1.3.4BETA' : if log is not None : log file = open ( log , 'a' ) log file . write ( '***Axis Aligned Bounding Results for file \"%s\":\\n' % fbasename ) log file . close ( ) aabb = measure aabb ( file out , log ) else : aabb = geometry [ 'aabb' ] return aabb , geometry , topology", "predictions": ["measure all with ml ."], "references": ["measures mesh geometry aabb and topology ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1592, "code": "def measure dimension ( fbasename = None , log = None , axis1 = None , offset1 = 0.0 , axis2 = None , offset2 = 0.0 , ml version = ml version ) : axis1 = axis1 . lower ( ) axis2 = axis2 . lower ( ) ml script1 file = 'TEMP3D measure dimension.mlx' file out = 'TEMP3D measure dimension.xyz' ml script1 = mlx . Filter Script ( file in = fbasename , file out = file out , ml version = ml version ) compute . section ( ml script1 , axis1 , offset1 , surface = True ) compute . section ( ml script1 , axis2 , offset2 , surface = False ) layers . delete lower ( ml script1 ) ml script1 . save to file ( ml script1 file ) ml script1 . run script ( log = log , script file = ml script1 file ) for val in ( 'x' , 'y' , 'z' ) : if val not in ( axis1 , axis2 ) : axis = val axis num = ord ( axis ) - ord ( 'x' ) aabb = measure aabb ( file out , log ) dimension = { 'min' : aabb [ 'min' ] [ axis num ] , 'max' : aabb [ 'max' ] [ axis num ] , 'length' : aabb [ 'size' ] [ axis num ] , 'axis' : axis } if log is None : print ( '\\n For file \"%s\"' % fbasename ) print ( 'Dimension parallel to %s with %s=%s & %s=%s:' % ( axis , axis1 , offset1 , axis2 , offset2 ) ) print ( '  Min = %s, Max = %s, Total length = %s' % ( dimension [ 'min' ] , dimension [ 'max' ] , dimension [ 'length' ] ) ) else : log file = open ( log , 'a' ) log file . write ( '\\n For file \"%s\"\\n' % fbasename ) log file . write ( 'Dimension parallel to %s with %s=%s & %s=%s:\\n' % ( axis , axis1 , offset1 , axis2 , offset2 ) ) log file . write ( 'min = %s\\n' % dimension [ 'min' ] ) log file . write ( 'max = %s\\n' % dimension [ 'max' ] ) log file . write ( 'Total length = %s\\n' % dimension [ 'length' ] ) log file . close ( ) return dimension", "predictions": ["measure dimension . . . . ."], "references": ["measure a dimension of a mesh"], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 1593, "code": "def get vprof version ( filename ) : with open ( filename ) as src file : version match = re . search ( r\"^ version  = ['\\\"]([^'\\\"]*)['\\\"]\" , src file . read ( ) , re . M ) if version match : return version match . group ( 1 ) raise Runtime Error ( 'Unable to find version info.' )", "predictions": ["return version version version"], "references": ["returns actual version specified in filename ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 1594, "code": "def get obj count difference ( objs1 , objs2 ) : clean obj list1 = process in memory objects ( objs1 ) clean obj list2 = process in memory objects ( objs2 ) obj count 1 = get object count by type ( clean obj list1 ) obj count 2 = get object count by type ( clean obj list2 ) return obj count 1 - obj count 2", "predictions": ["argument parser for the number of objects ."], "references": ["returns count difference in two collections of python objects ."], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 1595, "code": "def format obj count ( objects ) : result = [ ] regex = re . compile ( r'<(?P<type>\\w+) \\'(?P<name>\\S+)\\'>' ) for obj type , obj count in objects . items ( ) : if obj count != 0 : match = re . findall ( regex , repr ( obj type ) ) if match : obj type , obj name = match [ 0 ] result . append ( ( \"%s %s\" % ( obj type , obj name ) , obj count ) ) return sorted ( result , key = operator . itemgetter ( 1 ) , reverse = True )", "predictions": ["add parser to formatted string ."], "references": ["formats object count ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 1596, "code": "def trace memory usage ( self , frame , event , arg ) : #pylint: disable=unused-argument if event == 'line' and frame . f code . co filename in self . target modules : self . events list . append ( ( frame . f lineno , self . process . memory info ( ) . rss , frame . f code . co name , frame . f code . co filename ) ) return self . trace memory usage", "predictions": ["add plot plot plot plot"], "references": ["checks memory usage when line event occur ."], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 1597, "code": "def code events ( self ) : if self . resulting events : return self . resulting events for i , ( lineno , mem , func , fname ) in enumerate ( self . events list ) : mem in mb = float ( mem - self . mem overhead ) / BYTES IN MB if ( self . resulting events and self . resulting events [ - 1 ] [ 0 ] == lineno and self . resulting events [ - 1 ] [ 2 ] == func and self . resulting events [ - 1 ] [ 3 ] == fname and self . resulting events [ - 1 ] [ 1 ] < mem in mb ) : self . resulting events [ - 1 ] [ 1 ] = mem in mb else : self . resulting events . append ( [ i + 1 , lineno , mem in mb , func , fname ] ) return self . resulting events", "predictions": [". add add add add add add add add add events to the event argparser argparser ."], "references": ["returns processed memory usage ."], "bleu": 0.07223943354597204, "rouge_l": 0.10082644628099173}
{"id": 1598, "code": "def compute mem overhead ( self ) : self . mem overhead = ( self . process . memory info ( ) . rss - builtins . initial rss size )", "predictions": ["add the mem parser to the mem"], "references": ["returns memory overhead ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1599, "code": "def profile package ( self ) : target modules = base profiler . get pkg module names ( self . run object ) try : with Code Events Tracker ( target modules ) as prof : prof . compute mem overhead ( ) runpy . run path ( self . run object , run name = ' main ' ) except System Exit : pass return prof , None", "predictions": ["compute the get marts marts"], "references": ["returns memory stats for a package ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 1600, "code": "def profile module ( self ) : target modules = { self . run object } try : with open ( self . run object , 'rb' ) as srcfile , Code Events Tracker ( target modules ) as prof : code = compile ( srcfile . read ( ) , self . run object , 'exec' ) prof . compute mem overhead ( ) exec ( code , self . globs , None ) except System Exit : pass return prof , None", "predictions": ["compute the profile module module ."], "references": ["returns memory stats for a module ."], "bleu": 0.24608524656663955, "rouge_l": 0.3034825870646766}
{"id": 1601, "code": "def profile function ( self ) : target modules = { self . run object . code . co filename } with Code Events Tracker ( target modules ) as prof : prof . compute mem overhead ( ) result = self . run object ( * self . run args , * * self . run kwargs ) return prof , result", "predictions": ["profile the profile function"], "references": ["returns memory stats for a function ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 1602, "code": "def run ( self ) : existing objects = get in memory objects ( ) prof , result = self . profile ( ) new objects = get in memory objects ( ) new obj count = get obj count difference ( new objects , existing objects ) result obj count = new obj count - prof . obj overhead result obj count [ list ] -= 1 pretty obj count = format obj count ( result obj count ) return { 'object Name' : self . object name , 'code Events' : prof . code events , 'total Events' : len ( prof . code events ) , 'objects Count' : pretty obj count , 'result' : result , 'timestamp' : int ( time . time ( ) ) }", "predictions": ["run the new objects ."], "references": ["collects memory stats for specified python program ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1603, "code": "def get run object type ( run object ) : if isinstance ( run object , tuple ) : return 'function' run object , , = run object . partition ( ' ' ) if os . path . isdir ( run object ) : return 'package' return 'module'", "predictions": ["returns the type of the run object ."], "references": ["determines run object type ."], "bleu": 0.239802967618271, "rouge_l": 0.48157894736842105}
{"id": 1604, "code": "def init module ( self , run object ) : self . profile = self . profile module self . run object , , self . run args = run object . partition ( ' ' ) self . object name = '%s (module)' % self . run object self . globs = { ' file ' : self . run object , ' name ' : ' main ' , ' package ' : None , } program path = os . path . dirname ( self . run object ) if sys . path [ 0 ] != program path : sys . path . insert ( 0 , program path ) self . replace sysargs ( )", "predictions": ["initializes the profile ."], "references": ["initializes profiler with a module ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 1605, "code": "def init package ( self , run object ) : self . profile = self . profile package self . run object , , self . run args = run object . partition ( ' ' ) self . object name = '%s (package)' % self . run object self . replace sysargs ( )", "predictions": ["initializes the package object ."], "references": ["initializes profiler with a package ."], "bleu": 0.2658156069371863, "rouge_l": 0.5366568914956013}
{"id": 1606, "code": "def init function ( self , run object ) : self . profile = self . profile function self . run object , self . run args , self . run kwargs = run object filename = inspect . getsourcefile ( self . run object ) self . object name = '%s @ %s (function)' % ( self . run object . name , filename )", "predictions": ["initializes the function object ."], "references": ["initializes profiler with a function ."], "bleu": 0.2658156069371863, "rouge_l": 0.5366568914956013}
{"id": 1607, "code": "def replace sysargs ( self ) : sys . argv [ : ] = [ self . run object ] if self . run args : sys . argv += self . run args . split ( )", "predictions": ["replace sysargs with sysargs args ."], "references": ["replaces sys . argv with proper args to pass to script ."], "bleu": 0.09663861439684919, "rouge_l": 0.31443298969072164}
{"id": 1608, "code": "def fill sample count ( self , node ) : node [ 'sample Count' ] += sum ( self . fill sample count ( child ) for child in node [ 'children' ] ) return node [ 'sample Count' ]", "predictions": ["fill the sample count of the sample count"], "references": ["counts and fills sample counts inside call tree ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 1609, "code": "def format tree ( self , node , total samples ) : funcname , filename , = node [ 'stack' ] sample percent = self . get percentage ( node [ 'sample Count' ] , total samples ) color hash = base profiler . hash name ( '%s @ %s' % ( funcname , filename ) ) return { 'stack' : node [ 'stack' ] , 'children' : [ self . format tree ( child , total samples ) for child in node [ 'children' ] ] , 'sample Count' : node [ 'sample Count' ] , 'sample Percentage' : sample percent , 'color Hash' : color hash }", "predictions": ["format a tree into a dictionary of the tree ."], "references": ["reformats call tree for the ui ."], "bleu": 0.14991106946711685, "rouge_l": 0.36454183266932266}
{"id": 1610, "code": "def call tree ( self ) : call tree = { 'stack' : 'base' , 'sample Count' : 0 , 'children' : [ ] } for stack , sample count in self . stats . items ( ) : self . insert stack ( reversed ( stack ) , sample count , call tree ) self . fill sample count ( call tree ) if not call tree [ 'children' ] : return { } return self . format tree ( call tree [ 'children' ] [ 0 ] , call tree [ 'sample Count' ] )", "predictions": ["call the tree in the current stack ."], "references": ["returns call tree ."], "bleu": 0.19070828081828378, "rouge_l": 0.5319767441860466}
{"id": 1611, "code": "def profile package ( self ) : with Stat Profiler ( ) as prof : prof . base frame = inspect . currentframe ( ) try : runpy . run path ( self . run object , run name = ' main ' ) except System Exit : pass call tree = prof . call tree return { 'object Name' : self . object name , 'sample Interval' : SAMPLE INTERVAL , 'run Time' : prof . run time , 'call Stats' : call tree , 'total Samples' : call tree . get ( 'sample Count' , 0 ) , 'timestamp' : int ( time . time ( ) ) }", "predictions": ["return the profile package information ."], "references": ["runs statistical profiler on a package ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 1612, "code": "def profile module ( self ) : with open ( self . run object , 'rb' ) as srcfile , Stat Profiler ( ) as prof : code = compile ( srcfile . read ( ) , self . run object , 'exec' ) prof . base frame = inspect . currentframe ( ) try : exec ( code , self . globs , None ) except System Exit : pass call tree = prof . call tree return { 'object Name' : self . object name , 'sample Interval' : SAMPLE INTERVAL , 'run Time' : prof . run time , 'call Stats' : call tree , 'total Samples' : call tree . get ( 'sample Count' , 0 ) , 'timestamp' : int ( time . time ( ) ) }", "predictions": ["return the profile module module ."], "references": ["runs statistical profiler on a module ."], "bleu": 0.24608524656663955, "rouge_l": 0.3034825870646766}
{"id": 1613, "code": "def profile function ( self ) : with Stat Profiler ( ) as prof : result = self . run object ( * self . run args , * * self . run kwargs ) call tree = prof . call tree return { 'object Name' : self . object name , 'sample Interval' : SAMPLE INTERVAL , 'run Time' : prof . run time , 'call Stats' : call tree , 'total Samples' : call tree . get ( 'sample Count' , 0 ) , 'result' : result , 'timestamp' : int ( time . time ( ) ) }", "predictions": ["profile a profile from the profile ."], "references": ["runs statistical profiler on a function ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 1614, "code": "def transform stats ( prof ) : records = [ ] for info , params in prof . stats . items ( ) : filename , lineno , funcname = info cum calls , num calls , time per call , cum time , = params if prof . total tt == 0 : percentage = 0 else : percentage = round ( 100 * ( cum time / prof . total tt ) , 4 ) cum time = round ( cum time , 4 ) func name = '%s @ %s' % ( funcname , filename ) color hash = base profiler . hash name ( func name ) records . append ( ( filename , lineno , funcname , cum time , percentage , num calls , cum calls , time per call , filename , color hash ) ) return sorted ( records , key = operator . itemgetter ( 4 ) , reverse = True )", "predictions": ["transform stats into a list of profiler stats ."], "references": ["processes collected stats for ui ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 1615, "code": "def profile package ( self ) : prof = c Profile . Profile ( ) prof . enable ( ) try : runpy . run path ( self . run object , run name = ' main ' ) except System Exit : pass prof . disable ( ) prof stats = pstats . Stats ( prof ) prof stats . calc callees ( ) return { 'object Name' : self . object name , 'call Stats' : self . transform stats ( prof stats ) , 'total Time' : prof stats . total tt , 'primitive Calls' : prof stats . prim calls , 'total Calls' : prof stats . total calls , 'timestamp' : int ( time . time ( ) ) }", "predictions": ["get the profile package package ."], "references": ["runs cprofile on a package ."], "bleu": 0.2907153684841096, "rouge_l": 0.3333333333333333}
{"id": 1616, "code": "def profile module ( self ) : prof = c Profile . Profile ( ) try : with open ( self . run object , 'rb' ) as srcfile : code = compile ( srcfile . read ( ) , self . run object , 'exec' ) prof . runctx ( code , self . globs , None ) except System Exit : pass prof stats = pstats . Stats ( prof ) prof stats . calc callees ( ) return { 'object Name' : self . object name , 'call Stats' : self . transform stats ( prof stats ) , 'total Time' : prof stats . total tt , 'primitive Calls' : prof stats . prim calls , 'total Calls' : prof stats . total calls , 'timestamp' : int ( time . time ( ) ) }", "predictions": ["return the profile module module ."], "references": ["runs cprofile on a module ."], "bleu": 0.2907153684841096, "rouge_l": 0.3333333333333333}
{"id": 1617, "code": "def profile function ( self ) : prof = c Profile . Profile ( ) prof . enable ( ) result = self . run object ( * self . run args , * * self . run kwargs ) prof . disable ( ) prof stats = pstats . Stats ( prof ) prof stats . calc callees ( ) return { 'object Name' : self . object name , 'call Stats' : self . transform stats ( prof stats ) , 'total Time' : prof stats . total tt , 'primitive Calls' : prof stats . prim calls , 'total Calls' : prof stats . total calls , 'result' : result , 'timestamp' : int ( time . time ( ) ) }", "predictions": ["get the profile function ."], "references": ["runs cprofile on a function ."], "bleu": 0.2941733261715515, "rouge_l": 0.3577712609970674}
{"id": 1618, "code": "def show guestbook ( ) : cursor = flask . g . db . execute ( 'SELECT name, message FROM entry ORDER BY id DESC;' ) entries = [ { 'name' : row [ 0 ] , 'message' : row [ 1 ] } for row in cursor . fetchall ( ) ] return jinja2 . Template ( LAYOUT ) . render ( entries = entries )", "predictions": ["show the list of guestbook entries ."], "references": ["returns all existing guestbook records ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 1619, "code": "def add entry ( ) : name , msg = flask . request . form [ 'name' ] , flask . request . form [ 'message' ] flask . g . db . execute ( 'INSERT INTO entry (name, message) VALUES (?, ?)' , ( name , msg ) ) flask . g . db . commit ( ) return flask . redirect ( '/' )", "predictions": ["add an entry to the flask entry ."], "references": ["adds single guestbook record ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 1620, "code": "def handle root ( ) : res filename = os . path . join ( os . path . dirname ( file ) , PROFILE HTML ) with io . open ( res filename , 'rb' ) as res file : content = res file . read ( ) return content , 'text/html'", "predictions": ["read the root root file ."], "references": ["handles index . html requests ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 1621, "code": "def handle other ( self ) : res filename = os . path . join ( os . path . dirname ( file ) , STATIC DIR , self . path [ 1 : ] ) with io . open ( res filename , 'rb' ) as res file : content = res file . read ( ) , extension = os . path . splitext ( self . path ) return content , 'text/%s' % extension [ 1 : ]", "predictions": ["handle the other command ."], "references": ["handles static files requests ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 1622, "code": "def do GET ( self ) : handler = self . uri map . get ( self . path ) or self . handle other content , content type = handler ( ) compressed content = gzip . compress ( content ) self . send response ( 200 , headers = ( ( 'Content-type' , '%s; charset=utf-8' % content type ) , ( 'Content-Encoding' , 'gzip' ) , ( 'Content-Length' , len ( compressed content ) ) ) ) self . wfile . write ( compressed content )", "predictions": ["handle the response ."], "references": ["handles http get requests ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 1623, "code": "def do POST ( self ) : post data = self . rfile . read ( int ( self . headers [ 'Content-Length' ] ) ) json data = gzip . decompress ( post data ) self . profile json . update ( json . loads ( json data . decode ( 'utf-8' ) ) ) self . send response ( 200 , headers = ( ( 'Content-type' , '%s; charset=utf-8' % 'text/json' ) , ( 'Content-Encoding' , 'gzip' ) , ( 'Content-Length' , len ( post data ) ) ) )", "predictions": ["requests to the profile from the service ."], "references": ["handles http post requests ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 1624, "code": "def send response ( self , http code , message = None , headers = None ) : self . send response ( http code , message ) if headers : for header in headers : self . send header ( * header ) self . end headers ( )", "predictions": ["send a response to the client ."], "references": ["sends http response code message and headers ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 1625, "code": "def check standard dir ( module path ) : if 'site-packages' in module path : return True for stdlib path in STDLIB PATHS : if fnmatch . fnmatchcase ( module path , stdlib path + '*' ) : return True return False", "predictions": ["check if the module is not a standard module"], "references": ["checks whether path belongs to standard library or installed modules ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 1626, "code": "def record line ( self , frame , event , arg ) : if event == 'line' : if self . prev timestamp : runtime = time . time ( ) - self . prev timestamp self . lines . append ( [ self . prev path , self . prev lineno , runtime ] ) self . prev lineno = frame . f lineno self . prev path = frame . f code . co filename self . prev timestamp = time . time ( ) return self . record line", "predictions": ["add a line to the record"], "references": ["records line execution time ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 1627, "code": "def lines without stdlib ( self ) : prev line = None current module path = inspect . getabsfile ( inspect . currentframe ( ) ) for module path , lineno , runtime in self . lines : module abspath = os . path . abspath ( module path ) if not prev line : prev line = [ module abspath , lineno , runtime ] else : if ( not check standard dir ( module path ) and module abspath != current module path ) : yield prev line prev line = [ module abspath , lineno , runtime ] else : prev line [ 2 ] += runtime yield prev line", "predictions": ["return list of lines without standard lines ."], "references": ["filters code from standard library from self . lines ."], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 1628, "code": "def fill heatmap ( self ) : for module path , lineno , runtime in self . lines without stdlib : self . execution count [ module path ] [ lineno ] += 1 self . heatmap [ module path ] [ lineno ] += runtime", "predictions": ["fill the heatmap with the heatmap heatmap ."], "references": ["fills code heatmap and execution count dictionaries ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 1629, "code": "def skip lines ( src code , skip map ) : if not skip map : return [ [ 'line' , j + 1 , l ] for j , l in enumerate ( src code ) ] code with skips , i = [ ] , 0 for line , length in skip map : code with skips . extend ( [ 'line' , i + j + 1 , l ] for j , l in enumerate ( src code [ i : line ] ) ) if ( code with skips and code with skips [ - 1 ] [ 0 ] == 'skip' ) : code with skips [ - 1 ] [ 1 ] += length else : code with skips . append ( [ 'skip' , length ] ) i = line + length code with skips . extend ( [ 'line' , i + j + 1 , l ] for j , l in enumerate ( src code [ i : ] ) ) return code with skips", "predictions": ["return the code code with skips skips ."], "references": ["skips lines in src_code specified by skip map ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 1630, "code": "def profile package ( self ) : with Code Heatmap Calculator ( ) as prof : try : runpy . run path ( self . run object , run name = ' main ' ) except System Exit : pass heatmaps = [ ] for filename , heatmap in prof . heatmap . items ( ) : if os . path . isfile ( filename ) : heatmaps . append ( self . format heatmap ( filename , heatmap , prof . execution count [ filename ] ) ) run time = sum ( heatmap [ 'run Time' ] for heatmap in heatmaps ) return { 'object Name' : self . run object , 'run Time' : run time , 'heatmaps' : heatmaps }", "predictions": ["return the profile package information ."], "references": ["calculates heatmap for package ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 1631, "code": "def format heatmap ( self , filename , heatmap , execution count ) : with open ( filename ) as src file : file source = src file . read ( ) . split ( '\\n' ) skip map = self . calc skips ( heatmap , len ( file source ) ) run time = sum ( time for time in heatmap . values ( ) ) return { 'name' : filename , 'heatmap' : heatmap , 'execution Count' : execution count , 'src Code' : self . skip lines ( file source , skip map ) , 'run Time' : run time }", "predictions": ["format the heatmap from the heatmap ."], "references": ["formats heatmap for ui ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 1632, "code": "def profile module ( self ) : with open ( self . run object , 'r' ) as srcfile : src code = srcfile . read ( ) code = compile ( src code , self . run object , 'exec' ) try : with Code Heatmap Calculator ( ) as prof : exec ( code , self . globs , None ) except System Exit : pass heatmaps = [ ] for filename , heatmap in prof . heatmap . items ( ) : if os . path . isfile ( filename ) : heatmaps . append ( self . format heatmap ( filename , heatmap , prof . execution count [ filename ] ) ) run time = sum ( heatmap [ 'run Time' ] for heatmap in heatmaps ) return { 'object Name' : self . run object , 'run Time' : run time , 'heatmaps' : heatmaps }", "predictions": ["returns the profile module module = 1 if it is not a string"], "references": ["calculates heatmap for module ."], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 1633, "code": "def profile function ( self ) : with Code Heatmap Calculator ( ) as prof : result = self . run object ( * self . run args , * * self . run kwargs ) code lines , start line = inspect . getsourcelines ( self . run object ) source lines = [ ] for line in code lines : source lines . append ( ( 'line' , start line , line ) ) start line += 1 filename = os . path . abspath ( inspect . getsourcefile ( self . run object ) ) heatmap = prof . heatmap [ filename ] run time = sum ( time for time in heatmap . values ( ) ) return { 'object Name' : self . object name , 'run Time' : run time , 'result' : result , 'timestamp' : int ( time . time ( ) ) , 'heatmaps' : [ { 'name' : self . object name , 'heatmap' : heatmap , 'execution Count' : prof . execution count [ filename ] , 'src Code' : source lines , 'run Time' : run time } ] }", "predictions": ["profile the profile function co co co co co co co co co co co ."], "references": ["calculates heatmap for function ."], "bleu": 0.08513012360883544, "rouge_l": 0.21034482758620687}
{"id": 1634, "code": "def count vocab ( self , analyzed docs ) : vocabulary = self . vocabulary j indices = make int array ( ) indptr = make int array ( ) indptr . append ( 0 ) for doc in analyzed docs : for feature in doc : try : j indices . append ( vocabulary [ feature ] ) except Key Error : continue indptr . append ( len ( j indices ) ) j indices = frombuffer empty ( j indices , dtype = np . intc ) indptr = np . frombuffer ( indptr , dtype = np . intc ) values = np . ones ( len ( j indices ) ) X = sp . csr matrix ( ( values , j indices , indptr ) , shape = ( len ( indptr ) - 1 , len ( vocabulary ) ) , dtype = self . dtype ) X . sum duplicates ( ) if self . binary : X . data . fill ( 1 ) return X", "predictions": ["run the number of vocab"], "references": ["create sparse feature matrix and vocabulary where fixed_vocab = false"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 1635, "code": "def to scikit ( self ) : scaler = Standard Scaler ( with mean = self . with mean , with std = self . with std , copy = self . copy ) scaler . dict = self . dict return scaler", "predictions": ["return a run representation of this object ."], "references": ["convert to equivalent standardscaler"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1636, "code": "def fit ( self , Z , parameter iterable ) : self . scorer = check scoring ( self . estimator , scoring = self . scoring ) cv = self . cv cv = check cv ( cv , Z ) if self . verbose > 0 : if isinstance ( parameter iterable , Sized ) : n candidates = len ( parameter iterable ) print ( \"Fitting {0} folds for each of {1} candidates, totalling\" \" {2} fits\" . format ( len ( cv ) , n candidates , n candidates * len ( cv ) ) ) base estimator = clone ( self . estimator ) pre dispatch = self . pre dispatch out = Parallel ( n jobs = self . n jobs , verbose = self . verbose , pre dispatch = pre dispatch , backend = \"threading\" ) ( delayed ( fit and score ) ( clone ( base estimator ) , Z , self . scorer , train , test , self . verbose , parameters , self . fit params , return parameters = True , error score = self . error score ) for parameters in parameter iterable for train , test in cv ) n fits = len ( out ) n folds = len ( cv ) scores = list ( ) grid scores = list ( ) for grid start in range ( 0 , n fits , n folds ) : n test samples = 0 score = 0 all scores = [ ] for this score , this n test samples , , parameters in out [ grid start : grid start + n folds ] : all scores . append ( this score ) if self . iid : this score *= this n test samples n test samples += this n test samples score += this score if self . iid : score /= float ( n test samples ) else : score /= float ( n folds ) scores . append ( ( score , parameters ) ) grid scores . append ( CV Score Tuple ( parameters , score , np . array ( all scores ) ) ) self . grid scores = grid scores best = sorted ( grid scores , key = lambda x : x . mean validation score , reverse = True ) [ 0 ] self . best params = best . parameters self . best score = best . mean validation score if self . refit : best estimator = clone ( base estimator ) . set params ( * * best . parameters ) best estimator . fit ( Z , * * self . fit params ) self . best estimator = best estimator return self", "predictions": ["init the model with the given partition"], "references": ["actual fitting performing the search over parameters ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1637, "code": "def score ( estimator , Z test , scorer ) : score = scorer ( estimator , Z test ) if not isinstance ( score , numbers . Number ) : raise Value Error ( \"scoring must return a number, got %s (%s) instead.\" % ( str ( score ) , type ( score ) ) ) return score", "predictions": ["get the init init init init init"], "references": ["compute the score of an estimator on a given test set ."], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 1638, "code": "def block collection ( iterator , dtype , bsize = - 1 ) : i = 0 accumulated = [ ] for a in iterator : if ( bsize > 0 ) and ( i >= bsize ) : yield pack accumulated ( accumulated , dtype ) accumulated = [ ] i = 0 accumulated . append ( a ) i += 1 if i > 0 : yield pack accumulated ( accumulated , dtype )", "predictions": ["a generator for filename of filename ."], "references": ["pack rdd with a specific collection constructor ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 1639, "code": "def block tuple ( iterator , dtypes , bsize = - 1 ) : i = 0 blocked tuple = None for tuple i in iterator : if blocked tuple is None : blocked tuple = tuple ( [ ] for in range ( len ( tuple i ) ) ) if ( bsize > 0 ) and ( i >= bsize ) : yield tuple ( pack accumulated ( x , dtype ) for x , dtype in zip ( blocked tuple , dtypes ) ) blocked tuple = tuple ( [ ] for in range ( len ( tuple i ) ) ) i = 0 for x j , x in zip ( tuple i , blocked tuple ) : x . append ( x j ) i += 1 if i > 0 : yield tuple ( pack accumulated ( x , dtype ) for x , dtype in zip ( blocked tuple , dtypes ) )", "predictions": ["pack - tuple - separated replace of dtypes ."], "references": ["pack rdd of tuples as tuples of arrays or scipy . sparse matrices ."], "bleu": 0.09630141125179911, "rouge_l": 0.2510288065843621}
{"id": 1640, "code": "def shape ( self ) : first = self . first ( ) . shape shape = self . rdd . map ( lambda x : x . shape [ 0 ] ) . sum ( ) return ( shape , ) + first [ 1 : ]", "predictions": ["the fill fill fill of this rdd . . . . . . . . ."], "references": ["returns the shape of the data ."], "bleu": 0.09147827112247602, "rouge_l": 0.28067484662576686}
{"id": 1641, "code": "def toarray ( self ) : rdd = self . rdd . map ( lambda x : x . toarray ( ) ) return np . concatenate ( rdd . collect ( ) )", "predictions": ["filename of an node"], "references": ["returns the data as numpy . array from each partition ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 1642, "code": "def convert ( self , txn ) : ofxid = self . mk ofxid ( txn . id ) metadata = { } posting metadata = { \"ofxid\" : ofxid } if isinstance ( txn , Ofx Transaction ) : posting = Posting ( self . name , Amount ( txn . amount , self . currency ) , metadata = posting metadata ) return Transaction ( date = txn . date , payee = self . format payee ( txn ) , postings = [ posting , posting . clone inverted ( self . mk dynamic account ( self . format payee ( txn ) , exclude = self . name ) ) ] ) elif isinstance ( txn , Investment Transaction ) : acct1 = self . name acct2 = self . name posting1 = None posting2 = None security = self . maybe get ticker ( txn . security ) if isinstance ( txn . type , str ) : if re . match ( '^(buy|sell)' , txn . type ) : acct2 = self . unknownaccount or 'Assets:Unknown' elif txn . type == 'transfer' : acct2 = 'Transfer' elif txn . type == 'reinvest' : acct2 = 'Income:Interest' elif txn . type == 'income' and txn . income type == 'DIV' : metadata [ 'dividend from' ] = security acct2 = 'Income:Dividends' posting1 = Posting ( acct1 , Amount ( txn . total , self . currency ) , metadata = posting metadata ) posting2 = posting1 . clone inverted ( acct2 ) else : pass else : if ( txn . type in [ 0 , 1 , 3 , 4 ] ) : acct2 = self . unknownaccount or 'Assets:Unknown' elif ( txn . type == 2 ) : acct2 = 'Income:Interest' else : pass aux date = None if txn . settle Date is not None and txn . settle Date != txn . trade Date : aux date = txn . settle Date if posting1 is None and posting2 is None : posting1 = Posting ( acct1 , Amount ( txn . units , security , unlimited = True ) , unit price = Amount ( txn . unit price , self . currency , unlimited = True ) , metadata = posting metadata ) posting2 = Posting ( acct2 , Amount ( txn . units * txn . unit price , self . currency , reverse = True ) ) else : pass return Transaction ( date = txn . trade Date , aux date = aux date , payee = self . format payee ( txn ) , metadata = metadata , postings = [ posting1 , posting2 ] )", "predictions": ["call this to call the transaction"], "references": ["convert an ofx transaction to a posting"], "bleu": 0.20693220168471366, "rouge_l": 0.1517412935323383}
{"id": 1643, "code": "def compatibility ( session , install ) : session . install ( '-e' , '.[dev]' ) session . install ( install ) run tests ( session )", "predictions": ["install - . ."], "references": ["run the unit test suite with each support library and python version ."], "bleu": 0.037870374782798366, "rouge_l": 0.1073943661971831}
{"id": 1644, "code": "def text width ( self , text : str ) -> float : width , = self . font . getsize ( text ) return width", "predictions": ["return the profile module ."], "references": ["returns the width in pixels of a string in dejavu sans 110pt ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 1645, "code": "def text width ( self , text : str ) -> float : width = 0 for index , c in enumerate ( text ) : width += self . char to width . get ( c , self . default character width ) width -= self . pair to kern . get ( text [ index : index + 2 ] , 0 ) return width", "predictions": ["return the profile function . . ."], "references": ["returns the width in pixels of a string in dejavu sans 110pt ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 1646, "code": "def default ( cls ) -> 'Precalculated Text Measurer' : if cls . default cache is not None : return cls . default cache if pkg resources . resource exists ( name , 'default-widths.json.xz' ) : import lzma with pkg resources . resource stream ( name , 'default-widths.json.xz' ) as f : with lzma . open ( f , \"rt\" ) as g : cls . default cache = Precalculated Text Measurer . from json ( cast ( Text IO , g ) ) return cls . default cache elif pkg resources . resource exists ( name , 'default-widths.json' ) : with pkg resources . resource stream ( name , 'default-widths.json' ) as f : cls . default cache = Precalculated Text Measurer . from json ( io . Text IO Wrapper ( f , encoding = 'utf-8' ) ) return cls . default cache else : raise Value Error ( 'could not load default-widths.json' )", "predictions": [". transform instance from info for the current info for the package for the current user for the current info for the info for the info for the info for the"], "references": ["returns a reasonable default precalculatedtextmeasurer ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 1647, "code": "def generate supported characters ( deja vu sans path : str ) -> Iterable [ str ] : font = tt Lib . TT Font ( deja vu sans path ) for cmap in font [ 'cmap' ] . tables : if cmap . is Unicode ( ) : for code in cmap . cmap : yield chr ( code )", "predictions": ["profile package characters characters"], "references": ["generate the characters support by the font at the given path ."], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 1648, "code": "def write json ( f : Text IO , deja vu sans path : str , measurer : text measurer . Text Measurer , encodings : Iterable [ str ] ) -> None : supported characters = list ( generate supported characters ( deja vu sans path ) ) kerning characters = '' . join ( generate encodeable characters ( supported characters , encodings ) ) char to length = calculate character to length mapping ( measurer , supported characters ) pair to kerning = calculate pair to kern mapping ( measurer , char to length , kerning characters ) json . dump ( { 'mean-character-length' : statistics . mean ( char to length . values ( ) ) , 'character-lengths' : char to length , 'kerning-characters' : kerning characters , 'kerning-pairs' : pair to kerning } , f , sort keys = True , indent = 1 )", "predictions": ["profile module mapping from module self object object object object object object object object object object object object object object object"], "references": ["write the data required by precalculatedtextmeasurer to a stream ."], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 1649, "code": "def convolve gaussian 2d ( image , gaussian kernel 1d ) : result = scipy . ndimage . filters . correlate1d ( image , gaussian kernel 1d , axis = 0 ) result = scipy . ndimage . filters . correlate1d ( result , gaussian kernel 1d , axis = 1 ) return result", "predictions": ["stats for a function that returns a function that is used in the function of the function run run run run run run run run run run run run run run"], "references": ["convolve 2d gaussian ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1650, "code": "def get gaussian kernel ( gaussian kernel width = 11 , gaussian kernel sigma = 1.5 ) : gaussian kernel 1d = numpy . ndarray ( ( gaussian kernel width ) ) norm mu = int ( gaussian kernel width / 2 ) for i in range ( gaussian kernel width ) : gaussian kernel 1d [ i ] = ( exp ( - ( ( ( i - norm mu ) ** 2 ) ) / ( 2 * ( gaussian kernel sigma ** 2 ) ) ) ) return gaussian kernel 1d / numpy . sum ( gaussian kernel 1d )", "predictions": ["returns the gaussian kernel kernel of a gaussian"], "references": ["generate a gaussian kernel ."], "bleu": 0.25098621243978964, "rouge_l": 0.32105263157894737}
{"id": 1651, "code": "def main ( ) : description = '\\n' . join ( [ 'Compares an image with a list of images using the SSIM metric.' , '  Example:' , '    pyssim test-images/test1-1.png \"test-images/*\"' ] ) parser = argparse . Argument Parser ( prog = 'pyssim' , formatter class = argparse . Raw Text Help Formatter , description = description ) parser . add argument ( '--cw' , help = 'compute the complex wavelet SSIM' , action = 'store true' ) parser . add argument ( 'base image' , metavar = 'image1.png' , type = argparse . File Type ( 'r' ) ) parser . add argument ( 'comparison images' , metavar = 'image path with* or image2.png' ) parser . add argument ( '--width' , type = int , default = None , help = 'scales the image before computing SSIM' ) parser . add argument ( '--height' , type = int , default = None , help = 'scales the image before computing SSIM' ) args = parser . parse args ( ) if args . width and args . height : size = ( args . width , args . height ) else : size = None if not args . cw : gaussian kernel sigma = 1.5 gaussian kernel width = 11 gaussian kernel 1d = get gaussian kernel ( gaussian kernel width , gaussian kernel sigma ) comparison images = glob . glob ( args . comparison images ) is a single image = len ( comparison images ) == 1 for comparison image in comparison images : if args . cw : ssim = SSIM ( args . base image . name , size = size ) ssim value = ssim . cw ssim value ( comparison image ) else : ssim = SSIM ( args . base image . name , gaussian kernel 1d , size = size ) ssim value = ssim . ssim value ( comparison image ) if is a single image : sys . stdout . write ( '%.7g' % ssim value ) else : sys . stdout . write ( '%s - %s: %.7g' % ( args . base image . name , comparison image , ssim value ) ) sys . stdout . write ( '\\n' )", "predictions": ["entry point for the gaussian cli msg"], "references": ["main function for pyssim ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1652, "code": "def destroy ( self ) : if self . conf . auto Tick : self . destroying = True else : self . do Destroy ( )", "predictions": ["handle the tray action filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename filename"], "references": ["correctly destroy syncobj . stop autotickthread close connections etc ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1653, "code": "def get Status ( self ) : status = { } status [ 'version' ] = VERSION status [ 'revision' ] = REVISION status [ 'self' ] = self . self Node status [ 'state' ] = self . raft State status [ 'leader' ] = self . raft Leader status [ 'partner nodes count' ] = len ( self . other Nodes ) for node in self . other Nodes : status [ 'partner node status server ' + node . id ] = 2 if node in self . connected Nodes else 0 status [ 'readonly nodes count' ] = len ( self . readonly Nodes ) for node in self . readonly Nodes : status [ 'readonly node status server ' + node . id ] = 2 if node in self . connected Nodes else 0 status [ 'log len' ] = len ( self . raft Log ) status [ 'last applied' ] = self . raft Last Applied status [ 'commit idx' ] = self . raft Commit Index status [ 'raft term' ] = self . raft Current Term status [ 'next node idx count' ] = len ( self . raft Next Index ) for node , idx in iteritems ( self . raft Next Index ) : status [ 'next node idx server ' + node . id ] = idx status [ 'match idx count' ] = len ( self . raft Match Index ) for node , idx in iteritems ( self . raft Match Index ) : status [ 'match idx server ' + node . id ] = idx status [ 'leader commit idx' ] = self . leader Commit Index status [ 'uptime' ] = int ( time . time ( ) - self . start Time ) status [ 'self code version' ] = self . self Code Version status [ 'enabled code version' ] = self . enabled Code Version return status", "predictions": ["handle the res res res res res ."], "references": ["dumps different debug info about cluster to dict and return it"], "bleu": 0.09268172804333874, "rouge_l": 0.0}
{"id": 1654, "code": "def print Status ( self ) : status = self . get Status ( ) for k , v in iteritems ( status ) : logging . info ( '%s: %s' % ( str ( k ) , str ( v ) ) )", "predictions": ["do the actual do not print out"], "references": ["dumps different debug info about cluster to default logger"], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 1655, "code": "def check ( func ) : def wrapped ( * args , * * kwargs ) : check name = func . name arg name = None if args : arg name = args [ 0 ] try : if arg name : logger . debug ( \"Checking '%s' for '%s'\" , check name , arg name ) else : logger . debug ( \"Checking '%s'\" , check name ) response = func ( * args , * * kwargs ) except Exception as e : message = str ( e ) response = { \"ok\" : False , \"error\" : message , \"stacktrace\" : traceback . format exc ( ) , } if arg name : response = { arg name : response } logger . exception ( \"Error calling '%s' for '%s': %s\" , check name , arg name , message ) else : logger . exception ( \"Error calling '%s': %s\" , check name , message ) return response return wrapped", "predictions": ["do a decorator do nothing do not do nothing int"], "references": ["decorator which wraps checks and returns an error response on failure ."], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 1656, "code": "def str to list ( s ) : list = s . split ( \",\" ) return list ( map ( lambda i : i . lstrip ( ) , list ) )", "predictions": ["convert string to list"], "references": ["converts a comma separated string to a list"], "bleu": 0.18693159143202892, "rouge_l": 0.47164948453608246}
{"id": 1657, "code": "def cli parse ( file path , sa , nameservers , dns timeout , parallel = False ) : try : file results = parse report file ( file path , nameservers = nameservers , dns timeout = dns timeout , strip attachment payloads = sa , parallel = parallel ) except Parser Error as error : return error , file path finally : global counter with counter . get lock ( ) : counter . value += 1 return file results , file path", "predictions": ["standard standard standard standard standard standard standard standard standard standard standard library ."], "references": ["separated this function for multiprocessing"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 1658, "code": "def publish ( self , subject , reply , payload , payload size ) : if subject == \"\" : raise Err Bad Subject payload size bytes = ( \"%d\" % payload size ) . encode ( ) pub cmd = b'' . join ( [ PUB OP , SPC , subject . encode ( ) , SPC , reply , SPC , payload size bytes , CRLF , payload , CRLF ] ) self . stats [ 'out msgs' ] += 1 self . stats [ 'out bytes' ] += payload size yield from self . send command ( pub cmd ) if self . flush queue . empty ( ) : yield from self . flush pending ( )", "predictions": ["record a message to the server append it to the server append it to the server append ."], "references": ["sends pub command to the nats server ."], "bleu": 0.101824256461955, "rouge_l": 0.3306233062330623}
{"id": 1659, "code": "def process pong ( self ) : if len ( self . pongs ) > 0 : future = self . pongs . pop ( 0 ) future . set result ( True ) self . pongs received += 1 self . pings outstanding -= 1", "predictions": ["lines of the without blocking the without the without the without the without the without the without the without the without the without the without the without the without the without"], "references": ["process pong sent by server ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1660, "code": "def process msg ( self , sid , subject , reply , data ) : payload size = len ( data ) self . stats [ 'in msgs' ] += 1 self . stats [ 'in bytes' ] += payload size sub = self . subs . get ( sid ) if sub is None : return sub . received += 1 if sub . max msgs > 0 and sub . received >= sub . max msgs : self . subs . pop ( sid , None ) msg = self . build message ( subject , reply , data ) if sub . future is not None : if sub . future . cancelled ( ) : return sub . future . set result ( msg ) return try : sub . pending size += payload size if sub . pending size >= sub . pending bytes limit : sub . pending size -= payload size if self . error cb is not None : yield from self . error cb ( Err Slow Consumer ( subject = subject , sid = sid ) ) return sub . pending queue . put nowait ( msg ) except asyncio . Queue Full : if self . error cb is not None : yield from self . error cb ( Err Slow Consumer ( subject = subject , sid = sid ) )", "predictions": ["fill the message from the queue"], "references": ["process msg sent by server ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1661, "code": "def load features from array ( self , features ) : self . feature images = np . load ( features ) self . feature names = range ( self . feature images . shape [ 1 ] )", "predictions": ["skip lines from an src file"], "references": ["load feature data from a 2d ndarray on disk ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 1662, "code": "def dot product ( self , imgs to decode ) : return np . dot ( imgs to decode . T , self . feature images ) . T", "predictions": ["profile a profile in a profile try to a numpy array"], "references": ["decoding using the dot product ."], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 1663, "code": "def feature selection ( feat select , X , y ) : if re . match ( '.*-best' , feat select ) is not None : n = int ( feat select . split ( '-' ) [ 0 ] ) selector = Select K Best ( k = n ) import warnings with warnings . catch warnings ( ) : warnings . simplefilter ( 'ignore' , category = User Warning ) features selected = np . where ( selector . fit ( X , y ) . get support ( ) is True ) [ 0 ] elif re . match ( '.*-randombest' , feat select ) is not None : n = int ( feat select . split ( '-' ) [ 0 ] ) from random import shuffle features = range ( 0 , X . shape [ 1 ] ) shuffle ( features ) features selected = features [ : n ] return features selected", "predictions": ["shuffle format for format format"], "references": ["implements various kinds of feature selection"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 1664, "code": "def fit ( self , X , y , cv = None , class weight = 'auto' ) : self . X = X self . y = y self . set class weight ( class weight = class weight , y = y ) self . clf = self . clf . fit ( X , y ) return self . clf", "predictions": ["fit a model to the given statement"], "references": ["fits x to outcomes y using clf"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 1665, "code": "def set class weight ( self , class weight = 'auto' , y = None ) : if class weight is None : cw = None try : self . clf . set params ( class weight = cw ) except Value Error : pass elif class weight == 'auto' : c = np . bincount ( y ) ii = np . nonzero ( c ) [ 0 ] c = c / float ( c . sum ( ) ) cw = dict ( zip ( ii [ : : - 1 ] , c [ ii ] ) ) try : self . clf . set params ( class weight = cw ) except Value Error : import warnings warnings . warn ( \"Tried to set class weight, but failed. The classifier \" \"probably doesn't support it\" )", "predictions": ["set the class weight weight weight of the given class ."], "references": ["sets the class_weight of the classifier to match y"], "bleu": 0.16108992769687397, "rouge_l": 0.3055091819699499}
{"id": 1666, "code": "def cross val fit ( self , X , y , cross val = '4-Fold' , scoring = 'accuracy' , feat select = None , class weight = 'auto' ) : from sklearn import cross validation self . X = X self . y = y self . set class weight ( class weight = class weight , y = y ) if isinstance ( cross val , string types ) : if re . match ( '.*-Fold' , cross val ) is not None : n = int ( cross val . split ( '-' ) [ 0 ] ) self . cver = cross validation . Stratified K Fold ( self . y , n ) else : raise Exception ( 'Unrecognized cross validation method' ) else : self . cver = cross val if feat select is not None : self . features selected = [ ] from sklearn . grid search import Grid Search CV if isinstance ( self . clf , Grid Search CV ) : import warnings if feat select is not None : warnings . warn ( \"Cross-validated feature selection not supported with \" \"Grid Search CV\" ) self . clf . set params ( cv = self . cver , scoring = scoring ) with warnings . catch warnings ( ) : warnings . simplefilter ( 'ignore' , category = User Warning ) self . clf = self . clf . fit ( X , y ) self . cvs = self . clf . best score else : self . cvs = self . feat select cvs ( feat select = feat select , scoring = scoring ) if feat select is not None : fs = feature selection ( feat select , X , y ) self . features selected . append ( fs ) X = X [ : , fs ] self . clf . fit ( X , y ) return self . cvs . mean ( )", "predictions": ["fit a cross - validation feature using the cross - validation algorithm"], "references": ["fits x to outcomes y using clf and cv_method"], "bleu": 0.10390302174233558, "rouge_l": 0.09775641025641024}
{"id": 1667, "code": "def fit dataset ( self , dataset , y , features = None , feature type = 'features' ) : if feature type == 'features' : X = np . rot90 ( dataset . feature table . data . toarray ( ) ) elif feature type == 'voxels' : X = np . rot90 ( dataset . image table . data . toarray ( ) ) self . sk classifier . fit ( X , y )", "predictions": ["fit a dataset to the given type"], "references": ["given a dataset fits either features or voxels to y"], "bleu": 0.18094495256969623, "rouge_l": 0.34205607476635513}
{"id": 1668, "code": "def get top words ( model , feature names , n top words = 40 ) : topic words = [ ] for topic in model . components : top words = [ feature names [ i ] for i in topic . argsort ( ) [ : - n top words - 1 : - 1 ] ] topic words += [ top words ] return topic words", "predictions": ["returns the top words of all top words in a model ."], "references": ["return top forty words from each topic in trained topic model ."], "bleu": 0.16261701715194898, "rouge_l": 0.4166666666666667}
{"id": 1669, "code": "def pearson ( x , y ) : data = np . vstack ( ( x , y ) ) ms = data . mean ( axis = 1 ) [ ( slice ( None , None , None ) , None ) ] datam = data - ms datass = np . sqrt ( np . sum ( datam ** 2 , axis = 1 ) ) temp = np . dot ( datam [ 1 : ] , datam [ 0 ] . T ) rs = temp / ( datass [ 1 : ] * datass [ 0 ] ) return rs", "predictions": ["compute pearson - pearson pearson at x y"], "references": ["correlates row vector x with each row vector in 2d array y ."], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 1670, "code": "def load ( cls , filename ) : try : dataset = pickle . load ( open ( filename , 'rb' ) ) except Unicode Decode Error : dataset = pickle . load ( open ( filename , 'rb' ) , encoding = 'latin' ) if hasattr ( dataset , 'feature table' ) : dataset . feature table . csr to sdf ( ) return dataset", "predictions": ["load and return a dataset from disk ."], "references": ["load a pickled dataset instance from file ."], "bleu": 0.21105340631872638, "rouge_l": 0.625}
{"id": 1671, "code": "def save ( self , filename ) : if hasattr ( self , 'feature table' ) : self . feature table . sdf to csr ( ) pickle . dump ( self , open ( filename , 'wb' ) , - 1 ) if hasattr ( self , 'feature table' ) : self . feature table . csr to sdf ( )", "predictions": ["save this instance to a file"], "references": ["pickle the dataset instance to the provided file ."], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 1672, "code": "def get ids by expression ( self , expression , threshold = 0.001 , func = np . sum ) : lexer = lp . Lexer ( ) lexer . build ( ) parser = lp . Parser ( lexer , self . dataset , threshold = threshold , func = func ) parser . build ( ) return parser . parse ( expression ) . keys ( ) . values", "predictions": ["return the ids of the given expression ."], "references": ["use a peg to parse expression and return study ids ."], "bleu": 0.13859150907108325, "rouge_l": 0.3070469798657718}
{"id": 1673, "code": "def sdf to csr ( self ) : data = self . data . to dense ( ) self . data = { 'columns' : list ( data . columns ) , 'index' : list ( data . index ) , 'values' : sparse . csr matrix ( data . values ) }", "predictions": ["sdf sdf to csr"], "references": ["convert featuretable to scipy csr matrix ."], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 1674, "code": "def xyz to mat ( foci , xyz dims = None , mat dims = None ) : foci = np . hstack ( ( foci , np . ones ( ( foci . shape [ 0 ] , 1 ) ) ) ) mat = np . array ( [ [ - 0.5 , 0 , 0 , 45 ] , [ 0 , 0.5 , 0 , 63 ] , [ 0 , 0 , 0.5 , 36 ] ] ) . T result = np . dot ( foci , mat ) [ : , : : - 1 ] return np . round ( result ) . astype ( int )", "predictions": ["xyz xyz to mat"], "references": ["convert an n x 3 array of xyz coordinates to matrix indices ."], "bleu": 0.041910459064397936, "rouge_l": 0.2147887323943662}
{"id": 1675, "code": "def save img ( data , filename , masker , header = None ) : if not header : header = masker . get header ( ) header . set data dtype ( data . dtype ) header [ 'cal max' ] = data . max ( ) header [ 'cal min' ] = data . min ( ) img = nifti1 . Nifti1Image ( masker . unmask ( data ) , None , header ) img . to filename ( filename )", "predictions": ["save data to a nifti1 file"], "references": ["save a vectorized image to file ."], "bleu": 0.23512037509993022, "rouge_l": 0.45522388059701485}
{"id": 1676, "code": "def dict to object ( item , object name ) : fields = item . keys ( ) values = item . values ( ) return json . loads ( json . dumps ( item ) , object hook = lambda d : namedtuple ( object name , fields ) ( * values ) )", "predictions": ["convert an object to a json string ."], "references": ["converts a python dict to a namedtuple saving memory ."], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 1677, "code": "async def get bearer info ( self ) : if self . client id is None : raise Spotify Exception ( GET BEARER ERR % 'client id' ) elif self . client secret is None : raise Spotify Exception ( GET BEARER ERR % 'client secret' ) token = b64encode ( ':' . join ( ( self . client id , self . client secret ) ) . encode ( ) ) kwargs = { 'url' : 'https://accounts.spotify.com/api/token' , 'data' : { 'grant type' : 'client credentials' } , 'headers' : { 'Authorization' : 'Basic ' + token . decode ( ) } } async with self . session . post ( * * kwargs ) as resp : return json . loads ( await resp . text ( encoding = 'utf-8' ) )", "predictions": ["get bearer info from bearer client ."], "references": ["get the application bearer token from client_id and client_secret ."], "bleu": 0.15215596197411094, "rouge_l": 0.45607476635514016}
{"id": 1678, "code": "def assert hasattr ( attr : str , msg : str , tp : Base Exception = Spotify Exception ) -> Callable : def decorator ( func : Callable ) -> Callable : @ functools . wraps ( func ) def decorated ( self , * args , * * kwargs ) : if not hasattr ( self , attr ) : raise tp ( msg ) return func ( self , * args , * * kwargs ) if inspect . iscoroutinefunction ( func ) : @ functools . wraps ( func ) async def decorated ( * args , * * kwargs ) : return await decorated ( * args , * * kwargs ) return decorated return decorator", "predictions": ["assert that the function is not a iscoroutinefunction method ."], "references": ["decorator to assert an object has an attribute when run ."], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 1679, "code": "def from client ( cls , client , * args , * * kwargs ) : return cls ( client . http . client id , * args , * * kwargs )", "predictions": ["create an instance from a client ."], "references": ["construct a oauth2 object from a spotify . client ."], "bleu": 0.20024850746991507, "rouge_l": 0.45607476635514016}
{"id": 1680, "code": "def url ( client id : str , redirect uri : str , * , scope : str = None , state : str = None , secure : bool = True ) -> str : attrs = { 'client id' : client id , 'redirect uri' : quote ( redirect uri ) } if scope is not None : attrs [ 'scope' ] = quote ( scope ) if state is not None : attrs [ 'state' ] = state parameters = '&' . join ( '{0}={1}' . format ( * item ) for item in attrs . items ( ) ) return O Auth2 . BASE . format ( parameters = parameters )", "predictions": ["build the url for a client ."], "references": ["construct a oauth2 url instead of an oauth2 object ."], "bleu": 0.14390022429682173, "rouge_l": 0.22803738317757008}
{"id": 1681, "code": "def attrs ( self ) : data = { 'client id' : self . client id , 'redirect uri' : quote ( self . redirect uri ) , } if self . scope is not None : data [ 'scope' ] = quote ( self . scope ) if self . state is not None : data [ 'state' ] = self . state return data", "predictions": ["returns the attributes of the resource ."], "references": ["attributes used when constructing url parameters ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 1682, "code": "def parameters ( self ) -> str : return '&' . join ( '{0}={1}' . format ( * item ) for item in self . attrs . items ( ) )", "predictions": ["return the parameters as a string ."], "references": ["url parameters used ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 1683, "code": "async def from href ( self ) : if not hasattr ( self , 'href' ) : raise Type Error ( 'Spotify object has no `href` attribute, therefore cannot be retrived' ) elif hasattr ( self , 'http' ) : return await self . http . request ( ( 'GET' , self . href ) ) else : cls = type ( self ) try : client = getattr ( self , ' {0} client' . format ( cls . name ) ) except Attribute Error : raise Type Error ( 'Spotify object has no way to access a HTTP Client.' ) else : http = client . http data = await http . request ( ( 'GET' , self . href ) ) return cls ( client , data )", "predictions": ["return an object from the href server ."], "references": ["get the full object from spotify with a href attribute ."], "bleu": 0.17250013293422076, "rouge_l": 0.4093959731543625}
{"id": 1684, "code": "def update code urls ( self ) : to ignore = [ \".gitignore\" , \".keep\" ] for root , , files in Py Funceble . walk ( Py Funceble . CURRENT DIRECTORY + Py Funceble . directory separator + \"Py Funceble\" + Py Funceble . directory separator ) : for file in files : if file not in to ignore and \" pycache \" not in root : if root . endswith ( Py Funceble . directory separator ) : self . update docs ( root + file ) else : self . update docs ( root + Py Funceble . directory separator + file ) for root , , files in Py Funceble . walk ( Py Funceble . CURRENT DIRECTORY + Py Funceble . directory separator + \"tests\" + Py Funceble . directory separator ) : for file in files : if file not in to ignore and \" pycache \" not in root : if root . endswith ( Py Funceble . directory separator ) : self . update docs ( root + file ) else : self . update docs ( root + Py Funceble . directory separator + file )", "predictions": ["update all files urls"], "references": ["read the code and update all links ."], "bleu": 0.1739594473063345, "rouge_l": 0.31443298969072164}
{"id": 1685, "code": "def is version greater ( self ) : checked = Version ( True ) . check versions ( self . current version [ 0 ] , self . version yaml ) if checked is not None and not checked : return True return False", "predictions": ["return true if the version is running"], "references": ["check if the current version is greater as the older older one ."], "bleu": 0.13044969897820202, "rouge_l": 0.3794712286158632}
{"id": 1686, "code": "def is dev version ( cls ) : command = \"git branch\" command result = Command ( command ) . execute ( ) for branch in command result . split ( \"\\n\" ) : if branch . startswith ( \"*\" ) and \"dev\" in branch : return True return False", "predictions": ["returns true if the command is dev false otherwise ."], "references": ["check if the current branch is dev ."], "bleu": 0.21834177214239062, "rouge_l": 0.5669144981412639}
{"id": 1687, "code": "def does require deprecation ( self ) : for index , version number in enumerate ( self . current version [ 0 ] [ : 2 ] ) : if version number > self . version yaml [ index ] : return True return False", "predictions": ["check if the current version is require"], "references": ["check if we have to put the previous version into the deprecated list ."], "bleu": 0.10218289380194193, "rouge_l": 0.35935198821796754}
{"id": 1688, "code": "def backup ( self ) : if Py Funceble . CONFIGURATION [ \"auto continue\" ] : data to backup = { } configuration counter = Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] data to backup [ Py Funceble . INTERN [ \"file to test\" ] ] = { \"tested\" : configuration counter [ \"tested\" ] , \"up\" : configuration counter [ \"up\" ] , \"down\" : configuration counter [ \"down\" ] , \"invalid\" : configuration counter [ \"invalid\" ] , } to save = { } to save . update ( self . backup content ) to save . update ( data to backup ) Dict ( to save ) . to json ( self . autocontinue log file )", "predictions": ["backup the current configuration to the autocontinue"], "references": ["backup the current execution state ."], "bleu": 0.345720784641941, "rouge_l": 0.4680306905370844}
{"id": 1689, "code": "def restore ( self ) : if Py Funceble . CONFIGURATION [ \"auto continue\" ] and self . backup content : file to restore = Py Funceble . INTERN [ \"file to test\" ] if file to restore in self . backup content : to initiate = [ \"up\" , \"down\" , \"invalid\" , \"tested\" ] alternatives = { \"up\" : \"number of up\" , \"down\" : \"number of down\" , \"invalid\" : \"number of invalid\" , \"tested\" : \"number of tested\" , } for string in to initiate : try : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] . update ( { string : self . backup content [ file to restore ] [ string ] } ) except Key Error : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] . update ( { string : self . backup content [ file to restore ] [ alternatives [ string ] ] } )", "predictions": ["restore the backup back to disk"], "references": ["restore data from the given path ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 1690, "code": "def stay safe ( ) : random = int ( choice ( str ( int ( time ( ) ) ) ) ) if not CONFIGURATION [ \"quiet\" ] and random % 3 == 0 : print ( \"\\n\" + Fore . GREEN + Style . BRIGHT + \"Thanks for using Py Funceble!\" ) print ( Fore . YELLOW + Style . BRIGHT + \"Share your experience on \" + Fore . CYAN + \"Twitter\" + Fore . YELLOW + \" with \" + Fore . CYAN + \"#Py Funceble\" + Fore . YELLOW + \"!\" ) print ( Fore . GREEN + Style . BRIGHT + \"Have a feedback, an issue or an improvement idea ?\" ) print ( Fore . YELLOW + Style . BRIGHT + \"Let us know on \" + Fore . CYAN + \"Git Hub\" + Fore . YELLOW + \"!\" )", "predictions": ["print a improvement object"], "references": ["print a friendly message ."], "bleu": 0.36827215283744186, "rouge_l": 0.43571428571428567}
{"id": 1691, "code": "def entry management url ( self ) : if ( self . url file and not self . entry management url download ( self . url file ) ) : Py Funceble . INTERN [ \"file to test\" ] = self . url file", "predictions": ["download entry management url ."], "references": ["manage the loading of the url system ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 1692, "code": "def print header ( cls ) : if ( not Py Funceble . CONFIGURATION [ \"quiet\" ] and not Py Funceble . CONFIGURATION [ \"header printed\" ] ) : print ( \"\\n\" ) if Py Funceble . CONFIGURATION [ \"less\" ] : Prints ( None , \"Less\" ) . header ( ) else : Prints ( None , \"Generic\" ) . header ( ) Py Funceble . CONFIGURATION [ \"header printed\" ] = True", "predictions": ["print the header of the header"], "references": ["decide if we print or not the header ."], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 1693, "code": "def handle ( self ) : source = \"URL\" if self . catched . lower ( ) not in Py Funceble . STATUS [ \"list\" ] [ \"invalid\" ] : Generate ( self . catched , source ) . status file ( ) else : Generate ( self . catched , \"SYNTAX\" ) . status file ( ) return self . catched", "predictions": ["handle incoming packet ."], "references": ["handle the backend of the given status ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 1694, "code": "def delete uneeded ( self ) : structure = self . get structure ( ) list of key = list ( structure . keys ( ) ) structure = structure [ list of key [ 0 ] ] parent path = list of key [ 0 ] if not parent path . endswith ( Py Funceble . directory separator ) : parent path += Py Funceble . directory separator for root , , in Py Funceble . walk ( parent path ) : root = Directory ( root ) . fix path ( ) if root . replace ( parent path , \"\" ) not in structure : Py Funceble . rmtree ( root )", "predictions": ["delete all uneeded of the structure ."], "references": ["delete the directory which are not registered into our structure ."], "bleu": 0.15685718045401453, "rouge_l": 0.4273204903677758}
{"id": 1695, "code": "def load config file ( self ) : try : Py Funceble . CONFIGURATION . update ( Dict . from yaml ( File ( self . path to config ) . read ( ) ) ) self . install iana config ( ) self . install psl config ( ) self . install directory structure file ( ) except File Not Found Error as exception : if Py Funceble . path . isfile ( self . path to default config ) : File ( self . path to default config ) . copy ( self . path to config ) self . load config file ( ) else : raise exception", "predictions": ["load the config from the default config file"], "references": ["load . pyfunceble . yaml into the system ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 1696, "code": "def install iana config ( cls ) : iana link = Py Funceble . CONFIGURATION [ \"links\" ] [ \"iana\" ] iana link = Version ( True ) . right url from version ( iana link ) destination = Py Funceble . CURRENT DIRECTORY + \"iana-domains-db.json\" if not Version ( True ) . is cloned ( ) or not Py Funceble . path . isfile ( destination ) : return Download ( iana link , destination ) . text ( ) return None", "predictions": ["fit iana to iana"], "references": ["download iana - domains - db . json if not present ."], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 1697, "code": "def install psl config ( cls ) : psl link = Py Funceble . CONFIGURATION [ \"links\" ] [ \"psl\" ] psl link = Version ( True ) . right url from version ( psl link ) destination = ( Py Funceble . CURRENT DIRECTORY + Py Funceble . CONFIGURATION [ \"outputs\" ] [ \"default files\" ] [ \"public suffix\" ] ) if not Version ( True ) . is cloned ( ) or not Py Funceble . path . isfile ( destination ) : return Download ( psl link , destination ) . text ( ) return None", "predictions": ["set up the class weight weight . . ."], "references": ["download public - suffix . json if not present ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 1698, "code": "def install directory structure file ( cls ) : dir structure link = Py Funceble . CONFIGURATION [ \"links\" ] [ \"dir structure\" ] dir structure link = Version ( True ) . right url from version ( dir structure link ) destination = ( Py Funceble . CURRENT DIRECTORY + Py Funceble . CONFIGURATION [ \"outputs\" ] [ \"default files\" ] [ \"dir structure\" ] ) if not Version ( True ) . is cloned ( ) or not Py Funceble . path . isfile ( destination ) : data = Download ( dir structure link , destination , return data = True ) . text ( ) File ( destination ) . write ( data , overwrite = True ) return True return None", "predictions": ["cross val fit val fit into the if it exists select it ."], "references": ["download the latest version of dir_structure_production . json ."], "bleu": 0.10571070857151538, "rouge_l": 0.18798151001540828}
{"id": 1699, "code": "def merge values ( self ) : to remove = [ ] self . new config = Dict ( Dict ( self . upstream config ) . merge ( Py Funceble . CONFIGURATION ) ) . remove key ( to remove )", "predictions": ["fit the == config to the == config"], "references": ["simply merge the older into the new one ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 1700, "code": "def load ( self ) : if \"PYFUNCEBLE AUTO CONFIGURATION\" not in Py Funceble . environ : while True : response = input ( Py Funceble . Style . BRIGHT + Py Funceble . Fore . RED + \"A configuration key is missing.\\n\" + Py Funceble . Fore . RESET + \"Try to merge upstream configuration file into %s ? [y/n] \" % ( Py Funceble . Style . BRIGHT + self . path to config + Py Funceble . Style . RESET ALL ) ) if isinstance ( response , str ) : if response . lower ( ) == \"y\" : self . merge values ( ) self . save ( ) print ( Py Funceble . Style . BRIGHT + Py Funceble . Fore . GREEN + \"Done!\\n\" \"Please try again, if it happens again,\" \" please fill a new issue.\" ) break elif response . lower ( ) == \"n\" : raise Exception ( \"Configuration key still missing.\" ) else : self . merge values ( ) self . save ( )", "predictions": ["get the upstream configuration configuration"], "references": ["execute the logic behind the merging ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1701, "code": "def handle non existant index ( cls ) : try : Py Funceble . INTERN [ \"http code\" ] except Key Error : Py Funceble . INTERN [ \"http code\" ] = \"*\" * 3 try : Py Funceble . INTERN [ \"referer\" ] except Key Error : Py Funceble . INTERN [ \"referer\" ] = \"Unknown\"", "predictions": ["pearson for non ."], "references": ["handle and check that some configuration index exists ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 1702, "code": "def status file ( self ) : if \"file to test\" in Py Funceble . INTERN : Generate ( self . domain status , self . source , self . expiration date ) . info files ( ) Percentage ( self . domain status ) . count ( ) self . prints status screen ( ) if self . do not produce file ( ) : return None if ( not Py Funceble . CONFIGURATION [ \"no files\" ] and Py Funceble . CONFIGURATION [ \"split\" ] ) : self . prints status file ( ) else : self . unified file ( )", "predictions": ["return load the unified load file file"], "references": ["generate a file according to the domain status ."], "bleu": 0.15447878876032708, "rouge_l": 0.12224448897795591}
{"id": 1703, "code": "def load ( self ) : if not Py Funceble . INTERN [ \"psl db\" ] : Py Funceble . INTERN [ \"psl db\" ] = Dict ( ) . from json ( File ( self . destination ) . read ( ) )", "predictions": ["save the values from the destination"], "references": ["load the public suffix database into the system ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 1704, "code": "def load ( self ) : if \"iana db\" not in Py Funceble . INTERN or not Py Funceble . INTERN [ \"iana db\" ] : Py Funceble . INTERN [ \"iana db\" ] = self . iana db", "predictions": ["get the build build build build build build build build build ."], "references": ["initiate the iana database if it is not the case ."], "bleu": 0.11498759556447223, "rouge_l": 0.17528735632183906}
{"id": 1705, "code": "def update ( self ) : if not Py Funceble . CONFIGURATION [ \"quiet\" ] : print ( \"Update of iana-domains-db\" , end = \" \" ) for extension , referer in self . extensions ( ) : if extension not in self . iana db or self . iana db [ extension ] != referer : self . iana db [ extension ] = referer Dict ( self . iana db ) . to json ( self . destination ) if not Py Funceble . CONFIGURATION [ \"quiet\" ] : print ( Py Funceble . INTERN [ \"done\" ] )", "predictions": ["sdf sdf for the database"], "references": ["update the content of the iana - domains - db file ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 1706, "code": "def retrieve ( self ) : if Py Funceble . CONFIGURATION [ \"mining\" ] : if \"mined\" not in Py Funceble . INTERN : Py Funceble . INTERN [ \"mined\" ] = { } if Py Funceble . path . isfile ( self . file ) : data = Dict ( ) . from json ( File ( self . file ) . read ( ) ) for file path in data : Py Funceble . INTERN [ \"mined\" ] [ file path ] = { } for element in data [ file path ] : if data [ file path ] [ element ] : Py Funceble . INTERN [ \"mined\" ] [ file path ] [ element ] = data [ file path ] [ element ] return Py Funceble . INTERN [ \"mined\" ] = { } return", "predictions": ["xyz method to xyz xyz from disk"], "references": ["retrieve the mining informations ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1707, "code": "def backup ( self ) : if Py Funceble . CONFIGURATION [ \"mining\" ] : Dict ( Py Funceble . INTERN [ \"mined\" ] ) . to json ( self . file )", "predictions": ["save the current file to the dtype"], "references": ["backup the mined informations ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1708, "code": "def process ( self ) : if Py Funceble . CONFIGURATION [ \"mining\" ] : mined = self . mine ( ) if mined : self . add ( mined ) self . backup ( )", "predictions": ["dict the dumps ."], "references": ["process the logic and structuration of the mining database ."], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 1709, "code": "def json print ( self ) : if self . output : if Py Funceble . path . isfile ( self . output ) : content = Dict ( ) . from json ( File ( self . output ) . read ( ) ) if isinstance ( content , list ) : content . extend ( self . data to print ) content = List ( content ) . custom format ( Sort . standard ) if Py Funceble . CONFIGURATION [ \"hierarchical sorting\" ] : content = List ( content ) . custom format ( Sort . hierarchical ) Dict ( content ) . to json ( self . output ) else : raise Exception ( \"Output not correctly formatted.\" ) else : # Dict ( self . data to print ) . to json ( self . output ) else : raise Exception ( \"Empty output given.\" )", "predictions": ["get the if def is not none"], "references": ["management of the json template ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 1710, "code": "def file to delete ( cls ) : directory = Py Funceble . OUTPUT DIRECTORY + Py Funceble . OUTPUTS [ \"parent directory\" ] if not directory . endswith ( Py Funceble . directory separator ) : directory += Py Funceble . directory separator result = [ ] for root , , files in Py Funceble . walk ( directory ) : for file in files : if file not in [ \".gitignore\" , \".keep\" ] : if root . endswith ( Py Funceble . directory separator ) : result . append ( root + file ) else : result . append ( root + Py Funceble . directory separator + file ) return result", "predictions": ["converts a assert assert to a list of all the raise error if any ."], "references": ["return the list of file to delete ."], "bleu": 0.12874330508144843, "rouge_l": 0.2760180995475113}
{"id": 1711, "code": "def databases to delete ( cls ) : directory = Py Funceble . CURRENT DIRECTORY result = [ ] result . append ( directory + Py Funceble . CONFIGURATION [ \"outputs\" ] [ \"default files\" ] [ \"dir structure\" ] ) result . append ( directory + Py Funceble . CONFIGURATION [ \"outputs\" ] [ \"default files\" ] [ \"iana\" ] ) result . append ( directory + Py Funceble . CONFIGURATION [ \"outputs\" ] [ \"default files\" ] [ \"public suffix\" ] ) result . append ( directory + Py Funceble . CONFIGURATION [ \"outputs\" ] [ \"default files\" ] [ \"inactive db\" ] ) result . append ( directory + Py Funceble . CONFIGURATION [ \"outputs\" ] [ \"default files\" ] [ \"mining\" ] ) result . append ( directory + Py Funceble . CONFIGURATION [ \"outputs\" ] [ \"default files\" ] [ \"whois db\" ] ) return result", "predictions": ["removes from from from from from from from from from from kwargs ."], "references": ["set the databases files to delete ."], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 1712, "code": "def get ( self ) : result = { } if self . algorithm in self . valid algorithms : if self . algorithm == \"all\" : del self . valid algorithms [ 0 ] for algo in self . valid algorithms : if self . path and path . isfile ( self . path ) : result [ algo ] = self . hash file ( algo ) elif self . data : result [ algo ] = self . hash data ( algo ) else : return None else : if self . path and path . isfile ( self . path ) : result [ self . algorithm ] = self . hash file ( self . algorithm ) elif self . data : result [ self . algorithm ] = self . hash data ( self . algorithm ) else : return None else : return None if self . algorithm != \"all\" and self . only hash : return result [ self . algorithm ] return result", "predictions": ["returns a dict of } = value from the = value"], "references": ["return the hash of the given file"], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 1713, "code": "def count ( self ) : if self . status : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] += 1 if ( self . status . lower ( ) in Py Funceble . STATUS [ \"list\" ] [ \"up\" ] or self . status . lower ( ) in Py Funceble . STATUS [ \"list\" ] [ \"valid\" ] ) : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"up\" ] += 1 elif self . status . lower ( ) in Py Funceble . STATUS [ \"list\" ] [ \"down\" ] : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"down\" ] += 1 else : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"invalid\" ] += 1", "predictions": ["attrs the number of rows"], "references": ["count the number of domain for each status ."], "bleu": 0.22831876136235013, "rouge_l": 0.40757238307349664}
{"id": 1714, "code": "def calculate ( cls ) : percentages = { \"up\" : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"up\" ] , \"down\" : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"down\" ] , \"invalid\" : Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"invalid\" ] , } for percentage in percentages : calculation = ( percentages [ percentage ] * 100 // Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] ) Py Funceble . INTERN [ \"counter\" ] [ \"percentage\" ] . update ( { percentage : calculation } )", "predictions": ["parameters are in the percentage"], "references": ["calculate the percentage of each status ."], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 1715, "code": "def log ( self ) : if ( Py Funceble . CONFIGURATION [ \"show percentage\" ] and Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] > 0 ) : output = ( Py Funceble . OUTPUT DIRECTORY + Py Funceble . OUTPUTS [ \"parent directory\" ] + Py Funceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"parent\" ] + Py Funceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"percentage\" ] + Py Funceble . OUTPUTS [ \"logs\" ] [ \"filenames\" ] [ \"percentage\" ] ) File ( output ) . delete ( ) self . calculate ( ) if not Py Funceble . CONFIGURATION [ \"quiet\" ] : print ( \"\\n\" ) Prints ( None , \"Percentage\" , output ) . header ( ) lines to print = [ [ Py Funceble . STATUS [ \"official\" ] [ \"up\" ] , str ( Py Funceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"up\" ] ) + \"%\" , Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"up\" ] , ] , [ Py Funceble . STATUS [ \"official\" ] [ \"down\" ] , str ( Py Funceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"down\" ] ) + \"%\" , Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"down\" ] , ] , [ Py Funceble . STATUS [ \"official\" ] [ \"invalid\" ] , str ( Py Funceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"invalid\" ] ) + \"%\" , Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"invalid\" ] , ] , ] if Py Funceble . CONFIGURATION [ \"syntax\" ] : lines to print [ 0 ] [ 0 ] = Py Funceble . STATUS [ \"official\" ] [ \"valid\" ] del lines to print [ 1 ] for to print in lines to print : Prints ( to print , \"Percentage\" , output ) . data ( ) elif Py Funceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] > 0 : self . calculate ( )", "predictions": ["def the screen to the screen"], "references": ["print on screen and on file the percentages for each status ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 1716, "code": "def reformat historical formating error ( self ) : if Py Funceble . CONFIGURATION [ \"inactive database\" ] : historical formating error = ( Py Funceble . CURRENT DIRECTORY + \"inactive-db.json\" ) if Py Funceble . path . isfile ( historical formating error ) : data = Dict ( ) . from json ( File ( historical formating error ) . read ( ) ) data to parse = { } top keys = data . keys ( ) for top key in top keys : low keys = data [ top key ] . keys ( ) data to parse [ top key ] = { } for low key in low keys : if low key . isdigit ( ) : data to parse [ top key ] [ int ( low key ) - ( self . one day in seconds * 30 ) ] = data [ top key ] [ low key ] else : data to parse [ top key ] [ int ( Py Funceble . time ( ) ) - ( self . one day in seconds * 30 ) ] = data [ top key ] [ low key ] if \"inactive db\" in Py Funceble . INTERN : Py Funceble . INTERN [ \"inactive db\" ] . update ( data to parse ) else : Py Funceble . INTERN [ \"inactive db\" ] = data to parse File ( historical formating error ) . delete ( )", "predictions": ["extract urls from the code urls ."], "references": ["format the old format so it can be merged into the newer format ."], "bleu": 0.07562380261607851, "rouge_l": 0.17967599410898377}
{"id": 1717, "code": "def retrieve ( self ) : if Py Funceble . CONFIGURATION [ \"inactive database\" ] : self . reformat historical formating error ( ) if Py Funceble . path . isfile ( self . inactive db path ) : self . merge ( )", "predictions": ["is called when a if not exists = true ."], "references": ["return the current content of the inactive - db . json file ."], "bleu": 0.0933873085201553, "rouge_l": 0.084958217270195}
{"id": 1718, "code": "def backup ( self ) : if Py Funceble . CONFIGURATION [ \"inactive database\" ] : Dict ( Py Funceble . INTERN [ \"inactive db\" ] ) . to json ( self . inactive db path )", "predictions": ["is the inactive inactive to ."], "references": ["save the current database into the inactive - db . json file ."], "bleu": 0.09728049676725326, "rouge_l": 0.29611650485436897}
{"id": 1719, "code": "def is present ( cls ) : if Py Funceble . CONFIGURATION [ \"inactive database\" ] : if Py Funceble . INTERN [ \"to test\" ] in Py Funceble . INTERN [ \"flatten inactive db\" ] or ( Py Funceble . INTERN [ \"file to test\" ] in Py Funceble . INTERN [ \"inactive db\" ] and Py Funceble . INTERN [ \"inactive db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] and \"to test\" in Py Funceble . INTERN [ \"inactive db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] and Py Funceble . INTERN [ \"to test\" ] in Py Funceble . INTERN [ \"inactive db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] [ \"to test\" ] ) : return True return False", "predictions": ["check if the user is require require require"], "references": ["check if the currently tested element is into the database ."], "bleu": 0.21690743377623947, "rouge_l": 0.4093959731543625}
{"id": 1720, "code": "def retrieve ( self ) : if self . authorization ( ) and \"whois db\" not in Py Funceble . INTERN : if Py Funceble . path . isfile ( self . whois db path ) : Py Funceble . INTERN [ \"whois db\" ] = Dict ( ) . from json ( File ( self . whois db path ) . read ( ) ) else : Py Funceble . INTERN [ \"whois db\" ] = { }", "predictions": ["backup the } instances ."], "references": ["retrieve the data from the database ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 1721, "code": "def backup ( self ) : if self . authorization ( ) : Dict ( Py Funceble . INTERN [ \"whois db\" ] ) . to json ( self . whois db path )", "predictions": ["restore database to html . ."], "references": ["backup the database into its file ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 1722, "code": "def is in database ( self ) : if ( self . authorization ( ) and Py Funceble . INTERN [ \"file to test\" ] in Py Funceble . INTERN [ \"whois db\" ] and Py Funceble . INTERN [ \"to test\" ] in Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] ) : return True return False", "predictions": ["print out if the database is safe safe"], "references": ["check if the element is into the database ."], "bleu": 0.23420197753909952, "rouge_l": 0.34923664122137404}
{"id": 1723, "code": "def is time older ( self ) : if ( self . authorization ( ) and self . is in database ( ) and int ( Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] [ Py Funceble . INTERN [ \"to test\" ] ] [ \"epoch\" ] ) < int ( Py Funceble . time ( ) ) ) : return True return False", "predictions": ["return entry point ."], "references": ["check if the current time is older than the one in the database ."], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 1724, "code": "def add ( self ) : if self . authorization ( ) : if self . epoch < int ( Py Funceble . time ( ) ) : state = \"past\" else : state = \"future\" if self . is in database ( ) : if ( str ( self . epoch ) != Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] [ Py Funceble . INTERN [ \"to test\" ] ] [ \"epoch\" ] ) : Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] [ Py Funceble . INTERN [ \"to test\" ] ] . update ( { \"epoch\" : str ( self . epoch ) , \"state\" : state , \"expiration date\" : self . expiration date , } ) elif self . is time older ( ) : if ( Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] [ Py Funceble . INTERN [ \"to test\" ] ] [ \"state\" ] != \"past\" ) : Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] [ Py Funceble . INTERN [ \"to test\" ] ] . update ( { \"state\" : \"past\" } ) elif ( Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] [ Py Funceble . INTERN [ \"to test\" ] ] [ \"state\" ] != \"future\" ) : Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] [ Py Funceble . INTERN [ \"to test\" ] ] . update ( { \"state\" : \"future\" } ) else : if ( not Py Funceble . INTERN [ \"file to test\" ] in Py Funceble . INTERN [ \"whois db\" ] ) : Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] = { } Py Funceble . INTERN [ \"whois db\" ] [ Py Funceble . INTERN [ \"file to test\" ] ] . update ( { Py Funceble . INTERN [ \"to test\" ] : { \"epoch\" : str ( self . epoch ) , \"state\" : state , \"expiration date\" : self . expiration date , } } ) self . backup ( )", "predictions": ["print the epoch to the epoch"], "references": ["add the currently tested element into the database ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 1725, "code": "def travis permissions ( cls ) : if Py Funceble . CONFIGURATION [ \"travis\" ] : try : build dir = Py Funceble . environ [ \"TRAVIS BUILD DIR\" ] commands = [ \"sudo chown -R travis:travis %s\" % ( build dir ) , \"sudo chgrp -R travis %s\" % ( build dir ) , \"sudo chmod -R g+rw X %s\" % ( build dir ) , \"sudo chmod 777 -Rf %s.git\" % ( build dir + Py Funceble . directory separator ) , r\"sudo find %s -type d -exec chmod g+x '{}' \\;\" % ( build dir ) , ] for command in commands : Command ( command ) . execute ( ) if Command ( \"git config core.shared Repository\" ) . execute ( ) == \"\" : Command ( \"git config core.shared Repository group\" ) . execute ( ) except Key Error : pass", "predictions": ["handle handle permissions permissions permissions"], "references": ["set permissions in order to avoid issues before commiting ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 1726, "code": "def travis ( self ) : if Py Funceble . CONFIGURATION [ \"travis\" ] : try : = Py Funceble . environ [ \"TRAVIS BUILD DIR\" ] time autorisation = False try : time autorisation = int ( Py Funceble . time ( ) ) >= int ( Py Funceble . INTERN [ \"start\" ] ) + ( int ( Py Funceble . CONFIGURATION [ \"travis autosave minutes\" ] ) * 60 ) except Key Error : if self . last and not self . bypass : raise Exception ( \"Please review the way `Execution Time()` is called.\" ) if self . last or time autorisation or self . bypass : Percentage ( ) . log ( ) self . travis permissions ( ) command = 'git add --all && git commit -a -m \"%s\"' if self . last or self . bypass : if Py Funceble . CONFIGURATION [ \"command before end\" ] : for line in Command ( Py Funceble . CONFIGURATION [ \"command before end\" ] ) . run ( ) : sys stdout . write ( \"{}\\n\" . format ( line ) ) self . travis permissions ( ) message = ( Py Funceble . CONFIGURATION [ \"travis autosave final commit\" ] + \" [ci skip]\" ) Command ( command % message ) . execute ( ) else : if Py Funceble . CONFIGURATION [ \"command\" ] : for line in Command ( Py Funceble . CONFIGURATION [ \"command\" ] ) . run ( ) : sys stdout . write ( \"{}\\n\" . format ( line ) ) self . travis permissions ( ) Command ( command % Py Funceble . CONFIGURATION [ \"travis autosave commit\" ] ) . execute ( ) print ( Command ( \"git push origin %s\" % Py Funceble . CONFIGURATION [ \"travis branch\" ] ) . execute ( ) ) exit ( 0 ) except Key Error : pass", "predictions": ["push up the delete permissions"], "references": ["logic behind autosave under travis ci ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 1727, "code": "def nslookup ( cls ) : try : if \"current test data\" in Py Funceble . INTERN : if not Check ( ) . is ip valid ( ) : request = Py Funceble . socket . getaddrinfo ( Py Funceble . INTERN [ \"to test\" ] , 80 , 0 , 0 , Py Funceble . socket . IPPROTO TCP , ) for sequence in request : Py Funceble . INTERN [ \"current test data\" ] [ \"nslookup\" ] . append ( sequence [ - 1 ] [ 0 ] ) else : request = Py Funceble . socket . gethostbyaddr ( Py Funceble . INTERN [ \"to test\" ] ) Py Funceble . INTERN [ \"current test data\" ] [ \"nslookup\" ] [ \"hostname\" ] = request [ 0 ] Py Funceble . INTERN [ \"current test data\" ] [ \"nslookup\" ] [ \"aliases\" ] = request [ 1 ] Py Funceble . INTERN [ \"current test data\" ] [ \"nslookup\" ] [ \"ips\" ] = request [ 2 ] else : if not Check ( ) . is ip valid ( ) : Py Funceble . socket . getaddrinfo ( Py Funceble . INTERN [ \"to test\" ] , 80 , 0 , 0 , Py Funceble . socket . IPPROTO TCP , ) else : Py Funceble . socket . gethostbyaddr ( Py Funceble . INTERN [ \"to test\" ] ) return True except ( OS Error , Py Funceble . socket . herror , Py Funceble . socket . gaierror ) : return False", "predictions": ["check if the install is allowed"], "references": ["implementation of unix nslookup ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1728, "code": "def get ( self ) : if not Py Funceble . CONFIGURATION [ \"local\" ] : if self . domain extension not in self . ignored extension : referer = None if self . domain extension in Py Funceble . INTERN [ \"iana db\" ] : if not Py Funceble . CONFIGURATION [ \"no whois\" ] : referer = Py Funceble . INTERN [ \"iana db\" ] [ self . domain extension ] if not referer : Logs ( ) . referer not found ( self . domain extension ) return None return referer return None return False return None return None", "predictions": ["get the domain extension"], "references": ["return the referer aka the whois server of the current domain extension ."], "bleu": 0.05355679762998549, "rouge_l": 0.32218309859154926}
{"id": 1729, "code": "def standard paths ( ) : for is plat spec in [ True , False ] : path = distutils . sysconfig . get python lib ( standard lib = True , plat specific = is plat spec ) for name in os . listdir ( path ) : yield name try : for name in os . listdir ( os . path . join ( path , 'lib-dynload' ) ) : yield name except OS Error : pass", "predictions": ["return all standard paths in the python interpreter ."], "references": ["yield paths to standard modules ."], "bleu": 0.16784459625186196, "rouge_l": 0.27664399092970515}
{"id": 1730, "code": "def standard package names ( ) : for name in standard paths ( ) : if name . startswith ( ' ' ) or '-' in name : continue if '.' in name and name . rsplit ( '.' ) [ - 1 ] not in [ 'so' , 'py' , 'pyc' ] : continue yield name . split ( '.' ) [ 0 ]", "predictions": ["return list of standard package names ."], "references": ["yield standard module names ."], "bleu": 0.2626909894424158, "rouge_l": 0.5154929577464789}
{"id": 1731, "code": "def unused import line numbers ( messages ) : for message in messages : if isinstance ( message , pyflakes . messages . Unused Import ) : yield message . lineno", "predictions": ["yield all messages from the line numbers"], "references": ["yield line numbers of unused imports ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 1732, "code": "def unused import module name ( messages ) : pattern = r'\\'(.+?)\\'' for message in messages : if isinstance ( message , pyflakes . messages . Unused Import ) : module name = re . search ( pattern , str ( message ) ) module name = module name . group ( ) [ 1 : - 1 ] if module name : yield ( message . lineno , module name )", "predictions": ["import all unused messages in the module ."], "references": ["yield line number and module name of unused imports ."], "bleu": 0.1485237584394808, "rouge_l": 0.21785714285714283}
{"id": 1733, "code": "def star import used line numbers ( messages ) : for message in messages : if isinstance ( message , pyflakes . messages . Import Star Used ) : yield message . lineno", "predictions": ["yield all messages from the star line ."], "references": ["yield line number of star import usage ."], "bleu": 0.20164945583740668, "rouge_l": 0.375}
{"id": 1734, "code": "def star import usage undefined name ( messages ) : for message in messages : if isinstance ( message , pyflakes . messages . Import Star Usage ) : undefined name = message . message args [ 0 ] module name = message . message args [ 1 ] yield ( message . lineno , undefined name , module name )", "predictions": ["import all messages from an undefined name ."], "references": ["yield line number undefined name and its possible origin module ."], "bleu": 0.15587146574232644, "rouge_l": 0.3070469798657718}
{"id": 1735, "code": "def unused variable line numbers ( messages ) : for message in messages : if isinstance ( message , pyflakes . messages . Unused Variable ) : yield message . lineno", "predictions": ["yield messages numbers from all messages ."], "references": ["yield line numbers of unused variables ."], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 1736, "code": "def duplicate key line numbers ( messages , source ) : messages = [ message for message in messages if isinstance ( message , pyflakes . messages . Multi Value Repeated Key Literal ) ] if messages : key to messages = create key to messages dict ( messages ) lines = source . split ( '\\n' ) for ( key , messages ) in key to messages . items ( ) : good = True for message in messages : line = lines [ message . lineno - 1 ] key = message . message args [ 0 ] if not dict entry has key ( line , key ) : good = False if good : for message in messages : yield message . lineno", "predictions": ["duplicate messages numbers numbers numbers numbers numbers numbers ."], "references": ["yield line numbers of duplicate keys ."], "bleu": 0.16784459625186196, "rouge_l": 0.2557651991614256}
{"id": 1737, "code": "def create key to messages dict ( messages ) : dictionary = collections . defaultdict ( lambda : [ ] ) for message in messages : dictionary [ message . message args [ 0 ] ] . append ( message ) return dictionary", "predictions": ["create a list of messages from a dictionary ."], "references": ["return dict mapping the key to list of messages ."], "bleu": 0.24855227187657006, "rouge_l": 0.41709401709401706}
{"id": 1738, "code": "def check ( source ) : if sys . version info [ 0 ] == 2 and isinstance ( source , unicode ) : try : source = source . encode ( 'utf-8' ) except Unicode Error : return [ ] reporter = List Reporter ( ) try : pyflakes . api . check ( source , filename = '<string>' , reporter = reporter ) except ( Attribute Error , Recursion Error , Unicode Decode Error ) : pass return reporter . messages", "predictions": ["check if the source is valid ."], "references": ["return messages from pyflakes ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1739, "code": "def extract package name ( line ) : assert '\\\\' not in line assert '(' not in line assert ')' not in line assert ';' not in line if line . lstrip ( ) . startswith ( ( 'import' , 'from' ) ) : word = line . split ( ) [ 1 ] else : return None package = word . split ( '.' ) [ 0 ] assert ' ' not in package return package", "predictions": ["extract the name of a package ."], "references": ["return package name in import statement ."], "bleu": 0.22089591134157885, "rouge_l": 0.2857142857142857}
{"id": 1740, "code": "def multiline import ( line , previous line = '' ) : for symbol in '()' : if symbol in line : return True if line . lstrip ( ) . startswith ( '>' ) : return True return multiline statement ( line , previous line )", "predictions": ["return whether the line is multiline or not ."], "references": ["return true if import is spans multiples lines ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 1741, "code": "def multiline statement ( line , previous line = '' ) : for symbol in '\\\\:;' : if symbol in line : return True sio = io . String IO ( line ) try : list ( tokenize . generate tokens ( sio . readline ) ) return previous line . rstrip ( ) . endswith ( '\\\\' ) except ( Syntax Error , tokenize . Token Error ) : return True", "predictions": ["return a list of tokens in a multiline statement ."], "references": ["return true if this is part of a multiline statement ."], "bleu": 0.34531590082160607, "rouge_l": 0.56656346749226}
{"id": 1742, "code": "def break up import ( line ) : assert '\\\\' not in line assert '(' not in line assert ')' not in line assert ';' not in line assert '#' not in line assert not line . lstrip ( ) . startswith ( 'from' ) newline = get line ending ( line ) if not newline : return line ( indentation , imports ) = re . split ( pattern = r'\\bimport\\b' , string = line , maxsplit = 1 ) indentation += 'import ' assert newline return '' . join ( [ indentation + i . strip ( ) + newline for i in sorted ( imports . split ( ',' ) ) ] )", "predictions": ["break indentation up to import indentation from a line ."], "references": ["return line with imports on separate lines ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 1743, "code": "def filter code ( source , additional imports = None , expand star imports = False , remove all unused imports = False , remove duplicate keys = False , remove unused variables = False , ignore init module imports = False , ) : imports = SAFE IMPORTS if additional imports : imports |= frozenset ( additional imports ) del additional imports messages = check ( source ) if ignore init module imports : marked import line numbers = frozenset ( ) else : marked import line numbers = frozenset ( unused import line numbers ( messages ) ) marked unused module = collections . defaultdict ( lambda : [ ] ) for line number , module name in unused import module name ( messages ) : marked unused module [ line number ] . append ( module name ) if expand star imports and not ( re . search ( r'\\b all \\b' , source ) or re . search ( r'\\bdel\\b' , source ) ) : marked star import line numbers = frozenset ( star import used line numbers ( messages ) ) if len ( marked star import line numbers ) > 1 : marked star import line numbers = frozenset ( ) else : undefined names = [ ] for line number , undefined name , in star import usage undefined name ( messages ) : undefined names . append ( undefined name ) if not undefined names : marked star import line numbers = frozenset ( ) else : marked star import line numbers = frozenset ( ) if remove unused variables : marked variable line numbers = frozenset ( unused variable line numbers ( messages ) ) else : marked variable line numbers = frozenset ( ) if remove duplicate keys : marked key line numbers = frozenset ( duplicate key line numbers ( messages , source ) ) else : marked key line numbers = frozenset ( ) line messages = get messages by line ( messages ) sio = io . String IO ( source ) previous line = '' for line number , line in enumerate ( sio . readlines ( ) , start = 1 ) : if '#' in line : yield line elif line number in marked import line numbers : yield filter unused import ( line , unused module = marked unused module [ line number ] , remove all unused imports = remove all unused imports , imports = imports , previous line = previous line ) elif line number in marked variable line numbers : yield filter unused variable ( line ) elif line number in marked key line numbers : yield filter duplicate key ( line , line messages [ line number ] , line number , marked key line numbers , source ) elif line number in marked star import line numbers : yield filter star import ( line , undefined names ) else : yield line previous line = line", "predictions": ["filter out all unused variables ."], "references": ["yield code with unused imports removed ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 1744, "code": "def get messages by line ( messages ) : line messages = { } for message in messages : line messages [ message . lineno ] = message return line messages", "predictions": ["get all messages by line line"], "references": ["return dictionary that maps line number to message ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 1745, "code": "def filter star import ( line , marked star import undefined name ) : undefined name = sorted ( set ( marked star import undefined name ) ) return re . sub ( r'\\*' , ', ' . join ( undefined name ) , line )", "predictions": ["filter out star import import import files ."], "references": ["return line with the star import expanded ."], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 1746, "code": "def filter unused import ( line , unused module , remove all unused imports , imports , previous line = '' ) : if multiline import ( line , previous line ) : return line is from import = line . lstrip ( ) . startswith ( 'from' ) if ',' in line and not is from import : return break up import ( line ) package = extract package name ( line ) if not remove all unused imports and package not in imports : return line if ',' in line : assert is from import return filter from import ( line , unused module ) else : return ( get indentation ( line ) + 'pass' + get line ending ( line ) )", "predictions": ["return all unused imports that are unused ."], "references": ["return line if used otherwise return none ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 1747, "code": "def filter unused variable ( line , previous line = '' ) : if re . match ( EXCEPT REGEX , line ) : return re . sub ( r' as \\w+:$' , ':' , line , count = 1 ) elif multiline statement ( line , previous line ) : return line elif line . count ( '=' ) == 1 : split line = line . split ( '=' ) assert len ( split line ) == 2 value = split line [ 1 ] . lstrip ( ) if ',' in split line [ 0 ] : return line if is literal or name ( value ) : value = 'pass' + get line ending ( line ) return get indentation ( line ) + value else : return line", "predictions": ["filter unused variable names ."], "references": ["return line if used otherwise return none ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1748, "code": "def filter duplicate key ( line , message , line number , marked line numbers , source , previous line = '' ) : if marked line numbers and line number == sorted ( marked line numbers ) [ 0 ] : return '' return line", "predictions": ["filter the duplicate key to the duplicate line ."], "references": ["return if first occurrence of the key otherwise return line ."], "bleu": 0.1689983564524028, "rouge_l": 0.3929146537842191}
{"id": 1749, "code": "def is literal or name ( value ) : try : ast . literal eval ( value ) return True except ( Syntax Error , Value Error ) : pass if value . strip ( ) in [ 'dict()' , 'list()' , 'set()' ] : return True return re . match ( r'^\\w+\\s*$' , value )", "predictions": ["return true if value is a literal or not ."], "references": ["return true if value is a literal or a name ."], "bleu": 0.7302542206757164, "rouge_l": 0.8498452012383902}
{"id": 1750, "code": "def useless pass line numbers ( source ) : sio = io . String IO ( source ) previous token type = None last pass row = None last pass indentation = None previous line = '' for token in tokenize . generate tokens ( sio . readline ) : token type = token [ 0 ] start row = token [ 2 ] [ 0 ] line = token [ 4 ] is pass = ( token type == tokenize . NAME and line . strip ( ) == 'pass' ) if ( start row - 1 == last pass row and get indentation ( line ) == last pass indentation and token type in ATOMS and not is pass ) : yield start row - 1 if is pass : last pass row = start row last pass indentation = get indentation ( line ) if ( is pass and previous token type != tokenize . INDENT and not previous line . rstrip ( ) . endswith ( '\\\\' ) ) : yield start row previous token type = token type previous line = line", "predictions": ["pass in a pass line of tokens ."], "references": ["yield line numbers of unneeded pass statements ."], "bleu": 0.20164945583740668, "rouge_l": 0.375}
{"id": 1751, "code": "def filter useless pass ( source ) : try : marked lines = frozenset ( useless pass line numbers ( source ) ) except ( Syntax Error , tokenize . Token Error ) : marked lines = frozenset ( ) sio = io . String IO ( source ) for line number , line in enumerate ( sio . readlines ( ) , start = 1 ) : if line number not in marked lines : yield line", "predictions": ["filter the pass pass to the source code ."], "references": ["yield code with useless pass lines removed ."], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 1752, "code": "def get indentation ( line ) : if line . strip ( ) : non whitespace index = len ( line ) - len ( line . lstrip ( ) ) return line [ : non whitespace index ] else : return ''", "predictions": ["get the indentation of the line"], "references": ["return leading whitespace ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1753, "code": "def get line ending ( line ) : non whitespace index = len ( line . rstrip ( ) ) - len ( line ) if not non whitespace index : return '' else : return line [ non whitespace index : ]", "predictions": ["return the ending ending of the line"], "references": ["return line ending ."], "bleu": 0.22089591134157885, "rouge_l": 0.3824451410658307}
{"id": 1754, "code": "def fix code ( source , additional imports = None , expand star imports = False , remove all unused imports = False , remove duplicate keys = False , remove unused variables = False , ignore init module imports = False ) : if not source : return source if 'nonlocal' in source : remove unused variables = False filtered source = None while True : filtered source = '' . join ( filter useless pass ( '' . join ( filter code ( source , additional imports = additional imports , expand star imports = expand star imports , remove all unused imports = remove all unused imports , remove duplicate keys = remove duplicate keys , remove unused variables = remove unused variables , ignore init module imports = ignore init module imports , ) ) ) ) if filtered source == source : break source = filtered source return filtered source", "predictions": ["return a json - serializable code string ."], "references": ["return code with all filtering run on it ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 1755, "code": "def detect encoding ( filename , limit byte check = - 1 ) : try : with open ( filename , 'rb' ) as input file : encoding = detect encoding ( input file . readline ) with open with encoding ( filename , encoding ) as input file : input file . read ( limit byte check ) return encoding except ( Lookup Error , Syntax Error , Unicode Decode Error ) : return 'latin-1'", "predictions": ["detect the encoding of a file ."], "references": ["return file encoding ."], "bleu": 0.22089591134157885, "rouge_l": 0.3824451410658307}
{"id": 1756, "code": "def detect encoding ( readline ) : try : from lib2to3 . pgen2 import tokenize as lib2to3 tokenize encoding = lib2to3 tokenize . detect encoding ( readline ) [ 0 ] return encoding except ( Lookup Error , Syntax Error , Unicode Decode Error ) : return 'latin-1'", "predictions": ["detect the encoding of an lib2to3 string ."], "references": ["return file encoding ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 1757, "code": "def split comma separated ( string ) : return set ( text . strip ( ) for text in string . split ( ',' ) if text . strip ( ) )", "predictions": ["split a string into a set of comma separated separated separated list ."], "references": ["return a set of strings ."], "bleu": 0.18798317647335086, "rouge_l": 0.45101663585951934}
{"id": 1758, "code": "def is python file ( filename ) : if filename . endswith ( '.py' ) : return True try : with open with encoding ( filename , None , limit byte check = MAX PYTHON FILE DETECTION BYTES ) as f : text = f . read ( MAX PYTHON FILE DETECTION BYTES ) if not text : return False first line = text . splitlines ( ) [ 0 ] except ( IO Error , Index Error ) : return False if not PYTHON SHEBANG REGEX . match ( first line ) : return False return True", "predictions": ["return true if the file is a python file ."], "references": ["return true if filename is python file ."], "bleu": 0.3508439695638686, "rouge_l": 0.7936802973977695}
{"id": 1759, "code": "def is exclude file ( filename , exclude ) : base name = os . path . basename ( filename ) if base name . startswith ( '.' ) : return True for pattern in exclude : if fnmatch . fnmatch ( base name , pattern ) : return True if fnmatch . fnmatch ( filename , pattern ) : return True return False", "predictions": ["return true if the filename is an exclude file ."], "references": ["return true if file matches exclude pattern ."], "bleu": 0.26985534666825095, "rouge_l": 0.5669144981412639}
{"id": 1760, "code": "def create ( cls , name value , name type ) : if isinstance ( name value , Name . Name Value ) : value = name value elif isinstance ( name value , str ) : value = cls . Name Value ( name value ) else : name = 'Name' msg = exceptions . Error Strings . BAD EXP RECV member = 'name value' raise Type Error ( msg . format ( '{0}.{1}' . format ( name , member ) , 'name value' , type ( Name . Name Value ) , type ( name value ) ) ) if isinstance ( name type , Name . Name Type ) : n type = name type elif isinstance ( name type , Enum ) : n type = cls . Name Type ( name type ) else : name = 'Name' msg = exceptions . Error Strings . BAD EXP RECV member = 'name type' raise Type Error ( msg . format ( '{0}.{1}' . format ( name , member ) , 'name type' , type ( Name . Name Type ) , type ( name type ) ) ) return Name ( name value = value , name type = n type )", "predictions": ["get a new name if it is set"], "references": ["returns a name object populated with the given value and type"], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 1761, "code": "def get attribute from managed object ( self , managed object , attr name ) : if attr name == 'Unique Identifier' : return str ( managed object . unique identifier ) elif attr name == 'Name' : names = list ( ) for name in managed object . names : name = attributes . Name ( attributes . Name . Name Value ( name ) , attributes . Name . Name Type ( enums . Name Type . UNINTERPRETED TEXT STRING ) ) names . append ( name ) return names elif attr name == 'Object Type' : return managed object . object type elif attr name == 'Cryptographic Algorithm' : return managed object . cryptographic algorithm elif attr name == 'Cryptographic Length' : return managed object . cryptographic length elif attr name == 'Cryptographic Parameters' : return None elif attr name == 'Cryptographic Domain Parameters' : return None elif attr name == 'Certificate Type' : return managed object . certificate type elif attr name == 'Certificate Length' : return None elif attr name == 'X.509 Certificate Identifier' : return None elif attr name == 'X.509 Certificate Subject' : return None elif attr name == 'X.509 Certificate Issuer' : return None elif attr name == 'Certificate Identifier' : return None elif attr name == 'Certificate Subject' : return None elif attr name == 'Certificate Issuer' : return None elif attr name == 'Digital Signature Algorithm' : return None elif attr name == 'Digest' : return None elif attr name == 'Operation Policy Name' : return managed object . operation policy name elif attr name == 'Cryptographic Usage Mask' : return managed object . cryptographic usage masks elif attr name == 'Lease Time' : return None elif attr name == 'Usage Limits' : return None elif attr name == 'State' : return managed object . state elif attr name == 'Initial Date' : return managed object . initial date elif attr name == 'Activation Date' : return None elif attr name == 'Process Start Date' : return None elif attr name == 'Protect Stop Date' : return None elif attr name == 'Deactivation Date' : return None elif attr name == 'Destroy Date' : return None elif attr name == 'Compromise Occurrence Date' : return None elif attr name == 'Compromise Date' : return None elif attr name == 'Revocation Reason' : return None elif attr name == 'Archive Date' : return None elif attr name == 'Object Group' : return None elif attr name == 'Fresh' : return None elif attr name == 'Link' : return None elif attr name == 'Application Specific Information' : return None elif attr name == 'Contact Information' : return None elif attr name == 'Last Change Date' : return None else : return None", "predictions": ["returns the paths to the managed object object"], "references": ["get the attribute value from the kmip . pie managed object ."], "bleu": 0.14544785215055717, "rouge_l": 0.3860759493670886}
{"id": 1762, "code": "def set attribute on managed object ( self , managed object , attribute ) : attribute name = attribute [ 0 ] attribute value = attribute [ 1 ] if self . attribute policy . is attribute multivalued ( attribute name ) : if attribute name == 'Name' : managed object . names . extend ( [ x . name value . value for x in attribute value ] ) for name in managed object . names : if managed object . names . count ( name ) > 1 : raise exceptions . Invalid Field ( \"Cannot set duplicate name values.\" ) else : raise exceptions . Invalid Field ( \"The {0} attribute is unsupported.\" . format ( attribute name ) ) else : field = None value = attribute value . value if attribute name == 'Cryptographic Algorithm' : field = 'cryptographic algorithm' elif attribute name == 'Cryptographic Length' : field = 'cryptographic length' elif attribute name == 'Cryptographic Usage Mask' : field = 'cryptographic usage masks' value = list ( ) for e in enums . Cryptographic Usage Mask : if e . value & attribute value . value : value . append ( e ) elif attribute name == 'Operation Policy Name' : field = 'operation policy name' if field : existing value = getattr ( managed object , field ) if existing value : if existing value != value : raise exceptions . Invalid Field ( \"Cannot overwrite the {0} attribute.\" . format ( attribute name ) ) else : setattr ( managed object , field , value ) else : raise exceptions . Invalid Field ( \"The {0} attribute is unsupported.\" . format ( attribute name ) )", "predictions": ["sets the package s package object"], "references": ["set the attribute value on the kmip . pie managed object ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 1763, "code": "def validate ( self ) : if self . unique identifier is not None : if not isinstance ( self . unique identifier , attributes . Unique Identifier ) : msg = \"invalid unique identifier\" raise Type Error ( msg ) if self . compromise occurrence date is not None : if not isinstance ( self . compromise occurrence date , primitives . Date Time ) : msg = \"invalid compromise time\" raise Type Error ( msg ) if not isinstance ( self . revocation reason , objects . Revocation Reason ) : msg = \"invalid revocation reason\" raise Type Error ( msg )", "predictions": ["unused this requirement ."], "references": ["error check the attributes of the activaterequestpayload object ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 1764, "code": "def key wrapping data ( self , value ) : if value is None : value = { } elif not isinstance ( value , dict ) : raise Type Error ( \"Key wrapping data must be a dictionary.\" ) self . kdw wrapping method = value . get ( 'wrapping method' ) eki = value . get ( 'encryption key information' ) if eki is None : eki = { } self . kdw eki unique identifier = eki . get ( 'unique identifier' ) eki cp = eki . get ( 'cryptographic parameters' ) if eki cp is None : eki cp = { } self . kdw eki cp block cipher mode = eki cp . get ( 'block cipher mode' ) self . kdw eki cp padding method = eki cp . get ( 'padding method' ) self . kdw eki cp hashing algorithm = eki cp . get ( 'hashing algorithm' ) self . kdw eki cp key role type = eki cp . get ( 'key role type' ) self . kdw eki cp digital signature algorithm = eki cp . get ( 'digital signature algorithm' ) self . kdw eki cp cryptographic algorithm = eki cp . get ( 'cryptographic algorithm' ) self . kdw eki cp random iv = eki cp . get ( 'random iv' ) self . kdw eki cp iv length = eki cp . get ( 'iv length' ) self . kdw eki cp tag length = eki cp . get ( 'tag length' ) self . kdw eki cp fixed field length = eki cp . get ( 'fixed field length' ) self . kdw eki cp invocation field length = eki cp . get ( 'invocation field length' ) self . kdw eki cp counter length = eki cp . get ( 'counter length' ) self . kdw eki cp initial counter value = eki cp . get ( 'initial counter value' ) mski = value . get ( 'mac signature key information' ) if mski is None : mski = { } self . kdw mski unique identifier = mski . get ( 'unique identifier' ) mski cp = mski . get ( 'cryptographic parameters' ) if mski cp is None : mski cp = { } self . kdw mski cp block cipher mode = mski cp . get ( 'block cipher mode' ) self . kdw mski cp padding method = mski cp . get ( 'padding method' ) self . kdw mski cp hashing algorithm = mski cp . get ( 'hashing algorithm' ) self . kdw mski cp key role type = mski cp . get ( 'key role type' ) self . kdw mski cp digital signature algorithm = mski cp . get ( 'digital signature algorithm' ) self . kdw mski cp cryptographic algorithm = mski cp . get ( 'cryptographic algorithm' ) self . kdw mski cp random iv = mski cp . get ( 'random iv' ) self . kdw mski cp iv length = mski cp . get ( 'iv length' ) self . kdw mski cp tag length = mski cp . get ( 'tag length' ) self . kdw mski cp fixed field length = mski cp . get ( 'fixed field length' ) self . kdw mski cp invocation field length = mski cp . get ( 'invocation field length' ) self . kdw mski cp counter length = mski cp . get ( 'counter length' ) self . kdw mski cp initial counter value = mski cp . get ( 'initial counter value' ) self . kdw mac signature = value . get ( 'mac signature' ) self . kdw iv counter nonce = value . get ( 'iv counter nonce' ) self . kdw encoding option = value . get ( 'encoding option' )", "predictions": ["set the unused unused module module"], "references": ["set the key wrapping data attributes using a dictionary ."], "bleu": 0.14925824694560996, "rouge_l": 0.23921568627450981}
{"id": 1765, "code": "def get json files ( p ) : f = [ os . path . join ( p , x ) for x in os . listdir ( p ) if x . endswith ( \".json\" ) ] return sorted ( f )", "predictions": ["return list of all import used in a directory"], "references": ["scan the provided policy directory for all json policy files ."], "bleu": 0.12507277759788113, "rouge_l": 0.09822866344605477}
{"id": 1766, "code": "def scan policies ( self ) : policy files = get json files ( self . policy directory ) for f in set ( policy files ) - set ( self . policy files ) : self . file timestamps [ f ] = 0 for f in set ( self . policy files ) - set ( policy files ) : self . logger . info ( \"Removing policies for file: {}\" . format ( f ) ) self . file timestamps . pop ( f , None ) for p in self . policy cache . keys ( ) : self . disassociate policy and file ( p , f ) for p in [ k for k , v in self . policy map . items ( ) if v == f ] : self . restore or delete policy ( p ) self . policy files = policy files for f in sorted ( self . file timestamps . keys ( ) ) : t = os . path . getmtime ( f ) if t > self . file timestamps [ f ] : self . logger . info ( \"Loading policies for file: {}\" . format ( f ) ) self . file timestamps [ f ] = t old p = [ k for k , v in self . policy map . items ( ) if v == f ] try : new p = operation policy . read policy from file ( f ) except Value Error : self . logger . error ( \"Failure loading file: {}\" . format ( f ) ) self . logger . debug ( \"\" , exc info = True ) continue for p in new p . keys ( ) : self . logger . info ( \"Loading policy: {}\" . format ( p ) ) if p in self . reserved policies : self . logger . warning ( \"Policy '{}' overwrites a reserved policy and \" \"will be thrown out.\" . format ( p ) ) continue if p in sorted ( self . policy store . keys ( ) ) : self . logger . debug ( \"Policy '{}' overwrites an existing \" \"policy.\" . format ( p ) ) if f != self . policy map . get ( p ) : self . policy cache . get ( p ) . append ( ( time . time ( ) , self . policy map . get ( p ) , self . policy store . get ( p ) ) ) else : self . policy cache [ p ] = [ ] self . policy store [ p ] = new p . get ( p ) self . policy map [ p ] = f for p in set ( old p ) - set ( new p . keys ( ) ) : self . disassociate policy and file ( p , f ) self . restore or delete policy ( p )", "predictions": ["star all import import files files"], "references": ["scan the policy directory for policy data ."], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 1767, "code": "def run ( self ) : self . initialize tracking structures ( ) if self . live monitoring : self . logger . info ( \"Starting up the operation policy file monitor.\" ) while not self . halt trigger . is set ( ) : time . sleep ( 1 ) self . scan policies ( ) self . logger . info ( \"Stopping the operation policy file monitor.\" ) else : self . scan policies ( )", "predictions": ["tracking the tracking loop messages messages"], "references": ["start monitoring operation policy files ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1768, "code": "def get certificate from connection ( connection ) : certificate = connection . getpeercert ( binary form = True ) if certificate : return x509 . load der x509 certificate ( certificate , backends . default backend ( ) ) return None", "predictions": ["returns a key object from the numbers . ."], "references": ["extract an x . 509 certificate from a socket connection ."], "bleu": 0.14211011212459496, "rouge_l": 0.19645732689210954}
{"id": 1769, "code": "def get common names from certificate ( certificate ) : common names = certificate . subject . get attributes for oid ( x509 . oid . Name OID . COMMON NAME ) return [ common name . value for common name in common names ]", "predictions": ["args messages messages messages messages for the given dict = value = none = 1 = 0 = 1 = 1 = 1 = 1 = 1 = 1 = value"], "references": ["given an x . 509 certificate extract and return all common names ."], "bleu": 0.03901663112717908, "rouge_l": 0.04907481898632341}
{"id": 1770, "code": "def get client identity from certificate ( certificate ) : client ids = get common names from certificate ( certificate ) if len ( client ids ) > 0 : if len ( client ids ) > 1 : raise exceptions . Permission Denied ( \"Multiple client identities found.\" ) return client ids [ 0 ] else : raise exceptions . Permission Denied ( \"The certificate does not define any subject common names. \" \"Client identity unavailable.\" )", "predictions": ["return the client source from from from"], "references": ["given an x . 509 certificate extract and return the client identity ."], "bleu": 0.14671451318816847, "rouge_l": 0.2846034214618974}
{"id": 1771, "code": "def validate ( self ) : if not isinstance ( self . revocation code , Revocation Reason Code ) : msg = \"Revocation Reaon Code expected\" raise Type Error ( msg ) if self . revocation message is not None : if not isinstance ( self . revocation message , Text String ) : msg = \"Text String expect\" raise Type Error ( msg )", "predictions": ["extract the not block ."], "references": ["validate the revocationreason object"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 1772, "code": "def validate ( self ) : if self . unique identifier is not None : if not isinstance ( self . unique identifier , attributes . Unique Identifier ) : msg = \"invalid unique identifier\" raise Type Error ( msg )", "predictions": ["multiline multiline if the document is valid = true"], "references": ["error check the attributes of the activaterequestpayload object ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 1773, "code": "def load ( self ) : mod = import module ( self . module name ) obj = mod if self . object name : for attr in self . object name . split ( '.' ) : obj = getattr ( obj , attr ) return obj", "predictions": ["multiline the module class"], "references": ["load the object to which this entry point refers ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 1774, "code": "def generate controller ( args ) : controller template = os . path . join ( dirname ( abspath ( file ) ) , 'templates/controller.py' ) test template = os . path . join ( dirname ( abspath ( file ) ) , 'templates/unittest.py' ) controller name = args . get ( '<controller>' ) current path = os . getcwd ( ) logger . info ( 'Start generating controller.' ) if not controller name : logger . warning ( 'Controller name cannot be empty.' ) return with open ( controller template , 'r' ) as template file : controller file path = os . path . join ( current path , 'application/controllers' , controller name + '.py' ) with open ( controller file path , 'w+' ) as controller file : for line in template file : new line = line . replace ( '#{controller}' , controller name ) controller file . write ( new line ) logger . info ( \"New: %s\" % relative path ( controller file path ) ) with open ( test template , 'r' ) as template file : test file path = os . path . join ( current path , 'tests' , 'test %s.py' % controller name ) with open ( test file path , 'w+' ) as test file : for line in template file : new line = line . replace ( '#{controller}' , controller name ) . replace ( '#{controller|title}' , controller name . title ( ) ) test file . write ( new line ) logger . info ( \"New: %s\" % relative path ( test file path ) ) assets dir path = os . path . join ( current path , 'application/pages/%s' % controller name ) mkdir p ( assets dir path ) generate form ( controller name ) logger . info ( 'Finish generating controller.' )", "predictions": ["generates the up up up to the up up the up up not not passed in the up not ."], "references": ["generate controller include the controller file template & css & js directories ."], "bleu": 0.06760229884571738, "rouge_l": 0.12603305785123967}
{"id": 1775, "code": "def mkdir p ( path ) : try : os . makedirs ( path ) except OS Error as exc : if exc . errno == errno . EEXIST and os . path . isdir ( path ) : pass else : raise else : logger . info ( \"New: %s%s\" , path , os . path . sep )", "predictions": ["create directory if it doesn t exist"], "references": ["mkdir - p path"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1776, "code": "def rewrite and copy ( src file , dst file , project name ) : fh , abs path = mkstemp ( ) with io . open ( abs path , 'w' , encoding = 'utf-8' ) as new file : with io . open ( src file , 'r' , encoding = 'utf-8' ) as old file : for line in old file : new line = line . replace ( '#{project}' , project name ) . replace ( '#{project|title}' , project name . title ( ) ) new file . write ( new line ) shutil . copy ( abs path , dst file ) os . close ( fh )", "predictions": ["get the new messages from the given src"], "references": ["replace vars and copy ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1777, "code": "def check url ( form , field ) : url = field . data . strip ( ) if not url : return result = urlparse ( url ) if result . scheme == \"\" : field . data = \"http://%s\" % re . sub ( r'^:?/*' , '' , url )", "predictions": ["filter the star star . ."], "references": ["check url schema ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 1778, "code": "def encode ( something ) : secret key = current app . config . get ( 'SECRET KEY' ) s = URL Safe Serializer ( secret key ) return s . dumps ( something )", "predictions": ["filter the secret module previous module previous module previous module previous module previous module previous module previous module previous module previous module previous module previous module previous module previous module previous"], "references": ["encode something with secret_key ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1779, "code": "def decode ( something ) : secret key = current app . config . get ( 'SECRET KEY' ) s = URL Safe Serializer ( secret key ) try : return s . loads ( something ) except Bad Signature : return None", "predictions": ["filter the secret from the secret"], "references": ["decode something with secret_key ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1780, "code": "def absolute url for ( endpoint , * * values ) : config = current app . config site domain = config . get ( 'SITE DOMAIN' ) relative url = url for ( endpoint , * * values ) return join url ( site domain , relative url )", "predictions": ["return the filter duplicate number for the given line . . . . . . . . . . . . . . . . . . . . . ."], "references": ["absolute url for endpoint ."], "bleu": 0.04317900023606586, "rouge_l": 0.12774869109947642}
{"id": 1781, "code": "def signin user ( user , permenent = True ) : session . permanent = permenent session [ 'user id' ] = user . id", "predictions": ["is the literal literal literal"], "references": ["sign in user ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 1782, "code": "def get current user ( ) : if not 'user id' in session : return None user = User . query . filter ( User . id == session [ 'user id' ] ) . first ( ) if not user : signout user ( ) return None return user", "predictions": ["return the pass line"], "references": ["get current user ."], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 1783, "code": "def create app ( ) : config = load config ( ) app = Flask ( name ) app . config . from object ( config ) app . wsgi app = Proxy Fix ( app . wsgi app ) Csrf Protect ( app ) if app . debug or app . testing : Debug Toolbar Extension ( app ) app . wsgi app = Shared Data Middleware ( app . wsgi app , { '/pages' : os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'application/pages' ) } ) else : app . logger . add Handler ( logging . Stream Handler ( ) ) app . logger . set Level ( logging . ERROR ) if app . config . get ( 'SENTRY DSN' ) : from . utils . sentry import sentry sentry . init app ( app , dsn = app . config . get ( 'SENTRY DSN' ) ) app . wsgi app = Shared Data Middleware ( app . wsgi app , { '/static' : os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'output/static' ) , '/pkg' : os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'output/pkg' ) , '/pages' : os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'output/pages' ) } ) register db ( app ) register routes ( app ) register jinja ( app ) register error handle ( app ) register hooks ( app ) return app", "predictions": ["filter out the numbers of the flask application = 0 = 1 = 1 = 1 = 0"], "references": ["create flask app ."], "bleu": 0.06809398432036522, "rouge_l": 0.1026936026936027}
{"id": 1784, "code": "def register jinja ( app ) : import jinja2 from . utils import filters , permissions , helpers if app . debug or app . testing : my loader = jinja2 . Choice Loader ( [ app . jinja loader , jinja2 . File System Loader ( [ os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'application/macros' ) , os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'application/pages' ) ] ) ] ) else : my loader = jinja2 . Choice Loader ( [ app . jinja loader , jinja2 . File System Loader ( [ os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'output/macros' ) , os . path . join ( app . config . get ( 'PROJECT PATH' ) , 'output/pages' ) ] ) ] ) app . jinja loader = my loader app . jinja env . filters . update ( { 'timesince' : filters . timesince } ) def url for other page ( page ) : \"\"\"Generate url for pagination.\"\"\" view args = request . view args . copy ( ) args = request . args . copy ( ) . to dict ( ) combined args = dict ( view args . items ( ) + args . items ( ) ) combined args [ 'page' ] = page return url for ( request . endpoint , * * combined args ) rules = { } for endpoint , rules in iteritems ( app . url map . rules by endpoint ) : if any ( item in endpoint for item in [ ' debug toolbar' , 'debugtoolbar' , 'static' ] ) : continue rules [ endpoint ] = [ { 'rule' : rule . rule } for rule in rules ] app . jinja env . globals . update ( { 'absolute url for' : helpers . absolute url for , 'url for other page' : url for other page , 'rules' : rules , 'permissions' : permissions } )", "predictions": ["get indentation from indentation non - indentation non - indentation non - indentation indentation non - indentation non - indentation non - indentation non - indentation - indentation non - indentation"], "references": ["register jinja filters vars functions ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1785, "code": "def register error handle ( app ) : @ app . errorhandler ( 403 ) def page 403 ( error ) : return render template ( 'site/403/403.html' ) , 403 @ app . errorhandler ( 404 ) def page 404 ( error ) : return render template ( 'site/404/404.html' ) , 404 @ app . errorhandler ( 500 ) def page 500 ( error ) : return render template ( 'site/500/500.html' ) , 500", "predictions": ["get flask application index index index index index index index index index index index index index index index index index index index index index index index index index index index index"], "references": ["register http error pages ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1786, "code": "def dataframe to csv ( writer , dataframe , delimiter , with header ) : encoding writer = codecs . getwriter ( 'utf-8' ) ( writer ) dataframe . to csv ( path or buf = encoding writer , sep = delimiter , header = with header , index = False )", "predictions": ["write a fix fix data to csv unused unused unused unused ."], "references": ["serialize the dataframe with different delimiters"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 1787, "code": "def dataframe from csv ( reader , delimiter , with header , skipspace ) : sep = delimiter header = 0 if not with header : header = None return pd . read csv ( reader , header = header , sep = sep , skipinitialspace = skipspace , encoding = 'utf-8-sig' )", "predictions": ["create a detect detect detect detect and return a file object ."], "references": ["returns csv data as a pandas dataframe object"], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 1788, "code": "def contents url ( self ) : loc = self . download location return loc . base uri + loc . location + loc . access credential", "predictions": ["import the detect detect detect the content of the access lib2to3 lib2to3 lib2to3 lib2to3 lib2to3 lib2to3 lib2to3 lib2to3 ."], "references": ["full url to the dataset contents ."], "bleu": 0.0712695567709093, "rouge_l": 0.16781292984869325}
{"id": 1789, "code": "def open ( self ) : return self . workspace . rest . open intermediate dataset contents ( self . workspace . workspace id , self . experiment . experiment id , self . node id , self . port name )", "predictions": ["split the return into an instance of the return . . . . . . . . . ."], "references": ["open and return a stream for the dataset contents ."], "bleu": 0.07658412276041004, "rouge_l": 0.21916167664670658}
{"id": 1790, "code": "def read as binary ( self ) : return self . workspace . rest . read intermediate dataset contents binary ( self . workspace . workspace id , self . experiment . experiment id , self . node id , self . port name )", "predictions": ["is the try to an encoding of this endswith"], "references": ["read and return the dataset contents as binary ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 1791, "code": "def read as text ( self ) : return self . workspace . rest . read intermediate dataset contents text ( self . workspace . workspace id , self . experiment . experiment id , self . node id , self . port name )", "predictions": ["is the file pointed to the experiment"], "references": ["read and return the dataset contents as text ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 1792, "code": "def to dataframe ( self ) : #TODO: figure out why passing in the opened stream directly gives invalid data data = self . read as binary ( ) reader = Bytes IO ( data ) return deserialize dataframe ( reader , self . data type id )", "predictions": ["convert the opened stream to a dataframe ."], "references": ["read and return the dataset contents as a pandas dataframe ."], "bleu": 0.16481400866629634, "rouge_l": 0.4093959731543625}
{"id": 1793, "code": "def get experiments ( self , workspace id ) : api path = self . EXPERIMENTS URI FMT . format ( workspace id ) return self . send get req ( api path )", "predictions": ["get the experiments for a given workspace ."], "references": ["runs http get request to retrieve the list of experiments ."], "bleu": 0.13859150907108325, "rouge_l": 0.4093959731543625}
{"id": 1794, "code": "def get datasets ( self , workspace id ) : api path = self . DATASOURCES URI FMT . format ( workspace id ) return self . send get req ( api path )", "predictions": ["get all datasets for a given workspace ."], "references": ["runs http get request to retrieve the list of datasets ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 1795, "code": "def get dataset ( self , workspace id , dataset id ) : api path = self . DATASOURCE URI FMT . format ( workspace id , dataset id ) return self . send get req ( api path )", "predictions": ["get a dataset by id"], "references": ["runs http get request to retrieve a single dataset ."], "bleu": 0.11943865131127647, "rouge_l": 0.37731958762886597}
{"id": 1796, "code": "def find globals ( code ) : cur byte = 0 byte code = code . co code names = set ( ) while cur byte < len ( byte code ) : op = ord ( byte code [ cur byte ] ) if op >= dis . HAVE ARGUMENT : if op == LOAD GLOBAL : oparg = ord ( byte code [ cur byte + 1 ] ) + ( ord ( byte code [ cur byte + 2 ] ) << 8 ) name = code . co names [ oparg ] names . add ( name ) cur byte += 2 cur byte += 1 return names", "predictions": ["find names of 8 - bit code in 8 - place"], "references": ["walks the byte code to find the variables which are actually globals"], "bleu": 0.11510518494396255, "rouge_l": 0.08628005657708629}
{"id": 1797, "code": "def copy ( self ) : pen = Pen ( ) pen . dict = self . dict . copy ( ) return pen", "predictions": ["returns a copy of this instance ."], "references": ["create a copy of this pen ."], "bleu": 0.5410822690539396, "rouge_l": 0.7142857142857143}
{"id": 1798, "code": "def draw ( self , cr , highlight = False , bounding = None ) : if bounding is None or self . intersects ( bounding ) : self . draw ( cr , highlight , bounding )", "predictions": ["draw the specified cr"], "references": ["draw this shape with the given cairo context"], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 1799, "code": "def build choices ( self ) : tree token = u'sitetree tree from \"%s\" template \"%s\"' % ( self . tree , self . template ) context kwargs = { 'current app' : 'admin' } context = template . Context ( context kwargs ) if VERSION >= ( 1 , 8 ) else template . Context ( * * context kwargs ) context . update ( { 'request' : object ( ) } ) choices str = sitetree tree ( Parser ( None ) , Token ( token type = TOKEN BLOCK , contents = tree token ) ) . render ( context ) tree choices = [ ( ITEMS FIELD ROOT ID , self . root title ) ] for line in choices str . splitlines ( ) : if line . strip ( ) : splitted = line . split ( ':::' ) tree choices . append ( ( splitted [ 0 ] , mark safe ( splitted [ 1 ] ) ) ) return tree choices", "predictions": ["build the tree of the tree ."], "references": ["build choices list runtime using sitetree_tree tag"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 1800, "code": "def init ( self ) : cache . get ( 'sitetrees reset' ) and self . empty ( init = False ) self . cache = cache . get ( 'sitetrees' , { 'sitetrees' : { } , 'parents' : { } , 'items by ids' : { } , 'tree aliases' : { } } )", "predictions": ["initialize the cache ."], "references": ["initializes local cache from django cache ."], "bleu": 0.22336835181428535, "rouge_l": 0.346590909090909}
{"id": 1801, "code": "def empty ( self , * * kwargs ) : cache . delete ( 'sitetrees' ) cache . delete ( 'sitetrees reset' ) kwargs . get ( 'init' , True ) and self . init ( )", "predictions": ["remove all instance from the cache ."], "references": ["empties cached sitetree data ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1802, "code": "def for tag ( cls , parser , token , preposition , error hint ) : tokens = token . split contents ( ) if len ( tokens ) >= 3 and tokens [ 1 ] == preposition : as var = cls . get as var ( tokens ) tree alias = parser . compile filter ( tokens [ 2 ] ) return cls ( tree alias , as var ) raise template . Template Syntax Error ( '%r tag requires at least two arguments. E.g. {%% %s %%}.' % ( tokens [ 0 ] , error hint ) )", "predictions": ["create an instance of the tree ."], "references": ["node constructor to be used in tags ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1803, "code": "def get model url name ( model nfo , page , with namespace = False ) : prefix = '' if with namespace : prefix = 'admin:' return ( '%s%s %s' % ( prefix , '%s %s' % model nfo , page ) ) . lower ( )", "predictions": ["return the url of the model ."], "references": ["returns a url for a given tree admin page type ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 1804, "code": "def reregister tree admin ( ) : try : admin . site . unregister ( MODEL TREE CLASS ) except Not Registered : pass admin . site . register ( MODEL TREE CLASS , TREE ADMIN ( ) )", "predictions": ["register admin - admin admin admin ."], "references": ["forces unregistration of tree admin class with following re - registration ."], "bleu": 0.1081377510275021, "rouge_l": 0.30148270181219106}
{"id": 1805, "code": "def redirect ( self , request , response ) : if ' addanother' in request . POST : return Http Response Redirect ( '../item add/' ) elif ' save' in request . POST : return Http Response Redirect ( '../' ) elif ' continue' in request . POST : return response return Http Response Redirect ( '' )", "predictions": ["handles the request and returns the response"], "references": ["generic redirect for item editor ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1806, "code": "def get tree ( self , request , tree id , item id = None ) : if tree id is None : tree id = self . get object ( request , item id ) . tree id self . tree = MODEL TREE CLASS . default manager . get ( pk = tree id ) self . tree . verbose name plural = self . tree . meta . verbose name plural self . tree . urls = TREE URLS return self . tree", "predictions": ["return the tree of the item"], "references": ["fetches tree for current or given treeitem ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 1807, "code": "def item move ( self , request , tree id , item id , direction ) : current item = MODEL TREE ITEM CLASS . default manager . get ( pk = item id ) if direction == 'up' : sort order = 'sort order' else : sort order = '-sort order' siblings = MODEL TREE ITEM CLASS . default manager . filter ( parent = current item . parent , tree = current item . tree ) . order by ( sort order ) previous item = None for item in siblings : if item != current item : previous item = item else : break if previous item is not None : current item sort order = current item . sort order previous item sort order = previous item . sort order current item . sort order = previous item sort order previous item . sort order = current item sort order current item . save ( ) previous item . save ( ) return Http Response Redirect ( '../../' )", "predictions": ["move item to the item"], "references": ["moves item up or down by swapping sort_order field values of neighboring items ."], "bleu": 0.04512859433163675, "rouge_l": 0.09697933227344992}
{"id": 1808, "code": "def get urls ( self ) : urls = super ( Tree Admin , self ) . get urls ( ) prefix change = 'change/' if DJANGO POST 19 else '' sitetree urls = [ url ( r'^change/$' , redirects handler , name = get tree item url name ( 'changelist' ) ) , url ( r'^((?P<tree id>\\d+)/)?%sitem add/$' % prefix change , self . admin site . admin view ( self . tree admin . item add ) , name = get tree item url name ( 'add' ) ) , url ( r'^(?P<tree id>\\d+)/%sitem (?P<item id>\\d+)/$' % prefix change , self . admin site . admin view ( self . tree admin . item edit ) , name = get tree item url name ( 'change' ) ) , url ( r'^%sitem (?P<item id>\\d+)/$' % prefix change , self . admin site . admin view ( self . tree admin . item edit ) , name = get tree item url name ( 'change' ) ) , url ( r'^((?P<tree id>\\d+)/)?%sitem (?P<item id>\\d+)/delete/$' % prefix change , self . admin site . admin view ( self . tree admin . item delete ) , name = get tree item url name ( 'delete' ) ) , url ( r'^((?P<tree id>\\d+)/)?%sitem (?P<item id>\\d+)/history/$' % prefix change , self . admin site . admin view ( self . tree admin . item history ) , name = get tree item url name ( 'history' ) ) , url ( r'^(?P<tree id>\\d+)/%sitem (?P<item id>\\d+)/move (?P<direction>(up|down))/$' % prefix change , self . admin site . admin view ( self . tree admin . item move ) , name = get tree item url name ( 'move' ) ) , ] if not DJANGO POST 19 : sitetree urls = patterns func ( '' , * sitetree urls ) if SMUGGLER INSTALLED : sitetree urls += ( url ( r'^dump all/$' , self . admin site . admin view ( self . dump view ) , name = 'sitetree dump' ) , ) return sitetree urls + urls", "predictions": ["add the admin urls to the admin admin ."], "references": ["manages not only treeadmin urls but also treeitemadmin urls ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 1809, "code": "async def asgi send ( self , message : dict ) -> None : if message [ \"type\" ] == \"http.response.start\" and self . state == ASGIHTTP State . REQUEST : self . response = message elif message [ \"type\" ] == \"http.response.body\" and self . state in { ASGIHTTP State . REQUEST , ASGIHTTP State . RESPONSE , } : if self . state == ASGIHTTP State . REQUEST : headers = build and validate headers ( self . response [ \"headers\" ] ) headers . extend ( self . response headers ( ) ) await self . asend ( h11 . Response ( status code = int ( self . response [ \"status\" ] ) , headers = headers ) ) self . state = ASGIHTTP State . RESPONSE if ( not suppress body ( self . scope [ \"method\" ] , int ( self . response [ \"status\" ] ) ) and message . get ( \"body\" , b\"\" ) != b\"\" ) : await self . asend ( h11 . Data ( data = bytes ( message [ \"body\" ] ) ) ) if not message . get ( \"more body\" , False ) : if self . state != ASGIHTTP State . CLOSED : await self . asend ( h11 . End Of Message ( ) ) await self . asgi put ( { \"type\" : \"http.disconnect\" } ) self . state = ASGIHTTP State . CLOSED else : raise Unexpected Message ( self . state , message [ \"type\" ] )", "predictions": ["send a message to the service ."], "references": ["called by the asgi instance to send a message ."], "bleu": 0.2590352070150216, "rouge_l": 0.45607476635514016}
{"id": 1810, "code": "async def asgi send ( self , message : dict ) -> None : if message [ \"type\" ] == \"websocket.accept\" and self . state == ASGI Websocket State . HANDSHAKE : headers = build and validate headers ( message . get ( \"headers\" , [ ] ) ) raise if subprotocol present ( headers ) headers . extend ( self . response headers ( ) ) await self . asend ( Accept Connection ( extensions = [ Per Message Deflate ( ) ] , extra headers = headers , subprotocol = message . get ( \"subprotocol\" ) , ) ) self . state = ASGI Websocket State . CONNECTED self . config . access logger . access ( self . scope , { \"status\" : 101 , \"headers\" : [ ] } , time ( ) - self . start time ) elif ( message [ \"type\" ] == \"websocket.http.response.start\" and self . state == ASGI Websocket State . HANDSHAKE ) : self . response = message self . config . access logger . access ( self . scope , self . response , time ( ) - self . start time ) elif message [ \"type\" ] == \"websocket.http.response.body\" and self . state in { ASGI Websocket State . HANDSHAKE , ASGI Websocket State . RESPONSE , } : await self . asgi send rejection ( message ) elif message [ \"type\" ] == \"websocket.send\" and self . state == ASGI Websocket State . CONNECTED : data : Union [ bytes , str ] if message . get ( \"bytes\" ) is not None : await self . asend ( Bytes Message ( data = bytes ( message [ \"bytes\" ] ) ) ) elif not isinstance ( message [ \"text\" ] , str ) : raise Type Error ( f\"{message['text']} should be a str\" ) else : await self . asend ( Text Message ( data = message [ \"text\" ] ) ) elif message [ \"type\" ] == \"websocket.close\" and self . state == ASGI Websocket State . HANDSHAKE : await self . send http error ( 403 ) self . state = ASGI Websocket State . HTTPCLOSED elif message [ \"type\" ] == \"websocket.close\" : await self . asend ( Close Connection ( code = int ( message [ \"code\" ] ) ) ) self . state = ASGI Websocket State . CLOSED else : raise Unexpected Message ( self . state , message [ \"type\" ] )", "predictions": ["send a message to the service ."], "references": ["called by the asgi instance to send a message ."], "bleu": 0.2590352070150216, "rouge_l": 0.45607476635514016}
{"id": 1811, "code": "def update binary annotations ( self , extra annotations ) : if not self . logging context : self . binary annotations . update ( extra annotations ) else : self . logging context . tags . update ( extra annotations )", "predictions": ["update extra annotations of binary annotations ."], "references": ["updates the binary annotations for the current span ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 1812, "code": "def encode span ( self , v2 span ) : span = v2 span . build v1 span ( ) thrift endpoint = thrift . create endpoint ( span . endpoint . port , span . endpoint . service name , span . endpoint . ipv4 , span . endpoint . ipv6 , ) thrift annotations = thrift . annotation list builder ( span . annotations , thrift endpoint , ) thrift binary annotations = thrift . binary annotation list builder ( span . binary annotations , thrift endpoint , ) if v2 span . remote endpoint : self . encode remote endpoint ( v2 span . remote endpoint , v2 span . kind , thrift binary annotations , ) thrift span = thrift . create span ( span . id , span . parent id , span . trace id , span . name , thrift annotations , thrift binary annotations , span . timestamp , span . duration , ) encoded span = thrift . span to bytes ( thrift span ) return encoded span", "predictions": ["encode the remote endpoint ."], "references": ["encodes the current span to thrift ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 1813, "code": "def encode span ( self , v2 span ) : span = v2 span . build v1 span ( ) json span = { 'trace Id' : span . trace id , 'name' : span . name , 'id' : span . id , 'annotations' : [ ] , 'binary Annotations' : [ ] , } if span . parent id : json span [ 'parent Id' ] = span . parent id if span . timestamp : json span [ 'timestamp' ] = int ( span . timestamp * 1000000 ) if span . duration : json span [ 'duration' ] = int ( span . duration * 1000000 ) v1 endpoint = self . create json endpoint ( span . endpoint , True ) for key , timestamp in span . annotations . items ( ) : json span [ 'annotations' ] . append ( { 'endpoint' : v1 endpoint , 'timestamp' : int ( timestamp * 1000000 ) , 'value' : key , } ) for key , value in span . binary annotations . items ( ) : json span [ 'binary Annotations' ] . append ( { 'key' : key , 'value' : value , 'endpoint' : v1 endpoint , } ) if v2 span . remote endpoint : self . encode remote endpoint ( v2 span . remote endpoint , v2 span . kind , json span [ 'binary Annotations' ] , ) encoded span = json . dumps ( json span ) return encoded span", "predictions": ["encode the span of the given v2 span ."], "references": ["encodes a single span to json ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 1814, "code": "def encode span ( self , span ) : json span = { 'trace Id' : span . trace id , 'id' : span . span id , } if span . name : json span [ 'name' ] = span . name if span . parent id : json span [ 'parent Id' ] = span . parent id if span . timestamp : json span [ 'timestamp' ] = int ( span . timestamp * 1000000 ) if span . duration : json span [ 'duration' ] = int ( span . duration * 1000000 ) if span . shared is True : json span [ 'shared' ] = True if span . kind and span . kind . value is not None : json span [ 'kind' ] = span . kind . value if span . local endpoint : json span [ 'local Endpoint' ] = self . create json endpoint ( span . local endpoint , False , ) if span . remote endpoint : json span [ 'remote Endpoint' ] = self . create json endpoint ( span . remote endpoint , False , ) if span . tags and len ( span . tags ) > 0 : json span [ 'tags' ] = span . tags if span . annotations : json span [ 'annotations' ] = [ { 'timestamp' : int ( timestamp * 1000000 ) , 'value' : key , } for key , timestamp in span . annotations . items ( ) ] encoded span = json . dumps ( json span ) return encoded span", "predictions": ["encode the span of the given span ."], "references": ["encodes a single span to json ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 1815, "code": "def fits ( self , current count , current size , max size , new span ) : return current size + len ( new span ) <= max size", "predictions": ["returns a fits fits value of a fits string ."], "references": ["checks if the new span fits in the max payload size ."], "bleu": 0.11421946507590645, "rouge_l": 0.1788856304985337}
{"id": 1816, "code": "def encode span ( self , span ) : if not protobuf . installed ( ) : raise Zipkin Error ( 'protobuf encoding requires installing the protobuf\\'s extra ' 'requirements. Use py-zipkin[protobuf] in your requirements.txt.' ) pb span = protobuf . create protobuf span ( span ) return protobuf . encode pb list ( [ pb span ] )", "predictions": ["encode the protobuf span ."], "references": ["encodes a single span to protobuf ."], "bleu": 0.21763141204756337, "rouge_l": 0.32360742705570295}
{"id": 1817, "code": "def join lines ( string , strip = Strip . BOTH ) : lines = [ ] for line in string . splitlines ( ) : if strip & Strip . RIGHT : line = line . rstrip ( ) if strip & Strip . LEFT : line = line . lstrip ( ) lines . append ( line ) return '' . join ( lines )", "predictions": ["join all lines in a string"], "references": ["join strings together and strip whitespace in between if needed"], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 1818, "code": "async def json or text ( response ) : text = await response . text ( ) if response . headers [ 'Content-Type' ] == 'application/json; charset=utf-8' : return json . loads ( text ) return text", "predictions": ["return json or text content from response ."], "references": ["turns response into a properly formatted json or text object"], "bleu": 0.24578832304224082, "rouge_l": 0.3267857142857143}
{"id": 1819, "code": "async def limited ( until ) : duration = int ( round ( until - time . time ( ) ) ) mins = duration / 60 fmt = 'We have exhausted a ratelimit quota. Retrying in %.2f seconds (%.3f minutes).' log . warn ( fmt , duration , mins )", "predictions": ["print a exhausted message ."], "references": ["handles the message shown when we are ratelimited"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 1820, "code": "async def request ( self , method , url , * * kwargs ) : rate limiter = Rate Limiter ( max calls = 59 , period = 60 , callback = limited ) async with rate limiter : if not self . token : raise Unauthorized Detected ( 'Unauthorized Detected (status code: 401): No TOKEN provided' ) headers = { 'User-Agent' : self . user agent , 'Content-Type' : 'application/json' } if 'json' in kwargs : kwargs [ 'data' ] = to json ( kwargs . pop ( 'json' ) ) kwargs [ 'headers' ] = headers headers [ 'Authorization' ] = self . token for tries in range ( 5 ) : async with self . session . request ( method , url , * * kwargs ) as resp : log . debug ( '%s %s with %s has returned %s' , method , url , kwargs . get ( 'data' ) , resp . status ) data = await json or text ( resp ) if 300 > resp . status >= 200 : return data if resp . status == 429 : fmt = 'We are being rate limited. Retrying in %.2f seconds (%.3f minutes).' retry after = json . loads ( resp . headers . get ( 'Retry-After' ) ) mins = retry after / 60 log . warning ( fmt , retry after , mins ) is global = True if is global : self . global over . clear ( ) await asyncio . sleep ( retry after , loop = self . loop ) log . debug ( 'Done sleeping for the rate limit. Retrying...' ) if is global : self . global over . set ( ) log . debug ( 'Global rate limit is now over.' ) continue if resp . status == 400 : raise HTTP Exception ( resp , data ) elif resp . status == 401 : raise Unauthorized ( resp , data ) elif resp . status == 403 : raise Forbidden ( resp , data ) elif resp . status == 404 : raise Not Found ( resp , data ) else : raise HTTP Exception ( resp , data ) raise HTTP Exception ( resp , data )", "predictions": ["request the rate from the rate - end - rate - rate - rate - limit - limit - rate ."], "references": ["handles requests to the api"], "bleu": 0.05809665204409193, "rouge_l": 0.08652482269503546}
{"id": 1821, "code": "async def get bot info ( self , bot id ) : resp = await self . request ( 'GET' , '{}/bots/{}' . format ( self . BASE , bot id ) ) resp [ 'date' ] = datetime . strptime ( resp [ 'date' ] , '%Y-%m-%d T%H:%M:%S.%f Z' ) for k in resp : if resp [ k ] == '' : resp [ k ] = None return resp", "predictions": ["get information for a bot ."], "references": ["gets the information of the given bot id"], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 1822, "code": "async def get bots ( self , limit , offset ) : if limit > 500 : limit = 50 return await self . request ( 'GET' , '{}/bots?limit={}&offset={}' . format ( self . BASE , limit , offset ) )", "predictions": ["get the list of bots for a limit ."], "references": ["gets an object of bots on dbl"], "bleu": 0.18575057999133596, "rouge_l": 0.2557651991614256}
{"id": 1823, "code": "def read ( self ) : packet = self . packet with self . read lock : buffer = self . buffer while len ( buffer ) < packet : buffer += self . read data ( ) length = self . unpack ( buffer [ : packet ] ) [ 0 ] + packet while len ( buffer ) < length : buffer += self . read data ( ) term , self . buffer = decode ( buffer [ packet : ] ) return term", "predictions": ["read packet from the socket ."], "references": ["read incoming message ."], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 1824, "code": "def write ( self , message ) : data = encode ( message , compressed = self . compressed ) length = len ( data ) data = self . pack ( length ) + data with self . write lock : while data : try : n = os . write ( self . out d , data ) except OS Error as why : if why . errno in ( errno . EPIPE , errno . EINVAL ) : raise EOF Error ( ) raise if not n : raise EOF Error ( ) data = data [ n : ] return length + self . packet", "predictions": ["to the client in bytes mode in the packet"], "references": ["write outgoing message ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 1825, "code": "def decode ( string ) : if not string : raise Incomplete Data ( string ) if string [ 0 ] != 131 : raise Value Error ( \"unknown protocol version: %r\" % string [ 0 ] ) if string [ 1 : 2 ] == b'P' : if len ( string ) < 16 : raise Incomplete Data ( string ) d = decompressobj ( ) term string = d . decompress ( string [ 6 : ] ) + d . flush ( ) uncompressed size , = int4 unpack ( string [ 2 : 6 ] ) if len ( term string ) != uncompressed size : raise Value Error ( \"invalid compressed tag, \" \"%d bytes but got %d\" % ( uncompressed size , len ( term string ) ) ) term , tail = decode term ( term string ) return term , d . unused data return decode term ( string [ 1 : ] )", "predictions": ["get the compressed from a string"], "references": ["decode erlang external term ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1826, "code": "def encode ( term , compressed = False ) : encoded term = encode term ( term ) if compressed : if compressed is True : compressed = 6 elif compressed < 0 or compressed > 9 : raise Value Error ( \"invalid compression level: %r\" % ( compressed , ) ) zlib term = compress ( encoded term , compressed ) ln = len ( encoded term ) if len ( zlib term ) + 5 <= ln : return b\"\\x83P\" + int4 pack ( ln ) + zlib term return b\"\\x83\" + encoded term", "predictions": ["get an ascii - safe term representation of an api term ."], "references": ["encode erlang external term ."], "bleu": 0.1367440667823257, "rouge_l": 0.25416666666666665}
{"id": 1827, "code": "def add Source Addr ( self , addr ) : try : self . multi In Socket . setsockopt ( socket . IPPROTO IP , socket . IP ADD MEMBERSHIP , self . make Mreq ( addr ) ) except socket . error : pass sock = self . create Multicast Out Socket ( addr , self . observer . ttl ) self . multi Out Uni In Sockets [ addr ] = sock self . poll . register ( sock , select . POLLIN )", "predictions": ["get a socket object for a given address"], "references": ["none means system default"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1828, "code": "def send Pending Messages ( self ) : if len ( self . queue ) == 0 : time . sleep ( 0.1 ) return msg = self . queue . pop ( 0 ) if msg . can Send ( ) : self . send Msg ( msg ) msg . refresh ( ) if not ( msg . is Finished ( ) ) : self . queue . append ( msg ) else : self . queue . append ( msg ) time . sleep ( 0.01 )", "predictions": ["find the globals of the globals 0 0 0 0 0 0 0 0 0 0 0 0 0 - 1 0 0 0 0 - 1 0 - 1 0"], "references": ["method sleeps if nothing to do"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1829, "code": "def stop ( self ) : self . clear Remote Services ( ) self . clear Local Services ( ) self . stop Threads ( ) self . server Started = False", "predictions": ["copy the timer to the pool pen pen pen pen pen pen"], "references": ["cleans up and stops the discovery server"], "bleu": 0.10390302174233558, "rouge_l": 0.11050724637681159}
{"id": 1830, "code": "def clear Local Services ( self ) : for service in list ( self . local Services . values ( ) ) : self . send Bye ( service ) self . local Services . clear ( )", "predictions": ["clears all if any of the if any"], "references": ["send bye messages for the services and remove them"], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 1831, "code": "def search Services ( self , types = None , scopes = None , timeout = 3 ) : if not self . server Started : raise Exception ( \"Server not started\" ) self . send Probe ( types , scopes ) time . sleep ( timeout ) return self . filter Services ( list ( self . remote Services . values ( ) ) , types , scopes )", "predictions": ["build a list of available connections ."], "references": ["search for services given the types and scopes in a given timeout"], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 1832, "code": "def create SOAP Message ( env ) : if env . get Action ( ) == ACTION PROBE : return create Probe Message ( env ) if env . get Action ( ) == ACTION PROBE MATCH : return create Probe Match Message ( env ) if env . get Action ( ) == ACTION RESOLVE : return create Resolve Message ( env ) if env . get Action ( ) == ACTION RESOLVE MATCH : return create Resolve Match Message ( env ) if env . get Action ( ) == ACTION HELLO : return create Hello Message ( env ) if env . get Action ( ) == ACTION BYE : return create Bye Message ( env )", "predictions": ["init environment variables ."], "references": ["construct a a raw soap xml string given a prepared soapenvelope object"], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 1833, "code": "def discover ( scope , loglevel , capture ) : if loglevel : level = getattr ( logging , loglevel , None ) if not level : print ( \"Invalid log level '%s'\" % loglevel ) return logger . set Level ( level ) run ( scope = scope , capture = capture )", "predictions": ["empty a and return"], "references": ["discover systems using ws - discovery"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 1834, "code": "def save ( self , * * kwargs ) : child relation names = [ rel . get accessor name ( ) for rel in get all child relations ( self ) ] child m2m field names = [ field . name for field in get all child m2m relations ( self ) ] update fields = kwargs . pop ( 'update fields' , None ) if update fields is None : real update fields = None relations to commit = child relation names m2m fields to commit = child m2m field names else : real update fields = [ ] relations to commit = [ ] m2m fields to commit = [ ] for field in update fields : if field in child relation names : relations to commit . append ( field ) elif field in child m2m field names : m2m fields to commit . append ( field ) else : real update fields . append ( field ) super ( Clusterable Model , self ) . save ( update fields = real update fields , * * kwargs ) for relation in relations to commit : getattr ( self , relation ) . commit ( ) for field in m2m fields to commit : getattr ( self , field ) . commit ( )", "predictions": ["saves all error as 1 - mail objects to the db ."], "references": ["save the model and commit all child relations ."], "bleu": 0.1235622127262679, "rouge_l": 0.19551282051282048}
{"id": 1835, "code": "def validate unique ( self ) : all unique checks = set ( ) all date checks = set ( ) forms to delete = self . deleted forms valid forms = [ form for form in self . forms if form . is valid ( ) and form not in forms to delete ] for form in valid forms : unique checks , date checks = form . instance . get unique checks ( ) all unique checks . update ( unique checks ) all date checks . update ( date checks ) errors = [ ] for uclass , unique check in all unique checks : seen data = set ( ) for form in valid forms : row data = ( field if field in self . unique fields else form . cleaned data [ field ] for field in unique check if field in form . cleaned data ) row data = tuple ( d . get pk val ( ) if hasattr ( d , ' get pk val' ) else d for d in row data ) if row data and None not in row data : if row data in seen data : errors . append ( self . get unique error message ( unique check ) ) form . errors [ NON FIELD ERRORS ] = self . error class ( [ self . get form error ( ) ] ) for field in unique check : if field in form . cleaned data : del form . cleaned data [ field ] seen data . add ( row data ) if errors : raise Validation Error ( errors )", "predictions": ["validates that all model page page page page page page page page page page page are model prefix ."], "references": ["this clean method will check for unique_together condition"], "bleu": 0.05415315253510895, "rouge_l": 0.0}
{"id": 1836, "code": "def has changed ( self ) : if self . formsets : for formset in self . formsets . values ( ) : for form in formset . forms : if form . has changed ( ) : return True return bool ( self . changed data )", "predictions": ["returns true if any of the formset has has any tree tree ."], "references": ["return true if data differs from initial ."], "bleu": 0.1350862565735141, "rouge_l": 0.2985318107667211}
{"id": 1837, "code": "def with valid checksum ( self ) : return Address ( trytes = self . address + self . generate checksum ( ) , balance = self . balance , key index = self . key index , security level = self . security level , )", "predictions": ["generate a valid from the security in the security"], "references": ["returns the address with a valid checksum attached ."], "bleu": 0.19960198807747329, "rouge_l": 0.2222222222222222}
{"id": 1838, "code": "def generate checksum ( self ) : checksum trits = [ ] sponge = Kerl ( ) sponge . absorb ( self . address . as trits ( ) ) sponge . squeeze ( checksum trits ) checksum length = Address Checksum . LEN * TRITS PER TRYTE return Address Checksum . from trits ( checksum trits [ - checksum length : ] )", "predictions": ["get tree for this image if available if any ."], "references": ["generates the correct checksum for this address ."], "bleu": 0.17827531042796255, "rouge_l": 0.34014869888475835}
{"id": 1839, "code": "def prompt for seed ( ) : seed = secure input ( 'Enter seed and press return (typing will not be shown).\\n' 'If no seed is specified, a random one will be used instead.\\n' ) if isinstance ( seed , text type ) : seed = seed . encode ( 'ascii' ) return Seed ( seed ) if seed else Seed . random ( )", "predictions": ["item move move move seed"], "references": ["prompts the user to enter their seed via stdin ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 1840, "code": "def create sponge ( self , index ) : seed = self . seed as trits [ : ] sponge = Kerl ( ) sponge . absorb ( add trits ( seed , trits from int ( index ) ) ) sponge . squeeze ( seed ) sponge . reset ( ) sponge . absorb ( seed ) return sponge", "predictions": ["get a urls for the given index"], "references": ["prepares the hash sponge for the generator ."], "bleu": 0.21191828141393895, "rouge_l": 0.2634989200863931}
{"id": 1841, "code": "def transform ( self ) : # # state length = STATE LENGTH truth table = TRUTH TABLE # # prev state = self . state [ : ] new state = prev state [ : ] index = 0 for in range ( NUMBER OF ROUNDS ) : prev trit = prev state [ index ] for pos in range ( state length ) : index += ( 364 if index < 365 else - 365 ) new trit = prev state [ index ] new state [ pos ] = truth table [ prev trit + ( 3 * new trit ) + 4 ] prev trit = new trit prev state = new state new state = new state [ : ] self . state = new state", "predictions": ["set the message to the message message == message == 1 == 1 == 1 == 1"], "references": ["transforms internal state ."], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 1842, "code": "def full add trits ( left , right , carry ) : sum both = add trits ( left , right ) cons left = cons trits ( left , right ) cons right = cons trits ( sum both , carry ) return add trits ( sum both , carry ) , any trits ( cons left , cons right )", "predictions": ["asgi for send send"], "references": ["adds two trits together with support for a carry trit ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 1843, "code": "def resolve adapter ( uri ) : if isinstance ( uri , Base Adapter ) : return uri parsed = compat . urllib parse . urlsplit ( uri ) if not parsed . scheme : raise with context ( exc = Invalid Uri ( 'URI must begin with \"<protocol>://\" (e.g., \"udp://\").' , ) , context = { 'parsed' : parsed , 'uri' : uri , } , ) try : adapter type = adapter registry [ parsed . scheme ] except Key Error : raise with context ( exc = Invalid Uri ( 'Unrecognized protocol {protocol!r}.' . format ( protocol = parsed . scheme , ) ) , context = { 'parsed' : parsed , 'uri' : uri , } , ) return adapter type . configure ( parsed )", "predictions": ["update the binary binary binary binary tags tags tags tags tags tags tags tags tags tags tags tags tags tags tags"], "references": ["given a uri returns a properly - configured adapter instance ."], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 1844, "code": "def log ( self , level , message , context = None ) : if self . logger : self . logger . log ( level , message , extra = { 'context' : context or { } } )", "predictions": ["logs a v2 v2 into the endpoint thrift thrift thrift thrift thrift thrift thrift"], "references": ["sends a message to the instance s logger if configured ."], "bleu": 0.09782375748961449, "rouge_l": 0.16353887399463804}
{"id": 1845, "code": "def address from digest ( digest ) : address trits = [ 0 ] * ( Address . LEN * TRITS PER TRYTE ) sponge = Kerl ( ) sponge . absorb ( digest . as trits ( ) ) sponge . squeeze ( address trits ) return Address . from trits ( trits = address trits , key index = digest . key index , security level = digest . security level , )", "predictions": ["create a trits encode a digest"], "references": ["generates an address from a private key digest ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 1846, "code": "def encode ( self , input , errors = 'strict' ) : if isinstance ( input , memoryview ) : input = input . tobytes ( ) if not isinstance ( input , ( binary type , bytearray ) ) : raise with context ( exc = Type Error ( \"Can't encode {type}; byte string expected.\" . format ( type = type ( input ) . name , ) ) , context = { 'input' : input , } , ) if not isinstance ( input , bytearray ) : input = bytearray ( input ) trytes = bytearray ( ) for c in input : second , first = divmod ( c , len ( self . alphabet ) ) trytes . append ( self . alphabet [ first ] ) trytes . append ( self . alphabet [ second ] ) return binary type ( trytes ) , len ( input )", "predictions": ["encode the input and ."], "references": ["encodes a byte string into trytes ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1847, "code": "def decode ( self , input , errors = 'strict' ) : if isinstance ( input , memoryview ) : input = input . tobytes ( ) if not isinstance ( input , ( binary type , bytearray ) ) : raise with context ( exc = Type Error ( \"Can't decode {type}; byte string expected.\" . format ( type = type ( input ) . name , ) ) , context = { 'input' : input , } , ) if not isinstance ( input , bytearray ) : input = bytearray ( input ) bytes = bytearray ( ) for i in range ( 0 , len ( input ) , 2 ) : try : first , second = input [ i : i + 2 ] except Value Error : if errors == 'strict' : raise with context ( exc = Trytes Decode Error ( \"'{name}' codec can't decode value; \" \"tryte sequence has odd length.\" . format ( name = self . name , ) , ) , context = { 'input' : input , } , ) elif errors == 'replace' : bytes += b'?' continue try : bytes . append ( self . index [ first ] + ( self . index [ second ] * len ( self . index ) ) ) except Value Error : if errors == 'strict' : raise with context ( exc = Trytes Decode Error ( \"'{name}' codec can't decode trytes {pair} \" \"at position {i}-{j}: \" \"ordinal not in range(255)\" . format ( name = self . name , pair = chr ( first ) + chr ( second ) , i = i , j = i + 1 , ) , ) , context = { 'input' : input , } ) elif errors == 'replace' : bytes += b'?' return binary type ( bytes ) , len ( input )", "predictions": ["fits the sequence and return the binary type"], "references": ["decodes a tryte string into bytes ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 1848, "code": "def find addresses ( self , seed , index , count , security level , checksum ) : generator = Address Generator ( seed , security level , checksum ) if count is None : for addy in generator . create iterator ( start = index ) : response = Find Transactions Command ( self . adapter ) ( addresses = [ addy . address ] , ) if not response . get ( 'hashes' ) : return [ addy ] return generator . get addresses ( start = index , count = count )", "predictions": ["encode span span by index"], "references": ["find addresses matching the command parameters ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 1849, "code": "def as tryte string ( self ) : return Transaction Trytes ( self . signature message fragment + self . address . address + self . value as trytes + self . legacy tag + self . timestamp as trytes + self . current index as trytes + self . last index as trytes + self . bundle hash + self . trunk transaction hash + self . branch transaction hash + self . tag + self . attachment timestamp as trytes + self . attachment timestamp lower bound as trytes + self . attachment timestamp upper bound as trytes + self . nonce )", "predictions": ["return the string as a lines"], "references": ["returns a trytestring representation of the transaction ."], "bleu": 0.17516432701748888, "rouge_l": 0.13926940639269406}
{"id": 1850, "code": "def is confirmed ( self , new is confirmed ) : self . is confirmed = new is confirmed for txn in self : txn . is confirmed = new is confirmed", "predictions": ["def def of response = 0 = 1 = 0 = 0 = 1 = 1 = 0 = 1 = 0 = 1 = 1 = 1 = 0 ="], "references": ["sets the is_confirmed for the bundle ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1851, "code": "def group transactions ( self ) : groups = [ ] if self : last txn = self . tail transaction current group = [ last txn ] for current txn in self . transactions [ 1 : ] : if current txn . address == last txn . address : current group . append ( current txn ) else : groups . append ( current group ) current group = [ current txn ] last txn = current txn if current group : groups . append ( current group ) return groups", "predictions": ["in the list of limited limited to the tail"], "references": ["groups transactions in the bundle by address ."], "bleu": 0.18575057999133596, "rouge_l": 0.2378167641325536}
{"id": 1852, "code": "def errors ( self ) : try : self . errors . extend ( self . validator ) except Stop Iteration : pass return self . errors", "predictions": ["the def def url url url url url url url url url url url"], "references": ["returns all errors found with the bundle ."], "bleu": 0.08839374326825923, "rouge_l": 0.09561128526645768}
{"id": 1853, "code": "def is valid ( self ) : if not self . errors : try : self . errors . append ( next ( self . validator ) ) except Stop Iteration : pass return not self . errors", "predictions": ["return is get or not"], "references": ["returns whether the bundle is valid ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 1854, "code": "def create validator ( self ) : grouped transactions = self . bundle . group transactions ( ) bundle hash = self . bundle . hash last index = len ( self . bundle ) - 1 balance = 0 counter = 0 for group in grouped transactions : for txn in group : balance += txn . value if txn . bundle hash != bundle hash : yield 'Transaction {i} has invalid bundle hash.' . format ( i = counter , ) if txn . current index != counter : yield ( 'Transaction {i} has invalid current index value ' '(expected {i}, actual {actual}).' . format ( actual = txn . current index , i = counter , ) ) if txn . last index != last index : yield ( 'Transaction {i} has invalid last index value ' '(expected {expected}, actual {actual}).' . format ( actual = txn . last index , expected = last index , i = counter , ) ) counter += 1 if balance != 0 : yield ( 'Bundle has invalid balance ' '(expected 0, actual {actual}).' . format ( actual = balance , ) ) if not self . errors : signature validation queue = [ ] for group in grouped transactions : if group [ 0 ] . value >= 0 : continue validate group signature = True for j , txn in enumerate ( group ) : if ( j > 0 ) and ( txn . value != 0 ) : yield ( 'Transaction {i} has invalid value ' '(expected 0, actual {actual}).' . format ( actual = txn . value , i = txn . current index , ) ) validate group signature = False continue # # # if validate group signature : signature validation queue . append ( group ) if signature validation queue : for error in self . get bundle signature errors ( signature validation queue ) : yield error", "predictions": ["def the if it s invalid has"], "references": ["creates a generator that does all the work ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 1855, "code": "def start repl ( api ) : banner = ( 'IOTA API client for {uri} ({testnet}) ' 'initialized as variable `api`.\\n' 'Type `help(api)` for list of API commands.' . format ( testnet = 'testnet' if api . testnet else 'mainnet' , uri = api . adapter . get uri ( ) , ) ) scope vars = { 'api' : api } try : import I Python except Import Error : from code import Interactive Console Interactive Console ( locals = scope vars ) . interact ( banner , '' ) else : print ( banner ) I Python . start ipython ( argv = [ ] , user ns = scope vars )", "predictions": ["read the ipython api api decode the api ."], "references": ["starts the repl ."], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 1856, "code": "def Security Level ( ) : return ( f . Type ( int ) | f . Min ( 1 ) | f . Max ( 3 ) | f . Optional ( default = Address Generator . DEFAULT SECURITY LEVEL ) )", "predictions": ["return a string of the minimum value of the string ."], "references": ["generates a filter chain for validating a security level ."], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 1857, "code": "def as tryte string ( self ) : if not self . bundle hash : raise with context ( exc = Runtime Error ( 'Cannot get Tryte String representation of {cls} instance ' 'without a bundle hash; call ``bundle.finalize()`` first ' '(``exc.context`` has more info).' . format ( cls = type ( self ) . name , ) , ) , context = { 'transaction' : self , } , ) return super ( Proposed Transaction , self ) . as tryte string ( )", "predictions": ["returns the tryte object for the given bundle ."], "references": ["returns a trytestring representation of the transaction ."], "bleu": 0.16784459625186196, "rouge_l": 0.35672514619883033}
{"id": 1858, "code": "def tag ( self ) : for txn in reversed ( self ) : if txn . tag : return txn . tag return Tag ( b'' )", "predictions": ["return the tag tag of the tag ."], "references": ["determines the most relevant tag for the bundle ."], "bleu": 0.17795502018438056, "rouge_l": 0.465648854961832}
{"id": 1859, "code": "def finalize ( self ) : if self . hash : raise Runtime Error ( 'Bundle is already finalized.' ) if not self : raise Value Error ( 'Bundle has no transactions.' ) balance = self . balance if balance < 0 : if self . change address : self . add transaction ( Proposed Transaction ( address = self . change address , value = - balance , tag = self . tag , ) ) else : raise Value Error ( 'Bundle has unspent inputs (balance: {balance}); ' 'use ``send unspent inputs to`` to create ' 'change transaction.' . format ( balance = balance , ) , ) elif balance > 0 : raise Value Error ( 'Inputs are insufficient to cover bundle spend ' '(balance: {balance}).' . format ( balance = balance , ) , ) while True : sponge = Kerl ( ) last index = len ( self ) - 1 for i , txn in enumerate ( self ) : txn . current index = i txn . last index = last index sponge . absorb ( txn . get signature validation trytes ( ) . as trits ( ) ) bundle hash trits = [ 0 ] * HASH LENGTH sponge . squeeze ( bundle hash trits ) bundle hash = Bundle Hash . from trits ( bundle hash trits ) if any ( 13 in part for part in normalize ( bundle hash ) ) : tail transaction = ( self . tail transaction ) tail transaction . increment legacy tag ( ) else : break for txn in self : txn . bundle hash = bundle hash txn . signature message fragment = Fragment ( txn . message or b'' )", "predictions": ["finalize the cover and tail the bundle ."], "references": ["finalizes the bundle preparing it to be attached to the tangle ."], "bleu": 0.14544785215055717, "rouge_l": 0.28955696202531644}
{"id": 1860, "code": "def sign inputs ( self , key generator ) : if not self . hash : raise Runtime Error ( 'Cannot sign inputs until bundle is finalized.' ) i = 0 while i < len ( self ) : txn = self [ i ] if txn . value < 0 : if txn . address . key index is None : raise with context ( exc = Value Error ( 'Unable to sign input {input}; ' '``key index`` is None ' '(``exc.context`` has more info).' . format ( input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) if txn . address . security level is None : raise with context ( exc = Value Error ( 'Unable to sign input {input}; ' '``security level`` is None ' '(``exc.context`` has more info).' . format ( input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) self . sign input at ( i , key generator . get key for ( txn . address ) ) i += txn . address . security level else : i += 1", "predictions": ["sign the input inputs with the given generator ."], "references": ["sign inputs in a finalized bundle ."], "bleu": 0.16784459625186196, "rouge_l": 0.38364779874213834}
{"id": 1861, "code": "def create input transactions ( self , addy ) : self . transactions . append ( Proposed Transaction ( address = addy , tag = self . tag , value = - addy . balance , ) ) for in range ( addy . security level - 1 ) : self . transactions . append ( Proposed Transaction ( address = addy , tag = self . tag , value = 0 , ) )", "predictions": ["create transactions transactions for the given security tag ."], "references": ["creates transactions for the specified input address ."], "bleu": 0.2777619034011791, "rouge_l": 0.4756335282651072}
{"id": 1862, "code": "def decompress G1 ( z : G1Compressed ) -> G1Uncompressed : b flag = ( z % POW 2 383 ) // POW 2 382 if b flag == 1 : return Z1 x = z % POW 2 381 y = pow ( ( x ** 3 + b . n ) % q , ( q + 1 ) // 4 , q ) if pow ( y , 2 , q ) != ( x ** 3 + b . n ) % q : raise Value Error ( \"The given point is not on G1: y**2 = x**3 + b\" ) a flag = ( z % POW 2 382 ) // POW 2 381 if ( y * 2 ) // q != a flag : y = q - y return ( FQ ( x ) , FQ ( y ) , FQ ( 1 ) )", "predictions": ["decompress a g1 object from a string"], "references": ["recovers x and y coordinates from the compressed point ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 1863, "code": "def prime field inv ( a : int , n : int ) -> int : if a == 0 : return 0 lm , hm = 1 , 0 low , high = a % n , n while low > 1 : r = high // low nm , new = hm - lm * r , high - low * r lm , low , hm , high = nm , new , lm , low return lm % n", "predictions": ["return a lm of a prime field ."], "references": ["extended euclidean algorithm to find modular inverses for integers"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 1864, "code": "def repr html ( self ) : rows , c = '' , '' s = '<tr><td><strong>{k}</strong></td><td style=\"{stl}\">{v}</td></tr>' for k , v in self . dict . items ( ) : if k == ' colour' : k = 'colour' c = utils . text colour for hex ( v ) style = 'color:{}; background-color:{}' . format ( c , v ) else : style = 'color:black; background-color:white' if k == 'component' : try : v = v . repr html ( ) except Attribute Error : v = v . repr ( ) rows += s . format ( k = k , v = v , stl = style ) html = '<table>{}</table>' . format ( rows ) return html", "predictions": ["representation of the html table ."], "references": ["jupyter notebook magic repr function ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 1865, "code": "def random ( cls , component ) : colour = random . sample ( [ i for i in range ( 256 ) ] , 3 ) return cls ( { 'colour' : colour , 'component' : component , 'width' : 1.0 } )", "predictions": ["create a random component of component"], "references": ["returns a minimal decor with a random colour ."], "bleu": 0.17632778423526832, "rouge_l": 0.2573839662447257}
{"id": 1866, "code": "def repr html ( self ) : all keys = list ( set ( itertools . chain ( * [ d . keys for d in self ] ) ) ) rows = '' for decor in self : th , tr = decor . repr html row ( keys = all keys ) rows += '<tr>{}</tr>' . format ( tr ) header = '<tr>{}</tr>' . format ( th ) html = '<table>{}{}</table>' . format ( header , rows ) return html", "predictions": ["jupyter notebook magic magic magic magic ."], "references": ["jupyter notebook magic repr function ."], "bleu": 0.3655552228545123, "rouge_l": 0.6240409207161125}
{"id": 1867, "code": "def repr html ( self ) : rows = '' s = '<tr><td><strong>{k}</strong></td><td>{v}</td></tr>' for k , v in self . dict . items ( ) : rows += s . format ( k = k , v = v ) html = '<table>{}</table>' . format ( rows ) return html", "predictions": ["jupyter notebook magic magic magic magic ."], "references": ["jupyter notebook magic repr function ."], "bleu": 0.3655552228545123, "rouge_l": 0.6240409207161125}
{"id": 1868, "code": "def Rock ( * args , * * kwargs ) : with warnings . catch warnings ( ) : warnings . simplefilter ( \"always\" ) w = \"The 'Rock' class was renamed 'Component'. \" w += \"Please update your code.\" warnings . warn ( w , Deprecation Warning , stacklevel = 2 ) return Component ( * args , * * kwargs )", "predictions": ["wrap 'rock' in 'rock' . rock . rock . rock"], "references": ["graceful deprecation for old class name ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 1869, "code": "def process row ( text , columns ) : if not text : return coldict = { k : { 'start' : s , 'len' : l , 'read' : r , 'write' : w } for k , ( s , l , r , w ) in columns . items ( ) } item = { } for field in coldict : value = get field ( text , coldict , field ) if value is not None : item [ field ] = value return item", "predictions": ["process row names in row of columns ."], "references": ["processes a single row from the file ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 1870, "code": "def parse canstrat ( text ) : result = { } for row in text . split ( '\\n' ) : if not row : continue if len ( row ) < 8 : continue row header = process row ( row , columns ) or { 'card' : None } card = row header [ 'card' ] if card is not None : item = process row ( row , columns [ card ] ) this list = result . get ( card , [ ] ) this list . append ( item ) result [ card ] = this list for c , d in result . items ( ) : if len ( d ) == 1 : result [ c ] = d [ 0 ] return result", "predictions": ["parse canstrat list into a list of canstrat objects ."], "references": ["read all the rows and return a dict of the results ."], "bleu": 0.12273680279953825, "rouge_l": 0.2683284457478006}
{"id": 1871, "code": "def get template ( name ) : text = re . sub ( r'\\r\\n' , r'\\n' , name ) text = re . sub ( r'\\{([FISDE\u00b0].*?)\\}',   '{{\\1}}',   ext) return text", "predictions": ["get the template template from the name"], "references": ["still unsure about best way to do this hence cruft ."], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 1872, "code": "def clean longitudinal data ( cls , data , null = None ) : if ( 'top' not in data . keys ( ) ) : data [ 'top' ] = data . pop ( 'depth' , data . pop ( 'MD' , None ) ) idx = list ( data . keys ( ) ) . index ( 'top' ) values = sorted ( zip ( * data . values ( ) ) , key = lambda x : x [ idx ] ) data = { k : list ( v ) for k , v in zip ( data . keys ( ) , zip ( * values ) ) } if data [ 'top' ] is None : raise Striplog Error ( 'Could not get tops.' ) if null is not None : for k , v in data . items ( ) : data [ k ] = [ i if i != null else None for i in v ] return data", "predictions": ["clean longitudinal data from longitudinal ."], "references": ["private function . make sure we have what we need to make a striplog ."], "bleu": 0.04928854007377984, "rouge_l": 0.08840579710144927}
{"id": 1873, "code": "def from csv ( cls , filename = None , text = None , dlm = ',' , lexicon = None , points = False , include = None , exclude = None , remap = None , function = None , null = None , ignore = None , source = None , stop = None , fieldnames = None ) : if ( filename is None ) and ( text is None ) : raise Striplog Error ( \"You must provide a filename or CSV text.\" ) if ( filename is not None ) : if source is None : source = filename with open ( filename , 'r' ) as f : text = f . read ( ) source = source or 'CSV' if dlm == ' ' : text = re . sub ( r'[ \\t]+' , ' ' , text ) if fieldnames is not None : text = dlm . join ( fieldnames ) + '\\n' + text try : f = String IO ( text ) except Type Error : f = String IO ( unicode ( text ) ) reader = csv . Dict Reader ( f , delimiter = dlm ) reorg = { k . strip ( ) . lower ( ) : [ ] for k in reader . fieldnames if k is not None } t = f . tell ( ) for key in reorg : f . seek ( t ) for r in reader : s = { k . strip ( ) . lower ( ) : v . strip ( ) for k , v in r . items ( ) } try : reorg [ key ] . append ( float ( s [ key ] ) ) except Value Error : reorg [ key ] . append ( s [ key ] ) f . close ( ) remap = remap or { } for k , v in remap . items ( ) : reorg [ v ] = reorg . pop ( k ) data = cls . clean longitudinal data ( reorg , null = null ) list of Intervals = cls . build list of Intervals ( data , points = points , lexicon = lexicon , include = include , exclude = exclude , ignore = ignore , stop = stop ) return cls ( list of Intervals , source = source )", "predictions": ["convert csv data to a csv file"], "references": ["load from a csv file or text ."], "bleu": 0.29969770769039067, "rouge_l": 0.3952483801295896}
{"id": 1874, "code": "def from img ( cls , * args , * * kwargs ) : with warnings . catch warnings ( ) : warnings . simplefilter ( \"always\" ) w = \"from img() is deprecated; please use from image()\" warnings . warn ( w ) return cls . from image ( * args , * * kwargs )", "predictions": ["create an instance from a image ."], "references": ["for backwards compatibility ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 1875, "code": "def from canstrat ( cls , filename , source = 'canstrat' ) : with open ( filename ) as f : dat = f . read ( ) data = parse canstrat ( dat ) list of Intervals = [ ] for d in data [ 7 ] : if d . pop ( 'skip' ) : continue top = d . pop ( 'top' ) base = d . pop ( 'base' ) comps = [ Component ( { 'lithology' : d [ 'rtc' ] , 'colour' : d [ 'colour name' ] } ) ] iv = Interval ( top = top , base = base , components = comps , data = d ) list of Intervals . append ( iv ) return cls ( list of Intervals , source = source )", "predictions": ["read the top level data from a canstrat file"], "references": ["eat a canstrat dat file and make a striplog ."], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 1876, "code": "def copy ( self ) : return Striplog ( [ i . copy ( ) for i in self ] , order = self . order , source = self . source )", "predictions": ["return a copy of this instance ."], "references": ["returns a shallow copy ."], "bleu": 0.22089591134157885, "rouge_l": 0.5154929577464789}
{"id": 1877, "code": "def get data ( self , field , function = None , default = None ) : f = function or utils . null data = [ ] for iv in self : d = iv . data . get ( field ) if d is None : if default is not None : d = default else : d = np . nan data . append ( f ( d ) ) return np . array ( data )", "predictions": ["get data from the field"], "references": ["get data from the striplog ."], "bleu": 0.6221008431290531, "rouge_l": 0.7155425219941348}
{"id": 1878, "code": "def depth ( self , d ) : with warnings . catch warnings ( ) : warnings . simplefilter ( \"always\" ) w = \"depth() is deprecated; please use read at()\" warnings . warn ( w ) return self . read at ( d )", "predictions": ["return the depth of a group ."], "references": ["for backwards compatibility ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 1879, "code": "def dict repr html ( dictionary ) : rows = '' s = '<tr><td><strong>{k}</strong></td><td>{v}</td></tr>' for k , v in dictionary . items ( ) : rows += s . format ( k = k , v = v ) html = '<table>{}</table>' . format ( rows ) return html", "predictions": ["returns a string representation of the dictionary ."], "references": ["jupyter notebook magic repr function ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 1880, "code": "def convert field ( self , value , conversion ) : try : s = super ( Custom Formatter , self ) return s . convert field ( value , conversion ) except Value Error : funcs = { 's' : str , 'r' : repr , 'a' : ascii , 'u' : str . upper , 'l' : str . lower , 'c' : str . capitalize , 't' : str . title , 'm' : np . mean , '\u00b5':   p. m ean, 'v' : np . var , 'd' : np . std , '+' : np . sum , '\u2211':  n .s u m, 'x' : np . product , } return funcs . get ( conversion ) ( value )", "predictions": ["convert field to ascii representation ."], "references": ["define some extra field conversion functions ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 1881, "code": "def get random ( self , obj type ) : return self . mutator [ obj type ] [ random . randint ( 0 , self . config . level ) ]", "predictions": ["return a random object from the given type ."], "references": ["get a random mutator from a list of mutators"], "bleu": 0.19960198807747329, "rouge_l": 0.3333333333333333}
{"id": 1882, "code": "def get mutator ( self , obj , obj type ) : if obj type == unicode : obj type = str obj = str ( obj ) return self . get random ( obj type ) ( obj )", "predictions": ["return a random object ."], "references": ["get a random mutator for the given type"], "bleu": 0.1971902775417715, "rouge_l": 0.2953995157384988}
{"id": 1883, "code": "def get string polyglot attack ( self , obj ) : return self . polyglot attacks [ random . choice ( self . config . techniques ) ] % obj", "predictions": ["returns a random string of attacks ."], "references": ["return a polyglot attack containing the original object"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 1884, "code": "def safe unicode ( self , buf ) : tmp = \"\" buf = \"\" . join ( b for b in buf ) for character in buf : tmp += character return tmp", "predictions": ["return a string representation of the character ."], "references": ["safely return an unicode encoded string"], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 1885, "code": "def custom html ( self , filepath ) : try : response . headers . append ( \"Access-Control-Allow-Origin\" , \"*\" ) response . headers . append ( \"Accept-Encoding\" , \"identity\" ) response . headers . append ( \"Content-Type\" , \"text/html\" ) return static file ( filepath , root = self . config . html ) except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , \"message\" ) else str ( e ) )", "predictions": ["return a static html file containing the custom html content ."], "references": ["serve custom html page"], "bleu": 0.14991106946711685, "rouge_l": 0.2911694510739857}
{"id": 1886, "code": "def serve ( self ) : try : fuzzed = self . json . fuzzed if self . config . fuzz web : self . client queue . put ( ( request . environ . get ( 'REMOTE ADDR' ) , fuzzed ) ) response . headers . append ( \"Access-Control-Allow-Origin\" , \"*\" ) response . headers . append ( \"Accept-Encoding\" , \"identity\" ) response . headers . append ( \"Content-Type\" , self . config . content type ) if self . config . notify : PJF Testcase Server . send testcase ( fuzzed , '127.0.0.1' , self . config . ports [ \"servers\" ] [ \"TCASE PORT\" ] ) yield fuzzed except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , \"message\" ) else str ( e ) )", "predictions": ["serve the response to the client ."], "references": ["serve fuzzed json object"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 1887, "code": "def fuzz ( self , obj ) : decorators = self . decorators @ decorators . mutate object decorate def mutate ( ) : return obj return mutate ( )", "predictions": ["wrap an object into a mutate method ."], "references": ["generic fuzz mutator use a decorator for the given type"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 1888, "code": "def spawn ( self , cmd , stdin content = \"\" , stdin = False , shell = False , timeout = 2 ) : try : if type ( cmd ) != list : raise PJF Invalid Type ( type ( cmd ) , list ) if type ( stdin content ) != str : raise PJF Invalid Type ( type ( stdin content ) , str ) if type ( stdin ) != bool : raise PJF Invalid Type ( type ( stdin ) , bool ) self . in = stdin content try : self . process = subprocess . Popen ( cmd , stdout = PIPE , stderr = PIPE , stdin = PIPE , shell = shell ) self . finish read ( timeout , stdin content , stdin ) if self . process . poll ( ) is not None : self . close ( ) except Keyboard Interrupt : return except OS Error : raise PJF Process Execution Error ( \"Binary <%s> does not exist\" % cmd [ 0 ] ) except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , \"message\" ) else str ( e ) )", "predictions": ["spawn a command from the daemon ."], "references": ["spawn a new process using subprocess"], "bleu": 0.24446151121745047, "rouge_l": 0.31202046035805625}
{"id": 1889, "code": "def get output ( self , stdin content , stdin ) : try : if stdin : if sys . version info >= ( 3 , 0 ) : self . process . stdin . write ( bytes ( stdin content , \"utf-8\" ) ) else : self . process . stdin . write ( stdin content ) self . out = self . process . communicate ( ) [ 0 ] except ( error , IO Error ) : self . out = self . in pass", "predictions": ["as stdin but print out the output raise error if not ."], "references": ["try to get output in a separate thread"], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 1890, "code": "def finish read ( self , timeout = 2 , stdin content = \"\" , stdin = False ) : process = Thread ( target = self . get output , args = ( stdin content , stdin ) ) process . start ( ) if timeout > 0 : process . join ( timeout ) else : process . join ( ) if process . is alive ( ) : self . close ( ) self . return code = - signal . SIGHUP else : self . return code = self . process . returncode", "predictions": ["tag and tag the process and block until it is available ."], "references": ["wait until we got output or until timeout is over"], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 1891, "code": "def close ( self ) : try : self . process . terminate ( ) self . return code = self . process . returncode except OS Error : pass self . process . stdin . close ( ) self . process . stdout . close ( ) self . process . stderr . close ( ) self . logger . debug ( \"[{0}] - PJF Executor successfully completed\" . format ( time . strftime ( \"%H:%M:%S\" ) ) )", "predictions": ["finalize the hash hash ."], "references": ["terminate the newly created process"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 1892, "code": "def start ( self ) : from . pjf worker import PJF Worker worker = PJF Worker ( self ) if self . update pjf : worker . update library ( ) elif self . browser auto : worker . browser autopwn ( ) elif self . fuzz web : worker . web fuzzer ( ) elif self . json : if not self . web server and not self . ext fuzz and not self . cmd fuzz : worker . fuzz ( ) elif self . ext fuzz : if self . stdin : worker . fuzz stdin ( ) else : worker . fuzz command line ( ) elif self . cmd fuzz : if self . stdin : worker . fuzz external ( True ) else : worker . fuzz external ( ) else : worker . start http server ( ) elif self . json file : worker . start file fuzz ( ) elif self . process to monitor : worker . start process monitor ( )", "predictions": ["sign the worker worker server generator generator generator generator generator generator generator generator generator generator"], "references": ["parse the command line and start pyjfuzz"], "bleu": 0.08225964699966554, "rouge_l": 0.09728867623604465}
{"id": 1893, "code": "def execute ( self , obj ) : try : if self . config . stdin : self . spawn ( self . config . command , stdin content = obj , stdin = True , timeout = 1 ) else : if \"@@\" not in self . config . command : raise PJF Missing Argument ( \"Missing @@ filename indicator while using non-stdin fuzzing method\" ) for x in self . config . command : if \"@@\" in x : self . config . command [ self . config . command . index ( x ) ] = x . replace ( \"@@\" , obj ) self . spawn ( self . config . command , timeout = 2 ) self . logger . debug ( \"[{0}] - PJF External Fuzzer successfully completed\" . format ( time . strftime ( \"%H:%M:%S\" ) ) ) return self . out except Keyboard Interrupt : return \"\" except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , \"message\" ) else str ( e ) )", "predictions": ["create a @@ from the given self . append it to the given @@ ."], "references": ["perform the actual external fuzzing you may replace this method in order to increase performance"], "bleu": 0.09103526405546068, "rouge_l": 0.1333333333333333}
{"id": 1894, "code": "def shutdown ( self , * args ) : try : self . shutdown ( ) if self . process : self . process . wait ( ) self . process . stdout . close ( ) self . process . stdin . close ( ) self . process . stderr . close ( ) self . finished = True self . send testcase ( '' , '127.0.0.1' , self . config . ports [ \"servers\" ] [ \"TCASE PORT\" ] ) self . logger . debug ( \"[{0}] - PJF Process Monitor successfully completed\" . format ( time . strftime ( \"%H:%M:%S\" ) ) ) except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , \"message\" ) else str ( e ) )", "predictions": ["shut down the = daemon b b b b b b b b b b b b b b b b"], "references": ["shutdown the running process and the monitor"], "bleu": 0.05809665204409193, "rouge_l": 0.0785070785070785}
{"id": 1895, "code": "def run and monitor ( self ) : signal . signal ( signal . SIGINT , self . shutdown ) self . spawn ( self . config . process to monitor , timeout = 0 ) return self . is sigsegv ( self . return code )", "predictions": ["prime field field field n n - inv n - inv n - inv n n - inv"], "references": ["run command once and check exit code"], "bleu": 0.057259987315337726, "rouge_l": 0.0}
{"id": 1896, "code": "def start monitor ( self , standalone = True ) : try : self . start ( ) cmdline = shlex . split ( self . config . process to monitor ) if standalone : signal . signal ( signal . SIGINT , self . shutdown ) self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) while self . process and not self . finished : self . process . wait ( ) if self . is sigsegv ( self . process . returncode ) : if self . config . debug : print ( \"[\\033[92m INFO\\033[0m] Process crashed with \\033[91m SIGSEGV\\033[0m, waiting for testcase...\" ) while not self . got testcase ( ) : time . sleep ( 1 ) self . save testcase ( self . testcase [ - 10 : ] ) if self . process : self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) except OS Error : self . shutdown ( ) self . process = False self . got testcase = lambda : True raise PJF Process Execution Error ( \"Binary <%s> does not exist\" % cmdline [ 0 ] ) except Exception as e : raise PJF Base Exception ( \"Unknown error please send log to author\" )", "predictions": ["repr method to repr v s daemon s daemon s v"], "references": ["run command in a loop and check exit status plus restart process when needed"], "bleu": 0.07292088658244514, "rouge_l": 0.0}
{"id": 1897, "code": "def fuzz elements ( self , element ) : try : if type ( element ) == dict : tmp element = { } for key in element : if len ( self . config . parameters ) > 0 : if self . config . exclude parameters : fuzz = key not in self . config . parameters else : fuzz = key in self . config . parameters else : fuzz = True if fuzz : if type ( element [ key ] ) == dict : tmp element . update ( { key : self . fuzz elements ( element [ key ] ) } ) elif type ( element [ key ] ) == list : tmp element . update ( { key : self . fuzz elements ( element [ key ] ) } ) else : tmp element . update ( { key : self . mutator . fuzz ( element [ key ] ) } ) else : tmp element . update ( { key : self . fuzz elements ( element [ key ] ) } ) element = tmp element del tmp element elif type ( element ) == list : arr = [ ] for key in element : if type ( key ) == dict : arr . append ( self . fuzz elements ( key ) ) elif type ( key ) == list : arr . append ( self . fuzz elements ( key ) ) else : if len ( self . config . parameters ) <= 0 : arr . append ( self . mutator . fuzz ( key ) ) else : arr . append ( key ) element = arr del arr except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) return element", "predictions": ["convert an element into a dictionary ."], "references": ["fuzz all elements inside the object"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1898, "code": "def fuzzed ( self ) : try : if self . config . strong fuzz : fuzzer = PJF Mutators ( self . config ) if self . config . url encode : if sys . version info >= ( 3 , 0 ) : return urllib . parse . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) else : return urllib . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) else : if type ( self . config . json ) in [ list , dict ] : return fuzzer . fuzz ( json . dumps ( self . config . json ) ) else : return fuzzer . fuzz ( self . config . json ) else : if self . config . url encode : if sys . version info >= ( 3 , 0 ) : return urllib . parse . quote ( self . get fuzzed ( self . config . indent , self . config . utf8 ) ) else : return urllib . quote ( self . get fuzzed ( self . config . indent , self . config . utf8 ) ) else : return self . get fuzzed ( self . config . indent , self . config . utf8 ) except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , \"message\" ) else str ( e ) )", "predictions": ["return the json json"], "references": ["get a printable fuzzed object"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 1899, "code": "def get fuzzed ( self , indent = False , utf8 = False ) : try : if \"array\" in self . json : return self . fuzz elements ( dict ( self . json ) ) [ \"array\" ] else : return self . fuzz elements ( dict ( self . json ) ) except Exception as e : raise PJF Base Exception ( e . message if hasattr ( e , \"message\" ) else str ( e ) )", "predictions": ["repr method to repr of the entity"], "references": ["return the fuzzed object"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 1900, "code": "def mutate object decorate ( self , func ) : def mutate ( ) : obj = func ( ) return self . Mutators . get mutator ( obj , type ( obj ) ) return mutate", "predictions": ["* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"], "references": ["mutate a generic object based on type"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1901, "code": "def get User Id ( self ) : self . user Id = self ( \"GET\" , \"{0}/users/self/profile\" . format ( self . API USER ) , auth = self . Auth . Skype Token ) . json ( ) . get ( \"username\" )", "predictions": ["gets the user s user"], "references": ["ask skype for the authenticated user s identifier and store it on the connection object ."], "bleu": 0.04278081081211661, "rouge_l": 0.26105563480741795}
{"id": 1902, "code": "def sync Endpoints ( self ) : self . endpoints [ \"all\" ] = [ ] for json in self ( \"GET\" , \"{0}/users/ME/presence Docs/messaging Service\" . format ( self . msgs Host ) , params = { \"view\" : \"expanded\" } , auth = self . Auth . Reg Token ) . json ( ) . get ( \"endpoint Presence Docs\" , [ ] ) : id = json . get ( \"link\" , \"\" ) . split ( \"/\" ) [ 7 ] self . endpoints [ \"all\" ] . append ( Skype Endpoint ( self , id ) )", "predictions": ["syncs the user s endpoints from the local objects"], "references": ["retrieve all current endpoints for the connected user ."], "bleu": 0.16784459625186196, "rouge_l": 0.2222222222222222}
{"id": 1903, "code": "def u ( text , encoding = 'utf-8' ) : if isinstance ( text , six . binary type ) : text = text . decode ( encoding ) text = text . replace ( '\\r\\n' , '\\n' ) return text", "predictions": ["decode text text ."], "references": ["return unicode text no matter what"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 1904, "code": "def to dict ( self ) : d = self . metadata . copy ( ) d [ 'content' ] = self . content return d", "predictions": ["save this context into a dictionary ."], "references": ["post as a dict for serializing"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 1905, "code": "def load ( self , fm , * * kwargs ) : kwargs . setdefault ( 'Loader' , Safe Loader ) return yaml . load ( fm , * * kwargs )", "predictions": ["from a exclude object"], "references": ["parse yaml front matter . this uses yaml . safeloader by default ."], "bleu": 0.03184506239916981, "rouge_l": 0.0}
{"id": 1906, "code": "def export ( self , metadata , * * kwargs ) : kwargs . setdefault ( 'Dumper' , Safe Dumper ) kwargs . setdefault ( 'default flow style' , False ) kwargs . setdefault ( 'allow unicode' , True ) metadata = yaml . dump ( metadata , * * kwargs ) . strip ( ) return u ( metadata )", "predictions": ["from please to please ."], "references": ["export metadata as yaml . this uses yaml . safedumper by default ."], "bleu": 0.05512018958855254, "rouge_l": 0.10286677908937607}
{"id": 1907, "code": "def export ( self , metadata , * * kwargs ) : kwargs . setdefault ( 'indent' , 4 ) metadata = json . dumps ( metadata , * * kwargs ) return u ( metadata )", "predictions": ["from f to f . ."], "references": ["turn metadata into json"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 1908, "code": "def match ( self ) : cache match , cache string = self . match cache string = self . string if cache string == string : return cache match cache match = fullmatch ( LIST PATTERN FORMAT . replace ( b'{pattern}' , self . pattern . encode ( ) ) , self . shadow , MULTILINE , ) self . match cache = cache match , string return cache match", "predictions": ["return copy of the return value ."], "references": ["return the match object for the current list ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 1909, "code": "def convert ( self , newstart : str ) -> None : match = self . match ms = match . start ( ) for s , e in reversed ( match . spans ( 'pattern' ) ) : self [ s - ms : e - ms ] = newstart self . pattern = escape ( newstart )", "predictions": ["get the string with the iv f f f f f f f f f f f f f f f f f f f f f f f f f"], "references": ["convert to another list type by replacing starting pattern ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1910, "code": "def arguments ( self ) -> List [ Argument ] : shadow = self . shadow split spans = self . args matcher ( shadow ) . spans ( 'arg' ) if not split spans : return [ ] arguments = [ ] arguments append = arguments . append type to spans = self . type to spans ss , se = span = self . span type = id ( span ) lststr = self . lststr string = lststr [ 0 ] arg spans = type to spans . setdefault ( type , [ ] ) span tuple to span get = { ( s [ 0 ] , s [ 1 ] ) : s for s in arg spans } . get for arg self start , arg self end in split spans : s , e = arg span = [ ss + arg self start , ss + arg self end ] old span = span tuple to span get ( ( s , e ) ) if old span is None : insort ( arg spans , arg span ) else : arg span = old span arg = Argument ( lststr , type to spans , arg span , type ) arg . shadow cache = ( string [ s : e ] , shadow [ arg self start : arg self end ] ) arguments append ( arg ) return arguments", "predictions": ["returns the depth - first list of depth - 01 depth - 16 - 16 - 16 - 16 - 16 - 16 - 16 - 16 - 16 - 16"], "references": ["parse template content . create self . name and self . arguments ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1911, "code": "def pattern ( trie : dict ) -> str : if '' in trie : if len ( trie ) == 1 : return '' optional = True del trie [ '' ] else : optional = False subpattern to chars = defaultdict ( list ) for char , sub trie in trie . items ( ) : subpattern = pattern ( sub trie ) subpattern to chars [ subpattern ] . append ( char ) alts = [ ] for subpattern , chars in subpattern to chars . items ( ) : if len ( chars ) == 1 : alts . append ( chars [ 0 ] + subpattern ) else : chars . sort ( reverse = True ) alts . append ( '[' + '' . join ( chars ) + ']' + subpattern ) if len ( alts ) == 1 : result = alts [ 0 ] if optional : if len ( result ) == 1 : result += '?+' else : result = '(?:' + result + ')?+' else : alts . sort ( reverse = True ) result = '(?>' + '|' . join ( alts ) + ')' if optional : result += '?+' return result", "predictions": ["return a dict of html characters"], "references": ["convert a trie to a regex pattern ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 1912, "code": "def atomic partition ( self , char : int ) -> Tuple [ str , str , str ] : s , e = self . span index = self . shadow . find ( char ) if index == - 1 : return self . lststr [ 0 ] [ s : e ] , '' , '' lststr0 = self . lststr [ 0 ] return lststr0 [ s : s + index ] , chr ( char ) , lststr0 [ s + index + 1 : e ]", "predictions": ["convert a string to a character except it is in the funcs"], "references": ["partition self . string where char s not in atomic sub - spans ."], "bleu": 0.09733489823443878, "rouge_l": 0.1517412935323383}
{"id": 1913, "code": "def subspans ( self , type : str ) -> List [ List [ int ] ] : return self . type to spans [ type ]", "predictions": ["returns the list of config objects for the given type . . . ."], "references": ["return all the sub - span including self . _span ."], "bleu": 0.10511846841633776, "rouge_l": 0.24530831099195713}
{"id": 1914, "code": "def insert update ( self , index : int , length : int ) -> None : ss , se = self . span for spans in self . type to spans . values ( ) : for span in spans : if index < span [ 1 ] or span [ 1 ] == index == se : span [ 1 ] += length if index < span [ 0 ] or span [ 0 ] == index != ss : span [ 0 ] += length", "predictions": ["get the mutator at obj ."], "references": ["update self . _type_to_spans according to the added length ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 1915, "code": "def pprint ( self , indent : str = '    ' , remove comments = False ) : warn ( 'pprint method is deprecated, use pformat instead.' , Deprecation Warning , ) return self . pformat ( indent , remove comments )", "predictions": ["pretty - print the choice string"], "references": ["deprecated use self . pformat instead ."], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 1916, "code": "def parameters ( self ) -> List [ 'Parameter' ] : lststr = self . lststr type to spans = self . type to spans return [ Parameter ( lststr , type to spans , span , 'Parameter' ) for span in self . subspans ( 'Parameter' ) ]", "predictions": ["the list of safe safe safe safe safe safe safe safe safe safe safe by this model join ."], "references": ["return a list of parameter objects ."], "bleu": 0.09107438368292149, "rouge_l": 0.2517193947730399}
{"id": 1917, "code": "def parser functions ( self ) -> List [ 'Parser Function' ] : lststr = self . lststr type to spans = self . type to spans return [ Parser Function ( lststr , type to spans , span , 'Parser Function' ) for span in self . subspans ( 'Parser Function' ) ]", "predictions": ["custom custom html html html html html ."], "references": ["return a list of parser function objects ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 1918, "code": "def templates ( self ) -> List [ 'Template' ] : lststr = self . lststr type to spans = self . type to spans return [ Template ( lststr , type to spans , span , 'Template' ) for span in self . subspans ( 'Template' ) ]", "predictions": ["the list of serve serve serve serve . . ."], "references": ["return a list of templates as template objects ."], "bleu": 0.17827531042796255, "rouge_l": 0.31881533101045295}
{"id": 1919, "code": "def wikilinks ( self ) -> List [ 'Wiki Link' ] : lststr = self . lststr type to spans = self . type to spans return [ Wiki Link ( lststr , type to spans , span , 'Wiki Link' ) for span in self . subspans ( 'Wiki Link' ) ]", "predictions": ["the list of wikilinks objects ."], "references": ["return a list of wikilink objects ."], "bleu": 0.3094358155846605, "rouge_l": 0.6069651741293532}
{"id": 1920, "code": "def comments ( self ) -> List [ 'Comment' ] : lststr = self . lststr type to spans = self . type to spans return [ Comment ( lststr , type to spans , span , 'Comment' ) for span in self . subspans ( 'Comment' ) ]", "predictions": ["the list of comments ."], "references": ["return a list of comment objects ."], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 1921, "code": "def tables ( self ) -> List [ 'Table' ] : tables = [ ] tables append = tables . append type to spans = self . type to spans lststr = self . lststr shadow = self . shadow [ : ] ss , se = self . span spans = type to spans . setdefault ( 'Table' , [ ] ) if not spans : m = True while m : m = False for m in TABLE FINDITER ( shadow ) : ms , me = m . span ( ) span = [ ss + ms + len ( m [ 1 ] ) , ss + me ] spans . append ( span ) tables append ( Table ( lststr , type to spans , span , 'Table' ) ) shadow [ ms : me ] = b' ' * ( me - ms ) return tables span tuple to span get = { ( s [ 0 ] , s [ 1 ] ) : s for s in spans } . get m = True while m : m = False for m in TABLE FINDITER ( shadow ) : ms , me = m . span ( ) s , e = ss + ms + len ( m [ 1 ] ) , ss + me old span = span tuple to span get ( ( s , e ) ) if old span is None : span = [ s , e ] insort ( spans , span ) else : span = old span tables append ( Table ( lststr , type to spans , span , 'Table' ) ) shadow [ ms : me ] = b' ' * ( me - ms ) return tables", "predictions": ["list of tables in this database"], "references": ["return a list of found table objects ."], "bleu": 0.20830666398386113, "rouge_l": 0.2785388127853881}
{"id": 1922, "code": "def tags ( self , name = None ) -> List [ 'Tag' ] : lststr = self . lststr type to spans = self . type to spans if name : if name in tag extensions : string = lststr [ 0 ] return [ Tag ( lststr , type to spans , span , 'Extension Tag' ) for span in type to spans [ 'Extension Tag' ] if string . startswith ( '<' + name , span [ 0 ] ) ] tags = [ ] else : tags = [ Tag ( lststr , type to spans , span , 'Extension Tag' ) for span in type to spans [ 'Extension Tag' ] ] tags append = tags . append ss = self . span [ 0 ] shadow = self . shadow if name : reversed start matches = reversed ( [ m for m in regex compile ( START TAG PATTERN . replace ( rb'{name}' , rb'(?P<name>' + name . encode ( ) + rb')' ) ) . finditer ( shadow ) ] ) end search = regex compile ( END TAG PATTERN . replace ( b'{name}' , name . encode ( ) ) ) . search else : reversed start matches = reversed ( [ m for m in START TAG FINDITER ( shadow ) ] ) shadow copy = shadow [ : ] spans = type to spans . setdefault ( 'Tag' , [ ] ) span tuple to span get = { ( s [ 0 ] , s [ 1 ] ) : s for s in spans } . get spans append = spans . append for start match in reversed start matches : if start match [ 'self closing' ] : s , e = start match . span ( ) span = [ ss + s , ss + e ] else : if name : end match = end search ( shadow copy , start match . end ( ) ) else : end match = search ( END TAG PATTERN . replace ( b'{name}' , start match [ 'name' ] ) , shadow copy ) if end match : s , e = end match . span ( ) shadow copy [ s : e ] = b' ' * ( e - s ) span = [ ss + start match . start ( ) , ss + e ] else : s , e = start match . span ( ) span = [ ss + s , ss + e ] old span = span tuple to span get ( ( span [ 0 ] , span [ 1 ] ) ) if old span is None : spans append ( span ) else : span = old span tags append ( Tag ( lststr , type to spans , span , 'Tag' ) ) return sorted ( tags , key = attrgetter ( ' span' ) )", "predictions": ["return list of tags for this widget ."], "references": ["return all tags with the given name ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 1923, "code": "def subspans ( self , type : str ) -> Generator [ int , None , None ] : ss , se = self . span spans = self . type to spans [ type ] b = bisect ( spans , [ ss ] ) for span in spans [ b : bisect ( spans , [ se ] , b ) ] : if span [ 1 ] <= se : yield span", "predictions": ["get all span of the given type ."], "references": ["yield all the sub - span indices excluding self . _span ."], "bleu": 0.1223065774797558, "rouge_l": 0.28955696202531644}
{"id": 1924, "code": "def del arg ( self , name : str ) -> None : for arg in reversed ( self . arguments ) : if arg . name . strip ( WS ) == name . strip ( WS ) : del arg [ : ]", "predictions": ["delete an arg object from the dict ."], "references": ["delete all arguments with the given then ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 1925, "code": "def to ogc wkt ( self ) : return 'GEOGCS[\"%s\", %s, %s, %s, AXIS[\"Lon\", %s], AXIS[\"Lat\", %s]]' % ( self . name , self . datum . to ogc wkt ( ) , self . prime mer . to ogc wkt ( ) , self . angunit . to ogc wkt ( ) , self . twin ax [ 0 ] . ogc wkt , self . twin ax [ 1 ] . ogc wkt )", "predictions": ["convert the ogc condition to a ogc wkt ."], "references": ["returns the cs as a ogc wkt formatted string ."], "bleu": 0.2601435417217584, "rouge_l": 0.5213675213675214}
{"id": 1926, "code": "def to esri wkt ( self ) : return 'GEOGCS[\"%s\", %s, %s, %s, AXIS[\"Lon\", %s], AXIS[\"Lat\", %s]]' % ( self . name , self . datum . to esri wkt ( ) , self . prime mer . to esri wkt ( ) , self . angunit . to esri wkt ( ) , self . twin ax [ 0 ] . esri wkt , self . twin ax [ 1 ] . esri wkt )", "predictions": ["return equivalent wkt wkt representation of esri wkt ."], "references": ["returns the cs as a esri wkt formatted string ."], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 1927, "code": "def to ogc wkt ( self ) : string = 'PROJCS[\"%s\", %s, %s, ' % ( self . name , self . geogcs . to ogc wkt ( ) , self . proj . to ogc wkt ( ) ) string += \", \" . join ( param . to ogc wkt ( ) for param in self . params ) string += ', %s' % self . unit . to ogc wkt ( ) string += ', AXIS[\"X\", %s], AXIS[\"Y\", %s]]' % ( self . twin ax [ 0 ] . ogc wkt , self . twin ax [ 1 ] . ogc wkt ) return string", "predictions": ["return a string suitable for defining the ogc wkt ."], "references": ["returns the cs as a ogc wkt formatted string ."], "bleu": 0.20504572236241866, "rouge_l": 0.4}
{"id": 1928, "code": "def to esri wkt ( self ) : string = 'PROJCS[\"%s\", %s, %s, ' % ( self . name , self . geogcs . to esri wkt ( ) , self . proj . to esri wkt ( ) ) string += \", \" . join ( param . to esri wkt ( ) for param in self . params ) string += ', %s' % self . unit . to esri wkt ( ) string += ', AXIS[\"X\", %s], AXIS[\"Y\", %s]]' % ( self . twin ax [ 0 ] . esri wkt , self . twin ax [ 1 ] . esri wkt ) return string", "predictions": ["return a unicode object with the esri representation of this block ."], "references": ["returns the cs as a esri wkt formatted string ."], "bleu": 0.13065113298388567, "rouge_l": 0.2772727272727273}
{"id": 1929, "code": "def parse geo tiff ( key dir vlr : Geo Key Directory Vlr , double vlr : Geo Double Params Vlr , ascii vlr : Geo Ascii Params Vlr , ) -> List [ Geo Tiff Key ] : geotiff keys = [ ] for k in key dir vlr . geo keys : if k . tiff tag location == 0 : value = k . value offset elif k . tiff tag location == 34736 : value = double vlr . doubles [ k . value offset ] elif k . tiff tag location == 34737 : try : value = ascii vlr . strings [ k . value offset ] [ k . count : ] except Index Error : value = ascii vlr . strings [ 0 ] [ k . value offset : k . value offset + k . count ] else : logger . warning ( \"Geo Tiff Key with unknown tiff tag location ({})\" . format ( k . tiff tag location ) ) continue geotiff keys . append ( Geo Tiff Key ( k . id , value ) ) return geotiff keys", "predictions": ["parse the geo tiff tag from the vlr list"], "references": ["parses the geotiff vlrs information into nicer structs"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 1930, "code": "def copy fields from ( self , other record ) : for dim name in self . dimensions names : try : self [ dim name ] = other record [ dim name ] except Value Error : pass", "predictions": ["copies all fields from another record object to this record ."], "references": ["tries to copy the values of the current dimensions from other_record"], "bleu": 0.12605968092174913, "rouge_l": 0.09090909090909091}
{"id": 1931, "code": "def from stream ( cls , stream , point format , count ) : points dtype = point format . dtype point data buffer = bytearray ( stream . read ( count * points dtype . itemsize ) ) try : data = np . frombuffer ( point data buffer , dtype = points dtype , count = count ) except Value Error : expected bytes len = count * points dtype . itemsize if len ( point data buffer ) % points dtype . itemsize != 0 : missing bytes len = expected bytes len - len ( point data buffer ) raise not enough bytes error ( expected bytes len , missing bytes len , len ( point data buffer ) , points dtype , ) else : actual count = len ( point data buffer ) // points dtype . itemsize logger . critical ( \"Expected {} points, there are {} ({} missing)\" . format ( count , actual count , count - actual count ) ) data = np . frombuffer ( point data buffer , dtype = points dtype , count = actual count ) return cls ( data , point format )", "predictions": ["create a point from a stream ."], "references": ["construct the point record by reading the points from the stream"], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 1932, "code": "def x ( self ) : return scale dimension ( self . X , self . header . x scale , self . header . x offset )", "predictions": ["scale the dimension of the dimension ."], "references": ["returns the scaled x positions of the points as doubles"], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 1933, "code": "def y ( self ) : return scale dimension ( self . Y , self . header . y scale , self . header . y offset )", "predictions": ["return the dimension of this dimension ."], "references": ["returns the scaled y positions of the points as doubles"], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 1934, "code": "def z ( self ) : return scale dimension ( self . Z , self . header . z scale , self . header . z offset )", "predictions": ["return the dimension of this dimension ."], "references": ["returns the scaled z positions of the points as doubles"], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 1935, "code": "def min file version for point format ( point format id ) : for version , point formats in sorted ( VERSION TO POINT FMT . items ( ) ) : if point format id in point formats : return version else : raise errors . Point Format Not Supported ( point format id )", "predictions": ["returns the version of the point format that matches the point format ."], "references": ["returns the minimum file version that supports the given point_format_id"], "bleu": 0.14949751774990683, "rouge_l": 0.44525547445255476}
{"id": 1936, "code": "def is point fmt compatible with version ( point format id , file version ) : try : return point format id in VERSION TO POINT FMT [ str ( file version ) ] except Key Error : raise errors . File Version Not Supported ( file version )", "predictions": ["return true if point is compatible with version of point fmt ."], "references": ["returns true if the file version support the point_format_id"], "bleu": 0.14694106251955755, "rouge_l": 0.2932692307692307}
{"id": 1937, "code": "def files have same point format id ( las files ) : point format found = { las . header . point format id for las in las files } return len ( point format found ) == 1", "predictions": ["check if the files have the same point ."], "references": ["returns true if all the files have the same points format id"], "bleu": 0.38498150077635496, "rouge_l": 0.5570776255707762}
{"id": 1938, "code": "def files have same dtype ( las files ) : dtypes = { las . points . dtype for las in las files } return len ( dtypes ) == 1", "predictions": ["check if files have same dtype ."], "references": ["returns true if all the files have the same numpy datatype"], "bleu": 0.15685718045401453, "rouge_l": 0.4273204903677758}
{"id": 1939, "code": "def raise if wrong file signature ( stream ) : file sig = stream . read ( len ( headers . LAS FILE SIGNATURE ) ) if file sig != headers . LAS FILE SIGNATURE : raise errors . Pylas Error ( \"File Signature ({}) is not {}\" . format ( file sig , headers . LAS FILE SIGNATURE ) )", "predictions": ["raise a file signature if the file is not none ."], "references": ["reads the 4 first bytes of the stream to check that is lasf"], "bleu": 0.10510262682013449, "rouge_l": 0.1641991924629879}
{"id": 1940, "code": "def read header ( self ) : self . stream . seek ( self . start pos ) return headers . Header Factory ( ) . read from stream ( self . stream )", "predictions": ["read the header from the stream ."], "references": ["reads the head of the las file and returns it"], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 1941, "code": "def read vlrs ( self ) : self . stream . seek ( self . start pos + self . header . size ) return VLR List . read from ( self . stream , num to read = self . header . number of vlr )", "predictions": ["read the vlrs from the stream ."], "references": ["reads and return the vlrs of the file"], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 1942, "code": "def read compressed points data ( self , laszip vlr , point format ) : offset to chunk table = struct . unpack ( \"<q\" , self . stream . read ( 8 ) ) [ 0 ] size of point data = offset to chunk table - self . stream . tell ( ) if offset to chunk table <= 0 : logger . warning ( \"Strange offset to chunk table: {}, ignoring it..\" . format ( offset to chunk table ) ) size of point data = - 1 points = record . Packed Point Record . from compressed buffer ( self . stream . read ( size of point data ) , point format , self . header . point count , laszip vlr , ) return points", "predictions": ["read compressed points from the stream ."], "references": ["reads the compressed point record"], "bleu": 0.20556680845025982, "rouge_l": 0.17183098591549298}
{"id": 1943, "code": "def read internal waveform packet ( self ) : b = bytearray ( self . stream . read ( rawvlr . VLR HEADER SIZE ) ) waveform header = rawvlr . Raw VLR Header . from buffer ( b ) waveform record = self . stream . read ( ) logger . debug ( \"Read: {} M Bytes of waveform record\" . format ( len ( waveform record ) / 10 ** 6 ) ) return waveform header , waveform record", "predictions": ["read internal waveform packet packet"], "references": ["reads and returns the waveform vlr header waveform record"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 1944, "code": "def warn if not at expected pos ( self , expected pos , end of , start of ) : diff = expected pos - self . stream . tell ( ) if diff != 0 : logger . warning ( \"There are {} bytes between {} and {}\" . format ( diff , end of , start of ) )", "predictions": ["warn if the stream is at the current position ."], "references": ["helper function to warn about unknown bytes found in the file"], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 1945, "code": "def date ( self , date ) : self . creation year = date . year self . creation day of year = date . timetuple ( ) . tm yday", "predictions": ["set the date of the date of the given date"], "references": ["returns the date of file creation as a python date object"], "bleu": 0.22447582175704436, "rouge_l": 0.37770897832817335}
{"id": 1946, "code": "def mins ( self ) : return np . array ( [ self . x min , self . y min , self . z min ] )", "predictions": ["minimum value of the functional ."], "references": ["returns de minimum values of x y z as a numpy array"], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 1947, "code": "def mins ( self , value ) : self . x min , self . y min , self . z min = value", "predictions": ["set the mins and reset _block_matcher ."], "references": ["sets de minimum values of x y z as a numpy array"], "bleu": 0.07646493705380929, "rouge_l": 0.0}
{"id": 1948, "code": "def maxs ( self ) : return np . array ( [ self . x max , self . y max , self . z max ] )", "predictions": ["maxs - value of the maxs"], "references": ["returns de maximum values of x y z as a numpy array"], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 1949, "code": "def maxs ( self , value ) : self . x max , self . y max , self . z max = value", "predictions": ["set the maxs object"], "references": ["sets de maximum values of x y z as a numpy array"], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 1950, "code": "def scales ( self ) : return np . array ( [ self . x scale , self . y scale , self . z scale ] )", "predictions": ["returns the scales of this vector ."], "references": ["returns the scaling values of x y z as a numpy array"], "bleu": 0.1285981829222983, "rouge_l": 0.30148270181219106}
{"id": 1951, "code": "def offsets ( self ) : return np . array ( [ self . x offset , self . y offset , self . z offset ] )", "predictions": ["the offsets of this vector ."], "references": ["returns the offsets values of x y z as a numpy array"], "bleu": 0.11492332782473744, "rouge_l": 0.31443298969072164}
{"id": 1952, "code": "def num extra bytes ( self ) : return sum ( np . dtype ( extra dim [ 1 ] ) . itemsize for extra dim in self . extra dims )", "predictions": ["the total number of self . in the domain . ."], "references": ["returns the number of extra bytes"], "bleu": 0.16108992769687397, "rouge_l": 0.3727087576374745}
{"id": 1953, "code": "def has waveform packet ( self ) : dimensions = set ( self . dimension names ) return all ( name in dimensions for name in dims . WAVEFORM FIELDS NAMES )", "predictions": ["returns true if all append have any waveform = true"], "references": ["returns true if the point format has waveform packet dimensions"], "bleu": 0.24808415001701817, "rouge_l": 0.4}
{"id": 1954, "code": "def main ( port , ip , command , loglevel ) : numeric level = getattr ( logging , loglevel . upper ( ) , None ) if not isinstance ( numeric level , int ) : raise Value Error ( 'Invalid log level: %s' % loglevel ) logging . basic Config ( level = numeric level ) click . echo ( \"Demo of satel integra library\" ) if command == \"demo\" : demo ( ip , port )", "predictions": ["tags a matches command line tool ."], "references": ["console script for satel_integra ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 1955, "code": "def checksum ( command ) : crc = 0x147A for b in command : crc = ( ( crc << 1 ) & 0x FFFF ) | ( crc & 0x8000 ) >> 15 crc = crc ^ 0x FFFF crc = ( crc + ( crc >> 8 ) + b ) & 0x FFFF return crc", "predictions": ["compute the checksum checksum checksum ."], "references": ["function to calculate checksum as per satel manual ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 1956, "code": "def print hex ( data ) : hex msg = \"\" for c in data : hex msg += \"\\\\x\" + format ( c , \"02x\" ) LOGGER . debug ( hex msg )", "predictions": ["del a arg to a arg"], "references": ["debugging method to print out frames in hex ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 1957, "code": "def verify and strip ( resp ) : if resp [ 0 : 2 ] != b'\\x FE\\x FE' : LOGGER . error ( \"Houston, we got problem:\" ) print hex ( resp ) raise Exception ( \"Wrong header - got %X%X\" % ( resp [ 0 ] , resp [ 1 ] ) ) if resp [ - 2 : ] != b'\\x FE\\x0D' : raise Exception ( \"Wrong footer - got %X%X\" % ( resp [ - 2 ] , resp [ - 1 ] ) ) output = resp [ 2 : - 2 ] . replace ( b'\\x FE\\x F0' , b'\\x FE' ) c = checksum ( bytearray ( output [ 0 : - 2 ] ) ) if ( 256 * output [ - 2 : - 1 ] [ 0 ] + output [ - 1 : ] [ 0 ] ) != c : raise Exception ( \"Wrong checksum - got %d expected %d\" % ( ( 256 * output [ - 2 : - 1 ] [ 0 ] + output [ - 1 : ] [ 0 ] ) , c ) ) return output [ 0 : - 2 ]", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["verify checksum and strip header and footer of received frame ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1958, "code": "def generate query ( command ) : data = bytearray ( command ) c = checksum ( data ) data . append ( c >> 8 ) data . append ( c & 0x FF ) data . replace ( b'\\x FE' , b'\\x FE\\x F0' ) data = bytearray . fromhex ( \"FEFE\" ) + data + bytearray . fromhex ( \"FE0D\" ) return data", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["add header checksum and footer to command data ."], "bleu": 0.03901663112717908, "rouge_l": 0.055505004549590536}
{"id": 1959, "code": "def demo ( host , port ) : loop = asyncio . get event loop ( ) stl = Async Satel ( host , port , loop , [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 25 , 26 , 27 , 28 , 29 , 30 ] , [ 8 , 9 , 10 ] ) loop . run until complete ( stl . connect ( ) ) loop . create task ( stl . arm ( \"3333\" , 1 ) ) loop . create task ( stl . disarm ( \"3333\" ) ) loop . create task ( stl . keep alive ( ) ) loop . create task ( stl . monitor status ( ) ) loop . run forever ( ) loop . close ( )", "predictions": ["start the to the to to the to the to the to the to the to the to the to the to the to the to the to the to run"], "references": ["basic demo of the monitoring capabilities ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 1960, "code": "async def connect ( self ) : LOGGER . debug ( \"Connecting...\" ) try : self . reader , self . writer = await asyncio . open connection ( self . host , self . port , loop = self . loop ) LOGGER . debug ( \"sucess connecting...\" ) except Exception as e : LOGGER . warning ( \"Exception during connecting: %s.\" , e ) self . writer = None self . reader = None return False return True", "predictions": ["esri to the connection server string string string string string string string string string string string string string string string string string string string string string string string string string string"], "references": ["make a tcp connection to the alarm system ."], "bleu": 0.055177848898164926, "rouge_l": 0.11101000909918107}
{"id": 1961, "code": "async def start monitoring ( self ) : data = generate query ( b'\\x7F\\x01\\x DC\\x99\\x80\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00' ) await self . send data ( data ) resp = await self . read data ( ) if resp is None : LOGGER . warning ( \"Start monitoring - no data!\" ) return if resp [ 1 : 2 ] != b'\\x FF' : LOGGER . warning ( \"Monitoring not accepted.\" )", "predictions": ["geo tiff tiff . . ."], "references": ["start monitoring for interesting events ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 1962, "code": "async def disarm ( self , code , partition list ) : LOGGER . info ( \"Sending disarm command.\" ) while len ( code ) < 16 : code += 'F' code bytes = bytearray . fromhex ( code ) data = generate query ( b'\\x84' + code bytes + partition bytes ( partition list ) ) await self . send data ( data )", "predictions": ["send a other other command"], "references": ["send command to disarm ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 1963, "code": "async def clear alarm ( self , code , partition list ) : LOGGER . info ( \"Sending clear the alarm command.\" ) while len ( code ) < 16 : code += 'F' code bytes = bytearray . fromhex ( code ) data = generate query ( b'\\x85' + code bytes + partition bytes ( partition list ) ) await self . send data ( data )", "predictions": ["stream alarm alarm alarm"], "references": ["send command to clear the alarm ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 1964, "code": "async def set output ( self , code , output id , state ) : LOGGER . debug ( \"Turn on, output: %s, code: %s\" , output id , code ) while len ( code ) < 16 : code += 'F' code bytes = bytearray . fromhex ( code ) mode command = 0x88 if state else 0x89 data = generate query ( mode command . to bytes ( 1 , 'big' ) + code bytes + output bytes ( output id ) ) await self . send data ( data )", "predictions": ["set self offset offset offset offset offset"], "references": ["send output turn on command to the alarm ."], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 1965, "code": "def close ( self ) : LOGGER . debug ( \"Closing...\" ) self . closed = True if self . connected : self . writer . close ( )", "predictions": ["y - y - process scale scale scale scale scale"], "references": ["stop monitoring and close connection ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 1966, "code": "def purge db ( self ) : with self . engine . begin ( ) as db : purge user ( db , self . user id )", "predictions": ["remove the header of the header dimension dimension dimension dimension dimension dimension dimension dimension dimension dimension dimension dimension dimension dimension dimension dimension dimension dimension dimension dimension dimension dimension dimension dimension dimension"], "references": ["clear all matching our user_id ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 1967, "code": "def get notebook ( self , path , content , format ) : with self . engine . begin ( ) as db : try : record = get file ( db , self . user id , path , content , self . crypto . decrypt , ) except No Such File : self . no such entity ( path ) return self . notebook model from db ( record , content )", "predictions": ["min a file from from"], "references": ["get a notebook from the database ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 1968, "code": "def notebook model from db ( self , record , content ) : path = to api path ( record [ 'parent name' ] + record [ 'name' ] ) model = base model ( path ) model [ 'type' ] = 'notebook' model [ 'last modified' ] = model [ 'created' ] = record [ 'created at' ] if content : content = reads base64 ( record [ 'content' ] ) self . mark trusted cells ( content , path ) model [ 'content' ] = content model [ 'format' ] = 'json' self . validate notebook model ( model ) return model", "predictions": ["create is called when a is created ."], "references": ["build a notebook model from database record ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 1969, "code": "def get directory ( self , path , content , format ) : with self . engine . begin ( ) as db : try : record = get directory ( db , self . user id , path , content ) except No Such Directory : if self . file exists ( path ) : self . do 400 ( \"Wrong type: %s\" % path ) else : self . no such entity ( path ) return self . directory model from db ( record , content )", "predictions": ["files files in the cache have the given las = value"], "references": ["get a directory from the database ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 1970, "code": "def directory model from db ( self , record , content ) : model = base directory model ( to api path ( record [ 'name' ] ) ) if content : model [ 'format' ] = 'json' model [ 'content' ] = list ( chain ( self . convert file records ( record [ 'files' ] ) , ( self . directory model from db ( subdir , False ) for subdir in record [ 'subdirs' ] ) , ) ) return model", "predictions": ["build a have been created from a files in the database"], "references": ["build a directory model from database directory record ."], "bleu": 0.17033186037639278, "rouge_l": 0.4073455759599332}
{"id": 1971, "code": "def file model from db ( self , record , content , format ) : path = to api path ( record [ 'parent name' ] + record [ 'name' ] ) model = base model ( path ) model [ 'type' ] = 'file' model [ 'last modified' ] = model [ 'created' ] = record [ 'created at' ] if content : bcontent = record [ 'content' ] model [ 'content' ] , model [ 'format' ] , model [ 'mimetype' ] = from b64 ( path , bcontent , format , ) return model", "predictions": ["create a raise if the if it doesn t exist ."], "references": ["build a file model from database record ."], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 1972, "code": "def save file ( self , db , model , path ) : save file ( db , self . user id , path , to b64 ( model [ 'content' ] , model . get ( 'format' , None ) ) , self . crypto . encrypt , self . max file size bytes , ) return None", "predictions": ["read a header from the database"], "references": ["save a non - notebook file ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 1973, "code": "def delete file ( self , path ) : if self . file exists ( path ) : self . delete non directory ( path ) elif self . dir exists ( path ) : self . delete directory ( path ) else : self . no such entity ( path )", "predictions": ["read a file header"], "references": ["delete object corresponding to path ."], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 1974, "code": "def ensure db user ( db , user id ) : with ignore unique violation ( ) : db . execute ( users . insert ( ) . values ( id = user id ) , )", "predictions": ["read a points in the database ."], "references": ["add a new user if they don t already exist ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 1975, "code": "def purge user ( db , user id ) : db . execute ( files . delete ( ) . where ( files . c . user id == user id ) ) db . execute ( directories . delete ( ) . where ( directories . c . user id == user id ) ) db . execute ( users . delete ( ) . where ( users . c . id == user id ) )", "predictions": ["remove a internal internal internal internal internal internal internal method"], "references": ["delete a user and all of their resources ."], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 1976, "code": "def create directory ( db , user id , api path ) : name = from api dirname ( api path ) if name == '/' : parent name = null ( ) parent user id = null ( ) else : parent name = name [ : name . rindex ( '/' , 0 , - 1 ) + 1 ] parent user id = user id db . execute ( directories . insert ( ) . values ( name = name , user id = user id , parent name = parent name , parent user id = parent user id , ) )", "predictions": ["warn if the pos is not created"], "references": ["create a directory ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 1977, "code": "def delete directory ( db , user id , api path ) : db dirname = from api dirname ( api path ) try : result = db . execute ( directories . delete ( ) . where ( and ( directories . c . user id == user id , directories . c . name == db dirname , ) ) ) except Integrity Error as error : if is foreign key violation ( error ) : raise Directory Not Empty ( api path ) else : raise rowcount = result . rowcount if not rowcount : raise No Such Directory ( api path ) return rowcount", "predictions": ["date a directory from the database"], "references": ["delete a directory ."], "bleu": 0.2907153684841096, "rouge_l": 0.4149659863945578}
{"id": 1978, "code": "def files in directory ( db , user id , db dirname ) : fields = file default fields ( ) rows = db . execute ( select ( fields , ) . where ( is in directory ( files , user id , db dirname ) , ) . order by ( files . c . user id , files . c . parent name , files . c . name , files . c . created at , ) . distinct ( files . c . user id , files . c . parent name , files . c . name , ) ) return [ to dict no content ( fields , row ) for row in rows ]", "predictions": ["return in list of mins at in . ."], "references": ["return files in a directory ."], "bleu": 0.16784459625186196, "rouge_l": 0.4149659863945578}
{"id": 1979, "code": "def directories in directory ( db , user id , db dirname ) : fields = directory default fields ( ) rows = db . execute ( select ( fields , ) . where ( is in directory ( directories , user id , db dirname ) , ) ) return [ to dict no content ( fields , row ) for row in rows ]", "predictions": ["return list of mins in in ."], "references": ["return subdirectories of a directory ."], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 1980, "code": "def file where ( user id , api path ) : directory , name = split api filepath ( api path ) return and ( files . c . name == name , files . c . user id == user id , files . c . parent name == directory , )", "predictions": ["return for the file"], "references": ["return a where clause matching the given api path and user_id ."], "bleu": 0.0538140946637381, "rouge_l": 0.22932330827067668}
{"id": 1981, "code": "def select file ( user id , api path , fields , limit ) : query = select ( fields ) . where ( file where ( user id , api path ) , ) . order by ( file creation order ( ) , ) if limit is not None : query = query . limit ( limit ) return query", "predictions": ["select a file by its max max max and max ."], "references": ["return a select statement that returns the latest n versions of a file ."], "bleu": 0.1296737111713457, "rouge_l": 0.31322207958921694}
{"id": 1982, "code": "def file default fields ( ) : return [ files . c . name , files . c . created at , files . c . parent name , ]", "predictions": ["np . for all y self . . . . . . . . . . . . . . . . . . . . . ."], "references": ["default fields returned by a file query ."], "bleu": 0.04327969719414172, "rouge_l": 0.0617408906882591}
{"id": 1983, "code": "def file exists ( db , user id , path ) : try : get file ( db , user id , path , include content = False , decrypt func = unused decrypt func , ) return True except No Such File : return False", "predictions": ["check if a offsets offsets exists ."], "references": ["check if a file exists ."], "bleu": 0.4111336169005197, "rouge_l": 0.7800511508951408}
{"id": 1984, "code": "def rename file ( db , user id , old api path , new api path ) : if file exists ( db , user id , new api path ) : raise File Exists ( new api path ) old dir , old name = split api filepath ( old api path ) new dir , new name = split api filepath ( new api path ) if old dir != new dir : raise Value Error ( dedent ( . format ( old api path = old api path , new api path = new api path ) ) ) db . execute ( files . update ( ) . where ( file where ( user id , old api path ) , ) . values ( name = new name , created at = func . now ( ) , ) )", "predictions": ["rename a file in the current directory ."], "references": ["rename a file ."], "bleu": 0.3155984539112945, "rouge_l": 0.7093023255813954}
{"id": 1985, "code": "def rename directory ( db , user id , old api path , new api path ) : old db path = from api dirname ( old api path ) new db path = from api dirname ( new api path ) if old db path == '/' : raise Rename Root ( 'Renaming the root directory is not permitted.' ) if dir exists ( db , user id , new db path ) : raise Directory Exists ( new api path ) db . execute ( 'SET CONSTRAINTS ' 'pgcontents.directories parent user id fkey DEFERRED' ) db . execute ( directories . update ( ) . where ( and ( directories . c . user id == user id , directories . c . name == old db path , ) ) . values ( name = new db path , ) ) db . execute ( directories . update ( ) . where ( and ( directories . c . user id == user id , directories . c . name . startswith ( old db path ) , directories . c . parent name . startswith ( old db path ) , ) ) . values ( name = func . concat ( new db path , func . right ( directories . c . name , - func . length ( old db path ) ) ) , parent name = func . concat ( new db path , func . right ( directories . c . parent name , - func . length ( old db path ) ) ) , ) )", "predictions": ["rename a directory in the current db"], "references": ["rename a directory ."], "bleu": 0.345720784641941, "rouge_l": 0.5736677115987461}
{"id": 1986, "code": "def purge remote checkpoints ( db , user id ) : db . execute ( remote checkpoints . delete ( ) . where ( remote checkpoints . c . user id == user id , ) )", "predictions": ["remove remote checkpoints from remote device ."], "references": ["delete all database records for the given user_id ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 1987, "code": "def reencrypt row content ( db , table , row id , decrypt func , encrypt func , logger ) : q = ( select ( [ table . c . content ] ) . with for update ( ) . where ( table . c . id == row id ) ) [ ( content , ) ] = db . execute ( q ) logger . info ( \"Begin encrypting %s row %s.\" , table . name , row id ) db . execute ( table . update ( ) . where ( table . c . id == row id ) . values ( content = encrypt func ( decrypt func ( content ) ) ) ) logger . info ( \"Done encrypting %s row %s.\" , table . name , row id )", "predictions": ["reencrypt row content of reencrypt ."], "references": ["re - encrypt a row from table with id of row_id ."], "bleu": 0.09663861439684919, "rouge_l": 0.31443298969072164}
{"id": 1988, "code": "def select file ids ( db , user id ) : return list ( db . execute ( select ( [ files . c . id ] ) . where ( files . c . user id == user id ) ) )", "predictions": ["select the ids of a file ."], "references": ["get all file ids for a user ."], "bleu": 0.2024757945132846, "rouge_l": 0.3952483801295896}
{"id": 1989, "code": "def select remote checkpoint ids ( db , user id ) : return list ( db . execute ( select ( [ remote checkpoints . c . id ] ) . where ( remote checkpoints . c . user id == user id ) ) )", "predictions": ["select remote checkpoint ids ."], "references": ["get all file ids for a user ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 1990, "code": "def reencrypt user content ( engine , user id , old decrypt func , new encrypt func , logger ) : logger . info ( \"Begin re-encryption for user %s\" , user id ) with engine . begin ( ) as db : logger . info ( \"Re-encrypting files for %s\" , user id ) for ( file id , ) in select file ids ( db , user id ) : reencrypt row content ( db , files , file id , old decrypt func , new encrypt func , logger , ) logger . info ( \"Re-encrypting checkpoints for %s\" , user id ) for ( cp id , ) in select remote checkpoint ids ( db , user id ) : reencrypt row content ( db , remote checkpoints , cp id , old decrypt func , new encrypt func , logger , ) logger . info ( \"Finished re-encryption for user %s\" , user id )", "predictions": ["decrypt a remote user s content in the remote db ."], "references": ["re - encrypt all of the files and checkpoints for a single user ."], "bleu": 0.10904215887663019, "rouge_l": 0.2349165596919127}
{"id": 1991, "code": "def memoize single arg ( f ) : memo = { } @ wraps ( f ) def memoized f ( arg ) : try : return memo [ arg ] except Key Error : result = memo [ arg ] = f ( arg ) return result return memoized f", "predictions": ["decorator to memoize a function argument argument ."], "references": ["decorator memoizing a single - argument function"], "bleu": 0.20164945583740668, "rouge_l": 0.4048672566371681}
{"id": 1992, "code": "def delete checkpoint ( self , checkpoint id , path ) : with self . engine . begin ( ) as db : return delete single remote checkpoint ( db , self . user id , path , checkpoint id , )", "predictions": ["delete a checkpoint ."], "references": ["delete a checkpoint for a file"], "bleu": 0.4056114983537769, "rouge_l": 0.5791139240506329}
{"id": 1993, "code": "def get checkpoint content ( self , checkpoint id , path ) : with self . engine . begin ( ) as db : return get remote checkpoint ( db , self . user id , path , checkpoint id , self . crypto . decrypt , ) [ 'content' ]", "predictions": ["get the content of a checkpoint ."], "references": ["get the content of a checkpoint ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 1994, "code": "def list checkpoints ( self , path ) : with self . engine . begin ( ) as db : return list remote checkpoints ( db , self . user id , path )", "predictions": ["returns a list of remote checkpoints for the given path ."], "references": ["return a list of checkpoints for a given file"], "bleu": 0.2620251007173263, "rouge_l": 0.6110183639398998}
{"id": 1995, "code": "def rename all checkpoints ( self , old path , new path ) : with self . engine . begin ( ) as db : return move remote checkpoints ( db , self . user id , old path , new path , )", "predictions": ["rename all checkpoints in a remote db ."], "references": ["rename all checkpoints for old_path to new_path ."], "bleu": 0.3155984539112945, "rouge_l": 0.5}
{"id": 1996, "code": "def delete all checkpoints ( self , path ) : with self . engine . begin ( ) as db : delete remote checkpoints ( db , self . user id , path )", "predictions": ["delete all checkpoints from the database ."], "references": ["delete all checkpoints for the given path ."], "bleu": 0.33167003447658744, "rouge_l": 0.6587473002159828}
{"id": 1997, "code": "def purge db ( self ) : with self . engine . begin ( ) as db : purge remote checkpoints ( db , self . user id )", "predictions": ["purge the remote db ."], "references": ["purge all database records for the current user ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 1998, "code": "def apply prefix ( prefix , model ) : if not isinstance ( model , dict ) : raise Type Error ( \"Expected dict for model, got %s\" % type ( model ) ) model [ 'path' ] = '/' . join ( ( prefix , model [ 'path' ] ) ) . strip ( '/' ) if model [ 'type' ] in ( 'notebook' , 'file' ) : return model if model [ 'type' ] != 'directory' : raise Value Error ( \"Unknown model type %s.\" % type ( model ) ) content = model . get ( 'content' , None ) if content is not None : for sub model in content : apply prefix ( prefix , sub model ) return model", "predictions": ["apply prefix to model prefix ."], "references": ["prefix all path entries in model with the given prefix ."], "bleu": 0.1435549295013305, "rouge_l": 0.4468864468864468}
{"id": 1999, "code": "def path dispatch1 ( mname , returns model ) : def wrapper ( self , * args , * * kwargs ) : path , args = get arg ( 'path' , args , kwargs ) prefix , mgr , mgr path = resolve path ( path , self . managers ) result = getattr ( mgr , mname ) ( mgr path , * args , * * kwargs ) if returns model and prefix : return apply prefix ( prefix , result ) else : return result return wrapper", "predictions": ["decorator for dispatch1 dispatch1 ."], "references": ["decorator for methods that accept path as a first argument ."], "bleu": 0.11629030063732083, "rouge_l": 0.35124760076775424}
{"id": 2000, "code": "def path dispatch old new ( mname , returns model ) : def wrapper ( self , old path , new path , * args , * * kwargs ) : old prefix , old mgr , old mgr path = resolve path ( old path , self . managers ) new prefix , new mgr , new mgr path = resolve path ( new path , self . managers , ) if old mgr is not new mgr : raise HTTP Error ( 400 , \"Can't move files between backends ({old} -> {new})\" . format ( old = old path , new = new path , ) ) assert new prefix == old prefix result = getattr ( new mgr , mname ) ( old mgr path , new mgr path , * args , * * kwargs ) if returns model and new prefix : return apply prefix ( new prefix , result ) else : return result return wrapper", "predictions": ["dispatch a function between old and new prefix ."], "references": ["decorator for methods accepting old_path and new_path ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 2001, "code": "def managers changed ( self , name , old , new ) : for key in new : if '/' in key : raise Value Error ( \"Expected directory names w/o slashes.  Got [%s]\" % key ) self . managers = { k . strip ( '/' ) : v for k , v in new . items ( ) }", "predictions": ["changes the value of a directory that has changed ."], "references": ["strip slashes from directories before updating ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 2002, "code": "def get ( self , path , content = True , type = None , format = None ) : path = normalize api path ( path ) if path : return self . get ( path , content = content , type = type , format = format ) if not content : return base directory model ( '' ) extra content = self . extra root dirs ( ) rm = self . root manager if rm is None : root model = base directory model ( '' ) root model . update ( format = 'json' , content = extra content , ) else : root model = rm . get ( path , content = content , type = type , format = format , ) root model [ 'content' ] . extend ( extra content ) return root model", "predictions": ["return the root model for the given path ."], "references": ["special case handling for listing root dir ."], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 2003, "code": "def split api filepath ( path ) : parts = path . rsplit ( '/' , 1 ) if len ( parts ) == 1 : name = parts [ 0 ] dirname = '/' else : name = parts [ 1 ] dirname = parts [ 0 ] + '/' return from api dirname ( dirname ) , name", "predictions": ["split api api api api filepath into a directory"], "references": ["split an api file path into directory and name ."], "bleu": 0.15881076016027915, "rouge_l": 0.41709401709401706}
{"id": 2004, "code": "def writes base64 ( nb , version = NBFORMAT VERSION ) : return b64encode ( writes ( nb , version = version ) . encode ( 'utf-8' ) )", "predictions": ["encode an writes string to a base64 - encoded string ."], "references": ["write a notebook as base64 ."], "bleu": 0.1354599427337814, "rouge_l": 0.3727087576374745}
{"id": 2005, "code": "def reads base64 ( nb , as version = NBFORMAT VERSION ) : try : return reads ( b64decode ( nb ) . decode ( 'utf-8' ) , as version = as version ) except Exception as e : raise Corrupted File ( e )", "predictions": ["decode reads from base64 encoded string"], "references": ["read a notebook from base64 ."], "bleu": 0.2907153684841096, "rouge_l": 0.3333333333333333}
{"id": 2006, "code": "def prefix dirs ( path ) : dirname = posixpath . dirname path = path . strip ( '/' ) out = [ ] while path != '' : path = dirname ( path ) out . append ( path ) return reversed ( out )", "predictions": ["return a list of all prefix in the given path ."], "references": ["return an iterable of all prefix directories of path descending from root ."], "bleu": 0.2033038389239877, "rouge_l": 0.4925975773889637}
{"id": 2007, "code": "def outside root to 404 ( fn ) : @ wraps ( fn ) def wrapped ( * args , * * kwargs ) : try : return fn ( * args , * * kwargs ) except Path Outside Root as e : raise HTTP Error ( 404 , \"Path outside root: [%s]\" % e . args [ 0 ] ) return wrapped", "predictions": ["decorator to outside the root root root root ."], "references": ["decorator for converting pathoutsideroot errors to 404s ."], "bleu": 0.16784459625186196, "rouge_l": 0.35672514619883033}
{"id": 2008, "code": "def create user ( db url , user ) : Postgres Checkpoints ( db url = db url , user id = user , create user on startup = True , )", "predictions": ["create a user ."], "references": ["create a user ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 2009, "code": "def walk dirs ( mgr , dirs ) : for directory in dirs : children = mgr . get ( directory , content = True , type = 'directory' , ) [ 'content' ] dirs , files = map ( sorted , separate dirs files ( children ) ) yield directory , dirs , files if dirs : for entry in walk dirs ( mgr , dirs ) : yield entry", "predictions": ["walk directories and recursively recursively recursively recursively recursively recursively recursively recursively walk directories ."], "references": ["recursive helper for walk ."], "bleu": 0.09782375748961449, "rouge_l": 0.23018867924528305}
{"id": 2010, "code": "def walk files ( mgr ) : for dir , subdirs , files in walk files ( mgr ) : for file in files : yield file", "predictions": ["walk all files in a directory ."], "references": ["iterate over all files visible to mgr ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 2011, "code": "def walk files with content ( mgr ) : for , , files in walk ( mgr ) : for f in files : yield mgr . get ( f , content = True )", "predictions": ["walk files with content ."], "references": ["iterate over the contents of all files visible to mgr ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 2012, "code": "def reencrypt single user ( engine , user id , old crypto , new crypto , logger ) : crypto = Fallback Crypto ( [ new crypto , old crypto ] ) reencrypt user content ( engine = engine , user id = user id , old decrypt func = crypto . decrypt , new encrypt func = crypto . encrypt , logger = logger , )", "predictions": ["decrypt a single user"], "references": ["re - encrypt all files and checkpoints for a single user ."], "bleu": 0.09050415858572283, "rouge_l": 0.34398496240601506}
{"id": 2013, "code": "def unencrypt single user ( engine , user id , old crypto , logger ) : reencrypt user content ( engine = engine , user id = user id , old decrypt func = old crypto . decrypt , new encrypt func = lambda s : s , logger = logger , )", "predictions": ["decrypt a single user in the single user ."], "references": ["unencrypt all files and checkpoints for a single user ."], "bleu": 0.29558013016570783, "rouge_l": 0.41709401709401706}
{"id": 2014, "code": "def upgrade ( db url , revision ) : with temp alembic ini ( ALEMBIC DIR LOCATION , db url ) as alembic ini : subprocess . check call ( [ 'alembic' , '-c' , alembic ini , 'upgrade' , revision ] )", "predictions": ["upgrade the database ."], "references": ["upgrade the given database to revision ."], "bleu": 0.25379544718731684, "rouge_l": 0.693181818181818}
{"id": 2015, "code": "def queue instance ( self , embed type , data ) : serializer = self . serializers . get ( embed type , None ) if serializer is None : return instance id = serializer . get id ( data ) if embed type not in self . ids : self . ids [ embed type ] = [ ] self . ids [ embed type ] . append ( instance id )", "predictions": ["queue an embed instance ."], "references": ["queue an instance to be fetched from the database ."], "bleu": 0.1501861529550426, "rouge_l": 0.5030927835051546}
{"id": 2016, "code": "def insert instance ( self , block ) : embed type = block . get ( 'type' , None ) data = block . get ( 'data' , { } ) serializer = self . serializers . get ( embed type , None ) if serializer is None : return block try : instance id = serializer . get id ( data ) instance = self . instances [ embed type ] [ instance id ] data [ embed type ] = serializer . serialize ( instance ) except : data [ embed type ] = None block [ 'data' ] = data return block", "predictions": ["rename an file into the api user ."], "references": ["insert a fetched instance into embed block ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 2017, "code": "def load data ( self ) : for embed type in self . ids . keys ( ) : self . load instances ( embed type , self . ids [ embed type ] )", "predictions": ["rename the old directory new ids ."], "references": ["load data in bulk for each embed block ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 2018, "code": "def validate ( self , data ) : from dispatch . theme import Theme Manager errors = { } if data . get ( 'widget' ) is not None : try : widget = Theme Manager . Widgets . get ( data [ 'widget' ] ) except Widget Not Found as e : errors [ 'widget' ] = str ( e ) else : for field in widget . fields : field data = data [ 'data' ] . get ( field . name ) if field data is not None : try : field . validate ( field data ) except Invalid Field as e : errors [ field . name ] = str ( e ) elif field . required : errors [ field . name ] = '%s is required' % field . label if errors : raise Validation Error ( errors ) return data", "predictions": ["purge the db db db db"], "references": ["perform validation of the widget data"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 2019, "code": "def admin ( request ) : context = { 'api url' : settings . API URL , 'app js bundle' : 'manager-%s.js' % dispatch . version , 'app css bundle' : 'manager-%s.css' % dispatch . version } return render to response ( 'manager/index.html' , context )", "predictions": ["displays the admin s admin logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger logger"], "references": ["render html entry point for manager app ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2020, "code": "def to json ( self ) : result = { } for field in self . fields : result [ field . name ] = field . to json ( self . data . get ( field . name ) ) return result", "predictions": ["returns the file as a dictionary ."], "references": ["return json representation for this template"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2021, "code": "def exclude fields ( self ) : request = self . context . get ( 'request' ) if request : exclude = request . query params . get ( 'exclude' , None ) if exclude is None : return excluded fields = exclude . split ( ',' ) for field in excluded fields : self . fields . pop ( field )", "predictions": ["select all == remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote remote"], "references": ["excludes fields that are included in the queryparameters"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2022, "code": "def get ( self , * args , * * kwargs ) : if 'pk' in kwargs : kwargs [ 'parent' ] = kwargs [ 'pk' ] kwargs [ 'head' ] = True del kwargs [ 'pk' ] if 'request' in kwargs : request = kwargs [ 'request' ] version = request . GET . get ( 'version' , None ) preview id = request . GET . get ( 'preview id' , None ) if ( version is not None ) and ( preview id is not None ) : kwargs [ 'revision id' ] = version kwargs [ 'preview id' ] = preview id del kwargs [ 'is published' ] del kwargs [ 'request' ] return super ( Publishable Manager , self ) . get ( * args , * * kwargs )", "predictions": ["remove the files from the request begin begin begin"], "references": ["get the latest article with the given primary key ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 2023, "code": "def get attribute ( self , instance ) : attr = super ( Null Boolean Field , self ) . get attribute ( instance ) return True if attr else False", "predictions": ["override the default single single single single single single single single single single single single single single single single single single single single single single single single single single single single"], "references": ["overrides the default get_attribute method to convert none values to false ."], "bleu": 0.0513487742994337, "rouge_l": 0.10107705053852528}
{"id": 2024, "code": "def validate widget ( widget ) : if not has valid id ( widget ) : raise Invalid Widget ( \"%s must contain a valid 'id' attribute\" % widget . name ) if not has valid name ( widget ) : raise Invalid Widget ( \"%s must contain a valid 'name' attribute\" % widget . name ) if not has valid template ( widget ) : raise Invalid Widget ( \"%s must contain a valid 'template' attribute\" % widget . name ) if not hasattr ( widget , 'zones' ) or not widget . zones : raise Invalid Widget ( \"%s must be compatible with at least one zone\" % widget . name )", "predictions": ["validates that the checkpoint is of the checkpoint and is not defined ."], "references": ["checks that the given widget contains the required fields"], "bleu": 0.1350862565735141, "rouge_l": 0.2819722650231125}
{"id": 2025, "code": "def validate zone ( zone ) : if not has valid id ( zone ) : raise Invalid Zone ( \"%s must contain a valid 'id' attribute\" % zone . name ) if not has valid name ( zone ) : raise Invalid Zone ( \"%s must contain a valid 'name' attribute\" % zone . name )", "predictions": ["get a checkpoint from a checkpoint"], "references": ["checks that the given zone contains the required fields"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 2026, "code": "def is valid uuid ( id ) : if not isinstance ( id , basestring ) : return False try : val = UUID ( id , version = 4 ) except Value Error : return False return True", "predictions": ["returns true if id is a checkpoints"], "references": ["return true if id is a valid uuid false otherwise ."], "bleu": 0.38422005460399855, "rouge_l": 0.5341506129597198}
{"id": 2027, "code": "def get permissions ( self ) : permissions = '' if self . groups . filter ( name = 'Admin' ) . exists ( ) or self . is superuser : permissions = 'admin' return permissions", "predictions": ["remote all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all"], "references": ["returns the user s permissions ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2028, "code": "def modify permissions ( self , permissions ) : group = Group . objects . get ( name = 'Admin' ) if permissions == 'admin' : self . groups . add ( group ) else : self . groups . remove ( group )", "predictions": ["delete all all all all all all all all all all all all all all all all all all all all all all all all all all all all all all"], "references": ["modify the user s permissions ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2029, "code": "def Author Validator ( data ) : if not isinstance ( data , list ) : data = [ data ] for author in data : if 'person' not in author : raise Validation Error ( 'An author must contain a person.' ) if 'type' in author and not isinstance ( author [ 'type' ] , basestring ) : raise Validation Error ( 'The author type must be a string.' )", "predictions": ["validate that all id must must parameters are not none ."], "references": ["raise a validationerror if data does not match the author format ."], "bleu": 0.11510518494396255, "rouge_l": 0.17256011315417258}
{"id": 2030, "code": "def save ( self , validated data ) : ( zone , created ) = Zone Model . objects . get or create ( zone id = self . id ) zone . widget id = validated data [ 'widget' ] zone . data = validated data [ 'data' ] for key in list ( zone . data . keys ( ) ) : if isinstance ( zone . data [ key ] , dict ) and ( 'id' in zone . data [ key ] . keys ( ) ) and ( 'data' in zone . data [ key ] . keys ( ) ) : zone . data [ key ] [ 'data' ] = self . before save ( zone . data [ key ] [ 'id' ] , zone . data [ key ] [ 'data' ] ) zone . data = self . before save ( zone . widget id , zone . data ) return zone . save ( )", "predictions": ["apply a not to a not ."], "references": ["save widget data for this zone ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 2031, "code": "def get data ( self ) : result = { } for field in self . fields : result [ field . name ] = self . data . get ( field . name ) return result", "predictions": ["returns the data data data"], "references": ["returns data from each field ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 2032, "code": "def prepare data ( self ) : result = { } for field in self . fields : data = self . data . get ( field . name ) result [ field . name ] = field . prepare data ( data ) return result", "predictions": ["path to json dict"], "references": ["prepare widget data for template ."], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 2033, "code": "def render ( self , data = None , add context = None ) : template = loader . get template ( self . template ) if not data : data = self . context ( self . prepare data ( ) ) if add context is not None : for key , value in add context . iteritems ( ) : if key in self . accepted keywords : data [ key ] = value return template . render ( data )", "predictions": ["managers the data and returns a json string"], "references": ["renders the widget as html ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 2034, "code": "def callback ( cls , user , query ) : settings = cls . get settings ( show hidden = True ) fb = Facebook ( ) payload = { 'client id' : settings [ 'client id' ] , 'client secret' : settings [ 'client secret' ] , 'code' : query [ 'code' ] , 'redirect uri' : cls . REDIRECT URI } try : fb . get access token ( payload ) pages = fb . list pages ( 'me' ) except Facebook API Error , e : raise Integration Callback Error ( e . message ) return { 'pages' : pages }", "predictions": ["get or get a path"], "references": ["receive oauth callback request from facebook ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 2035, "code": "def get settings ( self , integration id ) : try : integration = self . get ( integration id = integration id ) return json . loads ( integration . settings ) except ( self . model . Does Not Exist , Value Error ) : return { }", "predictions": ["gets the api api api api api api api api api"], "references": ["return settings for given integration as a dictionary ."], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2036, "code": "def update settings ( self , integration id , settings ) : ( integration , created ) = self . get or create ( integration id = integration id ) try : current settings = json . loads ( integration . settings ) except Value Error : current settings = { } current settings . update ( settings ) integration . settings = json . dumps ( current settings ) integration . save ( )", "predictions": ["updates this version of an existing version"], "references": ["updates settings for given integration ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2037, "code": "def signup ( request , uuid = None ) : invite = get object or 404 ( Invite . objects . all ( ) , id = uuid ) if invite . expiration date < timezone . now ( ) : invite . delete ( ) raise Http404 ( 'This page does not exist.' ) if request . method == 'POST' : form = Sign Up Form ( request . POST ) if form . is valid ( ) : user = form . save ( commit = False ) user . email = invite . email user . person = invite . person user . save ( ) if invite . permissions == 'admin' : group = Group . objects . get ( name = 'Admin' ) user . groups . add ( group ) invite . delete ( ) return redirect ( 'dispatch-admin' ) else : return render ( request , 'registration/signup.html' , { 'form' : form , 'email' : invite . email } ) else : form = Sign Up Form ( ) return render ( request , 'registration/signup.html' , { 'form' : form , 'email' : invite . email } )", "predictions": ["reads a person and returns a new user ."], "references": ["handles requests to the user signup page ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 2038, "code": "def zone ( zone id , * * kwargs ) : try : zone = Theme Manager . Zones . get ( zone id ) except Zone Not Found : return '' try : return zone . widget . render ( add context = kwargs ) except ( Widget Not Found , Attribute Error ) : pass return ''", "predictions": ["return a prefix object"], "references": ["renders the contents of the zone with given zone_id ."], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 2039, "code": "def save subsection ( self , subsection id ) : Article . objects . filter ( parent id = self . parent . id ) . update ( subsection id = subsection id )", "predictions": ["outside the root node as a root node wraps the args ."], "references": ["save the subsection to the parent article"], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 2040, "code": "def get extension ( self ) : ext = os . path . splitext ( self . img . name ) [ 1 ] if ext : return ext [ 1 : ] return ext", "predictions": ["create the user s user user s user user input file ."], "references": ["returns the file extension ."], "bleu": 0.1235622127262679, "rouge_l": 0.38125000000000003}
{"id": 2041, "code": "def get medium url ( self ) : if self . is gif ( ) : return self . get absolute url ( ) return '%s%s-%s.jpg' % ( settings . MEDIA URL , self . get name ( ) , 'medium' )", "predictions": ["= = 1 directory url"], "references": ["returns the medium size image url ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 2042, "code": "def save ( self , * * kwargs ) : is new = self . pk is None if is new : self . img . name = self . img . name . lower ( ) super ( Image , self ) . save ( * * kwargs ) if is new and self . img : data = self . img . read ( ) if not data : return image = Img . open ( String IO . String IO ( data ) ) self . width , self . height = image . size super ( Image , self ) . save ( ) name = self . get name ( ) ext = self . get extension ( ) for size in self . SIZES . keys ( ) : self . save thumbnail ( image , self . SIZES [ size ] , name , size , ext )", "predictions": ["walk the image to the sizes file file file file file file file file file file file file file file file file file file file file file file file file file"], "references": ["custom save method to process thumbnails and save image dimensions ."], "bleu": 0.04317900023606586, "rouge_l": 0.05209222886421862}
{"id": 2043, "code": "def save thumbnail ( self , image , size , name , label , file type ) : width , height = size ( imw , imh ) = image . size if ( imw > width ) or ( imh > height ) : image . thumbnail ( size , Img . ANTIALIAS ) name = \"%s-%s.jpg\" % ( name , label ) if file type in self . JPG FORMATS : file type = 'JPEG' image io = String IO . String IO ( ) image . save ( image io , format = file type , quality = 75 ) thumb file = In Memory Uploaded File ( image io , None , name , 'image/jpeg' , image io . len , None ) default storage . save ( name , thumb file )", "predictions": ["saves the files in the given mgr . ."], "references": ["processes and saves a resized thumbnail version of the image ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 2044, "code": "def decrement ( self ) : with self . lock : if self . count == 0 : raise Runtime Error ( 'Counter is at zero. It cannot dip below zero' ) self . count -= 1 if self . is finalized and self . count == 0 : self . callback ( )", "predictions": ["decrement the old old old old callback id id id id id id id id id"], "references": ["decrement the count by one"], "bleu": 0.10123734869668824, "rouge_l": 0.21034482758620687}
{"id": 2045, "code": "def set exception ( self , exception ) : if not self . done ( ) : raise Transfer Not Done Error ( 'set exception can only be called once the transfer is ' 'complete.' ) self . coordinator . set exception ( exception , override = True )", "predictions": ["set the coordinator single single single single single single single single single single single single single single single single single single single single single single single single single single single single"], "references": ["sets the exception on the future ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 2046, "code": "def add done callback ( self , function , * args , * * kwargs ) : with self . done callbacks lock : self . done callbacks . append ( Function Container ( function , * args , * * kwargs ) )", "predictions": ["upgrade a done db db . ."], "references": ["add a done callback to be invoked when transfer is done"], "bleu": 0.1380518455178974, "rouge_l": 0.2136602451838879}
{"id": 2047, "code": "def add failure cleanup ( self , function , * args , * * kwargs ) : with self . failure cleanups lock : self . failure cleanups . append ( Function Container ( function , * args , * * kwargs ) )", "predictions": ["queue a instance of instance cleanup serializers serializers serializers serializers serializers serializers serializers serializers serializers serializers serializers serializers serializers serializers serializers serializers serializers serializers serializers serializers serializers serializers serializers serializers serializers"], "references": ["adds a callback to call upon failure"], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 2048, "code": "def iter step func decorators ( self ) : func defs = [ func for func in self . py tree . iter funcdefs ( ) ] + [ func for cls in self . py tree . iter classdefs ( ) for func in cls . iter funcdefs ( ) ] for func in func defs : for decorator in func . get decorators ( ) : if decorator . children [ 1 ] . value == 'step' : yield func , decorator break", "predictions": ["iterate over all decorators decorators ."], "references": ["find functions with step decorator in parsed file"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 2049, "code": "def iter steps ( self ) : for func , decorator in self . iter step func decorators ( ) : step = self . step decorator args ( decorator ) if step : span = self . span from pos ( decorator . start pos , func . end pos ) yield step , func . name . value , span", "predictions": ["iterate over all steps steps ."], "references": ["iterate over steps in the parsed file ."], "bleu": 0.236682065782701, "rouge_l": 0.5570776255707762}
{"id": 2050, "code": "def find step node ( self , step text ) : for func , decorator in self . iter step func decorators ( ) : step = self . step decorator args ( decorator ) arg node = decorator . children [ 3 ] if step == step text : return arg node , func elif isinstance ( step , list ) and step text in step : idx = step . index ( step text ) step node = arg node . children [ 1 ] . children [ idx * 2 ] return step node , func return None , None", "predictions": ["find the step node node node node ."], "references": ["find the ast node which contains the text ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 2051, "code": "def iter step func decorators ( self ) : for node in self . py tree . find all ( 'def' ) : for decorator in node . decorators : if decorator . name . value == 'step' : yield node , decorator break", "predictions": ["iterate over all decorators decorators ."], "references": ["find functions with step decorator in parsed file ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 2052, "code": "def step decorator args ( self , decorator ) : args = decorator . call . value step = None if len ( args ) == 1 : try : step = args [ 0 ] . value . to python ( ) except ( Value Error , Syntax Error ) : pass if isinstance ( step , six . string types + ( list , ) ) : return step logging . error ( , self . file path ) else : logging . error ( \"Decorator step accepts only one argument - %s\" , self . file path )", "predictions": ["step args for a step ."], "references": ["get arguments passed to step decorators converted to python objects ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 2053, "code": "def iter steps ( self ) : for func , decorator in self . iter step func decorators ( ) : step = self . step decorator args ( decorator ) if step : yield step , func . name , self . span for node ( func , True )", "predictions": ["iterate over all steps of this step ."], "references": ["iterate over steps in the parsed file ."], "bleu": 0.239802967618271, "rouge_l": 0.5}
{"id": 2054, "code": "def find step node ( self , step text ) : for func , decorator in self . iter step func decorators ( ) : step = self . step decorator args ( decorator ) arg node = decorator . call . value [ 0 ] . value if step == step text : return arg node , func elif isinstance ( step , list ) and step text in step : step node = arg node [ step . index ( step text ) ] return step node , func return None , None", "predictions": ["find a step node node by text text ."], "references": ["find the ast node which contains the text ."], "bleu": 0.21105340631872635, "rouge_l": 0.4444444444444444}
{"id": 2055, "code": "def POST ( self ) : json data = web . data ( ) print ( \"\\n WEBHOOK POST RECEIVED:\" ) print ( json data , \"\\n\" ) webhook obj = Webhook ( json data ) room = api . rooms . get ( webhook obj . data . room Id ) message = api . messages . get ( webhook obj . data . id ) person = api . people . get ( message . person Id ) print ( \"NEW MESSAGE IN ROOM '{}'\" . format ( room . title ) ) print ( \"FROM '{}'\" . format ( person . display Name ) ) print ( \"MESSAGE '{}'\\n\" . format ( message . text ) ) me = api . people . me ( ) if message . person Id == me . id : return 'OK' else : if \"/CAT\" in message . text : print ( \"FOUND '/CAT'\" ) cat fact = get catfact ( ) print ( \"SENDING CAT FACT '{}'\" . format ( cat fact ) ) api . messages . create ( room . id , text = cat fact ) return 'OK'", "predictions": ["create a fact from a webhook webhook ."], "references": ["respond to inbound webhook json http posts from webex teams ."], "bleu": 0.13107175678306446, "rouge_l": 0.20469798657718125}
{"id": 2056, "code": "def validate base url ( base url ) : parsed url = urllib . parse . urlparse ( base url ) if parsed url . scheme and parsed url . netloc : return parsed url . geturl ( ) else : error message = \"base url must contain a valid scheme (protocol \" \"specifier) and network location (hostname)\" raise Value Error ( error message )", "predictions": ["validates that the base url is valid ."], "references": ["verify that base_url specifies a protocol and network location ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 2057, "code": "def is web url ( string ) : assert isinstance ( string , basestring ) parsed url = urllib . parse . urlparse ( string ) return ( ( parsed url . scheme . lower ( ) == 'http' or parsed url . scheme . lower ( ) == 'https' ) and parsed url . netloc )", "predictions": ["return true if string is an web url ."], "references": ["check to see if string is an validly - formatted web url ."], "bleu": 0.33686462129547456, "rouge_l": 0.6161616161616162}
{"id": 2058, "code": "def open local file ( file path ) : assert isinstance ( file path , basestring ) assert is local file ( file path ) file name = os . path . basename ( file path ) file object = open ( file path , 'rb' ) content type = mimetypes . guess type ( file name ) [ 0 ] or 'text/plain' return Encodable File ( file name = file name , file object = file object , content type = content type )", "predictions": ["open a local local file in the local directory ."], "references": ["open the file and return an encodablefile tuple ."], "bleu": 0.15851165692617156, "rouge_l": 0.31881533101045295}
{"id": 2059, "code": "def strptime ( cls , date string , format = WEBEX TEAMS DATETIME FORMAT ) : return super ( Webex Teams Date Time , cls ) . strptime ( date string , format ) . replace ( tzinfo = Zulu Time Zone ( ) )", "predictions": ["convert a date string to a string ."], "references": ["strptime with the webex teams datetime format as the default ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 2060, "code": "def created ( self ) : created = self . json data . get ( 'created' ) if created : return Webex Teams Date Time . strptime ( created ) else : return None", "predictions": ["returns the created created time of this service ."], "references": ["creation date and time in iso8601 format ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 2061, "code": "def wait on rate limit ( self , value ) : check type ( value , bool , may be none = False ) self . wait on rate limit = value", "predictions": ["wait for a rate to be ready to be called on the client limit ."], "references": ["enable or disable automatic rate - limit handling ."], "bleu": 0.09782375748961449, "rouge_l": 0.26180257510729615}
{"id": 2062, "code": "def serialize ( cls , data ) : if hasattr ( data , \" hash \" ) and callable ( data . hash ) : return data elif isinstance ( data , list ) : return tuple ( ( cls . serialize ( item ) for item in data ) ) elif isinstance ( data , dict ) : key value tuples = [ ( key , cls . serialize ( value ) ) for key , value in data . items ( ) ] key value tuples . sort ( ) return tuple ( key value tuples ) else : raise Type Error ( \"Unable to freeze {} data type.\" . format ( type ( data ) ) )", "predictions": ["serialize data into json"], "references": ["serialize data to an frozen tuple ."], "bleu": 0.22336835181428535, "rouge_l": 0.346590909090909}
{"id": 2063, "code": "def last Activity ( self ) : last activity = self . json data . get ( 'last Activity' ) if last activity : return Webex Teams Date Time . strptime ( last activity ) else : return None", "predictions": ["return the last activity activity ."], "references": ["the date and time of the person s last activity ."], "bleu": 0.1588696449629768, "rouge_l": 0.4468864468864468}
{"id": 2064, "code": "def post events service ( request ) : json data = request . json log . info ( \"\\n\" ) log . info ( \"WEBHOOK POST RECEIVED:\" ) log . info ( json data ) log . info ( \"\\n\" ) webhook obj = Webhook ( json data ) room = api . rooms . get ( webhook obj . data . room Id ) message = api . messages . get ( webhook obj . data . id ) person = api . people . get ( message . person Id ) log . info ( \"NEW MESSAGE IN ROOM '{}'\" . format ( room . title ) ) log . info ( \"FROM '{}'\" . format ( person . display Name ) ) log . info ( \"MESSAGE '{}'\\n\" . format ( message . text ) ) me = api . people . me ( ) if message . person Id == me . id : return { 'Message' : 'OK' } else : if \"/CAT\" in message . text : log . info ( \"FOUND '/CAT'\" ) catfact = get catfact ( ) log . info ( \"SENDING CAT FACT'{}'\" . format ( catfact ) ) api . messages . create ( room . id , text = catfact ) return { 'Message' : 'OK' }", "predictions": ["create a new webhook service service ."], "references": ["respond to inbound webhook json http post from webex teams ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 2065, "code": "def get ngrok public url ( ) : try : response = requests . get ( url = NGROK CLIENT API BASE URL + \"/tunnels\" , headers = { 'content-type' : 'application/json' } ) response . raise for status ( ) except requests . exceptions . Request Exception : print ( \"Could not connect to the ngrok client API; \" \"assuming not running.\" ) return None else : for tunnel in response . json ( ) [ \"tunnels\" ] : if tunnel . get ( \"public url\" , \"\" ) . startswith ( \"http://\" ) : print ( \"Found ngrok public HTTP URL:\" , tunnel [ \"public url\" ] ) return tunnel [ \"public url\" ]", "predictions": ["get the ngrok public url"], "references": ["get the ngrok public http url from the local client api ."], "bleu": 0.1961113472255835, "rouge_l": 0.5475763016157988}
{"id": 2066, "code": "def delete webhooks with name ( api , name ) : for webhook in api . webhooks . list ( ) : if webhook . name == name : print ( \"Deleting Webhook:\" , webhook . name , webhook . target Url ) api . webhooks . delete ( webhook . id )", "predictions": ["delete an webhooks with the given name ."], "references": ["find a webhook by name ."], "bleu": 0.21105340631872638, "rouge_l": 0.2932692307692307}
{"id": 2067, "code": "def create ngrok webhook ( api , ngrok public url ) : print ( \"Creating Webhook...\" ) webhook = api . webhooks . create ( name = WEBHOOK NAME , target Url = urljoin ( ngrok public url , WEBHOOK URL SUFFIX ) , resource = WEBHOOK RESOURCE , event = WEBHOOK EVENT , ) print ( webhook ) print ( \"Webhook successfully created.\" ) return webhook", "predictions": ["create a ngrok webhook"], "references": ["create a webex teams webhook pointing to the public ngrok url ."], "bleu": 0.07271361304044333, "rouge_l": 0.34398496240601506}
{"id": 2068, "code": "def main ( ) : api = Webex Teams API ( ) delete webhooks with name ( api , name = WEBHOOK NAME ) public url = get ngrok public url ( ) if public url is not None : create ngrok webhook ( api , public url )", "predictions": ["create an ngrok webhook with the given name ."], "references": ["delete previous webhooks . if local ngrok tunnel create a webhook ."], "bleu": 0.12716571564598603, "rouge_l": 0.2785388127853881}
{"id": 2069, "code": "def console ( ) : parser = argparse . Argument Parser ( description = console . doc ) parser . add argument ( '--device' , default = '/dev/tty USB0' , help = 'port to read DSMR data from' ) parser . add argument ( '--host' , default = None , help = 'alternatively connect using TCP host.' ) parser . add argument ( '--port' , default = None , help = 'TCP port to use for connection' ) parser . add argument ( '--version' , default = '2.2' , choices = [ '2.2' , '4' ] , help = 'DSMR version (2.2, 4)' ) parser . add argument ( '--verbose' , '-v' , action = 'count' ) args = parser . parse args ( ) if args . verbose : level = logging . DEBUG else : level = logging . ERROR logging . basic Config ( level = level ) loop = asyncio . get event loop ( ) def print callback ( telegram ) : \"\"\"Callback that prints telegram values.\"\"\" for obiref , obj in telegram . items ( ) : if obj : print ( obj . value , obj . unit ) print ( ) if args . host and args . port : create connection = partial ( create tcp dsmr reader , args . host , args . port , args . version , print callback , loop = loop ) else : create connection = partial ( create dsmr reader , args . device , args . version , print callback , loop = loop ) try : while True : conn = create connection ( ) transport , protocol = loop . run until complete ( conn ) loop . run until complete ( protocol . wait closed ( ) ) loop . run until complete ( asyncio . sleep ( 5 ) ) except Keyboard Interrupt : transport . close ( ) loop . run until complete ( asyncio . sleep ( 0 ) ) finally : loop . close ( )", "predictions": ["console script to console ."], "references": ["output dsmr data to console ."], "bleu": 0.41602390756021224, "rouge_l": 0.5366568914956013}
{"id": 2070, "code": "def create dsmr protocol ( dsmr version , telegram callback , loop = None ) : if dsmr version == '2.2' : specification = telegram specifications . V2 2 serial settings = SERIAL SETTINGS V2 2 elif dsmr version == '4' : specification = telegram specifications . V4 serial settings = SERIAL SETTINGS V4 elif dsmr version == '5' : specification = telegram specifications . V5 serial settings = SERIAL SETTINGS V5 else : raise Not Implemented Error ( \"No telegram parser found for version: %s\" , dsmr version ) protocol = partial ( DSMR Protocol , loop , Telegram Parser ( specification ) , telegram callback = telegram callback ) return protocol , serial settings", "predictions": ["create a dsmr protocol protocol protocol protocol protocol ."], "references": ["creates a dsmr asyncio protocol ."], "bleu": 0.23356898886410005, "rouge_l": 0.5532879818594103}
{"id": 2071, "code": "def create dsmr reader ( port , dsmr version , telegram callback , loop = None ) : protocol , serial settings = create dsmr protocol ( dsmr version , telegram callback , loop = None ) serial settings [ 'url' ] = port conn = create serial connection ( loop , protocol , * * serial settings ) return conn", "predictions": ["create a connection to the dsmr server ."], "references": ["creates a dsmr asyncio protocol coroutine using serial port ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 2072, "code": "def create tcp dsmr reader ( host , port , dsmr version , telegram callback , loop = None ) : protocol , = create dsmr protocol ( dsmr version , telegram callback , loop = None ) conn = loop . create connection ( protocol , host , port ) return conn", "predictions": ["create a tcp connection to the kafka server ."], "references": ["creates a dsmr asyncio protocol coroutine using tcp connection ."], "bleu": 0.18885888592159467, "rouge_l": 0.41709401709401706}
{"id": 2073, "code": "def data received ( self , data ) : data = data . decode ( 'ascii' ) self . log . debug ( 'received data: %s' , data ) self . telegram buffer . append ( data ) for telegram in self . telegram buffer . get all ( ) : self . handle telegram ( telegram )", "predictions": ["handle incoming data received from network ."], "references": ["add incoming data to buffer ."], "bleu": 0.2626909894424158, "rouge_l": 0.4680306905370844}
{"id": 2074, "code": "def connection lost ( self , exc ) : if exc : self . log . exception ( 'disconnected due to exception' ) else : self . log . info ( 'disconnected because of close/abort.' ) self . closed . set ( )", "predictions": ["setter method ; for a description see the getter method ."], "references": ["stop when connection is lost ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 2075, "code": "def handle telegram ( self , telegram ) : self . log . debug ( 'got telegram: %s' , telegram ) try : parsed telegram = self . telegram parser . parse ( telegram ) except Invalid Checksum Error as e : self . log . warning ( str ( e ) ) except Parse Error : self . log . exception ( \"failed to parse telegram\" ) else : self . telegram callback ( parsed telegram )", "predictions": ["parse the telegram telegram ."], "references": ["send off parsed telegram to handling callback ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 2076, "code": "def ensure python ( specs ) : if not isinstance ( specs , ( list , tuple ) ) : specs = [ specs ] v = sys . version info part = '%s.%s' % ( v . major , v . minor ) for spec in specs : if part == spec : return try : if eval ( part + spec ) : return except Syntax Error : pass raise Value Error ( 'Python version %s unsupported' % part )", "predictions": ["ensure that the python version of the given python list is valid ."], "references": ["given a list of range specifiers for python ensure compatibility ."], "bleu": 0.13065113298388562, "rouge_l": 0.2538141470180305}
{"id": 2077, "code": "def find packages ( top = HERE ) : packages = [ ] for d , dirs , in os . walk ( top , followlinks = True ) : if os . path . exists ( pjoin ( d , ' init .py' ) ) : packages . append ( os . path . relpath ( d , top ) . replace ( os . path . sep , '.' ) ) elif d != top : dirs [ : ] = [ ] return packages", "predictions": ["find packages in the package ."], "references": ["find all of the packages ."], "bleu": 0.2777619034011791, "rouge_l": 0.5}
{"id": 2078, "code": "def command for func ( func ) : class Func Command ( Base Command ) : def run ( self ) : func ( ) update package data ( self . distribution ) return Func Command", "predictions": ["return the command function for the given function ."], "references": ["create a command that calls the given function ."], "bleu": 0.4111336169005197, "rouge_l": 0.5555555555555556}
{"id": 2079, "code": "def run ( cmd , * * kwargs ) : log . info ( '> ' + list2cmdline ( cmd ) ) kwargs . setdefault ( 'cwd' , HERE ) kwargs . setdefault ( 'shell' , os . name == 'nt' ) if not isinstance ( cmd , ( list , tuple ) ) and os . name != 'nt' : cmd = shlex . split ( cmd ) cmd [ 0 ] = which ( cmd [ 0 ] ) return subprocess . check call ( cmd , * * kwargs )", "predictions": ["run a command in the shell ."], "references": ["echo a command before running it . defaults to repo as cwd"], "bleu": 0.1285981829222983, "rouge_l": 0.30148270181219106}
{"id": 2080, "code": "def get file handler ( package data spec , data files spec ) : class File Handler ( Base Command ) : def run ( self ) : package data = self . distribution . package data package spec = package data spec or dict ( ) for ( key , patterns ) in package spec . items ( ) : package data [ key ] = get package data ( key , patterns ) self . distribution . data files = get data files ( data files spec , self . distribution . data files ) return File Handler", "predictions": ["returns a step func for the given package spec cls cls cls cls cls cls cls cls cls cls cls cls cls cls cls cls cls cls cls cls cls cls"], "references": ["get a package_data and data_files handler command ."], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 2081, "code": "def compile pattern ( pat , ignore case = True ) : if isinstance ( pat , bytes ) : pat str = pat . decode ( 'ISO-8859-1' ) res str = translate glob ( pat str ) res = res str . encode ( 'ISO-8859-1' ) else : res = translate glob ( pat ) flags = re . IGNORECASE if ignore case else 0 return re . compile ( res , flags = flags ) . match", "predictions": ["iter steps to string args args args args args args args args args args args"], "references": ["translate and compile a glob pattern to a regular expression matcher ."], "bleu": 0.08225964699966554, "rouge_l": 0.07558859975216851}
{"id": 2082, "code": "def translate glob ( pat ) : translated parts = [ ] for part in iexplode path ( pat ) : translated parts . append ( translate glob part ( part ) ) os sep class = '[%s]' % re . escape ( SEPARATORS ) res = join translated ( translated parts , os sep class ) return '{res}\\\\Z(?ms)' . format ( res = res )", "predictions": ["find all parts in a step = value = a string = a string = a string = none = a string = a string = value = 1 = 1"], "references": ["translate a glob pattern to a regular expression ."], "bleu": 0.04317900023606586, "rouge_l": 0.11101000909918107}
{"id": 2083, "code": "def translate glob part ( pat ) : if pat == '**' : return '.*' i , n = 0 , len ( pat ) res = [ ] while i < n : c = pat [ i ] i = i + 1 if c == '*' : res . append ( '[^%s]*' % SEPARATORS ) elif c == '?' : res . append ( '[^%s]?' % SEPARATORS ) elif c == '[' : j = i if j < n and pat [ j ] == '!' : j = j + 1 if j < n and pat [ j ] == ']' : j = j + 1 while j < n and pat [ j ] != ']' : j = j + 1 if j >= n : res . append ( '\\\\[' ) else : stuff = pat [ i : j ] . replace ( '\\\\' , '\\\\\\\\' ) i = j + 1 if stuff [ 0 ] == '!' : stuff = '^' + stuff [ 1 : ] elif stuff [ 0 ] == '^' : stuff = '\\\\' + stuff res . append ( '[%s]' % stuff ) else : res . append ( re . escape ( c ) ) return '' . join ( res )", "predictions": ["iter a step func to a step . ."], "references": ["translate a glob pattern part to a regular expression ."], "bleu": 0.18885888592159467, "rouge_l": 0.41709401709401706}
{"id": 2084, "code": "def qsize ( self , extra predicate = None ) : count = self . query queued ( 'COUNT(*) AS count' , extra predicate = extra predicate ) return count [ 0 ] . count", "predictions": ["try to get the number of rows in the if any"], "references": ["return an approximate number of queued tasks in the queue ."], "bleu": 0.18850319022747347, "rouge_l": 0.36363636363636365}
{"id": 2085, "code": "def enqueue ( self , data ) : jsonified data = json . dumps ( data ) with self . db conn ( ) as conn : return conn . execute ( 'INSERT INTO %s (created, data) VALUES (%%(created)s, %%(data)s)' % self . table name , created = datetime . utcnow ( ) , data = jsonified data )", "predictions": ["iter to table database"], "references": ["enqueue task with specified data ."], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 2086, "code": "def build extra predicate ( self , extra predicate ) : if extra predicate is None : return '' if not isinstance ( extra predicate [ 1 ] , ( list , dict , tuple ) ) : extra predicate = [ extra predicate [ 0 ] , ( extra predicate [ 1 ] , ) ] extra predicate = database . escape query ( * extra predicate ) return 'AND (' + extra predicate + ')'", "predictions": ["find the step node to create the step from the database"], "references": ["this method is a good one to extend if you want to create a queue which always applies an extra predicate ."], "bleu": 0.05514920046097622, "rouge_l": 0.11433926897844424}
{"id": 2087, "code": "def simplejson datetime serializer ( obj ) : if hasattr ( obj , 'isoformat' ) : return obj . isoformat ( ) else : raise Type Error ( 'Object of type %s with value of %s is not JSON serializable' % ( type ( obj ) , repr ( obj ) ) )", "predictions": ["simplejson for datetime objects"], "references": ["designed to be passed as the default kwarg in simplejson . dumps . serializes dates and datetimes to iso strings ."], "bleu": 0.005125197897506692, "rouge_l": 0.07126168224299065}
{"id": 2088, "code": "def reconnect ( self ) : conn = mysql . connect ( * * self . db args ) if conn is not None : self . close ( ) self . db = conn", "predictions": ["validate and validate the = database urllib urllib urllib urllib urllib urllib urllib urllib urllib urllib"], "references": ["closes the existing database connection and re - opens it ."], "bleu": 0.09147827112247602, "rouge_l": 0.15326633165829145}
{"id": 2089, "code": "def get ( self , query , * parameters , * * kwparameters ) : rows = self . query ( query , parameters , kwparameters ) if not rows : return None elif not isinstance ( rows , list ) : raise My SQL Error ( \"Query is not a select query\" ) elif len ( rows ) > 1 : raise My SQL Error ( \"Multiple rows returned for Database.get() query\" ) else : return rows [ 0 ]", "predictions": ["is the first row of the string or none"], "references": ["returns the first row returned for the given query ."], "bleu": 0.24855227187657006, "rouge_l": 0.41709401709401706}
{"id": 2090, "code": "def execute ( self , query , * parameters , * * kwparameters ) : return self . execute lastrowid ( query , * parameters , * * kwparameters )", "predictions": ["open a path and return the result"], "references": ["executes the given query returning the lastrowid from the query ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 2091, "code": "def execute lastrowid ( self , query , * parameters , * * kwparameters ) : self . execute ( query , parameters , kwparameters ) self . result = self . db . store result ( ) return self . db . insert id ( )", "predictions": ["strptime a string ."], "references": ["executes the given query returning the lastrowid from the query ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 2092, "code": "def get connection ( db = DATABASE ) : return database . connect ( host = HOST , port = PORT , user = USER , password = PASSWORD , database = db )", "predictions": ["data to use for get a connection to get a connection ."], "references": ["returns a new connection to the database ."], "bleu": 0.15537125692760353, "rouge_l": 0.4149659863945578}
{"id": 2093, "code": "def run benchmark ( ) : stopping = threading . Event ( ) workers = [ Insert Worker ( stopping ) for in range ( NUM WORKERS ) ] print ( 'Launching %d workers' % NUM WORKERS ) [ worker . start ( ) for worker in workers ] time . sleep ( WORKLOAD TIME ) print ( 'Stopping workload' ) stopping . set ( ) [ worker . join ( ) for worker in workers ] with get connection ( ) as conn : count = conn . get ( \"SELECT COUNT(*) AS count FROM %s\" % TABLE ) . count print ( \"%d rows inserted using %d workers\" % ( count , NUM WORKERS ) ) print ( \"%.1f rows per second\" % ( count / float ( WORKLOAD TIME ) ) )", "predictions": ["on all check check for the current worker"], "references": ["run a set of insertworkers and record their performance ."], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 2094, "code": "def connect ( self ) : with self . lock : if self . aggregator : try : return self . pool connect ( self . aggregator ) except Pool Connection Exception : self . aggregator = None if not len ( self . aggregators ) : with self . pool connect ( self . primary aggregator ) as conn : self . update aggregator list ( conn ) conn . expire ( ) random . shuffle ( self . aggregators ) last exception = None for aggregator in self . aggregators : self . logger . debug ( 'Attempting connection with %s:%s' % ( aggregator [ 0 ] , aggregator [ 1 ] ) ) try : conn = self . pool connect ( aggregator ) self . aggregator = aggregator return conn except Pool Connection Exception as e : last exception = e else : self . aggregator = None self . aggregators = [ ] raise last exception", "predictions": ["serialize and serialize a connection to the callable . . . . . . . ."], "references": ["returns an aggregator connection ."], "bleu": 0.08513012360883544, "rouge_l": 0.21034482758620687}
{"id": 2095, "code": "def lookup by number ( errno ) : for key , val in globals ( ) . items ( ) : if errno == val : print ( key )", "predictions": ["print a dictionary by by number"], "references": ["used for development only"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2096, "code": "def size ( self ) : return sum ( q . qsize ( ) for q in self . connections . values ( ) ) + len ( self . fairies )", "predictions": ["total number of info"], "references": ["returns the number of connections cached by the pool ."], "bleu": 0.10551173833795614, "rouge_l": 0.26521739130434785}
{"id": 2097, "code": "def ping ( self ) : with self . db conn ( ) as conn : affected rows = conn . query ( % self . manager . table name , datetime . utcnow ( ) , self . lock id , self . lock hash ) return bool ( affected rows == 1 )", "predictions": ["get the { % } . ."], "references": ["notify the manager that this lock is still active ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 2098, "code": "def release ( self ) : if self . valid ( ) : with self . db conn ( ) as conn : affected rows = conn . query ( % self . manager . table name , self . lock id , self . lock hash ) return bool ( affected rows == 1 ) else : return False", "predictions": ["delete this context . . . . . . . . ."], "references": ["release the lock ."], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 2099, "code": "def connect ( self , host = '127.0.0.1' , port = 3306 , user = 'root' , password = '' , database = None ) : if database is None : raise exceptions . Requires Database ( ) self . db args = { 'host' : host , 'port' : port , 'user' : user , 'password' : password , 'database' : database } with self . db conn ( ) as conn : conn . query ( 'SELECT 1' ) return self", "predictions": ["create a connection to the sqlite database urljoin urljoin urljoin urljoin urljoin"], "references": ["connect to the database specified"], "bleu": 0.14694106251955755, "rouge_l": 0.38125000000000003}
{"id": 2100, "code": "def setup ( self ) : with self . db conn ( ) as conn : for table defn in self . tables . values ( ) : conn . execute ( table defn ) return self", "predictions": ["main entry point for database - side sql sql"], "references": ["initialize the required tables in the database"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 2101, "code": "def destroy ( self ) : with self . db conn ( ) as conn : for table name in self . tables : conn . execute ( 'DROP TABLE IF EXISTS %s' % table name ) return self", "predictions": ["console script for all tables tables argparse argparse argparse argparse argparse"], "references": ["destroy the sqlstepqueue tables in the database"], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 2102, "code": "def ready ( self ) : with self . db conn ( ) as conn : tables = [ row . t for row in conn . query ( , self . db args [ 'database' ] ) ] return all ( [ table name in tables for table name in self . tables ] )", "predictions": ["list all tables tables tables telegram tables telegram telegram telegram telegram telegram telegram telegram telegram telegram telegram telegram telegram telegram telegram telegram telegram telegram telegram telegram telegram telegram telegram telegram telegram"], "references": ["returns true if the tables have been setup false otherwise"], "bleu": 0.03901663112717908, "rouge_l": 0.05374449339207048}
{"id": 2103, "code": "def valid ( self ) : if self . finished is not None : return False with self . db conn ( ) as conn : row = conn . get ( % self . queue . table name , now = datetime . utcnow ( ) , ttl = self . queue . execution ttl , task id = self . task id , execution id = self . execution id ) return bool ( row is not None and row . valid )", "predictions": ["returns true if this task is create a create task"], "references": ["check to see if we are still active ."], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 2104, "code": "def ping ( self ) : if self . finished is not None : raise Already Finished ( ) with self . db conn ( ) as conn : success = conn . query ( % self . queue . table name , now = datetime . utcnow ( ) , task id = self . task id , execution id = self . execution id , ttl = self . queue . execution ttl ) if success != 1 : raise Task Does Not Exist ( )", "predictions": ["create the underlying task connection . ."], "references": ["notify the queue that this task is still active ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 2105, "code": "def start step ( self , step name ) : if self . finished is not None : raise Already Finished ( ) step data = self . get step ( step name ) if step data is not None : if 'stop' in step data : raise Step Already Finished ( ) else : raise Step Already Started ( ) steps = copy . deepcopy ( self . steps ) steps . append ( { \"start\" : datetime . utcnow ( ) , \"name\" : step name } ) self . save ( steps = steps )", "predictions": ["data is called by the received received received decode decode"], "references": ["start a step ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2106, "code": "def stop step ( self , step name ) : if self . finished is not None : raise Already Finished ( ) steps = copy . deepcopy ( self . steps ) step data = self . get step ( step name , steps = steps ) if step data is None : raise Step Not Started ( ) elif 'stop' in step data : raise Step Already Finished ( ) step data [ 'stop' ] = datetime . utcnow ( ) step data [ 'duration' ] = util . timedelta total seconds ( step data [ 'stop' ] - step data [ 'start' ] ) self . save ( steps = steps )", "predictions": ["connection to step info"], "references": ["stop a step ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 2107, "code": "def create ( self , device Type ) : r = self . api Client . post ( \"api/v0002/device/types\" , device Type ) if r . status code == 201 : return Device Type ( api Client = self . api Client , * * r . json ( ) ) else : raise Api Exception ( r )", "predictions": ["handle a new device . . ."], "references": ["register one or more new device types each request can contain a maximum of 512kb ."], "bleu": 0.07678812443288274, "rouge_l": 0.2436750998668442}
{"id": 2108, "code": "def update ( self , device Uid , metadata = None , device Info = None , status = None ) : if not isinstance ( device Uid , Device Uid ) and isinstance ( device Uid , dict ) : device Uid = Device Uid ( * * device Uid ) device Url = \"api/v0002/device/types/%s/devices/%s\" % ( device Uid . type Id , device Uid . device Id ) data = { \"status\" : status , \"device Info\" : device Info , \"metadata\" : metadata } r = self . api Client . put ( device Url , data ) if r . status code == 200 : return Device ( api Client = self . api Client , * * r . json ( ) ) else : raise Api Exception ( r )", "predictions": ["ensure that the device exists in the server is not passed ."], "references": ["update an existing device"], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 2109, "code": "def find ( self , status = None , connected After = None ) : query Parms = { } if status : query Parms [ \"status\" ] = status if connected After : query Parms [ \"connected After\" ] = connected After return Iterable Client Status List ( self . api Client , filters = query Parms )", "predictions": ["list available objects by = = name"], "references": ["iterate through all connectors"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2110, "code": "def list ( self ) : url = \"api/v0002/mgmt/custom/bundle\" r = self . api Client . get ( url ) if r . status code == 200 : return r . json ( ) else : raise Api Exception ( r )", "predictions": ["command for showing the run"], "references": ["list all device management extension packages"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 2111, "code": "def update Schema ( self , schema Id , schema Definition ) : req = Api Client . one Schema Url % ( self . host , \"/draft\" , schema Id ) body = { \"schema Definition\" : schema Definition } resp = requests . put ( req , auth = self . credentials , headers = { \"Content-Type\" : \"application/json\" } , data = json . dumps ( body ) , verify = self . verify ) if resp . status code == 200 : self . logger . debug ( \"Schema updated\" ) else : raise ibmiotf . API Exception ( resp . status code , \"HTTP error updating schema\" , resp ) return resp . json ( )", "predictions": ["updates the kwargs in the kwargs"], "references": ["update a schema . throws apiexception on failure ."], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 2112, "code": "def disconnect ( self ) : self . client . disconnect ( ) self . client . loop stop ( ) self . logger . info ( \"Closed connection to the IBM Watson Io T Platform\" )", "predictions": ["disconnect from the connection ."], "references": ["disconnect the client from ibm watson iot platform"], "bleu": 0.1781815298791261, "rouge_l": 0.2953995157384988}
{"id": 2113, "code": "def get ( self , device Uid , event Id ) : if not isinstance ( device Uid , Device Uid ) and isinstance ( device Uid , dict ) : device Uid = Device Uid ( * * device Uid ) url = \"api/v0002/device/types/%s/devices/%s/events/%s\" % ( device Uid . type Id , device Uid . device Id , event Id ) r = self . api Client . get ( url ) if r . status code == 200 : return Last Event ( * * r . json ( ) ) else : raise Api Exception ( r )", "predictions": ["get the device object from the device"], "references": ["retrieves the last cached message for specified event from a specific device ."], "bleu": 0.09374222649442905, "rouge_l": 0.2846034214618974}
{"id": 2114, "code": "def get All ( self , device Uid ) : if not isinstance ( device Uid , Device Uid ) and isinstance ( device Uid , dict ) : device Uid = Device Uid ( * * device Uid ) url = \"api/v0002/device/types/%s/devices/%s/events\" % ( device Uid . type Id , device Uid . device Id ) r = self . api Client . get ( url ) if r . status code == 200 : events = [ ] for event in r . json ( ) : events . append ( Last Event ( * * event ) ) return events else : raise Api Exception ( r )", "predictions": ["returns all events in the device"], "references": ["retrieves a list of the last cached message for all events from a specific device ."], "bleu": 0.06238871518363817, "rouge_l": 0.25206611570247933}
{"id": 2115, "code": "def load Byte Array ( self , page , return Error ) : return Error . contents . value = self . Illegal State Error raise Not Implemented Error ( \"You must override this method.\" ) return ''", "predictions": ["load an ncstream object from the given page ."], "references": ["must be overridden . must return a string with the loaded data ."], "bleu": 0.10015045110931886, "rouge_l": 0.17604617604617603}
{"id": 2116, "code": "def check return ( result , func , cargs ) : if result != 0 : s = rt . Error Get Last Error Msg ( ) . decode ( ) msg = 'LAS Error in \"%s\": %s' % ( func . name , s ) rt . Error Reset ( ) raise R Tree Error ( msg ) return True", "predictions": ["check if the result is valid ."], "references": ["error checking for error calls"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2117, "code": "def check void ( result , func , cargs ) : if not bool ( result ) : s = rt . Error Get Last Error Msg ( ) . decode ( ) msg = 'Error in \"%s\": %s' % ( func . name , s ) rt . Error Reset ( ) raise R Tree Error ( msg ) return result", "predictions": ["check if result is valid ."], "references": ["error checking for void * returns"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2118, "code": "def check void done ( result , func , cargs ) : if rt . Error Get Error Count ( ) : s = rt . Error Get Last Error Msg ( ) . decode ( ) msg = 'Error in \"%s\": %s' % ( func . name , s ) rt . Error Reset ( ) raise R Tree Error ( msg ) return result", "predictions": ["check if result is done ."], "references": ["error checking for void * returns that might be empty with no error"], "bleu": 0.0578433294533084, "rouge_l": 0.0}
{"id": 2119, "code": "def load ( self ) : if isinstance ( self . application , str ) : return util . import app ( self . application ) else : return self . application", "predictions": ["load application from the application ."], "references": ["attempt an import of the specified application"], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 2120, "code": "def init app ( self , app ) : if not hasattr ( app , 'extensions' ) : app . extensions = { } if 'common' in app . extensions : raise Runtime Error ( \"Flask-Common extension already initialized\" ) app . extensions [ 'common' ] = self self . app = app if 'COMMON FILESERVER DISABLED' not in app . config : with app . test request context ( ) : app . wsgi app = White Noise ( app . wsgi app , root = url for ( 'static' , filename = '' ) [ 1 : ] ) self . cache = Cache ( app , config = { 'CACHE TYPE' : app . config . get ( \"COMMON CACHE TYPE\" , 'simple' ) } ) @ app . before request def before request callback ( ) : request . start time = maya . now ( ) @ app . after request def after request callback ( response ) : if 'COMMON POWERED BY DISABLED' not in current app . config : response . headers [ 'X-Powered-By' ] = 'Flask' if 'COMMON PROCESSED TIME DISABLED' not in current app . config : response . headers [ 'X-Processed-Time' ] = maya . now ( ) . epoch - request . start time . epoch return response @ app . route ( '/favicon.ico' ) def favicon ( ) : return redirect ( url for ( 'static' , filename = 'favicon.ico' ) , code = 301 )", "predictions": ["initialize the wsgi app ."], "references": ["initializes the flask application with common ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 2121, "code": "def serve ( self , workers = None , * * kwargs ) : if self . app . debug : print ( crayons . yellow ( 'Booting Flask development server...' ) ) self . app . run ( ) else : print ( crayons . yellow ( 'Booting Gunicorn...' ) ) server = Gunicorn Server ( self . app , workers = workers or number of gunicorn workers ( ) , worker class = 'egg:meinheld#gunicorn worker' , * * kwargs ) server . run ( )", "predictions": ["serve up the main application ."], "references": ["serves the flask application ."], "bleu": 0.31239399369202553, "rouge_l": 0.5545454545454546}
{"id": 2122, "code": "def process image ( self , image , image format , save kwargs = { } ) : imagefile = Bytes IO ( ) inv image = Image Ops . invert ( image ) inv image . save ( imagefile , * * save kwargs ) return imagefile", "predictions": ["process an image in the image ."], "references": ["return a bytesio instance of image with inverted colors ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 2123, "code": "def to python ( self , data ) : if data is not None : if hasattr ( data , 'open' ) : data . open ( ) return super ( Versatile Image Form Field , self ) . to python ( data )", "predictions": ["convert data to python ."], "references": ["ensure data is prepped properly before handing off to imagefield ."], "bleu": 0.09778809693469985, "rouge_l": 0.35124760076775424}
{"id": 2124, "code": "def pre save ( self , model instance , add ) : file = super ( Versatile Image Field , self ) . pre save ( model instance , add ) self . update ppoi field ( model instance ) return file", "predictions": ["generates a new model instance ."], "references": ["return field s value just before saving ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 2125, "code": "def formfield ( self , * * kwargs ) : defaults = { } if self . ppoi field : defaults [ 'form class' ] = Sized Image Centerpoint Click Django Admin Field if kwargs . get ( 'widget' ) is Admin File Widget : del kwargs [ 'widget' ] defaults . update ( kwargs ) return super ( Versatile Image Field , self ) . formfield ( * * defaults )", "predictions": ["gets the form field associated with this field ."], "references": ["return a formfield ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 2126, "code": "def value to string ( self , obj ) : if DJANGO VERSION > ( 1 , 9 ) : value = self . value from object ( obj ) else : value = self . get val from obj ( obj ) return self . get prep value ( value )", "predictions": ["returns field s value prepared for saving into a string"], "references": ["prepare field for serialization ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 2127, "code": "def build filters and sizers ( self , ppoi value , create on demand ) : name = self . name if not name and self . field . placeholder image name : name = self . field . placeholder image name self . filters = Filter Library ( name , self . storage , versatileimagefield registry , ppoi value , create on demand ) for ( attr name , sizedimage cls ) in iteritems ( versatileimagefield registry . sizedimage registry ) : setattr ( self , attr name , sizedimage cls ( path to image = name , storage = self . storage , create on demand = create on demand , ppoi = ppoi value ) )", "predictions": ["build the filters and sizers for the field ."], "references": ["build the filters and sizers for a field ."], "bleu": 0.7071067811865475, "rouge_l": 0.8888888888888888}
{"id": 2128, "code": "def get filtered root folder ( self ) : folder , filename = os . path . split ( self . name ) return os . path . join ( folder , VERSATILEIMAGEFIELD FILTERED DIRNAME , '' )", "predictions": ["return the root folder of the root folder ."], "references": ["return the location where filtered images are stored ."], "bleu": 0.19960198807747329, "rouge_l": 0.3333333333333333}
{"id": 2129, "code": "def get sized root folder ( self ) : folder , filename = os . path . split ( self . name ) return os . path . join ( VERSATILEIMAGEFIELD SIZED DIRNAME , folder , '' )", "predictions": ["return the root folder path to the root folder ."], "references": ["return the location where sized images are stored ."], "bleu": 0.17827531042796255, "rouge_l": 0.31881533101045295}
{"id": 2130, "code": "def get filtered sized root folder ( self ) : sized root folder = self . get sized root folder ( ) return os . path . join ( sized root folder , VERSATILEIMAGEFIELD FILTERED DIRNAME )", "predictions": ["return the path to the root folder ."], "references": ["return the location where filtered + sized images are stored ."], "bleu": 0.15587146574232644, "rouge_l": 0.3070469798657718}
{"id": 2131, "code": "def retrieve image ( self , path to image ) : image = self . storage . open ( path to image , 'rb' ) file ext = path to image . rsplit ( '.' ) [ - 1 ] image format , mime type = get image metadata from file ext ( file ext ) return ( Image . open ( image ) , file ext , image format , mime type )", "predictions": ["retrieve the image from the specified path ."], "references": ["return a pil image instance stored at path_to_image ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 2132, "code": "def ppoi as str ( self ) : return \"%s %s\" % ( str ( self . ppoi [ 0 ] ) . replace ( '.' , '-' ) , str ( self . ppoi [ 1 ] ) . replace ( '.' , '-' ) )", "predictions": ["return the string representation of the resource"], "references": ["return ppoi value as a string ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 2133, "code": "def get context ( self , name , value , attrs ) : if self . has template widget rendering : context = super ( Clearable File Input With Image Preview , self ) . get context ( name , value , attrs ) else : context = { } context [ 'widget' ] = { 'name' : name , 'is hidden' : self . is hidden , 'required' : self . is required , 'value' : self . format value ( value ) , 'attrs' : self . build attrs ( self . attrs , attrs ) , 'template name' : self . template name , 'type' : self . input type , } checkbox name = self . clear checkbox name ( name ) checkbox id = self . clear checkbox id ( checkbox name ) context [ 'widget' ] . update ( { 'checkbox name' : checkbox name , 'checkbox id' : checkbox id , 'is initial' : self . is initial ( value ) , 'input text' : self . input text , 'initial text' : self . initial text , 'clear checkbox label' : self . clear checkbox label , } ) if value and hasattr ( value , \"url\" ) : context [ 'widget' ] . update ( { 'hidden field id' : self . get hidden field id ( name ) , 'point stage id' : self . get point stage id ( name ) , 'ppoi id' : self . get ppoi id ( name ) , 'sized url' : self . get sized url ( value ) , 'image preview id' : self . image preview id ( name ) , } ) return context", "predictions": ["override default method to add to the context ."], "references": ["get the context to render this widget with ."], "bleu": 0.21105340631872635, "rouge_l": 0.3333333333333333}
{"id": 2134, "code": "def build attrs ( self , base attrs , extra attrs = None ) : attrs = base attrs . copy ( ) if extra attrs is not None : attrs . update ( extra attrs ) return attrs", "predictions": ["build the attrs dictionary for the given attrs ."], "references": ["build an attribute dictionary ."], "bleu": 0.16784459625186196, "rouge_l": 0.4518518518518518}
{"id": 2135, "code": "def get filtered path ( path to image , filename key , storage ) : containing folder , filename = os . path . split ( path to image ) filtered filename = get filtered filename ( filename , filename key ) path to return = os . path . join ( * [ containing folder , VERSATILEIMAGEFIELD FILTERED DIRNAME , filtered filename ] ) path to return = path to return . replace ( ' ' , '' ) return path to return", "predictions": ["return the path to the filtered filtered filtered ."], "references": ["return the filtered path"], "bleu": 0.23356898886410005, "rouge_l": 0.4959349593495934}
{"id": 2136, "code": "def get url from image key ( image instance , image key ) : img key split = image key . split ( ' ' ) if 'x' in img key split [ - 1 ] : size key = img key split . pop ( - 1 ) else : size key = None img url = reduce ( getattr , img key split , image instance ) if size key : img url = img url [ size key ] . url return img url", "predictions": ["get an image url from an image key ."], "references": ["build a url from image_key ."], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 2137, "code": "def decode bytecode ( bytecode ) : bytecode wnd = memoryview ( bytecode ) while bytecode wnd : opcode id = byte2int ( bytecode wnd [ 0 ] ) opcode = OPCODE MAP [ opcode id ] if opcode . imm struct is not None : offs , imm , = opcode . imm struct . from raw ( None , bytecode wnd [ 1 : ] ) else : imm = None offs = 0 insn len = 1 + offs yield Instruction ( opcode , imm , insn len ) bytecode wnd = bytecode wnd [ insn len : ]", "predictions": ["decode a bytecode into a bytecode"], "references": ["decodes raw bytecode yielding instruction s ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 2138, "code": "def decode module ( module , decode name subsections = False ) : module wnd = memoryview ( module ) hdr = Module Header ( ) hdr len , hdr data , = hdr . from raw ( None , module wnd ) yield Module Fragment ( hdr , hdr data ) module wnd = module wnd [ hdr len : ] while module wnd : sec = Section ( ) sec len , sec data , = sec . from raw ( None , module wnd ) if ( decode name subsections and sec data . id == SEC UNK and sec data . name == SEC NAME ) : sec wnd = sec data . payload while sec wnd : subsec = Name Sub Section ( ) subsec len , subsec data , = subsec . from raw ( None , sec wnd ) yield Module Fragment ( subsec , subsec data ) sec wnd = sec wnd [ subsec len : ] else : yield Module Fragment ( sec , sec data ) module wnd = module wnd [ sec len : ]", "predictions": ["decode a module - encoded module - encoded module ."], "references": ["decodes raw wasm modules yielding modulefragment s ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 2139, "code": "def deprecated func ( func ) : first usage = [ True ] @ functools . wraps ( func ) def wrapper ( * args , * * kwargs ) : if first usage [ 0 ] : warnings . warn ( \"Call to deprecated function {}.\" . format ( func . name ) , Deprecation Warning , ) first usage [ 0 ] = False return func ( * args , * * kwargs ) return wrapper", "predictions": ["decorator to deprecated deprecated ."], "references": ["deprecates a function printing a warning on the first usage ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 2140, "code": "def connect ( self ) : if self . loop is None : self . loop = asyncio . get event loop ( ) t = asyncio . Task ( self . loop . create connection ( self . config [ 'protocol factory' ] , self . config [ 'host' ] , self . config [ 'port' ] , ssl = self . config [ 'ssl' ] ) , loop = self . loop ) t . add done callback ( self . connection made ) return t", "predictions": ["connect to the socket ."], "references": ["connect to the server"], "bleu": 0.5081327481546147, "rouge_l": 0.6802973977695167}
{"id": 2141, "code": "def agi code check ( code = None , response = None , line = None ) : code = int ( code ) response = response or \"\" result = { 'status code' : code , 'result' : ( '' , '' ) , 'msg' : '' } if code == 100 : result [ 'msg' ] = line elif code == 200 : for key , value , data in re kv . findall ( response ) : result [ key ] = ( value , data ) if data == 'hangup' : return { 'error' : 'AGI Result Hangup' , 'msg' : 'User hungup during execution' } elif key == 'result' and value == '-1' : return { 'error' : 'AGI App Error' , 'msg' : 'Error executing application, or hangup' } elif code == 510 : result [ 'error' ] = 'AGI Invalid Command' elif code == 520 : result [ 'error' ] = 'AGI Usage Error' result [ 'msg' ] = line else : result [ 'error' ] = 'AGI Unknown Error' result [ 'msg' ] = line return result", "predictions": ["check the code code for the agi code"], "references": ["check the agi code and return a dict to help on error handling ."], "bleu": 0.1601949014840856, "rouge_l": 0.346590909090909}
{"id": 2142, "code": "def get instances ( self ) : return [ \"<%s prefix:%s (uid:%s)>\" % ( self . class . name , i . prefix , self . uid ) for i in self . instances ]", "predictions": ["return a list of instances of this widget ."], "references": ["mostly used for debugging"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2143, "code": "def gc ( ) : def after delete ( database ) : click . echo ( \"Deleted table %s\" % database ) app = get app ( ) upgrade from old version ( app ) app . delete orphan snapshots ( after delete )", "predictions": ["delete gc snapshots ."], "references": ["deletes old stellar tables that are not used anymore"], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 2144, "code": "def snapshot ( name ) : app = get app ( ) upgrade from old version ( app ) name = name or app . default snapshot name if app . get snapshot ( name ) : click . echo ( \"Snapshot with name %s already exists\" % name ) sys . exit ( 1 ) else : def before copy ( table name ) : click . echo ( \"Snapshotting database %s\" % table name ) app . create snapshot ( name , before copy = before copy )", "predictions": ["create disconnect disconnect to stop stop database"], "references": ["takes a snapshot of the database"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2145, "code": "def list ( ) : snapshots = get app ( ) . get snapshots ( ) click . echo ( '\\n' . join ( '%s: %s' % ( s . snapshot name , humanize . naturaltime ( datetime . utcnow ( ) - s . created at ) ) for s in snapshots ) )", "predictions": ["get a list of all device device names . . . . . . ."], "references": ["returns a list of snapshots"], "bleu": 0.15310245441182443, "rouge_l": 0.32972972972972975}
{"id": 2146, "code": "def restore ( name ) : app = get app ( ) if not name : snapshot = app . get latest snapshot ( ) if not snapshot : click . echo ( \"Couldn't find any snapshots for project %s\" % load config ( ) [ 'project name' ] ) sys . exit ( 1 ) else : snapshot = app . get snapshot ( name ) if not snapshot : click . echo ( \"Couldn't find snapshot with name %s.\\n\" \"You can list snapshots with 'stellar list'\" % name ) sys . exit ( 1 ) if not snapshot . slaves ready : if app . is copy process running ( snapshot ) : sys . stdout . write ( 'Waiting for background process(%s) to finish' % snapshot . worker pid ) sys . stdout . flush ( ) while not snapshot . slaves ready : sys . stdout . write ( '.' ) sys . stdout . flush ( ) sleep ( 1 ) app . db . session . refresh ( snapshot ) click . echo ( '' ) else : click . echo ( 'Background process missing, doing slow restore.' ) app . inline slave copy ( snapshot ) app . restore ( snapshot ) click . echo ( 'Restore complete.' )", "predictions": ["get latest not else get it"], "references": ["restores the database from a snapshot"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2147, "code": "def init ( ) : while True : url = click . prompt ( \"Please enter the url for your database.\\n\\n\" \"For example:\\n\" \"Postgre SQL: postgresql://localhost:5432/\\n\" \"My SQL: mysql+pymysql://root@localhost/\" ) if url . count ( '/' ) == 2 and not url . endswith ( '/' ) : url = url + '/' if ( url . count ( '/' ) == 3 and url . endswith ( '/' ) and url . startswith ( 'postgresql://' ) ) : connection url = url + 'template1' else : connection url = url engine = create engine ( connection url , echo = False ) try : conn = engine . connect ( ) except Operational Error as err : click . echo ( \"Could not connect to database: %s\" % url ) click . echo ( \"Error message: %s\" % err . message ) click . echo ( '' ) else : break if engine . dialect . name not in SUPPORTED DIALECTS : click . echo ( \"Your engine dialect %s is not supported.\" % ( engine . dialect . name ) ) click . echo ( \"Supported dialects: %s\" % ( ', ' . join ( SUPPORTED DIALECTS ) ) ) if url . count ( '/' ) == 3 and url . endswith ( '/' ) : while True : click . echo ( \"You have the following databases: %s\" % ', ' . join ( [ db for db in list of databases ( conn ) if not db . startswith ( 'stellar ' ) ] ) ) db name = click . prompt ( \"Please enter the name of the database (eg. projectdb)\" ) if database exists ( conn , db name ) : break else : click . echo ( \"Could not find database %s\" % db name ) click . echo ( '' ) else : db name = url . rsplit ( '/' , 1 ) [ - 1 ] url = url . rsplit ( '/' , 1 ) [ 0 ] + '/' name = click . prompt ( 'Please enter your project name (used internally, eg. %s)' % db name , default = db name ) raw url = url if engine . dialect . name == 'postgresql' : raw url = raw url + 'template1' with open ( 'stellar.yaml' , 'w' ) as project file : project file . write ( . strip ( ) % { 'name' : name , 'raw url' : raw url , 'url' : url , 'db name' : db name } ) click . echo ( \"Wrote stellar.yaml\" ) click . echo ( '' ) if engine . dialect . name == 'mysql' : click . echo ( \"Warning: My SQL support is still in beta.\" ) click . echo ( \"Tip: You probably want to take a snapshot: stellar snapshot\" )", "predictions": ["take up the database database"], "references": ["initializes stellar configuration ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2148, "code": "def on epoch end ( self ) -> None : self . indexes = np . arange ( self . nrows ) if self . shuffle : np . random . shuffle ( self . indexes )", "predictions": ["re - shuffle the field"], "references": ["updates indexes after each epoch for shuffling"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 2149, "code": "def process text constructor ( cleaner : Callable , tokenizer : Callable , append indicators : bool , start tok : str , end tok : str ) : def process text ( text ) : if append indicators : return [ [ start tok ] + tokenizer ( cleaner ( doc ) ) + [ end tok ] for doc in text ] return [ tokenizer ( cleaner ( doc ) ) for doc in text ] return process text", "predictions": ["check the void of the void in cleaner ."], "references": ["generate a function that will clean and tokenize text ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 2150, "code": "def process text ( self , text : List [ str ] ) -> List [ List [ str ] ] : process text = process text constructor ( cleaner = self . cleaner , tokenizer = self . tokenizer , append indicators = self . append indicators , start tok = self . start tok , end tok = self . end tok ) return process text ( text )", "predictions": ["return the void of the void"], "references": ["combine the cleaner and tokenizer ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 2151, "code": "def generate doc length stats ( self ) : heuristic = self . heuristic pct histdf = ( pd . Data Frame ( [ ( a , b ) for a , b in self . document length histogram . items ( ) ] , columns = [ 'bin' , 'doc count' ] ) . sort values ( by = 'bin' ) ) histdf [ 'cumsum pct' ] = histdf . doc count . cumsum ( ) / histdf . doc count . sum ( ) self . document length stats = histdf self . doc length huerestic = histdf . query ( f'cumsum pct >= {heuristic}' ) . bin . head ( 1 ) . values [ 0 ] logging . warning ( ' ' . join ( [ \"Setting maximum document length to\" , f'{self.doc length huerestic} based upon' , f'heuristic of {heuristic} percentile.\\n' , 'See full histogram by insepecting the' , \"`document length stats` attribute.\" ] ) ) self . padding maxlen = self . doc length huerestic", "predictions": ["generate the self self self str str"], "references": ["analyze document length statistics for padding strategy"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2152, "code": "def token count pandas ( self ) : freq df = pd . Data Frame . from dict ( self . indexer . word counts , orient = 'index' ) freq df . columns = [ 'count' ] return freq df . sort values ( 'count' , ascending = False )", "predictions": ["return of the dataframe"], "references": ["see token counts as pandas dataframe"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 2153, "code": "def inv cls ( cls ) : if cls . fwdm cls is cls . invm cls : return cls if not getattr ( cls , ' inv cls ' , None ) : class Inv ( cls ) : fwdm cls = cls . invm cls invm cls = cls . fwdm cls inv cls = cls Inv . name = cls . name + 'Inv' cls . inv cls = Inv return cls . inv cls", "predictions": ["class decorator to return class classes"], "references": ["the inverse of this bidict type i . e . one with * _fwdm_cls * and * _invm_cls * swapped ."], "bleu": 0.015247336102977219, "rouge_l": 0.0}
{"id": 2154, "code": "def update with rollback ( self , on dup , * args , * * kw ) : writelog = [ ] appendlog = writelog . append dedup item = self . dedup item write item = self . write item for ( key , val ) in iteritems args kw ( * args , * * kw ) : try : dedup result = dedup item ( key , val , on dup ) except Duplication Error : undo write = self . undo write for dedup result , write result in reversed ( writelog ) : undo write ( dedup result , write result ) raise if dedup result is not NOOP : write result = write item ( key , val , dedup result ) appendlog ( ( dedup result , write result ) )", "predictions": ["process a single * * kwargs * in the rollback ."], "references": ["update rolling back on failure ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 2155, "code": "def copy ( self ) : copy = self . class . new ( self . class ) copy . fwdm = self . fwdm . copy ( ) copy . invm = self . invm . copy ( ) copy . init inv ( ) return copy", "predictions": ["return to create a to to a to a to a to a to a to return"], "references": ["a shallow copy ."], "bleu": 0.07223943354597204, "rouge_l": 0.10720562390158171}
{"id": 2156, "code": "def copy ( self ) : copy = self . class . new ( self . class ) sntl = Sentinel ( ) fwdm = self . fwdm . copy ( ) invm = self . invm . copy ( ) cur = sntl nxt = sntl . nxt for ( key , val ) in iteritems ( self ) : nxt = Node ( cur , sntl ) cur . nxt = fwdm [ key ] = invm [ val ] = nxt cur = nxt sntl . prv = nxt copy . sntl = sntl copy . fwdm = fwdm copy . invm = invm copy . init inv ( ) return copy", "predictions": ["return pre - existing pre - existing pre - existing pre - existing nxt instance instance instance instance instance"], "references": ["a shallow copy of this ordered bidict ."], "bleu": 0.05415315253510895, "rouge_l": 0.0}
{"id": 2157, "code": "def clear ( self ) : self . fwdm . clear ( ) self . invm . clear ( ) self . sntl . nxt = self . sntl . prv = self . sntl", "predictions": ["formfield the = 0 kwargs"], "references": ["remove all items ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2158, "code": "def new contact ( cls , address book , supported private objects , version , localize dates ) : return cls ( address book , None , supported private objects , version , localize dates )", "predictions": ["returns a value of the to be used in this class obj ."], "references": ["use this to create a new and empty contact ."], "bleu": 0.12011055432195765, "rouge_l": 0.1781021897810219}
{"id": 2159, "code": "def from user input ( cls , address book , user input , supported private objects , version , localize dates ) : contact = cls ( address book , None , supported private objects , version , localize dates ) contact . process user input ( user input ) return contact", "predictions": ["create . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["use this if you want to create a new contact from user input ."], "bleu": 0.04317900023606586, "rouge_l": 0.09538702111024237}
{"id": 2160, "code": "def add category ( self , categories ) : categories obj = self . vcard . add ( 'categories' ) categories obj . value = helpers . convert to vcard ( \"category\" , categories , Object Type . list with strings )", "predictions": ["get filtered filtered path"], "references": ["categories variable must be a list"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 2161, "code": "def avail archs ( self ) : return { ARM32 : ( KS ARCH ARM , KS MODE ARM ) , ARM64 : ( KS ARCH ARM64 , KS MODE LITTLE ENDIAN ) , ARM TB : ( KS ARCH ARM , KS MODE THUMB ) , HEXAGON : ( KS ARCH HEXAGON , KS MODE BIG ENDIAN ) , MIPS32 : ( KS ARCH MIPS , KS MODE MIPS32 ) , MIPS64 : ( KS ARCH MIPS , KS MODE MIPS64 ) , PPC32 : ( KS ARCH PPC , KS MODE PPC32 ) , PPC64 : ( KS ARCH PPC , KS MODE PPC64 ) , SPARC32 : ( KS ARCH SPARC , KS MODE SPARC32 ) , SPARC64 : ( KS ARCH SPARC , KS MODE SPARC64 ) , SYSTEMZ : ( KS ARCH SYSTEMZ , KS MODE BIG ENDIAN ) , X86 16 : ( KS ARCH X86 , KS MODE 16 ) , X86 32 : ( KS ARCH X86 , KS MODE 32 ) , X86 64 : ( KS ARCH X86 , KS MODE 64 ) , }", "predictions": ["returns a dictionary of the sized terms ."], "references": ["initialize the dictionary of architectures for assembling via keystone"], "bleu": 0.20014292374951972, "rouge_l": 0.232824427480916}
{"id": 2162, "code": "def avail archs ( self ) : return { ARM32 : ( CS ARCH ARM , CS MODE ARM ) , ARM64 : ( CS ARCH ARM64 , CS MODE LITTLE ENDIAN ) , ARM TB : ( CS ARCH ARM , CS MODE THUMB ) , MIPS32 : ( CS ARCH MIPS , CS MODE MIPS32 ) , MIPS64 : ( CS ARCH MIPS , CS MODE MIPS64 ) , SPARC32 : ( CS ARCH SPARC , CS MODE BIG ENDIAN ) , SPARC64 : ( CS ARCH SPARC , CS MODE V9 ) , SYSTEMZ : ( CS ARCH SYSZ , CS MODE BIG ENDIAN ) , X86 16 : ( CS ARCH X86 , CS MODE 16 ) , X86 32 : ( CS ARCH X86 , CS MODE 32 ) , X86 64 : ( CS ARCH X86 , CS MODE 64 ) , }", "predictions": ["returns a list of all the filtered parameters ."], "references": ["initialize the dictionary of architectures for disassembling via capstone"], "bleu": 0.15619699684601276, "rouge_l": 0.1111111111111111}
{"id": 2163, "code": "def safe input ( prompt ) : if sys . version info < ( 3 , 0 ) : if isinstance ( prompt , compat . text type ) : encoding = locale . getpreferredencoding ( ) or 'utf-8' prompt = prompt . encode ( encoding ) else : if not isinstance ( prompt , compat . text type ) : prompt = prompt . decode ( ) return input ( prompt )", "predictions": ["handle image and self . . . . . . . . . . ."], "references": ["prompts user for input . correctly handles prompt message encoding ."], "bleu": 0.09103526405546068, "rouge_l": 0.1582360570687419}
{"id": 2164, "code": "def first ( self ) : lim = [ 0 , 1 ] if self . limit : lim [ 0 ] = self . limit [ 0 ] if not self . filters and not self . order by : for ent in self : return ent return None ids = self . limit ( * lim ) . search ( ) if ids : return self . model . get ( ids [ 0 ] ) return None", "predictions": ["returns the first of the page 0 if any ."], "references": ["returns only the first result from the query if any ."], "bleu": 0.27129029006327077, "rouge_l": 0.6609907120743034}
{"id": 2165, "code": "def redis prefix lua ( conn , dest , index , prefix , is first , pattern = None ) : tkey = '%s:%s' % ( index . partition ( ':' ) [ 0 ] , uuid . uuid4 ( ) ) start , end = start end ( prefix ) return redis prefix lua ( conn , [ dest , tkey , index ] , [ start , end , pattern or prefix , int ( pattern is not None ) , int ( bool ( is first ) ) ] )", "predictions": ["display a context for a context in a get context ."], "references": ["performs the actual prefix suffix and pattern match operations ."], "bleu": 0.11390778025531027, "rouge_l": 0.09606299212598425}
{"id": 2166, "code": "def add ( self , obj ) : if self . null session : return self . init ( ) pk = obj . pk if not pk . endswith ( ':None' ) : self . known [ pk ] = obj self . wknown [ pk ] = obj", "predictions": ["build a new object from the collection . . . ."], "references": ["adds an entity to the session ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 2167, "code": "def get ( self , pk ) : self . init ( ) return self . known . get ( pk ) or self . wknown . get ( pk )", "predictions": ["storage storage for to to get a resource object"], "references": ["fetches an entity from the session based on primary key ."], "bleu": 0.09503475972243516, "rouge_l": 0.0}
{"id": 2168, "code": "def register ( cls , type , reduce func ) : if sys . version info < ( 3 , ) : def dispatcher ( cls , obj ) : reduced = reduce func ( obj ) cls . save reduce ( obj = obj , * reduced ) cls . dispatch table [ type ] = dispatcher else : cls . dispatch table [ type ] = reduce func", "predictions": ["get a reduce split function split it into the collection split split split by key split split split it into the given key split split split it into the given key"], "references": ["attach a reducer function to a given type in the dispatch table ."], "bleu": 0.04906081629292276, "rouge_l": 0.19629927594529364}
{"id": 2169, "code": "def Queue ( self , maxsize = 0 , reducers = None ) : from . queues import Queue return Queue ( maxsize , reducers = reducers , ctx = self . get context ( ) )", "predictions": ["print the 0 - 1 - 1 - based 0 - 16 - 16 - 16 - 1 1 ."], "references": ["returns a queue object"], "bleu": 0.051366639095059514, "rouge_l": 0.0}
{"id": 2170, "code": "def Simple Queue ( self , reducers = None ) : from . queues import Simple Queue return Simple Queue ( reducers = reducers , ctx = self . get context ( ) )", "predictions": ["return up the hdr of the hdr"], "references": ["returns a queue object"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2171, "code": "def sendback result ( result queue , work id , result = None , exception = None ) : try : result queue . put ( Result Item ( work id , result = result , exception = exception ) ) except Base Exception as e : exc = Exception With Traceback ( e ) result queue . put ( Result Item ( work id , exception = exc ) )", "predictions": ["deprecated use 1 - use deprecated instead of results ."], "references": ["safely send back the given result or exception"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2172, "code": "def ensure executor running ( self ) : with self . processes management lock : if len ( self . processes ) != self . max workers : self . adjust process count ( ) self . start queue management thread ( )", "predictions": ["ensures that the processes is self is self is self is self is self is self is self is self is self is self is self is self is self is"], "references": ["ensures all workers and management thread are running"], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 2173, "code": "def start ( self , initializer = None , initargs = ( ) ) : assert self . state . value == State . INITIAL if ( initializer is not None and not hasattr ( initializer , ' call ' ) ) : raise Type Error ( 'initializer must be a callable' ) reader , writer = mp . Pipe ( duplex = False ) self . process = Process ( target = type ( self ) . run server , args = ( self . registry , self . address , bytes ( self . authkey ) , self . serializer , writer , initializer , initargs ) , ) ident = ':' . join ( str ( i ) for i in self . process . identity ) self . process . name = type ( self ) . name + '-' + ident self . process . start ( ) writer . close ( ) self . address = reader . recv ( ) reader . close ( ) self . state . value = State . STARTED self . shutdown = mp . util . Finalize ( self , type ( self ) . finalize manager , args = ( self . process , self . address , self . authkey , self . state , self . Client ) , exitpriority = 0 )", "predictions": ["agi the process process process int int int int int ."], "references": ["spawn a server process for this manager object"], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 2174, "code": "def Dup Fd ( fd ) : popen obj = get spawning popen ( ) if popen obj is not None : return popen obj . Dup Fd ( popen obj . duplicate for child ( fd ) ) elif HAVE SEND HANDLE and sys . version info [ : 2 ] > ( 3 , 3 ) : from multiprocessing import resource sharer return resource sharer . Dup Fd ( fd ) else : raise Type Error ( 'Cannot pickle connection object. This object can only be ' 'passed when spawning a new process' )", "predictions": ["create a new object from a string"], "references": ["return a wrapper for an fd ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 2175, "code": "def wait job completion ( self ) : if len ( self . pending work items ) > 0 : warnings . warn ( \"Trying to resize an executor with running jobs: \" \"waiting for jobs completion before resizing.\" , User Warning ) mp . util . debug ( \"Executor {} waiting for jobs completion before\" \" resizing\" . format ( self . executor id ) ) while len ( self . pending work items ) > 0 : time . sleep ( 1e-3 )", "predictions": ["gc completion completion until stopped echo the orphan items"], "references": ["wait for the cache to be empty before resizing the pool ."], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 2176, "code": "def get preparation data ( name , init main module = True ) : check not importing main ( ) d = dict ( log to stderr = util . log to stderr , authkey = bytes ( process . current process ( ) . authkey ) , ) if util . logger is not None : d [ 'log level' ] = util . logger . get Effective Level ( ) if len ( util . logger . handlers ) > 0 : h = util . logger . handlers [ 0 ] d [ 'log fmt' ] = h . formatter . fmt sys path = [ p for p in sys . path ] try : i = sys path . index ( '' ) except Value Error : pass else : sys path [ i ] = process . ORIGINAL DIR d . update ( name = name , sys path = sys path , sys argv = sys . argv , orig dir = process . ORIGINAL DIR , dir = os . getcwd ( ) ) if sys . platform != \"win32\" : from . import semaphore tracker semaphore tracker . ensure running ( ) d [ 'tracker pid' ] = semaphore tracker . semaphore tracker . pid if init main module : main module = sys . modules [ ' main ' ] try : main mod name = getattr ( main module . spec , \"name\" , None ) except Base Exception : main mod name = None if main mod name is not None : d [ 'init main from name' ] = main mod name elif sys . platform != 'win32' or ( not WINEXE and not WINSERVICE ) : main path = getattr ( main module , ' file ' , None ) if main path is not None : if ( not os . path . isabs ( main path ) and process . ORIGINAL DIR is not None ) : main path = os . path . join ( process . ORIGINAL DIR , main path ) d [ 'init main from path' ] = os . path . normpath ( main path ) d [ 'main path' ] = d [ 'init main from path' ] return d", "predictions": ["get preparation data from preparation ."], "references": ["return info about parent needed by child to unpickle process object"], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 2177, "code": "def prepare ( data ) : if 'name' in data : process . current process ( ) . name = data [ 'name' ] if 'authkey' in data : process . current process ( ) . authkey = data [ 'authkey' ] if 'log to stderr' in data and data [ 'log to stderr' ] : util . log to stderr ( ) if 'log level' in data : util . get logger ( ) . set Level ( data [ 'log level' ] ) if 'log fmt' in data : import logging util . get logger ( ) . handlers [ 0 ] . set Formatter ( logging . Formatter ( data [ 'log fmt' ] ) ) if 'sys path' in data : sys . path = data [ 'sys path' ] if 'sys argv' in data : sys . argv = data [ 'sys argv' ] if 'dir' in data : os . chdir ( data [ 'dir' ] ) if 'orig dir' in data : process . ORIGINAL DIR = data [ 'orig dir' ] if 'tracker pid' in data : from . import semaphore tracker semaphore tracker . semaphore tracker . pid = data [ \"tracker pid\" ] if 'init main from name' in data : fixup main from name ( data [ 'init main from name' ] ) elif 'init main from path' in data : fixup main from path ( data [ 'init main from path' ] )", "predictions": ["prepare data for current process ."], "references": ["try to get current process ready to unpickle process object"], "bleu": 0.14925824694560996, "rouge_l": 0.23921568627450981}
{"id": 2178, "code": "def close fds ( keep fds ) : keep fds = set ( keep fds ) . union ( [ 1 , 2 ] ) try : open fds = set ( int ( fd ) for fd in os . listdir ( '/proc/self/fd' ) ) except File Not Found Error : import resource max nfds = resource . getrlimit ( resource . RLIMIT NOFILE ) [ 0 ] open fds = set ( fd for fd in range ( 3 , max nfds ) ) open fds . add ( 0 ) for i in open fds - keep fds : try : os . close ( i ) except OS Error : pass", "predictions": ["close all open files ."], "references": ["close all the file descriptors except those in keep_fds ."], "bleu": 0.14203729394569906, "rouge_l": 0.37731958762886597}
{"id": 2179, "code": "def recursive terminate without psutil ( process ) : try : recursive terminate ( process . pid ) except OS Error as e : warnings . warn ( \"Failed to kill subprocesses on this platform. Please\" ) process . terminate ( ) process . join ( )", "predictions": ["terminate a recursive process without blocking ."], "references": ["terminate a process and its descendants ."], "bleu": 0.2777619034011791, "rouge_l": 0.5714285714285714}
{"id": 2180, "code": "def recursive terminate ( pid ) : if sys . platform == \"win32\" : try : subprocess . check output ( [ \"taskkill\" , \"/F\" , \"/T\" , \"/PID\" , str ( pid ) ] , stderr = None ) except subprocess . Called Process Error as e : if e . returncode not in [ 1 , 128 , 255 ] : raise elif e . returncode == 1 : try : os . kill ( pid , signal . SIGTERM ) except OS Error as e : if e . errno != errno . ESRCH : raise else : try : children pids = subprocess . check output ( [ \"pgrep\" , \"-P\" , str ( pid ) ] , stderr = None ) except subprocess . Called Process Error as e : if e . returncode == 1 : children pids = b'' else : raise children pids = children pids . decode ( ) . split ( '\\n' ) [ : - 1 ] for cpid in children pids : cpid = int ( cpid ) recursive terminate ( cpid ) try : os . kill ( pid , signal . SIGTERM ) except OS Error as e : if e . errno != errno . ESRCH : raise", "predictions": ["terminate a running process ."], "references": ["recursively kill the descendants of a process before killing it ."], "bleu": 0.09778809693469985, "rouge_l": 0.35124760076775424}
{"id": 2181, "code": "def format exitcodes ( exitcodes ) : str exitcodes = [ \"{}({})\" . format ( get exitcode name ( e ) , e ) for e in exitcodes if e is not None ] return \"{\" + \", \" . join ( str exitcodes ) + \"}\"", "predictions": ["format the exitcodes name for the exitcodes"], "references": ["format a list of exit code with names of the signals if possible"], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 2182, "code": "def main ( fd , verbose = 0 ) : signal . signal ( signal . SIGINT , signal . SIG IGN ) signal . signal ( signal . SIGTERM , signal . SIG IGN ) if HAVE SIGMASK : signal . pthread sigmask ( signal . SIG UNBLOCK , IGNORED SIGNALS ) for f in ( sys . stdin , sys . stdout ) : try : f . close ( ) except Exception : pass if verbose : sys . stderr . write ( \"Main semaphore tracker is running\\n\" ) sys . stderr . flush ( ) cache = set ( ) try : with os . fdopen ( fd , 'rb' ) as f : for line in f : try : cmd , name = line . strip ( ) . split ( b':' ) if cmd == b'REGISTER' : name = name . decode ( 'ascii' ) cache . add ( name ) if verbose : sys . stderr . write ( \"[Semaphore Tracker] register {}\\n\" . format ( name ) ) sys . stderr . flush ( ) elif cmd == b'UNREGISTER' : name = name . decode ( 'ascii' ) cache . remove ( name ) if verbose : sys . stderr . write ( \"[Semaphore Tracker] unregister {}\" \": cache({})\\n\" . format ( name , len ( cache ) ) ) sys . stderr . flush ( ) elif cmd == b'PROBE' : pass else : raise Runtime Error ( 'unrecognized command %r' % cmd ) except Base Exception : try : sys . excepthook ( * sys . exc info ( ) ) except Base Exception : pass finally : if cache : try : warnings . warn ( 'semaphore tracker: There appear to be %d ' 'leaked semaphores to clean up at shutdown' % len ( cache ) ) except Exception : pass for name in cache : try : try : sem unlink ( name ) if verbose : sys . stderr . write ( \"[Semaphore Tracker] unlink {}\\n\" . format ( name ) ) sys . stderr . flush ( ) except Exception as e : warnings . warn ( 'semaphore tracker: %s: %r' % ( name , e ) ) finally : pass if verbose : sys . stderr . write ( \"semaphore tracker shut down\\n\" ) sys . stderr . flush ( )", "predictions": ["main entry point for the application ."], "references": ["run semaphore tracker ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 2183, "code": "def event processor ( self , frame , event , arg ) : out = self . debugger . intf [ - 1 ] . output lineno = frame . f lineno filename = self . core . canonic filename ( frame ) filename = self . core . filename ( filename ) if not out : print ( \"%s - %s:%d\" % ( event , filename , lineno ) ) else : out . write ( \"%s - %s:%d\" % ( event , filename , lineno ) ) if arg is not None : out . writeline ( ', %s ' % repr ( arg ) ) else : out . writeline ( '' ) pass pass return self . event processor", "predictions": ["process event processor ."], "references": ["a simple event processor that prints out events ."], "bleu": 0.14558246978804804, "rouge_l": 0.43160377358490565}
{"id": 2184, "code": "def run hooks ( obj , hooks , * args ) : for hook in hooks : if hook ( obj , * args ) : return True pass return False", "predictions": ["runs the specified hooks in the specified hooks ."], "references": ["run each function in hooks with args"], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 2185, "code": "def forget ( self ) : self . stack = [ ] self . curindex = 0 self . curframe = None self . thread name = None self . frame thread name = None return", "predictions": ["forget the current thread ."], "references": ["remove memory of state variables set in the command processor"], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 2186, "code": "def process commands ( self ) : if self . core . execution status != 'No program' : self . setup ( ) self . location ( ) pass leave loop = run hooks ( self , self . preloop hooks ) self . continue running = False while not leave loop : try : run hooks ( self , self . precmd hooks ) leave loop = self . process command ( ) if leave loop or self . continue running : break except EOF Error : if len ( self . debugger . intf ) > 1 : del self . debugger . intf [ - 1 ] self . last command = '' else : if self . debugger . intf [ - 1 ] . output : self . debugger . intf [ - 1 ] . output . writeline ( 'Leaving' ) raise Mexcept . Debugger Quit pass break pass pass return run hooks ( self , self . postcmd hooks )", "predictions": ["run the commands in the queue ."], "references": ["handle debugger commands ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 2187, "code": "def read history file ( self ) : histfile = self . debugger . intf [ - 1 ] . histfile try : import readline readline . read history file ( histfile ) except IO Error : pass except Import Error : pass return", "predictions": ["read the history file"], "references": ["read the command history file -- possibly ."], "bleu": 0.21874242445215208, "rouge_l": 0.6288659793814433}
{"id": 2188, "code": "def write history file ( self ) : settings = self . debugger . settings histfile = self . debugger . intf [ - 1 ] . histfile if settings [ 'hist save' ] : try : import readline try : readline . write history file ( histfile ) except IO Error : pass except Import Error : pass pass return", "predictions": ["write the history file ."], "references": ["write the command history file -- possibly ."], "bleu": 0.2595169998796149, "rouge_l": 0.7384987893462469}
{"id": 2189, "code": "def errmsg ( self , msg , prefix = \"** \" ) : if not self . verbose : location = ( \"%s:%s: Error in source command file\" % ( self . script name , self . input lineno ) ) msg = \"%s%s:\\n%s%s\" % ( prefix , location , prefix , msg ) else : msg = \"%s%s\" % ( prefix , msg ) pass self . msg ( msg ) if self . abort on error : raise EOF Error return", "predictions": ["verbose - command message ."], "references": ["common routine for reporting debugger error messages ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2190, "code": "def close ( self ) : self . state = 'closing' if self . input : self . input . close ( ) pass if self . output : self . output . close ( ) pass self . state = 'disconnnected' return", "predictions": ["close the connection ."], "references": ["closes both input and output"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 2191, "code": "def disassemble ( msg , msg nocr , section , co , lasti = - 1 , start line = - 1 , end line = None , relative pos = False , highlight = 'light' , start offset = 0 , end offset = None ) : return disassemble bytes ( msg , msg nocr , co . co code , lasti , co . co firstlineno , start line , end line , relative pos , co . co varnames , co . co names , co . co consts , co . co cellvars , co . co freevars , dict ( findlinestarts ( co ) ) , highlight , start offset = start offset , end offset = end offset )", "predictions": ["disassemble a code section ."], "references": ["disassemble a code object ."], "bleu": 0.537284965911771, "rouge_l": 0.8}
{"id": 2192, "code": "def count frames ( frame , count start = 0 ) : count = - count start while frame : count += 1 frame = frame . f back return count", "predictions": ["count the number of frames in a frame ."], "references": ["return a count of the number of frames"], "bleu": 0.42728700639623407, "rouge_l": 0.594541910331384}
{"id": 2193, "code": "def print stack trace ( proc obj , count = None , color = 'plain' , opts = { } ) : if count is None : n = len ( proc obj . stack ) else : n = min ( len ( proc obj . stack ) , count ) try : for i in range ( n ) : print stack entry ( proc obj , i , color = color , opts = opts ) except Keyboard Interrupt : pass return", "predictions": ["print the count of a stack"], "references": ["print count entries of the stack trace"], "bleu": 0.2460852465666396, "rouge_l": 0.6069651741293532}
{"id": 2194, "code": "def eval print obj ( arg , frame , format = None , short = False ) : try : if not frame : val = eval ( arg , None , None ) else : val = eval ( arg , frame . f globals , frame . f locals ) pass except : return 'No symbol \"' + arg + '\" in current context.' return print obj ( arg , val , format , short )", "predictions": ["evaluate an argument in a statement"], "references": ["return a string representation of an object"], "bleu": 0.20693220168471366, "rouge_l": 0.1517412935323383}
{"id": 2195, "code": "def print obj ( arg , val , format = None , short = False ) : what = arg if format : what = format + ' ' + arg val = Mprint . printf ( val , format ) pass s = '%s = %s' % ( what , val ) if not short : s += '\\n  type = %s' % type ( val ) s = print dict ( s , val , \"object variables\" ) if hasattr ( val , \" class \" ) : s = print dict ( s , val . class , \"class variables\" ) pass pass return s", "predictions": ["print the string representation of a object"], "references": ["return a string representation of an object"], "bleu": 0.38260294162784475, "rouge_l": 0.5714285714285714}
{"id": 2196, "code": "def lookup ( self , subcmd prefix ) : for subcmd name in list ( self . subcmds . keys ( ) ) : if subcmd name . startswith ( subcmd prefix ) and len ( subcmd prefix ) >= self . subcmds [ subcmd name ] . class . min abbrev : return self . subcmds [ subcmd name ] pass return None", "predictions": ["looks up the subcmd with the given prefix ."], "references": ["find subcmd in self . subcmds"], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 2197, "code": "def short help ( self , subcmd cb , subcmd name , label = False ) : entry = self . lookup ( subcmd name ) if entry : if label : prefix = entry . name else : prefix = '' pass if hasattr ( entry , 'short help' ) : if prefix : prefix += ' -- ' self . cmd obj . msg ( prefix + entry . short help ) pass pass else : self . undefined subcmd ( \"help\" , subcmd name ) pass return", "predictions": ["shows the help for the given subcmd ."], "references": ["show short help for a subcommand ."], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 2198, "code": "def run ( self , subcmd name , arg ) : entry = self . lookup ( subcmd name ) if entry : entry [ 'callback' ] ( arg ) else : self . cmdproc . undefined cmd ( entry . class . name , subcmd name ) pass return", "predictions": ["run a shell command ."], "references": ["run subcmd_name with args using obj for the environent"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 2199, "code": "def help ( self , * args ) : print ( args ) subcmd prefix = args [ 0 ] if not subcmd prefix or len ( subcmd prefix ) == 0 : self . msg ( self . doc ) self . msg ( % ( self . name ) ) for subcmd name in self . list ( ) : self . subcmd helper ( subcmd name , self , True , True ) return entry = self . lookup ( subcmd prefix ) if entry and hasattr ( entry , 'help' ) : entry . help ( args ) else : self . cmd obj . errmsg ( \"Unknown 'help %s' subcommand %s\" % ( self . name , subcmd prefix ) )", "predictions": ["display a list of available subcmd"], "references": ["help for subcommands ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2200, "code": "def list categories ( self ) : self . section ( \"Classes of commands:\" ) cats = list ( categories . keys ( ) ) cats . sort ( ) for cat in cats : self . msg ( \"  %-13s -- %s\" % ( cat , categories [ cat ] ) ) pass final msg = for line in re . compile ( '\\n' ) . split ( final msg . rstrip ( '\\n' ) ) : self . rst msg ( line ) pass return", "predictions": ["list all categories in the section"], "references": ["list the command categories and a short description of each ."], "bleu": 0.1141650334026257, "rouge_l": 0.2234432234432234}
{"id": 2201, "code": "def show category ( self , category , args ) : n2cmd = self . proc . commands names = list ( n2cmd . keys ( ) ) if len ( args ) == 1 and args [ 0 ] == '*' : self . section ( \"Commands in class %s:\" % category ) cmds = [ cmd for cmd in names if category == n2cmd [ cmd ] . category ] cmds . sort ( ) self . msg nocr ( self . columnize commands ( cmds ) ) return self . msg ( \"%s.\\n\" % categories [ category ] ) self . section ( \"List of commands:\" ) names . sort ( ) for name in names : if category != n2cmd [ name ] . category : continue self . msg ( \"%-13s -- %s\" % ( name , n2cmd [ name ] . short help , ) ) pass return", "predictions": ["show the category category"], "references": ["show short help for all commands in category ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 2202, "code": "def run ( self , args ) : if not self . proc . curframe : self . errmsg ( \"No line number information available.\" ) return if len ( args ) == 3 : answer = self . lineinfo ( args [ 2 ] ) if answer [ 0 ] : item , filename , lineno = answer if not os . path . isfile ( filename ) : filename = Mclifns . search file ( filename , self . core . search path , self . main dirname ) self . msg ( 'Line %s of \"%s\" <%s>' % ( lineno , filename , item ) ) return filename = self . core . canonic filename ( self . proc . curframe ) if not os . path . isfile ( filename ) : filename = Mclifns . search file ( filename , self . core . search path , self . main dirname ) pass filename = self . core . canonic filename ( self . proc . curframe ) msg1 = 'Line %d of \\\"%s\\\"' % ( inspect . getlineno ( self . proc . curframe ) , self . core . filename ( filename ) ) msg2 = ( 'at instruction %d' % self . proc . curframe . f lasti ) if self . proc . event : msg2 += ', %s event' % self . proc . event pass self . msg ( Mmisc . wrapped lines ( msg1 , msg2 , self . settings [ 'width' ] ) ) return False", "predictions": ["run the line program ."], "references": ["current line number in source file"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 2203, "code": "def map thread names ( ) : name2id = { } for thread id in list ( threading . active . keys ( ) ) : thread = threading . active [ thread id ] name = thread . get Name ( ) if name not in list ( name2id . keys ( ) ) : name2id [ name ] = thread id pass pass return name2id", "predictions": ["map thread names to name2id"], "references": ["invert threading . _active"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2204, "code": "def open ( self , inp , opts = None ) : if isinstance ( inp , list ) : self . input = inp else : raise IO Error ( \"Invalid input type (%s) for %s\" % ( type ( inp ) , inp ) ) return", "predictions": ["open an input file ."], "references": ["use this to set where to read from ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 2205, "code": "def get int ( errmsg , arg , default = 1 , cmdname = None ) : if arg : try : default = int ( eval ( arg ) ) except ( Syntax Error , Name Error , Value Error ) : if cmdname : errmsg ( \"Command '%s' expects an integer; got: %s.\" % ( cmdname , str ( arg ) ) ) else : errmsg ( 'Expecting an integer, got: %s.' % str ( arg ) ) pass raise Value Error return default", "predictions": ["get an integer from an integer or default ."], "references": ["if arg is an int use that otherwise take default ."], "bleu": 0.15982877755018768, "rouge_l": 0.2946859903381642}
{"id": 2206, "code": "def run show int ( obj , what = None ) : val = obj . debugger . settings [ obj . name ] if not what : what = obj . name return obj . msg ( \"%s is %d.\" % ( what , val ) )", "predictions": ["show the value of an object ."], "references": ["generic subcommand integer value display"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 2207, "code": "def run show val ( obj , name ) : val = obj . debugger . settings [ obj . name ] obj . msg ( \"%s is %s.\" % ( obj . name , obj . cmd . proc . saferepr ( val ) , ) ) return False", "predictions": ["run the code object ."], "references": ["generic subcommand value display"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2208, "code": "def is def stmt ( line , frame ) : return ( line and re def . match ( line ) and op at frame ( frame ) == 'LOAD CONST' and stmt contains opcode ( frame . f code , frame . f lineno , 'MAKE FUNCTION' ) )", "predictions": ["module is a frame frame"], "references": ["return true if we are looking at a def statement"], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 2209, "code": "def is class def ( line , frame ) : return ( line and re class . match ( line ) and stmt contains opcode ( frame . f code , frame . f lineno , 'BUILD CLASS' ) )", "predictions": ["process whether the if the if the if the if it is a class class name name"], "references": ["return true if we are looking at a class definition statement"], "bleu": 0.10216198665886358, "rouge_l": 0.22289890377588306}
{"id": 2210, "code": "def nothread quit ( self , arg ) : self . debugger . core . stop ( ) self . debugger . core . execution status = 'Quit command' raise Mexcept . Debugger Quit", "predictions": ["1 - fds fds"], "references": ["quit command when there s just one thread ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 2211, "code": "def threaded quit ( self , arg ) : threading list = threading . enumerate ( ) mythread = threading . current Thread ( ) for t in threading list : if t != mythread : ctype async raise ( t , Mexcept . Debugger Quit ) pass pass raise Mexcept . Debugger Quit", "predictions": ["terminate the given argument except it s not already been parsed except it is not a terminate except it is not already ."], "references": ["quit command when several threads are involved ."], "bleu": 0.05291907393644996, "rouge_l": 0.07068366164542293}
{"id": 2212, "code": "def main ( dbg = None , sys argv = list ( sys . argv ) ) : global title orig sys argv = list ( sys argv ) opts , dbg opts , sys argv = process options ( title , version , sys argv ) dbg opts [ 'orig sys argv' ] = sys argv dbg opts [ 'interface' ] = Mbullwinkle . BW Interface ( ) dbg opts [ 'processor' ] = 'bullwinkle' if dbg is None : dbg = Mdebugger . Trepan ( dbg opts ) dbg . core . add ignore ( main ) pass postprocess options ( dbg , opts ) if len ( sys argv ) == 0 : mainpyfile = None else : mainpyfile = sys argv [ 0 ] if not os . path . isfile ( mainpyfile ) : mainpyfile = Mclifns . whence file ( mainpyfile ) is readable = Mfile . readable ( mainpyfile ) if is readable is None : print ( \"%s: Python script file '%s' does not exist\" % ( title , mainpyfile , ) ) sys . exit ( 1 ) elif not is readable : print ( \"%s: Can't read Python script file '%s'\" % ( title , mainpyfile , ) ) sys . exit ( 1 ) return mainpyfile noopt = Mfile . file pyc2py ( mainpyfile ) if mainpyfile != mainpyfile noopt and Mfile . readable ( mainpyfile noopt ) : print ( \"%s: Compiled Python script given and we can't use that.\" % title ) print ( \"%s: Substituting non-compiled name: %s\" % ( title , mainpyfile noopt , ) ) mainpyfile = mainpyfile noopt pass sys . path [ 0 ] = dbg . main dirname = os . path . dirname ( mainpyfile ) dbg . sig received = False while True : try : if dbg . program sys argv and mainpyfile : normal termination = dbg . run script ( mainpyfile ) if not normal termination : break else : dbg . core . execution status = 'No program' dbg . core . processor . process commands ( ) pass dbg . core . execution status = 'Terminated' dbg . intf [ - 1 ] . msg ( \"The program finished - quit or restart\" ) dbg . core . processor . process commands ( ) except Mexcept . Debugger Quit : break except Mexcept . Debugger Restart : dbg . core . execution status = 'Restart requested' if dbg . program sys argv : sys . argv = list ( dbg . program sys argv ) part1 = ( 'Restarting %s with arguments:' % dbg . core . filename ( mainpyfile ) ) args = ' ' . join ( dbg . program sys argv [ 1 : ] ) dbg . intf [ - 1 ] . msg ( Mmisc . wrapped lines ( part1 , args , dbg . settings [ 'width' ] ) ) else : break except System Exit : break pass sys . argv = orig sys argv return", "predictions": ["recursive function for the command line interface platform platform platform platform platform"], "references": ["routine which gets run if we were invoked directly"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 2213, "code": "def signature ( frame ) : if not frame : return None code = frame . f code return ( code . co name , code . co filename , code . co firstlineno )", "predictions": ["returns the format of the given frame frame exitcode exitcode exitcode ."], "references": ["return suitable frame signature to key display expressions off of ."], "bleu": 0.1235622127262679, "rouge_l": 0.17528735632183906}
{"id": 2214, "code": "def all ( self ) : found = False s = [ ] for display in self . list : if not found : s . append ( ) found = True pass s . append ( display . format ( ) ) return s", "predictions": ["returns a list of all the items in the if any"], "references": ["list all display items ; return 0 if none"], "bleu": 0.14323145079400493, "rouge_l": 0.4073455759599332}
{"id": 2215, "code": "def delete index ( self , display number ) : old size = len ( self . list ) self . list = [ disp for disp in self . list if display number != disp . number ] return old size != len ( self . list )", "predictions": ["event event in the list"], "references": ["delete display expression * display_number *"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 2216, "code": "def display ( self , frame ) : if not frame : return s = [ ] sig = signature ( frame ) for display in self . list : if display . signature == sig and display . enabled : s . append ( display . to s ( frame ) ) pass pass return s", "predictions": ["returns a list of frame objects"], "references": ["display any items that are active"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2217, "code": "def debug ( frame = None ) : if frame is None : frame = frame ( ) . f back dbg = Remote Celery Trepan ( ) dbg . say ( BANNER . format ( self = dbg ) ) trepan . api . debug ( dbg opts = dbg . dbg opts )", "predictions": ["write debugging information to the trepan instance"], "references": ["set breakpoint at current location or a specified frame"], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 2218, "code": "def run ( self , args ) : if len ( args ) < 2 : self . section ( \"List of %s commands (with minimum abbreviation in \" \"parenthesis):\" % self . name ) for subcmd name in self . cmds . list ( ) : subcmd = self . cmds . subcmds [ subcmd name ] self . summary help ( subcmd name , subcmd ) pass return False subcmd prefix = args [ 1 ] subcmd = self . cmds . lookup ( subcmd prefix ) if subcmd : nargs = len ( args ) - 2 if nargs < subcmd . min args : self . errmsg ( ( \"Subcommand '%s %s' needs at least %d argument(s); \" + \"got %d.\" ) % ( self . name , subcmd . name , subcmd . min args , nargs ) ) return False if subcmd . max args is not None and nargs > subcmd . max args : self . errmsg ( ( \"Subcommand '%s %s' takes at most %d argument(s); \" + \"got %d.\" ) % ( self . name , subcmd . name , subcmd . max args , nargs ) ) return False return subcmd . run ( args [ 2 : ] ) else : return self . undefined subcmd ( self . name , subcmd prefix ) return", "predictions": ["runs the specified pass program . ."], "references": ["ooops -- the debugger author didn t redefine this run docstring ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 2219, "code": "def undefined subcmd ( self , cmd , subcmd ) : self . proc . intf [ - 1 ] . errmsg ( ( 'Undefined \"%s\" subcommand: \"%s\". ' + 'Try \"help %s *\".' ) % ( cmd , subcmd , cmd ) ) return", "predictions": ["read a new memory"], "references": ["error message when subcommand asked for but doesn t exist"], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 2220, "code": "def info signal ( self , args ) : if len ( args ) == 0 : return None signame = args [ 0 ] if signame in [ 'handle' , 'signal' ] : if len ( args ) == 1 : self . dbgr . core . processor . section ( self . header ) for signame in self . siglist : self . print info signal entry ( signame ) return True else : signame = args [ 1 ] pass pass signame = self . is name or number ( signame ) self . dbgr . core . processor . section ( self . header ) self . print info signal entry ( signame ) return True", "predictions": ["print write write write write arguments"], "references": ["print information about a signal"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 2221, "code": "def handle print ( self , signame , set print ) : if set print : self . sigs [ signame ] . print method = self . dbgr . intf [ - 1 ] . msg else : self . sigs [ signame ] . print method = None pass return set print", "predictions": ["handles a = true if the verbose was set"], "references": ["set whether we print or not when this signal is caught ."], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 2222, "code": "def handle ( self , signum , frame ) : if self . print method : self . print method ( '\\n Program received signal %s.' % self . signame ) if self . print stack : import traceback strings = traceback . format stack ( frame ) for s in strings : if s [ - 1 ] == '\\n' : s = s [ 0 : - 1 ] self . print method ( s ) pass pass if self . b stop : core = self . dbgr . core old trace hook suspend = core . trace hook suspend core . trace hook suspend = True core . stop reason = ( 'intercepting signal %s (%d)' % ( self . signame , signum ) ) core . processor . event processor ( frame , 'signal' , signum ) core . trace hook suspend = old trace hook suspend pass if self . pass along : if self . old handler : self . old handler ( signum , frame ) pass pass return", "predictions": ["handles the signal events"], "references": ["this method is called when a signal is received ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 2223, "code": "def file2module ( filename ) : basename = osp . basename ( filename ) if '.' in basename : pos = basename . rfind ( '.' ) return basename [ : pos ] else : return basename return None", "predictions": ["convert a file name to a file name"], "references": ["given a file name extract the most likely module name ."], "bleu": 0.21690743377623947, "rouge_l": 0.4093959731543625}
{"id": 2224, "code": "def print obj ( arg , frame , format = None , short = False ) : try : if not frame : obj = eval ( arg , None , None ) else : obj = eval ( arg , frame . f globals , frame . f locals ) pass except : return 'No symbol \"' + arg + '\" in current context.' what = arg if format : what = format + ' ' + arg obj = printf ( obj , format ) s = '%s = %s' % ( what , obj ) if not short : s += '\\ntype = %s' % type ( obj ) if callable ( obj ) : argspec = print argspec ( obj , arg ) if argspec : s += ':\\n\\t' if inspect . isclass ( obj ) : s += 'Class constructor information:\\n\\t' obj = obj . init elif isinstance ( obj , types . Instance Type ) : obj = obj . call pass s += argspec pass s = print dict ( s , obj , \"object variables\" ) if hasattr ( obj , \" class \" ) : s = print dict ( s , obj . class , \"class variables\" ) pass return s", "predictions": ["count the given object in a pretty - printed string"], "references": ["return a string representation of an object"], "bleu": 0.14991106946711685, "rouge_l": 0.24302788844621517}
{"id": 2225, "code": "def pyfiles ( callername , level = 2 ) : d = os . path . dirname ( callername ) glob ( os . path . join ( d , '[a-z A-Z]*.py' ) ) py files = glob ( os . path . join ( d , '[a-z A-Z]*.py' ) ) return [ os . path . basename ( filename [ 0 : - 3 ] ) for filename in py files ]", "predictions": ["len the list of all n - grams n - grams - grams files ."], "references": ["all python files caller s dir without the path and trailing . py"], "bleu": 0.10343603005129705, "rouge_l": 0.21708185053380782}
{"id": 2226, "code": "def populate cmd lists ( self ) : self . commands = { } for cmd instance in self . cmd instances : cmd name = cmd instance . name self . commands [ cmd name ] = cmd instance pass return", "predictions": ["eval command line obj obj format"], "references": ["populate self . commands"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2227, "code": "def run ( self , args ) : mainfile = self . core . filename ( None ) if self . core . is running ( ) : if mainfile : part1 = \"Python program '%s' is stopped\" % mainfile else : part1 = 'Program is stopped' pass if self . proc . event : msg = 'via a %s event.' % self . proc . event else : msg = '.' self . msg ( Mmisc . wrapped lines ( part1 , msg , self . settings [ 'width' ] ) ) if self . proc . curframe : self . msg ( \"PC offset is %d.\" % self . proc . curframe . f lasti ) if self . proc . event == 'return' : val = self . proc . event arg part1 = 'Return value is' self . msg ( Mmisc . wrapped lines ( part1 , self . proc . saferepr ( val ) , self . settings [ 'width' ] ) ) pass elif self . proc . event == 'exception' : exc type , exc value , exc tb = self . proc . event arg self . msg ( 'Exception type: %s' % self . proc . saferepr ( exc type ) ) if exc value : self . msg ( 'Exception value: %s' % self . proc . saferepr ( exc value ) ) pass pass self . msg ( 'It stopped %s.' % self . core . stop reason ) if self . proc . event in [ 'signal' , 'exception' , 'c exception' ] : self . msg ( 'Note: we are stopped *after* running the ' 'line shown.' ) pass else : if mainfile : part1 = \"Python program '%s'\" % mainfile msg = \"is not currently running. \" self . msg ( Mmisc . wrapped lines ( part1 , msg , self . settings [ 'width' ] ) ) else : self . msg ( 'No Python program is currently running.' ) pass self . msg ( self . core . execution status ) pass return False", "predictions": ["print the . core ."], "references": ["execution status of the program ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 2228, "code": "def columnize commands ( self , commands ) : commands . sort ( ) width = self . debugger . settings [ 'width' ] return columnize . columnize ( commands , displaywidth = width , lineprefix = '    ' )", "predictions": ["lookup lookup commands commands"], "references": ["list commands arranged in an aligned columns"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 2229, "code": "def close ( self ) : self . state = 'closing' if self . inout : self . inout . close ( ) pass self . state = 'closing connection' if self . conn : self . conn . close ( ) self . state = 'disconnected' return", "predictions": ["short short cb subcmd and closes the connection subcmd subcmd subcmd subcmd subcmd"], "references": ["closes both socket and server connection ."], "bleu": 0.1135935489027116, "rouge_l": 0.21143847487001732}
{"id": 2230, "code": "def complete identifier ( cmd , prefix ) : if not cmd . proc . curframe : return [ None ] ns = cmd . proc . curframe . f globals . copy ( ) ns . update ( cmd . proc . curframe . f locals ) if '.' in prefix : dotted = prefix . split ( '.' ) try : obj = ns [ dotted [ 0 ] ] for part in dotted [ 1 : - 1 ] : obj = getattr ( obj , part ) except ( Key Error , Attribute Error ) : return [ ] pre prefix = '.' . join ( dotted [ : - 1 ] ) + '.' return [ pre prefix + n for n in dir ( obj ) if n . startswith ( dotted [ - 1 ] ) ] else : return Mcomplete . complete token ( ns . keys ( ) , prefix )", "predictions": ["completion for a identifier identifier"], "references": ["complete an arbitrary expression ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2231, "code": "def nothread quit ( self , arg ) : self . debugger . core . stop ( ) self . debugger . core . execution status = 'Quit command' self . proc . response [ 'event' ] = 'terminated' self . proc . response [ 'name' ] = 'status' self . proc . intf [ - 1 ] . msg ( self . proc . response ) raise Mexcept . Debugger Quit", "predictions": ["= quit quit ."], "references": ["quit command when there s just one thread ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 2232, "code": "def is started ( self ) : return ( tracer . is started ( ) and not self . trace hook suspend and tracer . find hook ( self . trace dispatch ) )", "predictions": [". returns whether the cats is categories . ."], "references": ["return true if debugging is in progress ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 2233, "code": "def set next ( self , frame , step ignore = 0 , step events = None ) : self . step events = None self . stop level = Mstack . count frames ( frame ) self . last level = self . stop level self . last frame = frame self . stop on finish = False self . step ignore = step ignore return", "predictions": ["show the category of the args and keys list list of frames list list"], "references": ["sets to stop on the next event that happens in frame frame ."], "bleu": 0.08839374326825923, "rouge_l": 0.0745721271393643}
{"id": 2234, "code": "def stack trace ( self , f ) : while f : if ( not self . core . ignore filter . is included ( f ) or self . settings [ 'dbg trepan' ] ) : s = Mstack . format stack entry ( self , ( f , f . f lineno ) ) self . msg ( \" \" * 4 + s ) pass f = f . f back pass return", "predictions": ["trace trace trace ."], "references": ["a mini stack trace routine for threads ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 2235, "code": "def checkfuncname ( b , frame ) : if not b . funcname : if b . line != frame . f lineno : return False return True if frame . f code . co name != b . funcname : return False if not b . func first executable line : b . func first executable line = frame . f lineno if b . func first executable line != frame . f lineno : return False return True", "predictions": ["check if a frame is a frame frame"], "references": ["check whether we should break here because of b . funcname ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 2236, "code": "def delete breakpoint by number ( self , bpnum ) : success , msg , bp = self . get breakpoint ( bpnum ) if not success : return False , msg self . delete breakpoint ( bp ) return ( True , '' )", "predictions": ["open a breakpoint object self input"], "references": ["remove a breakpoint given its breakpoint number ."], "bleu": 0.20830666398386113, "rouge_l": 0.2785388127853881}
{"id": 2237, "code": "def en disable all breakpoints ( self , do enable = True ) : bp list = [ bp for bp in self . bpbynumber if bp ] bp nums = [ ] if do enable : endis = 'en' else : endis = 'dis' pass if not bp list : return \"No breakpoints to %sable\" % endis for bp in bp list : bp . enabled = do enable bp nums . append ( str ( bp . number ) ) pass return ( \"Breakpoints %sabled: %s\" % ( endis , \", \" . join ( bp nums ) ) )", "predictions": ["int all errmsg errmsg"], "references": ["enable or disable all breakpoints ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 2238, "code": "def en disable breakpoint by number ( self , bpnum , do enable = True ) : success , msg , bp = self . get breakpoint ( bpnum ) if not success : return success , msg if do enable : endis = 'en' else : endis = 'dis' pass if bp . enabled == do enable : return ( False , ( 'Breakpoint (%r) previously %sabled' % ( str ( bpnum ) , endis , ) ) ) bp . enabled = do enable return ( True , '' )", "predictions": ["show a int by by"], "references": ["enable or disable a breakpoint given its breakpoint number ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 2239, "code": "def open ( self , inp , opts = None ) : if isinstance ( inp , io . Text IO Wrapper ) : self . input = inp elif isinstance ( inp , 'string' . class ) : self . name = inp self . input = open ( inp , 'r' ) else : raise IO Error ( \"Invalid input type (%s) for %s\" % ( inp . class . name , inp ) ) return", "predictions": ["run an is a file ."], "references": ["use this to set what file to read from ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 2240, "code": "def restore original login ( request ) : original session = request . session . get ( la settings . USER SESSION FLAG ) logout ( request ) if not original session : return try : original user pk = signer . unsign ( original session , max age = timedelta ( days = la settings . USER SESSION DAYS TIMESTAMP ) . total seconds ( ) ) user = get user model ( ) . objects . get ( pk = original user pk ) messages . info ( request , la settings . MESSAGE LOGIN REVERT . format ( username = user . dict [ username field ] ) , extra tags = la settings . MESSAGE EXTRA TAGS , ) login as ( user , request , store original user = False ) if la settings . USER SESSION FLAG in request . session : del request . session [ la settings . USER SESSION FLAG ] except Signature Expired : pass", "predictions": ["restore original login login ."], "references": ["restore an original login session checking the signed session"], "bleu": 0.17348474258688365, "rouge_l": 0.40757238307349664}
{"id": 2241, "code": "def load module ( path ) : i = path . rfind ( \".\" ) module , attr = path [ : i ] , path [ i + 1 : ] try : mod = import module ( module ) except Import Error : raise Improperly Configured ( \"Error importing CAN LOGIN AS function: {}\" . format ( module ) ) except Value Error : raise Improperly Configured ( \"Error importing CAN LOGIN AS\" \" function. Is CAN LOGIN AS a\" \" string?\" ) try : can login as = getattr ( mod , attr ) except Attribute Error : raise Improperly Configured ( \"Module {0} does not define a {1} \" \"function.\" . format ( module , attr ) ) return can login as", "predictions": ["load a module from a string ."], "references": ["code to load create user module . copied off django - browserid ."], "bleu": 0.09374222649442905, "rouge_l": 0.2846034214618974}
{"id": 2242, "code": "def main ( argv ) : parser = argparse . Argument Parser ( description = DESCRIPTION , formatter class = argparse . Raw Description Help Formatter ) parser . add argument ( '-b' , '--base-url' , default = URL BASE , help = 'API root url, default: %s' % URL BASE , ) parser . add argument ( '-e' , '--expanded' , help = \"Include Luminoso's analysis of each document, such as terms and\" ' document vectors' , action = 'store true' , ) parser . add argument ( '-t' , '--token' , help = 'API authentication token' ) parser . add argument ( '-s' , '--save-token' , action = 'store true' , help = 'save --token for --base-url to ~/.luminoso/tokens.json' , ) parser . add argument ( 'project id' , help = 'The ID of the project in the Daylight API' ) parser . add argument ( 'output file' , nargs = '?' , default = None , help = 'The JSON lines (.jsons) file to write to' ) args = parser . parse args ( argv ) if args . save token : if not args . token : raise Value Error ( \"error: no token provided\" ) Luminoso Client . save token ( args . token , domain = urlparse ( args . base url ) . netloc ) client = Luminoso Client . connect ( url = args . base url , token = args . token ) proj client = client . client for path ( 'projects/{}' . format ( args . project id ) ) download docs ( proj client , args . output file , args . expanded )", "predictions": ["download the docs ."], "references": ["handle arguments for the lumi - download command ."], "bleu": 0.12241977696855179, "rouge_l": 0.28773584905660377}
{"id": 2243, "code": "def stream json lines ( file ) : if isinstance ( file , string type ) : file = open ( file , 'rb' ) for line in file : line = line . strip ( ) if line : if isinstance ( line , bytes ) : line = line . decode ( 'utf-8' ) yield json . loads ( line )", "predictions": ["stream json lines from a file ."], "references": ["load a json stream and return a generator yielding one object at a time ."], "bleu": 0.07448668213629092, "rouge_l": 0.2559440559440559}
{"id": 2244, "code": "def get default account ( self ) : newclient = self . class ( self . session , self . root url ) account info = newclient . get ( '/accounts/' ) if account info [ 'default account' ] is not None : return account info [ 'default account' ] valid accounts = [ a [ 'account id' ] for a in account info [ 'accounts' ] if a [ 'account id' ] != 'public' ] if len ( valid accounts ) == 0 : raise Value Error ( \"Can't determine your default URL. \" \"Please request a specific URL or ask \" \"Luminoso for support.\" ) return valid accounts [ 0 ]", "predictions": ["get the default account info from the account ."], "references": ["get the id of an account you can use to access projects ."], "bleu": 0.135323305042906, "rouge_l": 0.35209235209235207}
{"id": 2245, "code": "def documentation ( self ) : newclient = self . class ( self . session , self . root url ) return newclient . get raw ( '/' )", "predictions": ["return the documentation of the documentation ."], "references": ["get the documentation that the server sends for the api ."], "bleu": 0.15685718045401453, "rouge_l": 0.4273204903677758}
{"id": 2246, "code": "def print csv ( result ) : if type ( result ) is not list : raise Type Error ( \"output not able to be displayed as CSV.\" ) first line = result [ 0 ] w = csv . Dict Writer ( sys . stdout , fieldnames = sorted ( first line . keys ( ) ) ) w . writeheader ( ) for line in result : w . writerow ( line )", "predictions": ["print the result as json"], "references": ["print a json list of json objects in csv format ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 2247, "code": "def read params ( input file , json body , p params ) : params = { } try : if input file : params . update ( json . load ( input file ) ) if json body is not None : params . update ( json . loads ( json body ) ) except Value Error as e : raise Value Error ( \"input is not valid JSON: %s\" % e ) try : params . update ( { p . split ( '=' , 1 ) [ 0 ] : p . split ( '=' , 1 ) [ 1 ] for p in p params } ) except Index Error : raise Value Error ( \"--param arguments must have key=value format\" ) return params", "predictions": ["read params from json file ."], "references": ["read parameters from input file - j and - p arguments in that order ."], "bleu": 0.06197705798903779, "rouge_l": 0.3536231884057971}
{"id": 2248, "code": "def batches ( iterable , size ) : sourceiter = iter ( iterable ) while True : try : batchiter = islice ( sourceiter , size ) yield chain ( [ next ( batchiter ) ] , batchiter ) except Stop Iteration : return", "predictions": ["split an iterable into chunks of size ."], "references": ["take an iterator and yield its contents in groups of size items ."], "bleu": 0.1283572790104489, "rouge_l": 0.3652694610778443}
{"id": 2249, "code": "def simplify doc ( doc ) : doc = dict ( doc ) if 'text' not in doc : raise Value Error ( \"The document {!r} has no text field\" . format ( doc ) ) return { 'text' : doc [ 'text' ] , 'metadata' : doc . get ( 'metadata' , [ ] ) , 'title' : doc . get ( 'title' , '' ) }", "predictions": ["simplify a document ."], "references": ["limit a document to just the three fields we should upload ."], "bleu": 0.06876828939330318, "rouge_l": 0.34398496240601506}
{"id": 2250, "code": "def create project with docs ( client , docs , language , name , account = None , progress = False ) : description = 'Uploaded using lumi-upload at {}' . format ( time . asctime ( ) ) if account is not None : proj record = client . post ( 'projects' , name = name , language = language , description = description , account id = account , ) else : proj record = client . post ( 'projects' , name = name , language = language , description = description ) proj id = proj record [ 'project id' ] proj client = client . client for path ( 'projects/' + proj id ) try : if progress : progress bar = tqdm ( desc = 'Uploading documents' ) else : progress bar = None for batch in batches ( docs , BATCH SIZE ) : docs to upload = [ simplify doc ( doc ) for doc in batch ] proj client . post ( 'upload' , docs = docs to upload ) if progress : progress bar . update ( BATCH SIZE ) finally : if progress : progress bar . close ( ) print ( 'The server is building project {!r}.' . format ( proj id ) ) proj client . post ( 'build' ) while True : time . sleep ( 10 ) proj status = proj client . get ( ) build info = proj status [ 'last build info' ] if 'success' in build info : if not build info [ 'success' ] : raise Luminoso Server Error ( build info [ 'reason' ] ) return proj status", "predictions": ["create a project with the given docs ."], "references": ["given an iterator of documents upload them as a luminoso project ."], "bleu": 0.1223065774797558, "rouge_l": 0.28955696202531644}
{"id": 2251, "code": "def main ( argv ) : parser = argparse . Argument Parser ( description = DESCRIPTION , formatter class = argparse . Raw Description Help Formatter , ) parser . add argument ( '-b' , '--base-url' , default = URL BASE , help = 'API root url, default: %s' % URL BASE , ) parser . add argument ( '-a' , '--account-id' , default = None , help = 'Account ID that should own the project, if not the default' , ) parser . add argument ( '-l' , '--language' , default = 'en' , help = 'The language code for the language the text is in. Default: en' , ) parser . add argument ( '-t' , '--token' , help = \"API authentication token\" ) parser . add argument ( '-s' , '--save-token' , action = 'store true' , help = 'save --token for --base-url to ~/.luminoso/tokens.json' , ) parser . add argument ( 'input filename' , help = 'The JSON-lines (.jsons) file of documents to upload' , ) parser . add argument ( 'project name' , nargs = '?' , default = None , help = 'What the project should be called' , ) args = parser . parse args ( argv ) if args . save token : if not args . token : raise Value Error ( \"error: no token provided\" ) Luminoso Client . save token ( args . token , domain = urlparse ( args . base url ) . netloc ) client = Luminoso Client . connect ( url = args . base url , token = args . token ) name = args . project name if name is None : name = input ( 'Enter a name for the project: ' ) if not name : print ( 'Aborting because no name was provided.' ) return result = upload docs ( client , args . input filename , args . language , name , account = args . account id , progress = True , ) print ( 'Project {!r} created with {} documents' . format ( result [ 'project id' ] , result [ 'document count' ] ) )", "predictions": ["the main function of the script ."], "references": ["handle arguments for the lumi - upload command ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 2252, "code": "def batches ( iterable , size ) : sourceiter = iter ( iterable ) while True : batchiter = islice ( sourceiter , size ) yield chain ( [ next ( batchiter ) ] , batchiter )", "predictions": ["generate an iterable of batches of iterable ."], "references": ["take an iterator and yield its contents in groups of size items ."], "bleu": 0.10207878682119532, "rouge_l": 0.2739520958083832}
{"id": 2253, "code": "def post login page ( self ) : data = { 'ID Token1' : self . username , 'ID Token2' : self . password , 'Sun Query Params String' : base64 . b64encode ( b'realm=particuliers' ) , 'encoded' : 'true' , 'gx charset' : 'UTF-8' } try : self . session . post ( LOGIN URL , data = data , allow redirects = False , timeout = self . timeout ) except OS Error : raise Py Linky Error ( \"Can not submit login form\" ) if 'i Planet Directory Pro' not in self . session . cookies : raise Py Linky Error ( \"Login error: Please check your username/password.\" ) return True", "predictions": ["login to the login page ."], "references": ["login to enedis ."], "bleu": 0.31239399369202553, "rouge_l": 0.6224489795918368}
{"id": 2254, "code": "def fetch data ( self ) : for t in [ HOURLY , DAILY , MONTHLY , YEARLY ] : self . data [ t ] = self . get data per period ( t )", "predictions": ["fetch data from period"], "references": ["get the latest data from enedis ."], "bleu": 0.22336835181428535, "rouge_l": 0.346590909090909}
{"id": 2255, "code": "def prepare ( self ) : if self . class . view : return #: Load the View class from the dotted view name with enaml . imports ( ) : View = pydoc . locate ( self . page . view ) assert View , \"Failed to import View: {}\" . format ( self . page . view ) #: Set initial view properties self . class . view = View ( site = self . site , page = self . page , request = self . request , )", "predictions": ["prepares the dotted view to show the dotted view ."], "references": ["load the view on first load"], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 2256, "code": "def initialize ( self ) : if self . class . view : self . view . handler = self self . view . request = self . request return #: Load the View class from the dotted view name with enaml . imports ( ) : from views . index import View #: Set initial view properties self . class . view = View ( company = current company , request = self . request , handler = self , )", "predictions": ["initializes the view properties ."], "references": ["load the view on first load could also load based on session group etc .."], "bleu": 0.04862652376060361, "rouge_l": 0.18345864661654135}
{"id": 2257, "code": "def get ( self , * args , * * kwargs ) : #: Render view for get request, view is cached for websocket if self . is websocket ( ) : return super ( Demo Handler , self ) . get ( * args , * * kwargs ) else : #return tornado.web.Request Handler.get(self, *args, **kwargs) self . write ( self . view . render ( ) )", "predictions": ["overrides websocket to check if the view is not cached ."], "references": ["execute the correct handler depending on what is connecting ."], "bleu": 0.1354599427337814, "rouge_l": 0.28818897637795277}
{"id": 2258, "code": "def on message ( self , message ) : #: Decode message change = tornado . escape . json decode ( message ) #print change #: Get the owner ID ref = change . get ( 'ref' ) if not ref : return #: Get the server side representation of the node #: If found will return the View declaration node node = self . view . xpath ( '//*[@ref=\"{}\"]' . format ( ref ) , first = True ) if node is None : return #: Handle the event if change . get ( 'type' ) and change . get ( 'name' ) : if change [ 'type' ] == 'event' : #: Trigger the event trigger = getattr ( node , change [ 'name' ] ) trigger ( ) if change [ 'type' ] == 'update' : #: Trigger the update setattr ( node , change [ 'name' ] , change [ 'value' ] )", "predictions": ["process the message from the server"], "references": ["when enaml . js sends a message"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 2259, "code": "def update menus ( self , change ) : menus = { } #: Get all links links = [ p . link for p in self . pages if p . link ] + self . links #: Put all links in the correct menu for link in links : for menu in link . menus : if menu not in menus : menus [ menu ] = [ ] menus [ menu ] . append ( link ) #: Update the menus for name , menu in menus . items ( ) : k = '{} menu' . format ( name ) if hasattr ( self , k ) : setattr ( self , k , menu )", "predictions": ["update the menus with the menus"], "references": ["when pages change update the menus"], "bleu": 0.4111336169005197, "rouge_l": 0.5}
{"id": 2260, "code": "def default handlers ( self ) : static path = os . path . abspath ( os . path . join ( os . path . dirname ( file ) , \"static\" ) ) urls = [ ( r\"/static/(.*)\" , cyclone . web . Static File Handler , { \"path\" : static path } ) , ] for p in self . pages : handler = p . handler handler . site = self handler . page = p urls . append ( ( p . link . url , handler ) ) return urls", "predictions": ["returns a list of all pages in the static repo ."], "references": ["generate the handlers for this site"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 2261, "code": "def set attribute ( self , name , value ) : if value is True : self . widget . set ( name , name ) elif value is False : del self . widget . attrib [ name ] else : self . widget . set ( name , str ( value ) )", "predictions": ["set an attribute of the widget"], "references": ["default handler for those not explicitly defined"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 2262, "code": "def xpath ( self , query , * * kwargs ) : nodes = self . proxy . find ( query , * * kwargs ) return [ n . declaration for n in nodes ]", "predictions": ["pass in a domain name ."], "references": ["find nodes matching the given xpath query"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 2263, "code": "def init widget ( self ) : d = self . declaration if d . source : self . set source ( d . source ) else : super ( Raw Component , self ) . init widget ( )", "predictions": ["initialize the underlying widget ."], "references": ["initialize the widget with the source ."], "bleu": 0.2736570128577077, "rouge_l": 0.6472148541114059}
{"id": 2264, "code": "def observe mode ( self , change ) : block = self . block if block and self . is initialized and change [ 'type' ] == 'update' : if change [ 'oldvalue' ] == 'replace' : raise Not Implemented Error for c in self . children : block . children . remove ( c ) c . set parent ( None ) self . refresh items ( )", "predictions": ["observe the mode ."], "references": ["if the mode changes . refresh the items ."], "bleu": 0.14558246978804804, "rouge_l": 0.43160377358490565}
{"id": 2265, "code": "def read ( * pathcomponents ) : with open ( join ( abspath ( dirname ( file ) ) , * pathcomponents ) ) as thefile : return thefile . read ( )", "predictions": ["read the contents of a file ."], "references": ["read the contents of a file located relative to setup . py"], "bleu": 0.4256279541919937, "rouge_l": 0.7034596375617792}
{"id": 2266, "code": "def error ( msg , exit code ) : sys . stderr . write ( \"%s\\ntry 'mongotail --help' for more information\\n\" % msg ) sys . stderr . flush ( ) exit ( exit code )", "predictions": ["print an error message to stdout ."], "references": ["print msg error and exit with status exit_code"], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 2267, "code": "def error parsing ( msg = \"unknown options\" ) : sys . stderr . write ( \"Error parsing command line: %s\\ntry 'mongotail --help' for more information\\n\" % msg ) sys . stderr . flush ( ) exit ( EINVAL )", "predictions": ["print error message parsing ."], "references": ["print any parsing error and exit with status - 1"], "bleu": 0.11943865131127647, "rouge_l": 0.2515463917525773}
{"id": 2268, "code": "def draw ( self ) : self . screen . border ( 0 ) if self . title is not None : self . screen . addstr ( 2 , 2 , self . title , curses . A STANDOUT ) if self . subtitle is not None : self . screen . addstr ( 4 , 2 , self . subtitle , curses . A BOLD ) for index , item in enumerate ( self . items ) : if self . current option == index : text style = self . highlight else : text style = self . normal self . screen . addstr ( 5 + index , 4 , item . show ( index ) , text style ) screen rows , screen cols = Curses Menu . stdscr . getmaxyx ( ) top row = 0 if 6 + len ( self . items ) > screen rows : if screen rows + self . current option < 6 + len ( self . items ) : top row = self . current option else : top row = 6 + len ( self . items ) - screen rows self . screen . refresh ( top row , 0 , 0 , 0 , screen rows - 1 , screen cols - 1 )", "predictions": ["draw the screen to the screen ."], "references": ["redraws the menu and refreshes the screen . should be called whenever something changes that needs to be redrawn ."], "bleu": 0.0597312233790797, "rouge_l": 0.2726256983240224}
{"id": 2269, "code": "def process user input ( self ) : user input = self . get input ( ) go to max = ord ( \"9\" ) if len ( self . items ) >= 9 else ord ( str ( len ( self . items ) ) ) if ord ( '1' ) <= user input <= go to max : self . go to ( user input - ord ( '0' ) - 1 ) elif user input == curses . KEY DOWN : self . go down ( ) elif user input == curses . KEY UP : self . go up ( ) elif user input == ord ( \"\\n\" ) : self . select ( ) return user input", "predictions": ["get user input from user input ."], "references": ["gets the next single character and decides what to do with it"], "bleu": 0.07646493705380929, "rouge_l": 0.0}
{"id": 2270, "code": "def select ( self ) : self . selected option = self . current option self . selected item . set up ( ) self . selected item . action ( ) self . selected item . clean up ( ) self . returned value = self . selected item . get return ( ) self . should exit = self . selected item . should exit if not self . should exit : self . draw ( )", "predictions": ["select the currently selected item ."], "references": ["select the current item and run it"], "bleu": 0.2644358066258934, "rouge_l": 0.45522388059701485}
{"id": 2271, "code": "def show ( self , index ) : if self . menu and self . menu . parent : self . text = \"Return to %s menu\" % self . menu . parent . title else : self . text = \"Exit\" return super ( Exit Item , self ) . show ( index )", "predictions": ["show the menu menu"], "references": ["this class overrides this method"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 2272, "code": "def action ( self ) : self . return value = self . function ( * self . args , * * self . kwargs )", "predictions": ["returns the restore of the current restore . . . ."], "references": ["this class overrides this method"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2273, "code": "def action ( self ) : commandline = \"{0} {1}\" . format ( self . command , \" \" . join ( self . arguments ) ) try : completed process = subprocess . run ( commandline , shell = True ) self . exit status = completed process . returncode except Attribute Error : self . exit status = subprocess . call ( commandline , shell = True )", "predictions": ["run the load command with the try to try to the try to the try to except for the try to except the try to try to except for the try"], "references": ["this class overrides this method"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2274, "code": "def set up ( self ) : self . menu . pause ( ) curses . def prog mode ( ) self . menu . clear screen ( )", "predictions": ["main method to main argparse argparse = true"], "references": ["this class overrides this method"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 2275, "code": "def clean up ( self ) : self . submenu . join ( ) self . menu . clear screen ( ) curses . reset prog mode ( ) curses . curs set ( 1 ) curses . curs set ( 0 ) self . menu . resume ( )", "predictions": ["string usable json and bytes"], "references": ["this class overrides this method"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2276, "code": "def add ( df , new column , column 1 , column 2 ) : return basic math operation ( df , new column , column 1 , column 2 , op = 'add' )", "predictions": ["get a self root root root root root root root ."], "references": ["deprecated - use formula instead"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2277, "code": "def subtract ( df , new column , column 1 , column 2 ) : return basic math operation ( df , new column , column 1 , column 2 , op = 'sub' )", "predictions": ["documentation for a new return the new ."], "references": ["deprecated - use formula instead"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2278, "code": "def multiply ( df , new column , column 1 , column 2 ) : return basic math operation ( df , new column , column 1 , column 2 , op = 'mul' )", "predictions": ["print the operation operation ."], "references": ["deprecated - use formula instead"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2279, "code": "def divide ( df , new column , column 1 , column 2 ) : return basic math operation ( df , new column , column 1 , column 2 , op = 'truediv' )", "predictions": ["read a dataframe with the given column and column ."], "references": ["deprecated - use formula instead"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2280, "code": "def cumsum ( df , new column : str , column : str , index : list , date column : str , date format : str ) : logging . get Logger ( name ) . warning ( f\"DEPRECATED: use compute cumsum\" ) date temp = ' date temp ' if isinstance ( index , str ) : index = [ index ] levels = list ( range ( 0 , len ( index ) ) ) df [ date temp ] = pd . to datetime ( df [ date column ] , format = date format ) reference cols = [ date temp , date column ] df = df . groupby ( index + reference cols ) . sum ( ) df [ new column ] = df . groupby ( level = levels ) [ column ] . cumsum ( ) df . reset index ( inplace = True ) del df [ date temp ] return df", "predictions": ["compute the dataframe as a dataframe"], "references": ["deprecated - please use compute_cumsum instead"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2281, "code": "def log message ( logger , message = \"\" ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : log message ( logger , func . name , message ) result = func ( * args , * * kwargs ) return result return wrapper return decorator", "predictions": ["simplify a doc doc has been set has been set has a doc doc has"], "references": ["decorator to log a message before executing a function"], "bleu": 0.09103526405546068, "rouge_l": 0.17453505007153075}
{"id": 2282, "code": "def log time ( logger ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : start = time . time ( ) result = func ( * args , * * kwargs ) end = time . time ( ) log time ( logger , func . name , start , end ) return result return wrapper return decorator", "predictions": ["decorator to create project project project project project project project project project project project project project project project project project project project project project project project project project project project project"], "references": ["decorator to log the execution time of a function"], "bleu": 0.0513487742994337, "rouge_l": 0.11101000909918107}
{"id": 2283, "code": "def clean cachedir old entries ( cachedir : Store Backend Base , func name : str , limit : int ) -> int : if limit < 1 : raise Value Error ( \"'limit' must be greater or equal to 1\" ) cache entries = get cachedir entries ( cachedir , func name ) cache entries = sorted ( cache entries , key = lambda e : e . last access , reverse = True ) cache entries to remove = cache entries [ limit : ] for entry in cache entries to remove : shutil . rmtree ( entry . path , ignore errors = True ) return len ( cache entries to remove )", "predictions": ["removes all entries entries entries from the cache . . ."], "references": ["remove old entries from the cache"], "bleu": 0.317023313852343, "rouge_l": 0.4969450101832994}
{"id": 2284, "code": "def ada family core ( params , gparams , learning rate = 0.01 , eps = 1e-6 , rho = 0.95 , method = \"ADADELTA\" , beta = 0.0 , gsum regularization = 0.0001 ) : , , , args = inspect . getargvalues ( inspect . currentframe ( ) ) logging . info ( \"ada family core: %s\" % str ( args . items ( ) ) ) free parameters = [ ] if method == \"FINETUNING ADAGRAD\" : method = \"ADAGRAD\" gsum regularization = 0 one Minus Beta = 1 - beta gsums = [ theano . shared ( np . zeros like ( param . get value ( borrow = True ) , dtype = FLOATX ) , name = \"gsum %s\" % param . name ) if ( method == 'ADADELTA' or method == 'ADAGRAD' ) else None for param in params ] xsums = [ theano . shared ( np . zeros like ( param . get value ( borrow = True ) , dtype = FLOATX ) , name = \"xsum %s\" % param . name ) if method == 'ADADELTA' else None for param in params ] if method == 'ADAGRAD' : for gsum in gsums : gsum . set value ( gsum . get value ( ) ** 0 ) updates = Ordered Dict ( ) for gparam , param , gsum , xsum in zip ( gparams , params , gsums , xsums ) : if method == 'ADADELTA' : updates [ gsum ] = rho * gsum + ( 1. - rho ) * ( gparam ** 2 ) dparam = - T . sqrt ( ( xsum + eps ) / ( updates [ gsum ] + eps ) ) * gparam updates [ xsum ] = rho * xsum + ( 1. - rho ) * ( dparam ** 2 ) updates [ param ] = param * one Minus Beta + dparam elif method == 'ADAGRAD' : updates [ gsum ] = gsum + ( gparam ** 2 ) - gsum regularization * gsum updates [ param ] = param * one Minus Beta - learning rate * ( gparam / ( T . sqrt ( updates [ gsum ] + eps ) ) ) else : updates [ param ] = param * one Minus Beta - gparam * learning rate if method == 'ADADELTA' : free parameters . extend ( gsums + xsums ) elif method == 'ADAGRAD' : free parameters . extend ( gsums ) for k in updates : if updates [ k ] . dtype != FLOATX : updates [ k ] = updates [ k ] . astype ( FLOATX ) return updates . items ( ) , free parameters", "predictions": ["find the batches of the batches family family . . . . . . . . . . . . . . . . . . . . ."], "references": ["optimize by sgd adagrad or adadelta ."], "bleu": 0.04175872565419194, "rouge_l": 0.06243602865916069}
{"id": 2285, "code": "def learning updates ( self ) : params = self . training params ( ) gradients = self . get gradients ( params ) return self . optimization updates ( params , gradients )", "predictions": ["post - login login login"], "references": ["return updates in the training ."], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 2286, "code": "def training params ( self ) : params = self . network . parameters if self . config . fixed parameters : logging . info ( \"fixed parameters: %s\" % \", \" . join ( map ( str , self . config . fixed parameters ) ) ) params = [ p for p in params if p not in self . config . fixed parameters ] return params", "predictions": ["return parameters from the fetch"], "references": ["get parameters to be optimized ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 2287, "code": "def optimization updates ( self , params , gradients ) : updates , free parameters = optimize updates ( params , gradients , self . config ) self . network . free parameters . extend ( free parameters ) logging . info ( \"Added %d free parameters for optimization\" % len ( free parameters ) ) return updates", "predictions": ["prepare the prepare parameters parameters for the given ."], "references": ["return updates from optimization ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 2288, "code": "def first glimpse sensor ( self , x t ) : downsampled img = theano . tensor . signal . downsample . max pool 2d ( x t , ( 4 , 4 ) ) downsampled img = downsampled img . flatten ( ) first l = T . dot ( downsampled img , self . W f ) if self . disable reinforce : wf grad = self . W f if self . random glimpse : first l = self . srng . uniform ( ( 2 , ) , low = - 1.7 , high = 1.7 ) else : sampled l t = self . sample gaussian ( first l , self . cov ) sampled pdf = self . multi gaussian pdf ( disconnected grad ( sampled l t ) , first l ) wf grad = T . grad ( T . log ( sampled pdf ) , self . W f ) first l = sampled l t return first l , wf grad", "predictions": ["initialize a self - self self ."], "references": ["compute first glimpse position using down - sampled image ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 2289, "code": "def prepare ( self ) : self . output dim = 10 self . encoder = Chain ( self . input dim ) . stack ( Dense ( self . internal layer size , 'tanh' ) ) self . decoder = Chain ( self . internal layer size ) . stack ( Dense ( self . input dim ) ) self . classifier = Chain ( self . internal layer size ) . stack ( Dense ( 50 , 'tanh' ) , Dense ( self . output dim ) , Softmax ( ) ) self . register inner layers ( self . encoder , self . decoder , self . classifier ) self . target input = T . ivector ( 'target' ) self . register external inputs ( self . target input )", "predictions": ["get rid of inner and target args args args args args args args args args args ."], "references": ["all codes that create parameters should be put into setup function ."], "bleu": 0.07223943354597204, "rouge_l": 0.07117852975495916}
{"id": 2290, "code": "def compute tensor ( self , x ) : internal variable = self . encoder . compute tensor ( x ) decoding output = self . decoder . compute tensor ( internal variable ) classification output = self . classifier . compute tensor ( internal variable ) auto encoder cost = Auto Encoder Cost ( decoding output , x ) . get ( ) classification cost = Cross Entropy Cost ( classification output , self . target input ) . get ( ) final cost = 0.01 * auto encoder cost + classification cost error rate = Error Rate Cost ( classification output , self . target input ) . get ( ) self . register monitors ( ( \"err\" , error rate ) , ( \"encoder cost\" , auto encoder cost ) , ( \"classify cost\" , classification cost ) ) return final cost", "predictions": ["on the side of the decode message tornado tornado"], "references": ["build the computation graph here ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 2291, "code": "def vectorize target ( self , size ) : if self . train set : self . train set = self . vectorize set ( self . train set , size ) if self . valid set : self . valid set = self . vectorize set ( self . valid set , size ) if self . test set : self . test set = self . vectorize set ( self . test set , size )", "predictions": ["update or create a menus with a change in the training"], "references": ["make targets be one - hot vectors ."], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2292, "code": "def report ( self ) : logging . info ( \"%s train=%d valid=%d test=%d\" % ( self . class . name , len ( list ( self . train set ) ) if self . train set else 0 , len ( list ( self . valid set ) ) if self . valid set else 0 , len ( list ( self . test set ) ) if self . test set else 0 ) )", "predictions": ["show the default default default default default default default default default default default default default default default default default default default default default default default default default default message"], "references": ["print dataset statistics ."], "bleu": 0.03511476270817333, "rouge_l": 0.0}
{"id": 2293, "code": "def train ( self , train set , valid set = None , test set = None , train size = None ) : iteration = 0 while True : if not iteration % self . config . test frequency and test set : try : self . test ( iteration , test set ) except Keyboard Interrupt : logging . info ( 'interrupted!' ) break if not iteration % self . validation frequency and valid set : try : if not self . evaluate ( iteration , valid set ) : logging . info ( 'patience elapsed, bailing out' ) break except Keyboard Interrupt : logging . info ( 'interrupted!' ) break train message = \"\" try : train message = self . train func ( train set ) except Keyboard Interrupt : logging . info ( 'interrupted!' ) break if not iteration % self . config . monitor frequency : logging . info ( 'monitor (iter=%i) %s' , iteration + 1 , train message ) iteration += 1 if hasattr ( self . network , \"iteration callback\" ) : self . network . iteration callback ( ) yield train message if valid set : self . set params ( self . best params ) if test set : self . test ( 0 , test set )", "predictions": ["set up the training training data ."], "references": ["we train over mini - batches and evaluate periodically ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 2294, "code": "def sample ( self , input , steps ) : inputs = [ [ onehot ( self . input dim , x ) for x in input ] ] for in range ( steps ) : target = self . compute ( inputs ) [ 0 , - 1 ] . argmax ( ) input . append ( target ) inputs [ 0 ] . append ( onehot ( self . input dim , target ) ) return input", "predictions": ["xpath - xpath xpath - match * * * * * * * * * * * * * * * * * * * * * query * * *"], "references": ["sample outputs from lm ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2295, "code": "def compute alignments ( self , prev state , precomputed values , mask = None ) : Wa Sp = T . dot ( prev state , self . Wa ) Ua H = precomputed values if Ua H . ndim == 2 : preact = Wa Sp [ : , None , : ] + Ua H [ None , : , : ] else : preact = Wa Sp [ : , None , : ] + Ua H act = T . activate ( preact , 'tanh' ) align scores = T . dot ( act , self . Va ) if mask : mask = ( 1 - mask ) * - 99.00 if align scores . ndim == 3 : align scores += mask [ None , : ] else : align scores += mask align weights = T . nnet . softmax ( align scores ) return align weights", "predictions": ["init the widget widget . based on the d of the d"], "references": ["compute the alignment weights based on the previous state ."], "bleu": 0.21401603033752975, "rouge_l": 0.3696969696969697}
{"id": 2296, "code": "def compute context vector ( self , prev state , inputs , precomputed values = None , mask = None ) : precomputed values = precomputed values if precomputed values else self . precompute ( inputs ) align weights = self . compute alignments ( prev state , precomputed values , mask ) context vector = T . sum ( align weights [ : , : , None ] * inputs , axis = 1 ) return context vector", "predictions": ["observe the mode of the mode == 1 == 1 == 1 == 1 == 1 == 1"], "references": ["compute the context vector with soft attention ."], "bleu": 0.06809398432036522, "rouge_l": 0.08265582655826557}
{"id": 2297, "code": "def concatenate ( vars , axis = - 1 ) : from deepy . core . neural var import Neural Variable if isinstance ( vars [ 0 ] , Neural Variable ) : concat var = Concatenate ( axis = axis ) . compute ( * vars ) if axis == - 1 or axis == vars [ 0 ] . tensor . ndim - 1 : concat var . output dim = sum ( [ x . output dim for x in vars ] , 0 ) else : concat var = TT . concatenate ( vars , axis ) return concat var", "predictions": ["read the values of the file using the file axis"], "references": ["a utility function of concatenate ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 2298, "code": "def pad ( self , side , length ) : if self . train set : self . train set = pad dataset ( self . train set , side , length ) if self . valid set : self . valid set = pad dataset ( self . valid set , side , length ) if self . test set : self . test set = pad dataset ( self . test set , side , length )", "predictions": ["error the training and error on the training dataset"], "references": ["pad sequences to given length in the left or right side ."], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 2299, "code": "def rmsprop core ( params , gradients , momentum = 0.9 , learning rate = 0.01 ) : for param , grad in zip ( params , gradients ) : rms = theano . shared ( np . zeros like ( param . get value ( ) ) , name = param . name + ' rms' ) rms = momentum * rms + ( 1 - momentum ) * grad * grad yield rms , rms yield param , param - learning rate * grad / T . sqrt ( rms + 1e-8 )", "predictions": ["generate the error of the error . . ."], "references": ["rmsprop optimization core ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 2300, "code": "def report ( self ) : if not self . end time : self . end ( ) print ( \"Time: {} mins\" . format ( ( self . end time - self . start time ) / 60 ) )", "predictions": ["draw the if the screen is running"], "references": ["report elapsed time ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2301, "code": "def run ( self , data x ) : output vars = self . compute ( * data x ) return self . extract costs ( output vars )", "predictions": ["process the self to len it to the given self to the given self to the given self to the given self to the self to the given self to the"], "references": ["run the model with validation data and return costs ."], "bleu": 0.03901663112717908, "rouge_l": 0.05374449339207048}
{"id": 2302, "code": "def invoke ( self ) : self . counter += 1 if self . counter % self . freq == 0 : cnt = 0. sum map = defaultdict ( float ) for x in self . trainer . get data ( self . data split ) : val map = self . run ( x ) if not isinstance ( val map , dict ) : raise Exception ( \"Monitor.run must return a dict.\" ) for k , val in val map . items ( ) : sum map [ k ] += val cnt += 1 for k in sum map : sum map [ k ] /= cnt new best = self . compare ( sum map ) self . trainer . report ( sum map , self . data split , new best = new best ) if new best : self . trainer . save checkpoint ( self . save path )", "predictions": ["select the best from the data ."], "references": ["this function will be called after each iteration ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 2303, "code": "def build loop vars ( self ) : from theano . tensor . var import Tensor Variable from deepy . core . neural var import Neural Variable if not self . loop vars : self . ordered out keys = self . outputs . keys ( ) seq keys = self . sequences . keys ( ) filled out keys = [ k for k in self . ordered out keys if self . outputs [ k ] ] nonseq keys = self . non sequences . keys ( ) dummy tensors , self . scan local vars = get dummy args ( sequences = [ self . sequences [ k ] . tensor for k in seq keys ] , outputs info = [ self . outputs [ k ] . tensor for k in self . ordered out keys ] , non sequences = [ self . non sequences [ k ] . tensor for k in nonseq keys ] , * * self . kwargs ) dummy map = dict ( zip ( seq keys + filled out keys + nonseq keys , dummy tensors ) ) arg map = self . sequences . copy ( ) arg map . update ( self . outputs ) arg map . update ( self . non sequences ) self . loop vars = Loop Vars ( ) for k , dummy tensor in dummy map . items ( ) : dummy var = Neural Variable ( dummy tensor , dim = arg map [ k ] . dim ( ) ) self . loop vars [ k ] = dummy var", "predictions": ["show the loop loop sequences"], "references": ["create inner loop variables ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 2304, "code": "def scan step ( self , vars ) : from neural var import Neural Variable if not self . loop vars : raise Exception ( \"The loop is not initialized. To initialize the loop, use `with loop as vars`\" ) replace map = { } for k , var in vars . items ( ) : if var is not None : replace map [ self . dummy nodes [ k ] . tensor ] = var . tensor outputs = { } for k in self . outputs : if k not in self . loop vars : raise Exception ( \"{} can not be found in loop vars.\" . format ( k ) ) output node = theano . clone ( self . loop vars [ k ] . tensor , replace map ) outputs [ k ] = Neural Variable ( output node , self . loop vars [ k ] . dim ( ) ) return outputs", "predictions": ["scan the theano step of the theano in the theano loop ."], "references": ["internal scan with dummy input variables ."], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 2305, "code": "def momentum core ( params , gradients , momentum = 0.9 , learning rate = 0.01 ) : free parameters = [ ] updates = [ ] for param , grad in zip ( params , gradients ) : delta = learning rate * grad velocity = theano . shared ( np . zeros like ( param . get value ( ) ) , name = param . name + ' vel' ) updates . append ( ( velocity , momentum * velocity - delta ) ) updates . append ( ( param , param + velocity ) ) free parameters . append ( velocity ) return updates , free parameters", "predictions": ["returns the core core core function for the model"], "references": ["momentum sgd optimization core ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 2306, "code": "def iftrain ( self , then branch , else branch ) : return ifelse ( self . training flag , then branch , else branch , name = \"iftrain\" )", "predictions": ["returns the name of the training branch"], "references": ["execute then_branch when training ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 2307, "code": "def skip ( self , n batches , n epochs = 0 ) : logging . info ( \"skip %d epochs and %d batches\" % ( n epochs , n batches ) ) self . skip batches = n batches self . skip epochs = n epochs", "predictions": ["skip batches of batches ."], "references": ["skip n batches in the training ."], "bleu": 0.21763141204756337, "rouge_l": 0.48541114058355433}
{"id": 2308, "code": "def train ( self , train set , valid set = None , test set = None , train size = None ) : self . epoch = 0 while True : if self . skip epochs > 0 : logging . info ( \"skipping one epoch ...\" ) self . skip epochs -= 1 self . epoch += 1 yield None continue if not self . epoch % self . config . test frequency and test set : try : self . run test ( self . epoch , test set ) except Keyboard Interrupt : logging . info ( 'interrupted!' ) break if not self . epoch % self . validation frequency and valid set : try : if not self . run valid ( self . epoch , valid set ) : logging . info ( 'patience elapsed, bailing out' ) break except Keyboard Interrupt : logging . info ( 'interrupted!' ) break try : costs = self . run train ( self . epoch , train set , train size ) except Keyboard Interrupt : logging . info ( 'interrupted!' ) break if np . isnan ( costs [ 0 ] [ 1 ] ) : logging . info ( \"Na N detected in costs, rollback to last parameters\" ) self . set params ( * self . checkpoint ) else : self . epoch += 1 self . network . epoch callback ( ) yield dict ( costs ) if valid set and self . config . get ( \"save best parameters\" , True ) : self . set params ( * self . best params ) if test set : self . run test ( - 1 , test set )", "predictions": ["train the training and return a new epoch ."], "references": ["train the model and return costs ."], "bleu": 0.24446151121745052, "rouge_l": 0.639412997903564}
{"id": 2309, "code": "def run train ( self , epoch , train set , train size = None ) : self . network . train logger . record epoch ( epoch + 1 ) costs = self . train step ( train set , train size ) if not epoch % self . config . monitor frequency : self . report ( dict ( costs ) , \"train\" , epoch ) self . last run costs = costs return costs", "predictions": ["run the training training"], "references": ["run one training iteration ."], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 2310, "code": "def run valid ( self , epoch , valid set , dry run = False , save path = None ) : costs = self . valid step ( valid set ) , J = costs [ 0 ] new best = False if self . best cost - J > self . best cost * self . min improvement : self . best params = self . copy params ( ) new best = True if not dry run : self . best cost = J self . best epoch = epoch self . save checkpoint ( save path ) self . report ( dict ( costs ) , type = \"valid\" , epoch = 0 if dry run else epoch , new best = new best ) self . last run costs = costs return epoch - self . best epoch < self . patience", "predictions": ["run a valid epoch"], "references": ["run one valid iteration return true if to continue training ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 2311, "code": "def report ( self , score map , type = \"valid\" , epoch = - 1 , new best = False ) : type str = type if len ( type str ) < 5 : type str += \" \" * ( 5 - len ( type str ) ) info = \" \" . join ( \"%s=%.2f\" % el for el in score map . items ( ) ) current epoch = epoch if epoch > 0 else self . current epoch ( ) epoch str = \"epoch={}\" . format ( current epoch + 1 ) if epoch < 0 : epoch str = \"dryrun\" sys . stdout . write ( \"\\r\" ) sys . stdout . flush ( ) marker = \" *\" if new best else \"\" message = \"{} ({}) {}{}\" . format ( type str , epoch str , info , marker ) self . network . train logger . record ( message ) logging . info ( message )", "predictions": ["print the best report of the given score ."], "references": ["report the scores and record them in the log ."], "bleu": 0.15881076016027915, "rouge_l": 0.31282051282051276}
{"id": 2312, "code": "def get data ( self , data split = \"train\" ) : if data split == 'train' : return self . current train set elif data split == 'valid' : return self . current valid set elif data split == 'test' : return self . current test set else : return None", "predictions": ["get data from the current value"], "references": ["get specified split of data ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 2313, "code": "def apply ( self , func , dim = None ) : output dim = dim if dim else self . output dim return Neural Variable ( func ( self . tensor ) , output dim )", "predictions": ["apply tensor to the output ."], "references": ["apply a function to tensors ."], "bleu": 0.2626909894424158, "rouge_l": 0.5}
{"id": 2314, "code": "def report ( self ) : if self . logger : self . logger . info ( \"accessed parameters:\" ) for key in self . used parameters : self . logger . info ( \" - %s %s\" % ( key , \"(undefined)\" if key in self . undefined parameters else \"\" ) )", "predictions": ["report the current report"], "references": ["report usage of training parameters ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 2315, "code": "def var ( self , tensor type , last dim = 0 , test shape = None ) : from deepy . tensor import var return var ( tensor type , last dim = last dim , test shape = test shape )", "predictions": ["returns a tensor object for a tensor"], "references": ["an alias of deepy . tensor . var ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 2316, "code": "def shared ( self , value , name = None ) : if type ( value ) == int : final value = np . array ( value , dtype = \"int32\" ) elif type ( value ) == float : final value = np . array ( value , dtype = env . FLOATX ) else : final value = value return theano . shared ( final value , name = name )", "predictions": ["populate the theano value in the theano ."], "references": ["create a shared theano scalar value ."], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 2317, "code": "def invoke ( self ) : self . iter += 1 if self . iter - max ( self . trainer . best iter , self . annealed iter ) >= self . patience : if self . annealed times >= self . anneal times : logging . info ( \"ending\" ) self . trainer . exit ( ) else : self . trainer . set params ( * self . trainer . best params ) self . learning rate . set value ( self . learning rate . get value ( ) * 0.5 ) self . annealed times += 1 self . annealed iter = self . iter logging . info ( \"annealed learning rate to %f\" % self . learning rate . get value ( ) )", "predictions": ["invoke the learning rate rate ."], "references": ["run it return whether to end training ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 2318, "code": "def invoke ( self ) : self . iter += 1 logging . info ( \"{} epochs left to run\" . format ( self . patience - self . iter ) ) if self . iter >= self . patience : self . trainer . exit ( )", "predictions": ["invoke the left left to its target ."], "references": ["run it return whether to end training ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 2319, "code": "def stack encoders ( self , * layers ) : self . stack ( * layers ) self . encoding layes . extend ( layers )", "predictions": ["extend the screen encoders with the current stack ."], "references": ["stack encoding layers this must be done before stacking decoding layers ."], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 2320, "code": "def stack decoders ( self , * layers ) : self . stack ( * layers ) self . decoding layers . extend ( layers )", "predictions": ["extend the stack decoders with the given layers ."], "references": ["stack decoding layers ."], "bleu": 0.19960198807747329, "rouge_l": 0.4959349593495934}
{"id": 2321, "code": "def encode ( self , x ) : if not self . encoding network : self . encoding network = Neural Network ( self . input dim , self . input tensor ) self . encoding network . input variables = self . input variables for layer in self . encoding layes : self . encoding network . stack layer ( layer , no setup = True ) return self . encoding network . compute ( * x )", "predictions": ["encode the input ."], "references": ["encode given input ."], "bleu": 0.5081327481546147, "rouge_l": 0.75}
{"id": 2322, "code": "def decode ( self , x ) : if not self . rep dim : raise Exception ( \"rep dim must be set to decode.\" ) if not self . decoding network : self . decoding network = Neural Network ( self . rep dim ) for layer in self . decoding layers : self . decoding network . stack layer ( layer , no setup = True ) return self . decoding network . compute ( x )", "predictions": ["decode the layer into a list of layers ."], "references": ["decode given representation ."], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 2323, "code": "def all parameters ( self ) : params = [ ] params . extend ( self . parameters ) params . extend ( self . free parameters ) return params", "predictions": ["return a list of all parameters ."], "references": ["return all parameters ."], "bleu": 0.3655552228545123, "rouge_l": 0.7648902821316614}
{"id": 2324, "code": "def setup variables ( self ) : if self . input tensor : if type ( self . input tensor ) == int : x = dim to var ( self . input tensor , name = \"x\" ) else : x = self . input tensor else : x = T . matrix ( 'x' ) self . input variables . append ( x ) self . output = x self . test output = x", "predictions": ["setup variables for the input tensor ."], "references": ["set up variables ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 2325, "code": "def compute ( self , * x ) : self . compile ( ) outs = self . compute ( * x ) if self . output keys : return Map Dict ( dict ( zip ( self . output keys , outs ) ) ) else : return outs", "predictions": ["compute the output of the output ."], "references": ["return network output ."], "bleu": 0.24446151121745047, "rouge_l": 0.3824451410658307}
{"id": 2326, "code": "def save params ( self , path , new thread = False ) : save logger . info ( path ) param variables = self . all parameters params = [ p . get value ( ) . copy ( ) for p in param variables ] if new thread : thread = Thread ( target = save network params , args = ( params , path ) ) thread . start ( ) else : save network params ( params , path ) self . train logger . save ( path )", "predictions": ["save all parameters in a thread ."], "references": ["save parameters to file ."], "bleu": 0.22089591134157885, "rouge_l": 0.5154929577464789}
{"id": 2327, "code": "def load params ( self , path , exclude free params = False ) : if not os . path . exists ( path ) : return logging . info ( \"loading parameters from %s\" % path ) if exclude free params : params to load = self . parameters else : params to load = self . all parameters if path . endswith ( \".gz\" ) : opener = gzip . open if path . lower ( ) . endswith ( '.gz' ) else open handle = opener ( path , 'rb' ) saved params = pickle . load ( handle ) handle . close ( ) for target , source in zip ( params to load , saved params ) : logging . info ( '%s: setting value %s' , target . name , source . shape ) target . set value ( source ) elif path . endswith ( \".npz\" ) : arrs = np . load ( path ) for target , idx in zip ( params to load , range ( len ( arrs . keys ( ) ) ) ) : source = arrs [ 'arr %d' % idx ] logging . info ( '%s: setting value %s' , target . name , source . shape ) target . set value ( source ) else : raise Exception ( \"File format of %s is not supported, use '.gz' or '.npz' or '.uncompressed.gz'\" % path ) self . train logger . load ( path )", "predictions": ["load parameters from a file ."], "references": ["load parameters from file ."], "bleu": 0.488923022434901, "rouge_l": 0.9242424242424241}
{"id": 2328, "code": "def report ( self ) : logging . info ( \"network inputs: %s\" , \" \" . join ( map ( str , self . input variables ) ) ) logging . info ( \"network targets: %s\" , \" \" . join ( map ( str , self . target variables ) ) ) logging . info ( \"network parameters: %s\" , \" \" . join ( map ( str , self . all parameters ) ) ) logging . info ( \"parameter count: %d\" , self . parameter count )", "predictions": ["report the module variables ."], "references": ["print network statistics ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 2329, "code": "def register updates ( self , * updates ) : for key , node in updates : if key not in self . registered updates : self . updates . append ( ( key , node ) ) self . registered updates . add ( key )", "predictions": ["register all updates updates to the updates ."], "references": ["register updates that will be executed in each iteration ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 2330, "code": "def register training updates ( self , * updates ) : for key , node in updates : if key not in self . registered training updates : self . training updates . append ( ( key , node ) ) self . registered training updates . add ( key )", "predictions": ["register training updates updates"], "references": ["register updates that will only be executed in training phase ."], "bleu": 0.07425134808660917, "rouge_l": 0.24596774193548387}
{"id": 2331, "code": "def register monitors ( self , * monitors ) : for key , node in monitors : if key not in self . registered monitors : node *= 1.0 self . training monitors . append ( ( key , node ) ) self . testing monitors . append ( ( key , node ) ) self . registered monitors . add ( key )", "predictions": ["register monitors with the given monitors ."], "references": ["register monitors they should be tuple of name and theano variable ."], "bleu": 0.1285981829222983, "rouge_l": 0.30148270181219106}
{"id": 2332, "code": "def dump one ( elt to pickle , file obj ) : pickled elt str = dumps ( elt to pickle ) file obj . write ( pickled elt str ) file obj . write ( '\\n\\n' )", "predictions": ["dump the instance to a file - like object"], "references": ["dumps one element to file_obj a file opened in write mode"], "bleu": 0.15982877755018768, "rouge_l": 0.2946859903381642}
{"id": 2333, "code": "def load params ( self , path , exclude free params = False ) : from deepy . core import graph from deepy . core . comp graph import Computational Graph model = graph . compile ( blocks = [ self ] ) model . load params ( path , exclude free params = exclude free params )", "predictions": ["load the graph and exclude the graph ."], "references": ["load parameters to the block ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 2334, "code": "def onehot tensor ( i matrix , vocab size ) : dim0 , dim1 = i matrix . shape i vector = i matrix . reshape ( ( - 1 , ) ) hot matrix = T . extra ops . to one hot ( i vector , vocab size ) . reshape ( ( dim0 , dim1 , vocab size ) ) return hot matrix", "predictions": ["returns a tensor matrix from a given matrix ."], "references": ["# batch x time"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2335, "code": "def create request elements ( cls , request type , credentials , url , method = 'GET' , params = None , headers = None , body = '' , secret = None , redirect uri = '' , scope = '' , csrf = '' , user state = '' ) : headers = headers or { } params = params or { } consumer key = credentials . consumer key or '' consumer secret = credentials . consumer secret or '' token = credentials . token or '' refresh token = credentials . refresh token or credentials . token or '' url , base params = cls . split url ( url ) params . update ( dict ( base params ) ) if request type == cls . USER AUTHORIZATION REQUEST TYPE : if consumer key and redirect uri and ( csrf or not cls . supports csrf protection ) : params [ 'client id' ] = consumer key params [ 'redirect uri' ] = redirect uri params [ 'scope' ] = scope if cls . supports user state : params [ 'state' ] = base64 . urlsafe b64encode ( json . dumps ( { \"csrf\" : csrf , \"user state\" : user state } ) . encode ( 'utf-8' ) ) else : params [ 'state' ] = csrf params [ 'response type' ] = 'code' headers . update ( cls . authorization header ( credentials ) ) else : raise O Auth2Error ( 'Credentials with valid consumer key and arguments ' 'redirect uri, scope and state are required to create ' 'O Auth 2.0 user authorization request elements!' ) elif request type == cls . ACCESS TOKEN REQUEST TYPE : if consumer key and consumer secret : params [ 'code' ] = token params [ 'client id' ] = consumer key params [ 'client secret' ] = consumer secret params [ 'redirect uri' ] = redirect uri params [ 'grant type' ] = 'authorization code' headers . update ( cls . authorization header ( credentials ) ) else : raise O Auth2Error ( 'Credentials with valid token, consumer key, ' 'consumer secret and argument redirect uri are required ' 'to create O Auth 2.0 access token request elements!' ) elif request type == cls . REFRESH TOKEN REQUEST TYPE : if refresh token and consumer key and consumer secret : params [ 'refresh token' ] = refresh token params [ 'client id' ] = consumer key params [ 'client secret' ] = consumer secret params [ 'grant type' ] = 'refresh token' else : raise O Auth2Error ( 'Credentials with valid refresh token, consumer key, ' 'consumer secret are required to create O Auth 2.0 ' 'refresh token request elements!' ) elif request type == cls . PROTECTED RESOURCE REQUEST TYPE : if credentials . token type == cls . BEARER : headers . update ( { 'Authorization' : 'Bearer {0}' . format ( credentials . token ) } ) elif token : params [ 'access token' ] = token else : raise O Auth2Error ( 'Credentials with valid token are required to create ' 'O Auth 2.0 protected resources request elements!' ) request elements = core . Request Elements ( url , method , params , headers , body ) return cls . x request elements filter ( request type , request elements , credentials )", "predictions": ["create a request to protected request ."], "references": ["creates |oauth2| request elements ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 2336, "code": "def x credentials parser ( credentials , data ) : credentials . expire in = data . get ( 'expires' ) if data . get ( 'token type' ) == 'bearer' : credentials . token type = 'Bearer' return credentials", "predictions": ["return step for scan step from vars from vars from vars from vars from vars from vars from vars from vars from vars from vars from vars from vars from vars"], "references": ["we need to override this method to fix facebooks naming deviation ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2337, "code": "def login ( provider name ) : response = make response ( ) result = authomatic . login ( Werkzeug Adapter ( request , response ) , provider name ) if result : if result . user : result . user . update ( ) return render template ( 'login.html' , result = result ) return response", "predictions": ["view function that handles a updates of the updates . . . . . . ."], "references": ["login handler must accept both get and post to be able to use openid ."], "bleu": 0.07692375026049747, "rouge_l": 0.06489361702127659}
{"id": 2338, "code": "def save ( self ) : if self . data : cookie = self . create cookie ( ) cookie len = len ( cookie ) if cookie len > 4093 : raise Session Error ( 'Cookie too long! The cookie size {0} ' 'is more than 4093 bytes.' . format ( cookie len ) ) self . adapter . set header ( 'Set-Cookie' , cookie ) self . data = { }", "predictions": ["save the data data"], "references": ["adds the session cookie to headers ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 2339, "code": "def get data ( self ) : cookie = self . adapter . cookies . get ( self . name ) return self . deserialize ( cookie ) if cookie else { }", "predictions": ["skip the data data"], "references": ["extracts the session data from cookie ."], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 2340, "code": "def data ( self ) : if not self . data : self . data = self . get data ( ) if self . data is None : self . data = { } return self . data", "predictions": ["returns the train train train train"], "references": ["gets session data lazily ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2341, "code": "def signature ( self , * parts ) : signature = hmac . new ( six . b ( self . secret ) , digestmod = hashlib . sha1 ) signature . update ( six . b ( '|' . join ( parts ) ) ) return signature . hexdigest ( )", "predictions": ["returns a run run run in the current form . . . . . ."], "references": ["creates signature for the session ."], "bleu": 0.09103526405546068, "rouge_l": 0.2064297800338409}
{"id": 2342, "code": "def valid ( self ) : if self . expiration time : return self . expiration time > int ( time . time ( ) ) else : return True", "predictions": ["returns true if the message is run"], "references": ["true if credentials are valid false if expired ."], "bleu": 0.18370727471078332, "rouge_l": 0.24448897795591182}
{"id": 2343, "code": "def is binary string ( content ) : textchars = ( bytearray ( [ 7 , 8 , 9 , 10 , 12 , 13 , 27 ] ) + bytearray ( range ( 0x20 , 0x100 ) ) ) return bool ( content . translate ( None , textchars ) )", "predictions": ["* report * as a binary self ."], "references": ["return true if string is binary data ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 2344, "code": "def content ( self ) : if not self . content : content = self . httplib response . read ( ) if self . is binary string ( content ) : self . content = content else : self . content = content . decode ( 'utf-8' ) return self . content", "predictions": ["the get get get get get the get get get get the get get get get get get get get get get get get get get get get get get get"], "references": ["the whole response content ."], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 2345, "code": "def create request elements ( cls , request type , credentials , url , params = None , headers = None , body = '' , method = 'GET' , verifier = '' , callback = '' ) : params = params or { } headers = headers or { } consumer key = credentials . consumer key or '' consumer secret = credentials . consumer secret or '' token = credentials . token or '' token secret = credentials . token secret or '' url , base params = cls . split url ( url ) params . update ( dict ( base params ) ) if request type == cls . USER AUTHORIZATION REQUEST TYPE : if token : params [ 'oauth token' ] = token else : raise O Auth1Error ( 'Credentials with valid token are required to create ' 'User Authorization URL!' ) else : if request type == cls . REQUEST TOKEN REQUEST TYPE : if consumer key and consumer secret and callback : params [ 'oauth consumer key' ] = consumer key params [ 'oauth callback' ] = callback else : raise O Auth1Error ( 'Credentials with valid consumer key, consumer secret ' 'and callback are required to create Request Token ' 'URL!' ) elif request type == cls . ACCESS TOKEN REQUEST TYPE : if consumer key and consumer secret and token and verifier : params [ 'oauth token' ] = token params [ 'oauth consumer key' ] = consumer key params [ 'oauth verifier' ] = verifier else : raise O Auth1Error ( 'Credentials with valid consumer key, ' 'consumer secret, token and argument verifier' ' are required to create Access Token URL!' ) elif request type == cls . PROTECTED RESOURCE REQUEST TYPE : if consumer key and consumer secret and token and token secret : params [ 'oauth token' ] = token params [ 'oauth consumer key' ] = consumer key else : raise O Auth1Error ( 'Credentials with valid consumer key, ' + 'consumer secret, token and token secret are required ' 'to create Protected Resources URL!' ) params [ 'oauth signature method' ] = cls . signature generator . method params [ 'oauth timestamp' ] = str ( int ( time . time ( ) ) ) params [ 'oauth nonce' ] = cls . csrf generator ( str ( uuid . uuid4 ( ) ) ) params [ 'oauth version' ] = '1.0' params [ 'oauth signature' ] = cls . signature generator . create signature ( method , url , params , consumer secret , token secret ) request elements = core . Request Elements ( url , method , params , headers , body ) return cls . x request elements filter ( request type , request elements , credentials )", "predictions": ["apply request to the request class"], "references": ["creates |oauth1| request elements ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 2346, "code": "def access user info ( self ) : response = super ( Bitbucket , self ) . access user info ( ) response . data . setdefault ( \"email\" , None ) email response = self . access ( self . user email url ) if email response . data : for item in email response . data : if item . get ( \"primary\" , False ) : response . data . update ( email = item . get ( \"email\" , None ) ) return response", "predictions": ["returns information about the user user key key key key key key key key key key key key key key key key key key key key key key key key key"], "references": ["email is available in separate method so second request is needed ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2347, "code": "def login ( self , * login args , * * login kwargs ) : def decorator ( f ) : @ wraps ( f ) def decorated ( * args , * * kwargs ) : self . response = make response ( ) adapter = Werkzeug Adapter ( request , self . response ) login kwargs . setdefault ( 'session' , session ) login kwargs . setdefault ( 'session saver' , self . session saver ) self . result = super ( Flask Authomatic , self ) . login ( adapter , * login args , * * login kwargs ) return f ( * args , * * kwargs ) return decorated return decorator", "predictions": ["add var var . to the from the from var ."], "references": ["decorator for flask view functions ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 2348, "code": "def login ( self ) : if self . params . get ( self . identifier param ) : self . log ( logging . INFO , u'Starting Open ID authentication procedure.' ) url = users . create login url ( dest url = self . url , federated identity = self . identifier ) self . log ( logging . INFO , u'Redirecting user to {0}.' . format ( url ) ) self . redirect ( url ) else : self . log ( logging . INFO , u'Continuing Open ID authentication procedure after redirect.' ) user = users . get current user ( ) if user : self . log ( logging . INFO , u'Authentication successful.' ) self . log ( logging . INFO , u'Creating user.' ) self . user = core . User ( self , id = user . federated identity ( ) , email = user . email ( ) , gae user = user ) else : raise Failure Error ( 'Unable to authenticate identifier \"{0}\"!' . format ( self . identifier ) )", "predictions": ["authenticate against the user service = 0 = 0 = 1 = 0 = 1 = 0 = 1 = 1 = 1 = 1 = 1 = 1 = 1"], "references": ["launches the openid authentication procedure ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 2349, "code": "def session set ( self , key , value ) : self . session [ self . session key ( key ) ] = value", "predictions": ["set a invoke invoke"], "references": ["saves a value to session ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 2350, "code": "def split url ( url ) : split = parse . urlsplit ( url ) base = parse . urlunsplit ( ( split . scheme , split . netloc , split . path , 0 , 0 ) ) params = parse . parse qsl ( split . query , True ) return base , params", "predictions": ["invoke a url url"], "references": ["splits given url to url base and params converted to list of tuples ."], "bleu": 0.032639898338235177, "rouge_l": 0.20198675496688742}
{"id": 2351, "code": "def get app kwarg dict ( app Instance ) : app config = getattr ( app Instance , 'config' , { } ) return dict ( ( k . lower ( ) . replace ( 'cors ' , '' ) , app config . get ( k ) ) for k in CONFIG OPTIONS if app config . get ( k ) is not None )", "predictions": ["returns the self . for the given encoders . . ."], "references": ["returns the dictionary of cors specific app configurations ."], "bleu": 0.16108992769687397, "rouge_l": 0.3055091819699499}
{"id": 2352, "code": "def ensure iterable ( inst ) : if isinstance ( inst , str ) : return [ inst ] elif not isinstance ( inst , collections . abc . Iterable ) : return [ inst ] else : return inst", "predictions": ["stack if we can decoding this decoders"], "references": ["wraps scalars or string types as a list or returns the iterable instance ."], "bleu": 0.057461663912368725, "rouge_l": 0.0}
{"id": 2353, "code": "def isclose ( a , b , * , rel tol = 1e-09 , abs tol = 0.0 ) : try : return math . isclose ( a , b , rel tol = rel tol , abs tol = abs tol ) except Attribute Error : if ( rel tol < 0.0 ) or ( abs tol < 0.0 ) : raise Value Error ( \"Tolerances must be non-negative, but are rel tol: {} and abs tol: {}\" . format ( rel tol , abs tol ) ) if math . isnan ( a ) or math . isnan ( b ) : return False if ( a == b ) : return True if math . isinf ( a ) or math . isinf ( b ) : return False diff = abs ( a - b ) return ( diff <= rel tol * abs ( b ) ) or ( diff <= rel tol * abs ( a ) ) or ( diff <= abs tol )", "predictions": ["check if two numbers are equal"], "references": ["python 3 . 4 does not have math . isclose so we need to steal it and add it here ."], "bleu": 0.015247336102977219, "rouge_l": 0.0}
{"id": 2354, "code": "def get corresponding offsets ( onset fronts , onset front id , onsets , offsets ) : corresponding offsets = [ ] for index in get front idxs from id ( onset fronts , onset front id ) : offset fidx , offset sidx = lookup offset by onset idx ( index , onsets , offsets ) corresponding offsets . append ( ( offset fidx , offset sidx ) ) return corresponding offsets", "predictions": ["decode the corresponding self layer into the corresponding self layer layer layer layer layer layer layer ."], "references": ["gets the offsets that occur as close as possible to the onsets in the given onset - front ."], "bleu": 0.07637273831183729, "rouge_l": 0.16501352569882777}
{"id": 2355, "code": "def remove overlaps ( segmentation mask , fronts ) : fidxs , sidxs = np . where ( ( segmentation mask != fronts ) & ( segmentation mask != 0 ) & ( fronts != 0 ) ) fronts [ fidxs , sidxs ] = 0", "predictions": ["all parameters in the self return the self return the self return the self return value"], "references": ["removes all points in the fronts that overlap with the segmentation mask ."], "bleu": 0.11502783619900048, "rouge_l": 0.28110599078341014}
{"id": 2356, "code": "def merge adjacent segments ( mask ) : mask ids = [ id for id in np . unique ( mask ) if id != 0 ] for id in mask ids : myfidxs , mysidxs = np . where ( mask == id ) for other in mask ids : if id == other : continue else : other fidxs , other sidxs = np . where ( mask == other ) if segments are adjacent ( ( myfidxs , mysidxs ) , ( other fidxs , other sidxs ) ) : mask [ other fidxs , other sidxs ] = id", "predictions": ["setup all variables from a list of variables"], "references": ["merges all segments in mask which are touching ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 2357, "code": "def asa task ( q , masks , stft , sample width , frame rate , nsamples for each fft ) : for mask in masks : mask = np . where ( mask > 0 , 1 , 0 ) masks = [ mask * stft for mask in masks ] nparrs = [ ] dtype dict = { 1 : np . int8 , 2 : np . int16 , 4 : np . int32 } dtype = dtype dict [ sample width ] for m in masks : times , nparr = signal . istft ( m , frame rate , nperseg = nsamples for each fft ) nparr = nparr . astype ( dtype ) nparrs . append ( nparr ) for m in nparrs : q . put ( m ) q . put ( \"DONE\" )", "predictions": ["compute the compute compute compute the compute - task"], "references": ["worker for the asa algorithm s multiprocessing step ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 2358, "code": "def list to tf input ( data , response index , num outcomes ) : matrix = np . matrix ( [ row [ : response index ] + row [ response index + 1 : ] for row in data ] ) outcomes = np . asarray ( [ row [ response index ] for row in data ] , dtype = np . uint8 ) outcomes onehot = ( np . arange ( num outcomes ) == outcomes [ : , None ] ) . astype ( np . float32 ) return matrix , outcomes onehot", "predictions": ["convert save outcomes to tf variables"], "references": ["separates the outcome feature from the data and creates the onehot vector for each row ."], "bleu": 0.03508375277329034, "rouge_l": 0.0}
{"id": 2359, "code": "def expand and standardize dataset ( response index , response header , data set , col vals , headers , standardizers , feats to ignore , columns to expand , outcome trans dict ) : modified set = [ ] for row index , row in enumerate ( data set ) : new row = [ ] for col index , val in enumerate ( row ) : header = headers [ col index ] if col index == response index : new outcome = outcome trans dict [ val ] new row . append ( new outcome ) elif header in feats to ignore : pass elif header in columns to expand : for poss val in col vals [ header ] : if val == poss val : new cat val = 1.0 else : new cat val = - 1.0 new row . append ( new cat val ) else : new cont val = float ( ( val - standardizers [ header ] [ 'mean' ] ) / standardizers [ header ] [ 'std dev' ] ) new row . append ( new cont val ) modified set . append ( new row ) expanded headers = [ ] for header in headers : if header in feats to ignore : pass elif ( header in columns to expand ) and ( header is not response header ) : for poss val in col vals [ header ] : new header = '{} {}' . format ( header , poss val ) expanded headers . append ( new header ) else : expanded headers . append ( header ) return modified set , expanded headers", "predictions": ["load the % self params from the path to the path . ."], "references": ["standardizes continuous features and expands categorical features ."], "bleu": 0.09552040806823771, "rouge_l": 0.09951060358890701}
{"id": 2360, "code": "def list to tf input ( data , response index , num outcomes ) : matrix = np . matrix ( [ row [ : response index ] + row [ response index + 1 : ] for row in data ] ) outcomes = np . asarray ( [ row [ response index ] for row in data ] , dtype = np . uint8 ) return matrix , outcomes", "predictions": ["convert report to self input input input join input outcomes input input input input input input input input input ."], "references": ["separates the outcome feature from the data ."], "bleu": 0.06108557268562171, "rouge_l": 0.07741116751269035}
{"id": 2361, "code": "def update index url from configs ( self ) : if 'VIRTUAL ENV' in os . environ : self . pip config locations . append ( os . path . join ( os . environ [ 'VIRTUAL ENV' ] , 'pip.conf' ) ) self . pip config locations . append ( os . path . join ( os . environ [ 'VIRTUAL ENV' ] , 'pip.ini' ) ) if site config files : self . pip config locations . extend ( site config files ) index url = None custom config = None if 'PIP INDEX URL' in os . environ and os . environ [ 'PIP INDEX URL' ] : index url = os . environ [ 'PIP INDEX URL' ] custom config = 'PIP INDEX URL environment variable' else : for pip config filename in self . pip config locations : if pip config filename . startswith ( '~' ) : pip config filename = os . path . expanduser ( pip config filename ) if os . path . isfile ( pip config filename ) : config = Config Parser ( ) config . read ( [ pip config filename ] ) try : index url = config . get ( 'global' , 'index-url' ) custom config = pip config filename break except ( No Option Error , No Section Error ) : pass if index url : self . PYPI API URL = self . prepare api url ( index url ) print ( Color ( 'Setting API url to {{autoyellow}}{}{{/autoyellow}} as found in {{autoyellow}}{}{{/autoyellow}}' '. Use --default-index-url to use pypi default index' . format ( self . PYPI API URL , custom config ) ) )", "predictions": ["register append updates self . append to the updates add add to the updates ."], "references": ["checks for alternative index - url in pip . conf"], "bleu": 0.08225964699966554, "rouge_l": 0.08299319727891155}
{"id": 2362, "code": "def main ( ) : options = get options ( ) Windows . enable ( auto colors = True , reset atexit = True ) try : check for virtualenv ( options ) filenames = Requirements Detector ( options . get ( '<requirements file>' ) ) . get filenames ( ) if filenames : print ( Color ( '{{autoyellow}}Found valid requirements file(s):{{/autoyellow}} ' '{{autocyan}}\\n{}{{/autocyan}}' . format ( '\\n' . join ( filenames ) ) ) ) else : print ( Color ( '{autoyellow}No requirements files found in current directory. CD into your project ' 'or manually specify requirements files as arguments.{/autoyellow}' ) ) return packages = Packages Detector ( filenames ) . get packages ( ) packages status map = Packages Status Detector ( packages , options . get ( '--use-default-index' ) ) . detect available upgrades ( options ) selected packages = Package Interactive Selector ( packages status map , options ) . get packages ( ) upgraded packages = Packages Upgrader ( selected packages , filenames , options ) . do upgrade ( ) print ( Color ( '{{autogreen}}Successfully upgraded (and updated requirements) for the following packages: ' '{}{{/autogreen}}' . format ( ',' . join ( [ package [ 'name' ] for package in upgraded packages ] ) ) ) ) if options [ '--dry-run' ] : print ( Color ( '{automagenta}Actually, no, because this was a simulation using --dry-run{/automagenta}' ) ) except Keyboard Interrupt : print ( Color ( '\\n{autored}Upgrade interrupted.{/autored}' ) )", "predictions": ["register all simulation requirements packages"], "references": ["main cli entrypoint ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2363, "code": "def autodetect files ( self ) : if self . is valid requirements file ( 'requirements.txt' ) : self . filenames . append ( 'requirements.txt' ) if self . is valid requirements file ( 'requirements.pip' ) : self . filenames . append ( 'requirements.pip' ) if os . path . isdir ( 'requirements' ) : for filename in os . listdir ( 'requirements' ) : file path = os . path . join ( 'requirements' , filename ) if self . is valid requirements file ( file path ) : self . filenames . append ( file path ) self . check inclusions recursively ( )", "predictions": ["register not key . ."], "references": ["attempt to detect requirements files in the current working directory"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 2364, "code": "def handle error ( errcode ) : if type ( errcode ) is c int : errcode = errcode . value if errcode == 0 : pass elif errcode == - 1 : raise Timeout Error ( \"the operation failed due to a timeout.\" ) elif errcode == - 2 : raise Lost Error ( \"the stream has been lost.\" ) elif errcode == - 3 : raise Invalid Argument Error ( \"an argument was incorrectly specified.\" ) elif errcode == - 4 : raise Internal Error ( \"an internal error has occurred.\" ) elif errcode < 0 : raise Runtime Error ( \"an unknown error has occurred.\" )", "predictions": ["raises valueerror if the one of the one of the one of the one of the one of the one of the one of the one of the one of the"], "references": ["error handler function . translates an error code into an exception ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2365, "code": "def child ( self , name ) : return XML Element ( lib . lsl child ( self . e , str . encode ( name ) ) )", "predictions": ["return the load representation of the load element ."], "references": ["get a child with a specified name ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 2366, "code": "def set name ( self , name ) : return bool ( lib . lsl set name ( self . e , str . encode ( name ) ) )", "predictions": ["set for the tensor"], "references": ["set the element s name . returns false if the node is empty ."], "bleu": 0.032639898338235177, "rouge_l": 0.20198675496688742}
{"id": 2367, "code": "def set value ( self , value ) : return bool ( lib . lsl set value ( self . e , str . encode ( value ) ) )", "predictions": ["create a request request request request request request request"], "references": ["set the element s value . returns false if the node is empty ."], "bleu": 0.06809538093398164, "rouge_l": 0.0}
{"id": 2368, "code": "def append child ( self , name ) : return XML Element ( lib . lsl append child ( self . e , str . encode ( name ) ) )", "predictions": ["append a child to the end of the child ."], "references": ["append a child element with the specified name ."], "bleu": 0.25965358893403384, "rouge_l": 0.5313588850174217}
{"id": 2369, "code": "def prepend child ( self , name ) : return XML Element ( lib . lsl prepend child ( self . e , str . encode ( name ) ) )", "predictions": ["return a new child name with the given name ."], "references": ["prepend a child element with the specified name ."], "bleu": 0.22692039365038064, "rouge_l": 0.6376306620209059}
{"id": 2370, "code": "def append copy ( self , elem ) : return XML Element ( lib . lsl append copy ( self . e , elem . e ) )", "predictions": ["return a copy of this element with the given elem ."], "references": ["append a copy of the specified element as a child ."], "bleu": 0.24384183193426084, "rouge_l": 0.45454545454545453}
{"id": 2371, "code": "def prepend copy ( self , elem ) : return XML Element ( lib . lsl prepend copy ( self . e , elem . e ) )", "predictions": ["return a copy of this element with the given elem ."], "references": ["prepend a copy of the specified element as a child ."], "bleu": 0.24384183193426084, "rouge_l": 0.45454545454545453}
{"id": 2372, "code": "def remove child ( self , rhs ) : if type ( rhs ) is XML Element : lib . lsl remove child ( self . e , rhs . e ) else : lib . lsl remove child n ( self . e , rhs )", "predictions": ["remove a child from the list ."], "references": ["remove a given child element specified by name or as element ."], "bleu": 0.13597602315271134, "rouge_l": 0.40197693574958815}
{"id": 2373, "code": "def do AUTOCOMPLETE ( cmd , s ) : s = list ( preprocess query ( s ) ) [ 0 ] keys = [ k . decode ( ) for k in DB . smembers ( edge ngram key ( s ) ) ] print ( white ( keys ) ) print ( magenta ( '({} elements)' . format ( len ( keys ) ) ) )", "predictions": ["autocomplete a list of strings"], "references": ["shows autocomplete results for a given token ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 2374, "code": "def compute edge ngrams ( token , min = None ) : if min is None : min = config . MIN EDGE NGRAMS token = token [ : config . MAX EDGE NGRAMS + 1 ] return [ token [ : i ] for i in range ( min , len ( token ) ) ]", "predictions": ["compute the edge ngrams ngrams ngrams ."], "references": ["compute edge ngram of token from min . does not include token itself ."], "bleu": 0.0812630644213965, "rouge_l": 0.2695139911634757}
{"id": 2375, "code": "def iter pipe ( pipe , processors ) : if isinstance ( pipe , str ) : pipe = [ pipe ] for it in processors : pipe = it ( pipe ) yield from pipe", "predictions": ["yield processors from a pipe"], "references": ["allow for iterators to return either an item or an iterator of items ."], "bleu": 0.037948473198912445, "rouge_l": 0.0}
{"id": 2376, "code": "def make fuzzy ( word , max = 1 ) : neighbors = [ ] for i in range ( 0 , len ( word ) - 1 ) : neighbor = list ( word ) neighbor [ i ] , neighbor [ i + 1 ] = neighbor [ i + 1 ] , neighbor [ i ] neighbors . append ( '' . join ( neighbor ) ) for letter in string . ascii lowercase : for i in range ( 0 , len ( word ) ) : neighbor = list ( word ) if letter != neighbor [ i ] : neighbor [ i ] = letter neighbors . append ( '' . join ( neighbor ) ) for letter in string . ascii lowercase : for i in range ( 0 , len ( word ) + 1 ) : neighbor = list ( word ) neighbor . insert ( i , letter ) neighbors . append ( '' . join ( neighbor ) ) if len ( word ) > 3 : for i in range ( 0 , len ( word ) ) : neighbor = list ( word ) del neighbor [ i ] neighbors . append ( '' . join ( neighbor ) ) return neighbors", "predictions": ["build a fuzzy from a word string ."], "references": ["naive neighborhoods algo ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 2377, "code": "def do help ( self , command ) : if command : doc = getattr ( self , 'do ' + command ) . doc print ( cyan ( doc . replace ( ' ' * 8 , '' ) ) ) else : print ( magenta ( 'Available commands:' ) ) print ( magenta ( 'Type \"HELP <command>\" to get more info.' ) ) names = self . get names ( ) names . sort ( ) for name in names : if name [ : 3 ] != 'do ' : continue doc = getattr ( self , name ) . doc doc = doc . split ( '\\n' ) [ 0 ] print ( '{} {}' . format ( yellow ( name [ 3 : ] ) , cyan ( doc . replace ( ' ' * 8 , ' ' ) . replace ( '\\n' , '' ) ) ) )", "predictions": ["print help for the given command ."], "references": ["display this help message ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 2378, "code": "def do DBINFO ( self , * args ) : info = DB . info ( ) keys = [ 'keyspace misses' , 'keyspace hits' , 'used memory human' , 'total commands processed' , 'total connections received' , 'connected clients' ] for key in keys : print ( '{}: {}' . format ( white ( key ) , blue ( info [ key ] ) ) ) nb of redis db = int ( DB . config get ( 'databases' ) [ 'databases' ] ) for db index in range ( nb of redis db - 1 ) : db name = 'db{}' . format ( db index ) if db name in info : label = white ( 'nb keys (db {})' . format ( db index ) ) print ( '{}: {}' . format ( label , blue ( info [ db name ] [ 'keys' ] ) ) )", "predictions": ["list commands in redis"], "references": ["print some useful infos from redis db ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 2379, "code": "def send ( r , stream = False ) : r . send ( stream = stream ) return r . response", "predictions": ["send a stream to the client ."], "references": ["just sends the request using its send method and returns its response ."], "bleu": 0.09374222649442905, "rouge_l": 0.1897356143079316}
{"id": 2380, "code": "def reinterptet harray to bits ( type From , sig Or Val , bits T ) : size = int ( type From . size ) width Of Elm = type From . elm Type . bit length ( ) w = bits T . bit length ( ) if size * width Of Elm != w : raise Type Conversion Err ( \"Size of types is different\" , size * width Of Elm , w ) part T = Bits ( width Of Elm ) parts = [ p . reinterpret cast ( part T ) for p in sig Or Val ] return Concat ( * reversed ( parts ) ) . reinterpret cast ( bits T )", "predictions": ["convert a reinterptet type to an equivalent bits ."], "references": ["cast harray signal or value to signal or value of type bits"], "bleu": 0.12026590852507517, "rouge_l": 0.1856925418569254}
{"id": 2381, "code": "def slice to SLICE ( slice Vals , width ) : if slice Vals . step is not None : raise Not Implemented Error ( ) start = slice Vals . start stop = slice Vals . stop if slice Vals . start is None : start = INT . from Py ( width ) else : start = to H Val ( slice Vals . start ) if slice Vals . stop is None : stop = INT . from Py ( 0 ) else : stop = to H Val ( slice Vals . stop ) start Is Val = isinstance ( start , Value ) stop Is Val = isinstance ( stop , Value ) indexes Are Values = start Is Val and stop Is Val if indexes Are Values : update Time = max ( start . update Time , stop . update Time ) else : update Time = - 1 return Slice . get Value Cls ( ) ( ( start , stop ) , SLICE , 1 , update Time )", "predictions": ["convert a slice to a slice"], "references": ["convert python slice to value of slice hdl type"], "bleu": 0.20034704329441452, "rouge_l": 0.5147679324894514}
{"id": 2382, "code": "def find files ( directory , pattern , recursive = True ) : if not os . path . isdir ( directory ) : if os . path . exists ( directory ) : raise IO Error ( directory + ' is not directory' ) else : raise IO Error ( directory + \" does not exists\" ) if recursive : for root , , files in os . walk ( directory ) : for basename in files : if fnmatch . fnmatch ( basename , pattern ) : filename = os . path . join ( root , basename ) yield filename else : root = directory for basename in os . listdir ( root ) : if fnmatch . fnmatch ( basename , pattern ) : filename = os . path . join ( root , basename ) if os . path . isfile ( filename ) : yield filename", "predictions": ["find all files under pattern in pattern ."], "references": ["find files by pattern in directory"], "bleu": 0.239802967618271, "rouge_l": 0.5865384615384615}
{"id": 2383, "code": "def is Pow2 ( num ) -> bool : if not isinstance ( num , int ) : num = int ( num ) return num != 0 and ( ( num & ( num - 1 ) ) == 0 )", "predictions": ["return true if the number is an integer number"], "references": ["check if number or constant is power of two"], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 2384, "code": "def Case ( self , case Val , * statements ) : assert self . parent Stm is None case Val = to H Val ( case Val , self . switch On . dtype ) assert isinstance ( case Val , Value ) , case Val assert case Val . is Full Vld ( ) , \"Cmp with invalid value\" assert case Val not in self . case value index , ( \"Switch statement already has case for value \" , case Val ) self . rank += 1 case = [ ] self . case value index [ case Val ] = len ( self . cases ) self . cases . append ( ( case Val , case ) ) cond = self . switch On . eq ( case Val ) self . inputs . append ( cond ) cond . endpoints . append ( self ) self . register stements ( statements , case ) return self", "predictions": ["create a stements object with a case - insensitive value ."], "references": ["c - like case of switch statement"], "bleu": 0.12605968092174913, "rouge_l": 0.1157495256166983}
{"id": 2385, "code": "def Default ( self , * statements ) : assert self . parent Stm is None self . rank += 1 self . default = [ ] self . register stements ( statements , self . default ) return self", "predictions": ["create a new node with a default value ."], "references": ["c - like default of switch statement"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 2386, "code": "def vcd Register Interfaces ( self , obj : Union [ Interface , Unit ] , parent : Optional [ Vcd Var Writing Scope ] ) : if hasattr ( obj , \" interfaces\" ) and obj . interfaces : name = obj . name parent = self . vcd Writer if parent is None else parent sub Scope = parent . var Scope ( name ) self . obj2scope [ obj ] = sub Scope with sub Scope : for ch Intf in obj . interfaces : self . vcd Register Interfaces ( ch Intf , sub Scope ) if isinstance ( obj , ( Unit , Sim Model ) ) : for u in obj . units : self . vcd Register Interfaces ( u , sub Scope ) return sub Scope else : t = obj . dtype if isinstance ( t , self . supported type classes ) : t Name , width , formatter = vcd Type Info For H Type ( t ) try : parent . add Var ( obj , get Signal Name ( obj ) , t Name , width , formatter ) except Var Already Registered : pass", "predictions": ["adds an object to the parent object ."], "references": ["register signals from interfaces for interface or unit instances"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 2387, "code": "def before Sim ( self , simulator , synthesised Unit ) : vcd = self . vcd Writer vcd . date ( datetime . now ( ) ) vcd . timescale ( 1 ) self . vcd Register Interfaces ( synthesised Unit , None ) self . vcd Register Remaining Signals ( synthesised Unit ) vcd . enddefinitions ( )", "predictions": ["before the sim is before the sim"], "references": ["this method is called before first step of simulation ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 2388, "code": "def log Change ( self , now Time , sig , next Val ) : try : self . vcd Writer . log Change ( now Time , sig , next Val ) except Key Error : pass", "predictions": ["log a message at the current log level ."], "references": ["this method is called for every value change of any signal ."], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 2389, "code": "def distinct By ( iterable , fn ) : s = set ( ) for i in iterable : r = fn ( i ) if r not in s : s . add ( r ) yield i", "predictions": ["yield all the distinct values from an iterable ."], "references": ["uniq operation with key selector"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2390, "code": "def remove Unconnected Signals ( netlist ) : to Delete = set ( ) to Search = netlist . signals while to Search : to Search = set ( ) for sig in to Search : if not sig . endpoints : try : if sig . interface is not None : continue except Attribute Error : pass for e in sig . drivers : if isinstance ( e , Operator ) : inputs = e . operands if e . result is sig : e . result = None else : inputs = e . inputs netlist . statements . discard ( e ) for op in inputs : if not isinstance ( op , Value ) : try : op . endpoints . remove ( e ) except Key Error : continue to Search . add ( op ) to Delete . add ( sig ) if to Delete : for sig in to Delete : if sig . ctx == netlist : netlist . signals . remove ( sig ) to Search . discard ( sig ) to Delete = set ( ) to Search = to Search", "predictions": ["remove statements from netlist ."], "references": ["if signal is not driving anything remove it"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 2391, "code": "def on Write Req ( self , sim , addr , data ) : self . requests . append ( ( WRITE , addr , data ) )", "predictions": ["write a data entry to the client ."], "references": ["on writereqrecieved in monitor mode"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2392, "code": "def name for process and mark outputs ( statements : List [ Hdl Statement ] ) -> str : out names = [ ] for stm in statements : for sig in stm . outputs : if not sig . has Generic Name : out names . append ( sig . name ) if out names : return min ( out names ) else : return \"\"", "predictions": ["return the name of the process and mark it as a string ."], "references": ["resolve name for process and mark outputs of statemens as not hidden"], "bleu": 0.2044800736021839, "rouge_l": 0.4029062087186261}
{"id": 2393, "code": "def cut off drivers of ( dst Signal , statements ) : separated = [ ] stm filter = [ ] for stm in statements : stm . clean signal meta ( ) d = stm . cut off drivers of ( dst Signal ) if d is not None : separated . append ( d ) f = d is not stm stm filter . append ( f ) return list ( compress ( statements , stm filter ) ) , separated", "predictions": ["cut statements off statements to dst statements ."], "references": ["cut off drivers from statements"], "bleu": 0.19070828081828378, "rouge_l": 0.48157894736842105}
{"id": 2394, "code": "def synthesize ( self , name , interfaces , target Platform ) : ent = Entity ( name ) ent . name = name + \" inst\" for , v in self . params . items ( ) : ent . generics . append ( v ) if isinstance ( interfaces , set ) : intf Set = interfaces else : intf Set = set ( interfaces ) for s in interfaces : pi = port Itemfrom Signal ( s , ent ) pi . register Intern Sig ( s ) ent . ports . append ( pi ) s . hidden = False remove Unconnected Signals ( self ) mark Visibility Of Signals ( self , name , self . signals , intf Set ) for proc in target Platform . before Hdl Arch Generation : proc ( self ) arch = Architecture ( ent ) for p in statements to HW Processes ( self . statements ) : arch . processes . append ( p ) for s in self . signals : if s not in intf Set and not s . hidden : arch . variables . append ( s ) for u in self . sub Units : arch . component Instances . append ( u ) for su in distinct By ( self . sub Units , lambda x : x . name ) : arch . components . append ( su ) self . synthesised = True return [ ent , arch ]", "predictions": ["remove interfaces from interfaces"], "references": ["build entity and architecture instance out of netlist representation"], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 2395, "code": "def get Max Stm Id For Stm ( stm ) : max Id = 0 if isinstance ( stm , Assignment ) : return stm . inst Id elif isinstance ( stm , Wait Stm ) : return max Id else : for stm in stm . iter stms ( ) : max Id = max ( max Id , get Max Stm Id For Stm ( stm ) ) return max Id", "predictions": ["returns the maximum value of a given stm stm or none if not found ."], "references": ["get maximum _instid from all assigments in statement"], "bleu": 0.08225964699966554, "rouge_l": 0.09200603318250376}
{"id": 2396, "code": "def monitor ( self , sim ) : if self . not Reset ( sim ) and self . enabled : self . wr Rd ( sim . write , 1 ) yield sim . wait On Comb Update ( ) d = self . do Read ( sim ) self . data . append ( d ) else : self . wr Rd ( sim . write , 0 )", "predictions": ["monitor the process with the same value of the process ."], "references": ["collect data from interface"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2397, "code": "def do Write ( self , sim , data ) : sim . write ( data , self . intf . data )", "predictions": ["writes the given data to the client ."], "references": ["write data to interface"], "bleu": 0.21105340631872638, "rouge_l": 0.3546511627906977}
{"id": 2398, "code": "def driver ( self , sim ) : r = sim . read if self . actual Data is NOP and self . data : self . actual Data = self . data . popleft ( ) do = self . actual Data is not NOP if do : self . do Write ( sim , self . actual Data ) else : self . do Write ( sim , None ) en = self . not Reset ( sim ) and self . enabled if not ( en and do ) : return yield sim . wait On Comb Update ( ) rd = self . is Rd ( r ) if en : assert rd . vld Mask , ( ( \"%r: ready signal for interface %r is in invalid state,\" \" this would cause desynchronization\" ) % ( sim . now , self . intf ) ) if rd . val : if self . debug Output is not None : self . debug Output . write ( \"%s, wrote, %d: %r\\n\" % ( self . intf . get Full Name ( ) , sim . now , self . actual Data ) ) if self . data : self . actual Data = self . data . popleft ( ) else : self . actual Data = NOP", "predictions": ["return the driver of the vm ."], "references": ["push data to interface"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2399, "code": "def get Physical Name ( self ) : if hasattr ( self , \" bounded Entity Port\" ) : return self . bounded Entity Port . name else : return self . get Full Name ( ) . replace ( '.' , self . NAME SEPARATOR )", "predictions": ["return the physical name of the bounded class ."], "references": ["get name in hdl"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 2400, "code": "def bit length ( self ) : try : interfaces = self . interfaces except Attribute Error : interfaces = None if interfaces is None : intf = self . clone ( ) intf . load Declarations ( ) interfaces = intf . interfaces if interfaces : w = 0 for i in interfaces : w += i . bit length ( ) return w else : return self . dtype . bit length ( )", "predictions": ["the append child child interfaces or none if not present ."], "references": ["sum of all width of interfaces in this interface"], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 2401, "code": "def sensitivity By Op ( op ) : if op == All Ops . RISING EDGE : return SENSITIVITY . RISING elif op == All Ops . FALLING EDGE : return SENSITIVITY . FALLING else : raise Type Error ( )", "predictions": ["prepend to the instruction"], "references": ["get sensitivity type for operator"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 2402, "code": "def eval ( self , operator , simulator = None ) : def get Val ( v ) : while not isinstance ( v , Value ) : v = v . val return v operands = list ( map ( get Val , operator . operands ) ) if is Event Dependent Op ( operator . operator ) : operands . append ( simulator . now ) elif operator . operator == All Ops . Int To Bits : operands . append ( operator . result . dtype ) return self . eval Fn ( * operands )", "predictions": ["evaluate the statement for the given operator operator ."], "references": ["load all operands and process them by self . _evalfn"], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 2403, "code": "def convert Bits ( self , sig Or Val , to Type ) : if isinstance ( sig Or Val , Value ) : return convert Bits val ( self , sig Or Val , to Type ) elif isinstance ( to Type , H Bool ) : if self . bit length ( ) == 1 : v = 0 if sig Or Val . dtype . negated else 1 return sig Or Val . eq ( self . get Value Cls ( ) . from Py ( v , self ) ) elif isinstance ( to Type , Bits ) : if self . bit length ( ) == to Type . bit length ( ) : return sig Or Val . conv Sign ( to Type . signed ) elif to Type == INT : return Operator . with Res ( All Ops . Bits To Int , [ sig Or Val ] , to Type ) return default auto cast fn ( self , sig Or Val , to Type )", "predictions": ["prepend the attribute to the appropriate type"], "references": ["cast signed - unsigned to int or bool"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 2404, "code": "def reinterpret bits to hstruct ( sig Or Val , h Struct T ) : container = h Struct T . from Py ( None ) offset = 0 for f in h Struct T . fields : t = f . dtype width = t . bit length ( ) if f . name is not None : s = sig Or Val [ ( width + offset ) : offset ] s = s . reinterpret cast ( t ) setattr ( container , f . name , s ) offset += width return container", "predictions": ["convert a . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["reinterpret signal of type bits to signal of type hstruct"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2405, "code": "def full Word Cnt ( self , start : int , end : int ) : assert end >= start , ( start , end ) gap = max ( 0 , ( end - start ) - ( start % self . word Width ) ) return gap // self . word Width", "predictions": ["do a do a do a do a do so that it can be used in the first in the first in the first in the in the in the in"], "references": ["count of complete words between two addresses"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2406, "code": "def discover sensitivity seq ( self , signals : List [ Rtl Signal Base ] , seen : set , ctx : Sensitivity Ctx ) -> None : casual Sensitivity = set ( ) for s in signals : s . walk sensitivity ( casual Sensitivity , seen , ctx ) if ctx . contains ev dependency : break if not ctx . contains ev dependency : ctx . extend ( casual Sensitivity )", "predictions": ["compute edge - specific edge - ngrams - ngrams - ngrams - dependency - dependency ."], "references": ["discover sensitivity for list of signals"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 2407, "code": "def get rtl context ( self ) : for sig in chain ( self . inputs , self . outputs ) : if sig . ctx : return sig . ctx else : continue raise Hwt Syntax Error ( \"Statement does not have any signal in any context\" , self )", "predictions": ["returns the pipe context context"], "references": ["get rtlnetlist context from signals"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 2408, "code": "def is mergable statement list ( cls , stms A , stms B ) : if stms A is None and stms B is None : return True elif stms A is None or stms B is None : return False a it = iter ( stms A ) b it = iter ( stms B ) a = get stm with branches ( a it ) b = get stm with branches ( b it ) while a is not None or b is not None : if a is None or b is None or not a . is mergable ( b ) : return False a = get stm with branches ( a it ) b = get stm with branches ( b it ) return True", "predictions": ["check if a word word word is fuzzy"], "references": ["walk statements and compare if they can be merged into one statement list"], "bleu": 0.08583768591139128, "rouge_l": 0.09131736526946108}
{"id": 2409, "code": "def try reduce list ( statements : List [ \"Hdl Statement\" ] ) : io change = False new statements = [ ] for stm in statements : reduced , io change = stm . try reduce ( ) new statements . extend ( reduced ) io change |= io change new statements , rank decrease = Hdl Statement . merge statements ( new statements ) return new statements , rank decrease , io change", "predictions": ["do a list but replace all statements with a print statements else else print the same statements"], "references": ["simplify statements in the list"], "bleu": 0.0859076483566362, "rouge_l": 0.20165289256198346}
{"id": 2410, "code": "def set parent stm ( self , parent Stm : \"Hdl Statement\" ) : was top = self . parent Stm is None self . parent Stm = parent Stm if not self . now is event dependent and parent Stm . now is event dependent : self . on parent event dependent ( ) top Statement = parent Stm while top Statement . parent Stm is not None : top Statement = top Statement . parent Stm parent out add = top Statement . outputs . append parent in add = top Statement . inputs . append if was top : for inp in self . inputs : inp . endpoints . discard ( self ) inp . endpoints . append ( top Statement ) parent in add ( inp ) for outp in self . outputs : outp . drivers . discard ( self ) outp . drivers . append ( top Statement ) parent out add ( outp ) ctx = self . get rtl context ( ) ctx . statements . discard ( self ) parent Stm . rank += self . rank", "predictions": ["do not use directly directly do not create a parent"], "references": ["assign parent statement and propagate dependency flags if necessary"], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 2411, "code": "def sig ( self , name , dtype = BIT , def Val = None ) : if isinstance ( dtype , H Struct ) : if def Val is not None : raise Not Implemented Error ( ) container = dtype . from Py ( None ) for f in dtype . fields : if f . name is not None : r = self . sig ( \"%s %s\" % ( name , f . name ) , f . dtype ) setattr ( container , f . name , r ) return container return self . ctx . sig ( name , dtype = dtype , def Val = def Val )", "predictions": ["returns a container instance corresponding to the given stream ."], "references": ["create signal in this unit"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2412, "code": "def clean As Subunit ( self ) : for pi in self . entity . ports : pi . connect Intern Sig ( ) for i in chain ( self . interfaces , self . private interfaces ) : i . clean ( )", "predictions": ["clean all width of the entity"], "references": ["disconnect internal signals so unit can be reused by parent unit"], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 2413, "code": "def walk Flatten Fields ( sig Or Val , skip Padding = True ) : t = sig Or Val . dtype if isinstance ( t , Bits ) : yield sig Or Val elif isinstance ( t , H Union ) : yield from walk Flatten Fields ( sig Or Val . val , skip Padding = skip Padding ) elif isinstance ( t , H Struct ) : for f in t . fields : is Padding = f . name is None if not is Padding or not skip Padding : if is Padding : v = f . dtype . from Py ( None ) else : v = getattr ( sig Or Val , f . name ) yield from walk Flatten Fields ( v ) elif isinstance ( t , H Array ) : for item in sig Or Val : yield from walk Flatten Fields ( item ) else : raise Not Implemented Error ( t )", "predictions": ["slice all the names in the list of to the type of the type"], "references": ["walk all simple values in hstruct or harray"], "bleu": 0.09782375748961449, "rouge_l": 0.19122257053291536}
{"id": 2414, "code": "def sensitivity ( proc : HW Process , * sensitive To ) : for s in sensitive To : if isinstance ( s , tuple ) : sen , s = s if sen == SENSITIVITY . ANY : s . sim Sens Procs . add ( proc ) elif sen == SENSITIVITY . RISING : s . sim Rising Sens Procs . add ( proc ) elif sen == SENSITIVITY . FALLING : s . sim Falling Sens Procs . add ( proc ) else : raise Assertion Error ( sen ) else : s . sim Sens Procs . add ( proc )", "predictions": ["find all of the given utf - 8 + tuple + tuple + long + tuple + ."], "references": ["register sensitivity for process"], "bleu": 0.057259987315337726, "rouge_l": 0.0}
{"id": 2415, "code": "def sim Eval Cond ( simulator , * conds ) : cond = True vld = True for v in conds : val = bool ( v . val ) full Vld = v . vld Mask == 1 if full Vld : if not val : return False , True else : return False , False cond = cond and val vld = vld and full Vld return cond , vld", "predictions": ["returns true if any of the column is a boolean value"], "references": ["evaluate list of values as condition"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 2416, "code": "def connect Sim Port ( sim Unit , sub Sim Unit , src Name , dst Name , direction ) : if direction == DIRECTION . OUT : orig Port = getattr ( sub Sim Unit , src Name ) new Port = getattr ( sim Unit , dst Name ) setattr ( sub Sim Unit , src Name , new Port ) else : orig Port = getattr ( sub Sim Unit , dst Name ) new Port = getattr ( sim Unit , src Name ) setattr ( sub Sim Unit , dst Name , new Port ) sub Sim Unit . ctx . signals . remove ( orig Port )", "predictions": ["connect a statements to a specific statements"], "references": ["connect ports of simulation models by name"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 2417, "code": "def vec ( val , width , signed = None ) : return Bits ( width , signed , force Vector = True ) . from Py ( val )", "predictions": ["convert an iterable to a text string . . . . . . . ."], "references": ["create hdl vector value"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 2418, "code": "def monitor ( self , sim ) : r = sim . read if self . not Reset ( sim ) : if self . last Rd is not 1 : self . wr Rd ( sim . write , 1 ) self . last Rd = 1 try : on Monitor Ready = self . on Monitor Ready except Attribute Error : on Monitor Ready = None if on Monitor Ready is not None : on Monitor Ready ( sim ) yield sim . wait On Comb Update ( ) vld = self . is Vld ( r ) assert vld . vld Mask , ( sim . now , self . intf , \"vld signal is in invalid state\" ) if vld . val : d = self . do Read ( sim ) if self . debug Output is not None : self . debug Output . write ( \"%s, read, %d: %r\\n\" % ( self . intf . get Full Name ( ) , sim . now , d ) ) self . data . append ( d ) if self . after Read is not None : self . after Read ( sim ) else : if self . last Rd is not 0 : self . wr Rd ( sim . write , 0 ) self . last Rd = 0", "predictions": ["vcd main loop ."], "references": ["collect data from interface"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 2419, "code": "def HW Process ( cls , proc : HW Process , ctx : Resource Context ) -> None : seen = ctx . seen for stm in proc . statements : encl = stm . enclosed for full ev dep = stm . is completly event dependent now ev dep = stm . now is event dependent ev dep = full ev dep or now ev dep out mux dim = count mux inputs for outputs ( stm ) for o in stm . outputs : if o in seen : continue i = out mux dim [ o ] if isinstance ( o . dtype , H Array ) : assert i == 1 , ( o , i , \" only one ram port per HW Process\" ) for a in walk assignments ( stm , o ) : assert len ( a . indexes ) == 1 , \"one address per RAM port\" addr = a . indexes [ 0 ] ctx . register RAM write port ( o , addr , ev dep ) elif ev dep : ctx . register FF ( o ) if i > 1 : ctx . register MUX ( stm , o , i ) elif o not in encl : ctx . register Latch ( o ) if i > 1 : ctx . register MUX ( stm , o , i ) elif i > 1 : ctx . register MUX ( stm , o , i ) else : continue if isinstance ( stm , Switch Container ) : case Eqs = set ( [ stm . switch On . eq ( c [ 0 ] ) for c in stm . cases ] ) inputs = chain ( [ sig for sig in stm . inputs if sig not in case Eqs ] , [ stm . switch On ] ) else : inputs = stm . inputs for i in inputs : if not i . hidden or i in seen : continue cls . HW Process operators ( i , ctx , ev dep )", "predictions": ["process the ram of the ram event ."], "references": ["gues resource usage by hwprocess"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2420, "code": "def eval Param ( p ) : while isinstance ( p , Param ) : p = p . get ( ) if isinstance ( p , Rtl Signal Base ) : return p . static Eval ( ) return to H Val ( p )", "predictions": ["evaluate evaluate a static instance ."], "references": ["get value of parameter"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2421, "code": "def set ( self , val ) : assert not self . is Read Only , ( \"This parameter(%s) was locked\" \" and now it can not be changed\" % self . name ) assert self . replaced With is None , ( \"This param was replaced with new one and this \" \"should not exists\" ) val = to H Val ( val ) self . def Val = val self . val = val . static Eval ( ) self . dtype = self . val . dtype", "predictions": ["distinct method for setting a value in the dictionary = value = value = value = value = value = value = value = value = value = value = value"], "references": ["set value of this param"], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 2422, "code": "def finalize ( self ) : ff to remove = 0 res = self . resources for m , addr Dict in self . memories . items ( ) : rw Sync Ports , r Sync Ports , w Sync Ports = 0 , 0 , 0 rw Async Ports , r Async Ports , w Async Ports = 0 , 0 , 0 r Sync w Async Ports , r Async w Sync Ports = 0 , 0 for , ( r Sync , w Sync , r Async , w Async ) in addr Dict . items ( ) : if r Sync : ff to remove += r Sync * m . dtype . elm Type . bit length ( ) rw Sync = min ( r Sync , w Sync ) r Sync -= rw Sync w Sync -= rw Sync rw Async = min ( r Async , w Async ) r Async -= rw Async w Async -= rw Async r Sync w Async = min ( r Sync , w Async ) r Sync -= r Sync w Async w Async -= r Sync w Async r Async w Sync = min ( r Async , w Sync ) r Async -= r Async w Sync w Sync -= r Async w Sync rw Sync Ports += rw Sync r Sync Ports += r Sync w Sync Ports += w Sync rw Async Ports += rw Async r Async Ports += r Async w Async Ports += w Async r Sync w Async Ports += r Sync w Async r Async w Sync Ports += r Async w Sync k = Resource RAM ( m . dtype . elm Type . bit length ( ) , int ( m . dtype . size ) , rw Sync Ports , r Sync Ports , w Sync Ports , r Sync w Async Ports , rw Async Ports , r Async Ports , w Async Ports , r Async w Sync Ports ) res [ k ] = res . get ( k , 0 ) + 1 self . memories . clear ( ) if ff to remove : ff cnt = res [ Resource FF ] ff cnt -= ff to remove if ff cnt : res [ Resource FF ] = ff cnt else : del res [ Resource FF ]", "predictions": ["to remove all . . . . . ."], "references": ["resolve ports of discovered memories"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2423, "code": "def eq ( self , other ) : return self . nary Op ( All Ops . EQ , tv ( self ) . eq , other )", "predictions": ["compare two nodes of the same type ."], "references": ["__eq__ is not overloaded because it will destroy hashability of object"], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 2424, "code": "def get Index Cascade ( self ) : try : d = self . single Driver ( ) try : op = d . operator except Attribute Error : return if op == All Ops . INDEX : indexed On = d . operands [ 0 ] if isinstance ( indexed On , Rtl Signal Base ) : return indexed On , [ d . operands [ 1 ] ] else : raise Exception ( \"can not drive static value %r\" % indexed On ) except ( Multiple Drivers Err , No Driver Err ) : pass", "predictions": ["name of static . . . ."], "references": ["find out if this signal is something indexed"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 2425, "code": "def walk Params ( intf , discovered ) : for si in intf . interfaces : yield from walk Params ( si , discovered ) for p in intf . params : if p not in discovered : discovered . add ( p ) yield p", "predictions": ["cut all = = context separated by this context separated by the context separated by this instance separated ."], "references": ["walk parameter instances on this interface"], "bleu": 0.06439931429457924, "rouge_l": 0.0882778581765557}
{"id": 2426, "code": "def register Intf In Impl ( self , i Name , intf ) : self . register Interface ( i Name , intf , is Private = True ) self . load Interface ( intf , False ) intf . signals For Interface ( self . ctx )", "predictions": ["synthesize a intf object ."], "references": ["register interface in implementation phase"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2427, "code": "def get Base Name Scope ( cls ) : s = Name Scope ( False ) s . set Level ( 1 ) s [ 0 ] . update ( cls . keywords dict ) return s", "predictions": ["returns the class of the class 0 if not found 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"], "references": ["get root of name space"], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 2428, "code": "def get Base Cond ( c ) : is Negated = False try : drivers = c . drivers except Attribute Error : return ( c , is Negated ) if len ( drivers ) == 1 : d = list ( c . drivers ) [ 0 ] if isinstance ( d , Operator ) and d . operator == All Ops . NOT : c = d . operands [ 0 ] is Negated = True return ( c , is Negated )", "predictions": ["yield boolean value of sim"], "references": ["if is negated return original cond and negated flag"], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 2429, "code": "def sim Bits T ( width : int , signed : Union [ bool , None ] ) : k = ( width , signed ) try : return sim Bits T Cache [ k ] except Key Error : t = Sim Bits T ( width , signed ) sim Bits T Cache [ k ] = t return t", "predictions": ["return is a column name"], "references": ["construct simbitst with cache"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2430, "code": "def cut off drivers of ( self , sig : Rtl Signal Base ) : if self . dst is sig : self . parent Stm = None return self else : return None", "predictions": ["return off off off"], "references": ["cut off statements which are driver of specified signal"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 2431, "code": "def load From H Type ( self , dtype : Hdl Type , bit Addr : int ) -> None : self . bit Addr = bit Addr children Are Choice = False if isinstance ( dtype , Bits ) : ld = self . load From Bits elif isinstance ( dtype , H Struct ) : ld = self . load From H Struct elif isinstance ( dtype , H Array ) : ld = self . load From Array elif isinstance ( dtype , H Stream ) : ld = self . load From H Stream elif isinstance ( dtype , H Union ) : ld = self . load From Union children Are Choice = True else : raise Type Error ( \"expected instance of Hdl Type\" , dtype ) self . bit Addr End = ld ( dtype , bit Addr ) self . children Are Choice = children Are Choice", "predictions": ["get the values from the passed in the form ."], "references": ["parse any hdl type to this transaction template instance"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2432, "code": "def sign Fix ( val , width ) : if val > 0 : msb = 1 << ( width - 1 ) if val & msb : val -= mask ( width ) + 1 return val", "predictions": ["sign a string with an optional width"], "references": ["convert negative int to positive int which has same bits set"], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 2433, "code": "def merge with other stm ( self , other : \"If Container\" ) -> None : merge = self . merge statement lists new Cases = [ ] for ( c , case A ) , ( , case B ) in zip ( self . cases , other . cases ) : new Cases . append ( ( c , merge ( case A , case B ) ) ) self . cases = new Cases if self . default is not None : self . default = merge ( self . default , other . default ) self . on merge ( other )", "predictions": ["merge two stm cases with other stm ."], "references": ["merge other statement to this statement"], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 2434, "code": "def get Indent ( indent Num ) : try : return indent Cache [ indent Num ] except Key Error : i = \"\" . join ( [ indent for in range ( indent Num ) ] ) indent Cache [ indent Num ] = i return i", "predictions": ["return the string representation of the indent"], "references": ["cached indent getter function"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 2435, "code": "def verilog Type Of Sig ( signal Item ) : driver cnt = len ( signal Item . drivers ) if signal Item . const or driver cnt > 1 or arr any ( signal Item . drivers , is Event Dependent Driver ) : return SIGNAL TYPE . REG else : if driver cnt == 1 : d = signal Item . drivers [ 0 ] if not isinstance ( d , ( Assignment , Port Item ) ) : return SIGNAL TYPE . REG return SIGNAL TYPE . WIRE", "predictions": ["returns the total number of signal type that the signal is in the signal ."], "references": ["check if is register or wire"], "bleu": 0.08225964699966554, "rouge_l": 0.10321489001692045}
{"id": 2436, "code": "def name Availability Check ( obj , prop Name , prop ) : if getattr ( obj , prop Name , None ) is not None : raise Intf Lvl Conf Err ( \"%r already has property %s old:%s new:%s\" % ( obj , prop Name , repr ( getattr ( obj , prop Name ) ) , prop ) )", "predictions": ["check if an object is availability or not ."], "references": ["check if not redefining property on obj"], "bleu": 0.19960198807747329, "rouge_l": 0.38364779874213834}
{"id": 2437, "code": "def register Parameter ( self , p Name , parameter ) -> None : name Availability Check ( self , p Name , parameter ) try : has Name = parameter . name is not None except Attribute Error : has Name = False if not has Name : parameter . name = p Name parameter . register Scope ( p Name , self ) if parameter . has Generic Name : parameter . name = p Name if parameter . parent is None : parameter . parent = self self . params . append ( parameter )", "predictions": ["register a parameter with the given name ."], "references": ["register param object on interface level object"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 2438, "code": "def register Unit ( self , u Name , unit ) : name Availability Check ( self , u Name , unit ) assert unit . parent is None unit . parent = self unit . name = u Name self . units . append ( unit )", "predictions": ["register a new unit to the registry ."], "references": ["register unit object on interface level object"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 2439, "code": "def register Interface ( self , i Name , intf , is Private = False ) : name Availability Check ( self , i Name , intf ) assert intf . parent is None intf . parent = self intf . name = i Name intf . ctx = self . ctx if is Private : self . private interfaces . append ( intf ) intf . is Extern = False else : self . interfaces . append ( intf ) intf . is Extern = True", "predictions": ["register a private name to be called by private interfaces ."], "references": ["register interface object on interface level object"], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 2440, "code": "def register Array ( self , name , items ) : items . parent = self items . name = name for i , item in enumerate ( items ) : setattr ( self , \"%s %d\" % ( name , i ) , item )", "predictions": ["register a new items to the registry ."], "references": ["register array of items on interface level object"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 2441, "code": "def single Driver ( self ) : drv cnt = len ( self . drivers ) if not drv cnt : raise No Driver Err ( self ) elif drv cnt != 1 : raise Multiple Drivers Err ( self ) return self . drivers [ 0 ]", "predictions": ["return the single single - single value of the single context ."], "references": ["returns a first driver if signal has only one driver ."], "bleu": 0.10390302174233558, "rouge_l": 0.08764367816091953}
{"id": 2442, "code": "def static Eval ( self ) : for o in self . operands : o . static Eval ( ) self . result . val = self . eval Fn ( )", "predictions": ["static method that moves all operands in the result ."], "references": ["recursively statistically evaluate result of this operator"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 2443, "code": "def with Indent ( self , indent = 1 ) : ctx = copy ( self ) ctx . indent += indent return ctx", "predictions": ["return a copy of the element with indent ."], "references": ["create copy of this context with increased indent"], "bleu": 0.21105340631872635, "rouge_l": 0.4756335282651072}
{"id": 2444, "code": "def propagate Clk ( obj ) : clk = obj . clk for u in obj . units : try Connect ( clk , u , 'clk' )", "predictions": ["propagate units to units ."], "references": ["propagate clk clock signal to all subcomponents"], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 2445, "code": "def propagate Clk Rst ( obj ) : clk = obj . clk rst = obj . rst for u in obj . units : try Connect ( clk , u , 'clk' ) try Connect ( ~ rst , u , 'rst n' ) try Connect ( rst , u , 'rst' )", "predictions": ["propagate clk to clk ."], "references": ["propagate clk clock and reset rst signal to all subcomponents"], "bleu": 0.14203729394569906, "rouge_l": 0.37731958762886597}
{"id": 2446, "code": "def get Full Name ( self ) : name = \"\" tmp = self while isinstance ( tmp , ( Interface Base , H Obj List ) ) : if hasattr ( tmp , \" name\" ) : n = tmp . name else : n = '' if name == '' : name = n else : name = n + '.' + name if hasattr ( tmp , \" parent\" ) : tmp = tmp . parent else : tmp = None return name", "predictions": ["returns the name of the class"], "references": ["get all name hierarchy separated by ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 2447, "code": "def on T Write Callback init ( self , sim ) : yield from self . on T Write Callback ( sim ) self . intf . t . sig Inside . register Write Callback ( self . on T Write Callback , self . get Enable ) self . intf . o . sig Inside . register Write Callback ( self . on T Write Callback , self . get Enable )", "predictions": ["called when the websocket is not cjdns ."], "references": ["process for injecting of this callback loop into simulator"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 2448, "code": "def connect Sig ( self , signal ) : if self . direction == DIRECTION . IN : if self . src is not None : raise Hwt Syntax Error ( \"Port %s is already associated with %r\" % ( self . name , self . src ) ) self . src = signal signal . endpoints . append ( self ) elif self . direction == DIRECTION . OUT : if self . dst is not None : raise Hwt Syntax Error ( \"Port %s is already associated with %r\" % ( self . name , self . dst ) ) self . dst = signal signal . drivers . append ( self ) else : raise Not Implemented Error ( self ) signal . hidden = False signal . ctx . sub Units . add ( self . unit )", "predictions": ["connect to the given signal ."], "references": ["connect to port item on subunit"], "bleu": 0.2907153684841096, "rouge_l": 0.3333333333333333}
{"id": 2449, "code": "def connect Intern Sig ( self ) : d = self . direction if d == DIRECTION . OUT : self . src . endpoints . append ( self ) elif d == DIRECTION . IN or d == DIRECTION . INOUT : self . dst . drivers . append ( self ) else : raise Not Implemented Error ( d )", "predictions": ["connect to the intern ."], "references": ["connet signal from internal side of of this component to this port"], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 2450, "code": "def get Intern Sig ( self ) : d = self . direction if d == DIRECTION . IN : return self . dst elif d == DIRECTION . OUT : return self . src else : raise Not Implemented Error ( d )", "predictions": ["return the intern or none if not found ."], "references": ["return signal inside unit which has this port"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 2451, "code": "def is Ev Dependent On ( sig , process ) -> bool : if sig is None : return False return process in sig . sim Falling Sens Procs or process in sig . sim Rising Sens Procs", "predictions": ["check if the process is a ev process ."], "references": ["check if hdl process has event depenency on signal"], "bleu": 0.19960198807747329, "rouge_l": 0.3333333333333333}
{"id": 2452, "code": "def add process ( self , proc , priority ) -> None : self . events . push ( self . now , priority , proc )", "predictions": ["add a process to the event ."], "references": ["schedule process on actual time with specified priority"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 2453, "code": "def schedule Apply Values ( self ) -> None : assert not self . apply Val Planed , self . now self . add process ( self . apply Values ( ) , PRIORITY APPLY COMB ) self . apply Val Planed = True if self . run Seq Processes Planed : return assert not self . seq Procs To Run and not self . run Seq Processes Planed , self . now self . add process ( self . run Seq Processes ( ) , PRIORITY APPLY SEQ ) self . run Seq Processes Planed = True", "predictions": ["schedule the schedule for the schedule ."], "references": ["apply stashed values to signals"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2454, "code": "def run Comb Processes ( self ) -> None : for proc in self . comb Procs To Run : cont = self . output Containers [ proc ] proc ( self , cont ) for sig Name , sig in cont . all signals : new Val = getattr ( cont , sig Name ) if new Val is not None : res = self . conflict Resolve Strategy ( new Val ) updater , is Ev Dependent = res self . values To Apply . append ( ( sig , updater , is Ev Dependent , proc ) ) setattr ( cont , sig Name , None ) self . comb Procs To Run = Uniq List ( )", "predictions": ["runs the conflict ."], "references": ["delta step for combinational processes"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 2455, "code": "def run Seq Processes ( self ) -> Generator [ None , None , None ] : updates = [ ] for proc in self . seq Procs To Run : try : out Container = self . output Containers [ proc ] except Key Error : out Container = None proc ( self , out Container ) if out Container is not None : updates . append ( out Container ) self . seq Procs To Run = Uniq List ( ) self . run Seq Processes Planed = False for cont in updates : for sig Name , sig in cont . all signals : new Val = getattr ( cont , sig Name ) if new Val is not None : v = self . conflict Resolve Strategy ( new Val ) updater , = v sig . sim Update Val ( self , updater ) setattr ( cont , sig Name , None ) return yield", "predictions": ["run the conflict ."], "references": ["delta step for event dependent processes"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 2456, "code": "def apply Values ( self ) -> Generator [ None , None , None ] : va = self . values To Apply self . apply Val Planed = False lav = self . config . log Applying Values if va and lav : lav ( self , va ) self . values To Apply = [ ] add Sp = self . seq Procs To Run . append for s , v Updater , is Event Dependent , comes From in va : if is Event Dependent : add Sp ( comes From ) else : s . sim Update Val ( self , v Updater ) self . run Comb Processes ( ) if self . values To Apply and not self . apply Val Planed : self . schedule Apply Values ( ) return yield", "predictions": ["apply the changes to the current configuration"], "references": ["perform delta step by writing stacked values to signals"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 2457, "code": "def read ( self , sig ) -> Value : try : v = sig . val except Attribute Error : v = sig . sig Inside . val return v . clone ( )", "predictions": ["read value from the stream"], "references": ["read value from signal or interface"], "bleu": 0.41602390756021224, "rouge_l": 0.5366568914956013}
{"id": 2458, "code": "def write ( self , val , sig : Sim Signal ) -> None : try : sim Sens Procs = sig . sim Sens Procs except Attribute Error : sig = sig . sig Inside sim Sens Procs = sig . sim Sens Procs t = sig . dtype if isinstance ( val , Value ) : v = val . clone ( ) v = v . auto cast ( t ) else : v = t . from Py ( val ) sig . sim Update Val ( self , lambda curent V : ( value Has Changed ( curent V , v ) , v ) ) if not self . apply Val Planed : if not ( sim Sens Procs or sig . sim Rising Sens Procs or sig . sim Falling Sens Procs ) : self . schedule Apply Values ( ) elif ( sig . write Callbacks or sig . write Callbacks To En ) : self . schedule Apply Values ( )", "predictions": ["write a frame to the underlying value ."], "references": ["write value to signal or interface ."], "bleu": 0.20164945583740668, "rouge_l": 0.4048672566371681}
{"id": 2459, "code": "def add process ( self , proc ) -> None : self . events . push ( self . now , PRIORITY NORMAL , proc )", "predictions": ["add a process to the event ."], "references": ["add process to events with default priority on current time"], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 2460, "code": "def sim Unit ( self , synthesised Unit : Unit , until : float , extra Processes = [ ] ) : before Sim = self . config . before Sim if before Sim is not None : before Sim ( self , synthesised Unit ) add proc = self . add process for p in extra Processes : add proc ( p ( self ) ) self . init Unit Signals ( synthesised Unit ) self . run ( until )", "predictions": ["start up the sim process ."], "references": ["run simulation for unit instance"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2461, "code": "def system C Type Of Sig ( signal Item ) : if signal Item . const or arr any ( signal Item . drivers , lambda d : isinstance ( d , Hdl Statement ) and d . now is event dependent ) : return SIGNAL TYPE . REG else : return SIGNAL TYPE . WIRE", "predictions": ["returns the system system system type"], "references": ["check if is register or wire"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2462, "code": "def ternary Ops To If ( statements ) : stms = [ ] for st in statements : if isinstance ( st , Assignment ) : try : if not isinstance ( st . src , Rtl Signal Base ) : raise Does Not Contains Ternary ( ) d = st . src . single Driver ( ) if not isinstance ( d , Operator ) or d . operator != All Ops . TERNARY : raise Does Not Contains Ternary ( ) else : ops = d . operands ifc = If Container ( ops [ 0 ] , [ Assignment ( ops [ 1 ] , st . dst ) ] , [ Assignment ( ops [ 2 ] , st . dst ) ] ) stms . append ( ifc ) continue except ( Multiple Drivers Err , Does Not Contains Ternary ) : pass except No Driver Err : assert ( hasattr ( st . src , \" interface\" ) and st . src . interface is not None ) or st . src . def Val . vld Mask , st . src stms . append ( st ) return stms", "predictions": ["converts a list of statements into a list of statements ."], "references": ["convert all ternary operators to ifcontainers"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2463, "code": "def hash distance ( left hash , right hash ) : if len ( left hash ) != len ( right hash ) : raise Value Error ( 'Hamming distance requires two strings of equal length' ) return sum ( map ( lambda x : 0 if x [ 0 ] == x [ 1 ] else 1 , zip ( left hash , right hash ) ) )", "predictions": ["hash a right hash based on the right hash"], "references": ["compute the hamming distance between two hashes"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 2464, "code": "def average hash ( image path , hash size = 8 ) : with open ( image path , 'rb' ) as f : image = Image . open ( f ) . resize ( ( hash size , hash size ) , Image . ANTIALIAS ) . convert ( 'L' ) pixels = list ( image . getdata ( ) ) avg = sum ( pixels ) / len ( pixels ) bits = \"\" . join ( map ( lambda pixel : '1' if pixel > avg else '0' , pixels ) ) hashformat = \"0{hashlength}x\" . format ( hashlength = hash size ** 2 // 4 ) return int ( bits , 2 ) . format ( hashformat )", "predictions": ["returns the sign hash hash"], "references": ["compute the average hash of the given image ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 2465, "code": "def distance ( image path , other image path ) : image hash = average hash ( image path ) other image hash = average hash ( other image path ) return hash distance ( image hash , other image hash )", "predictions": ["merge two other other other files ."], "references": ["compute the hamming distance between two images"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 2466, "code": "def setup platform ( hass , config , add entities , discovery info = None ) : host = config . get ( CONF HOST ) token = config . get ( CONF ACCESS TOKEN ) name = config . get ( CONF NAME ) volume step = config . get ( CONF VOLUME STEP ) device type = config . get ( CONF DEVICE CLASS ) device = Vizio Device ( host , token , name , volume step , device type ) if device . validate setup ( ) is False : LOGGER . error ( \"Failed to set up Vizio platform, \" \"please check if host and API key are correct\" ) return elif ( token is None or token == \"\" ) and device type == \"tv\" : LOGGER . error ( \"Failed to set up Vizio platform, \" \"if device class is 'tv' then an auth token needs \" \"to be provided, otherwise if device class is \" \"'soundbar' then add the right device class to config\" ) return if config . get ( CONF SUPPRESS WARNING ) : from requests . packages import urllib3 LOGGER . warning ( \"Insecure Request Warning is disabled \" \"because of Vizio platform configuration\" ) urllib3 . disable warnings ( urllib3 . exceptions . Insecure Request Warning ) add entities ( [ device ] , True )", "predictions": ["get the platform platform platform i can access to the device"], "references": ["set up the vizio media player platform ."], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 2467, "code": "def update ( self ) : is on = self . device . get power state ( ) if is on : self . state = STATE ON volume = self . device . get current volume ( ) if volume is not None : self . volume level = float ( volume ) / self . max volume input = self . device . get current input ( ) if input is not None : self . current input = input . meta name inputs = self . device . get inputs ( ) if inputs is not None : self . available inputs = [ input . name for input in inputs ] else : if is on is None : self . state = None else : self . state = STATE OFF self . volume level = None self . current input = None self . available inputs = None", "predictions": ["verilog the driver from the driver ."], "references": ["retrieve latest state of the device ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 2468, "code": "def mute volume ( self , mute ) : if mute : self . device . mute on ( ) else : self . device . mute off ( )", "predictions": ["name of the device volume . ."], "references": ["mute the volume ."], "bleu": 0.2626909894424158, "rouge_l": 0.5736677115987461}
{"id": 2469, "code": "def volume up ( self ) : self . volume level += self . volume step / self . max volume self . device . vol up ( num = self . volume step )", "predictions": ["register the register up up . . . . . . . . . ."], "references": ["increasing volume of the device ."], "bleu": 0.09103526405546068, "rouge_l": 0.2064297800338409}
{"id": 2470, "code": "def volume down ( self ) : self . volume level -= self . volume step / self . max volume self . device . vol down ( num = self . volume step )", "predictions": ["register the register down down . . . . . . . . . ."], "references": ["decreasing volume of the device ."], "bleu": 0.09103526405546068, "rouge_l": 0.2064297800338409}
{"id": 2471, "code": "def set volume level ( self , volume ) : if self . volume level is not None : if volume > self . volume level : num = int ( self . max volume * ( volume - self . volume level ) ) self . volume level = volume self . device . vol up ( num = num ) elif volume < self . volume level : num = int ( self . max volume * ( self . volume level - volume ) ) self . volume level = volume self . device . vol down ( num = num )", "predictions": ["register the volume level level"], "references": ["set volume level ."], "bleu": 0.35930411196308426, "rouge_l": 0.4535315985130111}
{"id": 2472, "code": "def reset ( self ) : self . piece bb = [ BB VOID , BB RANK C | BB RANK G , BB A1 | BB I1 | BB A9 | BB I9 , BB A2 | BB A8 | BB I2 | BB I8 , BB A3 | BB A7 | BB I3 | BB I7 , BB A4 | BB A6 | BB I4 | BB I6 , BB B2 | BB H8 , BB B8 | BB H2 , BB A5 | BB I5 , BB VOID , BB VOID , BB VOID , BB VOID , BB VOID , BB VOID , ] self . pieces in hand = [ collections . Counter ( ) , collections . Counter ( ) ] self . occupied = Occupied ( BB RANK G | BB H2 | BB H8 | BB RANK I , BB RANK A | BB B2 | BB B8 | BB RANK C ) self . king squares = [ I5 , A5 ] self . pieces = [ NONE for i in SQUARES ] for i in SQUARES : mask = BB SQUARES [ i ] for piece type in PIECE TYPES : if mask & self . piece bb [ piece type ] : self . pieces [ i ] = piece type self . turn = BLACK self . move number = 1 self . captured piece stack = collections . deque ( ) self . move stack = collections . deque ( ) self . incremental zobrist hash = self . board zobrist hash ( DEFAULT RANDOM ARRAY ) self . transpositions = collections . Counter ( ( self . zobrist hash ( ) , ) )", "predictions": ["resets the values of the zobrist variables to its initial state"], "references": ["restores the starting position ."], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 2473, "code": "def piece at ( self , square ) : mask = BB SQUARES [ square ] color = int ( bool ( self . occupied [ WHITE ] & mask ) ) piece type = self . piece type at ( square ) if piece type : return Piece ( piece type , color )", "predictions": ["return the single single single single single single single single single single single single single single single single single single single single single single single single single single single single single"], "references": ["gets the piece at the given square ."], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 2474, "code": "def remove piece at ( self , square , into hand = False ) : piece type = self . piece type at ( square ) if piece type == NONE : return if into hand : self . add piece into hand ( piece type , self . turn ) mask = BB SQUARES [ square ] self . piece bb [ piece type ] ^= mask color = int ( bool ( self . occupied [ WHITE ] & mask ) ) self . pieces [ square ] = NONE self . occupied . ixor ( mask , color , square ) if color == BLACK : piece index = ( piece type - 1 ) * 2 else : piece index = ( piece type - 1 ) * 2 + 1 self . incremental zobrist hash ^= DEFAULT RANDOM ARRAY [ 81 * piece index + 9 * rank index ( square ) + file index ( square ) ]", "predictions": ["static method for getting piece at"], "references": ["removes a piece from the given square if present ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 2475, "code": "def set piece at ( self , square , piece , from hand = False , into hand = False ) : if from hand : self . remove piece from hand ( piece . piece type , self . turn ) self . remove piece at ( square , into hand ) self . pieces [ square ] = piece . piece type mask = BB SQUARES [ square ] piece type = piece . piece type self . piece bb [ piece type ] |= mask if piece type == KING : self . king squares [ piece . color ] = square self . occupied . ixor ( mask , piece . color , square ) if piece . color == BLACK : piece index = ( piece . piece type - 1 ) * 2 else : piece index = ( piece . piece type - 1 ) * 2 + 1 self . incremental zobrist hash ^= DEFAULT RANDOM ARRAY [ 81 * piece index + 9 * rank index ( square ) + file index ( square ) ]", "predictions": ["with the piece at"], "references": ["sets a piece at the given square . an existing piece is replaced ."], "bleu": 0.041710075933029465, "rouge_l": 0.20198675496688742}
{"id": 2476, "code": "def is checkmate ( self ) : if not self . is check ( ) : return False try : next ( self . generate legal moves ( ) . iter ( ) ) return False except Stop Iteration : return True", "predictions": ["u u returns whether the current state is checkmate or not"], "references": ["checks if the current position is a checkmate ."], "bleu": 0.17033186037639278, "rouge_l": 0.4073455759599332}
{"id": 2477, "code": "def pop ( self ) : move = self . move stack . pop ( ) self . transpositions . subtract ( ( self . zobrist hash ( ) , ) ) self . move number -= 1 captured piece type = self . captured piece stack . pop ( ) captured piece color = self . turn if not move : self . turn ^= 1 return move piece type = self . piece type at ( move . to square ) if move . promotion : piece type = PIECE PROMOTED . index ( piece type ) if move . from square is None : self . add piece into hand ( piece type , self . turn ^ 1 ) else : self . set piece at ( move . from square , Piece ( piece type , self . turn ^ 1 ) ) if captured piece type : self . remove piece from hand ( captured piece type , captured piece color ^ 1 ) self . set piece at ( move . to square , Piece ( captured piece type , captured piece color ) ) else : self . remove piece at ( move . to square ) self . turn ^= 1 return move", "predictions": ["delete the current piece . . ."], "references": ["restores the previous position and returns the last move from the stack ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 2478, "code": "def sfen ( self ) : sfen = [ ] empty = 0 for square in SQUARES : piece = self . piece at ( square ) if not piece : empty += 1 else : if empty : sfen . append ( str ( empty ) ) empty = 0 sfen . append ( piece . symbol ( ) ) if BB SQUARES [ square ] & BB FILE 1 : if empty : sfen . append ( str ( empty ) ) empty = 0 if square != I1 : sfen . append ( '/' ) sfen . append ( ' ' ) if self . turn == WHITE : sfen . append ( 'w' ) else : sfen . append ( 'b' ) sfen . append ( ' ' ) pih len = 0 for color in COLORS : p = self . pieces in hand [ color ] pih len += len ( p ) for piece type in sorted ( p . keys ( ) , reverse = True ) : if p [ piece type ] >= 1 : if p [ piece type ] > 1 : sfen . append ( str ( p [ piece type ] ) ) piece = Piece ( piece type , color ) sfen . append ( piece . symbol ( ) ) if pih len == 0 : sfen . append ( '-' ) sfen . append ( ' ' ) sfen . append ( str ( self . move number ) ) return '' . join ( sfen )", "predictions": ["return the piece ."], "references": ["gets an sfen representation of the current position ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 2479, "code": "def zobrist hash ( self , array = None ) : zobrist hash = self . board zobrist hash ( array ) if array is None : array = DEFAULT RANDOM ARRAY if self . turn == WHITE : zobrist hash ^= array [ 2268 ] i = ( self . pieces in hand [ BLACK ] [ ROOK ] * 35625 + self . pieces in hand [ BLACK ] [ BISHOP ] * 11875 + self . pieces in hand [ BLACK ] [ GOLD ] * 2375 + self . pieces in hand [ BLACK ] [ SILVER ] * 475 + self . pieces in hand [ BLACK ] [ KNIGHT ] * 95 + self . pieces in hand [ BLACK ] [ LANCE ] * 19 + self . pieces in hand [ BLACK ] [ PAWN ] ) bit = bit scan ( i ) while bit != - 1 and bit is not None : zobrist hash ^= array [ 2269 + bit ] bit = bit scan ( i , bit + 1 ) return zobrist hash", "predictions": ["return the hash hash hash hash hash"], "references": ["returns a zobrist hash of the current position ."], "bleu": 0.15447878876032708, "rouge_l": 0.12224448897795591}
{"id": 2480, "code": "def symbol ( self ) : if self . color == BLACK : return PIECE SYMBOLS [ self . piece type ] . upper ( ) else : return PIECE SYMBOLS [ self . piece type ]", "predictions": ["the connect connect to the src ."], "references": ["gets the symbol p l n etc ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 2481, "code": "def load config from cli ( config : Good Conf , argv : List [ str ] ) -> List [ str ] : from django . core . management . base import Base Command original parser = Base Command . create parser def patched parser ( self , prog name , subcommand ) : parser = original parser ( self , prog name , subcommand ) argparser add argument ( parser , config ) return parser Base Command . create parser = patched parser try : parser = argparse . Argument Parser ( add help = False ) argparser add argument ( parser , config ) config arg , default args = parser . parse known args ( argv ) config . load ( config arg . config ) yield default args finally : Base Command . create parser = original parser", "predictions": ["connect to the management"], "references": ["loads config checking cli arguments for a config file"], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 2482, "code": "def execute from command line with config ( config : Good Conf , argv : List [ str ] ) : with load config from cli ( config , argv ) as args : from django . core . management import execute from command line execute from command line ( args )", "predictions": ["cli entry point for the command cli ."], "references": ["load s config then runs django s execute_from_command_line"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 2483, "code": "def argparser add argument ( parser : argparse . Argument Parser , config : Good Conf ) : help = \"Config file.\" if config . file env var : help += ( \" Can also be configured via the \" \"environment variable: {}\" . format ( config . file env var ) ) if config . default files : help += ( \" Defaults to the first file that exists from \" \"[{}].\" . format ( ', ' . join ( config . default files ) ) ) parser . add argument ( '-C' , '--config' , metavar = 'FILE' , help = help )", "predictions": ["add for argparse process process process process process process process process process process process process process process process process process process process process process process process process process process process process"], "references": ["adds argument for config to existing argparser"], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 2484, "code": "def load ( self , filename : str = None ) : if filename : self . config file = find file ( filename ) else : if self . file env var and self . file env var in os . environ : self . config file = find file ( os . environ [ self . file env var ] ) if not self . config file : for filename in self . default files : self . config file = find file ( filename , require = False ) if self . config file : break if self . config file : config = load config ( self . config file ) log . info ( \"Loading config from %s\" , self . config file ) else : config = { } log . info ( \"No config file specified. \" \"Loading with environment variables.\" ) self . set values ( config )", "predictions": ["add the events from the events to the events ."], "references": ["find config file and set values"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 2485, "code": "def generate yaml ( cls , * * override ) : import ruamel . yaml yaml = ruamel . yaml . YAML ( ) yaml str = String IO ( ) yaml . dump ( cls . get initial ( * * override ) , stream = yaml str ) yaml str . seek ( 0 ) dict from yaml = yaml . load ( yaml str ) if cls . doc : dict from yaml . yaml set start comment ( '\\n' + cls . doc + '\\n\\n' ) for k in dict from yaml . keys ( ) : if cls . values [ k ] . help : dict from yaml . yaml set comment before after key ( k , before = '\\n' + cls . values [ k ] . help ) yaml str = String IO ( ) yaml . dump ( dict from yaml , yaml str ) yaml str . seek ( 0 ) return yaml str . read ( )", "predictions": ["schedule a yaml object to a yaml yaml"], "references": ["dumps initial config in yaml"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 2486, "code": "def generate markdown ( cls ) : lines = [ ] if cls . doc : lines . extend ( [ . format ( cls . doc ) , '' ] ) for k , v in cls . values . items ( ) : lines . append ( '* **{}**  ' . format ( k ) ) if v . required : lines [ - 1 ] = lines [ - 1 ] + ' REQUIRED   ' if v . help : lines . append ( '  {}  ' . format ( v . help ) ) lines . append ( '  type: `{}`  ' . format ( v . cast as . name ) ) if v . default is not None : lines . append ( '  default: `{}`  ' . format ( v . default ) ) return '\\n' . join ( lines )", "predictions": ["run the markdown command"], "references": ["documents values in markdown"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 2487, "code": "def cast ( self , val : str ) : try : return getattr ( self , 'cast as {}' . format ( self . cast as . name . lower ( ) ) ) ( val ) except Attribute Error : return self . cast as ( val )", "predictions": ["run the value of the field = value = none if not found"], "references": ["converts string to type requested by cast_as"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 2488, "code": "def list dates between ( first date , last date ) : return [ first date + timedelta ( days = n ) for n in range ( 1 + ( last date - first date ) . days ) ]", "predictions": ["apply dates between between to the last"], "references": ["returns all dates from first to last included ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 2489, "code": "def parse date ( s ) : try : return datetime . date ( int ( s [ : 4 ] ) , int ( s [ 5 : 7 ] ) , int ( s [ 8 : 10 ] ) ) except Value Error : return datetime . datetime . strptime ( s , '%d %B %Y' ) . date ( )", "predictions": ["read a date date from a string"], "references": ["fast %y - %m - %d parsing ."], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 2490, "code": "def load file ( self , currency file ) : if currency file . startswith ( ( 'http://' , 'https://' ) ) : content = urlopen ( currency file ) . read ( ) else : with open ( currency file , 'rb' ) as f : content = f . read ( ) if currency file . endswith ( '.zip' ) : self . load lines ( get lines from zip ( content ) ) else : self . load lines ( content . decode ( 'utf-8' ) . splitlines ( ) )", "predictions": ["write the sig sig file"], "references": ["to be subclassed if alternate methods of loading data ."], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 2491, "code": "def set missing to none ( self , currency ) : rates = self . rates [ currency ] first date , last date = self . bounds [ currency ] for date in list dates between ( first date , last date ) : if date not in rates : rates [ date ] = None if self . verbose : missing = len ( [ r for r in itervalues ( rates ) if r is None ] ) if missing : print ( '{0}: {1} missing rates from {2} to {3} ({4} days)' . format ( currency , missing , first date , last date , 1 + ( last date - first date ) . days ) )", "predictions": ["add process process keys to self push"], "references": ["fill missing rates of a currency with the closest available ones ."], "bleu": 0.07646493705380929, "rouge_l": 0.0}
{"id": 2492, "code": "def read record ( self , n ) : self . file . seek ( n * K - K ) return self . file . read ( K )", "predictions": ["sim n from the extra extra data float"], "references": ["return record n as 1 024 bytes ; records are indexed from 1 ."], "bleu": 0.08383280652235028, "rouge_l": 0.1732954545454545}
{"id": 2493, "code": "def write record ( self , n , data ) : self . file . seek ( n * K - K ) return self . file . write ( data )", "predictions": ["system n record record"], "references": ["write data to file record n ; records are indexed from 1 ."], "bleu": 0.041910459064397936, "rouge_l": 0.1073943661971831}
{"id": 2494, "code": "def comments ( self ) : record numbers = range ( 2 , self . fward ) if not record numbers : return '' data = b'' . join ( self . read record ( n ) [ 0 : 1000 ] for n in record numbers ) try : return data [ : data . find ( b'\\4' ) ] . decode ( 'ascii' ) . replace ( '\\0' , '\\n' ) except Index Error : raise Value Error ( 'DAF file comment area is missing its EOT byte' ) except Unicode Decode Error : raise Value Error ( 'DAF file comment area is not ASCII text' )", "predictions": ["returns the comments comment area area"], "references": ["return the text inside the comment area of the file ."], "bleu": 0.13576587000692578, "rouge_l": 0.33516483516483514}
{"id": 2495, "code": "def close ( self ) : self . daf . file . close ( ) for segment in self . segments : if hasattr ( segment , ' data' ) : del segment . data self . daf . array = None self . daf . map = None", "predictions": ["hash all raise an active len len right len right now right ."], "references": ["close this spk file ."], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 2496, "code": "def describe ( self , verbose = True ) : center = titlecase ( target names . get ( self . center , 'Unknown center' ) ) target = titlecase ( target names . get ( self . target , 'Unknown target' ) ) text = ( '{0.start jd:.2f}..{0.end jd:.2f}  {1} ({0.center})' ' -> {2} ({0.target})' . format ( self , center , target ) ) if verbose : text += ( '\\n  frame={0.frame} data type={0.data type} source={1}' . format ( self , self . source . decode ( 'ascii' ) ) ) return text", "predictions": ["returns the text representation of the source ."], "references": ["return a textual description of the segment ."], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 2497, "code": "def compute ( self , tdb , tdb2 = 0.0 ) : for position in self . generate ( tdb , tdb2 ) : return position", "predictions": ["compute the position of the template ."], "references": ["compute the component values for the time tdb plus tdb2 ."], "bleu": 0.15685718045401453, "rouge_l": 0.4273204903677758}
{"id": 2498, "code": "def close ( self ) : self . daf . file . close ( ) for segment in self . segments : if hasattr ( segment , ' data' ) : del segment . data", "predictions": ["close all segments in the file ."], "references": ["close this file ."], "bleu": 0.2626909894424158, "rouge_l": 0.5736677115987461}
{"id": 2499, "code": "def describe ( self , verbose = True ) : body = titlecase ( target names . get ( self . body , 'Unknown body' ) ) text = ( '{0.start jd:.2f}..{0.end jd:.2f} frame={0.frame}' '  {1} ({0.body})' . format ( self , body ) ) if verbose : text += ( '\\n  data type={0.data type} source={1}' . format ( self , self . source . decode ( 'ascii' ) ) ) return text", "predictions": ["returns the text content of the source ."], "references": ["return a textual description of the segment ."], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 2500, "code": "def load ( self ) : if self . data type == 2 : component count = 3 else : raise Value Error ( 'only binary PCK data type 2 is supported' ) init , intlen , rsize , n = self . daf . read array ( self . end i - 3 , self . end i ) initial epoch = jd ( init ) interval length = intlen / S PER DAY coefficient count = int ( rsize - 2 ) // component count coefficients = self . daf . map array ( self . start i , self . end i - 4 ) coefficients . shape = ( int ( n ) , int ( rsize ) ) coefficients = coefficients [ : , 2 : ] coefficients . shape = ( int ( n ) , component count , coefficient count ) coefficients = rollaxis ( coefficients , 1 ) return initial epoch , interval length , coefficients", "predictions": ["load the next epoch from the data ."], "references": ["map the coefficients into memory using a numpy array ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 2501, "code": "def visit Bin Op ( self , node ) : if self . within logging statement ( ) and self . within logging argument ( ) : if isinstance ( node . op , Mod ) : self . violations . append ( ( node , PERCENT FORMAT VIOLATION ) ) if isinstance ( node . op , Add ) : self . violations . append ( ( node , STRING CONCAT VIOLATION ) ) super ( Logging Visitor , self ) . generic visit ( node )", "predictions": ["check if statement is within the logging statement ."], "references": ["process binary operations while processing the first logging argument ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 2502, "code": "def visit Dict ( self , node ) : if self . should check whitelist ( node ) : for key in node . keys : if key . s in self . whitelist or key . s . startswith ( \"debug \" ) : continue self . violations . append ( ( self . current logging call , WHITELIST VIOLATION . format ( key . s ) ) ) if self . should check extra exception ( node ) : for value in node . values : self . check exception arg ( value ) super ( Logging Visitor , self ) . generic visit ( node )", "predictions": ["triggered when an exception is seen ."], "references": ["process dict arguments ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 2503, "code": "def visit Joined Str ( self , node ) : if version info >= ( 3 , 6 ) : if self . within logging statement ( ) : if any ( isinstance ( i , Formatted Value ) for i in node . values ) : if self . within logging argument ( ) : self . violations . append ( ( node , FSTRING VIOLATION ) ) super ( Logging Visitor , self ) . generic visit ( node )", "predictions": ["check if logging is within the logging statement ."], "references": ["process f - string arguments ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 2504, "code": "def visit keyword ( self , node ) : if self . should check whitelist ( node ) : if node . arg not in self . whitelist and not node . arg . startswith ( \"debug \" ) : self . violations . append ( ( self . current logging call , WHITELIST VIOLATION . format ( node . arg ) ) ) if self . should check extra exception ( node ) : self . check exception arg ( node . value ) super ( Logging Visitor , self ) . generic visit ( node )", "predictions": ["call the keyword and register it with the \"debug ."], "references": ["process keyword arguments ."], "bleu": 0.13950796967929133, "rouge_l": 0.3096446700507614}
{"id": 2505, "code": "def visit Except Handler ( self , node ) : name = self . get except handler name ( node ) if not name : super ( Logging Visitor , self ) . generic visit ( node ) return self . current except names . append ( name ) super ( Logging Visitor , self ) . generic visit ( node ) self . current except names . pop ( )", "predictions": ["visitor for generic ast node ."], "references": ["process except blocks ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 2506, "code": "def detect logging level ( self , node ) : try : if self . get id attr ( node . func . value ) == \"warnings\" : return None if node . func . attr in LOGGING LEVELS : return node . func . attr except Attribute Error : pass return None", "predictions": ["detect the logging level level ."], "references": ["heuristic to decide whether an ast call is a logging call ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 2507, "code": "def get except handler name ( self , node ) : name = node . name if not name : return None if version info < ( 3 , ) : return name . id return name", "predictions": ["get the id of a node ."], "references": ["helper to get the exception name from an excepthandler node in both py2 and py3 ."], "bleu": 0.07678812443288274, "rouge_l": 0.3249001331557923}
{"id": 2508, "code": "def is bare exception ( self , node ) : return isinstance ( node , Name ) and node . id in self . current except names", "predictions": ["return true if node is a bare exception ."], "references": ["checks if the node is a bare exception name from an except block ."], "bleu": 0.31873368445393385, "rouge_l": 0.5857338820301784}
{"id": 2509, "code": "def check exc info ( self , node ) : if self . current logging level not in ( 'error' , 'exception' ) : return for kw in node . keywords : if kw . arg == 'exc info' : if self . current logging level == 'error' : violation = ERROR EXC INFO VIOLATION else : violation = REDUNDANT EXC INFO VIOLATION self . violations . append ( ( node , violation ) )", "predictions": ["check if the logging logging level is valid ."], "references": ["reports a violation if exc_info keyword is used with logging . error or logging . exception ."], "bleu": 0.07636434640327666, "rouge_l": 0.29151732377538825}
{"id": 2510, "code": "def db file widget ( cls ) : def get link display ( url ) : unquoted = unquote ( url . split ( '%2F' ) [ - 1 ] ) if sys . version info . major == 2 : from django . utils . encoding import force unicode unquoted = force unicode ( unquoted ) return escape ( unquoted ) def get template substitution values ( self , value ) : subst = super ( cls , self ) . get template substitution values ( value ) subst [ 'initial' ] = get link display ( value . url ) return subst setattr ( cls , 'get template substitution values' , get template substitution values ) def get context ( self , name , value , attrs ) : context = super ( cls , self ) . get context ( name , value , attrs ) if value and hasattr ( value , 'url' ) : context [ 'widget' ] [ 'display' ] = get link display ( value . url ) return context setattr ( cls , 'get context' , get context ) return cls", "predictions": ["returns the widget widget used in the db"], "references": ["edit the download - link inner text ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 2511, "code": "def render to response ( self , context , * * response kwargs ) : filename = response kwargs . pop ( 'filename' , None ) cmd options = response kwargs . pop ( 'cmd options' , None ) if issubclass ( self . response class , PDF Template Response ) : if filename is None : filename = self . get filename ( ) if cmd options is None : cmd options = self . get cmd options ( ) return super ( PDF Template View , self ) . render to response ( context = context , filename = filename , show content in browser = self . show content in browser , header template = self . header template , footer template = self . footer template , cmd options = cmd options , cover template = self . cover template , * * response kwargs ) else : return super ( PDF Template View , self ) . render to response ( context = context , * * response kwargs )", "predictions": ["render the response to a response ."], "references": ["returns a pdf response with a template rendered with the given context ."], "bleu": 0.09912033646614596, "rouge_l": 0.2846034214618974}
{"id": 2512, "code": "def parse file ( self , file path , currency ) -> List [ Price Model ] : contents = self . load file ( file path ) prices = [ ] for line in contents : price = self . parse line ( line ) assert isinstance ( price , Price Model ) price . currency = currency prices . append ( price ) return prices", "predictions": ["parse a file into a list of all prices prices ."], "references": ["load and parse a . csv file"], "bleu": 0.17033186037639278, "rouge_l": 0.3472485768500949}
{"id": 2513, "code": "def load file ( self , file path ) -> List [ str ] : content = [ ] content = read lines from file ( file path ) return content", "predictions": ["load all lines from a file path"], "references": ["loads the content of the text file"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 2514, "code": "def parse line ( self , line : str ) -> Price Model : line = line . rstrip ( ) parts = line . split ( ',' ) result = Price Model ( ) result . symbol = self . translate symbol ( parts [ 0 ] ) result . value = Decimal ( parts [ 1 ] ) date str = parts [ 2 ] date str = date str . replace ( '\"' , '' ) date parts = date str . split ( '/' ) year str = date parts [ 2 ] month str = date parts [ 1 ] day str = date parts [ 0 ] logging . debug ( f\"parsing {date parts} into date\" ) result . datetime = datetime ( int ( year str ) , int ( month str ) , int ( day str ) ) return result", "predictions": ["parse a line into a datetime"], "references": ["parse a csv line into a price element"], "bleu": 0.35032865403497776, "rouge_l": 0.6963470319634703}
{"id": 2515, "code": "def translate symbol ( self , in symbol : str ) -> str : if not self . symbol maps : self . load symbol maps ( ) result = self . symbol maps [ in symbol ] if in symbol in self . symbol maps else in symbol return result", "predictions": ["translate symbol to symbol ."], "references": ["translate the incoming symbol into locally - used"], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 2516, "code": "def load symbol maps ( self ) : repo = Symbol Map Repository ( self . get session ( ) ) all maps = repo . get all ( ) self . symbol maps = { } for item in all maps : self . symbol maps [ item . in symbol ] = item . out symbol", "predictions": ["load all symbol maps from the repo"], "references": ["loads all symbol maps from db"], "bleu": 0.5169731539571706, "rouge_l": 0.6240409207161125}
{"id": 2517, "code": "def get session ( self ) : if not self . session : self . session = dal . get default session ( ) return self . session", "predictions": ["return the session object"], "references": ["reuses the same db session"], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 2518, "code": "def import csv ( filepath : str , currency : str ) : logger . debug ( f\"currency = {currency}\" ) currency = currency . upper ( ) app = Price Db Application ( ) app . logger = logger app . import prices ( filepath , currency )", "predictions": ["import csv from csv file ."], "references": ["import prices from csv file"], "bleu": 0.4347208719449915, "rouge_l": 0.7393939393939394}
{"id": 2519, "code": "def last ( symbol : str ) : app = Price Db Application ( ) if symbol : symbol = symbol . upper ( ) sec symbol = Security Symbol ( \"\" , \"\" ) sec symbol . parse ( symbol ) latest = app . get latest price ( sec symbol ) assert isinstance ( latest , Price Model ) print ( f\"{latest}\" ) else : latest = app . get latest prices ( ) for price in latest : print ( f\"{price}\" )", "predictions": ["show latest prices for the latest symbol"], "references": ["displays last price for symbol if provided"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 2520, "code": "def download ( ctx , help : bool , symbol : str , namespace : str , agent : str , currency : str ) : if help : click . echo ( ctx . get help ( ) ) ctx . exit ( ) app = Price Db Application ( ) app . logger = logger if currency : currency = currency . strip ( ) currency = currency . upper ( ) app . download prices ( currency = currency , agent = agent , symbol = symbol , namespace = namespace )", "predictions": ["download prices from currency"], "references": ["download the latest prices"], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 2521, "code": "def prune ( symbol : str , all : str ) : app = Price Db Application ( ) app . logger = logger count = 0 if symbol is not None : sec symbol = Security Symbol ( \"\" , \"\" ) sec symbol . parse ( symbol ) deleted = app . prune ( sec symbol ) if deleted : count = 1 else : count = app . prune all ( ) print ( f\"Removed {count} old price entries.\" )", "predictions": ["prune all price in the symbol"], "references": ["delete old prices leaving just the last ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 2522, "code": "def get default session ( ) : from . config import Config , Config Keys db path = Config ( ) . get ( Config Keys . price database ) if not db path : raise Value Error ( \"Price database not set in the configuration file!\" ) return get session ( db path )", "predictions": ["get a default session object"], "references": ["return the default session . the path is read from the default config ."], "bleu": 0.059392570240942286, "rouge_l": 0.19395866454689983}
{"id": 2523, "code": "def add map ( incoming , outgoing ) : db path = Config ( ) . get ( Config Keys . pricedb path ) session = get session ( db path ) new map = Symbol Map ( ) new map . in symbol = incoming new map . out symbol = outgoing session . add ( new map ) session . commit ( ) click . echo ( \"Record saved.\" )", "predictions": ["add a map to the database ."], "references": ["creates a symbol mapping"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 2524, "code": "def list maps ( ) : db path = Config ( ) . get ( Config Keys . price database ) session = get session ( db path ) maps = session . query ( Symbol Map ) . all ( ) for item in maps : click . echo ( item )", "predictions": ["list all available price database ."], "references": ["displays all symbol maps"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 2525, "code": "def get by id ( self , symbol : str ) -> Symbol Map : return self . query . filter ( Symbol Map . in symbol == symbol ) . first ( )", "predictions": ["get an entry by id ."], "references": ["finds the map by in - symbol"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 2526, "code": "def read lines from file ( file path : str ) -> List [ str ] : with open ( file path ) as csv file : content = csv file . readlines ( ) return content", "predictions": ["read lines from a csv file ."], "references": ["read text lines from a file"], "bleu": 0.38260294162784475, "rouge_l": 0.7800511508951408}
{"id": 2527, "code": "def map entity ( self , entity : dal . Price ) -> Price Model : if not entity : return None result = Price Model ( ) result . currency = entity . currency dt string = entity . date format string = \"%Y-%m-%d\" if entity . time : dt string += f\"T{entity.time}\" format string += \"T%H:%M:%S\" price datetime = datetime . strptime ( dt string , format string ) result . datum = Datum ( ) result . datum . from datetime ( price datetime ) assert isinstance ( result . datum , Datum ) #result.namespace = entity.namespace #result.symbol = entity.symbol result . symbol = Security Symbol ( entity . namespace , entity . symbol ) value = Decimal ( entity . value ) / Decimal ( entity . denom ) result . value = Decimal ( value ) return result", "predictions": ["map the entity to the currency"], "references": ["map the price entity"], "bleu": 0.31239399369202553, "rouge_l": 0.6224489795918368}
{"id": 2528, "code": "def map model ( self , model : Price Model ) -> Price : assert isinstance ( model . symbol , Security Symbol ) assert isinstance ( model . datum , Datum ) entity = Price ( ) date iso = f\"{model.datum.value.year}-{model.datum.value.month:02d}-{model.datum.value.day:02d}\" entity . date = date iso entity . time = f\"{model.datum.value.hour:02d}:{model.datum.value.minute:02d}:{model.datum.value.second:02d}\" if model . symbol . namespace : entity . namespace = model . symbol . namespace . upper ( ) entity . symbol = model . symbol . mnemonic . upper ( ) assert isinstance ( model . value , Decimal ) dec places = abs ( model . value . as tuple ( ) . exponent ) entity . denom = 10 ** dec places entity . value = int ( model . value * entity . denom ) entity . currency = model . currency . upper ( ) return entity", "predictions": ["describe the model of the model names to the given model names names names names names names names names names names names names names names names names names names names names"], "references": ["parse into the price entity ready for saving"], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 2529, "code": "def read config ( self , file path : str ) : if not os . path . exists ( file path ) : raise File Not Found Error ( f\"File path not found: {file path}\" ) if not os . path . isfile ( file path ) : self . logger . error ( f\"file not found: {file path}\" ) raise File Not Found Error ( f\"configuration file not found {file path}\" ) self . config . read ( file path )", "predictions": ["compute the configuration file ."], "references": ["read the config file"], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 2530, "code": "def get config template path ( self ) -> str : filename = resource filename ( Requirement . parse ( package name ) , template path + config filename ) return filename", "predictions": ["return the in config self . ."], "references": ["gets the default config path from resources"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 2531, "code": "def create user config ( self ) : src path = self . get config template path ( ) src = os . path . abspath ( src path ) if not os . path . exists ( src ) : message = f\"Config template not found {src}\" self . logger . error ( message ) raise File Not Found Error ( message ) dst = os . path . abspath ( self . get config path ( ) ) shutil . copyfile ( src , dst ) if not os . path . exists ( dst ) : raise File Not Found Error ( \"Config file could not be copied to user dir!\" )", "predictions": ["describe the user self ."], "references": ["copy the config template into user s directory"], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 2532, "code": "def get contents ( self ) -> str : content = None in memory = io . String IO ( \"\" ) self . config . write ( in memory ) in memory . seek ( 0 ) content = in memory . read ( ) in memory . close ( ) return content", "predictions": ["load the contents from the config 3 3 3 3 3 3 3 3 3 3 3 3 3 ."], "references": ["reads the contents of the config file"], "bleu": 0.10108901518669619, "rouge_l": 0.324468085106383}
{"id": 2533, "code": "def set ( self , option : Config Keys , value ) : assert isinstance ( option , Config Keys ) section = SECTION self . config . set ( section , option . name , value ) self . save ( )", "predictions": ["visit an self argument argument argument to the given self argument argument argument argument argument argument argument argument argument argument argument argument argument argument argument argument argument argument argument argument argument"], "references": ["sets a value in config"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2534, "code": "def get ( self , option : Config Keys ) : assert isinstance ( option , Config Keys ) section = SECTION return self . config . get ( section , option . name )", "predictions": ["visit an option from the keys in the keys in the keys in the keys in the keys in the keys in the keys in the keys in the keys in"], "references": ["retrieves a config value"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2535, "code": "def save ( self ) : file path = self . get config path ( ) contents = self . get contents ( ) with open ( file path , mode = 'w' ) as cfg file : cfg file . write ( contents )", "predictions": ["visit the configuration of the config file"], "references": ["save the config file"], "bleu": 0.345720784641941, "rouge_l": 0.5736677115987461}
{"id": 2536, "code": "def parse ( self , symbol : str ) -> ( str , str ) : symbol parts = symbol . split ( \":\" ) namespace = None mnemonic = symbol if len ( symbol parts ) > 1 : namespace = symbol parts [ 0 ] mnemonic = symbol parts [ 1 ] self . namespace = namespace self . mnemonic = mnemonic return namespace , mnemonic", "predictions": ["visit a symbol symbol and returns a tuple of and and and and and and and and and and and and and and and and and and and and and and"], "references": ["splits the symbol into namespace symbol tuple"], "bleu": 0.046398855339878003, "rouge_l": 0.17818889970788704}
{"id": 2537, "code": "def add price ( self , price : Price Model ) : if not price : raise Value Error ( \"Cannot add price. The received model is null!\" ) mapper = mappers . Price Mapper ( ) entity = mapper . map model ( price ) self . add price entity ( entity )", "predictions": ["visit a price price"], "references": ["creates a new price record"], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 2538, "code": "def download price ( self , symbol : str , currency : str , agent : str ) -> Price Model : price = self . download price ( symbol , currency , agent ) self . save ( ) return price", "predictions": ["detect the logging logging logging func"], "references": ["download and save price online"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2539, "code": "def session ( self ) : if not self . session : self . session = dal . get default session ( ) return self . session", "predictions": ["if the get get a get get get it otherwise if it is set ."], "references": ["returns the current db session"], "bleu": 0.08225964699966554, "rouge_l": 0.10990990990990988}
{"id": 2540, "code": "def get prices ( self , date : str , currency : str ) -> List [ Price Model ] : from . repositories import Price Repository session = self . session repo = Price Repository ( session ) query = repo . query if date : query = query . filter ( dal . Price . date == date ) if currency : query = query . filter ( dal . Price . currency == currency ) query = query . order by ( dal . Price . namespace , dal . Price . symbol ) price entities = query . all ( ) mapper = mappers . Price Mapper ( ) result = [ ] for entity in price entities : model = mapper . map entity ( entity ) result . append ( model ) return result", "predictions": ["returns all bare bare bare bare date"], "references": ["fetches all the prices for the given arguments"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 2541, "code": "def get prices on ( self , on date : str , namespace : str , symbol : str ) : repo = self . get price repository ( ) query = ( repo . query . filter ( dal . Price . namespace == namespace ) . filter ( dal . Price . symbol == symbol ) . filter ( dal . Price . date == on date ) . order by ( dal . Price . time . desc ( ) ) ) result = query . first ( ) return result", "predictions": ["return up all exc exc in the given node"], "references": ["returns the latest price on the date"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 2542, "code": "def download price ( self , symbol : str , currency : str , agent : str ) : from finance quote python import Quote assert isinstance ( symbol , str ) assert isinstance ( currency , str ) assert isinstance ( agent , str ) if not symbol : return None #self.logger.info(f\"Downloading {symbol}... \") dl = Quote ( ) dl . logger = self . logger dl . set source ( agent ) dl . set currency ( currency ) result = dl . fetch ( agent , [ symbol ] ) if not result : raise Value Error ( f\"Did not receive a response for {symbol}.\" ) price = result [ 0 ] if not price : raise Value Error ( f\"Price not downloaded/parsed for {symbol}.\" ) else : self . add price ( price ) return price", "predictions": ["db the file to the file"], "references": ["downloads and parses the price"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 2543, "code": "def get securities ( self , currency : str , agent : str , symbol : str , namespace : str ) -> List [ dal . Security ] : repo = self . get security repository ( ) query = repo . query if currency is not None : query = query . filter ( dal . Security . currency == currency ) if agent is not None : query = query . filter ( dal . Security . updater == agent ) if symbol is not None : query = query . filter ( dal . Security . symbol == symbol ) if namespace is not None : query = query . filter ( dal . Security . namespace == namespace ) query = query . order by ( dal . Security . namespace , dal . Security . symbol ) securities = query . all ( ) return securities", "predictions": ["render all to the to the given kwargs"], "references": ["fetches the securities that match the given filters"], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 2544, "code": "def partial ( self ) : ba = self . data [ \"bound args\" ] return state partial ( self . data [ \"func\" ] , * ba . args [ 1 : ] , * * ba . kwargs )", "predictions": ["return of the parse parse ."], "references": ["return partial of original function call"], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 2545, "code": "def update child calls ( self ) : for node in filter ( lambda n : len ( n . arg name ) , self . child list ) : self . data [ \"bound args\" ] . arguments [ node . arg name ] = node . partial ( ) self . updated = True", "predictions": ["load file calls calls calls"], "references": ["replace child nodes on original function call with their partials"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 2546, "code": "def descend ( self , include me = True ) : if include me : yield self for child in self . child list : yield child yield from child . descend ( )", "predictions": ["generator that yields all rstrip of the given rstrip split split into the given list"], "references": ["descend depth first into all child nodes"], "bleu": 0.09103526405546068, "rouge_l": 0.09728867623604465}
{"id": 2547, "code": "def multi dec ( f ) : @ wraps ( f ) def wrapper ( * args , * * kwargs ) : args = ( args [ 0 ] if len ( args ) == 1 and isinstance ( args [ 0 ] , ( list , tuple ) ) else args ) for arg in args : if isinstance ( arg , Node ) and arg . parent . name is \"root\" : arg . parent . remove child ( arg ) arg . update child calls ( ) return f ( * args , * * kwargs ) return wrapper", "predictions": ["remove a function that makes a child calls . . . . . . . ."], "references": ["decorator for multi to remove nodes for original test functions from root node"], "bleu": 0.07692375026049747, "rouge_l": 0.07027649769585254}
{"id": 2548, "code": "def get Result From Process ( res , tempname , process ) : if not isinstance ( res , ( Undefined Value , Exception ) ) : value = get Representation ( tempname , process ) return value , res else : return res , str ( res )", "predictions": ["returns the } s values from the = value"], "references": ["get a value from process return tuple of value res if succesful"], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 2549, "code": "def defined items ( self ) : return self . class ( [ ( k , v ) for k , v in self . items ( ) if v is not self . EMPTY ] , is empty = False )", "predictions": ["if this path is get a list of all session session get none not ."], "references": ["return copy of instance omitting entries that are empty"], "bleu": 0.08225964699966554, "rouge_l": 0.08726752503576538}
{"id": 2550, "code": "def getx ( self , Parser , ext attr , tree ) : cache key = Parser . name + str ( hash ( tree ) ) if self . parser cache . get ( cache key ) : p = self . parser cache [ cache key ] else : p = Parser ( ) if ext attr != \"mappings\" and Parser in [ Function Parser , Object Access Parser , ] : p . mappings = self . context mappings . copy ( ) p . visit ( tree ) self . parser cache [ cache key ] = p return getattr ( p , ext attr )", "predictions": ["return the parser for the given currency = value = 0 = value = 1 = 0 = 1 = value = 1 = 1 = value = 1 = value"], "references": ["getter for parser outputs"], "bleu": 0.04317900023606586, "rouge_l": 0.06637649619151251}
{"id": 2551, "code": "def check part ( state , name , part msg , missing msg = None , expand msg = None ) : if missing msg is None : missing msg = \"Are you sure you defined the {{part}}? \" if expand msg is None : expand msg = \"Did you correctly specify the {{part}}? \" if not part msg : part msg = name append message = { \"msg\" : expand msg , \"kwargs\" : { \"part\" : part msg } } has part ( state , name , missing msg , append message [ \"kwargs\" ] ) stu part = state . student parts [ name ] sol part = state . solution parts [ name ] assert ast ( state , sol part , append message [ \"kwargs\" ] ) return part to child ( stu part , sol part , append message , state )", "predictions": ["last part in state ."], "references": ["return child state with name part as its ast tree"], "bleu": 0.11115018927487523, "rouge_l": 0.12577319587628866}
{"id": 2552, "code": "def detect openmp ( ) : compiler = new compiler ( ) print ( \"Checking for Open MP support... \" ) hasopenmp = hasfunction ( compiler , 'omp get num threads()' ) needs gomp = hasopenmp if not hasopenmp : compiler . add library ( 'gomp' ) hasopenmp = hasfunction ( compiler , 'omp get num threads()' ) needs gomp = hasopenmp if hasopenmp : print ( \"Compiler supports Open MP\" ) else : print ( \"Did not detect Open MP support.\" ) return hasopenmp , needs gomp", "predictions": ["download openmp and return"], "references": ["does this compiler support openmp parallelization?"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 2553, "code": "def get true anomaly ( self ) : self . f = rsky . getf ( self . t supersample , self . t0 , self . per , self . a , self . inc * pi / 180. , self . ecc , self . w * pi / 180. , self . transittype , self . nthreads ) return self . f", "predictions": ["returns the symbol in degrees of the symbol"], "references": ["return the true anomaly at each time"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 2554, "code": "def detect ( ) : compiler = new compiler ( ) hasopenmp = hasfunction ( compiler , 'omp get num threads()' ) needs gomp = hasopenmp if not hasopenmp : compiler . add library ( 'gomp' ) hasopenmp = hasfunction ( compiler , 'omp get num threads()' ) needs gomp = hasopenmp return hasopenmp", "predictions": ["get the current raise exception if the current compiler is enabled if not"], "references": ["does this compiler support openmp parallelization?"], "bleu": 0.09552040806823771, "rouge_l": 0.11275415896487984}
{"id": 2555, "code": "def teardown ( self , exception ) : ctx = stack . top if ctx is not None : if hasattr ( ctx , 'ldap3 manager connections' ) : for connection in ctx . ldap3 manager connections : self . destroy connection ( connection ) if hasattr ( ctx , 'ldap3 manager main connection' ) : log . debug ( \"Unbinding a connection used within the request context.\" ) ctx . ldap3 manager main connection . unbind ( ) ctx . ldap3 manager main connection = None", "predictions": ["add an exception used to the path = 0 = 0 = 0 = 1 = 0 = 1 = 1 = 0 = 0 = 0 = 0 = 1"], "references": ["cleanup after a request . close any open connections ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2556, "code": "def list all ( self , * * kwargs ) : quiet = False if \"quiet\" in kwargs : quiet = kwargs [ 'quiet' ] bot . spinner . start ( ) url = '%s/collections/' % self . base results = self . paginate get ( url ) bot . spinner . stop ( ) if len ( results ) == 0 : bot . info ( \"No container collections found.\" ) sys . exit ( 1 ) rows = [ ] for result in results : if \"containers\" in result : if result [ 'id' ] not in [ 37 , 38 , 39 ] : for c in result [ 'containers' ] : rows . append ( [ c [ 'detail' ] , \"%s:%s\" % ( c [ 'name' ] , c [ 'tag' ] ) ] ) if quiet is False : bot . info ( \"Collections\" ) bot . table ( rows ) return rows", "predictions": ["list all known results for the current container for the given container for the given container for the given container for the given container for the given container for the given"], "references": ["a show all search that doesn t require a query"], "bleu": 0.03901663112717908, "rouge_l": 0.05374449339207048}
{"id": 2557, "code": "def update headers ( self , fields = None ) : do reset = True if hasattr ( self , 'headers' ) : if self . headers is not None : do reset = False if do reset is True : self . reset headers ( ) if fields is not None : for key , value in fields . items ( ) : self . headers [ key ] = value header names = \",\" . join ( list ( self . headers . keys ( ) ) ) bot . debug ( \"Headers found: %s\" % header names )", "predictions": ["set the by bot to the bot filter filter"], "references": ["update headers with a token & other fields"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2558, "code": "def post ( url , data = None , return json = True ) : bot . debug ( \"POST %s\" % url ) return call ( url , headers = headers , func = requests . post , data = data , return json = return json )", "predictions": ["do a read call to an endpoint with the given from the given from the given from the given from the given from the given from the given from the given"], "references": ["post will use requests to get a particular url"], "bleu": 0.04317900023606586, "rouge_l": 0.055505004549590536}
{"id": 2559, "code": "def get ( url , headers = None , token = None , data = None , return json = True ) : bot . debug ( \"GET %s\" % url ) return call ( url , headers = headers , func = requests . get , data = data , return json = return json )", "predictions": ["do a map from url"], "references": ["get will use requests to get a particular url"], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 2560, "code": "def load secrets ( self ) : self . auth = self . get and update setting ( 'GLOBUS AUTH RESPONSE' ) self . transfer = self . get and update setting ( 'GLOBUS TRANSFER RESPONSE' )", "predictions": ["load secrets from the api ."], "references": ["load the secrets credentials file with the globus oauthtokenresponse"], "bleu": 0.1593301391270729, "rouge_l": 0.3860759493670886}
{"id": 2561, "code": "def list logs ( self ) : results = [ ] for image in self . bucket . list blobs ( ) : if image . name . endswith ( 'log' ) : results . append ( image ) if len ( results ) == 0 : bot . info ( \"No containers found, based on extension .log\" ) return results", "predictions": ["list all blobs on the bucket ."], "references": ["return a list of logs . we return any file that ends in . log"], "bleu": 0.06555660318294844, "rouge_l": 0.17062937062937064}
{"id": 2562, "code": "def init transfer client ( self ) : if self . tokens need update ( ) : self . update tokens ( ) access token = self . transfer [ 'access token' ] authorizer = globus sdk . Refresh Token Authorizer ( self . transfer [ 'refresh token' ] , self . client , access token = self . transfer [ 'access token' ] , expires at = self . transfer [ 'expires at seconds' ] ) self . transfer client = globus sdk . Transfer Client ( authorizer = authorizer )", "predictions": ["update transfer client client ."], "references": ["return a transfer client for the user"], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 2563, "code": "def status ( backend ) : print ( '[backend status]' ) settings = read client secrets ( ) print ( 'There are %s clients found in secrets.' % len ( settings ) ) if 'SREGISTRY CLIENT' in settings : print ( 'active: %s' % settings [ 'SREGISTRY CLIENT' ] ) update secrets ( settings ) else : print ( 'There is no active client.' )", "predictions": ["show current status of open open settings"], "references": ["print the status for all or one of the backends ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 2564, "code": "def add ( backend , variable , value , force = False ) : print ( '[add]' ) settings = read client secrets ( ) prefix = 'SREGISTRY %s ' % backend . upper ( ) if not variable . startswith ( prefix ) : variable = '%s%s' % ( prefix , variable ) variable = variable . upper ( ) bot . info ( \"%s %s\" % ( variable , value ) ) if backend in settings : if variable in settings [ backend ] and force is False : previous = settings [ backend ] [ variable ] bot . error ( '%s is already set as %s. Use --force to override.' % ( variable , previous ) ) sys . exit ( 1 ) if backend not in settings : settings [ backend ] = { } settings [ backend ] [ variable ] = value update secrets ( settings )", "predictions": ["add a variable to the settings"], "references": ["add the variable to the config"], "bleu": 0.4347208719449915, "rouge_l": 0.6666666666666666}
{"id": 2565, "code": "def remove ( backend , variable ) : print ( '[remove]' ) settings = read client secrets ( ) prefixed = variable prefix = 'SREGISTRY %s ' % backend . upper ( ) if not variable . startswith ( prefix ) : prefixed = '%s%s' % ( prefix , variable ) variable = variable . upper ( ) bot . info ( variable ) if backend in settings : if variable in settings [ backend ] : del settings [ backend ] [ variable ] if prefixed in settings [ backend ] : del settings [ backend ] [ prefixed ] update secrets ( settings )", "predictions": ["remove variable from settings"], "references": ["remove a variable from the config if found ."], "bleu": 0.14558246978804804, "rouge_l": 0.43160377358490565}
{"id": 2566, "code": "def activate ( backend ) : settings = read client secrets ( ) if backend is not None : settings [ 'SREGISTRY CLIENT' ] = backend update secrets ( settings ) print ( '[activate] %s' % backend )", "predictions": ["activate a backend in the project"], "references": ["activate a backend by adding it to the . sregistry configuration file ."], "bleu": 0.13537348102663535, "rouge_l": 0.39482200647249194}
{"id": 2567, "code": "def delete backend ( backend ) : settings = read client secrets ( ) if backend in settings : del settings [ backend ] if 'SREGISTRY CLIENT' in settings : if settings [ 'SREGISTRY CLIENT' ] == backend : del settings [ 'SREGISTRY CLIENT' ] update secrets ( settings ) print ( '[delete] %s' % backend ) else : if backend is not None : print ( '%s is not a known client.' % backend ) else : print ( 'Please specify a backend to delete.' )", "predictions": ["delete a backend from the dict ."], "references": ["delete a backend and update the secrets file"], "bleu": 0.31689174383082924, "rouge_l": 0.5269978401727862}
{"id": 2568, "code": "def delete ( self , url , headers = None , return json = True , default headers = True ) : bot . debug ( 'DELETE %s' % url ) return self . call ( url , headers = headers , func = requests . delete , return json = return json , default headers = default headers )", "predictions": ["delete a resource from remote url ."], "references": ["delete request use with caution"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 2569, "code": "def head ( self , url ) : bot . debug ( 'HEAD %s' % url ) return self . call ( url , func = requests . head )", "predictions": ["wrapper for the head method ."], "references": ["head request typically used for status code retrieval etc ."], "bleu": 0.13487005099534619, "rouge_l": 0.23921568627450981}
{"id": 2570, "code": "def post ( self , url , headers = None , data = None , return json = True , default headers = True ) : bot . debug ( \"POST %s\" % url ) return self . call ( url , headers = headers , func = requests . post , data = data , return json = return json , default headers = default headers )", "predictions": ["do a post request"], "references": ["post will use requests to get a particular url"], "bleu": 0.11392443929712959, "rouge_l": 0.14386792452830188}
{"id": 2571, "code": "def get ( self , url , headers = None , token = None , data = None , return json = True , default headers = True , quiet = False ) : bot . debug ( \"GET %s\" % url ) return self . call ( url , headers = headers , func = requests . get , data = data , return json = return json , default headers = default headers , quiet = quiet )", "predictions": ["do a get request"], "references": ["get will use requests to get a particular url"], "bleu": 0.11392443929712959, "rouge_l": 0.14386792452830188}
{"id": 2572, "code": "def paginate get ( self , url , headers = None , return json = True , start page = None ) : geturl = '%s&page=1' % ( url ) if start page is not None : geturl = '%s&page=%s' % ( url , start page ) results = [ ] while geturl is not None : result = self . get ( url , headers = headers , return json = return json ) if isinstance ( result , dict ) : if 'results' in result : results = results + result [ 'results' ] geturl = result [ 'next' ] else : return result return results", "predictions": ["get response from url"], "references": ["paginate_call is a wrapper for get to paginate results"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 2573, "code": "def remove ( self , image , force = False ) : q = parse image name ( remove uri ( image ) ) if q [ 'registry' ] == None : q [ 'registry' ] = self . base q = self . add https ( q ) url = '%s/container/%s/%s:%s' % ( q [ 'registry' ] , q [ \"collection\" ] , q [ \"image\" ] , q [ \"tag\" ] ) SREGISTRY EVENT = self . authorize ( request type = \"delete\" , names = q ) headers = { 'Authorization' : SREGISTRY EVENT } self . update headers ( fields = headers ) continue delete = True if force is False : response = input ( \"Are you sure you want to delete %s?\" % q [ 'uri' ] ) while len ( response ) < 1 or response [ 0 ] . lower ( ) . strip ( ) not in \"ynyesno\" : response = input ( \"Please answer yes or no: \" ) if response [ 0 ] . lower ( ) . strip ( ) in \"no\" : continue delete = False if continue delete is True : response = self . delete ( url ) message = self . read response ( response ) bot . info ( \"Response %s, %s\" % ( response . status code , message ) ) else : bot . info ( \"Delete cancelled.\" )", "predictions": ["remove an image from the collection ."], "references": ["delete an image to singularity registry"], "bleu": 0.24446151121745047, "rouge_l": 0.31202046035805625}
{"id": 2574, "code": "def get installdir ( ) : return os . path . abspath ( os . path . dirname ( os . path . dirname ( file ) ) )", "predictions": ["return the absolute path to the installdir directory ."], "references": ["get_installdir returns the installation directory of the application"], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 2575, "code": "def get collections ( self ) : collections = [ ] for container in self . conn . get account ( ) [ 1 ] : collections . append ( container [ 'name' ] ) return collections", "predictions": ["returns all collections of the account"], "references": ["get a listing of collections that the user has access to ."], "bleu": 0.09663861439684919, "rouge_l": 0.20962199312714777}
{"id": 2576, "code": "def ipython ( args ) : from sregistry . main import get client client = get client ( args . endpoint ) client . announce ( args . command ) from I Python import embed embed ( )", "predictions": ["announce an ipython ipython ipython ipython ipython ipython ipython ipython ."], "references": ["give the user an ipython shell optionally with an endpoint of choice ."], "bleu": 0.13430919728978852, "rouge_l": 0.24629878869448185}
{"id": 2577, "code": "def update base ( self ) : self . base = self . get and update setting ( 'SREGISTRY GITLAB BASE' , \"https://gitlab.com/\" ) self . api base = \"%s/api/v4\" % self . base . strip ( '/' ) self . artifacts = self . get and update setting ( 'SREGISTRY GITLAB FOLDER' , 'build' ) self . job = self . get and update setting ( 'SREGISTRY GITLAB JOB' , 'build' ) bot . debug ( '      Api: %s' % self . api base ) bot . debug ( 'Artifacts: %s' % self . artifacts ) bot . debug ( '      Job: %s' % self . job )", "predictions": ["get base base base base base base artifacts ."], "references": ["update the base including the url for gitlab and the api endpoint ."], "bleu": 0.10015045110931886, "rouge_l": 0.17604617604617603}
{"id": 2578, "code": "def update secrets ( self ) : self . token = self . required get and update ( 'SREGISTRY GITLAB TOKEN' ) self . headers [ \"Private-Token\" ] = self . token", "predictions": ["update the token from the token"], "references": ["update secrets will update metadata needed for pull and search"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 2579, "code": "def update setting ( self , name , value ) : if value is not None : updates = { name : value } update client secrets ( backend = self . client name , updates = updates )", "predictions": ["update the setting of a client ."], "references": ["just update a setting doesn t need to be returned ."], "bleu": 0.1319006407505858, "rouge_l": 0.32049036777583184}
{"id": 2580, "code": "def search all ( self ) : results = set ( ) for container in self . conn . get account ( ) [ 1 ] : for result in self . conn . get container ( container [ 'name' ] ) [ 1 ] : results . add ( '%s/%s' % ( container [ 'name' ] , result [ 'name' ] ) ) if len ( results ) == 0 : bot . info ( \"No container collections found.\" ) sys . exit ( 1 ) bot . info ( \"Collections\" ) bot . table ( [ [ x ] for x in list ( results ) ] ) return list ( results )", "predictions": ["search all container collections"], "references": ["a show all search that doesn t require a query"], "bleu": 0.08872444253557525, "rouge_l": 0.13260869565217392}
{"id": 2581, "code": "def search all ( self ) : results = [ ] for entry in self . dbx . files list folder ( '' ) . entries : for item in self . dbx . files list folder ( entry . path lower ) . entries : name = item . name . replace ( '.simg' , '' ) results . append ( [ \"%s/%s\" % ( entry . name , name ) ] ) if len ( results ) == 0 : bot . info ( \"No container collections found.\" ) sys . exit ( 1 ) bot . info ( \"Collections\" ) bot . table ( results ) return results", "predictions": ["search all files in the container ."], "references": ["a show all search that doesn t require a query"], "bleu": 0.13391424795650428, "rouge_l": 0.11401869158878504}
{"id": 2582, "code": "def get build template ( ) : base = get installdir ( ) name = \"%s/main/templates/build/singularity-cloudbuild.json\" % base if os . path . exists ( name ) : bot . debug ( \"Found template %s\" % name ) return read json ( name ) bot . warning ( \"Template %s not found.\" % name )", "predictions": ["return the build template template ."], "references": ["get default build template ."], "bleu": 0.345720784641941, "rouge_l": 0.5545454545454546}
{"id": 2583, "code": "def get bucket ( self ) : try : self . bucket = self . bucket service . get bucket ( self . bucket name ) except google . cloud . exceptions . Not Found : self . bucket = self . bucket service . create bucket ( self . bucket name ) except : bot . error ( 'Cannot get or create %s' % self . bucket name ) sys . exit ( 1 ) return self . bucket", "predictions": ["get the bucket from the bucket ."], "references": ["get a bucket based on a bucket name . if it doesn t exist create it ."], "bleu": 0.05597505026193914, "rouge_l": 0.3100381194409148}
{"id": 2584, "code": "def get subparsers ( parser ) : actions = [ action for action in parser . actions if isinstance ( action , argparse . Sub Parsers Action ) ] subparsers = dict ( ) for action in actions : for choice , subparser in action . choices . items ( ) : subparsers [ choice ] = subparser return subparsers", "predictions": ["get subparsers from all actions ."], "references": ["get_subparser will get a dictionary of subparsers to help with printing help"], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 2585, "code": "def get file hash ( filename ) : hasher = hashlib . sha256 ( ) with open ( filename , \"rb\" ) as f : for chunk in iter ( lambda : f . read ( 4096 ) , b\"\" ) : hasher . update ( chunk ) return hasher . hexdigest ( )", "predictions": ["return the hash of the contents of a file ."], "references": ["find the sha256 hash string of a file"], "bleu": 0.25965358893403384, "rouge_l": 0.5669144981412639}
{"id": 2586, "code": "def clean up ( files ) : if not isinstance ( files , list ) : files = [ files ] for f in files : if os . path . exists ( f ) : bot . verbose3 ( \"Cleaning up %s\" % f ) os . remove ( f )", "predictions": ["remove all up up to the given list ."], "references": ["clean up will delete a list of files only if they exist"], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 2587, "code": "def push ( self , path , name , tag = None ) : path = os . path . abspath ( path ) image = os . path . basename ( path ) bot . debug ( \"PUSH %s\" % path ) if not os . path . exists ( path ) : bot . error ( '%s does not exist.' % path ) sys . exit ( 1 ) names = parse image name ( remove uri ( name ) , tag = tag ) image size = os . path . getsize ( path ) >> 20 metadata = { 'sizemb' : \"%s\" % image size , 'client' : 'sregistry' } self . bucket . upload file ( path , names [ 'storage uri' ] , { \"Metadata\" : metadata } )", "predictions": ["push an image to the bucket"], "references": ["push an image to an s3 endpoint"], "bleu": 0.5204069361525009, "rouge_l": 0.6069651741293532}
{"id": 2588, "code": "def get collection ( self , name ) : from sregistry . database . models import Collection return Collection . query . filter ( Collection . name == name ) . first ( )", "predictions": ["get a collection by name"], "references": ["get a collection if it exists otherwise return none ."], "bleu": 0.18693159143202892, "rouge_l": 0.37731958762886597}
{"id": 2589, "code": "def get container ( self , name , collection id , tag = \"latest\" , version = None ) : from sregistry . database . models import Container if version is None : container = Container . query . filter by ( collection id = collection id , name = name , tag = tag ) . first ( ) else : container = Container . query . filter by ( collection id = collection id , name = name , tag = tag , version = version ) . first ( ) return container", "predictions": ["return a container by name"], "references": ["get a container otherwise return none ."], "bleu": 0.25880882365505126, "rouge_l": 0.32360742705570295}
{"id": 2590, "code": "def rmi ( self , image name ) : container = self . rm ( image name , delete = True ) if container is not None : bot . info ( \"[rmi] %s\" % container )", "predictions": ["remove an image by name ."], "references": ["remove an image from the database and filesystem ."], "bleu": 0.26367153725164694, "rouge_l": 0.5147679324894514}
{"id": 2591, "code": "def run build ( self , config , bucket , names ) : project = self . get project ( ) bot . custom ( 'PROJECT' , project , \"CYAN\" ) bot . custom ( 'BUILD  ' , config [ 'steps' ] [ 0 ] [ 'name' ] , \"CYAN\" ) response = self . build service . projects ( ) . builds ( ) . create ( body = config , project Id = project ) . execute ( ) build id = response [ 'metadata' ] [ 'build' ] [ 'id' ] status = response [ 'metadata' ] [ 'build' ] [ 'status' ] bot . log ( \"build %s: %s\" % ( build id , status ) ) start = time . time ( ) while status not in [ 'COMPLETE' , 'FAILURE' , 'SUCCESS' ] : time . sleep ( 15 ) response = self . build service . projects ( ) . builds ( ) . get ( id = build id , project Id = project ) . execute ( ) build id = response [ 'id' ] status = response [ 'status' ] bot . log ( \"build %s: %s\" % ( build id , status ) ) end = time . time ( ) bot . log ( 'Total build time: %s seconds' % ( round ( end - start , 2 ) ) ) if status == 'SUCCESS' : env = 'SREGISTRY GOOGLE STORAGE PRIVATE' blob = bucket . blob ( response [ 'artifacts' ] [ 'objects' ] [ 'paths' ] [ 0 ] ) if self . get and update setting ( env ) == None : blob . make public ( ) response [ 'public url' ] = blob . public url update blob metadata ( blob , response , config , bucket , names ) response [ 'media link' ] = blob . media link response [ 'size' ] = blob . size response [ 'file hash' ] = blob . md5 hash return response", "predictions": ["run a build on the project"], "references": ["run a build meaning creating a build . retry if there is failure"], "bleu": 0.12802833376249098, "rouge_l": 0.29611650485436897}
{"id": 2592, "code": "def search all ( self ) : url = '...' results = self . paginate get ( url ) if len ( results ) == 0 : bot . info ( \"No container collections found.\" ) sys . exit ( 1 ) bot . info ( \"Collections\" ) rows = [ ] for result in results : if \"containers\" in result : for c in result [ 'containers' ] : rows . append ( [ c [ 'uri' ] , c [ 'detail' ] ] ) bot . table ( rows ) return rows", "predictions": ["load all rows in the current container"], "references": ["a show all search that doesn t require a query"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 2593, "code": "def get manifest ( self , repo name , tag ) : image = None repo = self . aws . describe images ( repository Name = repo name ) if 'image Details' in repo : for contender in repo . get ( 'image Details' ) : if tag in contender [ 'image Tags' ] : image = contender break if image is None : bot . exit ( 'Cannot find %s:%s, is the uri correct?' % ( repo name , digest ) ) digest = image [ 'image Digest' ] digests = self . aws . batch get image ( repository Name = repo name , image Ids = [ { \"image Digest\" : digest , \"image Tag\" : tag } ] ) self . manifest = json . loads ( digests [ 'images' ] [ 0 ] [ 'image Manifest' ] ) return self . manifest", "predictions": ["list all if the logs is in the repo"], "references": ["return the image manifest via the aws client saved in self . manifest"], "bleu": 0.10761866342063775, "rouge_l": 0.17604617604617603}
{"id": 2594, "code": "def s3errors ( path ) : try : yield except Client Error as error : error = error . response . get ( \"Error\" , { } ) error code = error . get ( \"Code\" , None ) response meta = error . response . get ( \"Response Metadata\" , { } ) http status = response meta . get ( \"HTTP Status Code\" , 200 ) error msg = error . get ( \"Message\" , None ) if error code == \"No Such Bucket\" : raise errors . Resource Error ( path , exc = error , msg = error msg ) if http status == 404 : raise errors . Resource Not Found ( path ) elif http status == 403 : raise errors . Permission Denied ( path = path , msg = error msg ) else : raise errors . Operation Failed ( path = path , exc = error ) except SSL Error as error : raise errors . Operation Failed ( path , exc = error ) except Endpoint Connection Error as error : raise errors . Remote Connection Error ( path , exc = error , msg = \"{}\" . format ( error ) )", "predictions": ["do a get request"], "references": ["translate s3 errors to fserrors ."], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 2595, "code": "def factory ( cls , filename , mode , on close ) : temp file = tempfile . Temporary File ( ) proxy = cls ( temp file , filename , mode , on close = on close ) return proxy", "predictions": ["status to a class are defined in the given filename are the same"], "references": ["create a s3file backed with a temporary file ."], "bleu": 0.09552040806823771, "rouge_l": 0.09399075500770414}
{"id": 2596, "code": "def gravatar url ( user or email , size = GRAVATAR DEFAULT SIZE ) : if hasattr ( user or email , 'email' ) : email = user or email . email else : email = user or email try : return escape ( get gravatar url ( email = email , size = size ) ) except : return ''", "predictions": ["get the url url url url"], "references": ["builds a gravatar url from an user or email"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 2597, "code": "def has gravatar ( email ) : url = get gravatar url ( email , default = GRAVATAR DEFAULT IMAGE 404 ) try : request = Request ( url ) request . get method = lambda : 'HEAD' return 200 == urlopen ( request ) . code except ( HTTP Error , URL Error ) : return False", "predictions": ["returns true if the email is gravatar false otherwise"], "references": ["returns true if the user has a gravatar false if otherwise"], "bleu": 0.37405485108988873, "rouge_l": 0.6876006441223833}
{"id": 2598, "code": "def chimera blocks ( M = 16 , N = 16 , L = 4 ) : for x in xrange ( M ) : for y in xrange ( N ) : for u in ( 0 , 1 ) : yield tuple ( ( x , y , u , k ) for k in xrange ( L ) )", "predictions": ["yield - sequence activate activate - separated blocks ."], "references": ["generator for blocks for a chimera block quotient"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 2599, "code": "def main ( ) : parser = Molvs Parser ( epilog = 'use \"molvs <command> -h\" to show help for a specific command' ) subparsers = parser . add subparsers ( title = 'Available commands' ) common parser = Molvs Parser ( add help = False ) common parser . add argument ( 'infile' , nargs = '?' , help = 'input filename' , type = argparse . File Type ( 'r' ) , default = sys . stdin ) common parser . add argument ( '-i' , '--intype' , help = 'input filetype' , choices = FILETYPES ) common parser . add argument ( '-:' , '--smiles' , help = 'input SMILES instead of file' , metavar = '<smiles>' ) common parser . add argument ( '-O' , '--outfile' , help = 'output filename' , type = argparse . File Type ( 'w' ) , default = sys . stdout , metavar = '<outfile>' ) standardize parser = subparsers . add parser ( 'standardize' , help = 'standardize a molecule' , parents = [ common parser ] ) standardize parser . add argument ( '-o' , '--outtype' , help = 'output filetype' , choices = FILETYPES ) standardize parser . set defaults ( func = standardize main ) validate parser = subparsers . add parser ( 'validate' , help = 'validate a molecule' , parents = [ common parser ] ) validate parser . set defaults ( func = validate main ) args = parser . parse args ( ) try : args . func ( args ) except Exception as e : sys . stderr . write ( 'Error: %s\\n\\n' . encode ( ) % e . message ) parser . print help ( ) sys . exit ( 2 )", "predictions": ["command line interface for the program ."], "references": ["main function for molvs command line interface ."], "bleu": 0.33167003447658744, "rouge_l": 0.5269978401727862}
{"id": 2600, "code": "def integrate ivp ( u0 = 1.0 , v0 = 0.0 , mu = 1.0 , tend = 10.0 , dt0 = 1e-8 , nt = 0 , nsteps = 600 , t0 = 0.0 , atol = 1e-8 , rtol = 1e-8 , plot = False , savefig = 'None' , method = 'bdf' , dpi = 100 , verbose = False ) : f , j = get f and j ( mu ) if nt > 1 : tout = np . linspace ( t0 , tend , nt ) yout , nfo = integrate predefined ( f , j , [ u0 , v0 ] , tout , dt0 , atol , rtol , nsteps = nsteps , check indexing = False , method = method ) else : tout , yout , nfo = integrate adaptive ( f , j , [ u0 , v0 ] , t0 , tend , dt0 , atol , rtol , nsteps = nsteps , check indexing = False , method = method ) if verbose : print ( nfo ) if plot : import matplotlib . pyplot as plt plt . plot ( tout , yout [ : , 1 ] , 'g--' ) plt . plot ( tout , yout [ : , 0 ] , 'k-' , linewidth = 2 ) if savefig == 'None' : plt . show ( ) else : plt . savefig ( savefig , dpi = dpi )", "predictions": ["r func to delete the ivp of a ivp"], "references": ["example program integrating an ivp problem of van der pol oscillator"], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 2601, "code": "def get mems of org ( self ) : print 'Getting members.' counter = 0 for member in self . org retrieved . iter members ( ) : self . members json [ member . id ] = member . to json ( ) counter += 1 return counter", "predictions": ["return the mems of the org"], "references": ["retrieves the number of members of the organization ."], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 2602, "code": "def get teams of org ( self ) : print 'Getting teams.' counter = 0 for team in self . org retrieved . iter teams ( ) : self . teams json [ team . id ] = team . to json ( ) counter += 1 return counter", "predictions": ["return self debug self debug debug debug debug debug"], "references": ["retrieves the number of teams of the organization ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2603, "code": "def repos ( self , repo type = 'public' , organization = 'llnl' ) : print 'Getting repos.' for repo in self . org retrieved . iter repos ( type = repo type ) : #JSON json = repo . to json ( ) self . repos json [ repo . name ] = json #CSV temp repo = my repo . My Repo ( ) temp repo . name = repo . full name self . total repos += 1 temp repo . contributors = my github . get total contributors ( repo ) self . total contributors += temp repo . contributors temp repo . forks = repo . forks count self . total forks += temp repo . forks temp repo . stargazers = repo . stargazers self . total stars += temp repo . stargazers temp repo . pull requests open , temp repo . pull requests closed = my github . get pull reqs ( repo ) temp repo . pull requests = ( temp repo . pull requests open + temp repo . pull requests closed ) self . total pull reqs += temp repo . pull requests open self . total pull reqs += temp repo . pull requests closed self . total pull reqs open += temp repo . pull requests open self . total pull reqs closed += temp repo . pull requests closed temp repo . open issues = repo . open issues count self . total open issues += temp repo . open issues temp repo . closed issues = my github . get issues ( repo , organization = organization ) temp repo . issues = temp repo . closed issues + temp repo . open issues self . total closed issues += temp repo . closed issues self . total issues += temp repo . issues my github . get languages ( repo , temp repo ) temp repo . readme = my github . get readme ( repo ) #temp repo.license = my github.get license(repo) temp repo . commits = self . get commits ( repo = repo , organization = organization ) self . total commits += temp repo . commits self . all repos . append ( temp repo )", "predictions": ["list all get requests requests"], "references": ["retrieves info about the repos of the current organization ."], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 2604, "code": "def get pull reqs ( self , repo ) : pull reqs open = 0 pull reqs closed = 0 for pull request in repo . iter pulls ( state = 'all' ) : self . pull requests json [ repo . name ] . append ( pull request . to json ( ) ) if pull request . closed at is not None : pull reqs closed += 1 else : pull reqs open += 1 return pull reqs open , pull reqs closed", "predictions": ["return all not start not in the headers"], "references": ["retrieves the number of pull requests on a repo in the organization ."], "bleu": 0.11296874775996037, "rouge_l": 0.18263473053892215}
{"id": 2605, "code": "def get issues ( self , repo , organization = 'llnl' ) : #JSON path = ( '../github-data/' + organization + '/' + repo . name + '/issues' ) is only today = False if not os . path . exists ( path ) : #no previous path, get all issues all issues = repo . iter issues ( state = 'all' ) is only today = True else : files = os . listdir ( path ) date = str ( files [ - 1 ] [ : - 5 ] ) if date == str ( datetime . date . today ( ) ) : #most recent date is actually today, get previous most recent date if len ( files ) > 2 : date = str ( files [ - 2 ] [ : - 5 ] ) else : #This means there is only one file, today. Retrieve every issue all issues = repo . iter issues ( state = 'all' ) is only today = True if not is only today : #there's a previous saved JSON that's not today all issues = repo . iter issues ( since = date , state = 'all' ) for issue in all issues : self . issues json [ repo . name ] . append ( issue . to json ( ) ) #CSV closed issues = 0 for issue in repo . iter issues ( state = 'closed' ) : if issue is not None : closed issues += 1 return closed issues", "predictions": ["remove all issues in the = = 1"], "references": ["retrieves the number of closed issues ."], "bleu": 0.17747405280050269, "rouge_l": 0.13495575221238937}
{"id": 2606, "code": "def get license ( self , repo ) : if self . search limit >= 28 : print 'Hit search limit. Sleeping for 60 sec.' time . sleep ( 60 ) self . search limit = 0 self . search limit += 1 search results = self . logged in gh . search code ( 'license' + 'in:path repo:' + repo . full name ) try : for result in search results : path = result . path [ 1 : ] if '/' not in path and 'license' in path . lower ( ) : self . total licenses += 1 return path return 'MISS' except ( Stop Iteration ) as e : return 'MISS'", "predictions": ["return license license ."], "references": ["checks to see if the given repo has a top level license file ."], "bleu": 0.032639898338235177, "rouge_l": 0.20198675496688742}
{"id": 2607, "code": "def write org json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict to write = { } , path ending type = '' , is list = False ) : path = ( '../github-data/' + organization + '-org/' + path ending type + '/' + str ( date ) + '.json' ) self . check Dir ( path ) with open ( path , 'w' ) as out clear : #clear old data out clear . close ( ) with open ( path , 'a' ) as out : if is list : #used for list of items out . write ( '[' ) for item in dict to write : out . write ( json . dumps ( dict to write [ item ] , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) + ',' ) out . seek ( - 1 , os . SEEK END ) #kill last comma out . truncate ( ) if is list : out . write ( ']' ) out . close ( )", "predictions": ["get the data data data to a json for the given = = ."], "references": ["writes stats from the organization to json ."], "bleu": 0.11114924776032006, "rouge_l": 0.3824451410658307}
{"id": 2608, "code": "def write repo json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict to write = { } , path ending type = '' , is list = False , is dict = False ) : for repo in dict to write : path = ( '../github-data/' + organization + '/' + repo + '/' + path ending type + '/' + str ( date ) + '.json' ) self . check Dir ( path ) with open ( path , 'w' ) as out : if is list : out . write ( '[' ) for value in dict to write [ repo ] : if is dict : for inner dict in value : out . write ( json . dumps ( inner dict , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) + ',' ) else : out . write ( json . dumps ( value , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) + ',' ) out . seek ( - 1 , os . SEEK END ) #kill last comma out . truncate ( ) out . write ( ']' ) else : out . write ( json . dumps ( dict to write [ repo ] , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) ) out . close ( )", "predictions": ["ipython ipython repo args to args"], "references": ["#writes repo specific data to json ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 2609, "code": "def write totals ( self , file path = '' , date = str ( datetime . date . today ( ) ) , organization = 'N/A' , members = 0 , teams = 0 ) : total exists = os . path . isfile ( file path ) with open ( file path , 'a' ) as out total : if not total exists : out total . write ( 'date,organization,repos,members,teams,' + 'unique contributors,total contributors,forks,' + 'stargazers,pull requests,open issues,has readme,' + 'has license,pull requests open,pull requests closed,' + 'commits,id,closed issues,issues\\n' ) self . delete last line ( date = date , file path = file path ) out total . close ( ) with open ( file path , 'r' ) as file read : row count = sum ( 1 for row in file read ) - 1 file read . close ( ) with open ( file path , 'a' ) as out total : out total . write ( date + ',' + organization + ',' + str ( self . total repos ) + ',' + str ( members ) + ',' + str ( teams ) + ',' + str ( len ( self . unique contributors ) ) + ',' + str ( self . total contributors ) + ',' + str ( self . total forks ) + ',' + str ( self . total stars ) + ',' + str ( self . total pull reqs ) + ',' + str ( self . total open issues ) + ',' + str ( self . total readmes ) + ',' + str ( self . total licenses ) + ',' + str ( self . total pull reqs open ) + ',' + str ( self . total pull reqs closed ) + ',' + str ( self . total commits ) + ',' + str ( row count ) + ',' + str ( self . total closed issues ) + ',' + str ( self . total issues ) + '\\n' ) out total . close ( )", "predictions": ["update the artifacts of the organization ."], "references": ["updates the total . csv file with current data ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 2610, "code": "def write languages ( self , file path = '' , date = str ( datetime . date . today ( ) ) ) : self . remove date ( file path = file path , date = date ) languages exists = os . path . isfile ( file path ) with open ( file path , 'a' ) as out languages : if not languages exists : out languages . write ( 'date,language,count,size,size log\\n' ) languages sorted = sorted ( self . languages size ) #self.delete last line(date=date, file path=file path) for language in languages sorted : try : out languages . write ( date + ',' + language + ',' + str ( self . languages [ language ] ) + ',' + str ( self . languages size [ language ] ) + ',' + str ( math . log10 ( int ( self . languages size [ language ] ) ) ) + '\\n' ) except ( Type Error , Key Error ) as e : out languages . write ( date + ',' + language + ',' + str ( 0 ) + ',' + str ( self . languages size [ language ] ) + ',' + str ( math . log10 ( int ( self . languages size [ language ] ) ) ) + '\\n' )", "predictions": ["update the secrets list . ."], "references": ["updates languages . csv file with current data ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 2611, "code": "def connect ( url = , token = None ) : gh session = None if url == : gh session = create session ( token ) else : gh session = create enterprise session ( url , token ) if gh session is None : msg = 'Unable to connect to (%s) with provided token.' raise Runtime Error ( msg , url ) logger . info ( 'Connected to: %s' , url ) return gh session", "predictions": ["update a connection to the mysql host . . . . . . ."], "references": ["create a github session for making requests"], "bleu": 0.08839374326825923, "rouge_l": 0.10132890365448505}
{"id": 2612, "code": "def write to file ( self , file path = '' , date = ( datetime . date . today ( ) ) , organization = 'llnl' ) : with open ( file path , 'w+' ) as out : out . write ( 'date,organization,stargazers\\n' ) sorted stargazers = sorted ( self . stargazers ) #sort based on lowercase for star in sorted stargazers : out . write ( star + ',' + str ( self . stargazers [ star ] ) + '\\n' ) out . close ( )", "predictions": ["search the current get to a file in the get directory"], "references": ["writes stargazers data to file ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 2613, "code": "def from gitlab ( klass , repository , labor hours = True ) : if not isinstance ( repository , gitlab . v4 . objects . Project ) : raise Type Error ( 'Repository must be a gitlab Repository object' ) project = klass ( ) logger . debug ( 'Git Lab: repository id=%d path with namespace=%s' , repository . id , repository . path with namespace , ) project [ 'name' ] = repository . name project [ 'repository URL' ] = repository . http url to repo project [ 'description' ] = repository . description project [ 'permissions' ] [ 'licenses' ] = None web url = repository . web url public server = web url . startswith ( 'https://gitlab.com' ) if repository . visibility in ( 'public' ) and public server : project [ 'permissions' ] [ 'usage Type' ] = 'open Source' elif date parse ( repository . created at ) < POLICY START DATE : project [ 'permissions' ] [ 'usage Type' ] = 'exempt By Policy Date' if labor hours : project [ 'labor Hours' ] = labor hours from url ( project [ 'repository URL' ] ) else : project [ 'labor Hours' ] = 0 project [ 'tags' ] = [ 'gitlab' ] + repository . tag list project [ 'contact' ] = { 'email' : '' , 'URL' : web url , } project [ 'organization' ] = repository . namespace [ 'name' ] project [ 'status' ] = 'Development' project [ 'vcs' ] = 'git' project [ 'homepage URL' ] = repository . web url api url = repository . manager . gitlab . url archive suffix = '/projects/%s/repository/archive' % repository . get id ( ) project [ 'download URL' ] = api url + archive suffix project [ 'date' ] = { 'created' : date parse ( repository . created at ) . date ( ) . isoformat ( ) , 'last Modified' : date parse ( repository . last activity at ) . date ( ) . isoformat ( ) , 'metadata Last Updated' : '' , } prune dict null str ( project ) return project", "predictions": ["create a project object from a all the all the all - link attributes"], "references": ["create codegovproject object from gitlab repository"], "bleu": 0.1250076305588977, "rouge_l": 0.323321554770318}
{"id": 2614, "code": "def from stashy ( klass , repository , labor hours = True ) : if not isinstance ( repository , dict ) : raise Type Error ( 'Repository must be a dict' ) project = klass ( ) logger . debug ( 'Stashy: project key=%s repository slug=%s' , repository [ 'name' ] , repository [ 'project' ] [ 'key' ] , ) project [ 'name' ] = repository [ 'name' ] clone urls = [ clone [ 'href' ] for clone in repository [ 'links' ] [ 'clone' ] ] for url in clone urls : if url . startswith ( 'ssh://' ) : project [ 'repository URL' ] = url break description = repository [ 'project' ] . get ( 'description' , '' ) if description : project [ 'description' ] = 'Project description: %s' % description project [ 'permissions' ] [ 'licenses' ] = None web url = repository [ 'links' ] [ 'self' ] [ 0 ] [ 'href' ] public server = web url . startswith ( 'https://bitbucket.org' ) if repository [ 'public' ] and public server : project [ 'permissions' ] [ 'usage Type' ] = 'open Source' if labor hours : project [ 'labor Hours' ] = labor hours from url ( project [ 'repository URL' ] ) else : project [ 'labor Hours' ] = 0 project [ 'tags' ] = [ 'bitbucket' ] project [ 'contact' ] [ 'email' ] = '' project [ 'contact' ] [ 'URL' ] = repository [ 'links' ] [ 'self' ] [ 0 ] [ 'href' ] project [ 'status' ] = 'Development' project [ 'vcs' ] = repository [ 'scm Id' ] project [ 'homepage URL' ] = repository [ 'links' ] [ 'self' ] [ 0 ] [ 'href' ] prune dict null str ( project ) return project", "predictions": ["create a warning object from a repository repository"], "references": ["handles crafting code . gov project for bitbucket server repositories"], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 2615, "code": "def force attributes ( metadata , config ) : organization = config . get ( 'organization' , '' ) logger . debug ( 'Organization: %s' , organization ) contact email = config . get ( 'contact email' ) logger . debug ( 'Contact Email: %s' , contact email ) permissions = config . get ( 'permissions' , { } ) default usage = permissions . get ( 'usage Type' , '' ) default exemption text = permissions . get ( 'exemption Text' , '' ) logger . debug ( 'Default usage Type: %s' , default usage ) logger . debug ( 'Default exemption Text: %s' , default exemption text ) if organization : logger . debug ( 'Forcing Organization to: %s' , organization ) if contact email : logger . debug ( 'Forcing Contact Email to: %s' , contact email ) for release in metadata [ 'releases' ] : if organization : release [ 'organization' ] = organization if contact email : release [ 'contact' ] [ 'email' ] = contact email if 'licenses' not in release [ 'permissions' ] : release [ 'permissions' ] [ 'licenses' ] = None if 'description' not in release : release [ 'description' ] = 'No description available...' if 'usage Type' not in release [ 'permissions' ] : release [ 'permissions' ] [ 'usage Type' ] = default usage release [ 'permissions' ] [ 'exemption Text' ] = default exemption text return metadata", "predictions": ["get bucket bucket bucket self name from = name"], "references": ["forces certain fields in the code . gov metadata json"], "bleu": 0.10620315618312248, "rouge_l": 0.0}
{"id": 2616, "code": "def get traffic ( self ) : print 'Getting traffic.' #Uses the developer API. Note this could change. headers = { 'Accept' : 'application/vnd.github.spiderman-preview' , 'Authorization' : 'token ' + self . token } headers release = { 'Authorization' : 'token ' + self . token } for repo in self . org retrieved . iter repos ( type = 'public' ) : url = ( 'https://api.github.com/repos/' + self . organization name + '/' + repo . name ) self . get referrers ( url = url , headers = headers , repo name = repo . name ) self . get paths ( url = url , headers = headers ) self . get data ( url = url , headers = headers , dict to store = self . views , type = 'views' , repo name = repo . name ) self . get data ( url = url , headers = headers , dict to store = self . clones , type = 'clones' , repo name = repo . name ) self . get releases ( url = url , headers = headers release , repo name = repo . name )", "predictions": ["get all subparsers from the organization"], "references": ["retrieves the traffic for the repositories of the given organization ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 2617, "code": "def get releases ( self , url = '' , headers = { } , repo name = '' ) : url releases = ( url + '/releases' ) r = requests . get ( url releases , headers = headers ) self . releases json [ repo name ] = r . json ( )", "predictions": ["gets file file file file file contents for a given url"], "references": ["retrieves the releases for the given repo in json ."], "bleu": 0.12605968092174913, "rouge_l": 0.1921259842519685}
{"id": 2618, "code": "def write json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict to write = { } , path ending type = '' ) : for repo in dict to write : if len ( dict to write [ repo ] ) != 0 : #don't need to write out empty lists path = ( '../github-data/' + organization + '/' + repo + '/' + path ending type + '/' + str ( date ) + '.json' ) self . check Dir ( path ) with open ( path , 'w' ) as out : out . write ( json . dumps ( dict to write [ repo ] , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) ) out . close ( )", "predictions": ["clean the up - up up up to a file"], "references": ["writes all traffic data to file in json form ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 2619, "code": "def write to file ( self , referrers file path = '' , views file path = '' , clones file path = '' , date = ( datetime . date . today ( ) ) , organization = 'llnl' , views row count = 0 , clones row count = 0 ) : self . write referrers to file ( file path = referrers file path ) self . write data to file ( file path = views file path , dict to write = self . views , name = 'views' , row count = views row count ) self . write data to file ( file path = clones file path , dict to write = self . clones , name = 'clones' , row count = clones row count )", "predictions": ["push the exists to a file in the specified tag image image image image image"], "references": ["writes all traffic data to file ."], "bleu": 0.09103526405546068, "rouge_l": 0.1945773524720893}
{"id": 2620, "code": "def write data to file ( self , file path = '' , date = str ( datetime . date . today ( ) ) , organization = 'llnl' , dict to write = { } , name = '' , row count = 0 ) : exists = os . path . isfile ( file path ) with open ( file path , 'a' ) as out : if not exists : out . write ( 'date,organization,' + name + ',unique ' + name + ',id\\n' ) sorted dict = sorted ( dict to write ) for day in sorted dict : day formatted = datetime . datetime . utcfromtimestamp ( day ) . strftime ( '%Y-%m-%d' ) out . write ( day formatted + ',' + organization + ',' + str ( dict to write [ day ] [ 0 ] ) + ',' + str ( dict to write [ day ] [ 1 ] ) + ',' + str ( row count ) + '\\n' ) row count += 1", "predictions": ["get the collection of collection collection to a self import file"], "references": ["writes given dict to file ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 2621, "code": "def write referrers to file ( self , file path = '' , date = str ( datetime . date . today ( ) ) , organization = 'llnl' ) : self . remove date ( file path = file path , date = date ) referrers exists = os . path . isfile ( file path ) with open ( file path , 'a' ) as out : if not referrers exists : out . write ( 'date,organization,referrer,count,count log,uniques,' + 'uniques logged\\n' ) sorted referrers = sorted ( self . referrers lower ) #sort based on lowercase for referrer in sorted referrers : ref name = self . referrers lower [ referrer ] #grab real name from count = self . referrers [ ref name ] [ 0 ] uniques = self . referrers [ ref name ] [ 1 ] if count == 1 : #so we don't display 0 for count of 1 count = 1.5 if uniques == 1 : uniques = 1.5 count logged = math . log ( count ) uniques logged = math . log ( uniques ) out . write ( date + ',' + organization + ',' + ref name + ',' + str ( count ) + ',' + str ( count logged ) + ',' + str ( uniques ) + ',' + str ( uniques logged ) + '\\n' ) out . close ( )", "predictions": ["get the container for the organization organization"], "references": ["writes the referrers data to file ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 2622, "code": "def write to file ( self , file path = '' ) : with open ( file path , 'w+' ) as out : out . write ( 'user, email\\n' ) sorted names = sorted ( self . logins lower ) #sort based on lowercase for login in sorted names : out . write ( self . logins lower [ login ] + ',' + self . emails [ self . logins lower [ login ] ] + '\\n' ) out . close ( )", "predictions": ["write the emails to a file"], "references": ["writes the user emails to file ."], "bleu": 0.27960682295094563, "rouge_l": 0.6069651741293532}
{"id": 2623, "code": "def connect ( url , username , password ) : bb session = stashy . connect ( url , username , password ) logger . info ( 'Connected to: %s as %s' , url , username ) return bb session", "predictions": ["run a = = 0 ."], "references": ["return a connected bitbucket session"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 2624, "code": "def query repos ( gl session , repos = None ) : if repos is None : repos = [ ] for repo in repos : yield gl session . projects . get ( repo ) if not repos : for project in gl session . projects . list ( as list = False ) : yield project", "predictions": ["query for repos repos"], "references": ["yields gitlab project objects for all projects in bitbucket"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 2625, "code": "def prune dict null str ( dictionary ) : for key , value in list ( dictionary . items ( ) ) : if value is None or str ( value ) == '' : del dictionary [ key ] if isinstance ( value , dict ) : dictionary [ key ] = prune dict null str ( dictionary [ key ] ) return dictionary", "predictions": ["remove all null - case from a dictionary ."], "references": ["prune the none or emptry string values from dictionary items"], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 2626, "code": "def create tfs connection ( url , token ) : if token is None : token = os . environ . get ( 'TFS API TOKEN' , None ) tfs credentials = Basic Authentication ( '' , token ) tfs connection = Vss Connection ( base url = url , creds = tfs credentials ) return tfs connection", "predictions": ["create a tfs connection to tfs ."], "references": ["creates the tfs connection context"], "bleu": 0.24446151121745047, "rouge_l": 0.34366197183098596}
{"id": 2627, "code": "def create tfs git client ( url , token = None ) : if token is None : token = os . environ . get ( 'TFS API TOKEN' , None ) tfs connection = create tfs connection ( url , token ) tfs git client = tfs connection . get client ( 'vsts.git.v4 1.git client.Git Client' ) if tfs git client is None : msg = 'Unable to create TFS Git Client, failed to connect to TFS Enterprise (%s) with provided token.' raise Runtime Error ( msg , url ) return tfs git client", "predictions": ["create a tfs client client ."], "references": ["creates a tfs git client to pull git repo info"], "bleu": 0.16038842424444547, "rouge_l": 0.3588235294117647}
{"id": 2628, "code": "def create tfs tfvc client ( url , token = None ) : if token is None : token = os . environ . get ( 'TFS API TOKEN' , None ) tfs connection = create tfs connection ( url , token ) tfs tfvc client = tfs connection . get client ( 'vsts.tfvc.v4 1.tfvc client.Tfvc Client' ) if tfs tfvc client is None : msg = 'Unable to create TFS Git Client, failed to connect to TFS Enterprise (%s) with provided token.' raise Runtime Error ( msg , url ) return tfs tfvc client", "predictions": ["create a tfs client client ."], "references": ["creates a tfs tfvc client to pull tfvc repo info"], "bleu": 0.16038842424444547, "rouge_l": 0.3588235294117647}
{"id": 2629, "code": "def get git repos ( url , token , collection , project ) : git client = create tfs git client ( '{url}/{collection name}' . format ( url = url , collection name = collection . name ) , token ) logger . debug ( 'Retrieving Git Repos for Project: {project name}' . format ( project name = project . name ) ) return git client . get repositories ( project . id )", "predictions": ["get git repos for the specified url ."], "references": ["returns a list of all git repos for the supplied project within the supplied collection"], "bleu": 0.1860553630727644, "rouge_l": 0.32972972972972975}
{"id": 2630, "code": "def get tfvc repos ( url , token , collection , project ) : branch list = [ ] tfvc client = create tfs tfvc client ( '{url}/{collection name}' . format ( url = url , collection name = collection . name ) , token ) logger . debug ( 'Retrieving Tfvc Branches for Project: {project name}' . format ( project name = project . name ) ) branches = tfvc client . get branches ( project . id , True , True , False , True ) if branches : branch list . extend ( branches ) else : logger . debug ( 'No Tfvcc Branches in Project: {project name}' . format ( project name = project . name ) ) return branch list", "predictions": ["get tfvc branch list for a project ."], "references": ["returns a list of all tfvc branches for the supplied project within the supplied collection"], "bleu": 0.08798014921805314, "rouge_l": 0.24729729729729732}
{"id": 2631, "code": "def write to file ( self ) : with open ( '../github stats output/last year commits.csv' , 'w+' ) as output : output . write ( 'date,organization,repos,members,teams,' + 'unique contributors,total contributors,forks,' + 'stargazers,pull requests,open issues,has readme,' + 'has license,pull requests open,pull requests closed,' + 'commits\\n' ) #no reverse this time to print oldest first previous commits = 0 for week in self . sorted weeks : if str ( self . commits [ week ] ) != previous commits : #delete dups week formatted = datetime . datetime . utcfromtimestamp ( week ) . strftime ( '%Y-%m-%d' ) output . write ( week formatted + ',llnl,0,0,0,0,0,0,0,0,0,0,0,0,0,' + str ( self . commits [ week ] ) + '\\n' ) previous commits = str ( self . commits [ week ] )", "predictions": ["write the week week to disk"], "references": ["writes the weeks with associated commits to file ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 2632, "code": "def incr ( self , stat , value = 1 , tags = None ) : self . client . incr ( stat = stat , count = value )", "predictions": ["incr a stat ."], "references": ["increment a counter ."], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 2633, "code": "def timing ( self , stat , value , tags = None ) : self . client . timing ( stat = stat , delta = value )", "predictions": ["setter for the timing method ."], "references": ["measure a timing for statistical distribution ."], "bleu": 0.22236312185643822, "rouge_l": 0.3034825870646766}
{"id": 2634, "code": "def incr ( self , stat , value = 1 , tags = None ) : self . client . increment ( metric = stat , value = value , tags = tags )", "predictions": ["increment the value of a stat ."], "references": ["increment a counter ."], "bleu": 0.22089591134157885, "rouge_l": 0.5736677115987461}
{"id": 2635, "code": "def gauge ( self , stat , value , tags = None ) : self . client . gauge ( metric = stat , value = value , tags = tags )", "predictions": ["setter for a gauge entry ."], "references": ["set a gauge ."], "bleu": 0.31239399369202553, "rouge_l": 0.6224489795918368}
{"id": 2636, "code": "def timing ( self , stat , value , tags = None ) : self . client . timing ( metric = stat , value = value , tags = tags )", "predictions": ["setter for updating timing timing ."], "references": ["measure a timing for statistical distribution ."], "bleu": 0.22236312185643822, "rouge_l": 0.3034825870646766}
{"id": 2637, "code": "def histogram ( self , stat , value , tags = None ) : self . client . histogram ( metric = stat , value = value , tags = tags )", "predictions": ["write histogram histogram ."], "references": ["measure a value for statistical distribution ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 2638, "code": "def incr ( self , stat , value = 1 , tags = None ) : self . log ( 'incr' , stat , value , tags )", "predictions": ["increment a incr entry ."], "references": ["increment a counter ."], "bleu": 0.3860973950960897, "rouge_l": 0.6802973977695167}
{"id": 2639, "code": "def gauge ( self , stat , value , tags = None ) : self . log ( 'gauge' , stat , value , tags )", "predictions": ["setter for . attribute"], "references": ["set a gauge ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 2640, "code": "def timing ( self , stat , value , tags = None ) : self . log ( 'timing' , stat , value , tags )", "predictions": ["logs a timing value in the timing timing ."], "references": ["report a timing ."], "bleu": 0.22089591134157885, "rouge_l": 0.4959349593495934}
{"id": 2641, "code": "def histogram ( self , stat , value , tags = None ) : self . log ( 'histogram' , stat , value , tags )", "predictions": ["write a histogram entry"], "references": ["report a histogram ."], "bleu": 0.47287080450158786, "rouge_l": 0.5}
{"id": 2642, "code": "def rollup ( self ) : now = time . time ( ) if now < self . next rollup : return self . next rollup = now + self . flush interval for key , values in sorted ( self . incr stats . items ( ) ) : self . logger . info ( '%s INCR %s: count:%d|rate:%d/%d' , self . leader , key , len ( values ) , sum ( values ) , self . flush interval ) self . incr stats [ key ] = [ ] for key , values in sorted ( self . gauge stats . items ( ) ) : if values : self . logger . info ( '%s GAUGE %s: count:%d|current:%s|min:%s|max:%s' , self . leader , key , len ( values ) , values [ - 1 ] , min ( values ) , max ( values ) , ) else : self . logger . info ( '%s (gauge) %s: no data' , self . leader , key ) self . gauge stats [ key ] = [ ] for key , values in sorted ( self . histogram stats . items ( ) ) : if values : self . logger . info ( ( '%s HISTOGRAM %s: ' 'count:%d|min:%.2f|avg:%.2f|median:%.2f|ninety-five:%.2f|max:%.2f' ) , self . leader , key , len ( values ) , min ( values ) , statistics . mean ( values ) , statistics . median ( values ) , values [ int ( len ( values ) * 95 / 100 ) ] , max ( values ) ) else : self . logger . info ( '%s (histogram) %s: no data' , self . leader , key ) self . histogram stats [ key ] = [ ]", "predictions": ["rollup the histogram stats ."], "references": ["roll up stats and log them ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 2643, "code": "def incr ( self , stat , value = 1 , tags = None ) : self . rollup ( ) self . incr stats . setdefault ( stat , [ ] ) . append ( value )", "predictions": ["incr a stat ."], "references": ["increment a counter ."], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 2644, "code": "def gauge ( self , stat , value , tags = None ) : self . rollup ( ) self . gauge stats . setdefault ( stat , [ ] ) . append ( value )", "predictions": ["adds a gauge entry to the gauge list ."], "references": ["set a gauge ."], "bleu": 0.19960198807747329, "rouge_l": 0.4959349593495934}
{"id": 2645, "code": "def histogram ( self , stat , value , tags = None ) : self . rollup ( ) self . histogram stats . setdefault ( stat , [ ] ) . append ( value )", "predictions": ["set new histogram entry ."], "references": ["measure a value for statistical distribution ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 2646, "code": "def from db value ( self , value , expression , connection , context ) : if value is None : return value return self . enum [ value ]", "predictions": ["initialize instance from database ."], "references": ["convert a string from the database into an enum value"], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 2647, "code": "def to python ( self , value ) : if value is None : return value if isinstance ( value , self . enum ) : return value return self . enum [ value ]", "predictions": ["converts the enum value to a string ."], "references": ["convert a string from a form into an enum value ."], "bleu": 0.1909027782642041, "rouge_l": 0.3070469798657718}
{"id": 2648, "code": "def get prep value ( self , value ) : if value is None : return None if isinstance ( value , self . enum ) : return value . name raise Value Error ( \"Unknown value {value:r} of type {cls}\" . format ( value = value , cls = type ( value ) ) )", "predictions": ["convert the value to a database object ."], "references": ["convert an enum value into a string for the database"], "bleu": 0.1643685581109115, "rouge_l": 0.43571428571428567}
{"id": 2649, "code": "def t parse ( self , s ) : with self . lock : try : return self . parser . parse ( s , lexer = self . lexer , debug = False ) except Cannot Parse as e : e . s = s raise e", "predictions": ["parse an integer ."], "references": ["parses the input string and returns a reference to the created ast s root"], "bleu": 0.0248009595334312, "rouge_l": 0.0}
{"id": 2650, "code": "def parse ( self , s ) : with self . lock : try : return self . parser . parse ( s , lexer = self . lexer ) except Invalid IEML Object Argument as e : raise Cannot Parse ( s , str ( e ) ) except Cannot Parse as e : e . s = s raise e", "predictions": ["parse a string ."], "references": ["parses the input string and returns a reference to the created ast s root"], "bleu": 0.032639898338235177, "rouge_l": 0.10099337748344371}
{"id": 2651, "code": "def resolve path ( obj , path ) : if obj . class not in path . context . accept : result = set ( ) for ctx in path . context . accept : result |= { e for u in obj [ ctx ] for e in resolve path ( u , path ) } return result if isinstance ( obj , Text ) : if path . index is not None : return { obj . children [ path . index ] } return set ( obj . children ) if isinstance ( obj , ( Fact , Theory ) ) : return resolve path tree graph ( obj . tree graph , path ) if isinstance ( obj , Topic ) : if path . kind == 'r' : if path . index is not None : return { obj . root [ path . index ] } return set ( obj . root ) else : if path . index is not None : return { obj . flexing [ path . index ] } return set ( obj . flexing )", "predictions": ["resolve the path to the path of the path to the path"], "references": ["path is a mul of coord or a coord"], "bleu": 0.11498759556447223, "rouge_l": 0.19551282051282048}
{"id": 2652, "code": "def mean ( self ) : if self . counter . value > 0 : return self . sum . value / self . counter . value return 0.0", "predictions": ["mean of the distribution ."], "references": ["returns the mean value ."], "bleu": 0.32466791547509893, "rouge_l": 0.4}
{"id": 2653, "code": "def mean rate ( self ) : if self . counter . value == 0 : return 0.0 else : elapsed = time ( ) - self . start time return self . counter . value / elapsed", "predictions": ["returns the mean rate of the mean"], "references": ["returns the mean rate of the events since the start of the process ."], "bleu": 0.30934850332660563, "rouge_l": 0.5390279823269514}
{"id": 2654, "code": "def send metric ( self , name , metric ) : config = SERIALIZER CONFIG [ class name ( metric ) ] mmap ( self . buffered send metric , self . serialize metric ( metric , name , config [ 'keys' ] , config [ 'serialized type' ] ) ) if hasattr ( metric , 'snapshot' ) and config . get ( 'snapshot keys' ) : mmap ( self . buffered send metric , self . serialize metric ( metric . snapshot , name , config [ 'snapshot keys' ] , config [ 'serialized type' ] ) )", "predictions": ["send a metric to the buffered ."], "references": ["send metric and its snapshot ."], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 2655, "code": "def serialize metric ( self , metric , m name , keys , m type ) : return [ self . format metric string ( m name , getattr ( metric , key ) , m type ) for key in keys ]", "predictions": ["serialize a metric instance into json"], "references": ["serialize and send available measures of a metric ."], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 2656, "code": "def format metric string ( self , name , value , m type ) : template = '{name}:{value}|{m type}\\n' if self . prefix : name = \"{prefix}.{m name}\" . format ( prefix = self . prefix , m name = name ) return template . format ( name = name , value = value , m type = m type )", "predictions": ["query the repos string for the given repos yield ."], "references": ["compose a statsd compatible string for a metric s measurement ."], "bleu": 0.1613101715871968, "rouge_l": 0.28328173374613}
{"id": 2657, "code": "def buffered send metric ( self , metric str ) : self . batch count += 1 self . batch buffer += metric str if self . batch count >= self . batch size : self . send ( )", "predictions": ["dict - like null - safe null null - type - c - batch"], "references": ["add a metric to the buffer ."], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 2658, "code": "def json safe ( data ) : if not hasattr ( data , 'encode' ) : try : data = data . decode ( 'utf-8' ) except Unicode Decode Error : raise Value Error ( 'Expected valid UTF8 for JSON data, got %r' % ( data , ) ) return data", "predictions": [". create create data"], "references": ["json . loads wants an unistr in python3 . convert it ."], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 2659, "code": "def solve ( grid ) : clauses = sudoku clauses ( ) for i in range ( 1 , 10 ) : for j in range ( 1 , 10 ) : d = grid [ i - 1 ] [ j - 1 ] if d : clauses . append ( [ v ( i , j , d ) ] ) sol = set ( pycosat . solve ( clauses ) ) def read cell ( i , j ) : for d in range ( 1 , 10 ) : if v ( i , j , d ) in sol : return d for i in range ( 1 , 10 ) : for j in range ( 1 , 10 ) : grid [ i - 1 ] [ j - 1 ] = read cell ( i , j )", "predictions": ["create a set from a git git git git git git git git git git git git git ."], "references": ["solve a sudoku grid inplace"], "bleu": 0.06439931429457924, "rouge_l": 0.09312977099236641}
{"id": 2660, "code": "def view ( injector ) : handler = create handler ( View , injector ) apply http methods ( handler , injector ) return injector . let ( as view = handler . as view )", "predictions": ["returns a . . . . . . . . . . . . create a . . . . . . . . . . . . . . ."], "references": ["create django class - based view from injector class ."], "bleu": 0.04317900023606586, "rouge_l": 0.10748898678414096}
{"id": 2661, "code": "def form view ( injector ) : handler = create handler ( Form View , injector ) apply form methods ( handler , injector ) return injector . let ( as view = handler . as view )", "predictions": ["returns a let instance for the given injector injector . . . . . . . ."], "references": ["create django form processing class - based view from injector class ."], "bleu": 0.07994607499472013, "rouge_l": 0.1423570595099183}
{"id": 2662, "code": "def method view ( injector ) : handler = create handler ( Method View ) apply http methods ( handler , injector ) return injector . let ( as view = handler . as view )", "predictions": ["returns a create view instance client client client client client client client client client client client client client client client client client client client client client client client client client client"], "references": ["create flask method based dispatching view from injector class ."], "bleu": 0.04317900023606586, "rouge_l": 0.10748898678414096}
{"id": 2663, "code": "def api view ( injector ) : handler = create handler ( API View , injector ) apply http methods ( handler , injector ) apply api view methods ( handler , injector ) return injector . let ( as view = handler . as view )", "predictions": ["to show the write methods as a output of the write injector as a output as a string"], "references": ["create drf class - based api view from injector class ."], "bleu": 0.06809398432036522, "rouge_l": 0.07210401891252956}
{"id": 2664, "code": "def generic api view ( injector ) : handler = create handler ( Generic API View , injector ) apply http methods ( handler , injector ) apply api view methods ( handler , injector ) apply generic api view methods ( handler , injector ) return injector . let ( as view = handler . as view )", "predictions": ["returns a let object for the given stat . . . . . . . . . ."], "references": ["create drf generic class - based api view from injector class ."], "bleu": 0.06809398432036522, "rouge_l": 0.06916099773242629}
{"id": 2665, "code": "def model view set ( injector ) : handler = create handler ( Model View Set , injector ) apply api view methods ( handler , injector ) apply generic api view methods ( handler , injector ) apply model view set methods ( handler , injector ) return injector . let ( as viewset = lambda : handler )", "predictions": ["self . . ."], "references": ["create drf model view set from injector class ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 2666, "code": "def stream from fd ( fd , loop ) : reader = asyncio . Stream Reader ( loop = loop ) protocol = asyncio . Stream Reader Protocol ( reader , loop = loop ) waiter = asyncio . futures . Future ( loop = loop ) transport = Unix File Descriptor Transport ( loop = loop , fileno = fd , protocol = protocol , waiter = waiter , ) try : yield from waiter except Exception : transport . close ( ) if loop . get debug ( ) : logger . debug ( \"Read fd %r connected: (%r, %r)\" , fd , transport , protocol ) return reader , transport", "predictions": ["create a asyncio instance from an asyncio . . . . . . . . . ."], "references": ["recieve a streamer for a given file descriptor ."], "bleu": 0.07994607499472013, "rouge_l": 0.1628838451268358}
{"id": 2667, "code": "def read ready ( self ) : try : data = os . read ( self . fileno , self . max size ) except Interrupted Error : pass except OS Error as exc : self . fatal error ( exc , \"Fatal read error on file descriptor read\" ) else : if data : self . protocol . data received ( data ) else : if self . loop . get debug ( ) : logger . info ( \"%r was closed by the kernel\" , self ) self . closing = False self . pause reading ( ) self . loop . call soon ( self . protocol . eof received ) self . loop . call soon ( self . call connection lost , None )", "predictions": ["gauge gauge when the connection is available . ."], "references": ["called by the event loop whenever the fd is ready for reading ."], "bleu": 0.10761866342063775, "rouge_l": 0.26406926406926406}
{"id": 2668, "code": "def close ( self , error = None ) : self . closing = True self . pause reading ( ) self . loop . call soon ( self . call connection lost , error )", "predictions": ["timing method that closes the websocket connection . . . . . . . ."], "references": ["actual closing code both from manual close and errors ."], "bleu": 0.08225964699966554, "rouge_l": 0.08299319727891155}
{"id": 2669, "code": "def watch ( self , path , flags , * , alias = None ) : if alias is None : alias = path if alias in self . requests : raise Value Error ( \"A watch request is already scheduled for alias %s\" % alias ) self . requests [ alias ] = ( path , flags ) if self . fd is not None : self . setup watch ( alias , path , flags )", "predictions": ["setter method ; for a description see the getter method ."], "references": ["add a new watching rule ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 2670, "code": "def unwatch ( self , alias ) : if alias not in self . descriptors : raise Value Error ( \"Unknown watch alias %s; current set is %r\" % ( alias , list ( self . descriptors . keys ( ) ) ) ) wd = self . descriptors [ alias ] errno = Lib C . inotify rm watch ( self . fd , wd ) if errno != 0 : raise IO Error ( \"Failed to close watcher %d: errno=%d\" % ( wd , errno ) ) del self . descriptors [ alias ] del self . requests [ alias ] del self . aliases [ wd ]", "predictions": ["remove an stat stat . . . . ."], "references": ["stop watching a given rule ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 2671, "code": "def setup watch ( self , alias , path , flags ) : assert alias not in self . descriptors , \"Registering alias %s twice!\" % alias wd = Lib C . inotify add watch ( self . fd , path , flags ) if wd < 0 : raise IO Error ( \"Error setting up watch on %s with flags %s: wd=%s\" % ( path , flags , wd ) ) self . descriptors [ alias ] = wd self . aliases [ wd ] = alias", "predictions": ["gauge can be a watch instance ."], "references": ["actual rule setup ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 2672, "code": "def setup ( self , loop ) : self . loop = loop self . fd = Lib C . inotify init ( ) for alias , ( path , flags ) in self . requests . items ( ) : self . setup watch ( alias , path , flags ) self . stream , self . transport = yield from aioutils . stream from fd ( self . fd , loop )", "predictions": ["timing timing and yield requests"], "references": ["start the watcher registering new watches if any ."], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 2673, "code": "def touch ( self ) : assert not self . has responded self . trigger ( event . TOUCH , message = self )", "predictions": ["histogram the user to be invoked = false = false ."], "references": ["respond to nsqd that you need more time to process the message ."], "bleu": 0.11294012253658708, "rouge_l": 0.1641991924629879}
{"id": 2674, "code": "def success ( self ) : if self . interval == 0.0 : return self . short interval -= self . short unit self . long interval -= self . long unit self . short interval = max ( self . short interval , Decimal ( 0 ) ) self . long interval = max ( self . long interval , Decimal ( 0 ) ) self . update interval ( )", "predictions": ["set the success and in the success = 0 = 1 = 1 = 1 = 0 = 0 = 1 = 1 = 1 = 1 = 1 = 1"], "references": ["update the timer to reflect a successfull call"], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 2675, "code": "def failure ( self ) : self . short interval += self . short unit self . long interval += self . long unit self . short interval = min ( self . short interval , self . max short timer ) self . long interval = min ( self . long interval , self . max long timer ) self . update interval ( )", "predictions": ["set the values of the promise to the next value value value value value value value value value value value value"], "references": ["update the timer to reflect a failed call"], "bleu": 0.06429451441231726, "rouge_l": 0.15006150061500614}
{"id": 2676, "code": "def close ( self ) : for conn in self . conns . values ( ) : conn . close ( ) self . redist periodic . stop ( ) if self . query periodic is not None : self . query periodic . stop ( )", "predictions": ["closes the connection . . . . . ."], "references": ["closes all connections stops all periodic callbacks"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 2677, "code": "def set max in flight ( self , max in flight ) : assert isinstance ( max in flight , int ) self . max in flight = max in flight if max in flight == 0 : for conn in itervalues ( self . conns ) : if conn . rdy > 0 : logger . debug ( '[%s:%s] rdy: %d -> 0' , conn . id , self . name , conn . rdy ) self . send rdy ( conn , 0 ) self . total rdy = 0 else : self . need rdy redistributed = True self . redistribute rdy state ( )", "predictions": ["histogram max self ."], "references": ["dynamically adjust the reader max_in_flight . set to 0 to immediately disable a reader"], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 2678, "code": "def score function ( self , x , W ) : if ( self . svm kernel == 'polynomial kernel' or self . svm kernel == 'gaussian kernel' or self . svm kernel == 'soft polynomial kernel' or self . svm kernel == 'soft gaussian kernel' ) : x = x [ 1 : ] score = np . sign ( np . sum ( self . sv alpha * self . sv Y * utility . Kernel . kernel matrix x X ( self , x , self . sv X ) ) + self . sv avg b ) else : score = np . sign ( np . inner ( x , W ) ) return score", "predictions": ["from the from the from the is the from the is the is the from the is the from the from the is the from the is the is the from"], "references": ["score function to calculate score"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2679, "code": "def score function ( self , x , W ) : score = super ( Binary Classifier , self ) . score function ( x , W ) if score >= 0.5 : score = 1.0 else : score = - 1.0 return score", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["score function to calculate score"], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 2680, "code": "def score function ( self , x , W ) : score = self . sign * np . sign ( x [ self . feature index ] - self . theta ) return score", "predictions": ["get the get get get get get get get get get get get get get get get get get get get get get get get get get get get get get"], "references": ["score function to calculate score"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2681, "code": "def set feature transform ( self , mode = 'polynomial' , degree = 1 ) : if self . status != 'load train data' : print ( \"Please load train data first.\" ) return self . train X self . feature transform mode = mode self . feature transform degree = degree self . train X = self . train X [ : , 1 : ] self . train X = utility . Dataset Loader . feature transform ( self . train X , self . feature transform mode , self . feature transform degree ) return self . train X", "predictions": ["t the training and except it s except it s except it is not a parse"], "references": ["transform data feature to high level"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 2682, "code": "def score function ( self , x , W ) : score = self . theta ( np . inner ( x , W ) ) return score", "predictions": ["parse the parse function at x try try to the given number of x try to the given number of x try"], "references": ["score function to calculate score"], "bleu": 0.0612957497932821, "rouge_l": 0.16712328767123286}
{"id": 2683, "code": "def clean up ( fastq pairs , clear ) : unpaired fastq = [ f for f in os . listdir ( \".\" ) if f . endswith ( \" U.fastq.gz\" ) ] for fpath in unpaired fastq : os . remove ( fpath ) expected out = [ f for f in os . listdir ( \".\" ) if f . endswith ( \" trim.fastq.gz\" ) ] if clear == \"true\" and len ( expected out ) == 2 : for fq in fastq pairs : rp = os . path . realpath ( fq ) logger . debug ( \"Removing temporary fastq file path: {}\" . format ( rp ) ) if re . match ( \".*/work/.{2}/.{30}/.*\" , rp ) : os . remove ( rp )", "predictions": ["remove root pairs pairs pairs"], "references": ["cleans the working directory of unwanted temporary files"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 2684, "code": "def check required files ( self ) : if not os . path . exists ( self . trace file ) : raise eh . Inspection Error ( \"The provided trace file could not be \" \"opened: {}\" . format ( self . trace file ) ) if not os . path . exists ( self . log file ) : raise eh . Inspection Error ( \"The .nextflow.log files could not be \" \"opened. Are you sure you are in a \" \"nextflow project directory?\" )", "predictions": ["mean of .nextflow.log self 0 0"], "references": ["checks whetner the trace and log files are available"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 2685, "code": "def clear inspect ( self ) : self . trace info = defaultdict ( list ) self . process tags = { } self . process stats = { } self . samples = [ ] self . stored ids = [ ] self . stored log ids = [ ] self . time start = None self . time stop = None self . execution command = None self . nextflow version = None self . abort cause = None self . c = 0 for p in self . processes . values ( ) : p [ \"barrier\" ] = \"W\" for i in [ \"submitted\" , \"finished\" , \"failed\" , \"retry\" ] : p [ i ] = set ( )", "predictions": ["clears the processes of all processes"], "references": ["clears inspect attributes when re - executing a pipeline"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 2686, "code": "def update barrier status ( self ) : with open ( self . log file ) as fh : for line in fh : if \"Session aborted\" in line : return if \"<<< barrier arrive\" in line : process m = re . match ( \".*process: (.*)\\)\" , line ) if process m : process = process m . group ( 1 ) if process in self . processes : self . processes [ process ] [ \"barrier\" ] = \"C\"", "predictions": ["send the metric status to the metric = true"], "references": ["checks whether the channels to each process have been closed ."], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 2687, "code": "def display overview ( self ) : stay alive = True self . screen = curses . initscr ( ) self . screen . keypad ( True ) self . screen . nodelay ( - 1 ) curses . cbreak ( ) curses . noecho ( ) curses . start color ( ) self . screen lines = self . screen . getmaxyx ( ) [ 0 ] try : while stay alive : self . curses keybindings ( ) self . update inspection ( ) self . flush overview ( ) sleep ( self . refresh rate ) except File Not Found Error : sys . stderr . write ( colored print ( \"ERROR: nextflow log and/or trace files are no longer \" \"reachable!\" , \"red bold\" ) ) except Exception as e : sys . stderr . write ( str ( e ) ) finally : curses . nocbreak ( ) self . screen . keypad ( 0 ) curses . echo ( ) curses . endwin ( )", "predictions": ["serialize curses to curses . ."], "references": ["displays the default pipeline inspection overview"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2688, "code": "def updown ( self , direction ) : if direction == \"up\" and self . top line != 0 : self . top line -= 1 elif direction == \"down\" and self . screen . getmaxyx ( ) [ 0 ] + self . top line <= self . content lines + 3 : self . top line += 1", "predictions": ["set top line number of screen direction ."], "references": ["provides curses scroll functionality ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 2689, "code": "def rightleft ( self , direction ) : if direction == \"left\" and self . padding != 0 : self . padding -= 1 if direction == \"right\" and self . screen . getmaxyx ( ) [ 1 ] + self . padding < self . max width : self . padding += 1", "predictions": ["set new padding and reset padding ."], "references": ["provides curses horizontal padding"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 2690, "code": "def get run hash ( self ) : pipeline path = get nextflow filepath ( self . log file ) pipeline hash = hashlib . md5 ( ) with open ( pipeline path , \"rb\" ) as fh : for chunk in iter ( lambda : fh . read ( 4096 ) , b\"\" ) : pipeline hash . update ( chunk ) workdir = self . workdir . encode ( \"utf8\" ) hostname = socket . gethostname ( ) . encode ( \"utf8\" ) hardware addr = str ( uuid . getnode ( ) ) . encode ( \"utf8\" ) dir hash = hashlib . md5 ( workdir + hostname + hardware addr ) return pipeline hash . hexdigest ( ) + dir hash . hexdigest ( )", "predictions": ["get the hash of the pipeline ."], "references": ["gets the hash of the nextflow file"], "bleu": 0.5169731539571706, "rouge_l": 0.5714285714285714}
{"id": 2691, "code": "def write report data ( self ) : json plot = self . get plot data ( ) json table = self . get table data ( ) json dic = { * * json plot , * * json table } with open ( \".report.json\" , \"w\" ) as json report : json report . write ( json . dumps ( json dic , separators = ( \",\" , \":\" ) ) )", "predictions": ["writes the report data to the json file"], "references": ["writes the json report to a json file"], "bleu": 0.31020161970069987, "rouge_l": 0.75}
{"id": 2692, "code": "def build header ( self ) : logger . debug ( \"===============\" ) logger . debug ( \"Building header\" ) logger . debug ( \"===============\" ) self . template += hs . header", "predictions": ["build the header for the header\" ."], "references": ["adds the header template to the master template string"], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 2693, "code": "def build footer ( self ) : logger . debug ( \"===============\" ) logger . debug ( \"Building header\" ) logger . debug ( \"===============\" ) self . template += fs . footer", "predictions": ["build the footer footer ."], "references": ["adds the footer template to the master template string"], "bleu": 0.1614457444314309, "rouge_l": 0.2717149220489978}
{"id": 2694, "code": "def set status channels ( self ) : status inst = pc . Status Compiler ( template = \"status compiler\" ) report inst = pc . Report Compiler ( template = \"report compiler\" ) status channels = [ ] for p in [ p for p in self . processes ] : if not any ( [ isinstance ( p , x ) for x in self . skip class ] ) : status channels . extend ( p . status strs ) if not status channels : logger . debug ( \"No status channels found. Skipping status compiler\" \"process\" ) return logger . debug ( \"Setting status channels: {}\" . format ( status channels ) ) if len ( status channels ) != len ( set ( status channels ) ) : raise eh . Process Error ( \"Duplicate status channels detected. Please ensure that \" \"the 'status channels' attributes of each process are \" \"unique. Here are the status channels:\\n\\n{}\" . format ( \", \" . join ( status channels ) ) ) status inst . set compiler channels ( status channels ) report channels = [ \"REPORT {}\" . format ( x . lstrip ( \"STATUS \" ) ) for x in status channels ] report inst . set compiler channels ( report channels ) self . processes . extend ( [ status inst , report inst ] )", "predictions": ["set the status of all processes channels ."], "references": ["compiles all status channels for the status compiler process"], "bleu": 0.2116253761537182, "rouge_l": 0.232824427480916}
{"id": 2695, "code": "def export directives ( self ) : directives json = { } for p in self . processes [ 1 : ] : directives json [ p . template ] = p . directives sys . stdout . write ( json . dumps ( directives json ) )", "predictions": ["export directives to json"], "references": ["export pipeline directives as a json to stdout"], "bleu": 0.16620830006469264, "rouge_l": 0.47164948453608246}
{"id": 2696, "code": "def get report id ( self ) : if self . watch : pipeline path = get nextflow filepath ( self . log file ) pipeline hash = hashlib . md5 ( ) with open ( pipeline path , \"rb\" ) as fh : for chunk in iter ( lambda : fh . read ( 4096 ) , b\"\" ) : pipeline hash . update ( chunk ) workdir = os . getcwd ( ) . encode ( \"utf8\" ) hostname = socket . gethostname ( ) . encode ( \"utf8\" ) hardware addr = str ( uuid . getnode ( ) ) . encode ( \"utf8\" ) dir hash = hashlib . md5 ( workdir + hostname + hardware addr ) return pipeline hash . hexdigest ( ) + dir hash . hexdigest ( ) else : with open ( self . report file ) as fh : report json = json . loads ( fh . read ( ) ) metadata = report json [ \"data\" ] [ \"results\" ] [ 0 ] [ \"nf Metadata\" ] try : report id = metadata [ \"script Id\" ] + metadata [ \"session Id\" ] except Key Error : raise eh . Report Error ( \"Incomplete or corrupt report JSON file \" \"missing the 'script Id' and/or 'session Id' \" \"metadata information\" ) return report id", "predictions": ["returns the report id of the pipeline ."], "references": ["returns a hash of the reports json file"], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 2697, "code": "def update log watch ( self ) : size stamp = os . path . getsize ( self . log file ) self . trace retry = 0 if size stamp and size stamp == self . log sizestamp : return else : logger . debug ( \"Updating log size stamp to: {}\" . format ( size stamp ) ) self . log sizestamp = size stamp self . update pipeline status ( )", "predictions": ["update the log watch watch ."], "references": ["parses nextflow log file and updates the run status"], "bleu": 0.14827340167306757, "rouge_l": 0.12869198312236285}
{"id": 2698, "code": "def map w to data ( self ) : self . Wmapped index = vq ( self . data , self . W ) self . Wmapped = np . zeros ( self . W . shape ) for i , s in enumerate ( self . Wmapped index ) : self . Wmapped [ : , i ] = self . data [ : , s ]", "predictions": ["map data to data"], "references": ["return data points that are most similar to basis vectors w"], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 2699, "code": "def median filter ( X , M = 8 ) : for i in range ( X . shape [ 1 ] ) : X [ : , i ] = filters . median filter ( X [ : , i ] , size = M ) return X", "predictions": ["median a median filter ."], "references": ["median filter along the first axis of the feature matrix x ."], "bleu": 0.09521044541645862, "rouge_l": 0.3285457809694794}
{"id": 2700, "code": "def compute gaussian krnl ( M ) : g = signal . gaussian ( M , M // 3. , sym = True ) G = np . dot ( g . reshape ( - 1 , 1 ) , g . reshape ( 1 , - 1 ) ) G [ M // 2 : , : M // 2 ] = - G [ M // 2 : , : M // 2 ] G [ : M // 2 , M // 2 : ] = - G [ : M // 2 , M // 2 : ] return G", "predictions": ["compute the gaussian krnl krnl ."], "references": ["creates a gaussian kernel following foote s paper ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 2701, "code": "def compute ssm ( X , metric = \"seuclidean\" ) : D = distance . pdist ( X , metric = metric ) D = distance . squareform ( D ) D /= D . max ( ) return 1 - D", "predictions": ["compute the ssm of the ssm algorithm ."], "references": ["computes the self - similarity matrix of x ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 2702, "code": "def pick peaks ( nc , L = 16 ) : offset = nc . mean ( ) / 20. nc = filters . gaussian filter1d ( nc , sigma = 4 ) th = filters . median filter ( nc , size = L ) + offset #th = filters.gaussian filter(nc, sigma=L/2., mode=\"nearest\") + offset peaks = [ ] for i in range ( 1 , nc . shape [ 0 ] - 1 ) : if nc [ i - 1 ] < nc [ i ] and nc [ i ] > nc [ i + 1 ] : if nc [ i ] > th [ i ] : peaks . append ( i ) #plt.plot(nc) #plt.plot(th) #for peak in peaks: #plt.axvline(peak) #plt.show() return peaks", "predictions": ["pick a list of peaks peaks to be used in the gaussian distribution"], "references": ["obtain peaks from a novelty curve using an adaptive threshold ."], "bleu": 0.10571070857151538, "rouge_l": 0.08460471567267684}
{"id": 2703, "code": "def gaussian filter ( X , M = 8 , axis = 0 ) : for i in range ( X . shape [ axis ] ) : if axis == 1 : X [ : , i ] = filters . gaussian filter ( X [ : , i ] , sigma = M / 2. ) elif axis == 0 : X [ i , : ] = filters . gaussian filter ( X [ i , : ] , sigma = M / 2. ) return X", "predictions": ["gaussian filter filter ."], "references": ["gaussian filter along the first axis of the feature matrix x ."], "bleu": 0.06876828939330318, "rouge_l": 0.34398496240601506}
{"id": 2704, "code": "def compute nc ( X ) : N = X . shape [ 0 ] nc = np . zeros ( N ) for i in range ( N - 1 ) : nc [ i ] = distance . euclidean ( X [ i , : ] , X [ i + 1 , : ] ) nc += np . abs ( nc . min ( ) ) nc /= float ( nc . max ( ) ) return nc", "predictions": ["compute the nc of the nc"], "references": ["computes the novelty curve from the structural features ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 2705, "code": "def pick peaks ( nc , L = 16 , offset denom = 0.1 ) : offset = nc . mean ( ) * float ( offset denom ) th = filters . median filter ( nc , size = L ) + offset #th = filters.gaussian filter(nc, sigma=L/2., mode=\"nearest\") + offset #import pylab as plt #plt.plot(nc) #plt.plot(th) #plt.show() peaks = [ ] for i in range ( 1 , nc . shape [ 0 ] - 1 ) : if nc [ i - 1 ] < nc [ i ] and nc [ i ] > nc [ i + 1 ] : if nc [ i ] > th [ i ] : peaks . append ( i ) return peaks", "predictions": ["pick a list of peaks peaks to be added to the current sample"], "references": ["obtain peaks from a novelty curve using an adaptive threshold ."], "bleu": 0.10571070857151538, "rouge_l": 0.08460471567267684}
{"id": 2706, "code": "def embedded space ( X , m , tau = 1 ) : N = X . shape [ 0 ] - int ( np . ceil ( m ) ) Y = np . zeros ( ( N , int ( np . ceil ( X . shape [ 1 ] * m ) ) ) ) for i in range ( N ) : rem = int ( ( m % 1 ) * X . shape [ 1 ] ) Y [ i , : ] = np . concatenate ( ( X [ i : i + int ( m ) , : ] . flatten ( ) , X [ i + int ( m ) , : rem ] ) ) return Y", "predictions": ["return embedded space between m and tau"], "references": ["time - delay embedding with m dimensions and tau delays ."], "bleu": 0.14834636222628117, "rouge_l": 0.32049036777583184}
{"id": 2707, "code": "def plot one track ( file struct , est times , est labels , boundaries id , labels id , title = None ) : import matplotlib . pyplot as plt bid lid = boundaries id if labels id is not None : bid lid += \" + \" + labels id try : jam = jams . load ( file struct . ref file ) ann = jam . search ( namespace = 'segment .*' ) [ 0 ] ref inters , ref labels = ann . to interval values ( ) ref times = utils . intervals to times ( ref inters ) all boundaries = [ ref times , est times ] all labels = [ ref labels , est labels ] algo ids = [ \"GT\" , bid lid ] except : logging . warning ( \"No references found in %s. Not plotting groundtruth\" % file struct . ref file ) all boundaries = [ est times ] all labels = [ est labels ] algo ids = [ bid lid ] N = len ( all boundaries ) for i , labels in enumerate ( all labels ) : all labels [ i ] = mir eval . util . index labels ( labels ) [ 0 ] cm = plt . get cmap ( 'gist rainbow' ) max label = max ( max ( labels ) for labels in all labels ) figsize = ( 8 , 4 ) plt . figure ( 1 , figsize = figsize , dpi = 120 , facecolor = 'w' , edgecolor = 'k' ) for i , boundaries in enumerate ( all boundaries ) : color = \"b\" if i == 0 : color = \"g\" for b in boundaries : plt . axvline ( b , i / float ( N ) , ( i + 1 ) / float ( N ) , color = color ) if labels id is not None : labels = all labels [ i ] inters = utils . times to intervals ( boundaries ) for label , inter in zip ( labels , inters ) : plt . axvspan ( inter [ 0 ] , inter [ 1 ] , ymin = i / float ( N ) , ymax = ( i + 1 ) / float ( N ) , alpha = 0.6 , color = cm ( label / float ( max label ) ) ) plt . axhline ( i / float ( N ) , color = \"k\" , linewidth = 1 ) plot formatting ( title , os . path . basename ( file struct . audio file ) , algo ids , all boundaries [ 0 ] [ - 1 ] , N , None )", "predictions": ["plot the track of the track of the bid track ."], "references": ["plots the results of one track with ground truth if it exists ."], "bleu": 0.11941964005964323, "rouge_l": 0.3283983849259758}
{"id": 2708, "code": "def get dataset files ( in path ) : audio files = [ ] for ext in ds config . audio exts : audio files += glob . glob ( os . path . join ( in path , ds config . audio dir , \"*\" + ext ) ) utils . ensure dir ( os . path . join ( in path , ds config . features dir ) ) utils . ensure dir ( os . path . join ( in path , ds config . estimations dir ) ) utils . ensure dir ( os . path . join ( in path , ds config . references dir ) ) file structs = [ ] for audio file in audio files : file structs . append ( File Struct ( audio file ) ) file structs = sorted ( file structs , key = lambda file struct : file struct . audio file ) return file structs", "predictions": ["return all dataset files in a directory"], "references": ["gets the files of the given dataset ."], "bleu": 0.17820132316770915, "rouge_l": 0.13174946004319654}
{"id": 2709, "code": "def get dataset file ( self , dir , ext ) : audio file ext = \".\" + self . audio file . split ( \".\" ) [ - 1 ] base file = os . path . basename ( self . audio file ) . replace ( audio file ext , ext ) return os . path . join ( self . ds path , dir , base file )", "predictions": ["get the dataset file name from the given dir"], "references": ["gets the desired dataset file ."], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 2710, "code": "def write features ( self ) : out json = collections . Ordered Dict ( ) try : self . read features ( ) except ( Wrong Features Format Error , Features Not Found , No Features File Error ) : out json = collections . Ordered Dict ( { \"metadata\" : { \"versions\" : { \"librosa\" : librosa . version , \"msaf\" : msaf . version , \"numpy\" : np . version } , \"timestamp\" : datetime . datetime . today ( ) . strftime ( \"%Y/%m/%d %H:%M:%S\" ) } } ) out json [ \"globals\" ] = { \"dur\" : self . dur , \"sample rate\" : self . sr , \"hop length\" : self . hop length , \"audio file\" : self . file struct . audio file } out json [ \"est beats\" ] = self . est beats times . tolist ( ) out json [ \"est beatsync times\" ] = self . est beatsync times . tolist ( ) if self . ann beats times is not None : out json [ \"ann beats\" ] = self . ann beats times . tolist ( ) out json [ \"ann beatsync times\" ] = self . ann beatsync times . tolist ( ) except Feature Params Error : with open ( self . file struct . features file ) as f : out json = json . load ( f ) finally : out json [ self . get id ( ) ] = { } out json [ self . get id ( ) ] [ \"params\" ] = { } for param name in self . get param names ( ) : value = getattr ( self , param name ) if hasattr ( value , ' call ' ) : value = value . name else : value = str ( value ) out json [ self . get id ( ) ] [ \"params\" ] [ param name ] = value out json [ self . get id ( ) ] [ \"framesync\" ] = self . framesync features . tolist ( ) out json [ self . get id ( ) ] [ \"est beatsync\" ] = self . est beatsync features . tolist ( ) if self . ann beatsync features is not None : out json [ self . get id ( ) ] [ \"ann beatsync\" ] = self . ann beatsync features . tolist ( ) with open ( self . file struct . features file , \"w\" ) as f : json . dump ( out json , f , indent = 2 )", "predictions": ["write the features in json format to json ."], "references": ["saves features to file ."], "bleu": 0.16784459625186196, "rouge_l": 0.4518518518518518}
{"id": 2711, "code": "def compute framesync times ( self ) : self . framesync times = librosa . core . frames to time ( np . arange ( self . framesync features . shape [ 0 ] ) , self . sr , self . hop length )", "predictions": ["compute the framesync times for the framesync ."], "references": ["computes the framesync times based on the framesync features ."], "bleu": 0.2872949152245254, "rouge_l": 0.6535714285714286}
{"id": 2712, "code": "def preprocess ( self , valid features = [ \"pcp\" , \"tonnetz\" , \"mfcc\" , \"cqt\" , \"tempogram\" ] ) : if self . feature str not in valid features : raise Runtime Error ( \"Feature %s in not valid for algorithm: %s \" \"(valid features are %s).\" % ( self . feature str , name , valid features ) ) else : try : F = self . features . features except Key Error : raise Runtime Error ( \"Feature %s in not supported by MSAF\" % ( self . feature str ) ) return F", "predictions": ["load the feature feature feature ."], "references": ["this method obtains the actual features ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 2713, "code": "def main ( ) : parser = argparse . Argument Parser ( description = \"Runs the speficied algorithm(s) on the MSAF \" \"formatted dataset.\" , formatter class = argparse . Argument Defaults Help Formatter ) parser . add argument ( \"in path\" , action = \"store\" , help = \"Input dataset\" ) parser . add argument ( \"-f\" , action = \"store\" , dest = \"feature\" , default = \"pcp\" , type = str , help = \"Type of features\" , choices = [ \"pcp\" , \"tonnetz\" , \"mfcc\" , \"cqt\" , \"tempogram\" ] ) parser . add argument ( \"-b\" , action = \"store true\" , dest = \"annot beats\" , help = \"Use annotated beats\" , default = False ) parser . add argument ( \"-fs\" , action = \"store true\" , dest = \"framesync\" , help = \"Use frame-synchronous features\" , default = False ) parser . add argument ( \"-bid\" , action = \"store\" , help = \"Boundary algorithm identifier\" , dest = \"boundaries id\" , default = \"gt\" , choices = [ \"gt\" ] + io . get all boundary algorithms ( ) ) parser . add argument ( \"-lid\" , action = \"store\" , help = \"Label algorithm identifier\" , dest = \"labels id\" , default = None , choices = io . get all label algorithms ( ) ) parser . add argument ( \"-j\" , action = \"store\" , dest = \"n jobs\" , default = 4 , type = int , help = \"The number of threads to use\" ) args = parser . parse args ( ) start time = time . time ( ) process ( args . in path , annot beats = args . annot beats , feature = args . feature , framesync = args . framesync , boundaries id = args . boundaries id , labels id = args . labels id , n jobs = args . n jobs ) logging . info ( \"Done! Took %.2f seconds.\" % ( time . time ( ) - start time ) )", "predictions": ["main function for the script ."], "references": ["main function to sweep parameters of a certain algorithm ."], "bleu": 0.16038842424444547, "rouge_l": 0.3588235294117647}
{"id": 2714, "code": "def main ( ) : parser = argparse . Argument Parser ( description = \"Runs the speficied algorithm(s) on the input file and \" \"the results using the MIREX format.\" , formatter class = argparse . Argument Defaults Help Formatter ) parser . add argument ( \"-bid\" , action = \"store\" , help = \"Boundary algorithm identifier\" , dest = \"boundaries id\" , default = msaf . config . default bound id , choices = [ \"gt\" ] + msaf . io . get all boundary algorithms ( ) ) parser . add argument ( \"-lid\" , action = \"store\" , help = \"Label algorithm identifier\" , dest = \"labels id\" , default = msaf . config . default label id , choices = msaf . io . get all label algorithms ( ) ) parser . add argument ( \"-i\" , action = \"store\" , dest = \"in file\" , help = \"Input audio file\" ) parser . add argument ( \"-o\" , action = \"store\" , dest = \"out file\" , help = \"Output file with the results\" , default = \"out.txt\" ) args = parser . parse args ( ) start time = time . time ( ) logging . basic Config ( format = '%(asctime)s: %(levelname)s: %(message)s' , level = logging . INFO ) params = { \"annot beats\" : False , \"feature\" : \"cqt\" , \"framesync\" : False , \"boundaries id\" : args . boundaries id , \"labels id\" : args . labels id , \"n jobs\" : 1 , \"hier\" : False , \"sonify bounds\" : False , \"plot\" : False } res = msaf . run . process ( args . in file , * * params ) msaf . io . write mirex ( res [ 0 ] , res [ 1 ] , args . out file ) logging . info ( \"Done! Took %.2f seconds.\" % ( time . time ( ) - start time ) )", "predictions": ["run the msaf ."], "references": ["main function to parse the arguments and call the main process ."], "bleu": 0.0538140946637381, "rouge_l": 0.22932330827067668}
{"id": 2715, "code": "def compute all features ( file struct , framesync ) : for feature id in msaf . features registry : logging . info ( \"Computing %s for file %s\" % ( feature id , file struct . audio file ) ) feats = Features . select features ( feature id , file struct , False , framesync ) feats . features", "predictions": ["compute all features in the given file"], "references": ["computes all features for the given file ."], "bleu": 0.3564026463354183, "rouge_l": 0.6587473002159828}
{"id": 2716, "code": "def process ( in path , out file , n jobs , framesync ) : if os . path . isfile ( in path ) : file struct = msaf . io . File Struct ( in path ) file struct . features file = out file compute all features ( file struct , framesync ) else : file structs = msaf . io . get dataset files ( in path ) return Parallel ( n jobs = n jobs ) ( delayed ( compute all features ) ( file struct , framesync ) for file struct in file structs )", "predictions": ["process all jobs in a single file ."], "references": ["computes the features for the selected dataset or file ."], "bleu": 0.1643685581109115, "rouge_l": 0.21785714285714283}
{"id": 2717, "code": "def main ( ) : parser = argparse . Argument Parser ( description = \"Extracts a set of features from a given dataset \" \"or audio file and saves them into the 'features' folder of \" \"the dataset or the specified single file.\" , formatter class = argparse . Argument Defaults Help Formatter ) parser . add argument ( \"in path\" , action = \"store\" , help = \"Input dataset dir or audio file\" ) parser . add argument ( \"-j\" , action = \"store\" , dest = \"n jobs\" , type = int , help = \"Number of jobs (only for collection mode)\" , default = 4 ) parser . add argument ( \"-o\" , action = \"store\" , dest = \"out file\" , type = str , help = \"Output file (only for single file mode)\" , default = \"out.json\" ) parser . add argument ( \"-d\" , action = \"store\" , dest = \"ds name\" , default = \"*\" , help = \"The prefix of the dataset to use \" \"(e.g. Isophonics, SALAMI)\" ) parser . add argument ( \"-fs\" , action = \"store true\" , dest = \"framesync\" , help = \"Use frame-synchronous features\" , default = False ) args = parser . parse args ( ) start time = time . time ( ) logging . basic Config ( format = '%(asctime)s: %(levelname)s: %(message)s' , level = logging . INFO ) process ( args . in path , out file = args . out file , n jobs = args . n jobs , framesync = args . framesync ) logging . info ( \"Done! Took %.2f seconds.\" % ( time . time ( ) - start time ) )", "predictions": ["main function for command line arguments ."], "references": ["main function to parse the arguments and call the main process ."], "bleu": 0.13597602315271134, "rouge_l": 0.40197693574958815}
{"id": 2718, "code": "def gaussian cost ( X ) : d , n = X . shape if n < 2 : return 0 sigma = np . var ( X , axis = 1 , ddof = 1 ) cost = - 0.5 * d * n * np . log ( 2. * np . pi ) - 0.5 * ( n - 1. ) * np . sum ( sigma ) return cost", "predictions": ["compute the gaussian cost cost for a gaussian ."], "references": ["return the average log - likelihood of data under a standard normal"], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 2719, "code": "def lognormalize ( F , floor = 0.1 , min db = - 80 ) : assert min db < 0 F = min max normalize ( F , floor = floor ) F = np . abs ( min db ) * np . log10 ( F ) return F", "predictions": ["return the range of the lognormalize ."], "references": ["log - normalizes features such that each vector is between min_db to 0 ."], "bleu": 0.06833381956448398, "rouge_l": 0.08983799705449189}
{"id": 2720, "code": "def min max normalize ( F , floor = 0.001 ) : F += - F . min ( ) + floor F = F / F . max ( axis = 0 ) return F", "predictions": ["maximum deviation of the minimum coefficient of the maximum length line line line line line ."], "references": ["normalizes features such that each vector is between floor to 1 ."], "bleu": 0.07692375026049747, "rouge_l": 0.07331730769230768}
{"id": 2721, "code": "def get time frames ( dur , anal ) : n frames = get num frames ( dur , anal ) return np . linspace ( 0 , dur , num = n frames )", "predictions": ["get self 0 0 to linspace"], "references": ["gets the time frames and puts them in a numpy array ."], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 2722, "code": "def remove empty segments ( times , labels ) : assert len ( times ) - 1 == len ( labels ) inters = times to intervals ( times ) new inters = [ ] new labels = [ ] for inter , label in zip ( inters , labels ) : if inter [ 0 ] < inter [ 1 ] : new inters . append ( inter ) new labels . append ( label ) return intervals to times ( np . asarray ( new inters ) ) , new labels", "predictions": ["get all hash of a list of labels 4096 4096 4096 4096 4096 4096 4096 4096 4096 4096 4096 4096 4096 4096 4096 4096 4096 4096"], "references": ["removes empty segments if needed ."], "bleu": 0.03925345689749394, "rouge_l": 0.0}
{"id": 2723, "code": "def distance ( self , idx ) : if scipy . sparse . issparse ( self . data ) : step = self . data . shape [ 1 ] else : step = 50000 d = np . zeros ( ( self . data . shape [ 1 ] ) ) if idx == - 1 : vec = np . zeros ( ( self . data . shape [ 0 ] , 1 ) ) if scipy . sparse . issparse ( self . data ) : vec = scipy . sparse . csc matrix ( vec ) else : vec = self . data [ : , idx : idx + 1 ] self . logger . info ( 'compute distance to node ' + str ( idx ) ) for idx start in range ( 0 , self . data . shape [ 1 ] , step ) : if idx start + step > self . data . shape [ 1 ] : idx end = self . data . shape [ 1 ] else : idx end = idx start + step d [ idx start : idx end ] = self . distfunc ( self . data [ : , idx start : idx end ] , vec ) self . logger . info ( 'completed:' + str ( idx end / ( self . data . shape [ 1 ] / 100.0 ) ) + \"%\" ) return d", "predictions": ["write the write write write to the table"], "references": ["compute distances of a specific data point to all other samples"], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 2724, "code": "def run kmeans ( self , X , K ) : w X = vq . whiten ( X ) means , dist = vq . kmeans ( w X , K , iter = 100 ) labels , dist = vq . vq ( w X , means ) return means , labels", "predictions": ["build the header for the header"], "references": ["runs k - means and returns the labels assigned to the data ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 2725, "code": "def compute bic ( self , D , means , labels , K , R ) : D = vq . whiten ( D ) Rn = D . shape [ 0 ] M = D . shape [ 1 ] if R == K : return 1 mle var = 0 for k in range ( len ( means ) ) : X = D [ np . argwhere ( labels == k ) ] X = X . reshape ( ( X . shape [ 0 ] , X . shape [ - 1 ] ) ) for x in X : mle var += distance . euclidean ( x , means [ k ] ) #print x, means[k], mle var mle var /= float ( R - K ) l D = - Rn / 2. * np . log ( 2 * np . pi ) - ( Rn * M ) / 2. * np . log ( mle var ) - ( Rn - K ) / 2. + Rn * np . log ( Rn ) - Rn * np . log ( R ) p = ( K - 1 ) + M * K + mle var #print \"BIC:\", l D, p, R, K return l D - p / 2. * np . log ( R )", "predictions": ["build the footer between x and ."], "references": ["computes the bayesian information criterion ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 2726, "code": "def magnitude ( X ) : r = np . real ( X ) i = np . imag ( X ) return np . sqrt ( r * r + i * i )", "predictions": ["computes the set of the set of x"], "references": ["magnitude of a complex matrix ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 2727, "code": "def compute ffmc2d ( X ) : fft2 = scipy . fftpack . fft2 ( X ) fft2m = magnitude ( fft2 ) fftshift = scipy . fftpack . fftshift ( fft2m ) . flatten ( ) #cmap = plt.cm.get cmap('hot') #plt.imshow(np.log1p(scipy.fftpack.fftshift(fft2m)).T, interpolation=\"nearest\", #plt.show() return fftshift [ : fftshift . shape [ 0 ] // 2 + 1 ]", "predictions": ["export the directives between the directives and the directives"], "references": ["computes the 2d - fourier magnitude coefficients ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 2728, "code": "def compute labels ( X , rank , R , bound idxs , niter = 300 ) : try : F , G = cnmf ( X , rank , niter = niter , hull = False ) except : return [ 1 ] label frames = filter activation matrix ( G . T , R ) label frames = np . asarray ( label frames , dtype = int ) #labels = [label frames[0]] labels = [ ] bound inters = zip ( bound idxs [ : - 1 ] , bound idxs [ 1 : ] ) for bound inter in bound inters : if bound inter [ 1 ] - bound inter [ 0 ] <= 0 : labels . append ( np . max ( label frames ) + 1 ) else : labels . append ( most frequent ( label frames [ bound inter [ 0 ] : bound inter [ 1 ] ] ) ) #print bound inter, labels[-1] #labels.append(label frames[-1]) return labels", "predictions": ["get the report of the else if it is not a list of report"], "references": ["computes the labels using the bounds ."], "bleu": 0.09782375748961449, "rouge_l": 0.2026578073089701}
{"id": 2729, "code": "def filter activation matrix ( G , R ) : #import pylab as plt #plt.imshow(G, interpolation=\"nearest\", aspect=\"auto\") #plt.show() idx = np . argmax ( G , axis = 1 ) max idx = np . arange ( G . shape [ 0 ] ) max idx = ( max idx , idx . flatten ( ) ) G [ : , : ] = 0 G [ max idx ] = idx + 1 G = np . sum ( G , axis = 1 ) G = median filter ( G [ : , np . newaxis ] , R ) return G . flatten ( )", "predictions": ["update the log watch watch"], "references": ["filters the activation matrix g and returns a flattened copy ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 2730, "code": "def main ( ) : args = command line ( ) translate = partial ( translator , args . source , args . dest , version = ' ' . join ( [ version , build ] ) ) return source ( spool ( set task ( translate , translit = args . translit ) ) , args . text )", "predictions": ["map arguments to cli vq vq ."], "references": ["main entry point for translator and argument parser"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 2731, "code": "def print table ( language ) : table = translation table ( language ) for code , name in sorted ( table . items ( ) , key = operator . itemgetter ( 0 ) ) : print ( u'{language:<8} {name:\\u3000<20}' . format ( name = name , language = code ) ) return None", "predictions": ["median a filter filter filter ."], "references": ["generates a formatted table of language codes"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 2732, "code": "def disable ( self ) : if not self . active : return None self . mock engine . disable ( ) self . active = False", "predictions": ["compute this mock g g g g g g g g g g g g g g g g g g"], "references": ["disables interceptors and stops intercepting any outgoing http traffic ."], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 2733, "code": "def get setting ( connection , key ) : if key in connection . settings dict : return connection . settings dict [ key ] else : return getattr ( settings , key )", "predictions": ["return - specific method to compute the value of the dictionary"], "references": ["get key from connection or default to settings ."], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 2734, "code": "def as sql ( self , compiler , connection ) : sql , params = super ( Decrypted Col , self ) . as sql ( compiler , connection ) sql = self . target . get decrypt sql ( connection ) % ( sql , self . target . get cast sql ( ) ) return sql , params", "predictions": ["returns peaks s peaks as peaks"], "references": ["build sql with decryption and casting ."], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 2735, "code": "def pre save ( self , model instance , add ) : if self . original : original value = getattr ( model instance , self . original ) setattr ( model instance , self . attname , original value ) return super ( Hash Mixin , self ) . pre save ( model instance , add )", "predictions": ["set field values for saving saving ."], "references": ["save the original_value ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 2736, "code": "def get col ( self , alias , output field = None ) : if output field is None : output field = self if alias != self . model . meta . db table or output field != self : return Decrypted Col ( alias , self , output field ) else : return self . cached col", "predictions": ["retrieve column object for given alias = value"], "references": ["get the decryption for col ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 2737, "code": "def get placeholder ( self , value = None , compiler = None , connection = None ) : return self . encrypt sql . format ( get setting ( connection , 'PUBLIC PGP KEY' ) )", "predictions": ["gets a peaks from the graph"], "references": ["tell postgres to encrypt this field using pgp ."], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 2738, "code": "def attach to tree ( self ) : for clade in self . tree . find clades ( ) : if clade . up is not None : clade . branch length interpolator . merger cost = self . cost", "predictions": ["embedded all . ."], "references": ["attaches the the merger cost to each branch length interpolator in the tree ."], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 2739, "code": "def optimize Tc ( self ) : from scipy . optimize import minimize scalar initial Tc = self . Tc def cost ( Tc ) : self . set Tc ( Tc ) return - self . total LH ( ) sol = minimize scalar ( cost , bounds = [ ttconf . TINY NUMBER , 10.0 ] ) if \"success\" in sol and sol [ \"success\" ] : self . set Tc ( sol [ 'x' ] ) else : self . logger ( \"merger models:optimze Tc: optimization of coalescent time scale failed: \" + str ( sol ) , 0 , warn = True ) self . set Tc ( initial Tc . y , T = initial Tc . x )", "predictions": ["plot the id of the id of the est"], "references": ["determines the coalescent time scale that optimizes the coalescent likelihood of the tree"], "bleu": 0.135323305042906, "rouge_l": 0.35209235209235207}
{"id": 2740, "code": "def prepare nodes ( self ) : self . tree . root . up = None self . tree . root . bad branch = self . tree . root . bad branch if hasattr ( self . tree . root , 'bad branch' ) else False internal node count = 0 for clade in self . tree . get nonterminals ( order = 'preorder' ) : internal node count += 1 if clade . name is None : clade . name = \"NODE \" + format ( self . internal node count , '07d' ) self . internal node count += 1 for c in clade . clades : if c . is terminal ( ) : c . bad branch = c . bad branch if hasattr ( c , 'bad branch' ) else False c . up = clade for clade in self . tree . get nonterminals ( order = 'postorder' ) : clade . bad branch = all ( [ c . bad branch for c in clade ] ) self . calc dist2root ( ) self . internal node count = max ( internal node count , self . internal node count )", "predictions": ["get for for each utils ext ."], "references": ["set auxilliary parameters to every node of the tree ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 2741, "code": "def create gtr ( params ) : model = params . gtr gtr params = params . gtr params if model == 'infer' : gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) else : try : kwargs = { } if gtr params is not None : for param in gtr params : keyval = param . split ( '=' ) if len ( keyval ) != 2 : continue if keyval [ 0 ] in [ 'pis' , 'pi' , 'Pi' , 'Pis' ] : keyval [ 0 ] = 'pi' keyval [ 1 ] = list ( map ( float , keyval [ 1 ] . split ( ',' ) ) ) elif keyval [ 0 ] not in [ 'alphabet' ] : keyval [ 1 ] = float ( keyval [ 1 ] ) kwargs [ keyval [ 0 ] ] = keyval [ 1 ] else : print ( \"GTR params are not specified. Creating GTR model with default parameters\" ) gtr = GTR . standard ( model , * * kwargs ) infer gtr = False except : print ( \"Could not create GTR model from input arguments. Using default (Jukes-Cantor 1969)\" ) gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) infer gtr = False return gtr", "predictions": ["get gtr dir from gtr"], "references": ["parse the arguments referring to the gtr model and return a gtr structure"], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 2742, "code": "def read if vcf ( params ) : ref = None aln = params . aln fixed pi = None if hasattr ( params , 'aln' ) and params . aln is not None : if any ( [ params . aln . lower ( ) . endswith ( x ) for x in [ '.vcf' , '.vcf.gz' ] ] ) : if not params . vcf reference : print ( \"ERROR: a reference Fasta is required with VCF-format alignments\" ) return - 1 compress seq = read vcf ( params . aln , params . vcf reference ) sequences = compress seq [ 'sequences' ] ref = compress seq [ 'reference' ] aln = sequences if not hasattr ( params , 'gtr' ) or params . gtr == \"infer\" : #if not specified, set it: alpha = alphabets [ 'aa' ] if params . aa else alphabets [ 'nuc' ] fixed pi = [ ref . count ( base ) / len ( ref ) for base in alpha ] if fixed pi [ - 1 ] == 0 : fixed pi [ - 1 ] = 0.05 fixed pi = [ v - 0.01 for v in fixed pi ] return aln , ref , fixed pi", "predictions": ["write a to a"], "references": ["checks if input is vcf and reads in appropriately if it is"], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 2743, "code": "def delta function ( cls , x pos , weight = 1. , min width = MIN INTEGRATION PEAK ) : distribution = cls ( x pos , 0. , is log = True , min width = min width ) distribution . weight = weight return distribution", "predictions": ["create a compute shape of the compute shape sr ."], "references": ["create delta function distribution ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 2744, "code": "def multiply ( dists ) : if not all ( [ isinstance ( k , Distribution ) for k in dists ] ) : raise Not Implemented Error ( \"Can only multiply Distribution objects\" ) n delta = np . sum ( [ k . is delta for k in dists ] ) min width = np . max ( [ k . min width for k in dists ] ) if n delta > 1 : raise Arithmetic Error ( \"Cannot multiply more than one delta functions!\" ) elif n delta == 1 : delta dist ii = np . where ( [ k . is delta for k in dists ] ) [ 0 ] [ 0 ] delta dist = dists [ delta dist ii ] new xpos = delta dist . peak pos new weight = np . prod ( [ k . prob ( new xpos ) for k in dists if k != delta dist ii ] ) * delta dist . weight res = Distribution . delta function ( new xpos , weight = new weight , min width = min width ) else : new xmin = np . max ( [ k . xmin for k in dists ] ) new xmax = np . min ( [ k . xmax for k in dists ] ) x vals = np . unique ( np . concatenate ( [ k . x for k in dists ] ) ) x vals = x vals [ ( x vals > new xmin - TINY NUMBER ) & ( x vals < new xmax + TINY NUMBER ) ] y vals = np . sum ( [ k . call ( x vals ) for k in dists ] , axis = 0 ) peak = y vals . min ( ) ind = ( y vals - peak ) < BIG NUMBER / 1000 n points = ind . sum ( ) if n points == 0 : print ( \"ERROR in distribution multiplication: Distributions do not overlap\" ) x vals = [ 0 , 1 ] y vals = [ BIG NUMBER , BIG NUMBER ] res = Distribution ( x vals , y vals , is log = True , min width = min width , kind = 'linear' ) elif n points == 1 : res = Distribution . delta function ( x vals [ 0 ] ) else : res = Distribution ( x vals [ ind ] , y vals [ ind ] , is log = True , min width = min width , kind = 'linear' , assume sorted = True ) return res", "predictions": ["preprocess the one of the one algorithm"], "references": ["multiplies a list of distribution objects"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2745, "code": "def timetree likelihood ( self ) : LH = 0 for node in self . tree . find clades ( order = 'preorder' ) : if node . up is None : continue LH -= node . branch length interpolator ( node . branch length ) if self . aln : LH += self . gtr . sequence log LH ( self . tree . root . cseq , pattern multiplicity = self . multiplicity ) return LH", "predictions": ["returns the likelihood likelihood of the main main loop the speficied"], "references": ["return the likelihood of the data given the current branch length in the tree"], "bleu": 0.1919385676652243, "rouge_l": 0.39152759948652116}
{"id": 2746, "code": "def min interp ( interp object ) : try : return interp object . x [ interp object ( interp object . x ) . argmin ( ) ] except Exception as e : s = \"Cannot find minimum of the interpolation object\" + str ( interp object . x ) + \"Minimal x: \" + str ( interp object . x . min ( ) ) + \"Maximal x: \" + str ( interp object . x . max ( ) ) raise e", "predictions": ["get the results of a results object"], "references": ["find the global minimum of a function represented as an interpolation object ."], "bleu": 0.11787460936700446, "rouge_l": 0.3794712286158632}
{"id": 2747, "code": "def median interp ( interp object ) : new grid = np . sort ( np . concatenate ( [ interp object . x [ : - 1 ] + 0.1 * ii * np . diff ( interp object . x ) for ii in range ( 10 ) ] ) . flatten ( ) ) tmp prop = np . exp ( - ( interp object ( new grid ) - interp object . y . min ( ) ) ) tmp cumsum = np . cumsum ( 0.5 * ( tmp prop [ 1 : ] + tmp prop [ : - 1 ] ) * np . diff ( new grid ) ) median index = min ( len ( tmp cumsum ) - 3 , max ( 2 , np . searchsorted ( tmp cumsum , tmp cumsum [ - 1 ] * 0.5 ) + 1 ) ) return new grid [ median index ]", "predictions": ["compute the compute grid grid for the grid object"], "references": ["find the median of the function represented as an interpolation object ."], "bleu": 0.12026590852507517, "rouge_l": 0.2785388127853881}
{"id": 2748, "code": "def close ( self ) : self . client . close ( ) self . client = None self . connected = False self . logger . debug ( 'Connection closed.' )", "predictions": ["process the websocket connection out of the file out ."], "references": ["terminate connection with guacamole guacd server ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 2749, "code": "def receive ( self ) : start = 0 while True : idx = self . buffer . find ( INST TERM . encode ( ) , start ) if idx != - 1 : line = self . buffer [ : idx + 1 ] . decode ( ) self . buffer = self . buffer [ idx + 1 : ] self . logger . debug ( 'Received instruction: %s' % line ) return line else : start = len ( self . buffer ) buf = self . client . recv ( BUF LEN ) if not buf : self . close ( ) self . logger . debug ( 'Failed to receive instruction. Closing.' ) return None self . buffer . extend ( buf )", "predictions": ["main main loop ."], "references": ["receive instructions from guacamole guacd server ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 2750, "code": "def send ( self , data ) : self . logger . debug ( 'Sending data: %s' % data ) self . client . sendall ( data . encode ( ) )", "predictions": ["gaussian data to server ."], "references": ["send encoded instructions to guacamole guacd server ."], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 2751, "code": "def send instruction ( self , instruction ) : self . logger . debug ( 'Sending instruction: %s' % str ( instruction ) ) return self . send ( instruction . encode ( ) )", "predictions": ["send an instruction min"], "references": ["send instruction after encoding ."], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 2752, "code": "def handshake ( self , protocol = 'vnc' , width = 1024 , height = 768 , dpi = 96 , audio = None , video = None , image = None , * * kwargs ) : if protocol not in PROTOCOLS : self . logger . debug ( 'Invalid protocol: %s' % protocol ) raise Guacamole Error ( 'Cannot start Handshake. Missing protocol.' ) if audio is None : audio = list ( ) if video is None : video = list ( ) if image is None : image = list ( ) self . logger . debug ( 'Send `select` instruction.' ) self . send instruction ( Instruction ( 'select' , protocol ) ) instruction = self . read instruction ( ) self . logger . debug ( 'Expecting `args` instruction, received: %s' % str ( instruction ) ) if not instruction : self . close ( ) raise Guacamole Error ( 'Cannot establish Handshake. Connection Lost!' ) if instruction . opcode != 'args' : self . close ( ) raise Guacamole Error ( 'Cannot establish Handshake. Expected opcode `args`, ' 'received `%s` instead.' % instruction . opcode ) self . logger . debug ( 'Send `size` instruction (%s, %s, %s)' % ( width , height , dpi ) ) self . send instruction ( Instruction ( 'size' , width , height , dpi ) ) self . logger . debug ( 'Send `audio` instruction (%s)' % audio ) self . send instruction ( Instruction ( 'audio' , * audio ) ) self . logger . debug ( 'Send `video` instruction (%s)' % video ) self . send instruction ( Instruction ( 'video' , * video ) ) self . logger . debug ( 'Send `image` instruction (%s)' % image ) self . send instruction ( Instruction ( 'image' , * image ) ) connection args = [ kwargs . get ( arg . replace ( '-' , ' ' ) , '' ) for arg in instruction . args ] self . logger . debug ( 'Send `connect` instruction (%s)' % connection args ) self . send instruction ( Instruction ( 'connect' , * connection args ) ) instruction = self . read instruction ( ) self . logger . debug ( 'Expecting `ready` instruction, received: %s' % str ( instruction ) ) if instruction . opcode != 'ready' : self . logger . warning ( 'Expected `ready` instruction, received: %s instead' ) if instruction . args : self . id = instruction . args [ 0 ] self . logger . debug ( 'Established connection with client id: %s' % self . id ) self . logger . debug ( 'Handshake completed.' ) self . connected = True", "predictions": ["handshake a client to the client ."], "references": ["establish connection with guacamole guacd server via handshake ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 2753, "code": "def class url ( cls ) : base = 'v{0}' . format ( getattr ( cls , 'RESOURCE VERSION' , '1' ) ) return \"/{0}/{1}\" . format ( base , class to api name ( cls . class name ( ) ) )", "predictions": ["returns the class url to use for this class ."], "references": ["returns a versioned uri string for this class"], "bleu": 0.24808415001701817, "rouge_l": 0.4535315985130111}
{"id": 2754, "code": "def instance url ( self ) : id = self . get ( self . ID ATTR ) base = self . class url ( ) if id : return '/' . join ( [ base , six . text type ( id ) ] ) else : raise Exception ( 'Could not determine which URL to request: %s instance ' 'has invalid ID: %r' % ( type ( self ) . name , id ) , self . ID ATTR )", "predictions": ["url of the instance ."], "references": ["get instance url by id"], "bleu": 0.3021375397356768, "rouge_l": 0.2}
{"id": 2755, "code": "def parent object ( self ) : from . import types parent klass = types . get ( self . parent job model . split ( '.' ) [ 1 ] ) return parent klass . retrieve ( self . parent job id , client = self . client )", "predictions": ["return the parent object for this job ."], "references": ["get the commit objects parent import or migration"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 2756, "code": "def ask for credentials ( ) : print msg ( 'Please enter your Solve Bio credentials' ) domain = raw input ( 'Domain (e.g. <domain>.solvebio.com): ' ) try : account = client . request ( 'get' , '/p/accounts/{}' . format ( domain ) ) auth = account [ 'authentication' ] except : raise Solve Error ( 'Invalid domain: {}' . format ( domain ) ) if auth . get ( 'login' ) or auth . get ( 'SAML' , { } ) . get ( 'simple login' ) : email = raw input ( 'Email: ' ) password = getpass . getpass ( 'Password (typing will be hidden): ' ) return ( domain , email , password ) else : print msg ( 'Your domain uses Single Sign-On (SSO). ' 'Please visit https://{}.solvebio.com/settings/security ' 'for instructions on how to log in.' . format ( domain ) ) sys . exit ( 1 )", "predictions": ["ask user to ask user for domain ."], "references": ["asks the user for their email and password ."], "bleu": 0.20014292374951972, "rouge_l": 0.34923664122137404}
{"id": 2757, "code": "def print user ( user ) : email = user [ 'email' ] domain = user [ 'account' ] [ 'domain' ] role = user [ 'role' ] print ( 'You are logged-in to the \"{0}\" domain ' 'as {1} with role {2}.' . format ( domain , email , role ) )", "predictions": ["print the user information"], "references": ["prints information about the current user ."], "bleu": 0.20183609024241697, "rouge_l": 0.346590909090909}
{"id": 2758, "code": "def range ( self , chromosome , start , stop , exact = False ) : return self . clone ( filters = [ Genomic Filter ( chromosome , start , stop , exact ) ] )", "predictions": ["clone a range by chromosome and stop the first value ."], "references": ["shortcut to do range filters on genomic datasets ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 2759, "code": "def position ( self , chromosome , position , exact = False ) : return self . clone ( filters = [ Genomic Filter ( chromosome , position , exact = exact ) ] )", "predictions": ["clone a position in the current cursor position"], "references": ["shortcut to do a single position filter on genomic datasets ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 2760, "code": "def main ( argv = sys . argv [ 1 : ] ) : parser = Solve Argument Parser ( ) args = parser . parse solvebio args ( argv ) if args . api host : solvebio . api host = args . api host if args . api key : solvebio . api key = args . api key if not solvebio . api key : try : from . credentials import get credentials solvebio . api key = get credentials ( ) except : pass client . set host ( ) client . set token ( ) return args . func ( args )", "predictions": ["the main function of the solvebio command ."], "references": ["main entry point for solvebio cli"], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 2761, "code": "def construct from ( cls , values , * * kwargs ) : instance = cls ( values . get ( cls . ID ATTR ) , * * kwargs ) instance . refresh from ( values ) return instance", "predictions": ["construct an instance from a values ."], "references": ["used to create a new object from an http response"], "bleu": 0.14390022429682173, "rouge_l": 0.11401869158878504}
{"id": 2762, "code": "def logout ( self ) : if self . oauth client secret : try : oauth token = flask . request . cookies [ self . TOKEN COOKIE NAME ] requests . post ( urljoin ( self . api host , self . OAUTH2 REVOKE TOKEN PATH ) , data = { 'client id' : self . oauth client id , 'client secret' : self . oauth client secret , 'token' : oauth token } ) except : pass response = flask . redirect ( '/' ) self . clear cookies ( response ) return response", "predictions": ["logs out the oauth oauth 2 . 0 . 0 ."], "references": ["revoke the token and remove the cookie ."], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 2763, "code": "def child object ( self ) : from . import types child klass = types . get ( self . task type . split ( '.' ) [ 1 ] ) return child klass . retrieve ( self . task id , client = self . client )", "predictions": ["retrieve the child for the task ."], "references": ["get task child object class"], "bleu": 0.20556680845025982, "rouge_l": 0.17183098591549298}
{"id": 2764, "code": "def row to dict ( self , row , allele , alternate alleles ) : def variant sbid ( * * kwargs ) : \"\"\"Generates a Solve Bio variant ID (SBID).\"\"\" return '{build}-{chromosome}-{start}-{stop}-{allele}' . format ( * * kwargs ) . upper ( ) if allele == '.' : allele = row . REF or allele genomic coordinates = { 'build' : self . genome build , 'chromosome' : row . CHROM , 'start' : row . POS , 'stop' : row . POS + len ( row . REF ) - 1 } variant sbid = variant sbid ( allele = allele , * * genomic coordinates ) return { 'genomic coordinates' : genomic coordinates , 'variant' : variant sbid , 'allele' : allele , 'row id' : row . ID , 'reference allele' : row . REF , 'alternate alleles' : alternate alleles , 'info' : self . parse info ( row . INFO ) , 'qual' : row . QUAL , 'filter' : row . FILTER }", "predictions": ["converts a variant into a dictionary ."], "references": ["return a parsed dictionary for json ."], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 2765, "code": "def save ( self , path ) : rep = \"\" for host in self . hosts . keys ( ) : attrs = self . hosts [ host ] rep = rep + \"machine \" + host + \"\\n\\tlogin \" + six . text type ( attrs [ 0 ] ) + \"\\n\" if attrs [ 1 ] : rep = rep + \"account \" + six . text type ( attrs [ 1 ] ) rep = rep + \"\\tpassword \" + six . text type ( attrs [ 2 ] ) + \"\\n\" for macro in self . macros . keys ( ) : rep = rep + \"macdef \" + macro + \"\\n\" for line in self . macros [ macro ] : rep = rep + line rep = rep + \"\\n\" f = open ( path , 'w' ) f . write ( rep ) f . close ( )", "predictions": ["save the current state of the hosts to a file"], "references": ["dump the class data in the format of a . netrc file ."], "bleu": 0.12290460988295328, "rouge_l": 0.33983286908078}
{"id": 2766, "code": "def build row ( cells , padding , begin , sep , end ) : pad = \" \" * padding padded cells = [ pad + cell + pad for cell in cells ] rendered cells = ( begin + sep . join ( padded cells ) + end ) . rstrip ( ) if len ( rendered cells ) > TTY COLS : if not cells [ - 1 ] . endswith ( \" \" ) and not cells [ - 1 ] . endswith ( \"-\" ) : terminating str = \" ... \" else : terminating str = \"\" rendered cells = \"{0}{1}{2}\" . format ( rendered cells [ : TTY COLS - len ( terminating str ) - 1 ] , terminating str , end ) return rendered cells", "predictions": ["build a string from the cells of the cells ."], "references": ["return a string which represents a row of data cells ."], "bleu": 0.19756380535471652, "rouge_l": 0.47213622291021673}
{"id": 2767, "code": "def build line ( colwidths , padding , begin , fill , sep , end ) : cells = [ fill * ( w + 2 * padding ) for w in colwidths ] return build row ( cells , 0 , begin , sep , end )", "predictions": ["build a line from a string ."], "references": ["return a string which represents a horizontal line ."], "bleu": 0.21846599816382303, "rouge_l": 0.3667334669338677}
{"id": 2768, "code": "def mediawiki cell attrs ( row , colaligns ) : alignment = { \"left\" : '' , \"right\" : 'align=\"right\"| ' , \"center\" : 'align=\"center\"| ' , \"decimal\" : 'align=\"right\"| ' } row2 = [ alignment [ a ] + c for c , a in zip ( row , colaligns ) ] return row2", "predictions": ["returns the html for the cell"], "references": ["prefix every cell in a row with an html alignment attribute ."], "bleu": 0.08993236413460196, "rouge_l": 0.10481099656357389}
{"id": 2769, "code": "def format table ( fmt , headers , rows , colwidths , colaligns ) : lines = [ ] hidden = fmt . with header hide if headers else fmt . without header hide pad = fmt . padding headerrow = fmt . headerrow if fmt . headerrow else fmt . datarow if fmt . lineabove and \"lineabove\" not in hidden : lines . append ( build line ( colwidths , pad , * fmt . lineabove ) ) if headers : lines . append ( build row ( headers , pad , * headerrow ) ) if fmt . linebelowheader and \"linebelowheader\" not in hidden : begin , fill , sep , end = fmt . linebelowheader if fmt . usecolons : segs = [ line segment with colons ( fmt . linebelowheader , a , w + 2 * pad ) for w , a in zip ( colwidths , colaligns ) ] lines . append ( build row ( segs , 0 , begin , sep , end ) ) else : lines . append ( build line ( colwidths , pad , * fmt . linebelowheader ) ) if rows and fmt . linebetweenrows and \"linebetweenrows\" not in hidden : for row in rows [ : - 1 ] : lines . append ( build row ( row , pad , * fmt . datarow ) ) lines . append ( build line ( colwidths , pad , * fmt . linebetweenrows ) ) lines . append ( build row ( rows [ - 1 ] , pad , * fmt . datarow ) ) else : for row in rows : lines . append ( build row ( row , pad , * fmt . datarow ) ) if fmt . linebelow and \"linebelow\" not in hidden : lines . append ( build line ( colwidths , pad , * fmt . linebelow ) ) return \"\\n\" . join ( lines )", "predictions": ["format a table in a readable form ."], "references": ["produce a plain - text representation of the table ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 2770, "code": "def evaluate ( self , data = None , data type = 'string' , is list = False ) : payload = { 'data' : data , 'expression' : self . expr , 'data type' : data type , 'is list' : is list } res = self . client . post ( '/v1/evaluate' , payload ) return res [ 'result' ]", "predictions": ["evaluate the given data and return the result"], "references": ["evaluates the expression with the provided context and format ."], "bleu": 0.1485237584394808, "rouge_l": 0.21785714285714283}
{"id": 2771, "code": "def get column types ( self , data ) : columns = list ( zip longest ( * data ) ) return [ self . get column type ( column ) for column in columns ]", "predictions": ["return a list of column types for the given data ."], "references": ["get a list of the data types for each column in * data * ."], "bleu": 0.19913294675043053, "rouge_l": 0.5239263803680982}
{"id": 2772, "code": "def get column type ( self , column ) : type values = [ TYPES [ self . get type ( v ) ] for v in column ] inverse types = { v : k for k , v in TYPES . items ( ) } return inverse types [ max ( type values ) ]", "predictions": ["returns the inverse type for the given column ."], "references": ["get the most generic data type for iterable * column * ."], "bleu": 0.158278836853973, "rouge_l": 0.4642313546423136}
{"id": 2773, "code": "def get type ( self , value ) : if value is None : return type ( None ) elif type ( value ) in int types : return int elif type ( value ) in float types : return float elif isinstance ( value , binary type ) : return binary type else : return text type", "predictions": ["return the type of the given value"], "references": ["get the data type for * value * ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 2774, "code": "def adapter ( data , headers , table format = None , preserve whitespace = False , * * kwargs ) : keys = ( 'floatfmt' , 'numalign' , 'stralign' , 'showindex' , 'disable numparse' ) tkwargs = { 'tablefmt' : table format } tkwargs . update ( filter dict by key ( kwargs , keys ) ) if table format in supported markup formats : tkwargs . update ( numalign = None , stralign = None ) tabulate . PRESERVE WHITESPACE = preserve whitespace return iter ( tabulate . tabulate ( data , headers , * * tkwargs ) . split ( '\\n' ) )", "predictions": ["return a table of data from the data source"], "references": ["wrap tabulate inside a function for tabularoutputformatter ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 2775, "code": "def user config file ( self ) : return os . path . join ( get user config dir ( self . app name , self . app author ) , self . filename )", "predictions": ["return the path to the user s config file ."], "references": ["get the absolute path to the user config file ."], "bleu": 0.4832697830906221, "rouge_l": 0.8}
{"id": 2776, "code": "def system config files ( self ) : return [ os . path . join ( f , self . filename ) for f in get system config dirs ( self . app name , self . app author ) ]", "predictions": ["return a list of all config files ."], "references": ["get a list of absolute paths to the system config files ."], "bleu": 0.2618210963590692, "rouge_l": 0.5791139240506329}
{"id": 2777, "code": "def additional files ( self ) : return [ os . path . join ( f , self . filename ) for f in self . additional dirs ]", "predictions": ["return a list of all additional files in the current directory ."], "references": ["get a list of absolute paths to the additional config files ."], "bleu": 0.22997519112894443, "rouge_l": 0.5}
{"id": 2778, "code": "def truncate string ( value , max width = None ) : if isinstance ( value , text type ) and max width is not None and len ( value ) > max width : return value [ : max width ] return value", "predictions": ["truncates a string to its width"], "references": ["truncate string values ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 2779, "code": "def filter dict by key ( d , keys ) : return { k : v for k , v in d . items ( ) if k in keys }", "predictions": ["filter a dict by key ."], "references": ["filter the dict * d * to remove keys not in * keys * ."], "bleu": 0.05861428254383573, "rouge_l": 0.26521739130434785}
{"id": 2780, "code": "def replace ( s , replace ) : for r in replace : s = s . replace ( * r ) return s", "predictions": ["replace characters in a string with a string ."], "references": ["replace multiple values in a string"], "bleu": 0.2777619034011791, "rouge_l": 0.5532879818594103}
{"id": 2781, "code": "def adapter ( data , headers , * * kwargs ) : for row in chain ( ( headers , ) , data ) : yield \"\\t\" . join ( ( replace ( r , ( ( '\\n' , r'\\n' ) , ( '\\t' , r'\\t' ) ) ) for r in row ) )", "predictions": ["yield data from data ."], "references": ["wrap the formatting inside a function for tabularoutputformatter ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 2782, "code": "def call and exit ( self , cmd , shell = True ) : sys . exit ( subprocess . call ( cmd , shell = shell ) )", "predictions": ["run a command and exit ."], "references": ["run the * cmd * and exit with the proper exit code ."], "bleu": 0.10286160177491631, "rouge_l": 0.39482200647249194}
{"id": 2783, "code": "def call in sequence ( self , cmds , shell = True ) : for cmd in cmds : if subprocess . call ( cmd , shell = shell ) == 1 : sys . exit ( 1 )", "predictions": ["run a command in the sequence of the sequence ."], "references": ["run multiple commmands in a row exiting if one fails ."], "bleu": 0.1434272783816789, "rouge_l": 0.28328173374613}
{"id": 2784, "code": "def apply options ( self , cmd , options = ( ) ) : for option in ( self . default cmd options + options ) : cmd = self . apply option ( cmd , option , active = getattr ( self , option , False ) ) return cmd", "predictions": ["handshake options command 96 options"], "references": ["apply command - line options ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 2785, "code": "def apply option ( self , cmd , option , active = True ) : return re . sub ( r'{{{}\\:(?P<option>[^}}]*)}}' . format ( option ) , '\\g<option>' if active else '' , cmd )", "predictions": ["class for override to class ."], "references": ["apply a command - line option ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 2786, "code": "def initialize options ( self ) : self . branch = 'master' self . fix = False super ( lint , self ) . initialize options ( )", "predictions": ["initializes the config and set default values id"], "references": ["set the default options ."], "bleu": 0.19070828081828378, "rouge_l": 0.32105263157894737}
{"id": 2787, "code": "def run ( self ) : cmd = 'pep8radius {branch} {{fix: --in-place}}{{verbose: -vv}}' cmd = cmd . format ( branch = self . branch ) self . call and exit ( self . apply options ( cmd , ( 'fix' , ) ) )", "predictions": ["runs the command get command get the options and 1 get the options"], "references": ["run the linter ."], "bleu": 0.09552040806823771, "rouge_l": 0.1300639658848614}
{"id": 2788, "code": "def run ( self ) : cmds = ( self . clean docs cmd , self . html docs cmd , self . view docs cmd ) self . call in sequence ( cmds )", "predictions": ["run the command msg msg"], "references": ["generate and view the documentation ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 2789, "code": "def get separator ( num , sep title , sep character , sep length ) : left divider length = right divider length = sep length if isinstance ( sep length , tuple ) : left divider length , right divider length = sep length left divider = sep character * left divider length right divider = sep character * right divider length title = sep title . format ( n = num + 1 ) return \"{left divider}[ {title} ]{right divider}\\n\" . format ( left divider = left divider , right divider = right divider , title = title )", "predictions": ["print a user - specified user - specified string the given string ."], "references": ["get a row separator for row * num * ."], "bleu": 0.10571070857151538, "rouge_l": 0.1781021897810219}
{"id": 2790, "code": "def format row ( headers , row ) : formatted row = [ ' | ' . join ( field ) for field in zip ( headers , row ) ] return '\\n' . join ( formatted row )", "predictions": ["range a row string to a string"], "references": ["format a row ."], "bleu": 0.24446151121745047, "rouge_l": 0.3824451410658307}
{"id": 2791, "code": "def adapter ( data , headers , * * kwargs ) : keys = ( 'sep title' , 'sep character' , 'sep length' ) return vertical table ( data , headers , * * filter dict by key ( kwargs , keys ) )", "predictions": ["create a table table table ."], "references": ["wrap vertical table in a function for tabularoutputformatter ."], "bleu": 0.1593301391270729, "rouge_l": 0.2573839662447257}
{"id": 2792, "code": "def adapter ( data , headers , table format = 'csv' , * * kwargs ) : keys = ( 'dialect' , 'delimiter' , 'doublequote' , 'escapechar' , 'quotechar' , 'quoting' , 'skipinitialspace' , 'strict' ) if table format == 'csv' : delimiter = ',' elif table format == 'csv-tab' : delimiter = '\\t' else : raise Value Error ( 'Invalid table format specified.' ) ckwargs = { 'delimiter' : delimiter , 'lineterminator' : '' } ckwargs . update ( filter dict by key ( kwargs , keys ) ) l = linewriter ( ) writer = csv . writer ( l , * * ckwargs ) writer . writerow ( headers ) yield l . line for row in data : l . reset ( ) writer . writerow ( row ) yield l . line", "predictions": ["yield . . csv"], "references": ["wrap the formatting inside a function for tabularoutputformatter ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 2793, "code": "def adapter ( data , headers , table format = None , * * kwargs ) : keys = ( 'title' , ) table = table format handler [ table format ] t = table ( [ headers ] + list ( data ) , * * filter dict by key ( kwargs , keys ) ) dimensions = terminaltables . width and alignment . max dimensions ( t . table data , t . padding left , t . padding right ) [ : 3 ] for r in t . gen table ( * dimensions ) : yield u'' . join ( r )", "predictions": ["filter a values of data ."], "references": ["wrap terminaltables inside a function for tabularoutputformatter ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 2794, "code": "def to dict ( self ) : all attributes = Py KCS11 . CKA . keys ( ) all attributes = [ attr for attr in all attributes if isinstance ( attr , int ) ] attributes = self . session . get Attribute Value ( self , all attributes ) dico = dict ( ) for key , attr in zip ( all attributes , attributes ) : if attr is None : continue if key == CKA CLASS : dico [ Py KCS11 . CKA [ key ] ] = Py KCS11 . CKO [ attr ] elif key == CKA CERTIFICATE TYPE : dico [ Py KCS11 . CKA [ key ] ] = Py KCS11 . CKC [ attr ] elif key == CKA KEY TYPE : dico [ Py KCS11 . CKA [ key ] ] = Py KCS11 . CKK [ attr ] else : dico [ Py KCS11 . CKA [ key ] ] = attr return dico", "predictions": ["converts the current urljoin into a dictionary ."], "references": ["convert the fields of the object into a dictionnary"], "bleu": 0.20014292374951972, "rouge_l": 0.34923664122137404}
{"id": 2795, "code": "def to dict ( self ) : dico = dict ( ) for field in self . fields . keys ( ) : if field == \"flags\" : dico [ field ] = self . flags2text ( ) elif field == \"state\" : dico [ field ] = self . state2text ( ) else : dico [ field ] = eval ( \"self.\" + field ) return dico", "predictions": ["return the get dictionary of the get ."], "references": ["convert the fields of the object into a dictionnary"], "bleu": 0.20014292374951972, "rouge_l": 0.34923664122137404}
{"id": 2796, "code": "def insert img ( qr img , icon img = None , factor = 4 , icon box = None , static dir = None ) : img w , img h = qr img . size size w = int ( img w ) / int ( factor ) size h = int ( img h ) / int ( factor ) try : icon fp = os . path . join ( icon img ) if static dir : icon fp = os . path . join ( static dir , icon img ) if icon img . split ( \"://\" ) [ 0 ] in [ \"http\" , \"https\" , \"ftp\" ] : icon fp = Bytes IO ( urlopen ( icon img ) . read ( ) ) icon = Image . open ( icon fp ) except : return qr img icon w , icon h = icon . size icon w = size w if icon w > size w else icon w icon h = size h if icon h > size h else icon h icon = icon . resize ( ( int ( icon w ) , int ( icon h ) ) , Image . ANTIALIAS ) icon = icon . convert ( \"RGBA\" ) left = int ( ( img w - icon w ) / 2 ) top = int ( ( img h - icon h ) / 2 ) icon box = ( int ( icon box [ 0 ] ) , int ( icon box [ 1 ] ) ) if icon box else ( left , top ) qr img . paste ( im = icon , box = icon box , mask = icon ) return qr img", "predictions": ["inserts an icon into the qr ."], "references": ["inserts a small icon to qr code image"], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 2797, "code": "def biweekly helper ( self ) : self . num = 14 mycount = self . repeat biweekly ( ) if mycount : if self . event . is chunk ( ) and min ( mycount ) not in xrange ( 1 , 8 ) : mycount = chunk fill out first week ( self . year , self . month , mycount , self . event , diff = self . event . start end diff , ) for k , v in mycount . items ( ) : for item in v : self . count [ k ] . append ( item )", "predictions": ["helper the save week week"], "references": ["created to take some of the load off of _handle_weekly_repeat_out"], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 2798, "code": "def user ( context , user id , update role , add institute , remove admin , remove institute ) : adapter = context . obj [ 'adapter' ] user obj = adapter . user ( user id ) if not user obj : LOG . warning ( \"User %s could not be found\" , user id ) context . abort ( ) existing roles = set ( user obj . get ( 'roles' , [ ] ) ) if update role : if not update role in user obj [ 'roles' ] : existing roles = set ( user obj [ 'roles' ] ) existing roles . add ( update role ) LOG . info ( \"Adding role %s to user\" , update role ) else : LOG . warning ( \"User already have role %s\" , update role ) if remove admin : try : existing roles . remove ( 'admin' ) LOG . info ( \"Removing admin rights from user %s\" , user id ) except Key Error as err : LOG . info ( \"User %s does not have admin rights\" , user id ) user obj [ 'roles' ] = list ( existing roles ) existing institutes = set ( user obj . get ( 'institutes' , [ ] ) ) for institute id in add institute : institute obj = adapter . institute ( institute id ) if not institute obj : LOG . warning ( \"Institute %s could not be found\" , institute id ) else : existing institutes . add ( institute id ) LOG . info ( \"Adding institute %s to user\" , institute id ) for institute id in remove institute : try : existing institutes . remove ( institute id ) LOG . info ( \"Removing institute %s from user\" , institute id ) except Key Error as err : LOG . info ( \"User does not have access to institute %s\" , institute id ) user obj [ 'institutes' ] = list ( existing institutes ) updated user = adapter . update user ( user obj )", "predictions": ["sep build of an admin"], "references": ["update a user in the database"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 2799, "code": "def variant ( institute id , case name , variant id ) : institute obj , case obj = institute and case ( store , institute id , case name ) log . debug ( \"Variants view requesting data for variant {}\" . format ( variant id ) ) data = controllers . variant ( store , institute obj , case obj , variant id = variant id ) if data is None : log . warning ( \"An error occurred: variants view requesting data for variant {}\" . format ( variant id ) ) flash ( 'An error occurred while retrieving variant object' , 'danger' ) return redirect ( request . referrer ) if current app . config . get ( 'LOQUSDB SETTINGS' ) : data [ 'observations' ] = controllers . observations ( store , loqusdb , case obj , data [ 'variant' ] ) data [ 'cancer' ] = request . args . get ( 'cancer' ) == 'yes' return dict ( institute = institute obj , case = case obj , * * data )", "predictions": ["show a build build build and store it in a build"], "references": ["display a specific snv variant ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 2800, "code": "def str variants ( institute id , case name ) : page = int ( request . args . get ( 'page' , 1 ) ) variant type = request . args . get ( 'variant type' , 'clinical' ) form = Str Filters Form ( request . args ) institute obj , case obj = institute and case ( store , institute id , case name ) query = form . data query [ 'variant type' ] = variant type variants query = store . variants ( case obj [ ' id' ] , category = 'str' , query = query ) data = controllers . str variants ( store , institute obj , case obj , variants query , page ) return dict ( institute = institute obj , case = case obj , variant type = variant type , form = form , page = page , * * data )", "predictions": ["return cell cell . ."], "references": ["display a list of str variants ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 2801, "code": "def sv variant ( institute id , case name , variant id ) : data = controllers . sv variant ( store , institute id , case name , variant id ) return data", "predictions": ["create a table for a single format . ."], "references": ["display a specific structural variant ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 2802, "code": "def str variant ( institute id , case name , variant id ) : data = controllers . str variant ( store , institute id , case name , variant id ) return data", "predictions": ["returns a variant object for a given case ."], "references": ["display a specific str variant ."], "bleu": 0.16784459625186196, "rouge_l": 0.4149659863945578}
{"id": 2803, "code": "def verify ( institute id , case name , variant id , variant category , order ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) user obj = store . user ( current user . email ) comment = request . form . get ( 'verification comment' ) try : controllers . variant verification ( store = store , mail = mail , institute obj = institute obj , case obj = case obj , user obj = user obj , comment = comment , variant obj = variant obj , sender = current app . config [ 'MAIL USERNAME' ] , variant url = request . referrer , order = order , url builder = url for ) except controllers . Missing Verification Recipient Error : flash ( 'No verification recipients added to institute.' , 'danger' ) return redirect ( request . referrer )", "predictions": ["get a single verification . ."], "references": ["start procedure to validate variant using other techniques ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 2804, "code": "def clinvar ( institute id , case name , variant id ) : data = controllers . clinvar export ( store , institute id , case name , variant id ) if request . method == 'GET' : return data else : #POST form dict = request . form . to dict ( ) submission objects = set submission objects ( form dict ) open submission = store . get open clinvar submission ( current user . email , institute id ) updated submission = store . add to submission ( open submission [ ' id' ] , submission objects ) return redirect ( url for ( 'cases.clinvar submissions' , institute id = institute id ) )", "predictions": ["add a type for a specific case"], "references": ["build a clinvar submission form for a variant ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 2805, "code": "def cancer variants ( institute id , case name ) : data = controllers . cancer variants ( store , request . args , institute id , case name ) return data", "predictions": ["get type of a get in a case in a case in a case in the database in the case in the controllers in a case ."], "references": ["show cancer variants overview ."], "bleu": 0.044915755686574035, "rouge_l": 0.07134502923976607}
{"id": 2806, "code": "def variant acmg ( institute id , case name , variant id ) : if request . method == 'GET' : data = controllers . variant acmg ( store , institute id , case name , variant id ) return data else : criteria = [ ] criteria terms = request . form . getlist ( 'criteria' ) for term in criteria terms : criteria . append ( dict ( term = term , comment = request . form . get ( \"comment-{}\" . format ( term ) ) , links = [ request . form . get ( \"link-{}\" . format ( term ) ) ] , ) ) acmg = controllers . variant acmg post ( store , institute id , case name , variant id , current user . email , criteria ) flash ( \"classified as: {}\" . format ( acmg ) , 'info' ) return redirect ( url for ( '.variant' , institute id = institute id , case name = case name , variant id = variant id ) )", "predictions": ["create a adapter for a adapter * criteria * * * * ."], "references": ["acmg classification form ."], "bleu": 0.09552040806823771, "rouge_l": 0.1300639658848614}
{"id": 2807, "code": "def evaluation ( evaluation id ) : evaluation obj = store . get evaluation ( evaluation id ) controllers . evaluation ( store , evaluation obj ) if request . method == 'POST' : link = url for ( '.variant' , institute id = evaluation obj [ 'institute' ] [ ' id' ] , case name = evaluation obj [ 'case' ] [ 'display name' ] , variant id = evaluation obj [ 'variant specific' ] ) store . delete evaluation ( evaluation obj ) return redirect ( link ) return dict ( evaluation = evaluation obj , institute = evaluation obj [ 'institute' ] , case = evaluation obj [ 'case' ] , variant = evaluation obj [ 'variant' ] , CRITERIA = ACMG CRITERIA )", "predictions": ["delete an user s user account os os os os os os os os os ."], "references": ["show or delete an acmg evaluation ."], "bleu": 0.10878661088699644, "rouge_l": 0.28067484662576686}
{"id": 2808, "code": "def acmg ( ) : criteria = request . args . getlist ( 'criterion' ) classification = get acmg ( criteria ) return jsonify ( dict ( classification = classification ) )", "predictions": ["f - related path to the path"], "references": ["calculate an acmg classification from submitted criteria ."], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 2809, "code": "def upload panel ( institute id , case name ) : file = form . symbol file . data if file . filename == '' : flash ( 'No selected file' , 'warning' ) return redirect ( request . referrer ) try : stream = io . String IO ( file . stream . read ( ) . decode ( 'utf-8' ) , newline = None ) except Unicode Decode Error as error : flash ( \"Only text files are supported!\" , 'warning' ) return redirect ( request . referrer ) category = request . args . get ( 'category' ) if ( category == 'sv' ) : form = Sv Filters Form ( request . args ) else : form = Filters Form ( request . args ) hgnc symbols = set ( form . hgnc symbols . data ) new hgnc symbols = controllers . upload panel ( store , institute id , case name , stream ) hgnc symbols . update ( new hgnc symbols ) form . hgnc symbols . data = ',' . join ( hgnc symbols ) form . gene panels . data = '' if ( category == 'sv' ) : return redirect ( url for ( '.sv variants' , institute id = institute id , case name = case name , * * form . data ) , code = 307 ) else : return redirect ( url for ( '.variants' , institute id = institute id , case name = case name , * * form . data ) , code = 307 )", "predictions": ["additional a files to a files . . . . . . . . . ."], "references": ["parse gene panel file and fill in hgnc symbols for filter ."], "bleu": 0.07692375026049747, "rouge_l": 0.07331730769230768}
{"id": 2810, "code": "def download verified ( ) : user obj = store . user ( current user . email ) user institutes = user obj . get ( 'institutes' ) temp excel dir = os . path . join ( variants bp . static folder , 'verified folder' ) os . makedirs ( temp excel dir , exist ok = True ) written files = controllers . verified excel file ( store , user institutes , temp excel dir ) if written files : today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) data = io . Bytes IO ( ) with zipfile . Zip File ( data , mode = 'w' ) as z : for f name in pathlib . Path ( temp excel dir ) . iterdir ( ) : zipfile . Zip File z . write ( f name , os . path . basename ( f name ) ) data . seek ( 0 ) shutil . rmtree ( temp excel dir ) return send file ( data , mimetype = 'application/zip' , as attachment = True , attachment filename = ' ' . join ( [ 'scout' , 'verified variants' , today ] ) + '.zip' ) else : flash ( \"No verified variants could be exported for user's institutes\" , 'warning' ) return redirect ( request . referrer )", "predictions": ["truncate an string to a is rendered . ."], "references": ["download all verified variants for user s cases"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2811, "code": "def add incomplete penetrance ( genes , alias genes , hpo lines ) : LOG . info ( \"Add incomplete penetrance info\" ) for hgnc symbol in get incomplete penetrance genes ( hpo lines ) : for hgnc id in get correct ids ( hgnc symbol , alias genes ) : genes [ hgnc id ] [ 'incomplete penetrance' ] = True", "predictions": ["filter dict of items to items"], "references": ["add information of incomplete penetrance"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 2812, "code": "def panels ( ) : if request . method == 'POST' : csv file = request . files [ 'csv file' ] content = csv file . stream . read ( ) lines = None try : if b'\\n' in content : lines = content . decode ( 'utf-8' , 'ignore' ) . split ( '\\n' ) else : lines = content . decode ( 'windows-1252' ) . split ( '\\r' ) except Exception as err : flash ( 'Something went wrong while parsing the panel CSV file! ({})' . format ( err ) , 'danger' ) return redirect ( request . referrer ) new panel name = request . form . get ( 'new panel name' ) if new panel name : #create a new panel new panel id = controllers . new panel ( store = store , institute id = request . form [ 'institute' ] , panel name = new panel name , display name = request . form [ 'display name' ] , csv lines = lines , ) if new panel id is None : flash ( 'Something went wrong and the panel list was not updated!' , 'warning' ) return redirect ( request . referrer ) else : flash ( \"new gene panel added, {}!\" . format ( new panel name ) , 'success' ) return redirect ( url for ( 'panels.panel' , panel id = new panel id ) ) else : update option = request . form [ 'modify option' ] panel obj = controllers . update panel ( store = store , panel name = request . form [ 'panel name' ] , csv lines = lines , option = update option ) if panel obj is None : return abort ( 404 , \"gene panel not found: {}\" . format ( request . form [ 'panel name' ] ) ) else : return redirect ( url for ( 'panels.panel' , panel id = panel obj [ ' id' ] ) ) institutes = list ( user institutes ( store , current user ) ) panel names = [ name for institute in institutes for name in store . gene panels ( institute id = institute [ ' id' ] ) . distinct ( 'panel name' ) ] panel versions = { } for name in panel names : panel versions [ name ] = store . gene panels ( panel id = name ) panel groups = [ ] for institute obj in institutes : institute panels = store . latest panels ( institute obj [ ' id' ] ) panel groups . append ( ( institute obj , institute panels ) ) return dict ( panel groups = panel groups , panel names = panel names , panel versions = panel versions , institutes = institutes )", "predictions": ["display a institute for a panel for the panel for the panel for the panel for the panel for the panel for the panel for the panel for the panel for"], "references": ["show all panels for a case ."], "bleu": 0.0513487742994337, "rouge_l": 0.11879259980525803}
{"id": 2813, "code": "def panel update ( panel id ) : panel obj = store . panel ( panel id ) update version = request . form . get ( 'version' , None ) new panel id = store . apply pending ( panel obj , update version ) return redirect ( url for ( 'panels.panel' , panel id = new panel id ) )", "predictions": ["update a adapter for a adapter for a adapter for a adapter for a adapter for a adapter"], "references": ["update panel to a new version ."], "bleu": 0.07535838128770536, "rouge_l": 0.17378917378917377}
{"id": 2814, "code": "def panel export ( panel id ) : panel obj = store . panel ( panel id ) data = controllers . panel export ( store , panel obj ) data [ 'report created at' ] = datetime . datetime . now ( ) . strftime ( \"%Y-%m-%d\" ) html report = render template ( 'panels/panel pdf simple.html' , * * data ) return render pdf ( HTML ( string = html report ) , download filename = data [ 'panel' ] [ 'panel name' ] + ' ' + str ( data [ 'panel' ] [ 'version' ] ) + ' ' + datetime . datetime . now ( ) . strftime ( \"%Y-%m-%d\" ) + ' scout.pdf' )", "predictions": ["and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and"], "references": ["export panel to pdf file"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2815, "code": "def gene edit ( panel id , hgnc id ) : panel obj = store . panel ( panel id ) hgnc gene = store . hgnc gene ( hgnc id ) panel gene = controllers . existing gene ( store , panel obj , hgnc id ) form = Panel Gene Form ( ) transcript choices = [ ] for transcript in hgnc gene [ 'transcripts' ] : if transcript . get ( 'refseq id' ) : refseq id = transcript . get ( 'refseq id' ) transcript choices . append ( ( refseq id , refseq id ) ) form . disease associated transcripts . choices = transcript choices if form . validate on submit ( ) : action = 'edit' if panel gene else 'add' info data = form . data . copy ( ) if 'csrf token' in info data : del info data [ 'csrf token' ] store . add pending ( panel obj , hgnc gene , action = action , info = info data ) return redirect ( url for ( '.panel' , panel id = panel id ) ) if panel gene : for field key in [ 'disease associated transcripts' , 'reduced penetrance' , 'mosaicism' , 'inheritance models' , 'database entry version' , 'comment' ] : form field = getattr ( form , field key ) if not form field . data : panel value = panel gene . get ( field key ) if panel value is not None : form field . process data ( panel value ) return dict ( panel = panel obj , form = form , gene = hgnc gene , panel gene = panel gene )", "predictions": ["in a call to in a call to in a call"], "references": ["edit additional information about a panel gene ."], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 2816, "code": "def delivery report ( context , case id , report path , update ) : adapter = context . obj [ 'adapter' ] try : load delivery report ( adapter = adapter , case id = case id , report path = report path , update = update ) LOG . info ( \"saved report to case!\" ) except Exception as e : LOG . error ( e ) context . abort ( )", "predictions": ["create a delivery report ."], "references": ["add delivery report to an existing case ."], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 2817, "code": "def whitelist ( context ) : LOG . info ( \"Running scout view users\" ) adapter = context . obj [ 'adapter' ] for whitelist obj in adapter . whitelist collection . find ( ) : click . echo ( whitelist obj [ ' id' ] )", "predictions": ["whitelist your whitelist ."], "references": ["show all objects in the whitelist collection"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 2818, "code": "def gene ( store , hgnc id ) : res = { 'builds' : { '37' : None , '38' : None } , 'symbol' : None , 'description' : None , 'ensembl id' : None , 'record' : None } for build in res [ 'builds' ] : record = store . hgnc gene ( hgnc id , build = build ) if record : record [ 'position' ] = \"{this[chromosome]}:{this[start]}-{this[end]}\" . format ( this = record ) res [ 'aliases' ] = record [ 'aliases' ] res [ 'hgnc id' ] = record [ 'hgnc id' ] res [ 'description' ] = record [ 'description' ] res [ 'builds' ] [ build ] = record res [ 'symbol' ] = record [ 'hgnc symbol' ] res [ 'description' ] = record [ 'description' ] res [ 'entrez id' ] = record . get ( 'entrez id' ) res [ 'pli score' ] = record . get ( 'pli score' ) add gene links ( record , int ( build ) ) res [ 'omim id' ] = record . get ( 'omim id' ) res [ 'incomplete penetrance' ] = record . get ( 'incomplete penetrance' , False ) res [ 'inheritance models' ] = record . get ( 'inheritance models' , [ ] ) for transcript in record [ 'transcripts' ] : transcript [ 'position' ] = ( \"{this[chrom]}:{this[start]}-{this[end]}\" . format ( this = transcript ) ) add tx links ( transcript , build ) for phenotype in record . get ( 'phenotypes' , [ ] ) : phenotype [ 'omim link' ] = omim ( phenotype . get ( 'mim number' ) ) if not res [ 'record' ] : res [ 'record' ] = record if not any ( res . values ( ) ) : raise Value Error return res", "predictions": ["construct a gene object from a hgnc id"], "references": ["parse information about a gene ."], "bleu": 0.21105340631872638, "rouge_l": 0.2932692307692307}
{"id": 2819, "code": "def genes to json ( store , query ) : gene query = store . hgnc genes ( query , search = True ) json terms = [ { 'name' : \"{} | {} ({})\" . format ( gene [ 'hgnc id' ] , gene [ 'hgnc symbol' ] , ', ' . join ( gene [ 'aliases' ] ) ) , 'id' : gene [ 'hgnc id' ] } for gene in gene query ] return json terms", "predictions": ["transform genes to json"], "references": ["fetch matching genes and convert to json ."], "bleu": 0.18693159143202892, "rouge_l": 0.47164948453608246}
{"id": 2820, "code": "def index ( ) : accessible institutes = current user . institutes if not 'admin' in current user . roles : accessible institutes = current user . institutes if not accessible institutes : flash ( 'Not allowed to see information - please visit the dashboard later!' ) return redirect ( url for ( 'cases.dahboard general.html' ) ) LOG . debug ( 'User accessible institutes: {}' . format ( accessible institutes ) ) institutes = [ inst for inst in store . institutes ( accessible institutes ) ] institutes . insert ( 0 , { ' id' : None , 'display name' : 'All institutes' } ) institute id = None slice query = None panel = 1 if request . method == 'POST' : institute id = request . form . get ( 'institute' ) slice query = request . form . get ( 'query' ) panel = request . form . get ( 'pane id' ) elif request . method == 'GET' : institute id = request . args . get ( 'institute' ) slice query = request . args . get ( 'query' ) #1) Their default institute when the page is first loaded #2) if they ask for an institute that they don't belong to #3) if they want perform a query on all institutes if not institute id : institute id = accessible institutes [ 0 ] elif ( not current user . is admin ) and ( slice query and institute id == 'None' ) : institute id = accessible institutes [ 0 ] elif ( not institute id in accessible institutes ) and not ( institute id == 'None' ) : institute id = accessible institutes [ 0 ] LOG . info ( \"Fetch all cases with institute: %s\" , institute id ) data = get dashboard info ( store , institute id , slice query ) data [ 'institutes' ] = institutes data [ 'choice' ] = institute id total cases = data [ 'total cases' ] LOG . info ( \"Found %s cases\" , total cases ) if total cases == 0 : flash ( 'no cases found for institute {} (with that query) - please visit the dashboard later!' . format ( institute id ) , 'info' ) return render template ( 'dashboard/dashboard general.html' , institute = institute id , query = slice query , panel = panel , * * data )", "predictions": ["show all cases of a dashboard ."], "references": ["display the scout dashboard ."], "bleu": 0.24446151121745047, "rouge_l": 0.34366197183098596}
{"id": 2821, "code": "def transcripts ( context , build , hgnc id , json ) : LOG . info ( \"Running scout view transcripts\" ) adapter = context . obj [ 'adapter' ] if not json : click . echo ( \"Chromosome\\tstart\\tend\\ttranscript id\\thgnc id\\trefseq\\tis primary\" ) for tx obj in adapter . transcripts ( build = build , hgnc id = hgnc id ) : if json : pp ( tx obj ) continue click . echo ( \"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\" . format ( tx obj [ 'chrom' ] , tx obj [ 'start' ] , tx obj [ 'end' ] , tx obj [ 'ensembl transcript id' ] , tx obj [ 'hgnc id' ] , tx obj . get ( 'refseq id' , '' ) , tx obj . get ( 'is primary' ) or '' , ) )", "predictions": ["list transcripts for a specific hgnc ."], "references": ["show all transcripts in the database"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 2822, "code": "def variants ( store , institute obj , case obj , variants query , page = 1 , per page = 50 ) : variant count = variants query . count ( ) skip count = per page * max ( page - 1 , 0 ) more variants = True if variant count > ( skip count + per page ) else False variant res = variants query . skip ( skip count ) . limit ( per page ) genome build = case obj . get ( 'genome build' , '37' ) if genome build not in [ '37' , '38' ] : genome build = '37' variants = [ ] for variant obj in variant res : overlapping svs = [ sv for sv in store . overlapping ( variant obj ) ] variant obj [ 'overlapping' ] = overlapping svs or None variants . append ( parse variant ( store , institute obj , case obj , variant obj , update = True , genome build = genome build ) ) return { 'variants' : variants , 'more variants' : more variants , }", "predictions": ["get variants of all variants in a case ."], "references": ["pre - process list of variants ."], "bleu": 0.16784459625186196, "rouge_l": 0.38364779874213834}
{"id": 2823, "code": "def sv variants ( store , institute obj , case obj , variants query , page = 1 , per page = 50 ) : skip count = ( per page * max ( page - 1 , 0 ) ) more variants = True if variants query . count ( ) > ( skip count + per page ) else False genome build = case obj . get ( 'genome build' , '37' ) if genome build not in [ '37' , '38' ] : genome build = '37' return { 'variants' : ( parse variant ( store , institute obj , case obj , variant , genome build = genome build ) for variant in variants query . skip ( skip count ) . limit ( per page ) ) , 'more variants' : more variants , }", "predictions": ["sv variants in a case of a case ."], "references": ["pre - process list of sv variants ."], "bleu": 0.21105340631872635, "rouge_l": 0.35672514619883033}
{"id": 2824, "code": "def str variants ( store , institute obj , case obj , variants query , page = 1 , per page = 50 ) : return variants ( store , institute obj , case obj , variants query , page , per page )", "predictions": ["display variants in a case of a case ."], "references": ["pre - process list of str variants ."], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 2825, "code": "def get predictions ( genes ) : data = { 'sift predictions' : [ ] , 'polyphen predictions' : [ ] , 'region annotations' : [ ] , 'functional annotations' : [ ] } for gene obj in genes : for pred key in data : gene key = pred key [ : - 1 ] if len ( genes ) == 1 : value = gene obj . get ( gene key , '-' ) else : gene id = gene obj . get ( 'hgnc symbol' ) or str ( gene obj [ 'hgnc id' ] ) value = ':' . join ( [ gene id , gene obj . get ( gene key , '-' ) ] ) data [ pred key ] . append ( value ) return data", "predictions": ["retrieve predictions from predictions ."], "references": ["get sift predictions from genes ."], "bleu": 0.31610981104846864, "rouge_l": 0.5366568914956013}
{"id": 2826, "code": "def find bai file ( bam file ) : bai file = bam file . replace ( '.bam' , '.bai' ) if not os . path . exists ( bai file ) : bai file = \"{}.bai\" . format ( bam file ) return bai file", "predictions": ["find the bai file in the bai ."], "references": ["find out bai file by extension given the bam file ."], "bleu": 0.17250013293422076, "rouge_l": 0.511744966442953}
{"id": 2827, "code": "def observations ( store , loqusdb , case obj , variant obj ) : composite id = ( \"{this[chromosome]} {this[position]} {this[reference]} \" \"{this[alternative]}\" . format ( this = variant obj ) ) obs data = loqusdb . get variant ( { ' id' : composite id } ) or { } obs data [ 'total' ] = loqusdb . case count ( ) obs data [ 'cases' ] = [ ] institute id = variant obj [ 'institute' ] for case id in obs data . get ( 'families' , [ ] ) : if case id != variant obj [ 'case id' ] and case id . startswith ( institute id ) : other variant = store . variant ( variant obj [ 'variant id' ] , case id = case id ) other case = store . case ( case id ) obs data [ 'cases' ] . append ( dict ( case = other case , variant = other variant ) ) return obs data", "predictions": ["display status of a variant"], "references": ["query observations for a variant ."], "bleu": 0.2941733261715515, "rouge_l": 0.3577712609970674}
{"id": 2828, "code": "def parse gene ( gene obj , build = None ) : build = build or 37 if gene obj . get ( 'common' ) : add gene links ( gene obj , build ) refseq transcripts = [ ] for tx obj in gene obj [ 'transcripts' ] : parse transcript ( gene obj , tx obj , build ) if not tx obj . get ( 'refseq id' ) : continue refseq transcripts . append ( tx obj ) gene obj [ 'primary transcripts' ] = ( refseq transcripts if refseq transcripts else [ ] )", "predictions": ["parse transcripts and populate populate objects"], "references": ["parse variant genes ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 2829, "code": "def transcript str ( transcript obj , gene name = None ) : if transcript obj . get ( 'exon' ) : gene part , part count raw = 'exon' , transcript obj [ 'exon' ] elif transcript obj . get ( 'intron' ) : gene part , part count raw = 'intron' , transcript obj [ 'intron' ] else : gene part , part count raw = 'intergenic' , '0' part count = part count raw . rpartition ( '/' ) [ 0 ] change str = \"{}:{}{}:{}:{}\" . format ( transcript obj . get ( 'refseq id' , '' ) , gene part , part count , transcript obj . get ( 'coding sequence name' , 'NA' ) , transcript obj . get ( 'protein sequence name' , 'NA' ) , ) if gene name : change str = \"{}:\" . format ( gene name ) + change str return change str", "predictions": ["returns a transcript string from the transcript ."], "references": ["generate amino acid change as a string ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 2830, "code": "def end position ( variant obj ) : alt bases = len ( variant obj [ 'alternative' ] ) num bases = max ( len ( variant obj [ 'reference' ] ) , alt bases ) return variant obj [ 'position' ] + ( num bases - 1 )", "predictions": ["display the end of a variant"], "references": ["calculate end position for a variant ."], "bleu": 0.2644358066258934, "rouge_l": 0.45522388059701485}
{"id": 2831, "code": "def clinsig human ( variant obj ) : for clinsig obj in variant obj [ 'clnsig' ] : if isinstance ( clinsig obj [ 'accession' ] , int ) : link = \"https://www.ncbi.nlm.nih.gov/clinvar/variation/{}\" else : link = \"https://www.ncbi.nlm.nih.gov/clinvar/{}\" human str = 'not provided' if clinsig obj . get ( 'value' ) : try : int ( clinsig obj [ 'value' ] ) human str = CLINSIG MAP . get ( clinsig obj [ 'value' ] , 'not provided' ) except Value Error : human str = clinsig obj [ 'value' ] clinsig obj [ 'human' ] = human str clinsig obj [ 'link' ] = link . format ( clinsig obj [ 'accession' ] ) yield clinsig obj", "predictions": ["return the human - readable human - readable human - readable human - readable human - readable human - readable human - readable"], "references": ["convert to human readable version of clinsig evaluation ."], "bleu": 0.05856458233275369, "rouge_l": 0.13570634037819798}
{"id": 2832, "code": "def thousandg link ( variant obj , build = None ) : dbsnp id = variant obj . get ( 'dbsnp id' ) build = build or 37 if not dbsnp id : return None if build == 37 : url template = ( \"http://grch37.ensembl.org/Homo sapiens/Variation/Explore\" \"?v={};vdb=variation\" ) else : url template = ( \"http://www.ensembl.org/Homo sapiens/Variation/Explore\" \"?v={};vdb=variation\" ) return url template . format ( dbsnp id )", "predictions": ["returns the thousandg for the thousandg object ."], "references": ["compose link to 1000g page for detailed information ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 2833, "code": "def beacon link ( variant obj , build = None ) : build = build or 37 url template = ( \"https://beacon-network.org/#/search?pos={this[position]}&\" \"chrom={this[chromosome]}&allele={this[alternative]}&\" \"ref={this[reference]}&rs=GR Ch37\" ) return url template . format ( this = variant obj )", "predictions": ["return the beacon link for a beacon ."], "references": ["compose link to beacon network ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 2834, "code": "def ucsc link ( variant obj , build = None ) : build = build or 37 url template = ( \"http://genome.ucsc.edu/cgi-bin/hg Tracks?db=hg19&\" \"position=chr{this[chromosome]}:{this[position]}\" \"-{this[position]}&dgv=pack&known Gene=pack&omim Gene=pack\" ) if build == 38 : url template = ( \"http://genome.ucsc.edu/cgi-bin/hg Tracks?db=hg20&\" \"position=chr{this[chromosome]}:{this[position]}\" \"-{this[position]}&dgv=pack&known Gene=pack&omim Gene=pack\" ) return url template . format ( this = variant obj )", "predictions": ["return the ucsc link for the ucsc ucsc ."], "references": ["compose link to ucsc ."], "bleu": 0.19960198807747329, "rouge_l": 0.4518518518518518}
{"id": 2835, "code": "def spidex human ( variant obj ) : if variant obj . get ( 'spidex' ) is None : return 'not reported' elif abs ( variant obj [ 'spidex' ] ) < SPIDEX HUMAN [ 'low' ] [ 'pos' ] [ 1 ] : return 'low' elif abs ( variant obj [ 'spidex' ] ) < SPIDEX HUMAN [ 'medium' ] [ 'pos' ] [ 1 ] : return 'medium' else : return 'high'", "predictions": ["return the human - readable human - readable human - readable human - readable human - readable human - readable human - readable of the variant of the variant of the"], "references": ["translate spidex annotation to human readable string ."], "bleu": 0.04317900023606586, "rouge_l": 0.1147695202257761}
{"id": 2836, "code": "def expected inheritance ( variant obj ) : manual models = set ( ) for gene in variant obj . get ( 'genes' , [ ] ) : manual models . update ( gene . get ( 'manual inheritance' , [ ] ) ) return list ( manual models )", "predictions": ["returns a list of all expected inheritance models ."], "references": ["gather information from common gene information ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 2837, "code": "def callers ( variant obj , category = 'snv' ) : calls = set ( ) for caller in CALLERS [ category ] : if variant obj . get ( caller [ 'id' ] ) : calls . add ( ( caller [ 'name' ] , variant obj [ caller [ 'id' ] ] ) ) return list ( calls )", "predictions": ["show a list of all callers callers for a variant user ."], "references": ["return info about callers ."], "bleu": 0.11498759556447223, "rouge_l": 0.25416666666666665}
{"id": 2838, "code": "def cancer variants ( store , request args , institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) form = Cancer Filters Form ( request args ) variants query = store . variants ( case obj [ ' id' ] , category = 'cancer' , query = form . data ) . limit ( 50 ) data = dict ( institute = institute obj , case = case obj , variants = ( parse variant ( store , institute obj , case obj , variant , update = True ) for variant in variants query ) , form = form , variant type = request args . get ( 'variant type' , 'clinical' ) , ) return data", "predictions": ["get variants for a cancer ."], "references": ["fetch data related to cancer variants for a case ."], "bleu": 0.233601780743345, "rouge_l": 0.47843137254901963}
{"id": 2839, "code": "def variant acmg ( store , institute id , case name , variant id ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) return dict ( institute = institute obj , case = case obj , variant = variant obj , CRITERIA = ACMG CRITERIA , ACMG OPTIONS = ACMG OPTIONS )", "predictions": ["get parameter for a variant ."], "references": ["collect data relevant for rendering acmg classification form ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 2840, "code": "def variant acmg post ( store , institute id , case name , variant id , user email , criteria ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) user obj = store . user ( user email ) variant link = url for ( 'variants.variant' , institute id = institute id , case name = case name , variant id = variant id ) classification = store . submit evaluation ( institute obj = institute obj , case obj = case obj , variant obj = variant obj , user obj = user obj , link = variant link , criteria = criteria , ) return classification", "predictions": ["create a variant for a specific variant ."], "references": ["calculate an acmg classification based on a list of criteria ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 2841, "code": "def evaluation ( store , evaluation obj ) : evaluation obj [ 'institute' ] = store . institute ( evaluation obj [ 'institute id' ] ) evaluation obj [ 'case' ] = store . case ( evaluation obj [ 'case id' ] ) evaluation obj [ 'variant' ] = store . variant ( evaluation obj [ 'variant specific' ] ) evaluation obj [ 'criteria' ] = { criterion [ 'term' ] : criterion for criterion in evaluation obj [ 'criteria' ] } evaluation obj [ 'classification' ] = ACMG COMPLETE MAP [ evaluation obj [ 'classification' ] ] return evaluation obj", "predictions": ["display the evaluation of an evaluation"], "references": ["fetch and fill - in evaluation object ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 2842, "code": "def upload panel ( store , institute id , case name , stream ) : institute obj , case obj = institute and case ( store , institute id , case name ) raw symbols = [ line . strip ( ) . split ( '\\t' ) [ 0 ] for line in stream if line and not line . startswith ( '#' ) ] hgnc symbols = [ ] for raw symbol in raw symbols : if store . hgnc genes ( raw symbol ) . count ( ) == 0 : flash ( \"HGNC symbol not found: {}\" . format ( raw symbol ) , 'warning' ) else : hgnc symbols . append ( raw symbol ) return hgnc symbols", "predictions": ["upload a panel to a panel ."], "references": ["parse out hgnc symbols from a stream ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 2843, "code": "def export genes ( adapter , build = '37' ) : LOG . info ( \"Exporting all genes to .bed format\" ) for gene obj in adapter . all genes ( build = build ) : yield gene obj", "predictions": ["export all genes genes ."], "references": ["export all genes from the database"], "bleu": 0.41602390756021224, "rouge_l": 0.5366568914956013}
{"id": 2844, "code": "def index ( context , collection name ) : LOG . info ( \"Running scout view index\" ) adapter = context . obj [ 'adapter' ] i = 0 click . echo ( \"collection\\tindex\" ) for collection name in adapter . collections ( ) : for index in adapter . indexes ( collection name ) : click . echo ( \"{0}\\t{1}\" . format ( collection name , index ) ) i += 1 if i == 0 : LOG . info ( \"No indexes found\" )", "predictions": ["display or list indexes for a collection of indexes ."], "references": ["show all indexes in the database"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 2845, "code": "def genes ( context , build , json ) : LOG . info ( \"Running scout export genes\" ) adapter = context . obj [ 'adapter' ] result = adapter . all genes ( build = build ) if json : click . echo ( dumps ( result ) ) return gene string = ( \"{0}\\t{1}\\t{2}\\t{3}\\t{4}\" ) click . echo ( \"#Chromosom\\t Start\\t End\\t Hgnc id\\t Hgnc symbol\" ) for gene obj in result : click . echo ( gene string . format ( gene obj [ 'chromosome' ] , gene obj [ 'start' ] , gene obj [ 'end' ] , gene obj [ 'hgnc id' ] , gene obj [ 'hgnc symbol' ] , ) )", "predictions": ["lists all genes genes ."], "references": ["export all genes from a build"], "bleu": 0.2941733261715515, "rouge_l": 0.3577712609970674}
{"id": 2846, "code": "def case ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) if case obj is None : return abort ( 404 ) return Response ( json util . dumps ( case obj ) , mimetype = 'application/json' )", "predictions": ["show the case of a case ."], "references": ["return a variant ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 2847, "code": "def variant ( institute id , case name , variant id ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) return Response ( json util . dumps ( variant obj ) , mimetype = 'application/json' )", "predictions": ["show a variant in a specified case ."], "references": ["display a specific snv variant ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 2848, "code": "def collections ( context ) : LOG . info ( \"Running scout view collections\" ) adapter = context . obj [ 'adapter' ] for collection name in adapter . collections ( ) : click . echo ( collection name )", "predictions": ["display all delivery delivery delivery id id id id id id id id id id id id id id id id id id id id id id id id id id"], "references": ["show all collections in the database"], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 2849, "code": "def institute ( ctx , internal id , display name , sanger recipients ) : adapter = ctx . obj [ 'adapter' ] if not internal id : logger . warning ( \"A institute has to have an internal id\" ) ctx . abort ( ) if not display name : display name = internal id if sanger recipients : sanger recipients = list ( sanger recipients ) try : load institute ( adapter = adapter , internal id = internal id , display name = display name , sanger recipients = sanger recipients ) except Exception as e : logger . warning ( e ) ctx . abort ( )", "predictions": ["whitelist the scout adapter obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj"], "references": ["create a new institute and add it to the database"], "bleu": 0.05809665204409193, "rouge_l": 0.06892655367231638}
{"id": 2850, "code": "def get file handle ( file path ) : if file path . endswith ( '.gz' ) : file handle = getreader ( 'utf-8' ) ( gzip . open ( file path , 'r' ) , errors = 'replace' ) else : file handle = open ( file path , 'r' , encoding = 'utf-8' ) return file handle", "predictions": ["} the file store for the given file res res res res res res res ."], "references": ["return a opened file"], "bleu": 0.07692375026049747, "rouge_l": 0.11213235294117647}
{"id": 2851, "code": "def get net ( req ) : try : nxt , prev = map ( int , ( req . GET . get ( 'cal next' , 0 ) , req . GET . get ( 'cal prev' , 0 ) ) ) net = nxt - prev except Exception : net = 0 return net", "predictions": ["genes function to genes the to genes"], "references": ["get the net of any next and prev querystrings ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 2852, "code": "def get next and prev ( net ) : if net == 0 : nxt = prev = 1 elif net > 0 : nxt = net + 1 prev = - ( net - 1 ) else : nxt = net + 1 prev = abs ( net ) + 1 return nxt , prev", "predictions": ["returns the next and and of the institutes and"], "references": ["returns what the next and prev querystrings should be ."], "bleu": 0.24855227187657006, "rouge_l": 0.41709401709401706}
{"id": 2853, "code": "def check year ( year , month , error , error msg ) : if year not in xrange ( ( now . year - 50 ) , ( now . year + 51 ) ) : year = now . year month = now . month error = error msg return year , month , error", "predictions": ["transcripts for the year year"], "references": ["checks that the year is within 50 years from now ."], "bleu": 0.10822031883953476, "rouge_l": 0.2341650671785029}
{"id": 2854, "code": "def add peddy information ( config data ) : ped info = { } ped check = { } sex check = { } relations = [ ] if config data . get ( 'peddy ped' ) : file handle = open ( config data [ 'peddy ped' ] , 'r' ) for ind info in parse peddy ped ( file handle ) : ped info [ ind info [ 'sample id' ] ] = ind info if config data . get ( 'peddy ped check' ) : file handle = open ( config data [ 'peddy ped check' ] , 'r' ) for pair info in parse peddy ped check ( file handle ) : ped check [ ( pair info [ 'sample a' ] , pair info [ 'sample b' ] ) ] = pair info if config data . get ( 'peddy sex check' ) : file handle = open ( config data [ 'peddy sex check' ] , 'r' ) for ind info in parse peddy sex check ( file handle ) : sex check [ ind info [ 'sample id' ] ] = ind info if not ped info : return analysis inds = { } for ind in config data [ 'samples' ] : ind id = ind [ 'sample id' ] analysis inds [ ind id ] = ind for ind id in analysis inds : ind = analysis inds [ ind id ] if ind id in ped info : ind [ 'predicted ancestry' ] = ped info [ ind id ] . get ( 'ancestry-prediction' , 'UNKNOWN' ) if ind id in sex check : if sex check [ ind id ] [ 'error' ] : ind [ 'confirmed sex' ] = False else : ind [ 'confirmed sex' ] = True for parent in [ 'mother' , 'father' ] : if ind [ parent ] != '0' : for pair in ped check : if ( ind id in pair and ind [ parent ] in pair ) : if ped check [ pair ] [ 'parent error' ] : analysis inds [ ind [ parent ] ] [ 'confirmed parent' ] = False else : if 'confirmed parent' not in analysis inds [ ind [ parent ] ] : analysis inds [ ind [ parent ] ] [ 'confirmed parent' ] = True", "predictions": ["variants page store else variants from peddy"], "references": ["add information from peddy outfiles to the individuals"], "bleu": 0.21191828141393895, "rouge_l": 0.2634989200863931}
{"id": 2855, "code": "def panel ( context , path , date , display name , version , panel type , panel id , institute , omim , api key , panel app ) : adapter = context . obj [ 'adapter' ] institute = institute or 'cust000' if omim : api key = api key or context . obj . get ( 'omim api key' ) if not api key : LOG . warning ( \"Please provide a omim api key to load the omim gene panel\" ) context . abort ( ) #Check if OMIM-AUTO exists if adapter . gene panel ( panel id = 'OMIM-AUTO' ) : LOG . warning ( \"OMIM-AUTO already exists in database\" ) LOG . info ( \"To create a new version use scout update omim\" ) return try : adapter . load omim panel ( api key , institute = institute ) except Exception as err : LOG . error ( err ) context . abort ( ) if panel app : load panel app ( adapter , panel id , institute = institute ) if ( omim or panel app ) : return if path is None : LOG . info ( \"Please provide a panel\" ) return try : load panel ( path , adapter , date , display name , version , panel type , panel id , institute ) except Exception as err : LOG . warning ( err ) context . abort ( )", "predictions": ["create a new object - sv sv - sv sv - sv sv - sv sv"], "references": ["add a gene panel to the database ."], "bleu": 0.07692375026049747, "rouge_l": 0.08866279069767442}
{"id": 2856, "code": "def panel ( context , panel id , version ) : LOG . info ( \"Running scout delete panel\" ) adapter = context . obj [ 'adapter' ] panel objs = adapter . gene panels ( panel id = panel id , version = version ) if panel objs . count ( ) == 0 : LOG . info ( \"No panels found\" ) for panel obj in panel objs : adapter . delete panel ( panel obj )", "predictions": ["50 the panels of a str page page page page page page page page page page page page page page page page page page page page page page page page page"], "references": ["delete a version of a gene panel or all versions of a gene panel"], "bleu": 0.0513487742994337, "rouge_l": 0.09538702111024237}
{"id": 2857, "code": "def index ( context ) : LOG . info ( \"Running scout delete index\" ) adapter = context . obj [ 'adapter' ] for collection in adapter . db . collection names ( ) : adapter . db [ collection ] . drop indexes ( ) LOG . info ( \"All indexes deleted\" )", "predictions": ["delete get get get get get get get key data ."], "references": ["delete all indexes in the database"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 2858, "code": "def user ( context , mail ) : LOG . info ( \"Running scout delete user\" ) adapter = context . obj [ 'adapter' ] user obj = adapter . user ( mail ) if not user obj : LOG . warning ( \"User {0} could not be found in database\" . format ( mail ) ) else : adapter . delete user ( mail )", "predictions": ["delete a find find of the current find ."], "references": ["delete a user from the database"], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 2859, "code": "def genes ( context , build ) : LOG . info ( \"Running scout delete genes\" ) adapter = context . obj [ 'adapter' ] if build : LOG . info ( \"Dropping genes collection for build: %s\" , build ) else : LOG . info ( \"Dropping genes collection\" ) adapter . drop genes ( )", "predictions": ["= = false if the loqusdb is enabled . . ."], "references": ["delete all genes in the database"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 2860, "code": "def exons ( context , build ) : LOG . info ( \"Running scout delete exons\" ) adapter = context . obj [ 'adapter' ] adapter . drop exons ( build )", "predictions": ["if the parse is enabled if it exists . ."], "references": ["delete all exons in the database"], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 2861, "code": "def case ( context , institute , case id , display name ) : adapter = context . obj [ 'adapter' ] if not ( case id or display name ) : click . echo ( \"Please specify what case to delete\" ) context . abort ( ) if display name : if not institute : click . echo ( \"Please specify the owner of the case that should be \" \"deleted with flag '-i/--institute'.\" ) context . abort ( ) case id = \"{0}-{1}\" . format ( institute , display name ) LOG . info ( \"Running deleting case {0}\" . format ( case id ) ) case = adapter . delete case ( case id = case id , institute id = institute , display name = display name ) if case . deleted count == 1 : adapter . delete variants ( case id = case id , variant type = 'clinical' ) adapter . delete variants ( case id = case id , variant type = 'research' ) else : LOG . warning ( \"Case does not exist in database\" ) context . abort ( )", "predictions": ["delete a transcript ."], "references": ["delete a case and it s variants from the database"], "bleu": 0.10551173833795614, "rouge_l": 0.26521739130434785}
{"id": 2862, "code": "def individuals ( context , institute , causatives , case id ) : LOG . info ( \"Running scout view individuals\" ) adapter = context . obj [ 'adapter' ] individuals = [ ] if case id : case = adapter . case ( case id = case id ) if case : cases = [ case ] else : LOG . info ( \"Could not find case %s\" , case id ) return else : cases = [ case obj for case obj in adapter . cases ( collaborator = institute , has causatives = causatives ) ] if len ( cases ) == 0 : LOG . info ( \"Could not find cases that match criteria\" ) return individuals = ( ind obj for case obj in cases for ind obj in case obj [ 'individuals' ] ) click . echo ( \"#case id\\tind id\\tdisplay name\\tsex\\tphenotype\\tmother\\tfather\" ) for case in cases : for ind obj in case [ 'individuals' ] : ind info = [ case [ ' id' ] , ind obj [ 'individual id' ] , ind obj [ 'display name' ] , SEX MAP [ int ( ind obj [ 'sex' ] ) ] , PHENOTYPE MAP [ ind obj [ 'phenotype' ] ] , ind obj [ 'mother' ] , ind obj [ 'father' ] ] click . echo ( '\\t' . join ( ind info ) )", "predictions": ["display end of a case . . . ."], "references": ["show all individuals from all cases in the database"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2863, "code": "def cases ( context , institute , display name , case id , nr variants , variants treshold ) : LOG . info ( \"Running scout view institutes\" ) adapter = context . obj [ 'adapter' ] models = [ ] if case id : case obj = adapter . case ( case id = case id ) if case obj : models . append ( case obj ) else : models = adapter . cases ( collaborator = institute , name query = display name ) models = [ case obj for case obj in models ] if not models : LOG . info ( \"No cases could be found\" ) return header = [ 'case id' , 'display name' , 'institute' ] if variants treshold : LOG . info ( \"Only show cases with more than %s variants\" , variants treshold ) nr variants = True if nr variants : LOG . info ( \"Displaying number of variants for each case\" ) header . append ( 'clinical' ) header . append ( 'research' ) click . echo ( \"#\" + '\\t' . join ( header ) ) for model in models : output str = \"{:<12}\\t{:<12}\\t{:<12}\" output values = [ model [ ' id' ] , model [ 'display name' ] , model [ 'owner' ] ] if nr variants : output str += \"\\t{:<12}\\t{:<12}\" nr clinical = 0 nr research = 0 variants = adapter . variant collection . find ( { 'case id' : model [ ' id' ] } ) i = 0 for i , var in enumerate ( variants , 1 ) : if var [ 'variant type' ] == 'clinical' : nr clinical += 1 else : nr research += 1 output values . extend ( [ nr clinical , nr research ] ) if variants treshold and i < variants treshold : LOG . debug ( \"Case %s had to few variants, skipping\" , model [ ' id' ] ) continue click . echo ( output str . format ( * output values ) )", "predictions": ["show all cases cases cases cases ."], "references": ["display cases from the database"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 2864, "code": "def load user ( user email ) : user obj = store . user ( user email ) user inst = Login User ( user obj ) if user obj else None return user inst", "predictions": ["load link and return"], "references": ["returns the currently active user as an object ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 2865, "code": "def login ( ) : if 'next' in request . args : session [ 'next url' ] = request . args [ 'next' ] if current app . config . get ( 'GOOGLE' ) : callback url = url for ( '.authorized' , external = True ) return google . authorize ( callback = callback url ) user email = request . args . get ( 'email' ) user obj = store . user ( user email ) if user obj is None : flash ( \"email not whitelisted: {}\" . format ( user email ) , 'warning' ) return redirect ( url for ( 'public.index' ) ) return perform login ( user obj )", "predictions": ["log in the user user . . . . . ."], "references": ["login a user if they have access ."], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 2866, "code": "def user events ( self , user obj = None ) : query = dict ( user id = user obj [ ' id' ] ) if user obj else dict ( ) return self . event collection . find ( query )", "predictions": ["retrieve ucsc link link link object ."], "references": ["fetch all events by a specific user ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 2867, "code": "def hpo terms ( ) : if request . method == 'GET' : data = controllers . hpo terms ( store = store , limit = 100 ) return data else : search term = request . form . get ( 'hpo term' ) limit = request . form . get ( 'limit' ) data = controllers . hpo terms ( store = store , query = search term , limit = limit ) return dict ( data , query = search term , limit = limit )", "predictions": ["returns all human readable human readable human readable human readable form if any"], "references": ["render search box and view for hpo phenotype terms"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 2868, "code": "def transcripts ( context , build ) : LOG . info ( \"Running scout export transcripts\" ) adapter = context . obj [ 'adapter' ] header = [ \"#Chrom\\t Start\\t End\\t Transcript\\t Ref Seq\\t Hgnc ID\" ] for line in header : click . echo ( line ) transcript string = ( \"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\" ) for tx obj in export transcripts ( adapter ) : click . echo ( transcript string . format ( tx obj [ 'chrom' ] , tx obj [ 'start' ] , tx obj [ 'end' ] , tx obj [ 'ensembl transcript id' ] , tx obj . get ( 'refseq id' , '' ) , tx obj [ 'hgnc id' ] , ) )", "predictions": ["display status of all expected expected expected expected expected expected expected expected expected expected expected transcript"], "references": ["export all transcripts to . bed like format"], "bleu": 0.07692375026049747, "rouge_l": 0.08866279069767442}
{"id": 2869, "code": "def exons ( context , build ) : adapter = context . obj [ 'adapter' ] start = datetime . now ( ) nr exons = adapter . exons ( build = build ) . count ( ) if nr exons : LOG . warning ( \"Dropping all exons \" ) adapter . drop exons ( build = build ) LOG . info ( \"Exons dropped\" ) ensembl exons = fetch ensembl exons ( build = build ) load exons ( adapter , ensembl exons , build ) adapter . update indexes ( ) LOG . info ( \"Time to load exons: {0}\" . format ( datetime . now ( ) - start ) )", "predictions": ["callers the callers of the ensembl"], "references": ["load exons into the scout database"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 2870, "code": "def intervals ( context , build ) : LOG . info ( \"Running scout view index\" ) adapter = context . obj [ 'adapter' ] intervals = adapter . get coding intervals ( build ) nr intervals = 0 longest = 0 for chrom in CHROMOSOMES : for iv in intervals [ chrom ] : iv len = iv . end - iv . begin if iv len > longest : longest = iv len int nr = len ( intervals . get ( chrom , [ ] ) ) click . echo ( \"{0}\\t{1}\" . format ( chrom , int nr ) ) nr intervals += int nr LOG . info ( \"Total nr intervals:%s\" , nr intervals ) LOG . info ( \"Total nr genes:%s\" , adapter . all genes ( build ) . count ( ) ) LOG . info ( \"Longest interval:%s\" , longest )", "predictions": ["show cancer cancer cancer id"], "references": ["show all indexes in the database"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 2871, "code": "def region ( context , hgnc id , case id , chromosome , start , end ) : adapter = context . obj [ 'adapter' ] load region ( adapter = adapter , case id = case id , hgnc id = hgnc id , chrom = chromosome , start = start , end = end )", "predictions": ["load a variant ."], "references": ["load all variants in a region to a existing case"], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 2872, "code": "def parse reqs ( req path = './requirements.txt' ) : install requires = [ ] with io . open ( os . path . join ( here , 'requirements.txt' ) , encoding = 'utf-8' ) as handle : lines = ( line . strip ( ) for line in handle if line . strip ( ) and not line . startswith ( '#' ) ) for line in lines : if line . startswith ( '-r' ) : install requires += parse reqs ( req path = line [ 3 : ] ) else : install requires . append ( line ) return install requires", "predictions": ["variant all requirements from the pip pip files obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj obj"], "references": ["recursively parse requirements from nested pip files ."], "bleu": 0.0645676653773519, "rouge_l": 0.2295390404515522}
{"id": 2873, "code": "def existing gene ( store , panel obj , hgnc id ) : existing genes = { gene [ 'hgnc id' ] : gene for gene in panel obj [ 'genes' ] } return existing genes . get ( hgnc id )", "predictions": ["return the gene of the gene"], "references": ["check if gene is already added to a panel ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 2874, "code": "def panel export ( store , panel obj ) : panel obj [ 'institute' ] = store . institute ( panel obj [ 'institute' ] ) full name = \"{}({})\" . format ( panel obj [ 'display name' ] , panel obj [ 'version' ] ) panel obj [ 'name and version' ] = full name return dict ( panel = panel obj )", "predictions": ["get the upload for a upload"], "references": ["preprocess a panel of genes ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 2875, "code": "def archive info ( database : Database , archive case : dict ) -> dict : data = { 'collaborators' : archive case [ 'collaborators' ] , 'synopsis' : archive case . get ( 'synopsis' ) , 'assignees' : [ ] , 'suspects' : [ ] , 'causatives' : [ ] , 'phenotype terms' : [ ] , 'phenotype groups' : [ ] , } if archive case . get ( 'assignee' ) : archive user = database . user . find one ( { ' id' : archive case [ 'assignee' ] } ) data [ 'assignee' ] . append ( archive user [ 'email' ] ) for key in [ 'suspects' , 'causatives' ] : for variant id in archive case . get ( key , [ ] ) : archive variant = database . variant . find one ( { ' id' : variant id } ) data [ key ] . append ( { 'chromosome' : archive variant [ 'chromosome' ] , 'position' : archive variant [ 'position' ] , 'reference' : archive variant [ 'reference' ] , 'alternative' : archive variant [ 'alternative' ] , 'variant type' : archive variant [ 'variant type' ] , } ) for key in [ 'phenotype terms' , 'phenotype groups' ] : for archive term in archive case . get ( key , [ ] ) : data [ key ] . append ( { 'phenotype id' : archive term [ 'phenotype id' ] , 'feature' : archive term [ 'feature' ] , } ) return data", "predictions": ["returns information about export export export ."], "references": ["get information about a case from archive ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 2876, "code": "def migrate case ( adapter : Mongo Adapter , scout case : dict , archive data : dict ) : collaborators = list ( set ( scout case [ 'collaborators' ] + archive data [ 'collaborators' ] ) ) if collaborators != scout case [ 'collaborators' ] : LOG . info ( f\"set collaborators: {', '.join(collaborators)}\" ) scout case [ 'collaborators' ] = collaborators if len ( scout case . get ( 'assignees' , [ ] ) ) == 0 : scout user = adapter . user ( archive data [ 'assignee' ] ) if scout user : scout case [ 'assignees' ] = [ archive data [ 'assignee' ] ] else : LOG . warning ( f\"{archive data['assignee']}: unable to find assigned user\" ) for key in [ 'suspects' , 'causatives' ] : scout case [ key ] = scout case . get ( key , [ ] ) for archive variant in archive data [ key ] : variant id = get variantid ( archive variant , scout case [ ' id' ] ) scout variant = adapter . variant ( variant id ) if scout variant : if scout variant [ ' id' ] in scout case [ key ] : LOG . info ( f\"{scout variant[' id']}: variant already in {key}\" ) else : LOG . info ( f\"{scout variant[' id']}: add to {key}\" ) scout variant [ key ] . append ( scout variant [ ' id' ] ) else : LOG . warning ( f\"{scout variant[' id']}: unable to find variant ({key})\" ) scout variant [ key ] . append ( variant id ) if not scout case . get ( 'synopsis' ) : scout case [ 'synopsis' ] = archive data [ 'synopsis' ] scout case [ 'is migrated' ] = True adapter . case collection . find one and replace ( { ' id' : scout case [ ' id' ] } , scout case , ) scout institute = adapter . institute ( scout case [ 'owner' ] ) scout user = adapter . user ( 'mans.magnusson@scilifelab.se' ) for key in [ 'phenotype terms' , 'phenotype groups' ] : for archive term in archive data [ key ] : adapter . add phenotype ( institute = scout institute , case = scout case , user = scout user , link = f\"/{scout case['owner']}/{scout case['display name']}\" , hpo term = archive term [ 'phenotype id' ] , is group = key == 'phenotype groups' , )", "predictions": ["index the case case to the latest info echo the variant"], "references": ["migrate case information from archive ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 2877, "code": "def migrate ( uri : str , archive uri : str , case id : str , dry : bool , force : bool ) : scout client = Mongo Client ( uri ) scout database = scout client [ uri . rsplit ( '/' , 1 ) [ - 1 ] ] scout adapter = Mongo Adapter ( database = scout database ) scout case = scout adapter . case ( case id ) if not force and scout case . get ( 'is migrated' ) : print ( \"case already migrated\" ) return archive client = Mongo Client ( archive uri ) archive database = archive client [ archive uri . rsplit ( '/' , 1 ) [ - 1 ] ] archive case = archive database . case . find one ( { 'owner' : scout case [ 'owner' ] , 'display name' : scout case [ 'display name' ] } ) archive data = archive info ( archive database , archive case ) if dry : print ( ruamel . yaml . safe dump ( archive data ) ) else : #migrate case(scout adapter, scout case, archive data) pass", "predictions": ["genes for testing ."], "references": ["update all information that was manually annotated from a old instance ."], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 2878, "code": "def hpo ( context , term , description ) : LOG . info ( \"Running scout view hpo\" ) adapter = context . obj [ 'adapter' ] if term : term = term . upper ( ) if not term . startswith ( 'HP:' ) : while len ( term ) < 7 : term = '0' + term term = 'HP:' + term LOG . info ( \"Searching for term %s\" , term ) hpo terms = adapter . hpo terms ( hpo term = term ) elif description : sorted terms = sorted ( adapter . hpo terms ( query = description ) , key = itemgetter ( 'hpo number' ) ) for term in sorted terms : term . pop ( 'genes' ) print ( \"name: {} | {} | {}\" . format ( term [ ' id' ] , term [ 'description' ] , term [ 'hpo number' ] ) ) context . abort ( ) else : hpo terms = adapter . hpo terms ( ) if hpo terms . count ( ) == 0 : LOG . warning ( \"No matching terms found\" ) return click . echo ( \"hpo id\\tdescription\\tnr genes\" ) for hpo obj in hpo terms : click . echo ( \"{0}\\t{1}\\t{2}\" . format ( hpo obj [ 'hpo id' ] , hpo obj [ 'description' ] , len ( hpo obj . get ( 'genes' , [ ] ) ) ) )", "predictions": ["case - insensitive case - insensitive case = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 0 = 1 = 1 1 1 = 1"], "references": ["show all hpo terms in the database"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2879, "code": "def create app ( config file = None , config = None ) : app = Flask ( name ) app . config . from pyfile ( 'config.py' ) app . jinja env . add extension ( 'jinja2.ext.do' ) if config : app . config . update ( config ) if config file : app . config . from pyfile ( config file ) app . mme nodes = mme nodes ( app . config . get ( 'MME URL' ) , app . config . get ( 'MME TOKEN' ) ) app . config [ \"JSON SORT KEYS\" ] = False current log level = logger . get Effective Level ( ) coloredlogs . install ( level = 'DEBUG' if app . debug else current log level ) configure extensions ( app ) register blueprints ( app ) register filters ( app ) if not ( app . debug or app . testing ) and app . config . get ( 'MAIL USERNAME' ) : configure email logging ( app ) @ app . before request def check user ( ) : if not app . config . get ( 'LOGIN DISABLED' ) and request . endpoint : static endpoint = 'static' in request . endpoint or 'report' in request . endpoint public endpoint = getattr ( app . view functions [ request . endpoint ] , 'is public' , False ) relevant endpoint = not ( static endpoint or public endpoint ) if relevant endpoint and not current user . is authenticated : next url = \"{}?{}\" . format ( request . path , request . query string . decode ( ) ) login url = url for ( 'login.login' , next = next url ) return redirect ( login url ) return app", "predictions": ["variant to create a flask application store it in the json form store store store store store store store store store store store store store store store store store store as"], "references": ["flask app factory function ."], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 2880, "code": "def configure extensions ( app ) : extensions . toolbar . init app ( app ) extensions . bootstrap . init app ( app ) extensions . mongo . init app ( app ) extensions . store . init app ( app ) extensions . login manager . init app ( app ) extensions . oauth . init app ( app ) extensions . mail . init app ( app ) Markdown ( app ) if app . config . get ( 'SQLALCHEMY DATABASE URI' ) : configure coverage ( app ) if app . config . get ( 'LOQUSDB SETTINGS' ) : extensions . loqusdb . init app ( app )", "predictions": ["flask application initialization ."], "references": ["configure flask extensions ."], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 2881, "code": "def register blueprints ( app ) : app . register blueprint ( public . public bp ) app . register blueprint ( genes . genes bp ) app . register blueprint ( cases . cases bp ) app . register blueprint ( login . login bp ) app . register blueprint ( variants . variants bp ) app . register blueprint ( panels . panels bp ) app . register blueprint ( dashboard . dashboard bp ) app . register blueprint ( api . api bp ) app . register blueprint ( alignviewers . alignviewers bp ) app . register blueprint ( phenotypes . hpo bp ) app . register blueprint ( institutes . overview )", "predictions": ["register blueprints and dashboard ."], "references": ["register flask blueprints ."], "bleu": 0.32466791547509893, "rouge_l": 0.6802973977695167}
{"id": 2882, "code": "def configure coverage ( app ) : app . config [ 'SQLALCHEMY TRACK MODIFICATIONS' ] = True if app . debug else False if chanjo api : chanjo api . init app ( app ) configure template filters ( app ) app . register blueprint ( report bp , url prefix = '/reports' ) babel = Babel ( app ) @ babel . localeselector def get locale ( ) : \"\"\"Determine locale to use for translations.\"\"\" accept languages = current app . config . get ( 'ACCEPT LANGUAGES' , [ 'en' ] ) session language = request . args . get ( 'lang' ) if session language in accept languages : current app . logger . info ( \"using session language: %s\" , session language ) return session language user language = current app . config . get ( 'REPORT LANGUAGE' ) if user language : return user language return request . accept languages . best match ( accept languages )", "predictions": ["configures the coverage language ."], "references": ["setup coverage related extensions ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 2883, "code": "def aliases ( context , build , symbol ) : LOG . info ( \"Running scout view aliases\" ) adapter = context . obj [ 'adapter' ] if symbol : alias genes = { } res = adapter . gene by alias ( symbol , build = build ) for gene obj in res : hgnc id = gene obj [ 'hgnc id' ] hgnc symbol = gene obj [ 'hgnc symbol' ] for alias in gene obj [ 'aliases' ] : true id = None if alias == hgnc symbol : true id = hgnc id if alias in alias genes : alias genes [ alias ] [ 'ids' ] . add ( hgnc id ) if true id : alias genes [ alias ] [ 'true' ] = hgnc id else : alias genes [ alias ] = { 'true' : hgnc id , 'ids' : set ( [ hgnc id ] ) } else : alias genes = adapter . genes by alias ( build = build ) if len ( alias genes ) == 0 : LOG . info ( \"No gene found for build %s\" , build ) return click . echo ( \"#hgnc symbol\\ttrue id\\thgnc ids\" ) for alias symbol in alias genes : info = alias genes [ alias symbol ] click . echo ( \"{0}\\t{1}\\t{2}\\t\" . format ( alias symbol , ( alias genes [ alias symbol ] [ 'true' ] or 'None' ) , ', ' . join ( [ str ( gene id ) for gene id in alias genes [ alias symbol ] [ 'ids' ] ] ) ) )", "predictions": ["display aliases details for a gene symbol"], "references": ["show all alias symbols and how they map to ids"], "bleu": 0.10175282441454783, "rouge_l": 0.0}
{"id": 2884, "code": "def variants ( context , collaborator , document id , case id , json ) : LOG . info ( \"Running scout export variants\" ) adapter = context . obj [ 'adapter' ] collaborator = collaborator or 'cust000' variants = export variants ( adapter , collaborator , document id = document id , case id = case id ) if json : click . echo ( dumps ( [ var for var in variants ] ) ) return vcf header = VCF HEADER #If case id is given, print more complete vcf entries, with INFO, #and genotypes if case id : vcf header [ - 1 ] = vcf header [ - 1 ] + \"\\t FORMAT\" case obj = adapter . case ( case id = case id ) for individual in case obj [ 'individuals' ] : vcf header [ - 1 ] = vcf header [ - 1 ] + \"\\t\" + individual [ 'individual id' ] #print header for line in vcf header : click . echo ( line ) for variant obj in variants : variant string = get vcf entry ( variant obj , case id = case id ) click . echo ( variant string )", "predictions": ["show variants for a vcf document ."], "references": ["export causatives for a collaborator in . vcf format"], "bleu": 0.20873176328735715, "rouge_l": 0.3667334669338677}
{"id": 2885, "code": "def serve ( context , config , host , port , debug , livereload ) : pymongo config = dict ( MONGO HOST = context . obj [ 'host' ] , MONGO PORT = context . obj [ 'port' ] , MONGO DBNAME = context . obj [ 'mongodb' ] , MONGO USERNAME = context . obj [ 'username' ] , MONGO PASSWORD = context . obj [ 'password' ] , ) valid connection = check connection ( host = pymongo config [ 'MONGO HOST' ] , port = pymongo config [ 'MONGO PORT' ] , username = pymongo config [ 'MONGO USERNAME' ] , password = pymongo config [ 'MONGO PASSWORD' ] , authdb = context . obj [ 'authdb' ] , ) log . info ( \"Test if mongod is running\" ) if not valid connection : log . warning ( \"Connection could not be established\" ) log . info ( \"Is mongod running?\" ) context . abort ( ) config = os . path . abspath ( config ) if config else None app = create app ( config = pymongo config , config file = config ) if livereload : server = Server ( app . wsgi app ) server . serve ( host = host , port = port , debug = debug ) else : app . run ( host = host , port = port , debug = debug )", "predictions": ["serve the wsgi application ."], "references": ["start the web server ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 2886, "code": "def init app ( self , app ) : host = app . config . get ( 'MONGO HOST' , 'localhost' ) port = app . config . get ( 'MONGO PORT' , 27017 ) dbname = app . config [ 'MONGO DBNAME' ] log . info ( \"connecting to database: %s:%s/%s\" , host , port , dbname ) self . setup ( app . config [ 'MONGO DATABASE' ] )", "predictions": ["flask application initialization ."], "references": ["setup via flask ."], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 2887, "code": "def setup ( self , database ) : self . db = database self . hgnc collection = database . hgnc gene self . user collection = database . user self . whitelist collection = database . whitelist self . institute collection = database . institute self . event collection = database . event self . case collection = database . case self . panel collection = database . gene panel self . hpo term collection = database . hpo term self . disease term collection = database . disease term self . variant collection = database . variant self . acmg collection = database . acmg self . clinvar collection = database . clinvar self . clinvar submission collection = database . clinvar submission self . exon collection = database . exon self . transcript collection = database . transcript", "predictions": ["sets up the initial database"], "references": ["setup connection to database ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 2888, "code": "def index ( context , update ) : LOG . info ( \"Running scout index\" ) adapter = context . obj [ 'adapter' ] if update : adapter . update indexes ( ) else : adapter . load indexes ( )", "predictions": ["load indexes from database ."], "references": ["create indexes for the database"], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 2889, "code": "def database ( context , institute name , user name , user mail , api key ) : LOG . info ( \"Running scout setup database\" ) api key = api key or context . obj . get ( 'omim api key' ) if not api key : LOG . warning ( \"Please provide a omim api key with --api-key\" ) context . abort ( ) institute name = institute name or context . obj [ 'institute name' ] user name = user name or context . obj [ 'user name' ] user mail = user mail or context . obj [ 'user mail' ] adapter = context . obj [ 'adapter' ] LOG . info ( \"Setting up database %s\" , context . obj [ 'mongodb' ] ) setup scout ( adapter = adapter , institute id = institute name , user name = user name , user mail = user mail , api key = api key )", "predictions": ["setup the database ."], "references": ["setup a scout database ."], "bleu": 0.39573418216703893, "rouge_l": 0.6535714285714286}
{"id": 2890, "code": "def setup ( context , institute , user mail , user name ) : context . obj [ 'institute name' ] = institute context . obj [ 'user name' ] = user name context . obj [ 'user mail' ] = user mail if context . invoked subcommand == 'demo' : LOG . debug ( \"Change database name to scout-demo\" ) context . obj [ 'mongodb' ] = 'scout-demo' LOG . info ( \"Setting database name to %s\" , context . obj [ 'mongodb' ] ) LOG . debug ( \"Setting host to %s\" , context . obj [ 'host' ] ) LOG . debug ( \"Setting port to %s\" , context . obj [ 'port' ] ) try : client = get connection ( host = context . obj [ 'host' ] , port = context . obj [ 'port' ] , username = context . obj [ 'username' ] , password = context . obj [ 'password' ] , mongodb = context . obj [ 'mongodb' ] ) except Connection Failure : context . abort ( ) LOG . info ( \"connecting to database %s\" , context . obj [ 'mongodb' ] ) database = client [ context . obj [ 'mongodb' ] ] LOG . info ( \"Test if mongod is running\" ) try : LOG . info ( \"Test if mongod is running\" ) database . test . find one ( ) except Server Selection Timeout Error as err : LOG . warning ( \"Connection could not be established\" ) LOG . warning ( \"Please check if mongod is running\" ) context . abort ( ) LOG . info ( \"Setting up a mongo adapter\" ) mongo adapter = Mongo Adapter ( database ) context . obj [ 'adapter' ] = mongo adapter", "predictions": ["setup the database connection ."], "references": ["setup scout instances ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 2891, "code": "def institutes ( context , institute id , json ) : LOG . info ( \"Running scout view institutes\" ) adapter = context . obj [ 'adapter' ] if institute id : institute objs = [ ] institute obj = adapter . institute ( institute id ) if not institute obj : LOG . info ( \"Institute %s does not exost\" , institute id ) return institute objs . append ( institute obj ) else : institute objs = [ ins obj for ins obj in adapter . institutes ( ) ] if len ( institute objs ) == 0 : click . echo ( \"No institutes found\" ) context . abort ( ) header = '' if not json : for key in institute objs [ 0 ] . keys ( ) : header = header + \"{0}\\t\" . format ( key ) click . echo ( header ) for institute obj in institute objs : if json : click . echo ( institute obj ) continue row = '' for value in institute obj . values ( ) : row = row + \"{0}\\t\" . format ( value ) click . echo ( row )", "predictions": ["print out the institute adapter ."], "references": ["show all institutes in the database"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 2892, "code": "def panels ( context , institute ) : LOG . info ( \"Running scout view panels\" ) adapter = context . obj [ 'adapter' ] panel objs = adapter . gene panels ( institute id = institute ) if panel objs . count ( ) == 0 : LOG . info ( \"No panels found\" ) context . abort ( ) click . echo ( \"#panel name\\tversion\\tnr genes\\tdate\" ) for panel obj in panel objs : click . echo ( \"{0}\\t{1}\\t{2}\\t{3}\" . format ( panel obj [ 'panel name' ] , str ( panel obj [ 'version' ] ) , len ( panel obj [ 'genes' ] ) , str ( panel obj [ 'date' ] . strftime ( '%Y-%m-%d' ) ) ) )", "predictions": ["display panels of your panels ."], "references": ["show all gene panels in the database"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 2893, "code": "def hpo genes ( context , hpo term ) : LOG . info ( \"Running scout export hpo genes\" ) adapter = context . obj [ 'adapter' ] header = [ \"#Gene id\\t Count\" ] if not hpo term : LOG . warning ( \"Please use at least one hpo term\" ) context . abort ( ) for line in header : click . echo ( line ) for term in adapter . generate hpo gene list ( * hpo term ) : click . echo ( \"{0}\\t{1}\" . format ( term [ 0 ] , term [ 1 ] ) )", "predictions": ["export hpo genes ."], "references": ["export a list of genes based on hpo terms"], "bleu": 0.12241977696855179, "rouge_l": 0.28773584905660377}
{"id": 2894, "code": "def user ( context , institute id , user name , user mail , admin ) : adapter = context . obj [ 'adapter' ] institutes = [ ] for institute in institute id : institute obj = adapter . institute ( institute id = institute ) if not institute obj : LOG . warning ( \"Institute % does not exist\" , institute ) context . abort ( ) institutes . append ( institute ) roles = [ ] if admin : LOG . info ( \"User is admin\" ) roles . append ( 'admin' ) user info = dict ( email = user mail . lower ( ) , name = user name , roles = roles , institutes = institutes ) user obj = build user ( user info ) try : adapter . add user ( user obj ) except Exception as err : LOG . warning ( err ) context . abort ( )", "predictions": ["add user roles to institute ."], "references": ["add a user to the database ."], "bleu": 0.23512037509993022, "rouge_l": 0.6069651741293532}
{"id": 2895, "code": "def institutes ( ) : institute objs = user institutes ( store , current user ) institutes = [ ] for ins obj in institute objs : sanger recipients = [ ] for user mail in ins obj . get ( 'sanger recipients' , [ ] ) : user obj = store . user ( user mail ) if not user obj : continue sanger recipients . append ( user obj [ 'name' ] ) institutes . append ( { 'display name' : ins obj [ 'display name' ] , 'internal id' : ins obj [ ' id' ] , 'coverage cutoff' : ins obj . get ( 'coverage cutoff' , 'None' ) , 'sanger recipients' : sanger recipients , 'frequency cutoff' : ins obj . get ( 'frequency cutoff' , 'None' ) , 'phenotype groups' : ins obj . get ( 'phenotype groups' , PHENOTYPE GROUPS ) } ) data = dict ( institutes = institutes ) return render template ( 'overview/institutes.html' , * * data )", "predictions": ["view of the page ."], "references": ["display a list of all user institutes ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 2896, "code": "def remote static ( ) : file path = request . args . get ( 'file' ) range header = request . headers . get ( 'Range' , None ) if not range header and file path . endswith ( '.bam' ) : return abort ( 500 ) new resp = send file partial ( file path ) return new resp", "predictions": ["return a new static static static file ."], "references": ["stream * large * static files with special requirements ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 2897, "code": "def pileup ( ) : vcf file = request . args . get ( 'vcf' ) bam files = request . args . getlist ( 'bam' ) bai files = request . args . getlist ( 'bai' ) samples = request . args . getlist ( 'sample' ) alignments = [ { 'bam' : bam , 'bai' : bai , 'sample' : sample } for bam , bai , sample in zip ( bam files , bai files , samples ) ] position = { 'contig' : request . args [ 'contig' ] , 'start' : request . args [ 'start' ] , 'stop' : request . args [ 'stop' ] } genome = current app . config . get ( 'PILEUP GENOME' ) if genome : if not os . path . isfile ( genome ) : flash ( \"The pilup genome path ({}) provided does not exist\" . format ( genome ) ) genome = None LOG . debug ( \"Use pileup genome %s\" , genome ) exons = current app . config . get ( 'PILEUP EXONS' ) if exons : if not os . path . isfile ( exons ) : flash ( \"The pilup exons path ({}) provided does not exist\" . format ( exons ) ) genome = None LOG . debug ( \"Use pileup exons %s\" , exons ) LOG . debug ( \"View alignment for positions Chrom:{0}, Start:{1}, End: {2}\" . format ( position [ 'contig' ] , position [ 'start' ] , position [ 'stop' ] ) ) LOG . debug ( \"Use alignment files {}\" . format ( alignments ) ) return render template ( 'alignviewers/pileup.html' , alignments = alignments , position = position , vcf file = vcf file , genome = genome , exons = exons )", "predictions": ["pileup for pilup exons"], "references": ["visualize bam alignments ."], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 2898, "code": "def compounds ( context , case id ) : adapter = context . obj [ 'adapter' ] LOG . info ( \"Running scout update compounds\" ) case obj = adapter . case ( case id ) if not case obj : LOG . warning ( \"Case %s could not be found\" , case id ) context . abort ( ) try : adapter . update case compounds ( case obj ) except Exception as err : LOG . warning ( err ) context . abort ( )", "predictions": ["display compounds for a case ."], "references": ["update all compounds for a case"], "bleu": 0.6147881529512643, "rouge_l": 0.6666666666666666}
{"id": 2899, "code": "def hgnc ( ctx , hgnc symbol , hgnc id , build ) : adapter = ctx . obj [ 'adapter' ] if not ( hgnc symbol or hgnc id ) : log . warning ( \"Please provide a hgnc symbol or hgnc id\" ) ctx . abort ( ) if hgnc id : result = adapter . hgnc gene ( hgnc id , build = build ) if result : hgnc symbol = result [ 'hgnc symbol' ] else : log . warning ( \"Gene with id %s could not be found\" , hgnc id ) ctx . abort ( ) result = adapter . hgnc genes ( hgnc symbol , build = build ) if result . count ( ) == 0 : log . info ( \"No results found\" ) else : click . echo ( \"#hgnc id\\thgnc symbol\\taliases\\ttranscripts\" ) for gene in result : click . echo ( \"{0}\\t{1}\\t{2}\\t{3}\" . format ( gene [ 'hgnc id' ] , gene [ 'hgnc symbol' ] , ', ' . join ( gene [ 'aliases' ] ) , ', ' . join ( tx [ 'ensembl transcript id' ] for tx in gene [ 'transcripts' ] ) , ) )", "predictions": ["print out a hgnc symbol ."], "references": ["query the hgnc aliases"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 2900, "code": "def parse hpo obo ( hpo lines ) : term = { } for line in hpo lines : if len ( line ) == 0 : continue line = line . rstrip ( ) if line == '[Term]' : if term : yield term term = { } elif line . startswith ( 'id' ) : term [ 'hpo id' ] = line [ 4 : ] elif line . startswith ( 'name' ) : term [ 'description' ] = line [ 6 : ] elif line . startswith ( 'alt id' ) : if 'aliases' not in term : term [ 'aliases' ] = [ ] term [ 'aliases' ] . append ( line [ 8 : ] ) elif line . startswith ( 'is a' ) : if 'ancestors' not in term : term [ 'ancestors' ] = [ ] term [ 'ancestors' ] . append ( line [ 6 : 16 ] ) if term : yield term", "predictions": ["parse hpo lines from hpo lines ."], "references": ["parse a . obo formated hpo line"], "bleu": 0.22089591134157885, "rouge_l": 0.2857142857142857}
{"id": 2901, "code": "def genes ( ) : query = request . args . get ( 'query' , '' ) if '|' in query : hgnc id = int ( query . split ( ' | ' , 1 ) [ 0 ] ) return redirect ( url for ( '.gene' , hgnc id = hgnc id ) ) gene q = store . all genes ( ) . limit ( 20 ) return dict ( genes = gene q )", "predictions": ["redirect to the gene page ."], "references": ["render seach box for genes ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 2902, "code": "def gene ( hgnc id = None , hgnc symbol = None ) : if hgnc symbol : query = store . hgnc genes ( hgnc symbol ) if query . count ( ) == 1 : hgnc id = query . first ( ) [ 'hgnc id' ] else : return redirect ( url for ( '.genes' , query = hgnc symbol ) ) try : genes = controllers . gene ( store , hgnc id ) except Value Error as error : return abort ( 404 ) return genes", "predictions": ["handle a gene request ."], "references": ["render information about a gene ."], "bleu": 0.31610981104846864, "rouge_l": 0.5366568914956013}
{"id": 2903, "code": "def api genes ( ) : query = request . args . get ( 'query' ) json out = controllers . genes to json ( store , query ) return jsonify ( json out )", "predictions": ["return all api genes ."], "references": ["return json data about genes ."], "bleu": 0.31610981104846864, "rouge_l": 0.5366568914956013}
{"id": 2904, "code": "def institute and case ( store , institute id , case name = None ) : institute obj = store . institute ( institute id ) if institute obj is None and institute id != 'favicon.ico' : flash ( \"Can't find institute: {}\" . format ( institute id ) , 'warning' ) return abort ( 404 ) if case name : if case name : case obj = store . case ( institute id = institute id , display name = case name ) if case obj is None : return abort ( 404 ) if not current user . is admin : if institute id not in current user . institutes : if not case name or not any ( inst id in case obj [ 'collaborators' ] for inst id in current user . institutes ) : flash ( \"You don't have acccess to: {}\" . format ( institute id ) , 'danger' ) return abort ( 403 ) if case name : return institute obj , case obj else : return institute obj", "predictions": ["institute and case - insensitive and case - case ."], "references": ["fetch insitiute and case objects ."], "bleu": 0.17827531042796255, "rouge_l": 0.3927038626609442}
{"id": 2905, "code": "def user institutes ( store , login user ) : if login user . is admin : institutes = store . institutes ( ) else : institutes = [ store . institute ( inst id ) for inst id in login user . institutes ] return institutes", "predictions": ["return the user s institutes user s institutes ."], "references": ["preprocess institute objects ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 2906, "code": "def panel ( context , panel , version , update date , update version ) : adapter = context . obj [ 'adapter' ] panel obj = adapter . gene panel ( panel , version = version ) if not panel obj : LOG . warning ( \"Panel %s (version %s) could not be found\" % ( panel , version ) ) context . abort ( ) date obj = None if update date : try : date obj = get date ( update date ) except Exception as err : LOG . warning ( err ) context . abort ( ) update panel ( adapter , panel , panel version = panel obj [ 'version' ] , new version = update version , new date = date obj )", "predictions": ["update or update a panel in the panel"], "references": ["update a panel in the database"], "bleu": 0.5873949094699213, "rouge_l": 0.7331730769230769}
{"id": 2907, "code": "def diseases ( context , api key ) : adapter = context . obj [ 'adapter' ] api key = api key or context . obj . get ( 'omim api key' ) if not api key : LOG . warning ( \"Please provide a omim api key to load the omim gene panel\" ) context . abort ( ) try : mim files = fetch mim files ( api key , genemap2 = True ) except Exception as err : LOG . warning ( err ) context . abort ( ) LOG . info ( \"Dropping Disease Terms\" ) adapter . disease term collection . drop ( ) LOG . debug ( \"Disease Terms dropped\" ) load disease terms ( adapter = adapter , genemap lines = mim files [ 'genemap2' ] , ) LOG . info ( \"Successfully loaded all disease terms\" )", "predictions": ["fetch the contents of the disease api ."], "references": ["update disease terms in mongo database ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 2908, "code": "def users ( context ) : LOG . info ( \"Running scout view users\" ) adapter = context . obj [ 'adapter' ] user objs = adapter . users ( ) if user objs . count ( ) == 0 : LOG . info ( \"No users found\" ) context . abort ( ) click . echo ( \"#name\\temail\\troles\\tinstitutes\" ) for user obj in user objs : click . echo ( \"{0}\\t{1}\\t{2}\\t{3}\\t\" . format ( user obj [ 'name' ] , user obj . get ( 'mail' , user obj [ ' id' ] ) , ', ' . join ( user obj . get ( 'roles' , [ ] ) ) , ', ' . join ( user obj . get ( 'institutes' , [ ] ) ) , ) )", "predictions": ["list users for an user ."], "references": ["show all users in the database"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 2909, "code": "def load omim panel ( self , api key , institute = None ) : existing panel = self . gene panel ( panel id = 'OMIM-AUTO' ) if not existing panel : LOG . warning ( \"OMIM-AUTO does not exists in database\" ) LOG . info ( 'Creating a first version' ) version = 1.0 if existing panel : version = float ( math . floor ( existing panel [ 'version' ] ) + 1 ) LOG . info ( \"Setting version to %s\" , version ) try : mim files = fetch mim files ( api key = api key , genemap2 = True , mim2genes = True ) except Exception as err : raise err date string = None for line in mim files [ 'genemap2' ] : if 'Generated' in line : date string = line . split ( ':' ) [ - 1 ] . lstrip ( ) . rstrip ( ) date obj = get date ( date string ) if existing panel : if existing panel [ 'date' ] == date obj : LOG . warning ( \"There is no new version of OMIM\" ) return panel data = { } panel data [ 'path' ] = None panel data [ 'type' ] = 'clinical' panel data [ 'date' ] = date obj panel data [ 'panel id' ] = 'OMIM-AUTO' panel data [ 'institute' ] = institute or 'cust002' panel data [ 'version' ] = version panel data [ 'display name' ] = 'OMIM-AUTO' panel data [ 'genes' ] = [ ] alias genes = self . genes by alias ( ) genes = get omim panel genes ( genemap2 lines = mim files [ 'genemap2' ] , mim2gene lines = mim files [ 'mim2genes' ] , alias genes = alias genes , ) for gene in genes : panel data [ 'genes' ] . append ( gene ) panel obj = build panel ( panel data , self ) if existing panel : new genes = self . compare mim panels ( existing panel , panel obj ) if new genes : self . update mim version ( new genes , panel obj , old version = existing panel [ 'version' ] ) else : LOG . info ( \"The new version of omim does not differ from the old one\" ) LOG . info ( \"No update is added\" ) return self . add gene panel ( panel obj )", "predictions": ["load the panel panel from the panel ."], "references": ["create and load the omim - auto panel"], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 2910, "code": "def clinical symbols ( self , case obj ) : panel ids = [ panel [ 'panel id' ] for panel in case obj [ 'panels' ] ] query = self . panel collection . aggregate ( [ { '$match' : { ' id' : { '$in' : panel ids } } } , { '$unwind' : '$genes' } , { '$group' : { ' id' : '$genes.symbol' } } ] ) return set ( item [ ' id' ] for item in query )", "predictions": ["returns all symbols in the case that are not in the case"], "references": ["return all the clinical gene symbols for a case ."], "bleu": 0.13065113298388567, "rouge_l": 0.2772727272727273}
{"id": 2911, "code": "def cases ( context , case id , institute , reruns , finished , causatives , research requested , is research , status , json ) : adapter = context . obj [ 'adapter' ] models = [ ] if case id : case obj = adapter . case ( case id = case id ) if case obj : models . append ( case obj ) else : LOG . info ( \"No case with id {}\" . format ( case id ) ) else : models = adapter . cases ( collaborator = institute , reruns = reruns , finished = finished , has causatives = causatives , research requested = research requested , is research = is research , status = status ) models = [ case obj for case obj in models ] if len ( models ) == 0 : LOG . info ( \"No cases could be found\" ) if json : click . echo ( dumps ( models ) ) return for model in models : pp ( model )", "predictions": ["display cases for a case ."], "references": ["interact with cases existing in the database ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 2912, "code": "def drop indexes ( self ) : LOG . warning ( \"Dropping all indexe\" ) for collection name in INDEXES : LOG . warning ( \"Dropping all indexes for collection name %s\" , collection name ) self . db [ collection name ] . drop indexes ( )", "predictions": ["drops all extensions in the database"], "references": ["delete all indexes for the database"], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 2913, "code": "def wipe ( ctx ) : LOG . info ( \"Running scout wipe\" ) db name = ctx . obj [ 'mongodb' ] LOG . info ( \"Dropping database %s\" , db name ) try : ctx . obj [ 'client' ] . drop database ( db name ) except Exception as err : LOG . warning ( err ) ctx . abort ( ) LOG . info ( \"Dropped whole database\" )", "predictions": ["register the whole hpo hpo hpo"], "references": ["drop the mongo database given ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 2914, "code": "def parse panel ( csv stream ) : reader = csv . Dict Reader ( csv stream , delimiter = ';' , quoting = csv . QUOTE NONE ) genes = [ ] for gene row in reader : if not gene row [ 'HGNC I Dnumber' ] . strip ( ) . isdigit ( ) : continue transcripts raw = gene row . get ( 'Disease associated transcript' ) if transcripts raw : transcripts list = [ tx . split ( ':' , 1 ) [ - 1 ] . strip ( ) for tx in transcripts raw . split ( ',' ) ] else : transcripts list = [ ] models raw = gene row . get ( 'Genetic disease model' ) models list = [ model . strip ( ) for model in models raw . split ( ',' ) ] if models raw else [ ] panel gene = dict ( symbol = gene row [ 'HGNC symbol' ] . strip ( ) if gene row . get ( 'HGNC symbol' ) else None , hgnc id = int ( gene row [ 'HGNC I Dnumber' ] . strip ( ) ) , disease associated transcripts = transcripts list , reduced penetrance = True if gene row . get ( 'Reduced penetrance' ) else None , mosaicism = True if gene row . get ( 'Mosaicism' ) else None , inheritance models = models list , database entry version = gene row . get ( 'Database entry version' ) , ) genes . append ( panel gene ) return genes", "predictions": ["configure a list of transcripts objects from app stream"], "references": ["parse user submitted panel ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 2915, "code": "def drop genes ( self , build = None ) : if build : LOG . info ( \"Dropping the hgnc gene collection, build %s\" , build ) self . hgnc collection . delete many ( { 'build' : build } ) else : LOG . info ( \"Dropping the hgnc gene collection\" ) self . hgnc collection . drop ( )", "predictions": ["aliases the = = = = 0 scout scout scout scout scout scout scout scout scout scout scout scout scout scout"], "references": ["delete the genes collection"], "bleu": 0.05809665204409193, "rouge_l": 0.09118086696562032}
{"id": 2916, "code": "def drop transcripts ( self , build = None ) : if build : LOG . info ( \"Dropping the transcripts collection, build %s\" , build ) self . transcript collection . delete many ( { 'build' : build } ) else : LOG . info ( \"Dropping the transcripts collection\" ) self . transcript collection . drop ( )", "predictions": ["drops the transcripts transcripts = the adapter = 0 ."], "references": ["delete the transcripts collection"], "bleu": 0.16590387014219712, "rouge_l": 0.3096446700507614}
{"id": 2917, "code": "def drop exons ( self , build = None ) : if build : LOG . info ( \"Dropping the exons collection, build %s\" , build ) self . exon collection . delete many ( { 'build' : build } ) else : LOG . info ( \"Dropping the exons collection\" ) self . exon collection . drop ( )", "predictions": ["drops the exons exons exons"], "references": ["delete the exons collection"], "bleu": 0.35930411196308426, "rouge_l": 0.4535315985130111}
{"id": 2918, "code": "def omim ( context , api key , institute ) : LOG . info ( \"Running scout update omim\" ) adapter = context . obj [ 'adapter' ] api key = api key or context . obj . get ( 'omim api key' ) if not api key : LOG . warning ( \"Please provide a omim api key to load the omim gene panel\" ) context . abort ( ) institute obj = adapter . institute ( institute ) if not institute obj : LOG . info ( \"Institute %s could not be found in database\" , institute ) LOG . warning ( \"Please specify an existing institute\" ) context . abort ( ) try : adapter . load omim panel ( api key , institute = institute ) except Exception as err : LOG . error ( err ) context . abort ( )", "predictions": ["load the panel panel"], "references": ["update the automate generated omim gene panel in the database ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 2919, "code": "def index ( ) : institute objs = user institutes ( store , current user ) institutes count = ( ( institute obj , store . cases ( collaborator = institute obj [ ' id' ] ) . count ( ) ) for institute obj in institute objs if institute obj ) return dict ( institutes = institutes count )", "predictions": ["returns the setup of all users in the current user ."], "references": ["display a list of all user institutes ."], "bleu": 0.17033186037639278, "rouge_l": 0.433392539964476}
{"id": 2920, "code": "def cases ( institute id ) : institute obj = institute and case ( store , institute id ) query = request . args . get ( 'query' ) limit = 100 if request . args . get ( 'limit' ) : limit = int ( request . args . get ( 'limit' ) ) skip assigned = request . args . get ( 'skip assigned' ) is research = request . args . get ( 'is research' ) all cases = store . cases ( collaborator = institute id , name query = query , skip assigned = skip assigned , is research = is research ) data = controllers . cases ( store , all cases , limit ) sanger unevaluated = controllers . get sanger unevaluated ( store , institute id , current user . email ) if len ( sanger unevaluated ) > 0 : data [ 'sanger unevaluated' ] = sanger unevaluated return dict ( institute = institute obj , skip assigned = skip assigned , is research = is research , query = query , * * data )", "predictions": ["get all index of a context = true if no index is context = false otherwise ."], "references": ["display a list of cases for an institute ."], "bleu": 0.0859076483566362, "rouge_l": 0.1628838451268358}
{"id": 2921, "code": "def case ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) data = controllers . case ( store , institute obj , case obj ) return dict ( institute = institute obj , case = case obj , * * data )", "predictions": ["fetch a database object ."], "references": ["display one case ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 2922, "code": "def matchmaker matches ( institute id , case name ) : user obj = store . user ( current user . email ) if 'mme submitter' not in user obj [ 'roles' ] : flash ( 'unauthorized request' , 'warning' ) return redirect ( request . referrer ) mme base url = current app . config . get ( 'MME URL' ) mme token = current app . config . get ( 'MME TOKEN' ) if not mme base url or not mme token : flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) return redirect ( request . referrer ) institute obj , case obj = institute and case ( store , institute id , case name ) data = controllers . mme matches ( case obj , institute obj , mme base url , mme token ) if data and data . get ( 'server errors' ) : flash ( 'Match Maker server returned error:{}' . format ( data [ 'server errors' ] ) , 'danger' ) return redirect ( request . referrer ) elif not data : data = { 'institute' : institute obj , 'case' : case obj } return data", "predictions": ["determine if the setup setup for the given mail institute . . . . . . . . . . . . . . . . . . . . ."], "references": ["show all matchmaker matches for a given case"], "bleu": 0.04317900023606586, "rouge_l": 0.1147695202257761}
{"id": 2923, "code": "def matchmaker match ( institute id , case name , target ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) if 'mme submitter' not in user obj [ 'roles' ] : flash ( 'unauthorized request' , 'warning' ) return redirect ( request . referrer ) mme base url = current app . config . get ( 'MME URL' ) mme accepts = current app . config . get ( 'MME ACCEPTS' ) mme token = current app . config . get ( 'MME TOKEN' ) nodes = current app . mme nodes if not mme base url or not mme token or not mme accepts : flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) return redirect ( request . referrer ) match results = controllers . mme match ( case obj , target , mme base url , mme token , nodes , mme accepts ) ok responses = 0 for match results in match results : match results [ 'status code' ] == 200 ok responses += 1 if ok responses : flash ( \"Match request sent. Look for eventual matches in 'Matches' page.\" , 'info' ) else : flash ( 'An error occurred while sending match request.' , 'danger' ) return redirect ( request . referrer )", "predictions": ["institutes the user to match adapter adapter adapter adapter adapter adapter adapter adapter adapter adapter adapter"], "references": ["starts an internal match or a match against one or all mme external nodes"], "bleu": 0.07692375026049747, "rouge_l": 0.06747787610619468}
{"id": 2924, "code": "def matchmaker delete ( institute id , case name ) : user obj = store . user ( current user . email ) if 'mme submitter' not in user obj [ 'roles' ] : flash ( 'unauthorized request' , 'warning' ) return redirect ( request . referrer ) institute obj , case obj = institute and case ( store , institute id , case name ) mme base url = current app . config . get ( 'MME URL' ) mme token = current app . config . get ( 'MME TOKEN' ) if not mme base url or not mme token : flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) return redirect ( request . referrer ) delete result = controllers . mme delete ( case obj , mme base url , mme token ) n deleted = 0 category = 'warning' for resp in delete result : if resp [ 'status code' ] == 200 : n deleted += 1 else : flash ( resp [ 'message' ] , category ) if n deleted : category = 'success' user obj = store . user ( current user . email ) store . case mme delete ( case obj = case obj , user obj = user obj ) flash ( 'Number of patients deleted from Matchmaker: {} out of {}' . format ( n deleted , len ( delete result ) ) , category ) return redirect ( request . referrer )", "predictions": ["delete a case adapter adapter adapter adapter adapter adapter adapter adapter adapter adapter adapter adapter adapter"], "references": ["remove a case from matchmaker"], "bleu": 0.10123734869668824, "rouge_l": 0.21034482758620687}
{"id": 2925, "code": "def gene variants ( institute id ) : page = int ( request . form . get ( 'page' , 1 ) ) institute obj = institute and case ( store , institute id ) if ( request . method == \"POST\" ) : form = Gene Variant Filters Form ( request . form ) else : form = Gene Variant Filters Form ( request . args ) variant type = form . data . get ( 'variant type' , 'clinical' ) hgnc symbols = [ ] non clinical symbols = [ ] not found symbols = [ ] not found ids = [ ] data = { } if ( form . hgnc symbols . data ) and len ( form . hgnc symbols . data ) > 0 : is clinical = form . data . get ( 'variant type' , 'clinical' ) == 'clinical' clinical symbols = store . clinical symbols ( case obj ) if is clinical else None for hgnc symbol in form . hgnc symbols . data : if hgnc symbol . isdigit ( ) : hgnc gene = store . hgnc gene ( int ( hgnc symbol ) ) if hgnc gene is None : not found ids . append ( hgnc symbol ) else : hgnc symbols . append ( hgnc gene [ 'hgnc symbol' ] ) elif store . hgnc genes ( hgnc symbol ) . count ( ) == 0 : not found symbols . append ( hgnc symbol ) elif is clinical and ( hgnc symbol not in clinical symbols ) : non clinical symbols . append ( hgnc symbol ) else : hgnc symbols . append ( hgnc symbol ) if ( not found ids ) : flash ( \"HGNC id not found: {}\" . format ( \", \" . join ( not found ids ) ) , 'warning' ) if ( not found symbols ) : flash ( \"HGNC symbol not found: {}\" . format ( \", \" . join ( not found symbols ) ) , 'warning' ) if ( non clinical symbols ) : flash ( \"Gene not included in clinical list: {}\" . format ( \", \" . join ( non clinical symbols ) ) , 'warning' ) form . hgnc symbols . data = hgnc symbols log . debug ( \"query {}\" . format ( form . data ) ) variants query = store . gene variants ( query = form . data , category = 'snv' , variant type = variant type ) data = controllers . gene variants ( store , variants query , page ) return dict ( institute = institute obj , form = form , page = page , * * data )", "predictions": ["display genes for a hpo symbol ."], "references": ["display a list of snv variants ."], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 2926, "code": "def pdf case report ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) data = controllers . case report content ( store , institute obj , case obj ) if current app . config . get ( 'SQLALCHEMY DATABASE URI' ) : data [ 'coverage report' ] = controllers . coverage report contents ( store , institute obj , case obj , request . url root ) if case obj . get ( 'madeline info' ) is not None : with open ( os . path . join ( cases bp . static folder , 'madeline.svg' ) , 'w' ) as temp madeline : temp madeline . write ( case obj [ 'madeline info' ] ) html report = render template ( 'cases/case report.html' , institute = institute obj , case = case obj , format = 'pdf' , * * data ) return render pdf ( HTML ( string = html report ) , download filename = case obj [ 'display name' ] + ' ' + datetime . datetime . now ( ) . strftime ( \"%Y-%m-%d\" ) + ' scout.pdf' )", "predictions": ["context manager to context manager ."], "references": ["download a pdf report for a case"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 2927, "code": "def case diagnosis ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) link = url for ( '.case' , institute id = institute id , case name = case name ) level = 'phenotype' if 'phenotype' in request . form else 'gene' omim id = request . form [ 'omim id' ] remove = True if request . args . get ( 'remove' ) == 'yes' else False store . diagnose ( institute obj , case obj , user obj , link , level = level , omim id = omim id , remove = remove ) return redirect ( request . referrer )", "predictions": ["create a institutes institutes institutes ins ins ins ins ins ins ins ins ins ins ."], "references": ["add or remove a diagnosis for a case ."], "bleu": 0.08513012360883544, "rouge_l": 0.16850828729281767}
{"id": 2928, "code": "def phenotypes actions ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) case url = url for ( '.case' , institute id = institute id , case name = case name ) action = request . form [ 'action' ] hpo ids = request . form . getlist ( 'hpo id' ) user obj = store . user ( current user . email ) if action == 'DELETE' : for hpo id in hpo ids : store . remove phenotype ( institute obj , case obj , user obj , case url , hpo id ) elif action == 'PHENOMIZER' : if len ( hpo ids ) == 0 : hpo ids = [ term [ 'phenotype id' ] for term in case obj . get ( 'phenotype terms' , [ ] ) ] username = current app . config [ 'PHENOMIZER USERNAME' ] password = current app . config [ 'PHENOMIZER PASSWORD' ] diseases = controllers . hpo diseases ( username , password , hpo ids ) return render template ( 'cases/diseases.html' , diseases = diseases , institute = institute obj , case = case obj ) elif action == 'GENES' : hgnc symbols = set ( ) for raw symbols in request . form . getlist ( 'genes' ) : if raw symbols : hgnc symbols . update ( raw symbol . split ( ' ' , 1 ) [ 0 ] for raw symbol in raw symbols . split ( '|' ) ) store . update dynamic gene list ( case obj , hgnc symbols = hgnc symbols ) elif action == 'GENERATE' : if len ( hpo ids ) == 0 : hpo ids = [ term [ 'phenotype id' ] for term in case obj . get ( 'phenotype terms' , [ ] ) ] results = store . generate hpo gene list ( * hpo ids ) hpo count = int ( request . form . get ( 'min match' ) or 1 ) hgnc ids = [ result [ 0 ] for result in results if result [ 1 ] >= hpo count ] store . update dynamic gene list ( case obj , hgnc ids = hgnc ids , phenotype ids = hpo ids ) return redirect ( case url )", "predictions": ["show all remote static static static static static static static method ."], "references": ["perform actions on multiple phenotypes ."], "bleu": 0.10390302174233558, "rouge_l": 0.1182170542635659}
{"id": 2929, "code": "def status ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) status = request . form . get ( 'status' , case obj [ 'status' ] ) link = url for ( '.case' , institute id = institute id , case name = case name ) if status == 'archive' : store . archive case ( institute obj , case obj , user obj , status , link ) else : store . update status ( institute obj , case obj , user obj , status , link ) return redirect ( request . referrer )", "predictions": ["show the pileup pileup for a particular file . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["update status of a specific case ."], "bleu": 0.04317900023606586, "rouge_l": 0.11879259980525803}
{"id": 2930, "code": "def assign ( institute id , case name , user id = None ) : institute obj , case obj = institute and case ( store , institute id , case name ) link = url for ( '.case' , institute id = institute id , case name = case name ) if user id : user obj = store . user ( user id ) else : user obj = store . user ( current user . email ) if request . form . get ( 'action' ) == 'DELETE' : store . unassign ( institute obj , case obj , user obj , link ) else : store . assign ( institute obj , case obj , user obj , link ) return redirect ( request . referrer )", "predictions": ["compounds an existing id to a id . . . . . . . . . . . ."], "references": ["assign and unassign a user from a case ."], "bleu": 0.0712695567709093, "rouge_l": 0.15269086357947434}
{"id": 2931, "code": "def hpoterms ( ) : query = request . args . get ( 'query' ) if query is None : return abort ( 500 ) terms = sorted ( store . hpo terms ( query = query ) , key = itemgetter ( 'hpo number' ) ) json terms = [ { 'name' : '{} | {}' . format ( term [ ' id' ] , term [ 'description' ] ) , 'id' : term [ ' id' ] } for term in terms [ : 7 ] ] return jsonify ( json terms )", "predictions": ["return list of not hgnc"], "references": ["search for hpo terms ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 2932, "code": "def mark validation ( institute id , case name , variant id ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) user obj = store . user ( current user . email ) validate type = request . form [ 'type' ] or None link = url for ( 'variants.variant' , institute id = institute id , case name = case name , variant id = variant id ) store . validate ( institute obj , case obj , user obj , link , variant obj , validate type ) return redirect ( request . referrer or link )", "predictions": ["create or delete a = hpo hpo len len len len len len len len len len len ."], "references": ["mark a variant as sanger validated ."], "bleu": 0.0712695567709093, "rouge_l": 0.16781292984869325}
{"id": 2933, "code": "def mark causative ( institute id , case name , variant id ) : institute obj , case obj = institute and case ( store , institute id , case name ) variant obj = store . variant ( variant id ) user obj = store . user ( current user . email ) link = url for ( 'variants.variant' , institute id = institute id , case name = case name , variant id = variant id ) if request . form [ 'action' ] == 'ADD' : store . mark causative ( institute obj , case obj , user obj , link , variant obj ) elif request . form [ 'action' ] == 'DELETE' : store . unmark causative ( institute obj , case obj , user obj , link , variant obj ) case url = url for ( '.case' , institute id = institute id , case name = case name ) return redirect ( case url )", "predictions": ["redirects a args to a request in a request in a request in a request in a request in a request in a request in a request in a request in"], "references": ["mark a variant as confirmed causative ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 2934, "code": "def delivery report ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) if case obj . get ( 'delivery report' ) is None : return abort ( 404 ) date str = request . args . get ( 'date' ) if date str : delivery report = None analysis date = parse date ( date str ) for analysis data in case obj [ 'analyses' ] : if analysis data [ 'date' ] == analysis date : delivery report = analysis data [ 'delivery report' ] if delivery report is None : return abort ( 404 ) else : delivery report = case obj [ 'delivery report' ] out dir = os . path . dirname ( delivery report ) filename = os . path . basename ( delivery report ) return send from directory ( out dir , filename )", "predictions": ["send a gene report report"], "references": ["display delivery report ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 2935, "code": "def share ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) collaborator id = request . form [ 'collaborator' ] revoke access = 'revoke' in request . form link = url for ( '.case' , institute id = institute id , case name = case name ) if revoke access : store . unshare ( institute obj , case obj , collaborator id , user obj , link ) else : store . share ( institute obj , case obj , collaborator id , user obj , link ) return redirect ( request . referrer )", "predictions": ["create a new out of a query controllers controllers controllers controllers controllers controllers controllers controllers ."], "references": ["share a case with a different institute ."], "bleu": 0.09147827112247602, "rouge_l": 0.2659883720930233}
{"id": 2936, "code": "def rerun ( institute id , case name ) : sender = current app . config [ 'MAIL USERNAME' ] recipient = current app . config [ 'TICKET SYSTEM EMAIL' ] controllers . rerun ( store , mail , current user , institute id , case name , sender , recipient ) return redirect ( request . referrer )", "predictions": ["institute a new case ."], "references": ["request a case to be rerun ."], "bleu": 0.21763141204756337, "rouge_l": 0.48541114058355433}
{"id": 2937, "code": "def research ( institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) link = url for ( '.case' , institute id = institute id , case name = case name ) store . open research ( institute obj , case obj , user obj , link ) return redirect ( request . referrer )", "predictions": ["view function that redirects to the given login login . . . . . . . . ."], "references": ["open the research list for a case ."], "bleu": 0.07535838128770536, "rouge_l": 0.16531165311653115}
{"id": 2938, "code": "def default panels ( institute id , case name ) : panel ids = request . form . getlist ( 'panel ids' ) controllers . update default panels ( store , current user , institute id , case name , panel ids ) return redirect ( request . referrer )", "predictions": ["panel for a date . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["update default panels for a case ."], "bleu": 0.055177848898164926, "rouge_l": 0.17818889970788704}
{"id": 2939, "code": "def vcf2cytosure ( institute id , case name , individual id ) : ( display name , vcf2cytosure ) = controllers . vcf2cytosure ( store , institute id , case name , individual id ) outdir = os . path . abspath ( os . path . dirname ( vcf2cytosure ) ) filename = os . path . basename ( vcf2cytosure ) log . debug ( \"Attempt to deliver file {0} from dir {1}\" . format ( filename , outdir ) ) attachment filename = display name + \".vcf2cytosure.cgh\" return send from directory ( outdir , filename , attachment filename = attachment filename , as attachment = True )", "predictions": ["send an individual the controllers"], "references": ["download vcf2cytosure file for individual ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 2940, "code": "def multiqc ( institute id , case name ) : data = controllers . multiqc ( store , institute id , case name ) if data [ 'case' ] . get ( 'multiqc' ) is None : return abort ( 404 ) out dir = os . path . abspath ( os . path . dirname ( data [ 'case' ] [ 'multiqc' ] ) ) filename = os . path . basename ( data [ 'case' ] [ 'multiqc' ] ) return send from directory ( out dir , filename )", "predictions": ["format a context to a specific case adapter adapter adapter adapter adapter adapter adapter adapter ."], "references": ["load multiqc report for the case ."], "bleu": 0.08513012360883544, "rouge_l": 0.1871165644171779}
{"id": 2941, "code": "def clinvar submissions ( store , user id , institute id ) : submissions = list ( store . clinvar submissions ( user id , institute id ) ) return submissions", "predictions": ["load submissions submissions and store"], "references": ["get all clinvar submissions for a user and an institute"], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 2942, "code": "def rerun ( store , mail , current user , institute id , case name , sender , recipient ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) link = url for ( 'cases.case' , institute id = institute id , case name = case name ) store . request rerun ( institute obj , case obj , user obj , link ) html = . format ( institute = institute obj [ 'display name' ] , case = case obj [ 'display name' ] , case id = case obj [ ' id' ] , name = user obj [ 'name' ] . encode ( ) ) msg = Message ( subject = ( \"SCOUT: request RERUN for {}\" . format ( case obj [ 'display name' ] ) ) , html = html , sender = sender , recipients = [ recipient ] , cc = [ user obj [ 'email' ] ] ) mail . send ( msg )", "predictions": ["clinical an active obj . ."], "references": ["request a rerun by email ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 2943, "code": "def update default panels ( store , current user , institute id , case name , panel ids ) : institute obj , case obj = institute and case ( store , institute id , case name ) user obj = store . user ( current user . email ) link = url for ( 'cases.case' , institute id = institute id , case name = case name ) panel objs = [ store . panel ( panel id ) for panel id in panel ids ] store . update default panels ( institute obj , case obj , user obj , link , panel objs )", "predictions": ["cases for the default context ."], "references": ["update default panels for a case ."], "bleu": 0.22236312185643822, "rouge_l": 0.3034825870646766}
{"id": 2944, "code": "def vcf2cytosure ( store , institute id , case name , individual id ) : institute obj , case obj = institute and case ( store , institute id , case name ) for individual in case obj [ 'individuals' ] : if individual [ 'individual id' ] == individual id : individual obj = individual return ( individual obj [ 'display name' ] , individual obj [ 'vcf2cytosure' ] )", "predictions": ["update an individual case ."], "references": ["vcf2cytosure cgh file for inidividual ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 2945, "code": "def multiqc ( store , institute id , case name ) : institute obj , case obj = institute and case ( store , institute id , case name ) return dict ( institute = institute obj , case = case obj , )", "predictions": ["create a new case ."], "references": ["find multiqc report for the case ."], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 2946, "code": "def genes ( context , build , api key ) : LOG . info ( \"Running scout update genes\" ) adapter = context . obj [ 'adapter' ] api key = api key or context . obj . get ( 'omim api key' ) if not api key : LOG . warning ( \"Please provide a omim api key to load the omim gene panel\" ) context . abort ( ) try : mim files = fetch mim files ( api key , mim2genes = True , morbidmap = True , genemap2 = True ) except Exception as err : LOG . warning ( err ) context . abort ( ) LOG . warning ( \"Dropping all gene information\" ) adapter . drop genes ( build ) LOG . info ( \"Genes dropped\" ) LOG . warning ( \"Dropping all transcript information\" ) adapter . drop transcripts ( build ) LOG . info ( \"transcripts dropped\" ) hpo genes = fetch hpo genes ( ) if build : builds = [ build ] else : builds = [ '37' , '38' ] hgnc lines = fetch hgnc ( ) exac lines = fetch exac constraint ( ) for build in builds : ensembl genes = fetch ensembl genes ( build = build ) hgnc genes = load hgnc genes ( adapter = adapter , ensembl lines = ensembl genes , hgnc lines = hgnc lines , exac lines = exac lines , mim2gene lines = mim files [ 'mim2genes' ] , genemap lines = mim files [ 'genemap2' ] , hpo lines = hpo genes , build = build , ) ensembl genes = { } for gene obj in hgnc genes : ensembl id = gene obj [ 'ensembl id' ] ensembl genes [ ensembl id ] = gene obj ensembl transcripts = fetch ensembl transcripts ( build = build ) transcripts = load transcripts ( adapter , ensembl transcripts , build , ensembl genes ) adapter . update indexes ( ) LOG . info ( \"Genes, transcripts and Exons loaded\" )", "predictions": ["fetch transcripts for all transcripts ."], "references": ["load the hgnc aliases to the mongo database ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 2947, "code": "def parse cadd ( variant , transcripts ) : cadd = 0 cadd keys = [ 'CADD' , 'CADD PHRED' ] for key in cadd keys : cadd = variant . INFO . get ( key , 0 ) if cadd : return float ( cadd ) for transcript in transcripts : cadd entry = transcript . get ( 'cadd' ) if ( cadd entry and cadd entry > cadd ) : cadd = cadd entry return cadd", "predictions": ["parse the cadd list"], "references": ["check if the cadd phred score is annotated"], "bleu": 0.1739594473063345, "rouge_l": 0.31443298969072164}
{"id": 2948, "code": "def convert ( context , panel ) : adapter = context . obj [ 'adapter' ] new header = [ \"hgnc id\" , \"hgnc symbol\" , \"disease associated transcripts\" , \"reduced penetrance\" , \"genetic disease models\" , \"mosaicism\" , \"database entry version\" ] genes = parse genes ( panel ) adapter . add hgnc id ( genes ) click . echo ( \"#{0}\" . format ( '\\t' . join ( new header ) ) ) for gene in genes : if gene . get ( 'hgnc id' ) : print info = [ ] for head in new header : print info . append ( str ( gene [ head ] ) if gene . get ( head ) else '' ) click . echo ( '\\t' . join ( print info ) )", "predictions": ["convert a panel to a new gene"], "references": ["convert a gene panel with hgnc symbols to a new one with hgnc ids ."], "bleu": 0.14089030922576778, "rouge_l": 0.5118881118881118}
{"id": 2949, "code": "def cli ( context , morbid , genemap , mim2gene , mim titles , phenotypes ) : from scout . utils . handle import get file handle from pprint import pprint as pp print ( \"Morbid file: %s\" % morbid ) print ( \"Genemap file: %s\" % genemap ) print ( \"mim2gene file: %s\" % mim2gene ) print ( \"Mim Titles file: %s\" % mim titles ) if morbid : morbid handle = get file handle ( morbid ) if genemap : genemap handle = get file handle ( genemap ) if mim2gene : mim2gene handle = get file handle ( mim2gene ) if mim titles : mimtitles handle = get file handle ( mim titles ) mim genes = get mim genes ( genemap handle , mim2gene handle ) for entry in mim genes : if entry == 'C10orf11' : pp ( mim genes [ entry ] ) context . abort ( ) if phenotypes : if not genemap : click . echo ( \"Please provide the genemap file\" ) context . abort ( ) phenotypes = get mim phenotypes ( genemap handle ) for i , mim term in enumerate ( phenotypes ) : pass print ( \"Number of phenotypes found: %s\" % i ) context . abort ( ) genes = get mim genes ( genemap handle , mim2gene handle ) for hgnc symbol in genes : if hgnc symbol == 'OPA1' : print ( genes [ hgnc symbol ] )", "predictions": ["command line tool to convert from hgnc to hgnc symbol ."], "references": ["parse the omim files"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 2950, "code": "def formatmonth ( self , theyear , themonth , withyear = True , net = None , qs = None , template = 'happenings/partials/calendar/month table.html' ) : context = self . get context ( ) context [ 'month start date' ] = date ( self . yr , self . mo , 1 ) context [ 'week rows' ] = [ ] for week in self . monthdays2calendar ( theyear , themonth ) : week row = [ ] for day , weekday in week : week row . append ( self . formatday ( day , weekday ) ) context [ 'week rows' ] . append ( week row ) nxt , prev = get next and prev ( net ) extra qs = ( '&' + '&' . join ( qs ) ) if qs else '' context [ 'prev qs' ] = mark safe ( '?cal prev=%d%s' % ( prev , extra qs ) ) context [ 'next qs' ] = mark safe ( '?cal next=%d%s' % ( nxt , extra qs ) ) context [ 'withyear' ] = withyear return render to string ( template , context )", "predictions": ["render the html for the week"], "references": ["return a formatted month as a table ."], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 2951, "code": "def formatday ( self , day , weekday , day template = 'happenings/partials/calendar/day cell.html' , noday template = 'happenings/partials/calendar/day noday cell.html' , popover template = 'happenings/partials/calendar/popover.html' , ) : super ( Event Calendar , self ) . formatday ( day , weekday ) now = get now ( ) context = self . get context ( ) context [ 'events' ] = [ ] context [ 'day' ] = day context [ 'day url' ] = self . get day url ( day ) context [ 'month start date' ] = date ( self . yr , self . mo , 1 ) context [ 'weekday' ] = weekday context [ 'cssclass' ] = self . cssclasses [ weekday ] context [ 'popover template' ] = popover template context [ 'num events' ] = len ( self . count . get ( day , [ ] ) ) , try : processed date = date ( self . yr , self . mo , day ) except Value Error : processed date = None context [ 'month start date' ] = date ( self . yr , self . mo , 1 ) if day == 0 : template = noday template else : template = day template if now . date ( ) == processed date : context [ 'is current day' ] = True if processed date and ( day in self . count ) : for item in self . count [ day ] : self . pk = item [ 1 ] self . title = item [ 0 ] for event in self . events : if event . pk == self . pk : event . check if cancelled ( processed date ) context [ 'events' ] . append ( event ) return render to string ( template , context )", "predictions": ["allows to list the latest events to be added to the context ."], "references": ["return a day as a table cell ."], "bleu": 0.09552040806823771, "rouge_l": 0.09951060358890701}
{"id": 2952, "code": "def formatday ( self , day , weekday ) : return super ( Mini Event Calendar , self ) . formatday ( day , weekday , day template = 'happenings/partials/calendar/mini day cell.html' , popover template = 'happenings/partials/calendar/mini popover.html' , )", "predictions": ["override the default formatday method"], "references": ["return a day as a table cell ."], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 2953, "code": "def formatday ( self , day , weekday ) : self . wkday not today = '<td class=\"%s\"><div class=\"td-inner\">' % ( self . cssclasses [ weekday ] ) self . wkday today = ( '<td class=\"%s calendar-today\"><div class=\"td-inner\">' % ( self . cssclasses [ weekday ] ) ) if URLS NAMESPACE : url name = '%s:day list' % ( URLS NAMESPACE ) else : url name = 'day list' self . day url = reverse ( url name , args = ( self . yr , self . mo , day ) ) self . day = day self . anch = '<a href=\"%s\">%d</a>' % ( self . day url , day ) self . end = '</div></td>'", "predictions": ["update the day of the given day"], "references": ["set some commonly used variables ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 2954, "code": "def popover helper ( self ) : display month = month name [ self . mo ] if isinstance ( display month , six . binary type ) and self . encoding : display month = display month . decode ( 'utf-8' ) self . when = ( '<p><b>When:</b> ' + display month + ' ' + str ( self . day ) + ', ' + self . event . l start date . strftime ( LEGACY CALENDAR TIME FORMAT ) . lstrip ( '0' ) + ' - ' + self . event . l end date . strftime ( LEGACY CALENDAR TIME FORMAT ) . lstrip ( '0' ) + '</p>' ) if self . event . location . exists ( ) : self . where = '<p><b>Where:</b> ' for l in self . event . location . all ( ) : self . where += l . name self . where += '</p>' else : self . where = '' self . desc = '<p><b>Description:</b> ' + self . event . description [ : 100 ] self . desc += ( '...</p>' if len ( self . event . description ) > 100 else '</p>' ) self . event url = self . event . get absolute url ( ) t = LEGACY CALENDAR TIME FORMAT if self . event . l start date . minute else LEGACY CALENDAR HOUR FORMAT self . title2 = ( self . event . l start date . strftime ( t ) . lstrip ( '0' ) + ' ' + self . title )", "predictions": ["displays the event helper to the event loop ."], "references": ["populate variables used to build popovers ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 2955, "code": "def formatday ( self , day , weekday ) : super ( Event Calendar , self ) . formatday ( day , weekday ) now = get now ( ) self . day = day out = '' if day == 0 : return '<td class=\"noday\">&nbsp;</td>' elif now . month == self . mo and now . year == self . yr and day == now . day : if day in self . count : out = self . wkday today + self . anch else : return self . wkday today + self . anch + self . end elif day in self . count : out = self . wkday not today + self . anch else : return self . wkday not today + self . anch + self . end detail = \"%s%s%s<br><a href='%s'>View details</a>\" extras = ( '<div title=\"%s\" data-content=\"%s\" data-container=\"body\"' ' data-toggle=\"popover\" class=\"calendar-event\"%s>' ) common = ' style=background:%s;color:%s;' for item in self . count [ day ] : self . pk = item [ 1 ] self . title = item [ 0 ] for event in self . events : if event . pk == self . pk : self . event = event self . check if cancelled ( ) self . popover helper ( ) bg , fnt = self . event . get colors ( ) out += ( '<a class=\"event-anch\" href=\"' + self . event url + '\">' + extras % ( self . title , detail % ( self . when , self . where , self . desc , self . event url ) , common % ( bg , fnt ) ) + self . title2 + '</div></a>' ) return out + self . end", "predictions": ["add a specific day to the weekday"], "references": ["return a day as a table cell ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 2956, "code": "def formatday ( self , day , weekday ) : super ( Mini Event Calendar , self ) . formatday ( day , weekday ) now = get now ( ) self . day = day if day == 0 : return '<td class=\"noday\">&nbsp;</td>' elif now . month == self . mo and now . year == self . yr and day == now . day : if day in self . count : self . popover helper ( ) return self . wkday today + self . anch + self . cal event + self . end else : return self . wkday today + self . anch + self . end elif day in self . count : self . popover helper ( ) return self . wkday not today + self . anch + self . cal event + self . end else : return self . wkday not today + self . anch + self . end", "predictions": ["add the weekday to the weekday"], "references": ["return a day as a table cell ."], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 2957, "code": "def diseases ( context ) : LOG . info ( \"Running scout view diseases\" ) adapter = context . obj [ 'adapter' ] disease objs = adapter . disease terms ( ) nr diseases = disease objs . count ( ) if nr diseases == 0 : click . echo ( \"No diseases found\" ) else : click . echo ( \"Disease\" ) for disease obj in adapter . disease terms ( ) : click . echo ( \"{0}\" . format ( disease obj [ ' id' ] ) ) LOG . info ( \"{0} diseases found\" . format ( nr diseases ) )", "predictions": ["show all disease terms ."], "references": ["show all diseases in the database"], "bleu": 0.2941733261715515, "rouge_l": 0.3577712609970674}
{"id": 2958, "code": "def hpo ( context ) : LOG . info ( \"Running scout update hpo\" ) adapter = context . obj [ 'adapter' ] LOG . info ( \"Dropping HPO terms\" ) adapter . hpo term collection . drop ( ) LOG . debug ( \"HPO terms dropped\" ) load hpo terms ( adapter )", "predictions": ["drop all terms in the hpo collection ."], "references": ["update the hpo terms in the database . fetch the latest release and update terms ."], "bleu": 0.1305781107333599, "rouge_l": 0.31443298969072164}
{"id": 2959, "code": "def users ( store ) : user objs = list ( store . users ( ) ) total events = store . user events ( ) . count ( ) for user obj in user objs : if user obj . get ( 'institutes' ) : user obj [ 'institutes' ] = [ store . institute ( inst id ) for inst id in user obj . get ( 'institutes' ) ] else : user obj [ 'institutes' ] = [ ] user obj [ 'events' ] = store . user events ( user obj ) . count ( ) user obj [ 'events rank' ] = event rank ( user obj [ 'events' ] ) return dict ( users = sorted ( user objs , key = lambda user : - user [ 'events' ] ) , total events = total events , )", "predictions": ["get all users from the user"], "references": ["display a list of all users and which institutes they belong to ."], "bleu": 0.09052970298747198, "rouge_l": 0.19741100323624597}
{"id": 2960, "code": "def render to json response ( self , context , * * kwargs ) : return Http Response ( self . convert context to json ( context ) , content type = 'application/json' , * * kwargs )", "predictions": ["returns the json response data as a json string ."], "references": ["returns a json response transforming context to make the payload ."], "bleu": 0.20532606309031395, "rouge_l": 0.37770897832817335}
{"id": 2961, "code": "def check for cancelled events ( self , d ) : for event in self . events : for cn in event . cancellations . all ( ) : if cn . date == d : event . title += ' (CANCELLED)'", "predictions": ["check that all cancelled events have cancelled ."], "references": ["check if any events are cancelled on the given date d ."], "bleu": 0.1223065774797558, "rouge_l": 0.3860759493670886}
{"id": 2962, "code": "def setup time axis ( self , t start = None , t stop = None ) : ii start , ii stop = 0 , self . n ints in file if t start : ii start = t start if t stop : ii stop = t stop n ints = ii stop - ii start t0 = self . header [ b'tstart' ] t delt = self . header [ b'tsamp' ] self . timestamps = np . arange ( 0 , n ints ) * t delt / 24. / 60. / 60 + t0 return ii start , ii stop , n ints", "predictions": ["setup the time axis axis axis ."], "references": ["setup time axis ."], "bleu": 0.3073940764756322, "rouge_l": 0.7648902821316614}
{"id": 2963, "code": "def compute lst ( self ) : if self . header [ b'telescope id' ] == 6 : self . coords = gbt coords elif self . header [ b'telescope id' ] == 4 : self . coords = parkes coords else : raise Runtime Error ( \"Currently only Parkes and GBT supported\" ) if HAS SLALIB : dut1 = 0.0 mjd = self . header [ b'tstart' ] tellong = np . deg2rad ( self . coords [ 1 ] ) last = s . sla gmst ( mjd ) - tellong + s . sla eqeqx ( mjd ) + dut1 if last < 0.0 : last = last + 2.0 * np . pi return last else : raise Runtime Error ( \"This method requires py SLALIB\" )", "predictions": ["compute the lst s lst ."], "references": ["compute lst for observation"], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 2964, "code": "def calc extent ( self , plot f = None , plot t = None , MJD time = False ) : plot f begin = plot f [ 0 ] plot f end = plot f [ - 1 ] + ( plot f [ 1 ] - plot f [ 0 ] ) plot t begin = self . timestamps [ 0 ] plot t end = self . timestamps [ - 1 ] + ( self . timestamps [ 1 ] - self . timestamps [ 0 ] ) if MJD time : extent = ( plot f begin , plot f begin end , plot t begin , plot t end ) else : extent = ( plot f begin , plot f end , 0.0 , ( plot t end - plot t begin ) * 24. * 60. * 60 ) return extent", "predictions": ["calculate the extent of the extent extent ."], "references": ["setup ploting edges ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 2965, "code": "def closest ( xarr , val ) : idx closest = np . argmin ( np . abs ( np . array ( xarr ) - val ) ) return idx closest", "predictions": ["return the closest vector of the xarr"], "references": ["return the index of the closest in xarr to value val"], "bleu": 0.2029055433374586, "rouge_l": 0.5341506129597198}
{"id": 2966, "code": "def get diff ( dio cross , feedtype , * * kwargs ) : #Get Stokes parameters, frequencies, and time sample length obs = Waterfall ( dio cross , max load = 150 ) freqs = obs . populate freqs ( ) tsamp = obs . header [ 'tsamp' ] data = obs . data obs = None I , Q , U , V = get stokes ( data , feedtype ) #Fold noise diode data I OFF , I ON = foldcal ( I , tsamp , * * kwargs ) Q OFF , Q ON = foldcal ( Q , tsamp , * * kwargs ) U OFF , U ON = foldcal ( U , tsamp , * * kwargs ) V OFF , V ON = foldcal ( V , tsamp , * * kwargs ) #Do ON-OFF subtraction Idiff = I ON - I OFF Qdiff = Q ON - Q OFF Udiff = U ON - U OFF Vdiff = V ON - V OFF return Idiff , Qdiff , Udiff , Vdiff , freqs", "predictions": ["get difference between two cross - categorical cross - hot tables"], "references": ["returns on - off for all stokes parameters given a cross_pols noise diode measurement"], "bleu": 0.08671803715615023, "rouge_l": 0.07830551989730423}
{"id": 2967, "code": "def calc selection size ( self ) : #Check to see how many integrations requested n ints = self . t stop - self . t start #Check to see how many frequency channels requested n chan = ( self . f stop - self . f start ) / abs ( self . header [ b'foff' ] ) n bytes = self . n bytes selection size = int ( n ints * n chan * n bytes ) return selection size", "predictions": ["calculates the size of the selection selection ."], "references": ["calculate size of data of interest ."], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 2968, "code": "def calc selection shape ( self ) : #Check how many integrations requested n ints = int ( self . t stop - self . t start ) #Check how many frequency channels requested n chan = int ( np . round ( ( self . f stop - self . f start ) / abs ( self . header [ b'foff' ] ) ) ) selection shape = ( n ints , int ( self . header [ b'nifs' ] ) , n chan ) return selection shape", "predictions": ["calculate the shape of the selection of the selection of the selection ."], "references": ["calculate shape of data of interest ."], "bleu": 0.14949751774990683, "rouge_l": 0.5285961871750434}
{"id": 2969, "code": "def setup freqs ( self ) : if self . header [ b'foff' ] > 0 : self . f start = self . f begin + self . chan start idx * abs ( self . header [ b'foff' ] ) self . f stop = self . f begin + self . chan stop idx * abs ( self . header [ b'foff' ] ) else : self . f start = self . f end - self . chan stop idx * abs ( self . header [ b'foff' ] ) self . f stop = self . f end - self . chan start idx * abs ( self . header [ b'foff' ] )", "predictions": ["setup the header for the header"], "references": ["updating frequency borders from channel values"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 2970, "code": "def calc n blobs ( self , blob dim ) : n blobs = int ( np . ceil ( 1.0 * np . prod ( self . selection shape ) / np . prod ( blob dim ) ) ) return n blobs", "predictions": ["calculate number of blobs blobs ."], "references": ["given the blob dimensions calculate how many fit in the data selection ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 2971, "code": "def isheavy ( self ) : selection size bytes = self . calc selection size ( ) if selection size bytes > self . MAX DATA ARRAY SIZE : return True else : return False", "predictions": ["return whether the selection is bytes is bytes ."], "references": ["check if the current selection is too large ."], "bleu": 0.21105340631872635, "rouge_l": 0.4444444444444444}
{"id": 2972, "code": "def find blob start ( self , blob dim , n blob ) : #Convert input frequencies into what their corresponding channel number would be. self . setup chans ( ) #Check which is the blob time offset blob time start = self . t start + blob dim [ self . time axis ] * n blob #Check which is the blob frequency offset (in channels) blob freq start = self . chan start idx + ( blob dim [ self . freq axis ] * n blob ) % self . selection shape [ self . freq axis ] blob start = np . array ( [ blob time start , 0 , blob freq start ] ) return blob start", "predictions": ["find the start of the blob in the blob ."], "references": ["find first blob from selection ."], "bleu": 0.14991106946711685, "rouge_l": 0.3927038626609442}
{"id": 2973, "code": "def read blob ( self , blob dim , n blob = 0 ) : n blobs = self . calc n blobs ( blob dim ) if n blob > n blobs or n blob < 0 : raise Value Error ( 'Please provide correct n blob value. Given %i, but max values is %i' % ( n blob , n blobs ) ) #This prevents issues when the last blob is smaller than the others in time if blob dim [ self . time axis ] * ( n blob + 1 ) > self . selection shape [ self . time axis ] : updated blob dim = ( self . selection shape [ self . time axis ] - blob dim [ self . time axis ] * n blob , 1 , blob dim [ self . freq axis ] ) else : updated blob dim = [ int ( i ) for i in blob dim ] blob start = self . find blob start ( blob dim , n blob ) blob end = blob start + np . array ( updated blob dim ) blob = self . h5 [ \"data\" ] [ int ( blob start [ self . time axis ] ) : int ( blob end [ self . time axis ] ) , : , int ( blob start [ self . freq axis ] ) : int ( blob end [ self . freq axis ] ) ] return blob", "predictions": ["read a blob from the given dim"], "references": ["read blob from a selection ."], "bleu": 0.2777619034011791, "rouge_l": 0.4680306905370844}
{"id": 2974, "code": "def find blob start ( self ) : self . setup chans ( ) blob time start = self . t start blob freq start = self . chan start idx blob start = blob time start * self . n channels in file + blob freq start return blob start", "predictions": ["start the blob in the blob ."], "references": ["find first blob from selection ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 2975, "code": "def read blob ( self , blob dim , n blob = 0 ) : n blobs = self . calc n blobs ( blob dim ) if n blob > n blobs or n blob < 0 : raise Value Error ( 'Please provide correct n blob value. Given %i, but max values is %i' % ( n blob , n blobs ) ) if blob dim [ self . time axis ] * ( n blob + 1 ) > self . selection shape [ self . time axis ] : updated blob dim = ( int ( self . selection shape [ self . time axis ] - blob dim [ self . time axis ] * n blob ) , 1 , int ( blob dim [ self . freq axis ] ) ) else : updated blob dim = [ int ( i ) for i in blob dim ] blob start = self . find blob start ( ) blob = np . zeros ( updated blob dim , dtype = self . d type ) if self . f start == self . f begin and self . f stop == self . f end : blob flat size = np . prod ( blob dim ) updated blob flat size = np . prod ( updated blob dim ) with open ( self . filename , 'rb' ) as f : f . seek ( int ( self . idx data + self . n bytes * ( blob start + n blob * blob flat size ) ) ) dd = np . fromfile ( f , count = updated blob flat size , dtype = self . d type ) if dd . shape [ 0 ] == updated blob flat size : blob = dd . reshape ( updated blob dim ) else : logger . info ( 'DD shape != blob shape.' ) blob = dd . reshape ( ( int ( dd . shape [ 0 ] / blob dim [ self . freq axis ] ) , blob dim [ self . beam axis ] , blob dim [ self . freq axis ] ) ) else : for blobt in range ( updated blob dim [ self . time axis ] ) : #Load binary data with open ( self . filename , 'rb' ) as f : f . seek ( int ( self . idx data + self . n bytes * ( blob start + n blob * blob dim [ self . time axis ] * self . n channels in file + blobt * self . n channels in file ) ) ) dd = np . fromfile ( f , count = blob dim [ self . freq axis ] , dtype = self . d type ) blob [ blobt ] = dd return blob", "predictions": ["read a blob from the blob file"], "references": ["read blob from a selection ."], "bleu": 0.2777619034011791, "rouge_l": 0.4680306905370844}
{"id": 2976, "code": "def read data ( self , f start = None , f stop = None , t start = None , t stop = None ) : self . container . read data ( f start = f start , f stop = f stop , t start = t start , t stop = t stop ) self . load data ( )", "predictions": ["read the data data"], "references": ["reads data selection if small enough ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 2977, "code": "def update header ( self ) : #Updating frequency of first channel from selection if self . header [ b'foff' ] < 0 : self . header [ b'fch1' ] = self . container . f stop else : self . header [ b'fch1' ] = self . container . f start #Updating number of coarse channels. self . header [ b'nchans' ] = self . container . selection shape [ self . freq axis ] #Updating time stamp for first time bin from selection self . header [ b'tstart' ] = self . container . populate timestamps ( update header = True )", "predictions": ["multiqc the first header header"], "references": ["updates the header information from the original file to the selection ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 2978, "code": "def info ( self ) : print ( \"\\n--- File Info ---\" ) for key , val in self . file header . items ( ) : if key == 'src raj' : val = val . to string ( unit = u . hour , sep = ':' ) if key == 'src dej' : val = val . to string ( unit = u . deg , sep = ':' ) print ( \"%16s : %32s\" % ( key , val ) ) print ( \"\\n%16s : %32s\" % ( \"Num ints in file\" , self . n ints in file ) ) print ( \"%16s : %32s\" % ( \"File shape\" , self . file shape ) ) print ( \"--- Selection Info ---\" ) print ( \"%16s : %32s\" % ( \"Data selection shape\" , self . selection shape ) ) print ( \"%16s : %32s\" % ( \"Minimum freq (M Hz)\" , self . container . f start ) ) print ( \"%16s : %32s\" % ( \"Maximum freq (M Hz)\" , self . container . f stop ) )", "predictions": ["prints out information about the adapter"], "references": ["print header information and other derived information ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 2979, "code": "def get chunk dimensions ( self ) : #Usually '.0000.' is in self.filename if np . abs ( self . header [ b'foff' ] ) < 1e-5 : logger . info ( 'Detecting high frequency resolution data.' ) chunk dim = ( 1 , 1 , 1048576 ) #1048576 is the number of channels in a coarse channel. return chunk dim #Usually '.0001.' is in self.filename elif np . abs ( self . header [ b'tsamp' ] ) < 1e-3 : logger . info ( 'Detecting high time resolution data.' ) chunk dim = ( 2048 , 1 , 512 ) #512 is the total number of channels per single band (ie. blc00) return chunk dim #Usually '.0002.' is in self.filename elif np . abs ( self . header [ b'foff' ] ) < 1e-2 and np . abs ( self . header [ b'foff' ] ) >= 1e-5 : logger . info ( 'Detecting intermediate frequency and time resolution data.' ) chunk dim = ( 10 , 1 , 65536 ) #65536 is the total number of channels per single band (ie. blc00) return chunk dim else : logger . warning ( 'File format not known. Will use minimum chunking. NOT OPTIMAL.' ) chunk dim = ( 1 , 1 , 512 ) return chunk dim", "predictions": ["return the chunk dimensions"], "references": ["sets the chunking dimmentions depending on the file type ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 2980, "code": "def cmd tool ( args = None ) : from argparse import Argument Parser parser = Argument Parser ( description = \"Command line utility for creating spectra from Guppi Raw files.\" ) parser . add argument ( 'filename' , type = str , help = 'Name of file to read' ) parser . add argument ( '-o' , dest = 'outdir' , type = str , default = './' , help = 'output directory for PNG files' ) args = parser . parse args ( ) r = Guppi Raw ( args . filename ) r . print stats ( ) bname = os . path . splitext ( os . path . basename ( args . filename ) ) [ 0 ] bname = os . path . join ( args . outdir , bname ) r . plot histogram ( filename = \"%s hist.png\" % bname ) r . plot spectrum ( filename = \"%s spec.png\" % bname )", "predictions": ["convert tool to tool"], "references": ["command line tool for plotting and viewing info on guppi raw files"], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 2981, "code": "def print stats ( self ) : header , data = self . read next data block ( ) data = data . view ( 'float32' ) print ( \"AVG: %2.3f\" % data . mean ( ) ) print ( \"STD: %2.3f\" % data . std ( ) ) print ( \"MAX: %2.3f\" % data . max ( ) ) print ( \"MIN: %2.3f\" % data . min ( ) ) import pylab as plt", "predictions": ["cli for debugging ."], "references": ["compute some basic stats on the next block of data"], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 2982, "code": "def plot histogram ( self , filename = None ) : header , data = self . read next data block ( ) data = data . view ( 'float32' ) plt . figure ( \"Histogram\" ) plt . hist ( data . flatten ( ) , 65 , facecolor = '#cc0000' ) if filename : plt . savefig ( filename ) plt . show ( )", "predictions": ["plots the histogram of the qs qs file"], "references": ["plot a histogram of data values"], "bleu": 0.21105340631872638, "rouge_l": 0.2932692307692307}
{"id": 2983, "code": "def generate filterbank header ( self , nchans = 1 , ) : gp head = self . read first header ( ) fb head = { } telescope str = gp head . get ( \"TELESCOP\" , \"unknown\" ) if telescope str in ( 'GBT' , 'GREENBANK' ) : fb head [ \"telescope id\" ] = 6 elif telescope str in ( 'PKS' , 'PARKES' ) : fb head [ \"telescop id\" ] = 7 else : fb head [ \"telescop id\" ] = 0 fb head [ \"source name\" ] = gp head . get ( \"SRC NAME\" , \"unknown\" ) fb head [ \"az start\" ] = gp head . get ( \"AZ\" , 0 ) fb head [ \"za start\" ] = gp head . get ( \"ZA\" , 0 ) fb head [ \"src raj\" ] = Angle ( str ( gp head . get ( \"RA\" , 0.0 ) ) + \"hr\" ) fb head [ \"src dej\" ] = Angle ( str ( gp head . get ( \"DEC\" , 0.0 ) ) + \"deg\" ) fb head [ \"rawdatafile\" ] = self . filename fb head [ \"machine id\" ] = 20 fb head [ \"data type\" ] = 1 fb head [ \"barycentric\" ] = 0 fb head [ \"pulsarcentric\" ] = 0 fb head [ \"nbits\" ] = 32 fb head [ \"tstart\" ] = 0.0 fb head [ \"tsamp\" ] = 1.0 fb head [ \"fch1\" ] = 0.0 fb head [ \"foff\" ] = 187.5 / nchans fb head [ \"nchans\" ] = nchans fb head [ \"nifs\" ] = 1 fb head [ \"nbeams\" ] = 1 return fb head", "predictions": ["generate the self . self ."], "references": ["generate a blimpy header dictionary"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 2984, "code": "def find header size ( filename ) : filfile = open ( filename , 'rb' ) filfile . seek ( 0 ) #read some region larger than the header. round1 = filfile . read ( 1000 ) headersize = round1 . find ( 'HEADER END' ) + len ( 'HEADER END' ) return headersize", "predictions": ["find the header self . ."], "references": ["script to find the header size of a filterbank file"], "bleu": 0.21108303712651422, "rouge_l": 0.3588235294117647}
{"id": 2985, "code": "def cmd tool ( args = None ) : if 'bl' in local host : header loc = '/usr/local/sigproc/bin/header' #Current location of header command in GBT. else : raise IO Error ( 'Script only able to run in BL systems.' ) p = Option Parser ( ) p . set usage ( 'matchfils <FIL FILE1> <FIL FILE2>' ) opts , args = p . parse args ( sys . argv [ 1 : ] ) file1 = args [ 0 ] file2 = args [ 1 ] #------------------------------------ #Create batch script make batch script ( ) #------------------------------------ #First checksum headersize1 = find header size ( file1 ) file size1 = os . path . getsize ( file1 ) #Strip header from file, and calculate the md5sum of the rest. #command=['tail','-c',str(file size1-headersize1),file1,'|','md5sum'] command = [ './tail sum.sh' , file1 , str ( file size1 - headersize1 ) ] print ( '[matchfils] ' + ' ' . join ( command ) ) proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( out , err ) = proc . communicate ( ) check sum1 = out . split ( ) [ 0 ] print ( '[matchfils] Checksum is:' , check sum1 ) if err : raise Error ( 'There is an error.' ) #--- out , err = reset outs ( ) command = [ header loc , file1 ] print ( '[matchfils] Header information:' ) proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( out , err ) = proc . communicate ( ) header1 = out print ( header1 ) #------------------------------------ #Second checksum out , err = reset outs ( ) headersize2 = find header size ( file2 ) file size2 = os . path . getsize ( file2 ) #Strip header from file, and calculate the md5sum of the rest. command = [ './tail sum.sh' , file2 , str ( file size2 - headersize2 ) ] print ( '[matchfils] ' + ' ' . join ( command ) ) proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( out , err ) = proc . communicate ( ) check sum2 = out . split ( ) [ 0 ] print ( '[matchfils] Checksum is:' , check sum2 ) if err : raise Error ( 'There is an error.' ) #--- out , err = reset outs ( ) command = [ header loc , file2 ] print ( '[matchfils] Header information:' ) proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( out , err ) = proc . communicate ( ) header2 = out print ( header2 ) #------------------------------------ #check the checksums if check sum1 != check sum2 : print ( '[matchfils] Booo! Checksum does not match between files.' ) else : print ( '[matchfils] Hooray! Checksum matches between files.' ) #------------------------------------ #Remove batch script os . remove ( 'tail sum.sh' )", "predictions": ["run a batch script"], "references": ["command line tool to make a md5sum comparison of two . fil files ."], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 2986, "code": "def cmd tool ( args = None ) : from argparse import Argument Parser if not HAS BITSHUFFLE : print ( \"Error: the bitshuffle library is required to run this script.\" ) exit ( ) parser = Argument Parser ( description = \"Command line utility for creating HDF5 Raw files.\" ) parser . add argument ( 'filename' , type = str , help = 'Name of filename to read' ) args = parser . parse args ( ) fileroot = args . filename . split ( '.0000.raw' ) [ 0 ] filelist = glob . glob ( fileroot + '*.raw' ) filelist = sorted ( filelist ) r = Guppi Raw ( filelist [ 0 ] ) header , data = r . read next data block ( ) dshape = data . shape #r.read next data block shape() print ( dshape ) n blocks total = 0 for filename in filelist : print ( filename ) r = Guppi Raw ( filename ) n blocks total += r . n blocks print ( n blocks total ) full dshape = np . concatenate ( ( ( n blocks total , ) , dshape ) ) h5 = h5py . File ( fileroot + '.h5' , 'w' ) h5 . attrs [ 'CLASS' ] = 'GUPPIRAW' block size = 0 dset = h5 . create dataset ( 'data' , shape = full dshape , #compression=bitshuffle.h5.H5FILTER, #compression opts=(block size, bitshuffle.h5.H5 COMPRESS LZ4), dtype = data . dtype ) h5 idx = 0 for filename in filelist : print ( \"\\n Reading %s header...\" % filename ) r = Guppi Raw ( filename ) h5 = h5py . File ( filename + '.h5' , 'w' ) header , data = r . read next data block ( ) for ii in range ( 0 , r . n blocks ) : t0 = time . time ( ) print ( \"Reading block %i of %i\" % ( h5 idx + 1 , full dshape [ 0 ] ) ) header , data = r . read next data block ( ) t1 = time . time ( ) t2 = time . time ( ) print ( \"Writing block %i of %i\" % ( h5 idx + 1 , full dshape [ 0 ] ) ) dset [ h5 idx , : ] = data t3 = time . time ( ) print ( \"Read: %2.2fs, Write %2.2fs\" % ( ( t1 - t0 ) , ( t3 - t2 ) ) ) h5 idx += 1 for key , value in header . items ( ) : dset . attrs [ key ] = value h5 . close ( ) t1 = time . time ( ) print ( \"Conversion time: %2.2fs\" % ( t1 - t0 ) )", "predictions": ["run type in helper title"], "references": ["command line tool for converting guppi raw into hdf5 versions of guppi raw"], "bleu": 0.04635036983311895, "rouge_l": 0.0}
{"id": 2987, "code": "def is filterbank ( filename ) : with open ( filename , 'rb' ) as fh : is fil = True try : keyword , value , idx = read next header keyword ( fh ) try : assert keyword == b'HEADER START' except Assertion Error : is fil = False except Key Error : is fil = False return is fil", "predictions": ["return is a filterbank instance"], "references": ["open file and confirm if it is a filterbank file or not ."], "bleu": 0.10259023253147191, "rouge_l": 0.3086003372681282}
{"id": 2988, "code": "def to sigproc angle ( angle val ) : x = str ( angle val ) if '.' in x : if 'h' in x : d , m , s , ss = int ( x [ 0 : x . index ( 'h' ) ] ) , int ( x [ x . index ( 'h' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( '.' ) ] ) , float ( x [ x . index ( '.' ) : x . index ( 's' ) ] ) if 'd' in x : d , m , s , ss = int ( x [ 0 : x . index ( 'd' ) ] ) , int ( x [ x . index ( 'd' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( '.' ) ] ) , float ( x [ x . index ( '.' ) : x . index ( 's' ) ] ) else : if 'h' in x : d , m , s = int ( x [ 0 : x . index ( 'h' ) ] ) , int ( x [ x . index ( 'h' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( 's' ) ] ) if 'd' in x : d , m , s = int ( x [ 0 : x . index ( 'd' ) ] ) , int ( x [ x . index ( 'd' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( 's' ) ] ) ss = 0 num = str ( d ) . zfill ( 2 ) + str ( m ) . zfill ( 2 ) + str ( s ) . zfill ( 2 ) + '.' + str ( ss ) . split ( \".\" ) [ - 1 ] return np . float64 ( num ) . tostring ( )", "predictions": ["convert an self - self self . to an sigproc"], "references": ["convert an astropy . angle to the ridiculous sigproc angle format string ."], "bleu": 0.14615903653944176, "rouge_l": 0.42479108635097496}
{"id": 2989, "code": "def calc n ints in file ( filename ) : h = read header ( filename ) n bytes = int ( h [ b'nbits' ] / 8 ) n chans = h [ b'nchans' ] n ifs = h [ b'nifs' ] idx data = len header ( filename ) f = open ( filename , 'rb' ) f . seek ( idx data ) filesize = os . path . getsize ( filename ) n bytes data = filesize - idx data if h [ b'nbits' ] == 2 : n ints = int ( 4 * n bytes data / ( n chans * n ifs ) ) else : n ints = int ( n bytes data / ( n bytes * n chans * n ifs ) ) return n ints", "predictions": ["calculate the number of context in in in"], "references": ["calculate number of integrations in a given file"], "bleu": 0.239802967618271, "rouge_l": 0.5}
{"id": 2990, "code": "def to dict ( self ) : if self . tb next is None : tb next = None else : tb next = self . tb next . to dict ( ) code = { 'co filename' : self . tb frame . f code . co filename , 'co name' : self . tb frame . f code . co name , } frame = { 'f globals' : self . tb frame . f globals , 'f code' : code , } return { 'tb frame' : frame , 'tb lineno' : self . tb lineno , 'tb next' : tb next , }", "predictions": ["save this context into a dictionary ."], "references": ["convert a traceback into a dictionary representation"], "bleu": 0.345720784641941, "rouge_l": 0.42857142857142855}
{"id": 2991, "code": "def make rr subparser ( subparsers , rec type , args and types ) : sp = subparsers . add parser ( rec type ) sp . add argument ( \"name\" , type = str ) sp . add argument ( \"ttl\" , type = int , nargs = '?' ) sp . add argument ( rec type , type = str ) for my spec in args and types : ( argname , argtype ) = my spec [ : 2 ] if len ( my spec ) > 2 : nargs = my spec [ 2 ] sp . add argument ( argname , type = argtype , nargs = nargs ) else : sp . add argument ( argname , type = argtype ) return sp", "predictions": ["returns a get - friendly rr object for the given = = 0 in subparsers in subparsers in case in subparsers in subparsers in subparsers in subparsers in subparsers in subparsers"], "references": ["make a subparser for a given type of dns record"], "bleu": 0.046398855339878003, "rouge_l": 0.16123348017621145}
{"id": 2992, "code": "def make parser ( ) : line parser = Zonefile Line Parser ( ) subparsers = line parser . add subparsers ( ) sp = subparsers . add parser ( \"$ORIGIN\" ) sp . add argument ( \"$ORIGIN\" , type = str ) sp = subparsers . add parser ( \"$TTL\" ) sp . add argument ( \"$TTL\" , type = int ) args and types = [ ( \"mname\" , str ) , ( \"rname\" , str ) , ( \"serial\" , int ) , ( \"refresh\" , int ) , ( \"retry\" , int ) , ( \"expire\" , int ) , ( \"minimum\" , int ) ] make rr subparser ( subparsers , \"SOA\" , args and types ) make rr subparser ( subparsers , \"NS\" , [ ( \"host\" , str ) ] ) make rr subparser ( subparsers , \"A\" , [ ( \"ip\" , str ) ] ) make rr subparser ( subparsers , \"AAAA\" , [ ( \"ip\" , str ) ] ) make rr subparser ( subparsers , \"CNAME\" , [ ( \"alias\" , str ) ] ) make rr subparser ( subparsers , \"ALIAS\" , [ ( \"host\" , str ) ] ) make rr subparser ( subparsers , \"MX\" , [ ( \"preference\" , str ) , ( \"host\" , str ) ] ) make txt subparser ( subparsers ) make rr subparser ( subparsers , \"PTR\" , [ ( \"host\" , str ) ] ) make rr subparser ( subparsers , \"SRV\" , [ ( \"priority\" , int ) , ( \"weight\" , int ) , ( \"port\" , int ) , ( \"target\" , str ) ] ) make rr subparser ( subparsers , \"SPF\" , [ ( \"data\" , str ) ] ) make rr subparser ( subparsers , \"URI\" , [ ( \"priority\" , int ) , ( \"weight\" , int ) , ( \"target\" , str ) ] ) return line parser", "predictions": ["build the to - searches searches"], "references": ["make an argumentparser that accepts dns rrs"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 2993, "code": "def remove comments ( text ) : ret = [ ] lines = text . split ( \"\\n\" ) for line in lines : if len ( line ) == 0 : continue line = serialize ( tokenize line ( line ) ) ret . append ( line ) return \"\\n\" . join ( ret )", "predictions": ["check for for for for for for for the events in the events in the events in the events in the events in the events in the events in the events"], "references": ["remove comments from a zonefile"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 2994, "code": "def parse zone file ( text , ignore invalid = False ) : text = remove comments ( text ) text = flatten ( text ) text = remove class ( text ) text = add default name ( text ) json zone file = parse lines ( text , ignore invalid = ignore invalid ) return json zone file", "predictions": ["setup a time axis from a time axis ."], "references": ["parse a zonefile into a dict"], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 2995, "code": "def process origin ( data , template ) : record = \"\" if data is not None : record += \"$ORIGIN %s\" % data return template . replace ( \"{$origin}\" , record )", "predictions": ["compute a json - printed lst from a template template"], "references": ["replace { $origin } in template with a serialized $origin record"], "bleu": 0.12623203108004888, "rouge_l": 0.09442724458204334}
{"id": 2996, "code": "def process ttl ( data , template ) : record = \"\" if data is not None : record += \"$TTL %s\" % data return template . replace ( \"{$ttl}\" , record )", "predictions": ["calc the extent to a string"], "references": ["replace { $ttl } in template with a serialized $ttl record"], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 2997, "code": "def process soa ( data , template ) : record = template [ : ] if data is not None : assert len ( data ) == 1 , \"Only support one SOA RR at this time\" data = data [ 0 ] soadat = [ ] domain fields = [ 'mname' , 'rname' ] param fields = [ 'serial' , 'refresh' , 'retry' , 'expire' , 'minimum' ] for f in domain fields + param fields : assert f in data . keys ( ) , \"Missing '%s' (%s)\" % ( f , data ) data name = str ( data . get ( 'name' , '@' ) ) soadat . append ( data name ) if data . get ( 'ttl' ) is not None : soadat . append ( str ( data [ 'ttl' ] ) ) soadat . append ( \"IN\" ) soadat . append ( \"SOA\" ) for key in domain fields : value = str ( data [ key ] ) soadat . append ( value ) soadat . append ( \"(\" ) for key in param fields : value = str ( data [ key ] ) soadat . append ( value ) soadat . append ( \")\" ) soa txt = \" \" . join ( soadat ) record = record . replace ( \"{soa}\" , soa txt ) else : record = record . replace ( \"{soa}\" , \"\" ) return record", "predictions": ["closest soa to soa = record"], "references": ["replace { soa } in template with a set of serialized soa records"], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 2998, "code": "def process txt ( data , template ) : if data is None : to process = None else : to process = copy . deepcopy ( data ) for datum in to process : if isinstance ( datum [ \"txt\" ] , list ) : datum [ \"txt\" ] = \" \" . join ( [ '\"%s\"' % entry . replace ( \";\" , \"\\;\" ) for entry in datum [ \"txt\" ] ] ) else : datum [ \"txt\" ] = '\"%s\"' % datum [ \"txt\" ] . replace ( \";\" , \"\\;\" ) return process rr ( to process , \"TXT\" , \"txt\" , \"{txt}\" , template )", "predictions": ["get information from diff of diff and get a list of diff and get a list of variables"], "references": ["replace { txt } in template with the serialized txt records"], "bleu": 0.057259987315337726, "rouge_l": 0.0}
{"id": 2999, "code": "def parse schema string ( schema string ) : if isinstance ( schema string , str ) : schema string = schema string . decode ( \"utf8\" ) schema struct = json . loads ( schema string ) return Avro Schema Parser ( ) . parse schema struct ( schema struct )", "predictions": ["calc a selection size from a selection size"], "references": ["load and return a pyschema class from an avsc string"], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 3000, "code": "def to json compatible ( record ) : d = { } for fname , f in record . fields . iteritems ( ) : val = getattr ( record , fname ) if val is not None : d [ fname ] = f . dump ( val ) return d", "predictions": ["convert self to selection"], "references": ["dump record in json - encodable object format"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 3001, "code": "def from json compatible ( schema , dct ) : kwargs = { } for key in dct : field type = schema . fields . get ( key ) if field type is None : raise Parse Error ( \"Unexpected field encountered in line for record %s: %s\" % ( schema . name , key ) ) kwargs [ key ] = field type . load ( dct [ key ] ) return schema ( * * kwargs )", "predictions": ["create a schema from a json object"], "references": ["load from json - encodable"], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 3002, "code": "def from json compatible ( schema , dct ) : kwargs = { } for key in dct : field type = schema . fields . get ( key ) if field type is None : warnings . warn ( \"Unexpected field encountered in line for record %s: %r\" % ( schema . name , key ) ) continue kwargs [ key ] = field type . avro load ( dct [ key ] ) return schema ( * * kwargs )", "predictions": ["create a self from a n - grams self selection selection ."], "references": ["load from json - encodable"], "bleu": 0.11498759556447223, "rouge_l": 0.25416666666666665}
{"id": 3003, "code": "def all include attributes ( self , attributes ) : self . reload ( expand = True , attributes = attributes ) entities = [ Entity ( self , r , attributes = attributes ) for r in self . resources ] self . reload ( ) return entities", "predictions": ["return a list of include self = self = 0 = 1 = 1 = 1 = 0 = 1 = 0 = 0 = 1 = 1 = 0 ="], "references": ["returns all entities present in the collection with attributes included ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3004, "code": "def give another quote ( q ) : for qc in QUOTES : if qc != q : return qc else : raise Value Error ( u'Could not find a different quote for {}' . format ( q ) )", "predictions": ["start is the find function that is not a find ."], "references": ["when you pass a quote character returns you an another one if possible"], "bleu": 0.09497094417933137, "rouge_l": 0.08209959623149395}
{"id": 3005, "code": "def parse Command Line Arguments ( ) : parser = argparse . Argument Parser ( description = \"Plot predicted Gaia sky averaged proper motion errors as a function of V\" ) parser . add argument ( \"-p\" , action = \"store true\" , dest = \"pdf Output\" , help = \"Make PDF plot\" ) parser . add argument ( \"-b\" , action = \"store true\" , dest = \"png Output\" , help = \"Make PNG plot\" ) parser . add argument ( \"-g\" , action = \"store true\" , dest = \"gmag Abscissa\" , help = \"Plot performance vs G instead of V\" ) args = vars ( parser . parse args ( ) ) return args", "predictions": ["read command line arguments"], "references": ["set up command line parsing ."], "bleu": 0.2868106410131918, "rouge_l": 0.3860759493670886}
{"id": 3006, "code": "def parse Command Line Arguments ( ) : parser = argparse . Argument Parser ( description = \"Calculate parallax error for given G and (V-I)\" ) parser . add argument ( \"gmag\" , help = \"G-band magnitude of source\" , type = float ) parser . add argument ( \"vmini\" , help = \"(V-I) colour of source\" , type = float ) args = vars ( parser . parse args ( ) ) return args", "predictions": ["find the command line arguments"], "references": ["set up command line parsing ."], "bleu": 0.2941733261715515, "rouge_l": 0.3577712609970674}
{"id": 3007, "code": "def uniquote ( value ) : if isinstance ( value , six . binary type ) : try : value = value . decode ( 'utf-8' ) except Unicode Decode Error : value = six . text type ( dequote ( repr ( value ) ) ) result = six . text type ( value ) if isinstance ( value , six . text type ) : result = \"'%s'\" % result return result", "predictions": ["convert unicode value to string"], "references": ["convert to unicode and add quotes if initially a string"], "bleu": 0.1262909976406414, "rouge_l": 0.37731958762886597}
{"id": 3008, "code": "def serach path ( ) : operating system = get os ( ) return [ os . path . expanduser ( \"~/.kerncraft/iaca/{}/\" . format ( operating system ) ) , os . path . abspath ( os . path . dirname ( os . path . realpath ( file ) ) ) + '/iaca/{}/' . format ( operating system ) ]", "predictions": ["return the path to the serach directory ."], "references": ["return potential locations of iaca installation ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 3009, "code": "def build minimal runs ( events ) : events = [ e for i , e in enumerate ( events ) if events . index ( e ) == i ] scheduled runs = { } scheduled events = [ ] cur run = 0 while len ( scheduled events ) != len ( events ) : for event tpl in events : event , registers , parameters = event tpl if event tpl in scheduled events : continue for possible reg in register options ( registers ) : s = scheduled runs . setdefault ( cur run , { } ) if possible reg not in s : s [ possible reg ] = ( event , possible reg , parameters ) scheduled events . append ( event tpl ) break cur run += 1 runs = [ list ( v . values ( ) ) for v in scheduled runs . values ( ) ] return runs", "predictions": ["build a list of events from the registers ."], "references": ["compile list of minimal runs for given events ."], "bleu": 0.21105340631872635, "rouge_l": 0.4444444444444444}
{"id": 3010, "code": "def report ( self , output file = sys . stdout ) : max perf = self . results [ 'max perf' ] if self . args and self . args . verbose >= 3 : print ( '{}' . format ( pformat ( self . results ) ) , file = output file ) if self . args and self . args . verbose >= 1 : print ( '{}' . format ( pformat ( self . results [ 'verbose infos' ] ) ) , file = output file ) print ( 'Bottlenecks:' , file = output file ) print ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output file ) print ( '--------+--------------+-----------------+-------------------+----------------------' , file = output file ) print ( '    CPU |              | {!s:>15} |                   |' . format ( max perf [ self . args . unit ] ) , file = output file ) for b in self . results [ 'mem bottlenecks' ] : print ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . args . unit ] , * * b ) , file = output file ) print ( '' , file = output file ) if self . results [ 'min performance' ] [ 'FLOP/s' ] > max perf [ 'FLOP/s' ] : print ( 'CPU bound. {!s} due to CPU max. FLOP/s' . format ( max perf ) , file = output file ) else : print ( 'Cache or mem bound.' , file = output file ) bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] print ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output file ) print ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output file )", "predictions": ["print the results of the command line ."], "references": ["report analysis outcome in human readable form ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 3011, "code": "def analyze ( self ) : self . results = self . calculate cache access ( ) try : iaca analysis , asm block = self . kernel . iaca analysis ( micro architecture = self . machine [ 'micro-architecture' ] , asm block = self . asm block , pointer increment = self . pointer increment , verbose = self . verbose > 2 ) except Runtime Error as e : print ( \"IACA analysis failed: \" + str ( e ) ) sys . exit ( 1 ) block throughput = iaca analysis [ 'throughput' ] uops = iaca analysis [ 'uops' ] iaca output = iaca analysis [ 'output' ] port cycles = iaca analysis [ 'port cycles' ] elements per block = abs ( asm block [ 'pointer increment' ] / self . kernel . datatypes size [ self . kernel . datatype ] ) block size = elements per block * self . kernel . datatypes size [ self . kernel . datatype ] try : block to cl ratio = float ( self . machine [ 'cacheline size' ] ) / block size except Zero Division Error as e : print ( \"Too small block size / pointer increment:\" , e , file = sys . stderr ) sys . exit ( 1 ) port cycles = dict ( [ ( i [ 0 ] , i [ 1 ] * block to cl ratio ) for i in list ( port cycles . items ( ) ) ] ) uops = uops * block to cl ratio cl throughput = block throughput * block to cl ratio flops per element = sum ( self . kernel . flops . values ( ) ) self . results [ 'mem bottlenecks' ] [ 0 ] = None self . results [ 'min performance' ] = self . conv perf ( Prefixed Unit ( float ( 'inf' ) , 'FLOP/s' ) ) self . results [ 'bottleneck level' ] = None for level , bottleneck in enumerate ( self . results [ 'mem bottlenecks' ] ) : if level == 0 : continue if bottleneck [ 'performance' ] [ 'FLOP/s' ] < self . results [ 'min performance' ] [ 'FLOP/s' ] : self . results [ 'bottleneck level' ] = level self . results [ 'min performance' ] = bottleneck [ 'performance' ] self . results . update ( { 'cpu bottleneck' : { 'port cycles' : port cycles , 'cl throughput' : cl throughput , 'uops' : uops , 'performance throughput' : self . conv perf ( Prefixed Unit ( self . machine [ 'clock' ] / block throughput * elements per block * flops per element * self . cores , \"FLOP/s\" ) ) , 'IACA output' : iaca output } } )", "predictions": ["run the analysis on the screen"], "references": ["run complete analysis ."], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 3012, "code": "def report ( self , output file = sys . stdout ) : cpu perf = self . results [ 'cpu bottleneck' ] [ 'performance throughput' ] if self . verbose >= 3 : print ( '{}' . format ( pformat ( self . results ) ) , file = output file ) if self . verbose >= 1 : print ( 'Bottlenecks:' , file = output file ) print ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output file ) print ( '--------+--------------+-----------------+-------------------+----------------------' , file = output file ) print ( '    CPU |              | {!s:>15} |                   |' . format ( cpu perf [ self . args . unit ] ) , file = output file ) for b in self . results [ 'mem bottlenecks' ] : if b is None : continue print ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . args . unit ] , * * b ) , file = output file ) print ( '' , file = output file ) print ( 'IACA analisys:' , file = output file ) print ( '{!s}' . format ( { k : v for k , v in list ( self . results [ 'cpu bottleneck' ] . items ( ) ) if k not in [ 'IACA output' ] } ) , file = output file ) if self . results [ 'min performance' ] [ 'FLOP/s' ] > cpu perf [ 'FLOP/s' ] : print ( 'CPU bound. {!s} due to CPU bottleneck' . format ( cpu perf [ self . args . unit ] ) , file = output file ) else : print ( 'Cache or mem bound.' , file = output file ) bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] print ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output file ) print ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output file )", "predictions": ["print the cpu results to the screen ."], "references": ["print human readable report of model ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 3013, "code": "def analyze ( self ) : loop stack = list ( self . kernel . get loop stack ( ) ) if any ( [ l [ 'increment' ] != 1 for l in loop stack ] ) : raise Value Error ( \"Can not apply layer condition, since not all loops are of step \" \"length 1.\" ) for aref in list ( self . kernel . index order ( ) ) : while aref and len ( aref [ 0 ] ) == 0 : aref . pop ( 0 ) for i , idx names in enumerate ( aref ) : if i >= len ( loop stack ) or any ( [ loop stack [ i ] [ 'index' ] != idx . name for idx in idx names ] ) : raise Value Error ( \"Can not apply layer condition, order of indices in array \" \"does not follow order of loop indices. Single-dimension is \" \"currently not supported.\" ) for arefs in chain ( chain ( * self . kernel . sources . values ( ) ) , chain ( * self . kernel . destinations . values ( ) ) ) : if not arefs : continue while arefs and not arefs [ 0 ] . free symbols : arefs = arefs [ 1 : ] for i , expr in enumerate ( arefs ) : diff = sympy . diff ( expr , sympy . Symbol ( loop stack [ i ] [ 'index' ] ) ) if diff != 0 and diff != 1 : raise Value Error ( \"Can not apply layer condition, array references may not \" \"increment more then one per iteration.\" ) self . results = self . calculate cache access ( )", "predictions": ["apply the layer to the kernel stack ."], "references": ["run complete analysis ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 3014, "code": "def report ( self , output file = sys . stdout ) : if self . args and self . args . verbose > 2 : pprint ( self . results ) for dimension , lc info in self . results [ 'dimensions' ] . items ( ) : print ( \"{}D layer condition:\" . format ( dimension ) , file = output file ) for cache , lc solution in sorted ( lc info [ 'caches' ] . items ( ) ) : print ( cache + \": \" , end = '' , file = output file ) if lc solution [ 'lt' ] is sympy . true : print ( \"unconditionally fulfilled\" , file = output file ) else : if lc solution [ 'eq' ] is None : print ( \"{}\" . format ( lc solution [ 'lt' ] ) , file = output file ) elif type ( lc solution [ 'eq' ] ) is not list : print ( \"{}\" . format ( lc solution [ 'eq' ] ) , file = output file ) else : for solu in lc solution [ 'eq' ] : for s , v in solu . items ( ) : print ( \"{} <= {}\" . format ( s , v ) , file = output file )", "predictions": ["report the solution of the layer ."], "references": ["report generated model in human readable form ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 3015, "code": "def round to next ( x , base ) : return int ( base * math . ceil ( float ( x ) / base ) )", "predictions": ["round x to base coordinates"], "references": ["round float to next multiple of base ."], "bleu": 0.1781815298791261, "rouge_l": 0.44309927360774815}
{"id": 3016, "code": "def calculate cache access ( self ) : self . results . update ( { 'cycles' : [ ] , 'misses' : self . predictor . get misses ( ) , 'hits' : self . predictor . get hits ( ) , 'evicts' : self . predictor . get evicts ( ) , 'verbose infos' : self . predictor . get infos ( ) } )", "predictions": ["calculate cache access access access to cache ."], "references": ["dispatch to cache predictor to get cache stats ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 3017, "code": "def analyze ( self ) : self . calculate cache access ( ) self . calculate cycles ( ) self . results [ 'flops per iteration' ] = sum ( self . kernel . flops . values ( ) ) return self . results", "predictions": ["calculate the results from the cache ."], "references": ["run complete anaylysis and return results ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 3018, "code": "def report ( self , output file = sys . stdout ) : if self . verbose > 1 : print ( '{}' . format ( pprint . pformat ( self . results [ 'verbose infos' ] ) ) , file = output file ) for level , cycles in self . results [ 'cycles' ] : print ( '{} = {}' . format ( level , self . conv cy ( cycles ) [ self . args . unit ] ) , file = output file ) if self . verbose > 1 : if 'memory bandwidth kernel' in self . results : print ( 'memory cycles based on {} kernel with {}' . format ( self . results [ 'memory bandwidth kernel' ] , self . results [ 'memory bandwidth' ] ) , file = output file ) if self . verbose > 1 : print ( file = output file ) print ( self . report data transfers ( ) , file = output file )", "predictions": ["print the report of the results of the command ."], "references": ["print generated model data in human readable format ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 3019, "code": "def analyze ( self ) : try : incore analysis , asm block = self . kernel . iaca analysis ( micro architecture = self . machine [ 'micro-architecture' ] , asm block = self . asm block , pointer increment = self . pointer increment , verbose = self . verbose > 2 ) except Runtime Error as e : print ( \"IACA analysis failed: \" + str ( e ) ) sys . exit ( 1 ) block throughput = incore analysis [ 'throughput' ] port cycles = incore analysis [ 'port cycles' ] uops = incore analysis [ 'uops' ] elements per block = abs ( asm block [ 'pointer increment' ] // self . kernel . datatypes size [ self . kernel . datatype ] ) block size = elements per block * self . kernel . datatypes size [ self . kernel . datatype ] try : block to cl ratio = float ( self . machine [ 'cacheline size' ] ) / block size except Zero Division Error as e : print ( \"Too small block size / pointer increment:\" , e , file = sys . stderr ) sys . exit ( 1 ) port cycles = dict ( [ ( i [ 0 ] , i [ 1 ] * block to cl ratio ) for i in list ( port cycles . items ( ) ) ] ) uops = uops * block to cl ratio cl throughput = block throughput * block to cl ratio T OL = max ( [ v for k , v in list ( port cycles . items ( ) ) if k in self . machine [ 'overlapping model' ] [ 'ports' ] ] ) T n OL = max ( [ v for k , v in list ( port cycles . items ( ) ) if k in self . machine [ 'non-overlapping model' ] [ 'ports' ] ] ) if T n OL < cl throughput : T OL = cl throughput self . results = { 'port cycles' : port cycles , 'cl throughput' : self . conv cy ( cl throughput ) , 'uops' : uops , 'T n OL' : T n OL , 'T OL' : T OL , 'IACA output' : incore analysis [ 'output' ] , 'elements per block' : elements per block , 'pointer increment' : asm block [ 'pointer increment' ] , 'flops per iteration' : sum ( self . kernel . flops . values ( ) ) } return self . results", "predictions": ["find the analysis of the analysis ."], "references": ["run complete analysis and return results ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 3020, "code": "def report ( self , output file = sys . stdout ) : if self . verbose > 2 : print ( \"IACA Output:\" , file = output file ) print ( self . results [ 'IACA output' ] , file = output file ) print ( '' , file = output file ) if self . verbose > 1 : print ( 'Detected pointer increment: {}' . format ( self . results [ 'pointer increment' ] ) , file = output file ) print ( 'Derived elements stored to per asm block iteration: {}' . format ( self . results [ 'elements per block' ] ) , file = output file ) print ( 'Ports and cycles:' , str ( self . results [ 'port cycles' ] ) , file = output file ) print ( 'Uops:' , str ( self . results [ 'uops' ] ) , file = output file ) print ( 'Throughput: {}' . format ( self . results [ 'cl throughput' ] [ self . args . unit ] ) , file = output file ) print ( 'T n OL = {:.1f} cy/CL' . format ( self . results [ 'T n OL' ] ) , file = output file ) print ( 'T OL = {:.1f} cy/CL' . format ( self . results [ 'T OL' ] ) , file = output file )", "predictions": ["print the report of the results of the asm ."], "references": ["print generated model data in human readable format ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 3021, "code": "def strip and uncomment ( asm lines ) : asm stripped = [ ] for line in asm lines : asm stripped . append ( line . split ( '#' ) [ 0 ] . strip ( ) ) return asm stripped", "predictions": ["strip asm lines from the asm ."], "references": ["strip whitespaces and comments from asm lines ."], "bleu": 0.25201472805660513, "rouge_l": 0.5269978401727862}
{"id": 3022, "code": "def strip unreferenced labels ( asm lines ) : asm stripped = [ ] for line in asm lines : if re . match ( r'^\\S+:' , line ) : label = line [ 0 : line . find ( ':' ) ] if not any ( [ re . match ( r'^[^#]*\\s' + re . escape ( label ) + '[\\s,]?.*$' , l ) for l in asm lines ] ) : line = '' asm stripped . append ( line ) return asm stripped", "predictions": ["strip unreferenced labels from the asm ."], "references": ["strip all labels which are never referenced ."], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 3023, "code": "def select best block ( blocks ) : if not blocks : raise Value Error ( \"No suitable blocks were found in assembly.\" ) best block = max ( blocks , key = lambda b : b [ 1 ] [ 'packed instr' ] ) if best block [ 1 ] [ 'packed instr' ] == 0 : best block = max ( blocks , key = lambda b : ( b [ 1 ] [ 'ops' ] + b [ 1 ] [ 'packed instr' ] + b [ 1 ] [ 'avx instr' ] , b [ 1 ] [ 'ZMM' ] , b [ 1 ] [ 'YMM' ] , b [ 1 ] [ 'XMM' ] ) ) return best block [ 0 ]", "predictions": ["select the best best block block ."], "references": ["return best block selected based on simple heuristic ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 3024, "code": "def userselect increment ( block ) : print ( \"Selected block:\" ) print ( '\\n    ' + ( '\\n    ' . join ( block [ 'lines' ] ) ) ) print ( ) increment = None while increment is None : increment = input ( \"Choose store pointer increment (number of bytes): \" ) try : increment = int ( increment ) except Value Error : increment = None block [ 'pointer increment' ] = increment return increment", "predictions": ["increment the pointer of a block block ."], "references": ["let user interactively select byte increment ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 3025, "code": "def userselect block ( blocks , default = None , debug = False ) : print ( \"Blocks found in assembly file:\" ) print ( \"      block     | O Ps | pck. | AVX || Registers |    ZMM   |    YMM   |    XMM   |    GP   ||ptr.inc|\\n\" \"----------------+-----+------+-----++-----------+----------+----------+----------+---------++-------|\" ) for idx , b in blocks : print ( '{:>2} {b[labels]!r:>12} | {b[ops]:>3} | {b[packed instr]:>4} | {b[avx instr]:>3} |' '| {b[regs][0]:>3} ({b[regs][1]:>3}) | {b[ZMM][0]:>3} ({b[ZMM][1]:>2}) | ' '{b[YMM][0]:>3} ({b[YMM][1]:>2}) | ' '{b[XMM][0]:>3} ({b[XMM][1]:>2}) | {b[GP][0]:>2} ({b[GP][1]:>2}) || ' '{b[pointer increment]!s:>5} |' . format ( idx , b = b ) ) if debug : ln = b [ 'first line' ] print ( ' ' * 4 + 'Code:' ) for l in b [ 'lines' ] : print ( ' ' * 8 + '{:>5} | {}' . format ( ln , l ) ) ln += 1 print ( ' ' * 4 + 'Metadata:' ) print ( textwrap . indent ( pformat ( { k : v for k , v in b . items ( ) if k not in [ 'lines' ] } ) , ' ' * 8 ) ) block idx = - 1 while not ( 0 <= block idx < len ( blocks ) ) : block idx = input ( \"Choose block to be marked [\" + str ( default ) + \"]: \" ) or default try : block idx = int ( block idx ) except Value Error : block idx = - 1 return block idx", "predictions": ["print a block of the assembly blocks"], "references": ["let user interactively select block ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 3026, "code": "def insert markers ( asm lines , start line , end line ) : asm lines = ( asm lines [ : start line ] + START MARKER + asm lines [ start line : end line + 1 ] + END MARKER + asm lines [ end line + 1 : ] ) return asm lines", "predictions": ["insert markers into a single line of asm ."], "references": ["insert iaca marker into list of asm instructions at given indices ."], "bleu": 0.158278836853973, "rouge_l": 0.4642313546423136}
{"id": 3027, "code": "def main ( ) : parser = argparse . Argument Parser ( description = 'Find and analyze basic loop blocks and mark for IACA.' , epilog = '/RRZE-HPC/kerncraft\\n License: AGP Lv3' ) parser . add argument ( '--version' , action = 'version' , version = '%(prog)s {}' . format ( version ) ) parser . add argument ( 'source' , type = argparse . File Type ( ) , nargs = '?' , default = sys . stdin , help = 'assembly file to analyze (default: stdin)' ) parser . add argument ( '--outfile' , '-o' , type = argparse . File Type ( 'w' ) , nargs = '?' , default = sys . stdout , help = 'output file location (default: stdout)' ) parser . add argument ( '--debug' , action = 'store true' , help = 'Output nternal analysis information for debugging.' ) args = parser . parse args ( ) iaca instrumentation ( input file = args . source , output file = args . outfile , block selection = 'manual' , pointer increment = 1 , debug = args . debug )", "predictions": ["command line interface for the nternal cli ."], "references": ["execute command line interface ."], "bleu": 0.3155984539112945, "rouge_l": 0.6421052631578947}
{"id": 3028, "code": "def simulate ( kernel , model , define dict , blocking constant , blocking length ) : kernel . clear state ( ) for k , v in define dict . items ( ) : kernel . set constant ( k , v ) kernel . set constant ( blocking constant , blocking length ) model . analyze ( ) return sum ( [ cy for dscr , cy in model . results [ 'cycles' ] ] )", "predictions": ["simulate a model with a list of kernel numbers ."], "references": ["setup and execute model with given blocking length"], "bleu": 0.16590387014219712, "rouge_l": 0.22676579925650556}
{"id": 3029, "code": "def get last modified datetime ( dir path = os . path . dirname ( file ) ) : max mtime = 0 for root , dirs , files in os . walk ( dir path ) : for f in files : p = os . path . join ( root , f ) try : max mtime = max ( max mtime , os . stat ( p ) . st mtime ) except File Not Found Error : pass return datetime . utcfromtimestamp ( max mtime )", "predictions": ["get the last modified datetime in a directory"], "references": ["return datetime object of latest change in kerncraft module directory ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 3030, "code": "def create parser ( ) : parser = argparse . Argument Parser ( description = 'Analytical performance modelling and benchmarking toolkit.' , epilog = '/RRZE-HPC/kerncraft\\n License: AGP Lv3' ) parser . add argument ( '--version' , action = 'version' , version = '%(prog)s {}' . format ( version ) ) parser . add argument ( '--machine' , '-m' , type = argparse . File Type ( 'r' ) , required = True , help = 'Path to machine description yaml file.' ) parser . add argument ( '--pmodel' , '-p' , choices = models . all , required = True , action = 'append' , default = [ ] , help = 'Performance model to apply' ) parser . add argument ( '-D' , '--define' , nargs = 2 , metavar = ( 'KEY' , 'VALUE' ) , default = [ ] , action = Append String Range , help = 'Define constant to be used in C code. Values must be integer or ' 'match start-stop[:num[log[base]]]. If range is given, all ' 'permutation s will be tested. Overwrites constants from testcase ' 'file.' ) parser . add argument ( '--verbose' , '-v' , action = 'count' , default = 0 , help = 'Increases verbosity level.' ) parser . add argument ( 'code file' , metavar = 'FILE' , type = argparse . File Type ( ) , help = 'File with loop kernel C code' ) parser . add argument ( '--asm-block' , metavar = 'BLOCK' , default = 'auto' , help = 'Number of ASM block to mark for IACA, \"auto\" for automatic ' 'selection or \"manual\" for interactiv selection.' ) parser . add argument ( '--pointer-increment' , metavar = 'INCR' , default = 'auto' , type = int or str , help = 'Increment of store pointer within one ASM block in bytes. If \"auto\": ' 'automatic detection, error on failure to detect, if ' '\"auto with manual fallback\": fallback to manual input, or if ' '\"manual\": always prompt user.' ) parser . add argument ( '--store' , metavar = 'PICKLE' , type = argparse . File Type ( 'a+b' ) , help = 'Addes results to PICKLE file for later processing.' ) parser . add argument ( '--unit' , '-u' , choices = [ 'cy/CL' , 'cy/It' , 'It/s' , 'FLOP/s' ] , help = 'Select the output unit, defaults to model specific if not given.' ) parser . add argument ( '--cores' , '-c' , metavar = 'CORES' , type = int , default = 1 , help = 'Number of cores to be used in parallel. (default: 1) ' 'ECM model will consider the scaling of the last level cache and ' 'predict the overall performance in addition to single-core behavior. ' 'The benchmark mode will run the code with Open MP on as many physical ' 'cores.' ) parser . add argument ( '--kernel-description' , action = 'store true' , help = 'Use kernel description instead of analyzing the kernel code.' ) parser . add argument ( '--clean-intermediates' , action = 'store true' , help = 'If set, will delete all intermediate files after completion.' ) parser . add argument ( '--cache-predictor' , '-P' , choices = [ 'LC' , 'SIM' ] , default = 'SIM' , help = 'Change cache predictor to use, options are LC (layer conditions) and ' 'SIM (cache simulation with pycachesim), default is SIM.' ) parser . add argument ( '--compiler' , '-C' , type = str , default = None , help = 'Compiler to use, default is first in machine description file.' ) parser . add argument ( '--compiler-flags' , type = str , default = None , help = 'Compiler flags to use. If not set, flags are taken from machine ' 'description file (-std=c99 is always added).' ) for m in models . all : ag = parser . add argument group ( 'arguments for ' + m + ' model' , getattr ( models , m ) . name ) getattr ( models , m ) . configure arggroup ( ag ) return parser", "predictions": ["create the parser for the command line ."], "references": ["return argparse parser ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 3031, "code": "def check arguments ( args , parser ) : if args . asm block not in [ 'auto' , 'manual' ] : try : args . asm block = int ( args . asm block ) except Value Error : parser . error ( '--asm-block can only be \"auto\", \"manual\" or an integer' ) if not args . unit : if 'Roofline' in args . pmodel or 'Roofline IACA' in args . pmodel : args . unit = 'FLOP/s' else : args . unit = 'cy/CL'", "predictions": ["check if arguments are valid ."], "references": ["check arguments passed by user that are not checked by argparse itself ."], "bleu": 0.08649595219978225, "rouge_l": 0.39482200647249194}
{"id": 3032, "code": "def main ( ) : parser = create parser ( ) args = parser . parse args ( ) check arguments ( args , parser ) run ( parser , args )", "predictions": ["main entry point for the application ."], "references": ["initialize and run command line interface ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 3033, "code": "def main ( ) : parser = argparse . Argument Parser ( description = 'Recursively merges two or more pickle files. Only supports pickles consisting ' 'of a single dictionary object.' ) parser . add argument ( 'destination' , type = argparse . File Type ( 'r+b' ) , help = 'File to write to and include in resulting pickle. (WILL BE CHANGED)' ) parser . add argument ( 'source' , type = argparse . File Type ( 'rb' ) , nargs = '+' , help = 'File to include in resulting pickle.' ) args = parser . parse args ( ) result = pickle . load ( args . destination ) assert isinstance ( result , collections . Mapping ) , \"only Mapping types can be handled.\" for s in args . source : data = pickle . load ( s ) assert isinstance ( data , collections . Mapping ) , \"only Mapping types can be handled.\" update ( result , data ) args . destination . seek ( 0 ) args . destination . truncate ( ) pickle . dump ( result , args . destination )", "predictions": ["main function for the script ."], "references": ["comand line interface of picklemerge ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 3034, "code": "def symbol pos int ( * args , * * kwargs ) : kwargs . update ( { 'positive' : True , 'integer' : True } ) return sympy . Symbol ( * args , * * kwargs )", "predictions": ["update a symbol in a symbol ."], "references": ["create a sympy . symbol with positive and integer assumptions ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 3035, "code": "def find node type ( ast , node type ) : if type ( ast ) is node type : return [ ast ] elif type ( ast ) is list : return reduce ( operator . add , list ( map ( lambda a : find node type ( a , node type ) , ast ) ) , [ ] ) elif ast is None : return [ ] else : return reduce ( operator . add , [ find node type ( o [ 1 ] , node type ) for o in ast . children ( ) ] , [ ] )", "predictions": ["finds the type of the node in the given type ."], "references": ["return list of array references in ast ."], "bleu": 0.1354599427337814, "rouge_l": 0.32504440497335696}
{"id": 3036, "code": "def force iterable ( f ) : def wrapper ( * args , * * kwargs ) : r = f ( * args , * * kwargs ) if hasattr ( r , ' iter ' ) : return r else : return [ r ] return wrapper", "predictions": ["force a function to force iterable"], "references": ["will make any functions return an iterable objects by wrapping its result in a list ."], "bleu": 0.046172815301777345, "rouge_l": 0.0840220385674931}
{"id": 3037, "code": "def check ( self ) : datatypes = [ v [ 0 ] for v in self . variables . values ( ) ] assert len ( set ( datatypes ) ) <= 1 , 'mixing of datatypes within a kernel is not supported.'", "predictions": ["check that all variables have the same kernel ."], "references": ["check that information about kernel makes sens and is valid ."], "bleu": 0.1689983564524028, "rouge_l": 0.3929146537842191}
{"id": 3038, "code": "def subs consts ( self , expr ) : if isinstance ( expr , numbers . Number ) : return expr else : return expr . subs ( self . constants )", "predictions": ["return the consts expression ."], "references": ["substitute constants in expression unless it is already a number ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 3039, "code": "def remove duplicate accesses ( self ) : self . destinations = { var name : set ( acs ) for var name , acs in self . destinations . items ( ) } self . sources = { var name : set ( acs ) for var name , acs in self . sources . items ( ) }", "predictions": ["remove duplicate destinations from the acs ."], "references": ["remove duplicate source and destination accesses"], "bleu": 0.24446151121745047, "rouge_l": 0.31202046035805625}
{"id": 3040, "code": "def get loop stack ( self , subs consts = False ) : for l in self . loop stack : if subs consts : yield { 'index' : l [ 0 ] , 'start' : self . subs consts ( l [ 1 ] ) , 'stop' : self . subs consts ( l [ 2 ] ) , 'increment' : self . subs consts ( l [ 3 ] ) } else : yield { 'index' : l [ 0 ] , 'start' : l [ 1 ] , 'stop' : l [ 2 ] , 'increment' : l [ 3 ] }", "predictions": ["get the path of the path to the path of the path"], "references": ["yield loop stack dictionaries in order from outer to inner ."], "bleu": 0.10390302174233558, "rouge_l": 0.08764367816091953}
{"id": 3041, "code": "def global iterator ( self ) : global iterator = sympy . Integer ( 0 ) total length = sympy . Integer ( 1 ) for var name , start , end , incr in reversed ( self . loop stack ) : loop var = symbol pos int ( var name ) length = end - start global iterator += ( loop var - start ) * total length total length *= length return global iterator", "predictions": ["parameters are in the build cur ."], "references": ["return global iterator sympy expression"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3042, "code": "def max global iteration ( self ) : return self . indices to global iterator ( { symbol pos int ( var name ) : end - 1 for var name , start , end , incr in self . loop stack } )", "predictions": ["maximum self . stdout ."], "references": ["return global iterator with last iteration number"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 3043, "code": "def print kernel info ( self , output file = sys . stdout ) : table = ( '     idx |        min        max       step\\n' + '---------+---------------------------------\\n' ) for l in self . loop stack : table += '{:>8} | {!r:>10} {!r:>10} {!r:>10}\\n' . format ( * l ) print ( prefix indent ( 'loop stack:        ' , table ) , file = output file ) table = ( '    name |  offsets   ...\\n' + '---------+------------...\\n' ) for name , offsets in list ( self . sources . items ( ) ) : prefix = '{:>8} | ' . format ( name ) right side = '\\n' . join ( [ '{!r:}' . format ( o ) for o in offsets ] ) table += prefix indent ( prefix , right side , later prefix = '         | ' ) print ( prefix indent ( 'data sources:      ' , table ) , file = output file ) table = ( '    name |  offsets   ...\\n' + '---------+------------...\\n' ) for name , offsets in list ( self . destinations . items ( ) ) : prefix = '{:>8} | ' . format ( name ) right side = '\\n' . join ( [ '{!r:}' . format ( o ) for o in offsets ] ) table += prefix indent ( prefix , right side , later prefix = '         | ' ) print ( prefix indent ( 'data destinations: ' , table ) , file = output file ) table = ( ' op | count \\n' + '----+-------\\n' ) for op , count in list ( self . flops . items ( ) ) : table += '{:>3} | {:>4}\\n' . format ( op , count ) table += '     =======\\n' table += '      {:>4}' . format ( sum ( self . flops . values ( ) ) ) print ( prefix indent ( 'FLO Ps:     ' , table ) , file = output file )", "predictions": ["analyze the kernel information to the terminal access access verbose access access access access access access ."], "references": ["print kernel information in human readble format ."], "bleu": 0.10216198665886358, "rouge_l": 0.2566619915848527}
{"id": 3044, "code": "def print variables info ( self , output file = sys . stdout ) : table = ( '    name |   type size             \\n' + '---------+-------------------------\\n' ) for name , var info in list ( self . variables . items ( ) ) : table += '{:>8} | {:>6} {!s:<10}\\n' . format ( name , var info [ 0 ] , var info [ 1 ] ) print ( prefix indent ( 'variables: ' , table ) , file = output file )", "predictions": ["report variables variables information ."], "references": ["print variables information in human readble format ."], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 3045, "code": "def print constants info ( self , output file = sys . stdout ) : table = ( '    name | value     \\n' + '---------+-----------\\n' ) for name , value in list ( self . constants . items ( ) ) : table += '{!s:>8} | {:<10}\\n' . format ( name , value ) print ( prefix indent ( 'constants: ' , table ) , file = output file )", "predictions": ["analyze the constants information to the stack kernel kernel kernel kernel kernel kernel kernel kernel kernel ."], "references": ["print constants information in human readble format ."], "bleu": 0.10216198665886358, "rouge_l": 0.2566619915848527}
{"id": 3046, "code": "def print kernel code ( self , output file = sys . stdout ) : print ( self . kernel code , file = output file )", "predictions": ["report the kernel self . . ."], "references": ["print source code of kernel ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 3047, "code": "def get array declarations ( self ) : return [ d for d in self . kernel ast . block items if type ( d ) is c ast . Decl and type ( d . type ) is c ast . Array Decl ]", "predictions": ["return list of all . next to the ."], "references": ["return array declarations ."], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 3048, "code": "def get kernel loop nest ( self ) : loop nest = [ s for s in self . kernel ast . block items if type ( s ) in [ c ast . For , c ast . Pragma , c ast . Func Call ] ] assert len ( loop nest ) >= 1 , \"Found to few for statements in kernel\" return loop nest", "predictions": ["calculate the cache access nest nest nest"], "references": ["return kernel loop nest including any preceding pragmas and following swaps ."], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 3049, "code": "def find inner most loop ( self , loop nest ) : r = None for s in loop nest : if type ( s ) is c ast . For : return self . find inner most loop ( s ) or s else : r = r or self . find inner most loop ( s ) return r", "predictions": ["analyze the inner loop loop in the loop return none if not found return none otherwise ."], "references": ["return inner most for loop in loop nest"], "bleu": 0.11306082351602978, "rouge_l": 0.34221598877980364}
{"id": 3050, "code": "def build kernel function declaration ( self , name = 'kernel' ) : array declarations , array dimensions = self . build array declarations ( with init = False ) scalar declarations = self . build scalar declarations ( with init = False ) const declarations = self . build const declartions ( with init = False ) return c ast . Func Decl ( args = c ast . Param List ( params = array declarations + scalar declarations + const declarations ) , type = c ast . Type Decl ( declname = name , quals = [ ] , type = c ast . Identifier Type ( names = [ 'void' ] ) ) )", "predictions": ["report the kernel self 1 1 1 1 1 1 1 - d list of ast"], "references": ["build and return kernel function declaration"], "bleu": 0.07692375026049747, "rouge_l": 0.09902597402597402}
{"id": 3051, "code": "def build scalar declarations ( self , with init = True ) : scalar declarations = [ deepcopy ( d ) for d in self . kernel ast . block items if type ( d ) is c ast . Decl and type ( d . type ) is c ast . Type Decl ] if with init : random . seed ( 2342 ) for d in scalar declarations : if d . type . type . names [ 0 ] in [ 'double' , 'float' ] : d . init = c ast . Constant ( 'float' , str ( random . uniform ( 1.0 , 0.1 ) ) ) elif d . type . type . names [ 0 ] in [ 'int' , 'long' , 'long long' , 'unsigned int' , 'unsigned long' , 'unsigned long long' ] : d . init = c ast . Constant ( 'int' , 2 ) return scalar declarations", "predictions": ["analyze the scalar self machine machine machine machine machine machine machine machine machine machine machine machine"], "references": ["build and return scalar variable declarations"], "bleu": 0.07692375026049747, "rouge_l": 0.09902597402597402}
{"id": 3052, "code": "def build kernel call ( self , name = 'kernel' ) : return c ast . Func Call ( name = c ast . ID ( name = name ) , args = c ast . Expr List ( exprs = [ c ast . ID ( name = d . name ) for d in ( self . build array declarations ( ) [ 0 ] + self . build scalar declarations ( ) + self . build const declartions ( ) ) ] ) )", "predictions": ["report the kernel self . ."], "references": ["generate and return kernel call ast ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 3053, "code": "def get main code ( self , as filename = False , kernel function name = 'kernel' ) : assert self . kernel ast is not None , \"AST does not exist, this could be due to running \" \"based on a kernel description rather than code.\" fp , already available = self . get intermediate file ( 'main.c' , machine and compiler dependent = False ) if already available : code = fp . read ( ) else : parser = C Parser ( ) template code = self . CODE TEMPLATE template ast = parser . parse ( clean code ( template code , macros = True , comments = True , pragmas = False ) ) ast = deepcopy ( template ast ) replace id ( ast , \"DECLARE CONSTS\" , self . build const declartions ( with init = True ) ) array declarations , array dimensions = self . build array declarations ( ) replace id ( ast , \"DECLARE ARRAYS\" , array declarations ) replace id ( ast , \"DECLARE INIT SCALARS\" , self . build scalar declarations ( ) ) replace id ( ast , \"DUMMY CALLS\" , self . build dummy calls ( ) ) ast . ext . insert ( 0 , self . build kernel function declaration ( name = kernel function name ) ) replace id ( ast , \"KERNEL CALL\" , self . build kernel call ( ) ) replace id ( ast , \"INIT ARRAYS\" , self . build array initializations ( array dimensions ) ) code = C Generator ( ) . visit ( ast ) code = '\\n' . join ( [ l for l in template code . split ( '\\n' ) if l . startswith ( \"#include\" ) ] ) + '\\n\\n' + code fp . write ( code ) fp . close ( ) if as filename : return fp . name else : return code", "predictions": ["return the and kernel uncomment uncomment"], "references": ["generate and return compilable source code from ast ."], "bleu": 0.14827340167306757, "rouge_l": 0.12869198312236285}
{"id": 3054, "code": "def build executable ( self , lflags = None , verbose = False , openmp = False ) : compiler , compiler args = self . machine . get compiler ( ) kernel obj filename = self . compile kernel ( openmp = openmp , verbose = verbose ) out filename , already exists = self . get intermediate file ( os . path . splitext ( os . path . basename ( kernel obj filename ) ) [ 0 ] , binary = True , fp = False ) if not already exists : main source filename = self . get main code ( as filename = True ) if not ( ( 'LIKWID INCLUDE' in os . environ or 'LIKWID INC' in os . environ ) and 'LIKWID LIB' in os . environ ) : print ( 'Could not find LIKWID INCLUDE (e.g., \"-I/app/likwid/4.1.2/include\") and ' 'LIKWID LIB (e.g., \"-L/apps/likwid/4.1.2/lib\") environment variables' , file = sys . stderr ) sys . exit ( 1 ) compiler args += [ '-std=c99' , '-I' + reduce path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( file ) ) ) + '/headers/' ) , os . environ . get ( 'LIKWID INCLUDE' , '' ) , os . environ . get ( 'LIKWID INC' , '' ) , '-llikwid' ] if os . environ . get ( 'LIKWID LIB' ) == '' : compiler args = compiler args [ : - 1 ] if lflags is None : lflags = [ ] lflags += os . environ [ 'LIKWID LIB' ] . split ( ' ' ) + [ '-pthread' ] compiler args += os . environ [ 'LIKWID LIB' ] . split ( ' ' ) + [ '-pthread' ] infiles = [ reduce path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( file ) ) ) + '/headers/dummy.c' ) , kernel obj filename , main source filename ] cmd = [ compiler ] + infiles + compiler args + [ '-o' , out filename ] cmd = list ( filter ( bool , cmd ) ) if verbose : print ( 'Executing (build executable): ' , ' ' . join ( cmd ) ) try : subprocess . check output ( cmd ) except subprocess . Called Process Error as e : print ( \"Build failed:\" , e , file = sys . stderr ) sys . exit ( 1 ) else : if verbose : print ( 'Executing (build executable): ' , 'using cached' , out filename ) return out filename", "predictions": ["strip the unreferenced from the match"], "references": ["compile source to executable with likwid capabilities and return the executable name ."], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 3055, "code": "def string to sympy ( cls , s ) : if isinstance ( s , int ) : return sympy . Integer ( s ) elif isinstance ( s , list ) : return tuple ( [ cls . string to sympy ( e ) for e in s ] ) elif s is None : return None else : local dict = { c : symbol pos int ( c ) for c in s if c in string . ascii letters } preliminary expr = parse expr ( s , local dict = local dict ) local dict . update ( { s . name : symbol pos int ( s . name ) for s in preliminary expr . free symbols } ) return parse expr ( s , local dict = local dict )", "predictions": ["convert select select to block suitable for block suitable for block suitable for block suitable to block ."], "references": ["convert any string to a sympy object or none ."], "bleu": 0.08097785064266204, "rouge_l": 0.2259259259259259}
{"id": 3056, "code": "def get identifier ( self ) : if self . path : return os . path . basename ( self . path ) else : return hashlib . sha256 ( hashlib . sha256 ( repr ( self . data ) . encode ( ) ) ) . hexdigest ( )", "predictions": ["get sha256 increment the sha256 increment"], "references": ["return identifier which is either the machine file name or sha256 checksum of data ."], "bleu": 0.054546736148076896, "rouge_l": 0.17681159420289855}
{"id": 3057, "code": "def get last modified datetime ( self ) : if self . path : statbuf = os . stat ( self . path ) return datetime . utcfromtimestamp ( statbuf . st mtime ) else : return datetime . now ( )", "predictions": ["get blocks of the block . ."], "references": ["return datetime object of modified time of machine file . return now if not a file ."], "bleu": 0.05293793409875998, "rouge_l": 0.23252858958068615}
{"id": 3058, "code": "def enforce no overlap ( self , start at = 0 ) : i = start at while i + 1 < len ( self . data ) : if self . data [ i ] [ 1 ] >= self . data [ i + 1 ] [ 0 ] : if self . data [ i ] [ 1 ] < self . data [ i + 1 ] [ 1 ] : self . data [ i ] [ 1 ] = self . data [ i + 1 ] [ 1 ] del self . data [ i + 1 ] i += 1", "predictions": ["remove all overlap that are overlap overlap"], "references": ["enforce that no ranges overlap in internal storage ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 3059, "code": "def get header path ( ) -> str : import os return os . path . abspath ( os . path . dirname ( os . path . realpath ( file ) ) ) + '/headers/'", "predictions": ["return the header header directory ."], "references": ["return local folder path of header files ."], "bleu": 0.18822631894109965, "rouge_l": 0.4178082191780822}
{"id": 3060, "code": "def align iteration with cl boundary ( self , iteration , subtract = True ) : element size = self . kernel . datatypes size [ self . kernel . datatype ] cacheline size = self . machine [ 'cacheline size' ] elements per cacheline = int ( cacheline size // element size ) inner loop = list ( self . kernel . get loop stack ( subs consts = True ) ) [ - 1 ] inner increment = inner loop [ 'increment' ] o = self . kernel . compile global offsets ( iteration = iteration ) [ 0 ] if len ( o [ 1 ] ) : first offset = min ( o [ 1 ] ) else : first offset = min ( o [ 0 ] ) diff = first offset - ( int ( first offset ) >> self . csim . first level . cl bits << self . csim . first level . cl bits ) if diff == 0 : return iteration elif subtract : return iteration - ( diff // element size ) // inner increment else : return iteration + ( elements per cacheline - diff // element size ) // inner increment", "predictions": ["simulate the iteration iteration kernel kernel kernel for the k for the k stack for the k stack"], "references": ["align iteration with cacheline boundary ."], "bleu": 0.06809398432036522, "rouge_l": 0.09159159159159158}
{"id": 3061, "code": "def get loads ( self ) : return [ self . stats [ cache level ] [ 'LOAD count' ] / self . first dim factor for cache level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]", "predictions": ["path of the dirname walk"], "references": ["return a list with number of loaded cache lines per memory hierarchy level ."], "bleu": 0.04512859433163675, "rouge_l": 0.09697933227344992}
{"id": 3062, "code": "def get hits ( self ) : return [ self . stats [ cache level ] [ 'HIT count' ] / self . first dim factor for cache level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]", "predictions": ["argparse argparse parser for all parser . ."], "references": ["return a list with number of hit cache lines per memory hierarchy level ."], "bleu": 0.07575149194183216, "rouge_l": 0.08664772727272725}
{"id": 3063, "code": "def get misses ( self ) : return [ self . stats [ cache level ] [ 'MISS count' ] / self . first dim factor for cache level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]", "predictions": ["return arguments for the current asm if any"], "references": ["return a list with number of missed cache lines per memory hierarchy level ."], "bleu": 0.07575149194183216, "rouge_l": 0.08664772727272725}
{"id": 3064, "code": "def get stores ( self ) : return [ self . stats [ cache level ] [ 'STORE count' ] / self . first dim factor for cache level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]", "predictions": ["create a list of stores stores ."], "references": ["return a list with number of stored cache lines per memory hierarchy level ."], "bleu": 0.10218289380194193, "rouge_l": 0.35935198821796754}
{"id": 3065, "code": "def get evicts ( self ) : return [ self . stats [ cache level ] [ 'EVICT count' ] / self . first dim factor for cache level in range ( len ( self . machine [ 'memory hierarchy' ] ) ) ]", "predictions": ["argparse argparse machine . ."], "references": ["return a list with number of evicted cache lines per memory hierarchy level ."], "bleu": 0.04512859433163675, "rouge_l": 0.09697933227344992}
{"id": 3066, "code": "def get infos ( self ) : first dim factor = self . first dim factor infos = { 'memory hierarchy' : [ ] , 'cache stats' : self . stats , 'cachelines in stats' : first dim factor } for cache level , cache info in list ( enumerate ( self . machine [ 'memory hierarchy' ] ) ) : infos [ 'memory hierarchy' ] . append ( { 'index' : len ( infos [ 'memory hierarchy' ] ) , 'level' : '{}' . format ( cache info [ 'level' ] ) , 'total loads' : self . stats [ cache level ] [ 'LOAD byte' ] / first dim factor , 'total misses' : self . stats [ cache level ] [ 'MISS byte' ] / first dim factor , 'total hits' : self . stats [ cache level ] [ 'HIT byte' ] / first dim factor , 'total stores' : self . stats [ cache level ] [ 'STORE byte' ] / first dim factor , 'total evicts' : self . stats [ cache level ] [ 'EVICT byte' ] / first dim factor , 'total lines load' : self . stats [ cache level ] [ 'LOAD count' ] / first dim factor , 'total lines misses' : self . stats [ cache level ] [ 'MISS count' ] / first dim factor , 'total lines hits' : self . stats [ cache level ] [ 'HIT count' ] / first dim factor , 'total lines stores' : self . stats [ cache level ] [ 'STORE count' ] / first dim factor , 'total lines evicts' : self . stats [ cache level ] [ 'EVICT count' ] / first dim factor , 'cycles' : None } ) return infos", "predictions": ["symbol symbol for the first ."], "references": ["return verbose information about the predictor ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 3067, "code": "def measure bw ( type , total size , threads per core , max threads per core , cores per socket , sockets ) : groups = [ ] for s in range ( sockets ) : groups += [ '-w' , 'S' + str ( s ) + ':' + str ( total size ) + 'k B:' + str ( threads per core * cores per socket ) + ':1:' + str ( int ( max threads per core / threads per core ) ) ] cmd = [ 'likwid-bench' , '-t' , type ] + groups sys . stderr . write ( ' ' . join ( cmd ) ) output = subprocess . Popen ( cmd , stdout = subprocess . PIPE ) . communicate ( ) [ 0 ] . decode ( 'utf-8' ) if not output : print ( ' ' . join ( cmd ) + ' returned no output, possibly wrong version installed ' '(requires 4.0 or later)' , file = sys . stderr ) sys . exit ( 1 ) bw = float ( get match or break ( r'^M Byte/s:\\s+([0-9]+(?:\\.[0-9]+)?)\\s*$' , output ) [ 0 ] ) print ( ' ' , Prefixed Unit ( bw , 'MB/s' ) , file = sys . stderr ) return Prefixed Unit ( bw , 'MB/s' )", "predictions": ["find the node of the type in the given type"], "references": ["* size * is given in kilo bytes"], "bleu": 0.13950796967929133, "rouge_l": 0.11338289962825278}
{"id": 3068, "code": "def fix env variable ( name , value ) : orig = os . environ . get ( name , None ) if value is not None : os . environ [ name ] = value elif name in os . environ : del os . environ [ name ] try : yield finally : if orig is not None : os . environ [ name ] = orig elif name in os . environ : del os . environ [ name ]", "predictions": ["add environment variables to context = 0 to 1 = 0 = 1 = 1 = 0 = 1 = 1 = 1 = 1 = 1 = 1 = 1"], "references": ["fix environment variable to a value within context . unset if value is none ."], "bleu": 0.046398855339878003, "rouge_l": 0.1391634980988593}
{"id": 3069, "code": "def configure arggroup ( cls , parser ) : parser . add argument ( '--no-phenoecm' , action = 'store true' , help = 'Disables the phenomenological ECM model building.' ) parser . add argument ( '--iterations' , type = int , default = 10 , help = 'Number of outer-loop iterations (e.g. time loop) during benchmarking. ' 'Default is 10, but actual number will be adapted to at least 0.2s runtime.' ) parser . add argument ( '--ignore-warnings' , action = 'store true' , help = 'Ignore warnings about missmatched CPU model and frequency.' )", "predictions": ["check for the arggroup options v v for the datatypes v v v v v v v v v v v"], "references": ["configure argument parser ."], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 3070, "code": "def report ( self , output file = sys . stdout ) : if self . verbose > 1 : with pprint nosort ( ) : pprint . pprint ( self . results ) if self . verbose > 0 : print ( 'Runtime (per repetition): {:.2g} s' . format ( self . results [ 'Runtime (per repetition) [s]' ] ) , file = output file ) if self . verbose > 0 : print ( 'Iterations per repetition: {!s}' . format ( self . results [ 'Iterations per repetition' ] ) , file = output file ) print ( 'Runtime (per cacheline update): {:.2f} cy/CL' . format ( self . results [ 'Runtime (per cacheline update) [cy/CL]' ] ) , file = output file ) print ( 'MEM volume (per repetition): {:.0f} Byte' . format ( self . results [ 'MEM volume (per repetition) [B]' ] ) , file = output file ) print ( 'Performance: {:.2f} MFLOP/s' . format ( self . results [ 'Performance [MFLOP/s]' ] ) , file = output file ) print ( 'Performance: {:.2f} MLUP/s' . format ( self . results [ 'Performance [MLUP/s]' ] ) , file = output file ) print ( 'Performance: {:.2f} It/s' . format ( self . results [ 'Performance [M It/s]' ] ) , file = output file ) if self . verbose > 0 : print ( 'MEM bandwidth: {:.2f} M Byte/s' . format ( self . results [ 'MEM BW [M Byte/s]' ] ) , file = output file ) print ( '' , file = output file ) if not self . no phenoecm : print ( \"Data Transfers:\" ) print ( \"{:^8} |\" . format ( \"cache\" ) , end = '' ) for metrics in self . results [ 'data transfers' ] . values ( ) : for metric name in sorted ( metrics ) : print ( \" {:^14}\" . format ( metric name ) , end = '' ) print ( ) break for cache , metrics in sorted ( self . results [ 'data transfers' ] . items ( ) ) : print ( \"{!s:^8} |\" . format ( cache ) , end = '' ) for k , v in sorted ( metrics . items ( ) ) : print ( \" {!s:^14}\" . format ( v ) , end = '' ) print ( ) print ( ) print ( 'Phenomenological ECM model: {{ {T OL:.1f} || {T n OL:.1f} | {T L1L2:.1f} | ' '{T L2L3:.1f} | {T L3MEM:.1f} }} cy/CL' . format ( * * { k : float ( v ) for k , v in self . results [ 'ECM' ] . items ( ) } ) , file = output file ) print ( 'T OL assumes that two loads per cycle may be retiered, which is true for ' '128bit SSE/half-AVX loads on SNB and IVY, and 256bit full-AVX loads on HSW, ' 'BDW, SKL and SKX, but it also depends on AGU availability.' , file = output file )", "predictions": ["prints the subs results"], "references": ["report gathered analysis data in human readable form ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 3071, "code": "def build purchase item ( course id , course url , cost in cents , mode , course data , sku ) : item = { 'id' : \"{}-{}\" . format ( course id , mode ) , 'url' : course url , 'price' : cost in cents , 'qty' : 1 , } if 'title' in course data : item [ 'title' ] = course data [ 'title' ] else : item [ 'title' ] = 'Course {} mode: {}' . format ( course id , mode ) if 'tags' in course data : item [ 'tags' ] = course data [ 'tags' ] item [ 'vars' ] = dict ( course data . get ( 'vars' , { } ) , mode = mode , course run id = course id ) item [ 'vars' ] [ 'purchase sku' ] = sku return item", "predictions": ["remove the self accesses from the self sources"], "references": ["build and return sailthru purchase item object"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3072, "code": "def send offer assignment notification email ( config , user email , subject , email body , site code , task ) : try : sailthru client = get sailthru client ( site code ) except Sailthru Error : logger . exception ( '[Offer Assignment] A client error occurred while attempting to send a offer assignment notification.' ' Message: {message}' . format ( message = email body ) ) return None email vars = { 'subject' : subject , 'email body' : email body , } try : response = sailthru client . send ( template = config [ 'templates' ] [ 'assignment email' ] , email = user email , vars = email vars ) except Sailthru Client Error : logger . exception ( '[Offer Assignment] A client error occurred while attempting to send a offer assignment notification.' ' Message: {message}' . format ( message = email body ) ) return None if not response . is ok ( ) : error = response . get error ( ) logger . error ( '[Offer Assignment] A {token error code} - {token error message} error occurred' ' while attempting to send a offer assignment notification.' ' Message: {message}' . format ( message = email body , token error code = error . get error code ( ) , token error message = error . get message ( ) ) ) if can retry sailthru request ( error ) : logger . info ( '[Offer Assignment] An attempt will be made to resend the offer assignment notification.' ' Message: {message}' . format ( message = email body ) ) schedule retry ( task , config ) else : logger . warning ( '[Offer Assignment] No further attempts will be made to send the offer assignment notification.' ' Failed Message: {message}' . format ( message = email body ) ) return response", "predictions": ["send an email notification to the sailthru assignment ."], "references": ["handles sending offer assignment notification emails and retrying failed emails when appropriate ."], "bleu": 0.10761866342063775, "rouge_l": 0.17604617604617603}
{"id": 3073, "code": "def file refs ( self ) : if self . prepared file refs is None : self . prepared file refs = { FILE REFS . idf : File Info ( constructor = lambda path : self . epm cls . from idf ( path , idd or buffer or path = self . idd ) , get path = lambda : get input file path ( self . dir path , FILE REFS . idf ) ) , FILE REFS . epw : File Info ( constructor = lambda path : self . weather data cls . from epw ( path ) , get path = lambda : get input file path ( self . dir path , FILE REFS . epw ) ) , FILE REFS . eio : File Info ( constructor = lambda path : self . eio cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . eio ) ) , FILE REFS . eso : File Info ( constructor = lambda path : self . standard output cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . eso ) ) , FILE REFS . mtr : File Info ( constructor = lambda path : self . standard output cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . mtr ) ) , FILE REFS . mtd : File Info ( constructor = lambda path : self . mtd cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . mtd ) ) , FILE REFS . mdd : File Info ( constructor = lambda path : open ( path ) . read ( ) , get path = lambda : get output file path ( self . dir path , FILE REFS . mdd ) ) , FILE REFS . err : File Info ( constructor = lambda path : self . err cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . err ) ) , FILE REFS . summary table : File Info ( constructor = lambda path : self . summary table cls ( path ) , get path = lambda : get output file path ( self . dir path , FILE REFS . summary table ) ) } return self . prepared file refs", "predictions": ["returns all file refs in the form ."], "references": ["defined here so that we can use the class variables in order to subclass in oplusplus"], "bleu": 0.06528905536667998, "rouge_l": 0.07860824742268041}
{"id": 3074, "code": "def dev populate from json data ( self , json data ) : comment = json data . pop ( \" comment\" , None ) if comment is not None : self . comment = comment external files data = json data . pop ( \" external files\" , dict ( ) ) self . dev external files manager . populate from json data ( external files data ) added records = [ ] for table ref , json data records in json data . items ( ) : table = getattr ( self , table ref ) records = table . dev add inert ( json data records ) added records . extend ( records ) for r in added records : r . dev activate hooks ( ) for r in added records : r . dev activate links ( ) r . dev activate external files ( )", "predictions": ["load external files from a json file ."], "references": ["!! must only be called once when empty !!"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 3075, "code": "def get external files ( self ) : external files = [ ] for table in self . tables . values ( ) : for r in table : external files . extend ( [ ef for ef in r . get external files ( ) ] ) return external files", "predictions": ["return all external files in the external table ."], "references": ["an external file manages file paths ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 3076, "code": "def set defaults ( self ) : for table in self . tables . values ( ) : for r in table : r . set defaults ( )", "predictions": ["set all tables in the table ."], "references": ["all fields of epm with a default value and that are null will be set to their default value ."], "bleu": 0.034485837896937335, "rouge_l": 0.1363128491620112}
{"id": 3077, "code": "def prepare extensible ( self ) : for k in self . tags : if \"extensible\" in k : cycle len = int ( k . split ( \":\" ) [ 1 ] ) break else : return cycle start = None cycle patterns = [ ] for i , field descriptor in enumerate ( self . field descriptors ) : if ( cycle start is not None ) and ( i >= ( cycle start + cycle len ) ) : break if ( cycle start is None ) and ( \"begin-extensible\" in field descriptor . tags ) : cycle start = i if cycle start is None : continue cycle patterns . append ( field descriptor . ref . replace ( \"1\" , r\"(\\d+)\" ) ) else : raise Runtime Error ( \"cycle start not found\" ) self . field descriptors = self . field descriptors [ : cycle start + cycle len ] self . extensible info = ( cycle start , cycle len , tuple ( cycle patterns ) ) for i , fd in enumerate ( self . field descriptors [ cycle start : ] ) : fd . set extensible info ( cycle start , cycle len , cycle patterns [ i ] )", "predictions": ["prepare the extensible info for the extensible ."], "references": ["this function finishes initialization must be called once all field descriptors and tag have been filled ."], "bleu": 0.052063188264041965, "rouge_l": 0.0751231527093596}
{"id": 3078, "code": "def get value ( self , column name or i , filter column name or i , filter criterion ) : column i = self . get column index ( column name or i ) filter column i = self . get column index ( filter column name or i ) filter fct = { float : lambda x : float ( x ) == filter criterion , int : lambda x : int ( x ) == filter criterion , str : lambda x : x . lower ( ) == filter criterion . lower ( ) } [ type ( filter criterion ) ] for row i , row in enumerate ( self . data ) : if filter fct ( row [ filter column i ] ) : break else : raise Value Error ( \"Filter did not return any values.\" ) return self . data [ row i ] [ column i ]", "predictions": ["get the value of the column in the column ."], "references": ["returns first occurrence of value of filter column matching filter criterion ."], "bleu": 0.15433335889254143, "rouge_l": 0.3577712609970674}
{"id": 3079, "code": "def update value inert ( self , index , value ) : field descriptor = self . table . dev descriptor . get field descriptor ( index ) value = field descriptor . deserialize ( value , index ) if isinstance ( value , Link ) : current link = self . data . get ( index ) if current link is not None : current link . unregister ( ) if isinstance ( value , Record Hook ) : current record hook = self . data . get ( index ) if current record hook is not None : current record hook . unregister ( ) if isinstance ( value , External File ) : current external file = self . data . get ( index ) if current external file is not None : current external file . dev unregister ( ) if value in ( None , NONE RECORD HOOK , NONE LINK , NONE EXTERNAL FILE ) : self . dev set none without unregistering ( index , check not required = False ) return old hook = None if index == 0 and not self . table . dev auto pk : old hook = self . data . get ( 0 ) self . data [ index ] = value if old hook is not None : self . table . dev record pk was updated ( old hook . target value )", "predictions": ["update or change the value of a record in the external record"], "references": ["is only called by _update_inert"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 3080, "code": "def set defaults ( self ) : defaults = { } for i in range ( len ( self ) ) : if i in self . data : continue default = self . get field descriptor ( i ) . tags . get ( \"default\" , [ None ] ) [ 0 ] if default is not None : defaults [ i ] = default self . update ( defaults )", "predictions": ["set the default values for all defaults"], "references": ["sets all empty fields for which a default value is defined to default value"], "bleu": 0.0812630644213965, "rouge_l": 0.08983799705449189}
{"id": 3081, "code": "def delete ( self ) : self . unregister links ( ) self . unregister hooks ( ) self . unregister external files ( ) self . get table ( ) . dev remove record without unregistering ( self ) self . table = None self . data = None", "predictions": ["delete the records from the database ."], "references": ["deletes record and removes it from database ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 3082, "code": "def register record hook ( self , hook ) : for key in hook . keys : if key in self . record hooks : field descriptor = hook . target record . get field descriptor ( hook . target index ) raise Field Validation Error ( f\"Reference key already exists, can't create: {key}. \" f\"{field descriptor.get error location message(hook.target value, hook.target index)}\" ) self . record hooks [ key ] = hook", "predictions": ["register a record hook for the given hook ."], "references": ["target record must have been set"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 3083, "code": "def register link ( self , link ) : keys = tuple ( ( ref , link . initial hook value ) for ref in link . hook references ) for k in keys : if k in self . record hooks : link . set target ( target record = self . record hooks [ k ] . target record ) break else : for k in keys : if k in self . table hooks : link . set target ( target table = self . table hooks [ k ] ) break else : field descriptor = link . source record . get field descriptor ( link . source index ) raise Field Validation Error ( f\"No object found with any of given references : {keys}. \" f\"{field descriptor.get error location message(link.initial hook value)}\" ) if link . source record not in self . links by source : self . links by source [ link . source record ] = set ( ) self . links by source [ link . source record ] . add ( link ) if link . target not in self . links by target : self . links by target [ link . target ] = set ( ) self . links by target [ link . target ] . add ( link )", "predictions": ["register a link to the initial hooks"], "references": ["source record and index must have been set"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 3084, "code": "def create regex ( self , line , intent name ) : try : return re . compile ( self . create intent pattern ( line , intent name ) , re . IGNORECASE ) except sre constants . error as e : LOG . warning ( 'Failed to parse the line \"{}\" ' 'for {}' . format ( line , intent name ) ) return None", "predictions": ["create the regex object for the intent ."], "references": ["create regex and return . if error occurs returns none ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 3085, "code": "def remaining duration ( self , time ) : return max ( 0 , self . end - max ( self . start , time ) )", "predictions": ["returns the maximum duration in time between time and time ."], "references": ["returns the remaining duration for a recording ."], "bleu": 0.17033186037639278, "rouge_l": 0.433392539964476}
{"id": 3086, "code": "def http request ( url , post data = None ) : logger . debug ( 'Requesting URL: %s' % url ) buf = bio ( ) curl = pycurl . Curl ( ) curl . setopt ( curl . URL , url . encode ( 'ascii' , 'ignore' ) ) if config ( ) [ 'server' ] [ 'insecure' ] : curl . setopt ( curl . SSL VERIFYPEER , 0 ) curl . setopt ( curl . SSL VERIFYHOST , 0 ) if config ( ) [ 'server' ] [ 'certificate' ] : curl . setopt ( curl . SSL VERIFYPEER , 1 ) curl . setopt ( curl . SSL VERIFYHOST , 2 ) curl . setopt ( pycurl . CAINFO , config ( ) [ 'server' ] [ 'certificate' ] ) if post data : curl . setopt ( curl . HTTPPOST , post data ) curl . setopt ( curl . WRITEFUNCTION , buf . write ) curl . setopt ( pycurl . HTTPAUTH , pycurl . HTTPAUTH DIGEST ) curl . setopt ( pycurl . USERPWD , \"%s:%s\" % ( config ( ) [ 'server' ] [ 'username' ] , config ( ) [ 'server' ] [ 'password' ] ) ) curl . setopt ( curl . HTTPHEADER , [ 'X-Requested-Auth: Digest' ] ) curl . setopt ( curl . FAILONERROR , True ) curl . setopt ( curl . FOLLOWLOCATION , True ) curl . perform ( ) curl . close ( ) result = buf . getvalue ( ) buf . close ( ) return result", "predictions": ["make a request to the api ."], "references": ["make an http request to a given url with optional parameters ."], "bleu": 0.1423172839464222, "rouge_l": 0.40197693574958815}
{"id": 3087, "code": "def try mkdir ( directory ) : try : os . mkdir ( directory ) except OS Error as err : if err . errno != errno . EEXIST : raise err", "predictions": ["create directories if needed ."], "references": ["try to create a directory . pass without error if it already exists ."], "bleu": 0.053667245469253895, "rouge_l": 0.2909379968203497}
{"id": 3088, "code": "def update event status ( event , status ) : dbs = db . get session ( ) dbs . query ( db . Recorded Event ) . filter ( db . Recorded Event . start == event . start ) . update ( { 'status' : status } ) event . status = status dbs . commit ( )", "predictions": ["update event status in the dbs"], "references": ["update the status of a particular event in the database ."], "bleu": 0.15024963364904895, "rouge_l": 0.4468864468864468}
{"id": 3089, "code": "def set service status ( service , status ) : srv = db . Service States ( ) srv . type = service srv . status = status dbs = db . get session ( ) dbs . merge ( srv ) dbs . commit ( ) dbs . close ( )", "predictions": ["set the status of a service ."], "references": ["update the status of a particular service in the database ."], "bleu": 0.31756433708884857, "rouge_l": 0.6409807355516637}
{"id": 3090, "code": "def get service status ( service ) : dbs = db . get session ( ) srvs = dbs . query ( db . Service States ) . filter ( db . Service States . type == service ) if srvs . count ( ) : return srvs [ 0 ] . status return db . Service Status . STOPPED", "predictions": ["get the status of a service ."], "references": ["update the status of a particular service in the database ."], "bleu": 0.31756433708884857, "rouge_l": 0.6409807355516637}
{"id": 3091, "code": "def update agent state ( ) : configure service ( 'capture.admin' ) status = 'idle' if get service status ( db . Service . SCHEDULE ) == db . Service Status . STOPPED : status = 'offline' elif get service status ( db . Service . CAPTURE ) == db . Service Status . BUSY : status = 'capturing' elif get service status ( db . Service . INGEST ) == db . Service Status . BUSY : status = 'uploading' register ca ( status = status )", "predictions": ["update the agent state"], "references": ["update the current agent state in opencast ."], "bleu": 0.21874242445215208, "rouge_l": 0.6288659793814433}
{"id": 3092, "code": "def configuration file ( cfgfile ) : if cfgfile is not None : return cfgfile cfg = './etc/pyca.conf' if not os . path . isfile ( cfg ) : return '/etc/pyca.conf' return cfg", "predictions": ["return the configuration file name of the file ."], "references": ["find the best match for the configuration file ."], "bleu": 0.31239399369202553, "rouge_l": 0.4444444444444444}
{"id": 3093, "code": "def check ( ) : if config ( 'server' ) [ 'insecure' ] : logger . warning ( 'HTTPS CHECKS ARE TURNED OFF. A SECURE CONNECTION IS ' 'NOT GUARANTEED' ) if config ( 'server' ) [ 'certificate' ] : open ( config ( 'server' ) [ 'certificate' ] , 'rb' ) . close ( ) if config ( 'agent' ) [ 'backup mode' ] : logger . info ( 'Agent runs in backup mode. No data will be sent to ' 'Opencast' )", "predictions": ["check if the data file is valid"], "references": ["check configuration for sanity ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 3094, "code": "def logger init ( ) : handlers = [ ] logconf = config ( 'logging' ) if logconf [ 'syslog' ] : handlers . append ( logging . handlers . Sys Log Handler ( address = '/dev/log' ) ) if logconf [ 'stderr' ] : handlers . append ( logging . Stream Handler ( sys . stderr ) ) if logconf [ 'file' ] : handlers . append ( logging . handlers . Watched File Handler ( logconf [ 'file' ] ) ) for handler in handlers : handler . set Formatter ( logging . Formatter ( logconf [ 'format' ] ) ) logging . root . add Handler ( handler ) logging . root . set Level ( logconf [ 'level' ] . upper ( ) ) logger . info ( 'Log level set to %s' % logconf [ 'level' ] )", "predictions": ["initialize the logger ."], "references": ["initialize logger based on configuration"], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 3095, "code": "def home ( ) : preview = config ( ) [ 'capture' ] [ 'preview' ] previewdir = config ( ) [ 'capture' ] [ 'preview dir' ] preview = [ p . replace ( '{{previewdir}}' , previewdir ) for p in preview ] preview = zip ( preview , range ( len ( preview ) ) ) preview = [ p [ 1 ] for p in preview if os . path . isfile ( p [ 0 ] ) ] try : limit upcoming = int ( request . args . get ( 'limit upcoming' , 5 ) ) limit processed = int ( request . args . get ( 'limit processed' , 15 ) ) except Value Error : limit upcoming = 5 limit processed = 15 db = get session ( ) upcoming events = db . query ( Upcoming Event ) . order by ( Upcoming Event . start ) . limit ( limit upcoming ) recorded events = db . query ( Recorded Event ) . order by ( Recorded Event . start . desc ( ) ) . limit ( limit processed ) recording = get service status ( Service . CAPTURE ) == Service Status . BUSY uploading = get service status ( Service . INGEST ) == Service Status . BUSY processed = db . query ( Recorded Event ) . count ( ) upcoming = db . query ( Upcoming Event ) . count ( ) return render template ( 'home.html' , preview = preview , config = config ( ) , recorded events = recorded events , upcoming events = upcoming events , recording = recording , uploading = uploading , processed = processed , upcoming = upcoming , limit upcoming = limit upcoming , limit processed = limit processed , dtfmt = dtfmt )", "predictions": ["show upcoming events ."], "references": ["serve the status page of the capture agent ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 3096, "code": "def serve image ( image id ) : try : preview dir = config ( ) [ 'capture' ] [ 'preview dir' ] filepath = config ( ) [ 'capture' ] [ 'preview' ] [ image id ] filepath = filepath . replace ( '{{previewdir}}' , preview dir ) filepath = os . path . abspath ( filepath ) if os . path . isfile ( filepath ) : directory , filename = filepath . rsplit ( '/' , 1 ) return send from directory ( directory , filename ) except ( Index Error , Key Error ) : pass return '' , 404", "predictions": ["serve an image from the image directory"], "references": ["serve the preview image with the given id"], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 3097, "code": "def sigterm handler ( signum , frame ) : sigint handler ( signum , frame ) for process in multiprocessing . active children ( ) : process . terminate ( ) sys . exit ( 0 )", "predictions": ["terminate all active children of the given frame ."], "references": ["intercept sigterm and terminate all processes ."], "bleu": 0.19960198807747329, "rouge_l": 0.38364779874213834}
{"id": 3098, "code": "def run all ( * modules ) : processes = [ multiprocessing . Process ( target = mod . run ) for mod in modules ] for p in processes : p . start ( ) for p in processes : p . join ( )", "predictions": ["run all processes in the given modules ."], "references": ["start all services ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 3099, "code": "def parse ical ( vcal ) : vcal = vcal . replace ( '\\r\\n ' , '' ) . replace ( '\\r\\n\\r\\n' , '\\r\\n' ) vevents = vcal . split ( '\\r\\n BEGIN:VEVENT\\r\\n' ) del ( vevents [ 0 ] ) events = [ ] for vevent in vevents : event = { } for line in vevent . split ( '\\r\\n' ) : line = line . split ( ':' , 1 ) key = line [ 0 ] . lower ( ) if len ( line ) <= 1 or key == 'end' : continue if key . startswith ( 'dt' ) : event [ key ] = unix ts ( dateutil . parser . parse ( line [ 1 ] ) ) continue if not key . startswith ( 'attach' ) : event [ key ] = line [ 1 ] continue event [ 'attach' ] = event . get ( 'attach' , [ ] ) attachment = { } for x in [ x . split ( '=' ) for x in line [ 0 ] . split ( ';' ) ] : if x [ 0 ] . lower ( ) in [ 'fmttype' , 'x-apple-filename' ] : attachment [ x [ 0 ] . lower ( ) ] = x [ 1 ] attachment [ 'data' ] = b64decode ( line [ 1 ] ) . decode ( 'utf-8' ) event [ 'attach' ] . append ( attachment ) events . append ( event ) return events", "predictions": ["parse the ical event ."], "references": ["parse opencast schedule icalendar file and return events as dict"], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 3100, "code": "def control loop ( ) : set service status ( Service . SCHEDULE , Service Status . BUSY ) notify . notify ( 'READY=1' ) while not terminate ( ) : notify . notify ( 'WATCHDOG=1' ) get schedule ( ) session = get session ( ) next event = session . query ( Upcoming Event ) . filter ( Upcoming Event . end > timestamp ( ) ) . order by ( Upcoming Event . start ) . first ( ) if next event : logger . info ( 'Next scheduled recording: %s' , datetime . fromtimestamp ( next event . start ) ) notify . notify ( 'STATUS=Next scheduled recording: %s' % datetime . fromtimestamp ( next event . start ) ) else : logger . info ( 'No scheduled recording' ) notify . notify ( 'STATUS=No scheduled recording' ) session . close ( ) next update = timestamp ( ) + config ( ) [ 'agent' ] [ 'update frequency' ] while not terminate ( ) and timestamp ( ) < next update : time . sleep ( 0.1 ) logger . info ( 'Shutting down schedule service' ) set service status ( Service . SCHEDULE , Service Status . STOPPED )", "predictions": ["the main loop ."], "references": ["main loop retrieving the schedule ."], "bleu": 0.3258798048281462, "rouge_l": 0.5791139240506329}
{"id": 3101, "code": "def control loop ( ) : set service status ( Service . AGENTSTATE , Service Status . BUSY ) notify . notify ( 'READY=1' ) notify . notify ( 'STATUS=Running' ) while not terminate ( ) : notify . notify ( 'WATCHDOG=1' ) update agent state ( ) next update = timestamp ( ) + config ( ) [ 'agent' ] [ 'update frequency' ] while not terminate ( ) and timestamp ( ) < next update : time . sleep ( 0.1 ) logger . info ( 'Shutting down agentstate service' ) set service status ( Service . AGENTSTATE , Service Status . STOPPED )", "predictions": ["the main loop ."], "references": ["main loop updating the capture agent state ."], "bleu": 0.19765609300943976, "rouge_l": 0.47164948453608246}
{"id": 3102, "code": "def make error response ( error , status = 500 ) : content = { 'errors' : [ { 'status' : status , 'title' : error } ] } return make response ( jsonify ( content ) , status )", "predictions": ["make an error response"], "references": ["return a response with a jsonapi error object"], "bleu": 0.14628187563941414, "rouge_l": 0.15721649484536082}
{"id": 3103, "code": "def make data response ( data , status = 200 ) : content = { 'data' : ensurelist ( data ) } return make response ( jsonify ( content ) , status )", "predictions": ["make a response from data ."], "references": ["return a response with a list of jsonapi data objects"], "bleu": 0.16038842424444547, "rouge_l": 0.3588235294117647}
{"id": 3104, "code": "def internal state ( ) : data = { 'services' : { 'capture' : Service Status . str ( get service status ( Service . CAPTURE ) ) , 'ingest' : Service Status . str ( get service status ( Service . INGEST ) ) , 'schedule' : Service Status . str ( get service status ( Service . SCHEDULE ) ) , 'agentstate' : Service Status . str ( get service status ( Service . AGENTSTATE ) ) } } return make response ( jsonify ( { 'meta' : data } ) )", "predictions": ["returns the current offer offer"], "references": ["serve a json representation of internal agentstate as meta data"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 3105, "code": "def events ( ) : db = get session ( ) upcoming events = db . query ( Upcoming Event ) . order by ( Upcoming Event . start ) recorded events = db . query ( Recorded Event ) . order by ( Recorded Event . start . desc ( ) ) result = [ event . serialize ( ) for event in upcoming events ] result += [ event . serialize ( ) for event in recorded events ] return make data response ( result )", "predictions": ["list all prepared file file file file file file file file file file file file file file file file file file file file file file file file file file file file"], "references": ["serve a json representation of events"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3106, "code": "def event ( uid ) : db = get session ( ) event = db . query ( Recorded Event ) . filter ( Recorded Event . uid == uid ) . first ( ) or db . query ( Upcoming Event ) . filter ( Upcoming Event . uid == uid ) . first ( ) if event : return make data response ( event . serialize ( ) ) return make error response ( 'No event with specified uid' , 404 )", "predictions": ["handle an dev dev dev dev dev dev dev dev ."], "references": ["return a specific events json"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 3107, "code": "def ingest ( event ) : set service status ( Service . INGEST , Service Status . BUSY ) notify . notify ( 'STATUS=Uploading' ) recording state ( event . uid , 'uploading' ) update event status ( event , Status . UPLOADING ) service = config ( 'service-ingest' ) service = service [ randrange ( 0 , len ( service ) ) ] logger . info ( 'Selecting ingest service to use: ' + service ) logger . info ( 'Creating new mediapackage' ) mediapackage = http request ( service + '/create Media Package' ) prop = 'org.opencastproject.capture.agent.properties' dcns = 'http://www.opencastproject.org/xsd/1.0/dublincore/' for attachment in event . get data ( ) . get ( 'attach' ) : data = attachment . get ( 'data' ) if attachment . get ( 'x-apple-filename' ) == prop : workflow def , workflow config = get config params ( data ) elif attachment . get ( 'fmttype' ) == 'application/xml' and dcns in data : name = attachment . get ( 'x-apple-filename' , '' ) . rsplit ( '.' , 1 ) [ 0 ] logger . info ( 'Adding %s DC catalog' % name ) fields = [ ( 'media Package' , mediapackage ) , ( 'flavor' , 'dublincore/%s' % name ) , ( 'dublin Core' , data . encode ( 'utf-8' ) ) ] mediapackage = http request ( service + '/add DC Catalog' , fields ) for ( flavor , track ) in event . get tracks ( ) : logger . info ( 'Adding track ({0} -> {1})' . format ( flavor , track ) ) track = track . encode ( 'ascii' , 'ignore' ) fields = [ ( 'media Package' , mediapackage ) , ( 'flavor' , flavor ) , ( 'BODY1' , ( pycurl . FORM FILE , track ) ) ] mediapackage = http request ( service + '/add Track' , fields ) logger . info ( 'Ingest recording' ) fields = [ ( 'media Package' , mediapackage ) ] if workflow def : fields . append ( ( 'workflow Definition Id' , workflow def ) ) if event . uid : fields . append ( ( 'workflow Instance Id' , event . uid . encode ( 'ascii' , 'ignore' ) ) ) fields += workflow config mediapackage = http request ( service + '/ingest' , fields ) recording state ( event . uid , 'upload finished' ) update event status ( event , Status . FINISHED UPLOADING ) notify . notify ( 'STATUS=Running' ) set service status immediate ( Service . INGEST , Service Status . IDLE ) logger . info ( 'Finished ingest' )", "predictions": ["get the = = available service"], "references": ["ingest a finished recording to the opencast server ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 3108, "code": "def sigterm handler ( signum , frame ) : if captureproc and captureproc . poll ( ) is None : captureproc . terminate ( ) terminate ( True ) sys . exit ( 0 )", "predictions": ["terminate a daemon values values values values values values values values values values values values values values values values values values values values values values values values values values values values"], "references": ["intercept sigterm and terminate all processes ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 3109, "code": "def render to fragment ( self , request , * * kwargs ) : fragment = Fragment ( TEST HTML ) fragment . add javascript ( TEST JS ) fragment . add css ( TEST CSS ) return fragment", "predictions": ["prepare the fragment instance for rendering ."], "references": ["returns a simple fragment"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 3110, "code": "def resources ( self ) : seen = set ( ) return [ x for x in self . resources if x not in seen and not seen . add ( x ) ]", "predictions": ["i have the get method = order = 0 = 1 = 0 = 1 = 0 = 1 = 0 = 1 = 0 = 1 = 1 = 1"], "references": ["returns list of unique fragmentresource s by order of first appearance ."], "bleu": 0.03901663112717908, "rouge_l": 0.05053852526926264}
{"id": 3111, "code": "def to dict ( self ) : return { 'content' : self . content , 'resources' : [ r . asdict ( ) for r in self . resources ] , 'js init fn' : self . js init fn , 'js init version' : self . js init version , 'json init args' : self . json init args }", "predictions": ["index of the is only available in the is serialized to a dictionary ."], "references": ["returns the fragment in a dictionary representation ."], "bleu": 0.13834368456410945, "rouge_l": 0.47805642633228845}
{"id": 3112, "code": "def from dict ( cls , pods ) : frag = cls ( ) frag . content = pods [ 'content' ] frag . resources = [ Fragment Resource ( * * d ) for d in pods [ 'resources' ] ] frag . js init fn = pods [ 'js init fn' ] frag . js init version = pods [ 'js init version' ] frag . json init args = pods [ 'json init args' ] return frag", "predictions": ["create a new instance from a defaults ."], "references": ["returns a new fragment from a dictionary representation ."], "bleu": 0.2451240194075422, "rouge_l": 0.5820610687022901}
{"id": 3113, "code": "def resource to html ( resource ) : if resource . mimetype == \"text/css\" : if resource . kind == \"text\" : return u\"<style type='text/css'>\\n%s\\n</style>\" % resource . data elif resource . kind == \"url\" : return u\"<link rel='stylesheet' href='%s' type='text/css'>\" % resource . data else : raise Exception ( \"Unrecognized resource kind %r\" % resource . kind ) elif resource . mimetype == \"application/javascript\" : if resource . kind == \"text\" : return u\"<script>\\n%s\\n</script>\" % resource . data elif resource . kind == \"url\" : return u\"<script src='%s' type='application/javascript'></script>\" % resource . data else : raise Exception ( \"Unrecognized resource kind %r\" % resource . kind ) elif resource . mimetype == \"text/html\" : assert resource . kind == \"text\" return resource . data else : raise Exception ( \"Unrecognized mimetype %r\" % resource . mimetype )", "predictions": ["dev delete delete delete delete delete"], "references": ["returns resource wrapped in the appropriate html tag for it s mimetype ."], "bleu": 0.0578433294533084, "rouge_l": 0.0}
{"id": 3114, "code": "def get ( self , request , * args , * * kwargs ) : fragment = self . render to fragment ( request , * * kwargs ) response format = request . GET . get ( 'format' ) or request . POST . get ( 'format' ) or 'html' if response format == 'json' or WEB FRAGMENT RESPONSE TYPE in request . META . get ( 'HTTP ACCEPT' , 'text/html' ) : return Json Response ( fragment . to dict ( ) ) else : return self . render standalone response ( request , fragment , * * kwargs )", "predictions": ["if the in - memory json representation of the in - memory field"], "references": ["render a fragment to html or return json describing it based on the request ."], "bleu": 0.09063677582644454, "rouge_l": 0.14104046242774565}
{"id": 3115, "code": "def render standalone response ( self , request , fragment , * * kwargs ) : if fragment is None : return Http Response ( status = 204 ) html = self . render to standalone html ( request , fragment , * * kwargs ) return Http Response ( html )", "predictions": ["returns serialized hooks for rendering requests ."], "references": ["renders a standalone page as a response for the specified fragment ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 3116, "code": "def render to standalone html ( self , request , fragment , * * kwargs ) : template = get template ( STANDALONE TEMPLATE NAME ) context = { 'head html' : fragment . head html ( ) , 'body html' : fragment . body html ( ) , 'foot html' : fragment . foot html ( ) , } return template . render ( context )", "predictions": ["create self . from standalone . . . . . . . . ."], "references": ["render the specified fragment to html for a standalone page ."], "bleu": 0.09782375748961449, "rouge_l": 0.16353887399463804}
{"id": 3117, "code": "def calc ( pvalues , lamb ) : m = len ( pvalues ) pi0 = ( pvalues > lamb ) . sum ( ) / ( ( 1 - lamb ) * m ) p FDR = np . ones ( m ) print ( \"p FDR    y        Pr     fast Pow\" ) for i in range ( m ) : y = pvalues [ i ] Pr = max ( 1 , m - i ) / float ( m ) p FDR [ i ] = ( pi0 * y ) / ( Pr * ( 1 - math . pow ( 1 - y , m ) ) ) print ( i , p FDR [ i ] , y , Pr , 1.0 - math . pow ( 1 - y , m ) ) num null = pi0 * m num alt = m - num null num negs = np . array ( range ( m ) ) num pos = m - num negs pp = num pos / float ( m ) qvalues = np . ones ( m ) qvalues [ 0 ] = p FDR [ 0 ] for i in range ( m - 1 ) : qvalues [ i + 1 ] = min ( qvalues [ i ] , p FDR [ i + 1 ] ) sens = ( ( 1.0 - qvalues ) * num pos ) / num alt sens [ sens > 1.0 ] = 1.0 df = pd . Data Frame ( dict ( pvalue = pvalues , qvalue = qvalues , FDR = p FDR , percentile positive = pp , sens = sens ) ) df [ \"svalue\" ] = df . sens [ : : - 1 ] . cummax ( ) [ : : - 1 ] return df , num null , m", "predictions": ["calculate the pvalues of the data"], "references": ["meaning pvalues presorted i descending order"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 3118, "code": "def to one dim array ( values , as type = None ) : if isinstance ( values , ( list , tuple ) ) : values = np . array ( values , dtype = np . float32 ) elif isinstance ( values , pd . Series ) : values = values . values values = values . flatten ( ) assert values . ndim == 1 , \"values has wrong dimension\" if as type is not None : return values . astype ( as type ) return values", "predictions": ["convert post post post post post to request buf ."], "references": ["converts list or flattens n - dim array to 1 - dim array if possible"], "bleu": 0.07645906143263256, "rouge_l": 0.07721518987341772}
{"id": 3119, "code": "def lookup values from error table ( scores , err df ) : ix = find nearest matches ( np . float32 ( err df . cutoff . values ) , np . float32 ( scores ) ) return err df . pvalue . iloc [ ix ] . values , err df . svalue . iloc [ ix ] . values , err df . pep . iloc [ ix ] . values , err df . qvalue . iloc [ ix ] . values", "predictions": ["try to find mkdir mkdir mkdir mkdir mkdir mkdir"], "references": ["find matching q - value for each score in scores"], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 3120, "code": "def summary err table ( df , qvalues = [ 0 , 0.01 , 0.02 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ] ) : qvalues = to one dim array ( qvalues ) ix = find nearest matches ( np . float32 ( df . qvalue . values ) , qvalues ) df sub = df . iloc [ ix ] . copy ( ) for i sub , ( i0 , i1 ) in enumerate ( zip ( ix , ix [ 1 : ] ) ) : if i1 == i0 : df sub . iloc [ i sub + 1 , : ] = None df sub . qvalue = qvalues df sub . reset index ( inplace = True , drop = True ) return df sub [ [ 'qvalue' , 'pvalue' , 'svalue' , 'pep' , 'fdr' , 'fnr' , 'fpr' , 'tp' , 'tn' , 'fp' , 'fn' , 'cutoff' ] ]", "predictions": ["remove the update status of a update status status"], "references": ["summary error table for some typical q - values"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3121, "code": "def error statistics ( target scores , decoy scores , parametric , pfdr , pi0 lambda , pi0 method = \"smoother\" , pi0 smooth df = 3 , pi0 smooth log pi0 = False , compute lfdr = False , lfdr trunc = True , lfdr monotone = True , lfdr transf = \"probit\" , lfdr adj = 1.5 , lfdr eps = np . power ( 10.0 , - 8 ) ) : target scores = to one dim array ( target scores ) target scores = np . sort ( target scores [ ~ np . isnan ( target scores ) ] ) decoy scores = to one dim array ( decoy scores ) decoy scores = np . sort ( decoy scores [ ~ np . isnan ( decoy scores ) ] ) if parametric : target pvalues = pnorm ( target scores , decoy scores ) else : target pvalues = pemp ( target scores , decoy scores ) pi0 = pi0est ( target pvalues , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 ) target qvalues = qvalue ( target pvalues , pi0 [ 'pi0' ] , pfdr ) metrics = stat metrics ( target pvalues , pi0 [ 'pi0' ] , pfdr ) error stat = pd . Data Frame ( { 'cutoff' : target scores , 'pvalue' : target pvalues , 'qvalue' : target qvalues , 'svalue' : metrics [ 'svalue' ] , 'tp' : metrics [ 'tp' ] , 'fp' : metrics [ 'fp' ] , 'tn' : metrics [ 'tn' ] , 'fn' : metrics [ 'fn' ] , 'fpr' : metrics [ 'fpr' ] , 'fdr' : metrics [ 'fdr' ] , 'fnr' : metrics [ 'fnr' ] } ) if compute lfdr : error stat [ 'pep' ] = lfdr ( target pvalues , pi0 [ 'pi0' ] , lfdr trunc , lfdr monotone , lfdr transf , lfdr adj , lfdr eps ) return error stat , pi0", "predictions": ["calculate the set of set of set - service service service scores"], "references": ["takes list of decoy and target scores and creates error statistics for target values"], "bleu": 0.09733489823443878, "rouge_l": 0.1517412935323383}
{"id": 3122, "code": "def find cutoff ( tt scores , td scores , cutoff fdr , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 ) : error stat , pi0 = error statistics ( tt scores , td scores , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , False ) if not len ( error stat ) : raise click . Click Exception ( \"Too little data for calculating error statistcs.\" ) i0 = ( error stat . qvalue - cutoff fdr ) . abs ( ) . idxmin ( ) cutoff = error stat . iloc [ i0 ] [ \"cutoff\" ] return cutoff", "predictions": ["get the service service service service service scores"], "references": ["finds cut off target score for specified false discovery rate fdr"], "bleu": 0.09268172804333874, "rouge_l": 0.0}
{"id": 3123, "code": "def score ( infile , outfile , classifier , xgb autotune , apply weights , xeval fraction , xeval num iter , ss initial fdr , ss iteration fdr , ss num iter , ss main score , group id , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps , level , ipf max peakgroup rank , ipf max peakgroup pep , ipf max transition isotope overlap , ipf min transition sn , tric chromprob , threads , test ) : if outfile is None : outfile = infile else : outfile = outfile xgb hyperparams = { 'autotune' : xgb autotune , 'autotune num rounds' : 10 , 'num boost round' : 100 , 'early stopping rounds' : 10 , 'test size' : 0.33 } xgb params = { 'eta' : 0.3 , 'gamma' : 0 , 'max depth' : 6 , 'min child weight' : 1 , 'subsample' : 1 , 'colsample bytree' : 1 , 'colsample bylevel' : 1 , 'colsample bynode' : 1 , 'lambda' : 1 , 'alpha' : 0 , 'scale pos weight' : 1 , 'silent' : 1 , 'objective' : 'binary:logitraw' , 'nthread' : 1 , 'eval metric' : 'auc' } xgb params space = { 'eta' : hp . uniform ( 'eta' , 0.0 , 0.3 ) , 'gamma' : hp . uniform ( 'gamma' , 0.0 , 0.5 ) , 'max depth' : hp . quniform ( 'max depth' , 2 , 8 , 1 ) , 'min child weight' : hp . quniform ( 'min child weight' , 1 , 5 , 1 ) , 'subsample' : 1 , 'colsample bytree' : 1 , 'colsample bylevel' : 1 , 'colsample bynode' : 1 , 'lambda' : hp . uniform ( 'lambda' , 0.0 , 1.0 ) , 'alpha' : hp . uniform ( 'alpha' , 0.0 , 1.0 ) , 'scale pos weight' : 1.0 , 'silent' : 1 , 'objective' : 'binary:logitraw' , 'nthread' : 1 , 'eval metric' : 'auc' } if not apply weights : Py Prophet Learner ( infile , outfile , classifier , xgb hyperparams , xgb params , xgb params space , xeval fraction , xeval num iter , ss initial fdr , ss iteration fdr , ss num iter , ss main score , group id , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps , level , ipf max peakgroup rank , ipf max peakgroup pep , ipf max transition isotope overlap , ipf min transition sn , tric chromprob , threads , test ) . run ( ) else : Py Prophet Weight Applier ( infile , outfile , classifier , xgb hyperparams , xgb params , xgb params space , xeval fraction , xeval num iter , ss initial fdr , ss iteration fdr , ss num iter , ss main score , group id , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps , level , ipf max peakgroup rank , ipf max peakgroup pep , ipf max transition isotope overlap , ipf min transition sn , tric chromprob , threads , test , apply weights ) . run ( )", "predictions": ["update the update transition transition with the specified classifier"], "references": ["conduct semi - supervised learning and error - rate estimation for ms1 ms2 and transition - level data ."], "bleu": 0.046462271735933564, "rouge_l": 0.0671067106710671}
{"id": 3124, "code": "def ipf ( infile , outfile , ipf ms1 scoring , ipf ms2 scoring , ipf h0 , ipf grouped fdr , ipf max precursor pep , ipf max peakgroup pep , ipf max precursor peakgroup pep , ipf max transition pep ) : if outfile is None : outfile = infile else : outfile = outfile infer peptidoforms ( infile , outfile , ipf ms1 scoring , ipf ms2 scoring , ipf h0 , ipf grouped fdr , ipf max precursor pep , ipf max peakgroup pep , ipf max precursor peakgroup pep , ipf max transition pep )", "predictions": ["write a . . . . . gz file"], "references": ["infer peptidoforms after scoring of ms1 ms2 and transition - level data ."], "bleu": 0.09049614828481034, "rouge_l": 0.08802308802308802}
{"id": 3125, "code": "def peptide ( infile , outfile , context , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps ) : if outfile is None : outfile = infile else : outfile = outfile infer peptides ( infile , outfile , context , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps )", "predictions": ["write the data to the check"], "references": ["infer peptides and conduct error - rate estimation in different contexts ."], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 3126, "code": "def protein ( infile , outfile , context , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps ) : if outfile is None : outfile = infile else : outfile = outfile infer proteins ( infile , outfile , context , parametric , pfdr , pi0 lambda , pi0 method , pi0 smooth df , pi0 smooth log pi0 , lfdr truncate , lfdr monotone , lfdr transformation , lfdr adj , lfdr eps )", "predictions": ["write a logger to a logger"], "references": ["infer proteins and conduct error - rate estimation in different contexts ."], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 3127, "code": "def subsample ( infile , outfile , subsample ratio , test ) : if outfile is None : outfile = infile else : outfile = outfile subsample osw ( infile , outfile , subsample ratio , test )", "predictions": ["write a file or file descriptor to a file - like object ."], "references": ["subsample openswath file to minimum for integrated scoring"], "bleu": 0.10571070857151538, "rouge_l": 0.19902120717781402}
{"id": 3128, "code": "def reduce ( infile , outfile ) : if outfile is None : outfile = infile else : outfile = outfile reduce osw ( infile , outfile )", "predictions": ["serve a file or file - like object ."], "references": ["reduce scored pyprophet file to minimum for global scoring"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 3129, "code": "def backpropagate ( infile , outfile , apply scores ) : if outfile is None : outfile = infile else : outfile = outfile backpropagate oswr ( infile , outfile , apply scores )", "predictions": ["frame the given file into a dict ."], "references": ["backpropagate multi - run peptide and protein scores to single files"], "bleu": 0.09268172804333874, "rouge_l": 0.0}
{"id": 3130, "code": "def create group ( self , group ) : self . valid group id ( group . id ) body = { \"data\" : group . json data ( ) } url = \"{}/group/{}\" . format ( self . API , group . name ) data = self . put resource ( url , headers = { } , body = body ) return self . group from json ( data . get ( \"data\" ) )", "predictions": ["creates a new all of a all children ."], "references": ["creates a group from the passed restclients . group object ."], "bleu": 0.15982877755018768, "rouge_l": 0.2946859903381642}
{"id": 3131, "code": "def delete group ( self , group id ) : self . valid group id ( group id ) url = \"{}/group/{}\" . format ( self . API , group id ) self . delete resource ( url ) return True", "predictions": ["parse a ical ical ical replace with the given ical replace it with the given id replace it ."], "references": ["deletes the group identified by the passed group id ."], "bleu": 0.08097785064266201, "rouge_l": 0.29221556886227545}
{"id": 3132, "code": "def is effective member ( self , group id , netid ) : self . valid group id ( group id ) netid = re . sub ( '@washington.edu' , '' , netid ) url = \"{}/group/{}/effective member/{}\" . format ( self . API , group id , netid ) try : data = self . get resource ( url ) return True except Data Failure Exception as ex : if ex . status == 404 : return False else : raise", "predictions": ["control if a service control member is loop"], "references": ["returns true if the netid is in the group false otherwise ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 3133, "code": "def parse version ( ) : from os . path import dirname , join import ast modname = setupkw [ 'name' ] init fpath = join ( dirname ( file ) , modname , ' init .py' ) with open ( init fpath ) as file : sourcecode = file . read ( ) pt = ast . parse ( sourcecode ) class Version Visitor ( ast . Node Visitor ) : def visit Assign ( self , node ) : for target in node . targets : if target . id == ' version ' : self . version = node . value . s visitor = Version Visitor ( ) visitor . visit ( pt ) return visitor . version", "predictions": ["control the visitor loop status status"], "references": ["statically parse the version number from __init__ . py"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 3134, "code": "def create container ( context , path , l mtime , size ) : new context = context . copy ( ) new context . input = None new context . headers = None new context . query = None container = path . split ( '/' , 1 ) [ 0 ] + ' segments' cli put container ( new context , container ) prefix = container + '/' + path . split ( '/' , 1 ) [ 1 ] prefix = '%s/%s/%s/' % ( prefix , l mtime , size ) return prefix", "predictions": ["make a error error"], "references": ["creates container for segments of file with path"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 3135, "code": "def is empty ( self ) : something = self . read ( 1 ) if something : if self . buf : self . buf = something + self . buf else : self . buf = something return False else : return True", "predictions": ["return true if the lock is data data ."], "references": ["check whether the file is empty reading the single byte ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 3136, "code": "def forwards ( self , orm ) : for title in orm [ 'hero slider.Slider Item Title' ] . objects . all ( ) : title . is published = True title . save ( )", "predictions": ["write your forwards methods here ."], "references": ["write your forwards methods here ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 3137, "code": "def get slider items ( context , amount = None ) : req = context . get ( 'request' ) qs = Slider Item . objects . published ( req ) . order by ( 'position' ) if amount : qs = qs [ : amount ] return qs", "predictions": ["return all slider items in the database ."], "references": ["returns the published slider items ."], "bleu": 0.239802967618271, "rouge_l": 0.43990384615384615}
{"id": 3138, "code": "def render hero slider ( context ) : req = context . get ( 'request' ) qs = Slider Item . objects . published ( req ) . order by ( 'position' ) return { 'slider items' : qs , }", "predictions": ["render the hero slider slider ."], "references": ["renders the hero slider ."], "bleu": 0.46713797772820015, "rouge_l": 0.7393939393939394}
{"id": 3139, "code": "def reader acquire ( self ) : self . order mutex . acquire ( ) self . readers mutex . acquire ( ) if self . readers == 0 : self . access mutex . acquire ( ) self . readers += 1 self . order mutex . release ( ) self . readers mutex . release ( )", "predictions": ["acquire the reader ."], "references": ["acquire the lock to read"], "bleu": 0.36827215283744186, "rouge_l": 0.43571428571428567}
{"id": 3140, "code": "def reader release ( self ) : self . readers mutex . acquire ( ) self . readers -= 1 if self . readers == 0 : self . access mutex . release ( ) self . readers mutex . release ( )", "predictions": ["release the readers ."], "references": ["release the lock after reading"], "bleu": 0.36827215283744186, "rouge_l": 0.43571428571428567}
{"id": 3141, "code": "def writer acquire ( self ) : self . order mutex . acquire ( ) self . access mutex . acquire ( ) self . order mutex . release ( )", "predictions": ["acquire the writer ."], "references": ["acquire the lock to write"], "bleu": 0.36827215283744186, "rouge_l": 0.43571428571428567}
{"id": 3142, "code": "def tasks ( self ) : self . rwlock . reader acquire ( ) tl = [ v for v in self . tasks . values ( ) ] tl . sort ( key = lambda x : x . task id ) self . rwlock . reader release ( ) return tl", "predictions": ["return all tasks in the task ."], "references": ["get the list of tasks"], "bleu": 0.20556680845025982, "rouge_l": 0.17183098591549298}
{"id": 3143, "code": "def to dict ( self ) : properties = find class properties ( self . class ) config = { name : self . getattribute ( name ) for name , in properties } return config", "predictions": ["save this target config object into a dictionary ."], "references": ["returns a dict with the representation of this task configuration object ."], "bleu": 0.12716571564598603, "rouge_l": 0.2785388127853881}
{"id": 3144, "code": "def create index ( idx url , clean = False ) : try : r = requests . get ( idx url ) except requests . exceptions . Connection Error : cause = \"Error connecting to Elastic Search (index: %s)\" % idx url raise Elastic Search Error ( cause = cause ) if r . status code != 200 : r = requests . put ( idx url ) if r . status code != 200 : logger . info ( \"Can't create index %s (%s)\" , idx url , r . status code ) cause = \"Error creating Elastic Search index %s\" % idx url raise Elastic Search Error ( cause = cause ) logger . info ( \"Index %s created\" , idx url ) return True elif r . status code == 200 and clean : requests . delete ( idx url ) requests . put ( idx url ) logger . info ( \"Index deleted and created (index: %s)\" , idx url ) return True return False", "predictions": ["create a index index"], "references": ["configure the index to work with"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 3145, "code": "def json encoder ( * args , * * kwargs ) : obj = cherrypy . serving . request . json inner handler ( * args , * * kwargs ) for chunk in JSON Encoder ( ) . iterencode ( obj ) : yield chunk . encode ( 'utf-8' )", "predictions": ["encode a json stream into a json encoded string ."], "references": ["custom json encoder handler"], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 3146, "code": "def items ( self ) : pipe = self . conn . pipeline ( ) pipe . lrange ( Q STORAGE ITEMS , 0 , - 1 ) pipe . ltrim ( Q STORAGE ITEMS , 1 , 0 ) items = pipe . execute ( ) [ 0 ] for item in items : item = pickle . loads ( item ) yield item", "predictions": ["return the items from the pipeline"], "references": ["get the items fetched by the jobs ."], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 3147, "code": "def validate args ( task id , backend , category , backend args ) : if not task id or task id . strip ( ) == \"\" : msg = \"Missing task id for task\" raise Value Error ( msg ) if not backend or backend . strip ( ) == \"\" : msg = \"Missing backend for task '%s'\" % task id raise Value Error ( msg ) if backend args and not isinstance ( backend args , dict ) : msg = \"Backend args is not a dict, task '%s'\" % task id raise Value Error ( msg ) if not category or category . strip ( ) == \"\" : msg = \"Missing category for task '%s'\" % task id raise Value Error ( msg )", "predictions": ["validate that the task id is a valid backend ."], "references": ["check that the task arguments received are valid"], "bleu": 0.24808415001701817, "rouge_l": 0.4535315985130111}
{"id": 3148, "code": "def parse archive args ( self , archive args ) : if not archive args : return None archiving args = copy . deepcopy ( archive args ) if self . archive path : archiving args [ 'archive path' ] = self . archive path else : archiving args [ 'archive path' ] = os . path . expanduser ( ARCHIVES DEFAULT PATH ) return Archiving Task Config . from dict ( archiving args )", "predictions": ["parse command line arguments ."], "references": ["parse the archive arguments of a task"], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 3149, "code": "def schedule job task ( self , queue id , task id , job args , delay = 0 ) : self . rwlock . writer acquire ( ) job id = self . generate job id ( task id ) event = self . scheduler . enter ( delay , 1 , self . enqueue job , argument = ( queue id , job id , job args , ) ) self . jobs [ job id ] = event self . tasks [ task id ] = job id self . rwlock . writer release ( ) logging . debug ( \"Job #%s (task: %s) scheduled on %s (wait: %s)\" , job id , task id , queue id , delay ) return job id", "predictions": ["schedule a job in the queue ."], "references": ["schedule a job in the given queue ."], "bleu": 0.6633400689531749, "rouge_l": 0.9222462203023757}
{"id": 3150, "code": "def cancel job task ( self , task id ) : try : self . rwlock . writer acquire ( ) job id = self . tasks . get ( task id , None ) if job id : self . cancel job ( job id ) else : logger . warning ( \"Task %s set to be removed was not found\" , task id ) finally : self . rwlock . writer release ( )", "predictions": ["cancel a job task ."], "references": ["cancel the job related to the given task ."], "bleu": 0.18343778145675418, "rouge_l": 0.5434298440979956}
{"id": 3151, "code": "def run ( self ) : try : self . listen ( ) except Exception as e : logger . critical ( \"Job Listener instence crashed. Error: %s\" , str ( e ) ) logger . critical ( traceback . format exc ( ) )", "predictions": ["run the main thread ."], "references": ["run thread to listen for jobs and reschedule successful ones ."], "bleu": 0.09778809693469985, "rouge_l": 0.35124760076775424}
{"id": 3152, "code": "def listen ( self ) : pubsub = self . conn . pubsub ( ) pubsub . subscribe ( self . pubsub channel ) logger . debug ( \"Listening on channel %s\" , self . pubsub channel ) for msg in pubsub . listen ( ) : logger . debug ( \"New message received of type %s\" , str ( msg [ 'type' ] ) ) if msg [ 'type' ] != 'message' : logger . debug ( \"Ignoring job message\" ) continue data = pickle . loads ( msg [ 'data' ] ) job id = data [ 'job id' ] job = rq . job . Job . fetch ( job id , connection = self . conn ) if data [ 'status' ] == 'finished' : logging . debug ( \"Job #%s completed\" , job id ) handler = self . result handler elif data [ 'status' ] == 'failed' : logging . debug ( \"Job #%s failed\" , job id ) handler = self . result handler err else : continue if handler : logging . debug ( \"Calling handler for job #%s\" , job id ) handler ( job )", "predictions": ["listen for messages ."], "references": ["listen for completed jobs and reschedule successful ones ."], "bleu": 0.14558246978804804, "rouge_l": 0.43160377358490565}
{"id": 3153, "code": "def schedule ( self ) : if self . async mode : self . scheduler . start ( ) self . listener . start ( ) else : self . scheduler . schedule ( )", "predictions": ["schedule the next scheduler ."], "references": ["start scheduling jobs ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 3154, "code": "def build job arguments ( task ) : job args = { } job args [ 'qitems' ] = Q STORAGE ITEMS job args [ 'task id' ] = task . task id job args [ 'backend' ] = task . backend backend args = copy . deepcopy ( task . backend args ) if 'next from date' in backend args : backend args [ 'from date' ] = backend args . pop ( 'next from date' ) if 'next offset' in backend args : backend args [ 'offset' ] = backend args . pop ( 'next offset' ) job args [ 'backend args' ] = backend args job args [ 'category' ] = task . category archiving cfg = task . archiving cfg job args [ 'archive args' ] = archiving cfg . to dict ( ) if archiving cfg else None sched cfg = task . scheduling cfg job args [ 'max retries' ] = sched cfg . max retries if sched cfg else MAX JOB RETRIES return job args", "predictions": ["build a list of arguments for a job ."], "references": ["build the set of arguments required for running a job"], "bleu": 0.2273485806048418, "rouge_l": 0.6256410256410255}
{"id": 3155, "code": "def reverse action ( self , url name , * args , * * kwargs ) : if self . request and not self . request . version : return reverse ( self . get url name ( url name ) , * args , * * kwargs ) return super ( ) . reverse action ( url name , * args , * * kwargs )", "predictions": ["reverse the action if it is not reverse ."], "references": ["extended drf with fallback to requested namespace if request . version is missing"], "bleu": 0.10761866342063775, "rouge_l": 0.17604617604617603}
{"id": 3156, "code": "def get version ( version = None ) : if version is None : version = VERSION assert len ( version ) == 5 assert version [ 3 ] in ( \"alpha\" , \"beta\" , \"rc\" , \"final\" ) parts = 2 if version [ 2 ] == 0 else 3 main = \".\" . join ( str ( x ) for x in version [ : parts ] ) sub = \"\" if version [ 3 ] != \"final\" : mapping = { \"alpha\" : \"a\" , \"beta\" : \"b\" , \"rc\" : \"c\" } sub = mapping [ version [ 3 ] ] + str ( version [ 4 ] ) return main + sub", "predictions": ["returns the version string from the version string ."], "references": ["derives a pep386 - compliant version number from version ."], "bleu": 0.15881076016027915, "rouge_l": 0.41709401709401706}
{"id": 3157, "code": "def create ( self , request ) : login form = Authentication Form ( request , data = request . data ) if not login form . is valid ( ) : raise serializers . Validation Error ( login form . errors ) auth login ( request , login form . get user ( ) ) serializer = User Serializer ( request . user ) return Response ( serializer . data , status = status . HTTP 200 OK )", "predictions": ["create a user ."], "references": ["log in django staff user"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 3158, "code": "def list ( self , request ) : serializer = self . get serializer ( request . user ) return Response ( serializer . data , status = status . HTTP 200 OK )", "predictions": ["lists all user info ."], "references": ["retrieve logged in user info"], "bleu": 0.35930411196308426, "rouge_l": 0.4}
{"id": 3159, "code": "def create ( self , request ) : password form = Password Change Form ( request . user , data = request . data ) if not password form . is valid ( ) : raise serializers . Validation Error ( password form . errors ) password form . save ( ) update session auth hash ( request , password form . user ) return Response ( status = status . HTTP 204 NO CONTENT )", "predictions": ["create a new user ."], "references": ["change password for logged in django staff user"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 3160, "code": "def create field ( field info ) : field type = field info . get ( 'type' ) if field type not in FIELDS NAME MAP : raise Value Error ( ( 'not support this field: {}' ) . format ( field type ) ) field class = FIELDS NAME MAP . get ( field type ) params = dict ( field info ) params . pop ( 'type' ) return field class . from dict ( params )", "predictions": ["create a field from a field type ."], "references": ["create a field by field info dict ."], "bleu": 0.3303164318013807, "rouge_l": 0.625}
{"id": 3161, "code": "def change logging kwargs ( kwargs ) : log levels = kwargs . pop ( 'log level' , None ) log folder = kwargs . pop ( 'log folder' , 'logs' ) logger names = kwargs . pop ( 'logger names' , '' ) if log levels is None : log levels = kwargs . pop ( 'log levels' , logging . INFO ) log multiproc = kwargs . pop ( 'log multiproc' , True ) if not isinstance ( logger names , ( tuple , list ) ) : logger names = [ logger names ] if not isinstance ( log levels , ( tuple , list ) ) : log levels = [ log levels ] if len ( log levels ) == 1 : log levels = [ log levels [ 0 ] for in logger names ] dictionary = copy . deepcopy ( LOGGING DICT ) prefixes = [ '' ] if not log multiproc : for key in list ( dictionary . keys ( ) ) : if key . startswith ( 'multiproc ' ) : del dictionary [ key ] else : prefixes . append ( 'multiproc ' ) for prefix in prefixes : for handler dict in dictionary [ prefix + 'handlers' ] . values ( ) : if 'filename' in handler dict : filename = os . path . join ( log folder , handler dict [ 'filename' ] ) filename = os . path . normpath ( filename ) handler dict [ 'filename' ] = filename dictionary [ prefix + 'loggers' ] = { } logger dict = dictionary [ prefix + 'loggers' ] for idx , logger name in enumerate ( logger names ) : logger dict [ logger name ] = { 'level' : log levels [ idx ] , 'handlers' : list ( dictionary [ prefix + 'handlers' ] . keys ( ) ) } kwargs [ 'log config' ] = dictionary", "predictions": ["change logging logging to default values"], "references": ["helper function to turn the simple logging kwargs into a log_config ."], "bleu": 0.08993236413460196, "rouge_l": 0.10481099656357389}
{"id": 3162, "code": "def get strings ( args ) : string list = [ ] for elem in ast . walk ( ast . parse ( args ) ) : if isinstance ( elem , ast . Str ) : string list . append ( elem . s ) return string list", "predictions": ["return a list of strings from the ast ."], "references": ["returns all valid python strings inside a given argument string ."], "bleu": 0.1343994460963362, "rouge_l": 0.19645732689210954}
{"id": 3163, "code": "def extract replacements ( self , trajectory ) : self . env name = trajectory . v environment name self . traj name = trajectory . v name self . set name = trajectory . f wildcard ( '$set' ) self . run name = trajectory . f wildcard ( '$' )", "predictions": ["extract the replacements for the given trajectory ."], "references": ["extracts the wildcards and file replacements from the trajectory"], "bleu": 0.17795502018438056, "rouge_l": 0.465648854961832}
{"id": 3164, "code": "def parser to string io ( parser ) : memory file = String IO ( ) parser . write ( memory file ) memory file . flush ( ) memory file . seek ( 0 ) return memory file", "predictions": ["convert memory to memory ."], "references": ["turns a configparser into a stringio stream ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 3165, "code": "def make logging handlers and tools ( self , multiproc = False ) : log stdout = self . log stdout if sys . stdout is self . stdout to logger : log stdout = False if self . log config : if multiproc : proc log config = self . mp config else : proc log config = self . sp config if proc log config : if isinstance ( proc log config , dict ) : new dict = self . handle dict config ( proc log config ) dict Config ( new dict ) else : parser = self . handle config parsing ( proc log config ) memory file = self . parser to string io ( parser ) file Config ( memory file , disable existing loggers = False ) if log stdout : std name , std level = self . log stdout stdout = Stdout To Logger ( std name , log level = std level ) stdout . start ( ) self . tools . append ( stdout )", "predictions": ["make the logging handlers and tools to tools ."], "references": ["creates logging handlers and redirects stdout ."], "bleu": 0.2777619034011791, "rouge_l": 0.5115303983228512}
{"id": 3166, "code": "def finalize ( self , remove all handlers = True ) : for tool in self . tools : tool . finalize ( ) self . tools = [ ] self . stdout to logger = None for config in ( self . sp config , self . mp config ) : if hasattr ( config , 'close' ) : config . close ( ) self . sp config = None self . mp config = None if remove all handlers : self . tabula rasa ( )", "predictions": ["finalize all tools and sp tools ."], "references": ["finalizes the manager closes and removes all handlers if desired ."], "bleu": 0.1247439242120089, "rouge_l": 0.2136602451838879}
{"id": 3167, "code": "def start ( self ) : if sys . stdout is not self : self . original steam = sys . stdout sys . stdout = self self . redirection = True if self . redirection : print ( 'Established redirection of `stdout`.' )", "predictions": ["start the timer ."], "references": ["starts redirection of stdout"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 3168, "code": "def write ( self , buf ) : if not self . recursion : self . recursion = True try : for line in buf . rstrip ( ) . splitlines ( ) : self . logger . log ( self . log level , line . rstrip ( ) ) finally : self . recursion = False else : sys . stderr . write ( 'ERROR: Recursion in Stream redirection!' )", "predictions": ["forwards the contents of the internal is not a string in the is complete in the is complete in the is not in the current is not in the is a"], "references": ["writes data from buffer to logger"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3169, "code": "def get all attributes ( instance ) : try : result dict = instance . dict . copy ( ) except Attribute Error : result dict = { } if hasattr ( instance , ' all slots ' ) : all slots = instance . all slots else : all slots = slots . get all slots ( instance . class ) for slot in all slots : result dict [ slot ] = getattr ( instance , slot ) result dict . pop ( ' dict ' , None ) result dict . pop ( ' weakref ' , None ) return result dict", "predictions": ["returns all items in a dictionary"], "references": ["returns an attribute value dictionary much like __dict__ but incorporates __slots__"], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 3170, "code": "def kwargs mutual exclusive ( param1 name , param2 name , map2to1 = None ) : def wrapper ( func ) : @ functools . wraps ( func ) def new func ( * args , * * kwargs ) : if param2 name in kwargs : if param1 name in kwargs : raise Value Error ( 'You cannot specify `%s` and `%s` at the same time, ' 'they are mutually exclusive.' % ( param1 name , param2 name ) ) param2 = kwargs . pop ( param2 name ) if map2to1 is not None : param1 = map2to1 ( param2 ) else : param1 = param2 kwargs [ param1 name ] = param1 return func ( * args , * * kwargs ) return new func return wrapper", "predictions": ["decorator to slider a hero hero hero order order order order order order order order order ."], "references": ["if there exist mutually exclusive parameters checks for them and maps param2 to 1 ."], "bleu": 0.07994607499472013, "rouge_l": 0.12642487046632125}
{"id": 3171, "code": "def not in run ( func ) : doc = func . doc na string = '''\\n ATTENTION: This function is not available during a single run!\\n''' if doc is not None : func . doc = '\\n' . join ( [ doc , na string ] ) func . not in run = True @ functools . wraps ( func ) def new func ( self , * args , * * kwargs ) : if self . is run : raise Type Error ( 'Function `%s` is not available during a single run.' % func . name ) return func ( self , * args , * * kwargs ) return new func", "predictions": ["decorator to check if a method is acquire acquire acquire"], "references": ["this is a decorator that signaling that a function is not available during a single run ."], "bleu": 0.07444363397302332, "rouge_l": 0.21229698375870074}
{"id": 3172, "code": "def with open store ( func ) : doc = func . doc na string = '''\\n ATTENTION: This function can only be used if the store is open!\\n''' if doc is not None : func . doc = '\\n' . join ( [ doc , na string ] ) func . with open store = True @ functools . wraps ( func ) def new func ( self , * args , * * kwargs ) : if not self . traj . v storage service . is open : raise Type Error ( 'Function `%s` is only available if the storage is open.' % func . name ) return func ( self , * args , * * kwargs ) return new func", "predictions": ["decorator to release a access access access to the storage store"], "references": ["this is a decorator that signaling that a function is only available if the storage is open ."], "bleu": 0.09014189299371085, "rouge_l": 0.2643553629469122}
{"id": 3173, "code": "def prefix naming ( cls ) : if hasattr ( cls , ' getattr ' ) : raise Type Error ( ' getattr  already defined' ) cls . getattr = prfx getattr cls . setattr = prfx setattr return cls", "predictions": ["class decorator to return class . ."], "references": ["decorate that adds the prefix naming scheme"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3174, "code": "def add params ( traj ) : traj . v standard parameter = Brian2Parameter traj . v fast access = True traj . f add parameter ( 'Net.C' , 281 * p F ) traj . f add parameter ( 'Net.g L' , 30 * n S ) traj . f add parameter ( 'Net.EL' , - 70.6 * m V ) traj . f add parameter ( 'Net.VT' , - 50.4 * m V ) traj . f add parameter ( 'Net.Delta T' , 2 * m V ) traj . f add parameter ( 'Net.tauw' , 40 * ms ) traj . f add parameter ( 'Net.a' , 4 * n S ) traj . f add parameter ( 'Net.b' , 0.08 * n A ) traj . f add parameter ( 'Net.I' , .8 * n A ) traj . f add parameter ( 'Net.Vcut' , 'vm > 0*m V' ) traj . f add parameter ( 'Net.N' , 50 ) eqs = traj . f add parameter ( 'Net.eqs' , eqs ) traj . f add parameter ( 'reset' , 'vm=Vr;w+=b' )", "predictions": ["tasks for the parameters in the global global parameters ."], "references": ["adds all necessary parameters to traj ."], "bleu": 0.13950796967929133, "rouge_l": 0.24302788844621517}
{"id": 3175, "code": "def run net ( traj ) : eqs = traj . eqs namespace = traj . Net . f to dict ( short names = True , fast access = True ) neuron = Neuron Group ( traj . N , model = eqs , threshold = traj . Vcut , reset = traj . reset , namespace = namespace ) neuron . vm = traj . EL neuron . w = traj . a * ( neuron . vm - traj . EL ) neuron . Vr = linspace ( - 48.3 * m V , - 47.7 * m V , traj . N ) print ( 'Initial Run' ) net = Network ( neuron ) net . run ( 100 * ms , report = 'text' ) M Spike = Spike Monitor ( neuron ) net . add ( M Spike ) M State V = State Monitor ( neuron , variables = [ 'vm' ] , record = [ 1 , 2 , 3 ] ) net . add ( M State V ) print ( 'Measurement run' ) net . run ( 500 * ms , report = 'text' ) traj . v standard result = Brian2Monitor Result traj . f add result ( 'Spike Monitor' , M Spike ) traj . f add result ( 'State Monitor V' , M State V )", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["creates and runs brian network based on the parameters in traj ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3176, "code": "def add parameters ( traj ) : traj . f add parameter ( 'steps' , 10000 , comment = 'Number of time steps to simulate' ) traj . f add parameter ( 'dt' , 0.01 , comment = 'Step size' ) traj . f add parameter ( Array Parameter , 'initial conditions' , np . array ( [ 0.0 , 0.0 , 0.0 ] ) , comment = 'Our initial conditions, as default we will start from' ' origin!' ) traj . f add parameter ( 'func params.sigma' , 10.0 ) traj . f add parameter ( 'func params.beta' , 8.0 / 3.0 ) traj . f add parameter ( 'func params.rho' , 28.0 ) #For the fun of it we will annotate the  group traj . func params . v annotations . info = 'This group contains as default the original values chosen ' 'by Edward Lorenz in 1963. Check it out on wikipedia ' '(https://en.wikipedia.org/wiki/Lorenz attractor)!'", "predictions": ["create the index for the"], "references": ["adds all necessary parameters to the traj container"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 3177, "code": "def create storage ( storage service , trajectory = None , * * kwargs ) : kwargs copy = kwargs . copy ( ) kwargs copy [ 'trajectory' ] = trajectory matching kwargs = get matching kwargs ( storage service , kwargs copy ) storage service = storage service ( * * matching kwargs ) unused kwargs = set ( kwargs . keys ( ) ) - set ( matching kwargs . keys ( ) ) return storage service , unused kwargs", "predictions": ["json - serializable encoder for the current * * * * * * * * * * * * * * * * * * * * * * * *"], "references": ["creates a service from a constructor and checks which kwargs are not used"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3178, "code": "def add parameters ( traj ) : assert ( isinstance ( traj , Trajectory ) ) scale = traj . simulation . scale traj . v standard parameter = Brian2Parameter model eqs = conn eqs = traj . f add parameter ( 'model.eqs' , model eqs , comment = 'The differential equation for the neuron model' ) traj . f add parameter ( 'model.synaptic.eqs' , conn eqs , comment = 'The differential equation for the synapses. ' 'PRE will be replaced by `i` or `e` depending ' 'on the source population' ) traj . f add parameter ( 'model.synaptic.tau1' , 1 * ms , comment = 'The decay time' ) traj . f add parameter ( 'model.synaptic.tau2 e' , 3 * ms , comment = 'The rise time, excitatory' ) traj . f add parameter ( 'model.synaptic.tau2 i' , 2 * ms , comment = 'The rise time, inhibitory' ) traj . f add parameter ( 'model.V th' , 'V >= 1.0' , comment = \"Threshold value\" ) traj . f add parameter ( 'model.reset func' , 'V=0.0' , comment = \"String representation of reset function\" ) traj . f add parameter ( 'model.refractory' , 5 * ms , comment = \"Absolute refractory period\" ) traj . f add parameter ( 'model.N e' , int ( 2000 * scale ) , comment = \"Amount of excitatory neurons\" ) traj . f add parameter ( 'model.N i' , int ( 500 * scale ) , comment = \"Amount of inhibitory neurons\" ) traj . f add parameter ( 'model.tau e' , 15 * ms , comment = \"Membrane time constant, excitatory\" ) traj . f add parameter ( 'model.tau i' , 10 * ms , comment = \"Membrane time constant, inhibitory\" ) traj . f add parameter ( 'model.mu e min' , 1.1 , comment = \"Lower bound for bias, excitatory\" ) traj . f add parameter ( 'model.mu e max' , 1.2 , comment = \"Upper bound for bias, excitatory\" ) traj . f add parameter ( 'model.mu i min' , 1.0 , comment = \"Lower bound for bias, inhibitory\" ) traj . f add parameter ( 'model.mu i max' , 1.05 , comment = \"Upper bound for bias, inhibitory\" )", "predictions": ["items are bound bound"], "references": ["adds all neuron group parameters to traj ."], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 3179, "code": "def add parameters ( traj ) : assert ( isinstance ( traj , Trajectory ) ) traj . v standard parameter = Brian2Parameter scale = traj . simulation . scale traj . f add parameter ( 'connections.R ee' , 1.0 , comment = 'Scaling factor for clustering' ) traj . f add parameter ( 'connections.clustersize e' , 100 , comment = 'Size of a cluster' ) traj . f add parameter ( 'connections.strength factor' , 2.5 , comment = 'Factor for scaling cluster weights' ) traj . f add parameter ( 'connections.p ii' , 0.25 , comment = 'Connection probability from inhibitory to inhibitory' ) traj . f add parameter ( 'connections.p ei' , 0.25 , comment = 'Connection probability from inhibitory to excitatory' ) traj . f add parameter ( 'connections.p ie' , 0.25 , comment = 'Connection probability from excitatory to inhibitory' ) traj . f add parameter ( 'connections.p ee' , 0.1 , comment = 'Connection probability from excitatory to excitatory' ) traj . f add parameter ( 'connections.J ii' , 0.027 / np . sqrt ( scale ) , comment = 'Connection strength from inhibitory to inhibitory' ) traj . f add parameter ( 'connections.J ei' , 0.032 / np . sqrt ( scale ) , comment = 'Connection strength from inhibitory to excitatroy' ) traj . f add parameter ( 'connections.J ie' , 0.009 / np . sqrt ( scale ) , comment = 'Connection strength from excitatory to inhibitory' ) traj . f add parameter ( 'connections.J ee' , 0.012 / np . sqrt ( scale ) , comment = 'Connection strength from excitatory to excitatory' )", "predictions": ["validate the args from from"], "references": ["adds all neuron group parameters to traj ."], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 3180, "code": "def add parameters ( self , traj ) : par = traj . f add parameter ( Brian2Parameter , 'simulation.durations.initial run' , 500 * ms , comment = 'Initialisation run for more realistic ' 'measurement conditions.' ) par . v annotations . order = 0 par = traj . f add parameter ( Brian2Parameter , 'simulation.durations.measurement run' , 1500 * ms , comment = 'Measurement run that is considered for ' 'statistical evaluation' ) par . v annotations . order = 1", "predictions": ["parse the archive archive archive return a list of archive archive return them as string"], "references": ["adds all necessary parameters to traj container ."], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 3181, "code": "def add monitors ( self , traj , network , network dict ) : neurons e = network dict [ 'neurons e' ] monitor list = [ ] self . spike monitor = Spike Monitor ( neurons e ) monitor list . append ( self . spike monitor ) self . V monitor = State Monitor ( neurons e , 'V' , record = list ( traj . neuron records ) ) monitor list . append ( self . V monitor ) self . I syn e monitor = State Monitor ( neurons e , 'I syn e' , record = list ( traj . neuron records ) ) monitor list . append ( self . I syn e monitor ) self . I syn i monitor = State Monitor ( neurons e , 'I syn i' , record = list ( traj . neuron records ) ) monitor list . append ( self . I syn i monitor ) network . add ( * monitor list ) network dict [ 'monitors' ] = monitor list", "predictions": ["schedule a queue to the scheduler"], "references": ["adds monitors to the network"], "bleu": 0.2907153684841096, "rouge_l": 0.3696969696969697}
{"id": 3182, "code": "def plot result ( self , traj , result name ) : result = traj . f get ( result name ) varname = result . record variables [ 0 ] values = result [ varname ] times = result . t record = result . record for idx , celia neuron in enumerate ( record ) : plt . subplot ( len ( record ) , 1 , idx + 1 ) plt . plot ( times , values [ idx , : ] ) if idx == 0 : plt . title ( '%s' % varname ) if idx == 1 : plt . ylabel ( '%s' % ( varname ) ) if idx == len ( record ) - 1 : plt . xlabel ( 't' )", "predictions": ["cancel the job job information"], "references": ["plots a state variable graph for several neurons into one figure"], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 3183, "code": "def print graphs ( self , traj ) : print folder = self . make folder ( traj ) plt . figure ( ) plt . scatter ( self . spike monitor . t , self . spike monitor . i , s = 1 ) plt . xlabel ( 't' ) plt . ylabel ( 'Exc. Neurons' ) plt . title ( 'Spike Raster Plot' ) filename = os . path . join ( print folder , 'spike.png' ) print ( 'Current plot: %s ' % filename ) plt . savefig ( filename ) plt . close ( ) fig = plt . figure ( ) self . plot result ( traj , 'monitors.V' ) filename = os . path . join ( print folder , 'V.png' ) print ( 'Current plot: %s ' % filename ) fig . savefig ( filename ) plt . close ( ) plt . figure ( ) self . plot result ( traj , 'monitors.I syn e' ) filename = os . path . join ( print folder , 'I syn e.png' ) print ( 'Current plot: %s ' % filename ) plt . savefig ( filename ) plt . close ( ) plt . figure ( ) self . plot result ( traj , 'monitors.I syn i' ) filename = os . path . join ( print folder , 'I syn i.png' ) print ( 'Current plot: %s ' % filename ) plt . savefig ( filename ) plt . close ( ) if not traj . analysis . show plots : plt . close ( 'all' ) else : plt . show ( )", "predictions": ["run the spike graphs"], "references": ["makes some plots and stores them into subfolders"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 3184, "code": "def get batch ( ) : optlist , args = getopt . getopt ( sys . argv [ 1 : ] , '' , longopts = 'batch=' ) batch = 0 for o , a in optlist : if o == '--batch' : batch = int ( a ) print ( 'Found batch %d' % batch ) return batch", "predictions": ["returns a batch object for the given pubsub"], "references": ["function that parses the batch id from the command line arguments"], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 3185, "code": "def explore batch ( traj , batch ) : explore dict = { } explore dict [ 'sigma' ] = np . arange ( 10.0 * batch , 10.0 * ( batch + 1 ) , 1.0 ) . tolist ( ) traj . f explore ( explore dict )", "predictions": ["schedule a batch batch batch"], "references": ["chooses exploration according to batch"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 3186, "code": "def vars ( self ) : if self . vars is None : self . vars = NN Tree Node Vars ( self ) return self . vars", "predictions": ["return - . build task ."], "references": ["alternative naming you can use node . vars . name instead of node . v_name"], "bleu": 0.054546736148076896, "rouge_l": 0.17681159420289855}
{"id": 3187, "code": "def func ( self ) : if self . func is None : self . func = NN Tree Node Func ( self ) return self . func", "predictions": ["request wrapper to request reverse reverse function name name name name name name name name name name name name name name name name name name name name name name name name"], "references": ["alternative naming you can use node . func . name instead of node . f_func"], "bleu": 0.03901663112717908, "rouge_l": 0.04638783269961977}
{"id": 3188, "code": "def rename ( self , full name ) : self . full name = full name if full name : self . name = full name . rsplit ( '.' , 1 ) [ - 1 ]", "predictions": ["get the name of the full is the full is the full is the full is the full is the full"], "references": ["renames the tree node"], "bleu": 0.05809665204409193, "rouge_l": 0.09118086696562032}
{"id": 3189, "code": "def set details ( self , depth , branch , run branch ) : self . depth = depth self . branch = branch self . run branch = run branch", "predictions": ["create the details details"], "references": ["sets some details for internal handling ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 3190, "code": "def determine types ( start node , first name , add leaf , add link ) : if start node . v is root : where = first name else : where = start node . branch if where in SUBTREE MAPPING : type tuple = SUBTREE MAPPING [ where ] else : type tuple = ( GROUP , LEAF ) if add link : return type tuple [ 0 ] , LINK if add leaf : return type tuple else : return type tuple [ 0 ] , type tuple [ 0 ]", "predictions": ["list the types types types for the given start data request data data data data request data data data request data request data data type data type data data data type"], "references": ["determines types for generic additions"], "bleu": 0.0513487742994337, "rouge_l": 0.12774869109947642}
{"id": 3191, "code": "def create link ( self , act node , name , instance ) : act node . links [ name ] = instance act node . children [ name ] = instance full name = instance . v full name if full name not in self . root instance . linked by : self . root instance . linked by [ full name ] = { } linking = self . root instance . linked by [ full name ] if act node . v full name not in linking : linking [ act node . v full name ] = ( act node , set ( ) ) linking [ act node . v full name ] [ 1 ] . add ( name ) if name not in self . links count : self . links count [ name ] = 0 self . links count [ name ] = self . links count [ name ] + 1 self . logger . debug ( 'Added link `%s` under `%s` pointing ' 'to `%s`.' % ( name , act node . v full name , instance . v full name ) ) return instance", "predictions": ["create a link form"], "references": ["creates a link and checks if names are appropriate"], "bleu": 0.1354797537848421, "rouge_l": 0.28773584905660377}
{"id": 3192, "code": "def create any group ( self , parent node , name , type name , instance = None , constructor = None , args = None , kwargs = None ) : if args is None : args = [ ] if kwargs is None : kwargs = { } full name = self . make full name ( parent node . v full name , name ) if instance is None : if constructor is None : if type name == RESULT GROUP : constructor = Result Group elif type name == PARAMETER GROUP : constructor = Parameter Group elif type name == CONFIG GROUP : constructor = Config Group elif type name == DERIVED PARAMETER GROUP : constructor = Derived Parameter Group elif type name == GROUP : constructor = NN Group Node else : raise Runtime Error ( 'You shall not pass!' ) instance = self . root instance . construct instance ( constructor , full name , * args , * * kwargs ) else : instance . rename ( full name ) if type name == RESULT GROUP : if type ( instance ) in ( NN Group Node , Parameter Group , Config Group , Derived Parameter Group ) : raise Type Error ( 'You cannot add a `%s` type of group under results' % str ( type ( instance ) ) ) elif type name == PARAMETER GROUP : if type ( instance ) in ( NN Group Node , Result Group , Config Group , Derived Parameter Group ) : raise Type Error ( 'You cannot add a `%s` type of group under parameters' % str ( type ( instance ) ) ) elif type name == CONFIG GROUP : if type ( instance ) in ( NN Group Node , Parameter Group , Result Group , Derived Parameter Group ) : raise Type Error ( 'You cannot add a `%s` type of group under config' % str ( type ( instance ) ) ) elif type name == DERIVED PARAMETER GROUP : if type ( instance ) in ( NN Group Node , Parameter Group , Config Group , Result Group ) : raise Type Error ( 'You cannot add a `%s` type of group under derived ' 'parameters' % str ( type ( instance ) ) ) elif type name == GROUP : if type ( instance ) in ( Result Group , Parameter Group , Config Group , Derived Parameter Group ) : raise Type Error ( 'You cannot add a `%s` type of group under other data' % str ( type ( instance ) ) ) else : raise Runtime Error ( 'You shall not pass!' ) self . set details tree node ( parent node , name , instance ) instance . nn interface = self self . root instance . all groups [ instance . v full name ] = instance self . add to nodes and leaves ( instance ) parent node . children [ name ] = instance parent node . groups [ name ] = instance return instance", "predictions": ["create a group instance instance instance"], "references": ["generically creates a new group inferring from the type_name ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 3193, "code": "def add group from storage ( self , args , kwargs ) : return self . nn interface . add generic ( self , type name = GROUP , group type name = GROUP , args = args , kwargs = kwargs , add prefix = False , check naming = False )", "predictions": ["change an instance of an instance of an storage . . . . . . . ."], "references": ["can be called from storage service to create a new group to bypass name checking"], "bleu": 0.07223943354597204, "rouge_l": 0.06321243523316063}
{"id": 3194, "code": "def add leaf from storage ( self , args , kwargs ) : return self . nn interface . add generic ( self , type name = LEAF , group type name = GROUP , args = args , kwargs = kwargs , add prefix = False , check naming = False )", "predictions": ["get a strings from ."], "references": ["can be called from storage service to create a new leaf to bypass name checking"], "bleu": 0.040889869516541145, "rouge_l": 0.09172932330827067}
{"id": 3195, "code": "def f dir data ( self ) : if ( self . nn interface is not None and self . nn interface . root instance is not None and self . v root . v auto load ) : try : if self . v is root : self . f load ( recursive = True , max depth = 1 , load data = pypetconstants . LOAD SKELETON , with meta data = False , with run information = False ) else : self . f load ( recursive = True , max depth = 1 , load data = pypetconstants . LOAD SKELETON ) except Exception as exc : pass return list ( self . children . keys ( ) )", "predictions": ["returns a dictionary of all the data extract data from the directory ."], "references": ["returns a list of all children names"], "bleu": 0.15807437922444714, "rouge_l": 0.42287694974003465}
{"id": 3196, "code": "def unit from expression ( expr ) : if expr == '1' : return get unit fast ( 1 ) elif isinstance ( expr , str ) : mod = ast . parse ( expr , mode = 'eval' ) expr = mod . body return unit from expression ( expr ) elif expr . class is ast . Name : return ALLUNITS [ expr . id ] elif expr . class is ast . Num : return expr . n elif expr . class is ast . Unary Op : op = expr . op . class . name operand = unit from expression ( expr . operand ) if op == 'U Sub' : return - operand else : raise Syntax Error ( \"Unsupported operation \" + op ) elif expr . class is ast . Bin Op : op = expr . op . class . name left = unit from expression ( expr . left ) right = unit from expression ( expr . right ) if op == 'Add' : u = left + right elif op == 'Sub' : u = left - right elif op == 'Mult' : u = left * right elif op == 'Div' : u = left / right elif op == 'Pow' : n = unit from expression ( expr . right ) u = left ** n elif op == 'Mod' : u = left % right else : raise Syntax Error ( \"Unsupported operation \" + op ) return u else : raise Runtime Error ( 'You shall not pass' )", "predictions": ["convert an ast to a parser"], "references": ["takes a unit string like 1 . * volt and returns the brian2 unit ."], "bleu": 0.04928854007377984, "rouge_l": 0.08840579710144927}
{"id": 3197, "code": "def f supports ( self , data ) : if isinstance ( data , Quantity ) : return True elif super ( Brian2Parameter , self ) . f supports ( data ) : return True return False", "predictions": ["check if data is a valid type"], "references": ["simply checks if data is supported"], "bleu": 0.345720784641941, "rouge_l": 0.4680306905370844}
{"id": 3198, "code": "def supports ( self , data ) : if isinstance ( data , Quantity ) : return True elif super ( Brian2Result , self ) . supports ( data ) : return True return False", "predictions": ["check if remove remove any of the remove"], "references": ["simply checks if data is supported"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 3199, "code": "def add commit variables ( traj , commit ) : git time value = time . strftime ( '%Y %m %d %Hh%Mm%Ss' , time . localtime ( commit . committed date ) ) git short name = str ( commit . hexsha [ 0 : 7 ] ) git commit name = 'commit %s ' % git short name git commit name = 'git.' + git commit name + git time value if not traj . f contains ( 'config.' + git commit name , shortcuts = False ) : git commit name += '.' traj . f add config ( git commit name + 'hexsha' , commit . hexsha , comment = 'SHA-1 hash of commit' ) traj . f add config ( git commit name + 'name rev' , commit . name rev , comment = 'String describing the commits hex sha based on ' 'the closest Reference' ) traj . f add config ( git commit name + 'committed date' , commit . committed date , comment = 'Date of commit as unix epoch seconds' ) traj . f add config ( git commit name + 'message' , str ( commit . message ) , comment = 'The commit message' )", "predictions": ["start commit self steam self steam"], "references": ["adds commit information to the trajectory ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 3200, "code": "def get argspec ( func ) : if inspect . isclass ( func ) : func = func . init if not inspect . isfunction ( func ) : return [ ] , False parameters = inspect . signature ( func ) . parameters args = [ ] uses starstar = False for par in parameters . values ( ) : if ( par . kind == inspect . Parameter . POSITIONAL OR KEYWORD or par . kind == inspect . Parameter . KEYWORD ONLY ) : args . append ( par . name ) elif par . kind == inspect . Parameter . VAR KEYWORD : uses starstar = True return args , uses starstar", "predictions": ["returns a function to get the argspec of the function ."], "references": ["helper function to support both python versions"], "bleu": 0.14991106946711685, "rouge_l": 0.2314990512333966}
{"id": 3201, "code": "def get matching kwargs ( func , kwargs ) : args , uses startstar = get argspec ( func ) if uses startstar : return kwargs . copy ( ) else : matching kwargs = dict ( ( k , kwargs [ k ] ) for k in args if k in kwargs ) return matching kwargs", "predictions": ["retrieve the first kwargs of a func ."], "references": ["takes a function and keyword arguments and returns the ones that can be passed ."], "bleu": 0.07949903911132591, "rouge_l": 0.16486486486486487}
{"id": 3202, "code": "def format time ( timestamp ) : format string = '%Y %m %d %Hh%Mm%Ss' formatted time = datetime . datetime . fromtimestamp ( timestamp ) . strftime ( format string ) return formatted time", "predictions": ["format a time time string to a string"], "references": ["formats timestamp to human readable format"], "bleu": 0.17747405280050269, "rouge_l": 0.14663461538461536}
{"id": 3203, "code": "def port to tcp ( port = None ) : #address = 'tcp://' + socket.gethostbyname(socket.getfqdn()) domain name = socket . getfqdn ( ) try : addr list = socket . getaddrinfo ( domain name , None ) except Exception : addr list = socket . getaddrinfo ( '127.0.0.1' , None ) family , socktype , proto , canonname , sockaddr = addr list [ 0 ] host = convert ipv6 ( sockaddr [ 0 ] ) address = 'tcp://' + host if port is None : port = ( ) if not isinstance ( port , int ) : context = zmq . Context ( ) try : socket = context . socket ( zmq . REP ) socket . ipv6 = is ipv6 ( address ) port = socket . bind to random port ( address , * port ) except Exception : print ( 'Could not connect to {} using {}' . format ( address , addr list ) ) pypet root logger = logging . get Logger ( 'pypet' ) pypet root logger . exception ( 'Could not connect to {}' . format ( address ) ) raise socket . close ( ) context . term ( ) return address + ':' + str ( port )", "predictions": ["attempt to convert a port to tcp tcp ."], "references": ["returns local tcp address for a given port automatic port if none"], "bleu": 0.12026590852507517, "rouge_l": 0.1856925418569254}
{"id": 3204, "code": "def racedirs ( path ) : if os . path . isfile ( path ) : raise IO Error ( 'Path `%s` is already a file not a directory' ) while True : try : if os . path . isdir ( path ) : break os . makedirs ( path ) except Environment Error as exc : if exc . errno != 17 : raise", "predictions": ["validates that path is a file or directory ."], "references": ["like os . makedirs but takes care about race conditions"], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 3205, "code": "def reset ( self , index , total , percentage step , length ) : self . start time = datetime . datetime . now ( ) self . start index = index self . current index = index self . percentage step = percentage step self . total = float ( total ) self . total minus one = total - 1 self . length = length self . norm factor = total * percentage step / 100.0 self . current interval = int ( ( index + 1.0 ) / self . norm factor )", "predictions": ["reset the index of the given index ."], "references": ["resets to the progressbar to start a new one"], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 3206, "code": "def get remaining ( self , index ) : try : current time = datetime . datetime . now ( ) time delta = current time - self . start time try : total seconds = time delta . total seconds ( ) except Attribute Error : total seconds = ( ( time delta . microseconds + ( time delta . seconds + time delta . days * 24 * 3600 ) * 10 ** 6 ) / 10.0 ** 6 ) remaining seconds = int ( ( self . total - self . start index - 1.0 ) * total seconds / float ( index - self . start index ) - total seconds ) remaining delta = datetime . timedelta ( seconds = remaining seconds ) remaining str = ', remaining: ' + str ( remaining delta ) except Zero Division Error : remaining str = '' return remaining str", "predictions": ["get the remaining remaining in seconds"], "references": ["calculates remaining time as a string"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 3207, "code": "def f remove ( self , key ) : key = self . translate key ( key ) try : del self . dict [ key ] except Key Error : raise Attribute Error ( 'Your annotations do not contain %s' % key )", "predictions": ["remove an annotations from the key ."], "references": ["removes key from annotations"], "bleu": 0.22089591134157885, "rouge_l": 0.19122257053291536}
{"id": 3208, "code": "def f ann to str ( self ) : resstr = '' for key in sorted ( self . dict . keys ( ) ) : resstr += '%s=%s; ' % ( key , str ( self . dict [ key ] ) ) return resstr [ : - 2 ]", "predictions": ["convert the table to a readable dictionary ."], "references": ["returns all annotations lexicographically sorted as a concatenated string ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 3209, "code": "def supports ( self , item ) : result = super ( Shared Result , self ) . supports ( item ) result = result or type ( item ) in Shared Result . SUPPORTED DATA return result", "predictions": ["supports the given item ."], "references": ["checks if outer data structure is supported ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 3210, "code": "def create shared data ( self , name = None , * * kwargs ) : if name is None : item = self . f get ( ) else : item = self . f get ( name ) return item . create shared data ( * * kwargs )", "predictions": ["create a shared data item ."], "references": ["calls the corresponding function of the shared data item"], "bleu": 0.2493651438887133, "rouge_l": 0.3860759493670886}
{"id": 3211, "code": "def send done ( self ) : self . start ( test connection = False ) self . logger . debug ( 'Sending shutdown signal' ) self . req rep ( ZMQ Server . DONE )", "predictions": ["send done to server ."], "references": ["notifies the server to shutdown"], "bleu": 0.3021375397356768, "rouge_l": 0.2}
{"id": 3212, "code": "def req rep retry ( self , request ) : retries left = self . RETRIES while retries left : self . logger . log ( 1 , 'Sending REQ `%s`' , request ) self . send request ( request ) socks = dict ( self . poll . poll ( self . TIMEOUT ) ) if socks . get ( self . socket ) == zmq . POLLIN : response = self . receive response ( ) self . logger . log ( 1 , 'Received REP `%s`' , response ) return response , self . RETRIES - retries left else : self . logger . debug ( 'No response from server (%d retries left)' % retries left ) self . close socket ( confused = True ) retries left -= 1 if retries left == 0 : raise Runtime Error ( 'Server seems to be offline!' ) time . sleep ( self . SLEEP ) self . start socket ( )", "predictions": ["poll for retries requests ."], "references": ["returns response and number of retries"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 3213, "code": "def put on queue ( self , to put ) : old = self . pickle queue self . pickle queue = False try : self . queue . put ( to put , block = True ) finally : self . pickle queue = old", "predictions": ["put data in queue to queue"], "references": ["puts data on queue"], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 3214, "code": "def put on pipe ( self , to put ) : self . acquire lock ( ) self . send chunks ( to put ) self . release lock ( )", "predictions": ["put a pipe into the device ."], "references": ["puts data on queue"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3215, "code": "def handle data ( self , msg , args , kwargs ) : stop = False try : if msg == 'DONE' : stop = True elif msg == 'STORE' : if 'msg' in kwargs : store msg = kwargs . pop ( 'msg' ) else : store msg = args [ 0 ] args = args [ 1 : ] if 'stuff to store' in kwargs : stuff to store = kwargs . pop ( 'stuff to store' ) else : stuff to store = args [ 0 ] args = args [ 1 : ] trajectory name = kwargs [ 'trajectory name' ] if self . trajectory name != trajectory name : if self . storage service . is open : self . close file ( ) self . trajectory name = trajectory name self . open file ( ) self . storage service . store ( store msg , stuff to store , * args , * * kwargs ) self . storage service . store ( pypetconstants . FLUSH , None ) self . check and collect garbage ( ) else : raise Runtime Error ( 'You queued something that was not ' 'intended to be queued. I did not understand message ' '`%s`.' % msg ) except Exception : self . logger . exception ( 'ERROR occurred during storing!' ) time . sleep ( 0.01 ) pass return stop", "predictions": ["handle the data sent to the storage"], "references": ["handles data and returns true or false if everything is done ."], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 3216, "code": "def run ( self ) : try : while True : msg , args , kwargs = self . receive data ( ) stop = self . handle data ( msg , args , kwargs ) if stop : break finally : if self . storage service . is open : self . close file ( ) self . trajectory name = ''", "predictions": ["run the trajectory ."], "references": ["starts listening to the queue ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 3217, "code": "def receive data ( self ) : result = self . queue . get ( block = True ) if hasattr ( self . queue , 'task done' ) : self . queue . task done ( ) return result", "predictions": ["receive the data from the queue ."], "references": ["gets data from queue"], "bleu": 0.2626909894424158, "rouge_l": 0.5736677115987461}
{"id": 3218, "code": "def receive data ( self ) : while True : while len ( self . buffer ) < self . max size and self . conn . poll ( ) : data = self . read chunks ( ) if data is not None : self . buffer . append ( data ) if len ( self . buffer ) > 0 : return self . buffer . popleft ( )", "predictions": ["receive data from the buffer"], "references": ["gets data from pipe"], "bleu": 0.35930411196308426, "rouge_l": 0.4535315985130111}
{"id": 3219, "code": "def store ( self , * args , * * kwargs ) : try : self . acquire lock ( ) return self . storage service . store ( * args , * * kwargs ) finally : if self . lock is not None : try : self . release lock ( ) except Runtime Error : self . logger . error ( 'Could not release lock `%s`!' % str ( self . lock ) )", "predictions": ["store the lock in the storage ."], "references": ["acquires a lock before storage and releases it afterwards ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 3220, "code": "def store ( self , msg , stuff to store , * args , * * kwargs ) : trajectory name = kwargs [ 'trajectory name' ] if trajectory name not in self . references : self . references [ trajectory name ] = [ ] self . references [ trajectory name ] . append ( ( msg , cp . copy ( stuff to store ) , args , kwargs ) )", "predictions": ["store the message in the store ."], "references": ["simply keeps a reference to the stored data"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 3221, "code": "def store references ( self , references ) : for trajectory name in references : self . storage service . store ( pypetconstants . LIST , references [ trajectory name ] , trajectory name = trajectory name ) self . check and collect garbage ( )", "predictions": ["store the references of the given trajectory ."], "references": ["stores references to disk and may collect garbage ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 3222, "code": "def parse config ( init func ) : @ functools . wraps ( init func ) def new func ( env , * args , * * kwargs ) : config interpreter = Config Interpreter ( kwargs ) new kwargs = config interpreter . interpret ( ) init func ( env , * args , * * new kwargs ) config interpreter . add parameters ( env . traj ) return new func", "predictions": ["parse the config and return a callable instance ."], "references": ["decorator wrapping the environment to use a config file"], "bleu": 0.16784459625186196, "rouge_l": 0.2222222222222222}
{"id": 3223, "code": "def collect section ( self , section ) : kwargs = { } try : if self . parser . has section ( section ) : options = self . parser . options ( section ) for option in options : str val = self . parser . get ( section , option ) val = ast . literal eval ( str val ) kwargs [ option ] = val return kwargs except : raise", "predictions": ["collect the options from the parser ."], "references": ["collects all settings within a section"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3224, "code": "def collect config ( self ) : kwargs = { } sections = ( 'storage service' , 'trajectory' , 'environment' ) for section in sections : kwargs . update ( self . collect section ( section ) ) return kwargs", "predictions": ["collect the configuration from the config file ."], "references": ["collects all info from three sections"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 3225, "code": "def interpret ( self ) : if self . config file : new kwargs = self . collect config ( ) for key in new kwargs : if key not in self . kwargs : self . kwargs [ key ] = new kwargs [ key ] if not use simple logging ( self . kwargs ) and 'log config' not in self . kwargs : self . kwargs [ 'log config' ] = self . config file return self . kwargs", "predictions": ["interpret the logging ."], "references": ["copies parsed arguments into the kwargs passed to the environment"], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 3226, "code": "def add parameters ( self , traj ) : if self . config file : parameters = self . collect section ( 'parameters' ) for name in parameters : value = parameters [ name ] if not isinstance ( value , tuple ) : value = ( value , ) traj . f add parameter ( name , * value ) config = self . collect section ( 'config' ) for name in config : value = config [ name ] if not isinstance ( value , tuple ) : value = ( value , ) traj . f add config ( name , * value )", "predictions": ["add parameters to config file ."], "references": ["adds parameters and config from the . ini file to the trajectory"], "bleu": 0.1069482072978842, "rouge_l": 0.31443298969072164}
{"id": 3227, "code": "def overview group ( self ) : if self . overview group is None : self . overview group = self . all create or get groups ( 'overview' ) [ 0 ] return self . overview group", "predictions": ["return the overview for the overview group ."], "references": ["direct link to the overview group"], "bleu": 0.2984745896009823, "rouge_l": 0.43990384615384615}
{"id": 3228, "code": "def trj load exploration ( self , traj ) : if hasattr ( self . overview group , 'explorations' ) : explorations table = self . overview group . f get child ( 'explorations' ) for row in explorations table . iterrows ( ) : param name = row [ 'explorations' ] . decode ( 'utf-8' ) if param name not in traj . explored parameters : traj . explored parameters [ param name ] = None else : for what in ( 'parameters' , 'derived parameters' ) : if hasattr ( self . trajectory group , what ) : parameters = self . trajectory group . f get child ( what ) for group in parameters . f walk groups ( ) : if self . all get from attrs ( group , HDF5Storage Service . LENGTH ) : group location = group . v pathname full name = '.' . join ( group location . split ( '/' ) [ 2 : ] ) traj . explored parameters [ full name ] = None", "predictions": ["load the child from the overview"], "references": ["recalls names of all explored parameters"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3229, "code": "def trj store explorations ( self , traj ) : nexplored = len ( traj . explored parameters ) if nexplored > 0 : if hasattr ( self . overview group , 'explorations' ) : explorations table = self . overview group . f get child ( 'explorations' ) if len ( explorations table ) != nexplored : self . hdf5file . remove node ( where = self . overview group , name = 'explorations' ) if not hasattr ( self . overview group , 'explorations' ) : explored list = list ( traj . explored parameters . keys ( ) ) if explored list : string col = self . all get table col ( 'explorations' , explored list , 'overview.explorations' ) else : string col = pt . String Col ( 1 ) description = { 'explorations' : string col } explorations table = self . hdf5file . create table ( where = self . overview group , name = 'explorations' , description = description ) rows = [ ( x . encode ( 'utf-8' ) , ) for x in explored list ] if rows : explorations table . append ( rows ) explorations table . flush ( )", "predictions": ["store the explorations in the explorations"], "references": ["stores a all explored parameter names for internal recall"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 3230, "code": "def srvc make overview tables ( self , tables to make , traj = None ) : for table name in tables to make : paramdescriptiondict = { } expectedrows = 0 paramdescriptiondict [ 'location' ] = pt . String Col ( pypetconstants . HDF5 STRCOL MAX LOCATION LENGTH , pos = 0 ) paramdescriptiondict [ 'name' ] = pt . String Col ( pypetconstants . HDF5 STRCOL MAX NAME LENGTH , pos = 1 ) paramdescriptiondict [ 'comment' ] = pt . String Col ( pypetconstants . HDF5 STRCOL MAX COMMENT LENGTH ) paramdescriptiondict [ 'value' ] = pt . String Col ( pypetconstants . HDF5 STRCOL MAX VALUE LENGTH , pos = 2 ) if table name == 'config overview' : if traj is not None : expectedrows = len ( traj . config ) if table name == 'parameters overview' : if traj is not None : expectedrows = len ( traj . parameters ) if table name == 'explored parameters overview' : paramdescriptiondict [ 'range' ] = pt . String Col ( pypetconstants . HDF5 STRCOL MAX RANGE LENGTH ) paramdescriptiondict [ 'length' ] = pt . Int Col ( ) if traj is not None : expectedrows = len ( traj . explored parameters ) if table name . endswith ( 'summary' ) : paramdescriptiondict [ 'hexdigest' ] = pt . String Col ( 64 , pos = 10 ) if table name == 'derived parameters overview' : expectedrows = self . derived parameters per run if traj is not None : expectedrows *= len ( traj ) expectedrows += len ( traj . derived parameters ) if table name == 'results overview' : expectedrows = self . results per run if traj is not None : expectedrows *= len ( traj ) expectedrows += len ( traj . results ) if expectedrows > 0 : paramtable = self . all get or create table ( where = self . overview group , tablename = table name , description = paramdescriptiondict , expectedrows = expectedrows ) else : paramtable = self . all get or create table ( where = self . overview group , tablename = table name , description = paramdescriptiondict ) paramtable . flush ( )", "predictions": ["make all overview tables ."], "references": ["creates the overview tables in overview group"], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 3231, "code": "def all get or create table ( self , where , tablename , description , expectedrows = None ) : where node = self . hdf5file . get node ( where ) if not tablename in where node : if not expectedrows is None : table = self . hdf5file . create table ( where = where node , name = tablename , description = description , title = tablename , expectedrows = expectedrows , filters = self . all get filters ( ) ) else : table = self . hdf5file . create table ( where = where node , name = tablename , description = description , title = tablename , filters = self . all get filters ( ) ) else : table = where node . f get child ( tablename ) return table", "predictions": ["create a table table ."], "references": ["creates a new table or if the table already exists returns it ."], "bleu": 0.06930996903910726, "rouge_l": 0.41146711635750427}
{"id": 3232, "code": "def all get node by name ( self , name ) : path name = name . replace ( '.' , '/' ) where = '/%s/%s' % ( self . trajectory name , path name ) return self . hdf5file . get node ( where = where )", "predictions": ["signature for get a node node node"], "references": ["returns an hdf5 node by the path specified in name"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 3233, "code": "def all insert into row ( self , row , insert dict ) : for key , val in insert dict . items ( ) : try : row [ key ] = val except Key Error as ke : self . logger . warning ( 'Could not write `%s` into a table, ' % key + repr ( ke ) )", "predictions": ["helper to matching get get data from a row"], "references": ["copies data from insert_dict into a pytables row ."], "bleu": 0.21105340631872635, "rouge_l": 0.4444444444444444}
{"id": 3234, "code": "def all create or get group ( self , name , parent hdf5 group = None ) : if not name in parent hdf5 group : new hdf5 group = self . hdf5file . create group ( where = parent hdf5 group , name = name , title = name , filters = self . all get filters ( ) ) return new hdf5 group , True else : new hdf5 group = parent hdf5 group . f get child ( name ) return new hdf5 group , False", "predictions": ["create a new group group or"], "references": ["creates or returns a group"], "bleu": 0.2626909894424158, "rouge_l": 0.3696969696969697}
{"id": 3235, "code": "def ann store annotations ( self , item with annotations , node , overwrite = False ) : if overwrite is True or overwrite == 'v annotations' : annotated = self . all get from attrs ( node , HDF5Storage Service . ANNOTATED ) if annotated : current attrs = node . v attrs for attr name in current attrs . v attrnames : if attr name . startswith ( HDF5Storage Service . ANNOTATION PREFIX ) : delattr ( current attrs , attr name ) delattr ( current attrs , HDF5Storage Service . ANNOTATED ) self . hdf5file . flush ( ) if not item with annotations . v annotations . f is empty ( ) : anno dict = item with annotations . v annotations . dict current attrs = node . v attrs changed = False for field name in anno dict : val = anno dict [ field name ] field name with prefix = HDF5Storage Service . ANNOTATION PREFIX + field name if field name with prefix not in current attrs : setattr ( current attrs , field name with prefix , val ) changed = True if changed : setattr ( current attrs , HDF5Storage Service . ANNOTATED , True ) self . hdf5file . flush ( )", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["stores annotations into an hdf5 file ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3236, "code": "def ann load annotations ( self , item with annotations , node ) : annotated = self . all get from attrs ( node , HDF5Storage Service . ANNOTATED ) if annotated : annotations = item with annotations . v annotations if not annotations . f is empty ( ) : raise Type Error ( 'Loading into non-empty annotations!' ) current attrs = node . v attrs for attr name in current attrs . v attrnames : if attr name . startswith ( HDF5Storage Service . ANNOTATION PREFIX ) : key = attr name key = key . replace ( HDF5Storage Service . ANNOTATION PREFIX , '' ) data = getattr ( current attrs , attr name ) setattr ( annotations , key , data )", "predictions": ["loads path path from path . ."], "references": ["loads annotations from disk ."], "bleu": 0.22089591134157885, "rouge_l": 0.5154929577464789}
{"id": 3237, "code": "def grp load group ( self , traj group , load data = pypetconstants . LOAD DATA , with links = True , recursive = False , max depth = None , traj = None , as new = False , hdf5 group = None ) : if hdf5 group is None : hdf5 group = self . all get node by name ( traj group . v full name ) traj = traj group . v root if recursive : parent traj node = traj group . f get parent ( ) self . tree load nodes dfs ( parent traj node , load data = load data , with links = with links , recursive = recursive , max depth = max depth , current depth = 0 , trajectory = traj , as new = as new , hdf5 group = hdf5 group ) else : if load data == pypetconstants . LOAD NOTHING : return elif load data == pypetconstants . OVERWRITE DATA : traj group . v annotations . f empty ( ) traj group . v comment = '' self . all load skeleton ( traj group , hdf5 group ) traj group . stored = not as new self . node processing timer . signal update ( )", "predictions": ["create a 1 - self self . self ."], "references": ["loads a group node and potentially everything recursively below"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 3238, "code": "def all load skeleton ( self , traj node , hdf5 group ) : if traj node . v annotations . f is empty ( ) : self . ann load annotations ( traj node , hdf5 group ) if traj node . v comment == '' : comment = self . all get from attrs ( hdf5 group , HDF5Storage Service . COMMENT ) if comment is None : comment = '' traj node . v comment = comment", "predictions": ["loads get get get get get get get get data from an try current current current current current current current current current current current current current current current current current current"], "references": ["reloads skeleton data of a tree node"], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 3239, "code": "def prm write shared array ( self , key , data , hdf5 group , full name , flag , * * kwargs ) : if flag == HDF5Storage Service . ARRAY : self . prm write into array ( key , data , hdf5 group , full name , * * kwargs ) elif flag in ( HDF5Storage Service . CARRAY , HDF5Storage Service . EARRAY , HDF5Storage Service . VLARRAY ) : self . prm write into other array ( key , data , hdf5 group , full name , flag = flag , * * kwargs ) else : raise Runtime Error ( 'Flag `%s` of hdf5 data `%s` of `%s` not understood' % ( flag , key , full name ) ) self . hdf5file . flush ( )", "predictions": ["remove the f self annotations from the . hdf5file"], "references": ["creates and array that can be used with an hdf5 array object"], "bleu": 0.08504083946364166, "rouge_l": 0.0}
{"id": 3240, "code": "def prm write shared table ( self , key , hdf5 group , fullname , * * kwargs ) : first row = None description = None if 'first row' in kwargs : first row = kwargs . pop ( 'first row' ) if not 'description' in kwargs : description = { } for colname in first row : data = first row [ colname ] column = self . all get table col ( key , [ data ] , fullname ) description [ colname ] = column if 'description' in kwargs : description = kwargs . pop ( 'description' ) if 'filters' in kwargs : filters = kwargs . pop ( 'filters' ) else : filters = self . all get filters ( kwargs ) table = self . hdf5file . create table ( where = hdf5 group , name = key , description = description , filters = filters , * * kwargs ) table . flush ( ) if first row is not None : row = table . row for key in description : row [ key ] = first row [ key ] row . append ( ) table . flush ( )", "predictions": ["write - . testing str ."], "references": ["creates a new empty table"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3241, "code": "def lnk delete link ( self , link name ) : translated name = '/' + self . trajectory name + '/' + link name . replace ( '.' , '/' ) link = self . hdf5file . get node ( where = translated name ) link . f remove ( )", "predictions": ["removes a self or self or a self or self or a self or a self or a self or a self or a self or self or a self or"], "references": ["removes a link from disk"], "bleu": 0.0513487742994337, "rouge_l": 0.12774869109947642}
{"id": 3242, "code": "def prm make description ( self , data , fullname ) : def convert lists and tuples ( series of data ) : \"\"\"Converts lists and tuples to numpy arrays\"\"\" if isinstance ( series of data [ 0 ] , ( list , tuple ) ) : for idx , item in enumerate ( series of data ) : series of data [ idx ] = np . array ( item ) descriptiondict = { } original data type dict = { } for key in data : val = data [ key ] self . all set attributes to recall natives ( val [ 0 ] , PT Item Mock ( original data type dict ) , HDF5Storage Service . FORMATTED COLUMN PREFIX % key ) convert lists and tuples ( val ) col = self . all get table col ( key , val , fullname ) descriptiondict [ key ] = col return descriptiondict , original data type dict", "predictions": ["shared data into get data structures"], "references": ["returns a description dictionary for pytables table creation"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 3243, "code": "def prm get longest stringsize ( string list ) : maxlength = 1 for stringar in string list : if isinstance ( stringar , np . ndarray ) : if stringar . ndim > 0 : for string in stringar . ravel ( ) : maxlength = max ( len ( string ) , maxlength ) else : maxlength = max ( len ( stringar . tolist ( ) ) , maxlength ) else : maxlength = max ( len ( stringar ) , maxlength ) return int ( maxlength * 1.5 )", "predictions": ["done the longest of the longest self ."], "references": ["returns the longest string size for a string entry across data ."], "bleu": 0.13755608571892394, "rouge_l": 0.28955696202531644}
{"id": 3244, "code": "def make set name ( idx ) : GROUPSIZE = 1000 set idx = idx // GROUPSIZE if set idx >= 0 : return pypetconstants . FORMATTED SET NAME % set idx else : return pypetconstants . SET NAME DUMMY", "predictions": ["returns a rep retry retry retry retry log"], "references": ["creates a run set name based on idx"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 3245, "code": "def preset ( self , name , args , kwargs ) : if self . f contains ( name , shortcuts = False ) : raise Value Error ( 'Parameter `%s` is already part of your trajectory, use the normal' 'accessing routine to change config.' % name ) else : self . changed default parameters [ name ] = ( args , kwargs )", "predictions": ["set the put routine routine"], "references": ["generic preset function marks a parameter or config for presetting ."], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 3246, "code": "def remove exploration ( self ) : for param in self . explored parameters . values ( ) : if param . stored : try : self . f delete item ( param ) except Exception : self . logger . exception ( 'Could not delete expanded parameter `%s` ' 'from disk.' % param . v full name )", "predictions": ["put all the lock in this parameter ."], "references": ["called if trajectory is expanded deletes all explored parameters from disk"], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 3247, "code": "def update run information ( self , run information dict ) : idx = run information dict [ 'idx' ] name = run information dict [ 'name' ] self . run information [ name ] = run information dict self . updated run information . add ( idx )", "predictions": ["handle handle information information information ."], "references": ["overwrites the run information of a particular run"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 3248, "code": "def add run info ( self , idx , name = '' , timestamp = 42.0 , finish timestamp = 1.337 , runtime = 'forever and ever' , time = '>>Maybe time`s gone on strike' , completed = 0 , parameter summary = 'Not yet my friend!' , short environment hexsha = 'N/A' ) : if idx in self . single run ids : old name = self . single run ids [ idx ] del self . single run ids [ old name ] del self . single run ids [ idx ] del self . run information [ old name ] if name == '' : name = self . f wildcard ( '$' , idx ) self . single run ids [ name ] = idx self . single run ids [ idx ] = name info dict = { 'idx' : idx , 'timestamp' : timestamp , 'finish timestamp' : finish timestamp , 'runtime' : runtime , 'time' : time , 'completed' : completed , 'name' : name , 'parameter summary' : parameter summary , 'short environment hexsha' : short environment hexsha } self . run information [ name ] = info dict self . length = len ( self . run information )", "predictions": ["run a run run"], "references": ["adds a new run to the _run_information dict ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 3249, "code": "def f lock parameters ( self ) : for par in self . parameters . values ( ) : if not par . f is empty ( ) : par . f lock ( )", "predictions": ["receive all data parameters"], "references": ["locks all non - empty parameters"], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 3250, "code": "def f lock derived parameters ( self ) : for par in self . derived parameters . values ( ) : if not par . f is empty ( ) : par . f lock ( )", "predictions": ["receive all derived self . self . ."], "references": ["locks all non - empty derived parameters"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 3251, "code": "def make reversed wildcards ( self , old length = - 1 ) : if len ( self . reversed wildcards ) > 0 : start = old length else : start = - 1 for wildcards , func in self . wildcard functions . items ( ) : for irun in range ( start , len ( self ) ) : translated name = func ( irun ) if not translated name in self . reversed wildcards : self . reversed wildcards [ translated name ] = ( [ ] , wildcards ) self . reversed wildcards [ translated name ] [ 0 ] . append ( irun )", "predictions": ["store reversed self service functions with their self service service service service service service service service service service ."], "references": ["creates a full mapping from all wildcard translations to the corresponding wildcards"], "bleu": 0.05415315253510895, "rouge_l": 0.0}
{"id": 3252, "code": "def merge single runs ( self , other trajectory , used runs ) : count = len ( self ) run indices = range ( len ( other trajectory ) ) run name dict = Ordered Dict ( ) to store groups with annotations = [ ] for idx in run indices : if idx in used runs : other info dict = other trajectory . f get run information ( idx ) time = other info dict [ 'time' ] timestamp = other info dict [ 'timestamp' ] completed = other info dict [ 'completed' ] short environment hexsha = other info dict [ 'short environment hexsha' ] finish timestamp = other info dict [ 'finish timestamp' ] runtime = other info dict [ 'runtime' ] new idx = used runs [ idx ] new runname = self . f wildcard ( '$' , new idx ) run name dict [ idx ] = new runname info dict = dict ( idx = new idx , time = time , timestamp = timestamp , completed = completed , short environment hexsha = short environment hexsha , finish timestamp = finish timestamp , runtime = runtime ) self . add run info ( * * info dict )", "predictions": ["store single in single ."], "references": ["updates the run_information of the current trajectory ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 3253, "code": "def rename full name ( self , full name , other trajectory , used runs = None , new run idx = None ) : split name = full name . split ( '.' ) for idx , name in enumerate ( split name ) : if name in other trajectory . reversed wildcards : run indices , wildcards = other trajectory . reversed wildcards [ name ] if new run idx is None : run idx = None for run jdx in run indices : if run jdx in used runs : run idx = used runs [ run jdx ] break elif run jdx == - 1 : run idx = - 1 break if run idx is None : raise Runtime Error ( 'You shall not pass!' ) else : run idx = new run idx new name = self . f wildcard ( wildcards [ 0 ] , run idx ) split name [ idx ] = new name full name = '.' . join ( split name ) return full name", "predictions": ["store the references in the references to the references = value = false = value"], "references": ["renames a full name based on the wildcards and a particular run"], "bleu": 0.08225964699966554, "rouge_l": 0.07558859975216851}
{"id": 3254, "code": "def make single run ( self ) : self . is run = False self . new nodes = Ordered Dict ( ) self . new links = Ordered Dict ( ) self . is run = True return self", "predictions": ["parse the config and then interpreter it"], "references": ["modifies the trajectory for single runs executed by the environment"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 3255, "code": "def set start ( self ) : init time = time . time ( ) formatted time = datetime . datetime . fromtimestamp ( init time ) . strftime ( '%Y %m %d %Hh%Mm%Ss' ) run info dict = self . run information [ self . v crun ] run info dict [ 'timestamp' ] = init time run info dict [ 'time' ] = formatted time if self . environment hexsha is not None : run info dict [ 'short environment hexsha' ] = self . environment hexsha [ 0 : 7 ]", "predictions": ["section for the option in the option { option }"], "references": ["sets the start timestamp and formatted time to the current time ."], "bleu": 0.11421946507590645, "rouge_l": 0.1788856304985337}
{"id": 3256, "code": "def set finish ( self ) : run info dict = self . run information [ self . v crun ] timestamp run = run info dict [ 'timestamp' ] run summary = self . summarize explored parameters ( ) finish timestamp run = time . time ( ) findatetime = datetime . datetime . fromtimestamp ( finish timestamp run ) startdatetime = datetime . datetime . fromtimestamp ( timestamp run ) runtime run = str ( findatetime - startdatetime ) run info dict [ 'parameter summary' ] = run summary run info dict [ 'completed' ] = 1 run info dict [ 'finish timestamp' ] = finish timestamp run run info dict [ 'runtime' ] = runtime run", "predictions": ["sets the kwargs for the kwargs to be used in the defaults"], "references": ["sets the finish time and computes the runtime in human readable format"], "bleu": 0.15537125692760353, "rouge_l": 0.3333333333333333}
{"id": 3257, "code": "def pool single run ( kwargs ) : wrap mode = kwargs [ 'wrap mode' ] traj = kwargs [ 'traj' ] traj . v storage service = pool single run . storage service if wrap mode == pypetconstants . WRAP MODE LOCAL : traj . v storage service . free references ( ) return sigint handling single run ( kwargs )", "predictions": ["create a single object for the interpret interpret ."], "references": ["starts a pool single run and passes the storage service"], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 3258, "code": "def frozen pool single run ( kwargs ) : idx = kwargs . pop ( 'idx' ) frozen kwargs = frozen pool single run . kwargs frozen kwargs . update ( kwargs ) traj = frozen kwargs [ 'traj' ] traj . f set crun ( idx ) return sigint handling single run ( frozen kwargs )", "predictions": ["create a connection to the add parameters config config config config ."], "references": ["single run wrapper for the frozen pool makes a single run and passes kwargs"], "bleu": 0.09733489823443878, "rouge_l": 0.07587064676616916}
{"id": 3259, "code": "def configure pool ( kwargs ) : pool single run . storage service = kwargs [ 'storage service' ] configure niceness ( kwargs ) configure logging ( kwargs , extract = False )", "predictions": ["overview group for the group ."], "references": ["configures the pool and keeps the storage service"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 3260, "code": "def configure frozen pool ( kwargs ) : frozen pool single run . kwargs = kwargs configure niceness ( kwargs ) configure logging ( kwargs , extract = False ) traj = kwargs [ 'traj' ] traj . v full copy = kwargs [ 'full copy' ]", "predictions": ["configure - exploration load - exploration exploration exploration ."], "references": ["configures the frozen pool and keeps all kwargs"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3261, "code": "def process single run ( kwargs ) : configure niceness ( kwargs ) configure logging ( kwargs ) result queue = kwargs [ 'result queue' ] result = sigint handling single run ( kwargs ) result queue . put ( result ) result queue . close ( )", "predictions": ["process - . the store ."], "references": ["wrapper function that first configures logging and starts a single run afterwards ."], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 3262, "code": "def scoop single run ( kwargs ) : try : try : is origin = scoop . IS ORIGIN except Attribute Error : is origin = True if not is origin : configure niceness ( kwargs ) configure logging ( kwargs ) return single run ( kwargs ) except Exception : scoop . logger . exception ( 'ERROR occurred during a single run!' ) raise", "predictions": ["wrapper for the scoop s scoop overview ."], "references": ["wrapper function for scoop that does not configure logging"], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 3263, "code": "def configure niceness ( kwargs ) : niceness = kwargs [ 'niceness' ] if niceness is not None : try : try : current = os . nice ( 0 ) if niceness - current > 0 : os . nice ( niceness - current ) except Attribute Error : psutil . Process ( ) . nice ( niceness ) except Exception as exc : sys . stderr . write ( 'Could not configure niceness because of: %s' % repr ( exc ) ) traceback . print exc ( )", "predictions": ["all the get variables from the get ."], "references": ["sets niceness of a process"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3264, "code": "def sigint handling single run ( kwargs ) : try : graceful exit = kwargs [ 'graceful exit' ] if graceful exit : sigint handling . start ( ) if sigint handling . hit : result = ( sigint handling . SIGINT , None ) else : result = single run ( kwargs ) if sigint handling . hit : result = ( sigint handling . SIGINT , result ) return result return single run ( kwargs ) except : pypet root logger = logging . get Logger ( 'pypet' ) pypet root logger . exception ( 'ERROR occurred during a single run! ' ) raise", "predictions": ["run a single sigint handling the single run ."], "references": ["wrapper that allow graceful exits of single runs"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 3265, "code": "def wrap handling ( kwargs ) : configure logging ( kwargs , extract = False ) handler = kwargs [ 'handler' ] graceful exit = kwargs [ 'graceful exit' ] if graceful exit : sigint handling . start ( ) handler . run ( )", "predictions": ["wrap the handling of the handling command line ."], "references": ["starts running a queue handler and creates a log file for the queue ."], "bleu": 0.08961856124931385, "rouge_l": 0.1673525377229081}
{"id": 3266, "code": "def f supports ( self , data ) : dtype = type ( data ) if dtype is tuple or dtype is list : if len ( data ) == 0 : return False old type = None for item in data : if not type ( item ) in pypetconstants . PARAMETER SUPPORTED DATA : return False if not old type is None and old type != type ( item ) : return False old type = type ( item ) return True elif dtype is np . ndarray or dtype is np . matrix : if data . size == 0 : return False dtype = data . dtype if np . issubdtype ( dtype , np . str ) : dtype = np . str return dtype in pypetconstants . PARAMETER SUPPORTED DATA", "predictions": ["check if data is an array of the data type ."], "references": ["checks if input data is supported by the parameter ."], "bleu": 0.17827531042796255, "rouge_l": 0.4803149606299213}
{"id": 3267, "code": "def f supports ( self , data ) : dtype = type ( data ) if dtype is tuple or dtype is list and len ( data ) == 0 : return True elif dtype is np . ndarray and data . size == 0 and data . ndim == 1 : return True else : return super ( Array Parameter , self ) . f supports ( data )", "predictions": ["check if data is a numpy array ."], "references": ["checks if input data is supported by the parameter ."], "bleu": 0.1867587389639562, "rouge_l": 0.43571428571428567}
{"id": 3268, "code": "def equal values ( self , val1 , val2 ) : if self . is supported matrix ( val1 ) : if self . is supported matrix ( val2 ) : , , hash tuple 1 = self . serialize matrix ( val1 ) , , hash tuple 2 = self . serialize matrix ( val2 ) return hash ( hash tuple 1 ) == hash ( hash tuple 2 ) else : return False else : return super ( Sparse Parameter , self ) . equal values ( val1 , val2 )", "predictions": ["compare two values of the same values ."], "references": ["matrices are equal if they hash to the same value ."], "bleu": 0.15587146574232644, "rouge_l": 0.3070469798657718}
{"id": 3269, "code": "def is supported matrix ( data ) : return ( spsp . isspmatrix csc ( data ) or spsp . isspmatrix csr ( data ) or spsp . isspmatrix bsr ( data ) or spsp . isspmatrix dia ( data ) )", "predictions": ["return true if the data is supported by the given data matrix ."], "references": ["checks if a data is csr csc bsr or dia scipy sparse matrix"], "bleu": 0.14283632578659286, "rouge_l": 0.30769230769230765}
{"id": 3270, "code": "def f translate key ( self , key ) : if isinstance ( key , int ) : if key == 0 : key = self . v name else : key = self . v name + ' %d' % key return key", "predictions": ["convert key to int key ."], "references": ["translates integer indices into the appropriate names"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 3271, "code": "def f remove ( self , * args ) : for arg in args : arg = self . f translate key ( arg ) if arg in self . data : del self . data [ arg ] else : raise Attribute Error ( 'Your result `%s` does not contain %s.' % ( self . name , arg ) )", "predictions": ["remove all arguments from the data store ."], "references": ["removes * args from the result"], "bleu": 0.21105340631872638, "rouge_l": 0.2932692307692307}
{"id": 3272, "code": "def supports ( self , item ) : if Sparse Parameter . is supported matrix ( item ) : return True else : return super ( Sparse Result , self ) . supports ( item )", "predictions": ["check if the item is supported by the given item ."], "references": ["supports everything of parent class and csr csc bsr and dia sparse matrices ."], "bleu": 0.08671803715615023, "rouge_l": 0.07830551989730423}
{"id": 3273, "code": "def store ( self ) : store dict = { } for key , val in self . data . items ( ) : store dict [ key ] = pickle . dumps ( val , protocol = self . v protocol ) store dict [ Pickle Result . PROTOCOL ] = self . v protocol return store dict", "predictions": ["save the current data to a dictionary ."], "references": ["returns a dictionary containing pickle dumps"], "bleu": 0.21105340631872638, "rouge_l": 0.2932692307692307}
{"id": 3274, "code": "def main ( ) : folder = os . getcwd ( ) print ( 'Merging all files' ) merge all in folder ( folder , delete other files = True , dynamic imports = Function Parameter , backup = False ) print ( 'Done' )", "predictions": ["merge all files in a folder"], "references": ["simply merge all trajectories in the working directory"], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 3275, "code": "def create session ( ) : ctx = saga . Context ( \"User Pass\" ) ctx . user id = USER ctx . user pass = PASSWORD session = saga . Session ( ) session . add context ( ctx ) return session", "predictions": ["create a new session ."], "references": ["creates and returns a new saga session"], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 3276, "code": "def merge trajectories ( session ) : jd = saga . job . Description ( ) jd . executable = 'python' jd . arguments = [ 'merge trajs.py' ] jd . output = \"mysagajob merge.stdout\" jd . error = \"mysagajob merge.stderr\" jd . working directory = WORKING DIR js = saga . job . Service ( 'ssh://' + ADDRESS , session = session ) myjob = js . create job ( jd ) print ( \"\\n...starting job...\\n\" ) myjob . run ( ) print ( \"Job ID    : %s\" % ( myjob . id ) ) print ( \"Job State : %s\" % ( myjob . state ) ) print ( \"\\n...waiting for job...\\n\" ) myjob . wait ( ) print ( \"Job State : %s\" % ( myjob . state ) ) print ( \"Exitcode  : %s\" % ( myjob . exit code ) )", "predictions": ["merge the job into the trajectories ."], "references": ["merges all trajectories found in the working directory"], "bleu": 0.17820132316770915, "rouge_l": 0.13174946004319654}
{"id": 3277, "code": "def start jobs ( session ) : js = saga . job . Service ( 'ssh://' + ADDRESS , session = session ) batches = range ( 3 ) jobs = [ ] for batch in batches : print ( 'Starting batch %d' % batch ) jd = saga . job . Description ( ) jd . executable = 'python' jd . arguments = [ 'the task.py --batch=' + str ( batch ) ] jd . output = \"mysagajob.stdout\" + str ( batch ) jd . error = \"mysagajob.stderr\" + str ( batch ) jd . working directory = WORKING DIR myjob = js . create job ( jd ) print ( \"Job ID    : %s\" % ( myjob . id ) ) print ( \"Job State : %s\" % ( myjob . state ) ) print ( \"\\n...starting job...\\n\" ) myjob . run ( ) jobs . append ( myjob ) for myjob in jobs : print ( \"Job ID    : %s\" % ( myjob . id ) ) print ( \"Job State : %s\" % ( myjob . state ) ) print ( \"\\n...waiting for job...\\n\" ) myjob . wait ( ) print ( \"Job State : %s\" % ( myjob . state ) ) print ( \"Exitcode  : %s\" % ( myjob . exit code ) )", "predictions": ["start all jobs ."], "references": ["starts all jobs and runs the_task . py in batches ."], "bleu": 0.0883002314431393, "rouge_l": 0.3689516129032258}
{"id": 3278, "code": "def multiply ( traj ) : z = traj . x * traj . y traj . f add result ( 'z' , z = z , comment = 'I am the product of two reals!' )", "predictions": ["add a product of two numbers ."], "references": ["sophisticated simulation of multiplication"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 3279, "code": "def add parameters ( traj ) : print ( 'Adding Parameters' ) traj . f add parameter ( 'neuron.V init' , 0.0 , comment = 'The initial condition for the ' 'membrane potential' ) traj . f add parameter ( 'neuron.I' , 0.0 , comment = 'The externally applied current.' ) traj . f add parameter ( 'neuron.tau V' , 10.0 , comment = 'The membrane time constant in milliseconds' ) traj . f add parameter ( 'neuron.tau ref' , 5.0 , comment = 'The refractory period in milliseconds ' 'where the membrane potnetial ' 'is clamped.' ) traj . f add parameter ( 'simulation.duration' , 1000.0 , comment = 'The duration of the experiment in ' 'milliseconds.' ) traj . f add parameter ( 'simulation.dt' , 0.1 , comment = 'The step size of an Euler integration step.' )", "predictions": ["add parameters to the experiment"], "references": ["adds all parameters to traj"], "bleu": 0.35930411196308426, "rouge_l": 0.4}
{"id": 3280, "code": "def add exploration ( traj ) : print ( 'Adding exploration of I and tau ref' ) explore dict = { 'neuron.I' : np . arange ( 0 , 1.01 , 0.01 ) . tolist ( ) , 'neuron.tau ref' : [ 5.0 , 7.5 , 10.0 ] } explore dict = cartesian product ( explore dict , ( 'neuron.tau ref' , 'neuron.I' ) ) traj . f explore ( explore dict )", "predictions": ["add the current product product to the dictionary"], "references": ["explores different values of i and tau_ref ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3281, "code": "def make filename ( traj ) : explored parameters = traj . f get explored parameters ( ) filename = '' for param in explored parameters . values ( ) : short name = param . v name val = param . f get ( ) filename += '%s %s ' % ( short name , str ( val ) ) return filename [ : - 2 ] + '.png'", "predictions": ["return a filename for the given traj ."], "references": ["function to create generic filenames based on what has been explored"], "bleu": 0.09268172804333874, "rouge_l": 0.0}
{"id": 3282, "code": "def main ( ) : logger = logging . get Logger ( ) folder = os . path . join ( os . getcwd ( ) , 'experiments' , 'ca patterns pypet' ) if not os . path . isdir ( folder ) : os . makedirs ( folder ) filename = os . path . join ( folder , 'all patterns.hdf5' ) env = Environment ( trajectory = 'cellular automata' , multiproc = True , ncores = 4 , wrap mode = 'QUEUE' , filename = filename , overwrite file = True ) traj = env . traj traj . par . ncells = Parameter ( 'ncells' , 400 , 'Number of cells' ) traj . par . steps = Parameter ( 'steps' , 250 , 'Number of timesteps' ) traj . par . rule number = Parameter ( 'rule number' , 30 , 'The ca rule' ) traj . par . initial name = Parameter ( 'initial name' , 'random' , 'The type of initial state' ) traj . par . seed = Parameter ( 'seed' , 100042 , 'RNG Seed' ) exp dict = { 'rule number' : [ 10 , 30 , 90 , 110 , 184 ] , 'initial name' : [ 'single' , 'random' ] , } exp dict = cartesian product ( exp dict ) traj . f explore ( exp dict ) logger . info ( 'Starting Simulation' ) env . run ( wrap automaton ) traj . f load ( load data = 2 ) logger . info ( 'Printing data' ) for idx , run name in enumerate ( traj . f iter runs ( ) ) : filename = os . path . join ( folder , make filename ( traj ) ) plot pattern ( traj . crun . pattern , traj . rule number , filename ) progressbar ( idx , len ( traj ) , logger = logger ) env . disable logging ( )", "predictions": ["main function for the script ."], "references": ["main * boilerplate * function to start simulation"], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 3283, "code": "def config from file ( filename , config = None ) : if config : try : with open ( filename , 'w' ) as fdesc : fdesc . write ( json . dumps ( config ) ) except IO Error as error : logger . exception ( error ) return False return True else : if os . path . isfile ( filename ) : try : with open ( filename , 'r' ) as fdesc : return json . loads ( fdesc . read ( ) ) except IO Error as error : return False else : return { }", "predictions": ["read the config from a json file ."], "references": ["small configuration file management function"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 3284, "code": "def request pin ( self ) : url = 'https://api.ecobee.com/authorize' params = { 'response type' : 'ecobee Pin' , 'client id' : self . api key , 'scope' : 'smart Write' } try : request = requests . get ( url , params = params ) except Request Exception : logger . warn ( \"Error connecting to Ecobee.  Possible connectivity outage.\" \"Could not request pin.\" ) return self . authorization code = request . json ( ) [ 'code' ] self . pin = request . json ( ) [ 'ecobee Pin' ] logger . error ( 'Please authorize your ecobee developer app with PIN code ' + self . pin + '\\n Goto https://www.ecobee.com/consumerportal' '/index.html, click\\n My Apps, Add application, Enter Pin' ' and click Authorize.\\n After authorizing, call request ' 'tokens() method.' )", "predictions": ["request a pin from the api ."], "references": ["method to request a pin from ecobee for authorization"], "bleu": 0.38849358632832764, "rouge_l": 0.48897795591182364}
{"id": 3285, "code": "def request tokens ( self ) : url = 'https://api.ecobee.com/token' params = { 'grant type' : 'ecobee Pin' , 'code' : self . authorization code , 'client id' : self . api key } try : request = requests . post ( url , params = params ) except Request Exception : logger . warn ( \"Error connecting to Ecobee.  Possible connectivity outage.\" \"Could not request token.\" ) return if request . status code == requests . codes . ok : self . access token = request . json ( ) [ 'access token' ] self . refresh token = request . json ( ) [ 'refresh token' ] self . write tokens to file ( ) self . pin = None else : logger . warn ( 'Error while requesting tokens from ecobee.com.' ' Status code: ' + str ( request . status code ) ) return", "predictions": ["request tokens from the api ."], "references": ["method to request api tokens from ecobee"], "bleu": 0.27960682295094563, "rouge_l": 0.45522388059701485}
{"id": 3286, "code": "def refresh tokens ( self ) : url = 'https://api.ecobee.com/token' params = { 'grant type' : 'refresh token' , 'refresh token' : self . refresh token , 'client id' : self . api key } request = requests . post ( url , params = params ) if request . status code == requests . codes . ok : self . access token = request . json ( ) [ 'access token' ] self . refresh token = request . json ( ) [ 'refresh token' ] self . write tokens to file ( ) return True else : self . request pin ( )", "predictions": ["refresh the access token ."], "references": ["method to refresh api tokens from ecobee"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 3287, "code": "def get thermostats ( self ) : url = 'https://api.ecobee.com/1/thermostat' header = { 'Content-Type' : 'application/json;charset=UTF-8' , 'Authorization' : 'Bearer ' + self . access token } params = { 'json' : ( '{\"selection\":{\"selection Type\":\"registered\",' '\"include Runtime\":\"true\",' '\"include Sensors\":\"true\",' '\"include Program\":\"true\",' '\"include Equipment Status\":\"true\",' '\"include Events\":\"true\",' '\"include Weather\":\"true\",' '\"include Settings\":\"true\"}}' ) } try : request = requests . get ( url , headers = header , params = params ) except Request Exception : logger . warn ( \"Error connecting to Ecobee.  Possible connectivity outage.\" ) return None if request . status code == requests . codes . ok : self . authenticated = True self . thermostats = request . json ( ) [ 'thermostat List' ] return self . thermostats else : self . authenticated = False logger . info ( \"Error connecting to Ecobee while attempting to get \" \"thermostat data.  Refreshing tokens and trying again.\" ) if self . refresh tokens ( ) : return self . get thermostats ( ) else : return None", "predictions": ["get the thermostats from the thermostats"], "references": ["set self . thermostats to a json list of thermostats from ecobee"], "bleu": 0.11492332782473744, "rouge_l": 0.20962199312714777}
{"id": 3288, "code": "def write tokens to file ( self ) : config = dict ( ) config [ 'API KEY' ] = self . api key config [ 'ACCESS TOKEN' ] = self . access token config [ 'REFRESH TOKEN' ] = self . refresh token config [ 'AUTHORIZATION CODE' ] = self . authorization code if self . file based config : config from file ( self . config filename , config ) else : self . config = config", "predictions": ["write tokens to file ."], "references": ["write api tokens to a file"], "bleu": 0.3342454302942773, "rouge_l": 0.7155425219941348}
{"id": 3289, "code": "def set hvac mode ( self , index , hvac mode ) : body = { \"selection\" : { \"selection Type\" : \"thermostats\" , \"selection Match\" : self . thermostats [ index ] [ 'identifier' ] } , \"thermostat\" : { \"settings\" : { \"hvac Mode\" : hvac mode } } } log msg action = \"set HVAC mode\" return self . make request ( body , log msg action )", "predictions": ["set the hvac mode ."], "references": ["possible hvac modes are auto auxheatonly cool heat off"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 3290, "code": "def set fan min on time ( self , index , fan min on time ) : body = { \"selection\" : { \"selection Type\" : \"thermostats\" , \"selection Match\" : self . thermostats [ index ] [ 'identifier' ] } , \"thermostat\" : { \"settings\" : { \"fan Min On Time\" : fan min on time } } } log msg action = \"set fan minimum on time.\" return self . make request ( body , log msg action )", "predictions": ["set fan min on time on fan"], "references": ["the minimum time in minutes to run the fan each hour . value from 1 to 60"], "bleu": 0.04926429870313275, "rouge_l": 0.1550190597204574}
{"id": 3291, "code": "def set climate hold ( self , index , climate , hold type = \"next Transition\" ) : body = { \"selection\" : { \"selection Type\" : \"thermostats\" , \"selection Match\" : self . thermostats [ index ] [ 'identifier' ] } , \"functions\" : [ { \"type\" : \"set Hold\" , \"params\" : { \"hold Type\" : hold type , \"hold Climate Ref\" : climate } } ] } log msg action = \"set climate hold\" return self . make request ( body , log msg action )", "predictions": ["set the hold hold hold ."], "references": ["set a climate hold - ie away home sleep"], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 3292, "code": "def delete vacation ( self , index , vacation ) : body = { \"selection\" : { \"selection Type\" : \"thermostats\" , \"selection Match\" : self . thermostats [ index ] [ 'identifier' ] } , \"functions\" : [ { \"type\" : \"delete Vacation\" , \"params\" : { \"name\" : vacation } } ] } log msg action = \"delete a vacation\" return self . make request ( body , log msg action )", "predictions": ["delete a vacation request"], "references": ["delete the vacation with name vacation"], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 3293, "code": "def resume program ( self , index , resume all = False ) : body = { \"selection\" : { \"selection Type\" : \"thermostats\" , \"selection Match\" : self . thermostats [ index ] [ 'identifier' ] } , \"functions\" : [ { \"type\" : \"resume Program\" , \"params\" : { \"resume All\" : resume all } } ] } log msg action = \"resume program\" return self . make request ( body , log msg action )", "predictions": ["resume a program ."], "references": ["resume currently scheduled program"], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 3294, "code": "def send message ( self , index , message = \"Hello from python-ecobee!\" ) : body = { \"selection\" : { \"selection Type\" : \"thermostats\" , \"selection Match\" : self . thermostats [ index ] [ 'identifier' ] } , \"functions\" : [ { \"type\" : \"send Message\" , \"params\" : { \"text\" : message [ 0 : 500 ] } } ] } log msg action = \"send message\" return self . make request ( body , log msg action )", "predictions": ["send a message to the server ."], "references": ["send a message to the thermostat"], "bleu": 0.6803749333171202, "rouge_l": 0.7800511508951408}
{"id": 3295, "code": "def dict self ( self ) : return { k : v for k , v in self . dict . items ( ) if k in FSM ATTRS }", "predictions": ["returns a dictionary of the dict with the current dict ."], "references": ["return the self object attributes not inherited as dict ."], "bleu": 0.16108992769687397, "rouge_l": 0.28818897637795277}
{"id": 3296, "code": "def reset ( self , iface = None , client mac = None , xid = None , scriptfile = None ) : logger . debug ( 'Reseting attributes.' ) if iface is None : iface = conf . iface if client mac is None : tempmac = get if raw hwaddr ( iface ) if isinstance ( tempmac , tuple ) and len ( tempmac ) == 2 : mac = tempmac [ 1 ] else : mac = tempmac client mac = str2mac ( mac ) self . client = DHCPCAP ( iface = iface , client mac = client mac , xid = xid ) if scriptfile is not None : self . script = Client Script ( scriptfile ) else : self . script = None self . time sent request = None self . discover attempts = 0 self . request attempts = 0 self . current state = STATE PREINIT self . offers = list ( )", "predictions": ["sigint the client and try to iface"], "references": ["reset object attributes when state is init ."], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 3297, "code": "def get timeout ( self , state , function ) : state = STATES2NAMES [ state ] for timeout fn t in self . timeout [ state ] : if timeout fn t [ 1 ] is not None and timeout fn t [ 1 ] . atmt condname == function . atmt condname : logger . debug ( 'Timeout for state %s, function %s, is %s' , state , function . atmt condname , timeout fn t [ 0 ] ) return timeout fn t [ 0 ] return None", "predictions": ["wrap the handling function to wrap the handling"], "references": ["workaround to get timeout in the atmt . timeout class method ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 3298, "code": "def set timers ( self ) : logger . debug ( 'setting timeouts' ) self . set timeout ( self . current state , self . renewing time expires , self . client . lease . renewal time ) self . set timeout ( self . current state , self . rebinding time expires , self . client . lease . rebinding time )", "predictions": ["f is the if if if not already set"], "references": ["set renewal rebinding times ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 3299, "code": "def process received nak ( self , pkt ) : if isnak ( pkt ) : logger . info ( 'DHCPNAK of %s from %s' , self . client . client ip , self . client . server ip ) return True return False", "predictions": ["f is supports supports supports supports supports supports supports 0"], "references": ["process a received nak packet ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 3300, "code": "def receive offer ( self , pkt ) : logger . debug ( \"C2. Received OFFER?, in SELECTING state.\" ) if isoffer ( pkt ) : logger . debug ( \"C2: T, OFFER received\" ) self . offers . append ( pkt ) if len ( self . offers ) >= MAX OFFERS COLLECTED : logger . debug ( \"C2.5: T, raise REQUESTING.\" ) self . select offer ( ) raise self . REQUESTING ( ) logger . debug ( \"C2.5: F, raise SELECTING.\" ) raise self . SELECTING ( )", "predictions": ["equal a values from the offers object"], "references": ["receive offer on selecting state ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3301, "code": "def receive ack requesting ( self , pkt ) : logger . debug ( \"C3. Received ACK?, in REQUESTING state.\" ) if self . process received ack ( pkt ) : logger . debug ( \"C3: T. Received ACK, in REQUESTING state, \" \"raise BOUND.\" ) raise self . BOUND ( )", "predictions": ["is the message a previously registered with the client . ."], "references": ["receive ack in requesting state ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 3302, "code": "def receive nak requesting ( self , pkt ) : logger . debug ( \"C3.1. Received NAK?, in REQUESTING state.\" ) if self . process received nak ( pkt ) : logger . debug ( \"C3.1: T. Received NAK, in REQUESTING state, \" \"raise INIT.\" ) raise self . INIT ( )", "predictions": ["f when a translate message is received ."], "references": ["receive nak in requesting state ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 3303, "code": "def receive ack renewing ( self , pkt ) : logger . debug ( \"C3. Received ACK?, in RENEWING state.\" ) if self . process received ack ( pkt ) : logger . debug ( \"C3: T. Received ACK, in RENEWING state, \" \"raise BOUND.\" ) raise self . BOUND ( )", "predictions": ["f when the remove packet has been received ."], "references": ["receive ack in renewing state ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 3304, "code": "def receive nak renewing ( self , pkt ) : logger . debug ( \"C3.1. Received NAK?, in RENEWING state.\" ) if self . process received nak ( pkt ) : logger . debug ( \"C3.1: T. Received NAK, in RENEWING state, \" \" raise INIT.\" ) raise self . INIT ( )", "predictions": ["supports the self . self . self . self . self . self . self . self . self . self . self . self . self . self . self"], "references": ["receive nak in renewing state ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 3305, "code": "def receive ack rebinding ( self , pkt ) : logger . debug ( \"C3. Received ACK?, in REBINDING state.\" ) if self . process received ack ( pkt ) : logger . debug ( \"C3: T. Received ACK, in REBINDING state, \" \"raise BOUND.\" ) raise self . BOUND ( )", "predictions": ["store the message data in the self key key key key key key key key key key key key key key key key key key key key key key key key"], "references": ["receive ack in rebinding state ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 3306, "code": "def receive nak rebinding ( self , pkt ) : logger . debug ( \"C3.1. Received NAK?, in RENEWING state.\" ) if self . process received nak ( pkt ) : logger . debug ( \"C3.1: T. Received NAK, in RENEWING state, \" \"raise INIT.\" ) raise self . INIT ( )", "predictions": ["main rebinding rebinding instance ."], "references": ["receive nak in rebinding state ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 3307, "code": "def set ( self , name , value ) : clone = self . clone ( ) if django . VERSION [ 0 ] <= 1 and django . VERSION [ 1 ] <= 4 : value = value or None clone . qsl = [ ( q , v ) for ( q , v ) in self . qsl if q != name ] if value is not None : clone . qsl . append ( ( name , value ) ) return clone", "predictions": ["create a new ctx object"], "references": ["assign a value remove if it s none"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 3308, "code": "def add ( self , name , value ) : clone = self . clone ( ) clone . qsl = [ p for p in self . qsl if not ( p [ 0 ] == name and p [ 1 ] == value ) ] clone . qsl . append ( ( name , value , ) ) return clone", "predictions": ["merge a new value with the given name job job job job job job job job job job job job job job job job job job job job job job job"], "references": ["append a value to multiple value parameter ."], "bleu": 0.04317900023606586, "rouge_l": 0.1147695202257761}
{"id": 3309, "code": "def remove ( self , name , value ) : clone = self . clone ( ) clone . qsl = [ qb for qb in self . qsl if qb != ( name , str ( value ) ) ] return clone", "predictions": ["start a new value"], "references": ["remove a value from multiple value parameter ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 3310, "code": "def read tdms ( tdms file ) : tdms file = nptdms . Tdms File ( tdms file ) ch names = [ ] ch data = [ ] for o in tdms file . objects . values ( ) : if o . data is not None and len ( o . data ) : chn = o . path . split ( '/' ) [ - 1 ] . strip ( \"'\" ) if \"unit string\" in o . properties : unit = o . properties [ \"unit string\" ] ch names . append ( \"{} [{}]\" . format ( chn , unit ) ) else : ch names . append ( chn ) ch data . append ( o . data ) return ch names , ch data", "predictions": ["multiply tdms data and unit data"], "references": ["read tdms file and return channel names and data"], "bleu": 0.1593301391270729, "rouge_l": 0.3860759493670886}
{"id": 3311, "code": "def tdms2fcs ( tdms file ) : fcs file = tdms file [ : - 4 ] + \"fcs\" chn names , data = read tdms ( tdms file ) chn names , data = add deformation ( chn names , data ) fcswrite . write fcs ( filename = fcs file , chn names = chn names , data = np . array ( data ) . transpose ( ) )", "predictions": ["writes a tdms to a tdms file"], "references": ["creates an fcs file for a given tdms file"], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 3312, "code": "def equal ( self , cwd ) : cmd = [ \"diff\" ] cmd . append ( \"-q\" ) cmd . append ( self . left . get name ( ) ) cmd . append ( self . right . get name ( ) ) try : Process ( cmd ) . run ( cwd = cwd , suppress output = True ) except Subprocess Error as e : if e . get returncode ( ) == 1 : return False else : raise e return True", "predictions": ["do a full full command"], "references": ["returns true if left and right are equal"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 3313, "code": "def backup file ( self , file , patch ) : dest dir = self . quilt pc + patch . get name ( ) file dir = file . get directory ( ) if file dir : #TODO get relative path dest dir = dest dir + file dir backup = Backup ( ) backup . backup file ( file , dest dir , copy empty = True )", "predictions": ["make a filename to the quilt"], "references": ["creates a backup of file"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 3314, "code": "def link ( self , link ) : if isinstance ( link , File ) : link = link . filename os . link ( self . filename , link )", "predictions": ["set the main main main window to the given main main main loop folder folder folder folder folder"], "references": ["create hard link as link to this file"], "bleu": 0.06809398432036522, "rouge_l": 0.08265582655826557}
{"id": 3315, "code": "def copy ( self , dest ) : if isinstance ( dest , File ) : dest dir = dest . get directory ( ) dest dir . create ( ) dest = dest . filename elif isinstance ( dest , Directory ) : dest = dest . dirname shutil . copy2 ( self . filename , dest )", "predictions": ["config a file to the destination destination"], "references": ["copy file to destination"], "bleu": 0.2626909894424158, "rouge_l": 0.5736677115987461}
{"id": 3316, "code": "def apply patch ( self , patch name , force = False , quiet = False ) : self . check ( ) patch = Patch ( patch name ) patches = self . series . patches until ( patch ) [ : ] applied = self . db . applied patches ( ) for patch in applied : if patch in patches : patches . remove ( patch ) if not patches : raise All Patches Applied ( self . series , self . db . top patch ( ) ) self . applying ( patch ) try : for cur patch in patches : self . apply patch ( cur patch , force , quiet ) finally : self . db . save ( ) self . applied ( self . db . top patch ( ) )", "predictions": ["request a pin pin"], "references": ["apply all patches up to patch_name"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 3317, "code": "def apply next patch ( self , force = False , quiet = False ) : self . check ( ) top = self . db . top patch ( ) if not top : patch = self . series . first patch ( ) else : patch = self . series . patch after ( top ) if not patch : raise All Patches Applied ( self . series , top ) self . applying ( patch ) self . apply patch ( patch , force , quiet ) self . db . save ( ) self . applied ( self . db . top patch ( ) )", "predictions": ["request tokens from the db db ."], "references": ["apply next patch in series file"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3318, "code": "def apply all ( self , force = False , quiet = False ) : self . check ( ) top = self . db . top patch ( ) if top : patches = self . series . patches after ( top ) else : patches = self . series . patches ( ) if not patches : raise All Patches Applied ( self . series , top ) try : for patch in patches : self . applying ( patch ) self . apply patch ( patch , force , quiet ) finally : self . db . save ( ) self . applied ( self . db . top patch ( ) )", "predictions": ["refresh all token after"], "references": ["apply all patches in series file"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 3319, "code": "def read ( self ) : self . patchlines = [ ] self . patch2line = dict ( ) if self . exists ( ) : with open ( self . series file , \"r\" ) as f : for line in f : self . add patch ( line )", "predictions": ["get the values from the series"], "references": ["reads all patches from the series file"], "bleu": 0.34801709319446883, "rouge_l": 0.45522388059701485}
{"id": 3320, "code": "def save ( self ) : with open ( self . series file , \"wb\" ) as f : for patchline in self . patchlines : f . write ( encode str ( str ( patchline ) ) ) f . write ( b\"\\n\" )", "predictions": ["write the current config to disk"], "references": ["saves current patches list in the series file"], "bleu": 0.17516432701748888, "rouge_l": 0.13926940639269406}
{"id": 3321, "code": "def add patch ( self , patch ) : patchline = Patch Line ( patch ) patch = patchline . get patch ( ) if patch : self . patch2line [ patch ] = patchline self . patchlines . append ( patchline )", "predictions": ["set a hvac hvac hvac"], "references": ["add a patch to the patches list"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 3322, "code": "def insert patches ( self , patches ) : patchlines = [ ] for patch name in patches : patchline = Patch Line ( patch name ) patch = patchline . get patch ( ) if patch : self . patch2line [ patch ] = patchline patchlines . append ( patchline ) patchlines . extend ( self . patchlines ) self . patchlines = patchlines", "predictions": ["set fan fan fan"], "references": ["insert list of patches at the front of the curent patches list"], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 3323, "code": "def add patches ( self , patches , after = None ) : if after is None : self . insert patches ( patches ) else : self . check patch ( after ) patchlines = self . patchlines before ( after ) patchlines . append ( self . patch2line [ after ] ) for patch in patches : patchline = Patch Line ( patch ) patchlines . append ( patchline ) self . patch2line [ patchline . get patch ( ) ] = patchline patchlines . extend ( self . patchlines after ( after ) ) self . patchlines = patchlines", "predictions": ["set climate to the patchlines list"], "references": ["add a list of patches to the patches list"], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 3324, "code": "def remove patch ( self , patch ) : self . check patch ( patch ) patchline = self . patch2line [ patch ] del self . patch2line [ patch ] self . patchlines . remove ( patchline )", "predictions": ["delete a vacation vacation"], "references": ["remove a patch from the patches list"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 3325, "code": "def patches after ( self , patch ) : return [ line . get patch ( ) for line in self . patchlines after ( patch ) if line . get patch ( ) ]", "predictions": ["return - . program program program program program body body body body body body body body body body body body body body body body body body body body body body body"], "references": ["returns a list of patches after patch from the patches list"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3326, "code": "def patches before ( self , patch ) : return [ line . get patch ( ) for line in self . patchlines before ( patch ) if line . get patch ( ) ]", "predictions": ["from all send message message message message message message message . . . ."], "references": ["returns a list of patches before patch from the patches list"], "bleu": 0.08839374326825923, "rouge_l": 0.08176943699731902}
{"id": 3327, "code": "def create ( self ) : if not os . path . exists ( self . dirname ) : os . makedirs ( self . dirname ) self . create version ( self . version file )", "predictions": ["dict to dict v v v"], "references": ["creates the dirname and inserts a . version file"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 3328, "code": "def import patches ( self , patches ) : dest dir = self . quilt patches patch names = [ ] for patch in patches : patch name = os . path . basename ( patch ) patch file = File ( patch ) dest file = dest dir + File ( patch name ) patch file . copy ( dest file ) patch names . append ( patch name ) self . import patches ( patch names )", "predictions": ["import all patches of the patches patches"], "references": ["import several patches into the patch queue"], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 3329, "code": "def way ( self , w ) : if w . id not in self . way ids : return way points = [ ] for n in w . nodes : try : way points . append ( Point ( n . location . lon , n . location . lat ) ) except o . Invalid Location Error : logging . debug ( 'Invalid Location Error at way %s node %s' , w . id , n . ref ) self . ways [ w . id ] = Way ( w . id , way points )", "predictions": ["return true if the node is way to be way to the way ."], "references": ["process each way ."], "bleu": 0.11633270842295028, "rouge_l": 0.2469635627530364}
{"id": 3330, "code": "def missing node ids ( self ) : present node ids = self . nodes . keys ( ) for nid in self . node ids : if nid not in present node ids : yield nid", "predictions": ["return an iterator over all missing node ids ."], "references": ["get a list of nodes not found in osm data ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 3331, "code": "def node ( self , n ) : if n . id not in self . node ids : return try : self . nodes [ n . id ] = Node ( n . id , n . location . lon , n . location . lat , { t . k : t . v for t in n . tags } ) except o . Invalid Location Error : logging . debug ( 'Invalid Location Error at node %s' , n . id )", "predictions": ["return the node s node s n - 1 ."], "references": ["process each node ."], "bleu": 0.13950796967929133, "rouge_l": 0.3096446700507614}
{"id": 3332, "code": "def build route ( relation ) : if relation . tags . get ( 'type' ) != 'route' : return short name = create route short name ( relation ) color = relation . tags . get ( 'color' ) return Route ( relation . id , short name , create route long name ( relation , short name ) , map osm route type to gtfs ( relation . tags . get ( 'route' ) ) , 'https://www.openstreetmap.org/relation/{}' . format ( relation . id ) , color . strip ( '#' ) if color else '' , get agency id ( relation ) )", "predictions": ["build a route route from a relation relation ."], "references": ["extract information of one route ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 3333, "code": "def create route long name ( relation , short name ) : if relation . tags . get ( 'from' ) and relation . tags . get ( 'to' ) : return \"{0}-to-{1}\" . format ( relation . tags . get ( 'from' ) , relation . tags . get ( 'to' ) ) name = relation . tags . get ( 'name' ) or relation . tags . get ( 'alt name' ) or \"OSM Route No. {}\" . format ( relation . id ) if short name and name . startswith ( short name ) : return name [ len ( short name ) : ] return name", "predictions": ["create a route name from the relation"], "references": ["create a meaningful route name ."], "bleu": 0.3073940764756322, "rouge_l": 0.6240409207161125}
{"id": 3334, "code": "def get agency id ( relation ) : op = relation . tags . get ( 'operator' ) if op : return int ( hashlib . sha256 ( op . encode ( 'utf-8' ) ) . hexdigest ( ) , 16 ) % 10 ** 8 return - 1", "predictions": ["get the agency id of the given agency ."], "references": ["construct an id for agency using its tags ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 3335, "code": "def process ( self ) : self . rh = Relation Handler ( ) self . rh . apply file ( self . filename ) logging . debug ( 'Found %d public transport relations.' , len ( self . rh . relations ) ) node ids , stop node ids , way ids , reverse map = self . collect ids ( ) self . nh = Node Handler ( node ids ) self . nh . apply file ( self . filename , locations = True ) count = 0 for idx , missing node id in enumerate ( self . nh . missing node ids ) : count += 1 logging . warning ( '[no data] missing stop node. rel: https://osm.org/relation/%s node: https://osm.org/node/%s.' , reverse map [ missing node id ] , missing node id ) if count : logging . warning ( '%d nodes that appear in relations are missing.' , count ) else : logging . debug ( 'Lucky you! All relation member nodes were found.' ) self . wh = Way Handler ( way ids ) self . wh . apply file ( self . filename , locations = True )", "predictions": ["process public public transport ."], "references": ["process the files and collect necessary data ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 3336, "code": "def relation ( self , rel ) : rel type = rel . tags . get ( 'type' ) if any ( [ rel . deleted , not rel . visible , not self . is new version ( rel ) , rel type not in [ 'route' , 'public transport' ] ] ) : return route tag = rel . tags . get ( 'route' ) if rel type == 'route' and route tag not in self . transit route types : return public transport = rel . tags . get ( 'public transport' ) if rel type == 'public transport' and public transport != 'stop area' : return self . relations [ rel . id ] = Relation ( rel . id , { 'type' : rel type , 'public transport' : public transport , 'route' : route tag , 'operator' : rel . tags . get ( 'operator' ) , 'color' : rel . tags . get ( 'color' ) , 'ref' : rel . tags . get ( 'ref' ) , 'from' : rel . tags . get ( 'from' ) , 'to' : rel . tags . get ( 'to' ) , 'name' : rel . tags . get ( 'name' ) , 'alt name' : rel . tags . get ( 'alt name' ) , 'url' : rel . tags . get ( 'url' ) , 'contact website' : rel . tags . get ( 'contact:website' ) } , [ ( member . type , member . ref , member . role ) for member in rel . members ] ) self . versions [ rel . id ] = rel . version", "predictions": ["add a public relation to the current item"], "references": ["process each relation ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 3337, "code": "def patch agencies ( agencies ) : yield Agency ( - 1 , 'http://hiposfer.com' , 'Unknown agency' , 'Europe/Berlin' ) for agency id , agency url , agency name , agency timezone in agencies : if not agency url : agency url = 'http://hiposfer.com' if not agency timezone : agency timezone = 'Europe/Berlin' yield Agency ( agency id , agency url , agency name , agency timezone )", "predictions": ["patch agency agency agency agency agency agency"], "references": ["fill the fields that are necessary for passing transitfeed checks ."], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 3338, "code": "def create dummy trip stoptimes ( trip id , stops , first service time ) : waiting = datetime . timedelta ( seconds = 30 ) arrival = first service time last departure = first service time last departure hour = ( arrival + waiting ) . hour last stop = None departure hour = None arrival hour = None for stop sequence , stop in enumerate ( stops ) : arrival = last departure + get time from last stop ( last stop , stop ) departure = arrival + waiting if arrival . hour < last departure hour : diff = last departure hour arrival hour = arrival . hour + diff departure hour = departure . hour + diff last departure hour = departure . hour + diff else : arrival hour = arrival . hour departure hour = departure . hour last departure hour = departure . hour if departure . hour < arrival . hour : diff = last departure hour departure hour = departure . hour + diff last departure hour = departure . hour + diff yield { 'trip id' : trip id , 'arrival time' : '{:02}:{}' . format ( arrival hour , arrival . strftime ( '%M:%S' ) ) , 'departure time' : '{:02}:{}' . format ( departure hour , departure . strftime ( '%M:%S' ) ) , 'stop id' : stop . stop id , 'stop sequence' : stop sequence } last stop = stop last departure = departure", "predictions": ["create dummy trip stoptimes for the given trip ."], "references": ["create station stop times for each trip ."], "bleu": 0.21105340631872635, "rouge_l": 0.4756335282651072}
{"id": 3339, "code": "def write zipped ( self , filepath ) : with zipfile . Zip File ( filepath , mode = 'w' , compression = zipfile . ZIP DEFLATED ) as zfile : for name , buffer in self . buffers . items ( ) : encoded values = io . Bytes IO ( buffer . getvalue ( ) . encode ( 'utf-8' ) ) zfile . writestr ( '{}.txt' . format ( name ) , encoded values . getbuffer ( ) ) for name , path in self . files . items ( ) : zfile . write ( path , arcname = name )", "predictions": ["write the getbuffer to a file ."], "references": ["write the gtfs feed in the given file ."], "bleu": 0.23099966849728554, "rouge_l": 0.48897795591182364}
{"id": 3340, "code": "def write unzipped ( self , destination ) : for name , buffer in self . buffers . items ( ) : with open ( os . path . join ( destination , '{}.txt' . format ( name ) ) , 'w' , encoding = 'utf-8' ) as file : file . write ( buffer . getvalue ( ) ) for name , path in self . files . items ( ) : shutil . copy ( path , os . path . join ( destination , name ) )", "predictions": ["write the contents of the given destination to the given destination ."], "references": ["write gtfs text files in the given path ."], "bleu": 0.15537125692760353, "rouge_l": 0.39102564102564097}
{"id": 3341, "code": "def build agency ( relation , nodes ) : # op = relation . tags . get ( 'operator' ) agency url = relation . tags . get ( 'url' ) or relation . tags . get ( 'contact website' ) if not op : return agency id = int ( hashlib . sha256 ( op . encode ( 'utf8' ) ) . hexdigest ( ) , 16 ) % 10 ** 8 return Agency ( agency id , agency url , op , '' )", "predictions": ["build the agency from the agency"], "references": ["extract agency information ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 3342, "code": "def extract stops ( relation , nodes , visited stop ids , stop to station map ) : for member type , member id , member role in relation . member info : if member id not in visited stop ids and member id in nodes and member role in ( 'stop' , 'halt' ) : location type = '' visited stop ids . add ( member id ) yield Stop ( member id , nodes [ member id ] . tags . get ( 'name' ) or \"Unnamed {} stop.\" . format ( relation . tags . get ( 'route' ) ) , nodes [ member id ] . lon if member id in nodes else '' , nodes [ member id ] . lat if member id in nodes else '' , relation . id , map wheelchair ( nodes [ member id ] . tags . get ( 'wheelchair' ) ) , location type , stop to station map . get ( member id , '' ) )", "predictions": ["extract stops from nodes"], "references": ["extract stops in a relation ."], "bleu": 0.2868106410131918, "rouge_l": 0.3860759493670886}
{"id": 3343, "code": "def build shape ( relation , nodes , ways ) : sequence index = 0 for member type , member id , member role in relation . member info : if member id in nodes : yield Shape ( relation . id , nodes [ member id ] . lat , nodes [ member id ] . lon , sequence index ) sequence index += 1 elif member id in ways : continue else : pass", "predictions": ["build the shape of a member of the given relation ."], "references": ["extract shape of one route ."], "bleu": 0.16108992769687397, "rouge_l": 0.3727087576374745}
{"id": 3344, "code": "def get supported versions ( self ) : if not hasattr ( self , ' versions' ) : try : self . versions = [ self . send apdu ( INS GET VERSION ) . decode ( ) ] except exc . APDU Error as e : self . versions = [ 'v0' ] if e . code == 0x6d00 else [ ] return self . versions", "predictions": ["return the supported versions of the supported versions ."], "references": ["gets a list of supported u2f versions from the device ."], "bleu": 0.14873743701255318, "rouge_l": 0.3929146537842191}
{"id": 3345, "code": "def send apdu ( self , ins , p1 = 0 , p2 = 0 , data = b'' ) : if data is None : data = b'' elif isinstance ( data , int ) : data = int2byte ( data ) size = len ( data ) l0 = size >> 16 & 0xff l1 = size >> 8 & 0xff l2 = size & 0xff apdu data = struct . pack ( 'B B B B B B B %is B B' % size , 0 , ins , p1 , p2 , l0 , l1 , l2 , data , 0x00 , 0x00 ) try : resp = self . do send apdu ( apdu data ) except Exception as e : raise exc . Device Error ( e ) status = struct . unpack ( '>H' , resp [ - 2 : ] ) [ 0 ] data = resp [ : - 2 ] if status != APDU OK : raise exc . APDU Error ( status ) return data", "predictions": ["send a apdu to the client"], "references": ["sends an apdu to the device and waits for a response ."], "bleu": 0.1599248714366856, "rouge_l": 0.31443298969072164}
{"id": 3346, "code": "def register ( devices , params , facet ) : for device in devices [ : ] : try : device . open ( ) except : devices . remove ( device ) sys . stderr . write ( '\\n Touch the U2F device you wish to register...\\n' ) try : while devices : removed = [ ] for device in devices : try : return u2f . register ( device , params , facet ) except exc . APDU Error as e : if e . code == APDU USE NOT SATISFIED : pass else : removed . append ( device ) except exc . Device Error : removed . append ( device ) devices = [ d for d in devices if d not in removed ] for d in removed : d . close ( ) time . sleep ( 0.25 ) finally : for device in devices : device . close ( ) sys . stderr . write ( '\\n Unable to register with any U2F device.\\n' ) sys . exit ( 1 )", "predictions": ["register the device with the given facet ."], "references": ["interactively registers a single u2f device given the registrationrequest ."], "bleu": 0.157044754112095, "rouge_l": 0.3267857142857143}
{"id": 3347, "code": "def u2str ( data ) : if isinstance ( data , dict ) : return { u2str ( k ) : u2str ( v ) for k , v in data . items ( ) } elif isinstance ( data , list ) : return [ u2str ( x ) for x in data ] elif isinstance ( data , text type ) : return data . encode ( 'utf-8' ) else : return data", "predictions": ["convert data into json"], "references": ["recursively converts unicode objects to utf - 8 encoded byte strings ."], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 3348, "code": "def wrap function ( func = None , error threshold = None , reraise exception = True , save current stack trace = True ) : # if func : return flawless . client . client . wrap function with error decorator ( func = func , error threshold = error threshold , reraise exception = reraise exception , save current stack trace = save current stack trace ) else : return functools . partial ( flawless . client . client . wrap function with error decorator , error threshold = error threshold , reraise exception = reraise exception , save current stack trace = save current stack trace )", "predictions": ["wrap the function to wrap it in the stack ."], "references": ["wraps a function with reporting to errors backend"], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 3349, "code": "def get entry ( self , entry , entry tree ) : for e in entry tree [ entry . filename ] : if entry == e : return e", "predictions": ["get the entry from the given entry ."], "references": ["helper function for retrieving a particular entry from the prefix trees"], "bleu": 0.20513838542429053, "rouge_l": 0.3070469798657718}
{"id": 3350, "code": "def markdown to re ST ( text ) : text = re . sub ( pattern = r\"\\n       (\\w+) - (.+)\\n\" , repl = r\"\\n\\n       *\\g<1>* - \\g<2>\\n\" , string = text ) text = re . sub ( pattern = r\"\\[([^\\]]+)\\]\\([^)]+\\)\" , repl = r\"\\g<1>\" , string = text ) text = re . sub ( pattern = r\"\\n(\\d+). \" , repl = r\"\\n\\\\\\g<1>. \" , string = text ) return text", "predictions": ["convert markdown to re - st ."], "references": ["this is not a general purpose converter . only converts this readme"], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 3351, "code": "def record error ( hostname , exc info , preceding stack = None , error threshold = None , additional info = None ) : stack = [ ] exc type , exc value , sys traceback = exc info while sys traceback is not None : stack . append ( sys traceback ) sys traceback = sys traceback . tb next stack lines = [ ] for row in preceding stack or [ ] : stack lines . append ( api ttypes . Stack Line ( filename = os . path . abspath ( row [ 0 ] ) , line number = row [ 1 ] , function name = row [ 2 ] , text = row [ 3 ] ) ) for index , tb in enumerate ( stack ) : filename = tb . tb frame . f code . co filename func name = tb . tb frame . f code . co name lineno = tb . tb lineno line = linecache . getline ( filename , lineno , tb . tb frame . f globals ) frame locals = None if index >= ( len ( stack ) - NUM FRAMES TO SAVE ) : frame locals = dict ( ( k , myrepr ( k , v ) ) for k , v in list ( tb . tb frame . f locals . items ( ) ) [ : MAX LOCALS ] if k != \"self\" ) if \"self\" in tb . tb frame . f locals and hasattr ( tb . tb frame . f locals [ \"self\" ] , \" dict \" ) : frame locals . update ( dict ( ( \"self.\" + k , myrepr ( k , v ) ) for k , v in list ( tb . tb frame . f locals [ \"self\" ] . dict . items ( ) ) [ : MAX LOCALS ] if k != \"self\" ) ) stack lines . append ( api ttypes . Stack Line ( filename = os . path . abspath ( filename ) , line number = lineno , function name = func name , text = line , frame locals = frame locals ) ) key = Cached Error Info . get hash key ( stack lines ) info = ERROR CACHE . get ( key ) or Cached Error Info ( ) info . increment ( ) ERROR CACHE [ key ] = info if info . should report ( ) : error count = info . mark reported ( ) send request ( api ttypes . Record Error Request ( traceback = stack lines , exception message = repr ( exc value ) , exception type = exc type . module + \".\" + exc type . name , hostname = hostname , error threshold = error threshold , additional info = additional info , error count = error count , ) )", "predictions": ["return a record of the error traceback ."], "references": ["helper function to record errors to the flawless backend"], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 3352, "code": "def url to image ( url ) : r = requests . get ( url ) image = String IO ( r . content ) return image", "predictions": ["return the image of a url"], "references": ["fetch an image from url and convert it into a pillow image object"], "bleu": 0.08180282100568384, "rouge_l": 0.19741100323624597}
{"id": 3353, "code": "def string to image ( image string ) : image filelike = String IO ( image string ) image = Image . open ( image filelike ) return image", "predictions": ["convert an image to an image"], "references": ["convert string datas into a pillow image object"], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 3354, "code": "def is big enough ( image , size ) : if ( size [ 0 ] > image . size [ 0 ] ) and ( size [ 1 ] > image . size [ 1 ] ) : raise Image Size Error ( image . size , size )", "predictions": ["check if image is a enough enough enough ."], "references": ["check that the image s size superior to size"], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 3355, "code": "def width is big enough ( image , width ) : if width > image . size [ 0 ] : raise Image Size Error ( image . size [ 0 ] , width )", "predictions": ["check if image is big big enough enough ."], "references": ["check that the image width is superior to width"], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 3356, "code": "def height is big enough ( image , height ) : if height > image . size [ 1 ] : raise Image Size Error ( image . size [ 1 ] , height )", "predictions": ["check if image height is big big enough ."], "references": ["check that the image height is superior to height"], "bleu": 0.2777619034011791, "rouge_l": 0.4444444444444444}
{"id": 3357, "code": "def parse category ( self , item , field name , source name ) : slug = category map . get ( self . get value ( item , source name ) , None ) if not slug : return None try : return Category . objects . get ( slug = slug ) except Category . Does Not Exist : pass", "predictions": ["parse a category field ."], "references": ["converts the text category to a tasks . category instance ."], "bleu": 0.09778809693469985, "rouge_l": 0.35124760076775424}
{"id": 3358, "code": "def parse totals ( self , item , field name , source name ) : val = self . get value ( item , source name ) try : return int ( val ) except : return 0", "predictions": ["parse the value of the field ."], "references": ["parse numeric fields ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 3359, "code": "def get items ( self ) : for event , item in Element Tree . iterparse ( self . source ) : if item . tag == self . item tag name : yield item item . clear ( )", "predictions": ["return an iterator over all items in the source ."], "references": ["iterator of the list of items in the xml source ."], "bleu": 0.26238334426718246, "rouge_l": 0.56656346749226}
{"id": 3360, "code": "def save error ( self , data , exception info ) : self . errors . append ( { 'data' : data , 'exception' : '' . join ( format exception ( * exception info ) ) , } )", "predictions": ["import an patches patches"], "references": ["saves an error in the error list ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 3361, "code": "def parse ( self ) : if not self . loaded : self . load ( self . source ) for item in self . get items ( ) : data = self . parse item ( item ) instance = self . get instance ( data ) self . feed instance ( data , instance ) try : self . save item ( item , data , instance ) except Exception as e : self . save error ( data , sys . exc info ( ) ) self . unload ( )", "predictions": ["way to way to way if necessary"], "references": ["parses all data from the source saving model instances ."], "bleu": 0.10175282441454783, "rouge_l": 0.0}
{"id": 3362, "code": "def parse item ( self , item ) : parsed data = { } for field name in self . fields : source name = self . field map . get ( field name , field name ) parse = getattr ( self , 'parse %s' % field name , None ) if parse : value = parse ( item , field name , source name ) else : value = self . get value ( item , source name ) parsed data [ field name ] = value return parsed data", "predictions": ["missing method for parsing node data"], "references": ["receives an item and returns a dictionary of field values ."], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 3363, "code": "def get instance ( self , data ) : unique fields = self . unique fields if not unique fields : return self . model ( ) filter = dict ( [ ( f , data [ f ] ) for f in unique fields ] ) try : instance = self . model . default manager . get ( * * filter ) except self . model . Does Not Exist : return self . model ( ) return instance", "predictions": ["try to get ."], "references": ["get an item from the database or an empty one if not found ."], "bleu": 0.032639898338235177, "rouge_l": 0.20198675496688742}
{"id": 3364, "code": "def save item ( self , item , data , instance , commit = True ) : if commit : instance . save ( ) return instance", "predictions": ["build an route object"], "references": ["saves a model instance to the database ."], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 3365, "code": "def load ( self , source ) : self . source = open ( self . source , 'rb' ) self . loaded = True", "predictions": ["create the source from the source"], "references": ["opens the source file ."], "bleu": 0.2907153684841096, "rouge_l": 0.3696969696969697}
{"id": 3366, "code": "def get items ( self ) : reader = csv . reader ( self . source ) headers = reader . next ( ) for row in reader : if not row : continue yield dict ( zip ( headers , row ) )", "predictions": ["returns a list of agency agency from the = value ."], "references": ["iterator to read the rows of the csv file ."], "bleu": 0.1354599427337814, "rouge_l": 0.28818897637795277}
{"id": 3367, "code": "def allow network access ( self , value : bool ) : if self . is running : raise Value Error ( \"Cannot change network access settings on a running sandbox\" ) self . allow network access = value", "predictions": ["process self self file self file self file file self file file file self file file file self file self file self file self file self file self file self file"], "references": ["raises valueerror if this sandbox instance is currently running ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3368, "code": "def get enrollments for course by sis id ( self , sis course id , params = { } ) : return self . get enrollments for course ( self . sis id ( sis course id , sis field = \"course\" ) , params )", "predictions": ["returns a list of enrollments self self not self not found by sis"], "references": ["return a list of all enrollments for the passed course sis id ."], "bleu": 0.19674979811155635, "rouge_l": 0.38461538461538464}
{"id": 3369, "code": "def get enrollments for section by sis id ( self , sis section id , params = { } ) : return self . get enrollments for section ( self . sis id ( sis section id , sis field = \"section\" ) , params )", "predictions": ["returns all agencies for for for for for the given yield section"], "references": ["return a list of all enrollments for the passed section sis id ."], "bleu": 0.14294845713017917, "rouge_l": 0.31770833333333337}
{"id": 3370, "code": "def get roles by account sis id ( self , account sis id , params = { } ) : return self . get roles in account ( self . sis id ( account sis id , sis field = \"account\" ) , params )", "predictions": ["create dummy dummy dummy dummy dummy sis trip ."], "references": ["list the roles for an account for the passed account sis id ."], "bleu": 0.10015045110931886, "rouge_l": 0.17604617604617603}
{"id": 3371, "code": "def get role by account sis id ( self , account sis id , role id ) : return self . get role ( self . sis id ( account sis id , sis field = \"account\" ) , role id )", "predictions": ["write a zipped zipped by its self sis"], "references": ["get information about a single role for the passed account sis id ."], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 3372, "code": "def get course by sis id ( self , sis course id , params = { } ) : return self . get course ( self . sis id ( sis course id , sis field = \"course\" ) , params )", "predictions": ["write a unzipped by instance by self with the given self with the given self id"], "references": ["return course resource for given sis id ."], "bleu": 0.08513012360883544, "rouge_l": 0.17732558139534885}
{"id": 3373, "code": "def get courses in account by sis id ( self , sis account id , params = { } ) : return self . get courses in account ( self . sis id ( sis account id , sis field = \"account\" ) , params )", "predictions": ["build list of agency in in in for the given nodes or nodes or nodes or = nodes"], "references": ["return a list of courses for the passed account sis id ."], "bleu": 0.11268706361337427, "rouge_l": 0.27664399092970515}
{"id": 3374, "code": "def get published courses in account ( self , account id , params = { } ) : params [ \"published\" ] = True return self . get courses in account ( account id , params )", "predictions": ["extract stops relation relation relation for the given account"], "references": ["return a list of published courses for the passed account id ."], "bleu": 0.1430210741102858, "rouge_l": 0.2785388127853881}
{"id": 3375, "code": "def get published courses in account by sis id ( self , sis account id , params = { } ) : return self . get published courses in account ( self . sis id ( sis account id , sis field = \"account\" ) , params )", "predictions": ["returns the shape of the shape relation relation in the given ways in the given ways in the given ways in the given ways list"], "references": ["return a list of published courses for the passed account sis id ."], "bleu": 0.057783239927083445, "rouge_l": 0.11161939615736508}
{"id": 3376, "code": "def get users for course ( self , course id , params = { } ) : url = COURSES API . format ( course id ) + \"/users\" data = self . get paged resource ( url , params = params ) users = [ ] for datum in data : users . append ( Canvas User ( data = datum ) ) return users", "predictions": ["get all supported supported supported supported by course"], "references": ["returns a list of users for the given course id ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 3377, "code": "def get users for sis course id ( self , sis course id , params = { } ) : return self . get users for course ( self . sis id ( sis course id , sis field = \"course\" ) , params )", "predictions": ["send users for for to all users"], "references": ["returns a list of users for the given sis course id ."], "bleu": 0.11967409389919142, "rouge_l": 0.20098846787479407}
{"id": 3378, "code": "def next page ( self , response ) : for link in response . getheader ( \"link\" , \"\" ) . split ( \",\" ) : try : ( url , rel ) = link . split ( \";\" ) if \"next\" in rel : return url . lstrip ( \"<\" ) . rstrip ( \">\" ) except Exception : return", "predictions": ["return is the register of the facet"], "references": ["return url path to next page of paginated data"], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 3379, "code": "def get resource ( self , url , params = None , data key = None ) : if not params : params = { } self . set as user ( params ) full url = url + self . params ( params ) return self . get resource url ( full url , True , data key )", "predictions": ["get a resource instance"], "references": ["canvas get method . return representation of the requested resource ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 3380, "code": "def put resource ( self , url , body ) : params = { } self . set as user ( params ) headers = { 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } url = url + self . params ( params ) response = DAO . put URL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 201 or response . status == 204 ) : raise Data Failure Exception ( url , response . status , response . data ) return json . loads ( response . data )", "predictions": ["do a function to the actual request"], "references": ["canvas put method ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3381, "code": "def post resource ( self , url , body ) : params = { } self . set as user ( params ) headers = { 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } url = url + self . params ( params ) response = DAO . post URL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 204 ) : raise Data Failure Exception ( url , response . status , response . data ) return json . loads ( response . data )", "predictions": ["do a get request"], "references": ["canvas post method ."], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 3382, "code": "def delete resource ( self , url ) : params = { } self . set as user ( params ) headers = { 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } url = url + self . params ( params ) response = DAO . delete URL ( url , headers ) if not ( response . status == 200 or response . status == 204 ) : raise Data Failure Exception ( url , response . status , response . data ) return response", "predictions": ["do a to markdown to the given text"], "references": ["canvas delete method ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3383, "code": "def create admin by sis id ( self , sis account id , user id , role ) : return self . create admin ( self . sis id ( sis account id ) , user id , role )", "predictions": ["record the error by for the given hostname ."], "references": ["flag an existing user as an admin within the account sis id ."], "bleu": 0.10015045110931886, "rouge_l": 0.17604617604617603}
{"id": 3384, "code": "def delete admin by sis id ( self , sis account id , user id , role ) : return self . delete admin ( self . sis id ( sis account id ) , user id , role )", "predictions": ["url to the specified to the specified sis return the to the specified sis return the new get the to the to the to the to the to the to the"], "references": ["remove an account admin role from a user for the account sis id ."], "bleu": 0.04317900023606586, "rouge_l": 0.09538702111024237}
{"id": 3385, "code": "def get section by sis id ( self , sis section id , params = { } ) : return self . get section ( self . sis id ( sis section id , sis field = \"section\" ) , params )", "predictions": ["string image by sis"], "references": ["return section resource for given sis id ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 3386, "code": "def get sections in course by sis id ( self , sis course id , params = { } ) : return self . get sections in course ( self . sis id ( sis course id , sis field = \"course\" ) , params )", "predictions": ["is the course enough enough enough enough"], "references": ["return list of sections for the passed course sis id ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 3387, "code": "def get sections with students in course ( self , course id , params = { } ) : include = params . get ( \"include\" , [ ] ) if \"students\" not in include : include . append ( \"students\" ) params [ \"include\" ] = include return self . get sections in course ( course id , params )", "predictions": ["returns all is is big big a image"], "references": ["return list of sections including students for the passed course id ."], "bleu": 0.08179133792443427, "rouge_l": 0.0}
{"id": 3388, "code": "def get sections with students in course by sis id ( self , sis course id , params = { } ) : return self . get sections with students in course ( self . sis id ( sis course id , sis field = \"course\" ) , params )", "predictions": ["height of all is big big sis"], "references": ["return list of sections including students for the passed sis id ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 3389, "code": "def get term by sis id ( self , sis term id ) : for term in self . get all terms ( ) : if term . sis term id == sis term id : return term", "predictions": ["return - . category by self id"], "references": ["return a term resource for the passed sis id ."], "bleu": 0.14390022429682173, "rouge_l": 0.22803738317757008}
{"id": 3390, "code": "def build archive ( self , dir path ) : zip path = os . path . join ( dir path , \"import.zip\" ) archive = zipfile . Zip File ( zip path , \"w\" ) for filename in CSV FILES : filepath = os . path . join ( dir path , filename ) if os . path . exists ( filepath ) : archive . write ( filepath , filename , zipfile . ZIP DEFLATED ) archive . close ( ) with open ( zip path , \"rb\" ) as f : body = f . read ( ) return body", "predictions": ["parse the zip and return the body body"], "references": ["creates a zip archive from files in path ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 3391, "code": "def get report data ( self , report ) : if report . report id is None or report . status is None : raise Report Failure Exception ( report ) interval = getattr ( settings , 'CANVAS REPORT POLLING INTERVAL' , 5 ) while report . status != \"complete\" : if report . status == \"error\" : raise Report Failure Exception ( report ) sleep ( interval ) report = self . get report status ( report ) if report . attachment is None or report . attachment . url is None : return data = self . get report file ( report . attachment . url ) return data . split ( \"\\n\" )", "predictions": ["returns the items in the items in the items item"], "references": ["returns a completed report as a list of csv strings ."], "bleu": 0.11406351620367239, "rouge_l": 0.09442724458204334}
{"id": 3392, "code": "def empty value ( self ) : edit empty value = self . config . get ( 'edit empty value' , False ) if edit empty value : return edit empty value else : return unicode ( inplace settings . INPLACEEDIT EDIT EMPTY VALUE )", "predictions": ["edit the current value of the field ."], "references": ["get the text to display when the field is empty ."], "bleu": 0.16481400866629634, "rouge_l": 0.4093959731543625}
{"id": 3393, "code": "def create metrics ( self , metric configs : Iterable [ Metric Config ] ) -> Dict [ str , Metric ] : return self . registry . create metrics ( metric configs )", "predictions": ["create metrics for a metric ."], "references": ["create and register metrics from a list of metricconfigs ."], "bleu": 0.14260771622124252, "rouge_l": 0.47843137254901963}
{"id": 3394, "code": "def configure registry ( self , include process stats : bool = False ) : if include process stats : self . registry . register additional collector ( Process Collector ( registry = None ) )", "predictions": ["configure the collector registry ."], "references": ["configure the metricregistry ."], "bleu": 0.3860973950960897, "rouge_l": 0.6802973977695167}
{"id": 3395, "code": "def create metrics ( self , configs : Iterable [ Metric Config ] ) -> Dict [ str , Metric ] : metrics : Dict [ str , Metric ] = { config . name : self . register metric ( config ) for config in configs } self . metrics . update ( metrics ) return metrics", "predictions": ["create metrics for this metric"], "references": ["create prometheus metrics from a list of metricconfigs ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 3396, "code": "def get metric ( self , name : str , labels : Union [ Dict [ str , str ] , None ] = None ) -> Metric : metric = self . metrics [ name ] if labels : return metric . labels ( * * labels ) return metric", "predictions": ["get metric metric metric labels ."], "references": ["return a metric optionally configured with labels ."], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 3397, "code": "async def handle home ( self , request : Request ) -> Response : if self . description : title = f'{self.name} - {self.description}' else : title = self . name text = dedent ( ) return Response ( content type = 'text/html' , text = text )", "predictions": ["handle the home action ."], "references": ["home page request handler ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 3398, "code": "async def handle metrics ( self , request : Request ) -> Response : if self . update handler : await self . update handler ( self . registry . get metrics ( ) ) response = Response ( body = self . registry . generate metrics ( ) ) response . content type = CONTENT TYPE LATEST return response", "predictions": ["handle the metrics from the service ."], "references": ["handler for metrics ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 3399, "code": "def info ( self ) : return itertools . chain ( self . pods , self . assumptions , self . warnings )", "predictions": ["return current info ."], "references": ["the pods assumptions and warnings of this result ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 3400, "code": "def results ( self ) : return ( pod for pod in self . pods if pod . primary or pod . title == 'Result' )", "predictions": ["return the results of all pods in this pod ."], "references": ["the pods that hold the response to a simple discrete query ."], "bleu": 0.12273680279953825, "rouge_l": 0.2683284457478006}
{"id": 3401, "code": "def vector ( members : Iterable [ T ] , meta : Optional [ I Persistent Map ] = None ) -> Vector [ T ] : return Vector ( pvector ( members ) , meta = meta )", "predictions": ["create a vector of members ."], "references": ["creates a new vector ."], "bleu": 0.2626909894424158, "rouge_l": 0.5545454545454546}
{"id": 3402, "code": "def v ( * members : T , meta : Optional [ I Persistent Map ] = None ) -> Vector [ T ] : return Vector ( pvector ( members ) , meta = meta )", "predictions": ["add members to the first level of members ."], "references": ["creates a new vector from members ."], "bleu": 0.18575057999133596, "rouge_l": 0.2557651991614256}
{"id": 3403, "code": "def eval file ( filename : str , ctx : compiler . Compiler Context , module : types . Module Type ) : last = None for form in reader . read file ( filename , resolver = runtime . resolve alias ) : last = compiler . compile and exec form ( form , ctx , module ) return last", "predictions": ["evaluate the contents of a file ."], "references": ["evaluate a file with the given name into a python module ast node ."], "bleu": 0.1069482072978842, "rouge_l": 0.35935198821796754}
{"id": 3404, "code": "def eval stream ( stream , ctx : compiler . Compiler Context , module : types . Module Type ) : last = None for form in reader . read ( stream , resolver = runtime . resolve alias ) : last = compiler . compile and exec form ( form , ctx , module ) return last", "predictions": ["evaluate a stream in a stream ."], "references": ["evaluate the forms in stdin into a python module ast node ."], "bleu": 0.11434175042957104, "rouge_l": 0.40197693574958815}
{"id": 3405, "code": "def eval str ( s : str , ctx : compiler . Compiler Context , module : types . Module Type , eof : Any ) : last = eof for form in reader . read str ( s , resolver = runtime . resolve alias , eof = eof ) : last = compiler . compile and exec form ( form , ctx , module ) return last", "predictions": ["evaluate str in str ."], "references": ["evaluate the forms in a string into a python module ast node ."], "bleu": 0.06554932163900559, "rouge_l": 0.3086003372681282}
{"id": 3406, "code": "def run ( file or code , code , in ns , use var indirection , warn on shadowed name , warn on shadowed var , warn on var indirection , ) : basilisp . init ( ) ctx = compiler . Compiler Context ( filename = CLI INPUT FILE PATH if code else ( STDIN INPUT FILE PATH if file or code == STDIN FILE NAME else file or code ) , opts = { compiler . WARN ON SHADOWED NAME : warn on shadowed name , compiler . WARN ON SHADOWED VAR : warn on shadowed var , compiler . USE VAR INDIRECTION : use var indirection , compiler . WARN ON VAR INDIRECTION : warn on var indirection , } , ) eof = object ( ) with runtime . ns bindings ( in ns ) as ns : if code : print ( runtime . lrepr ( eval str ( file or code , ctx , ns . module , eof ) ) ) elif file or code == STDIN FILE NAME : print ( runtime . lrepr ( eval stream ( click . get text stream ( \"stdin\" ) , ctx , ns . module ) ) ) else : print ( runtime . lrepr ( eval file ( file or code , ctx , ns . module ) ) )", "predictions": ["run the basilisp or ."], "references": ["run a basilisp script or a line of code if it is provided ."], "bleu": 0.05674620314719988, "rouge_l": 0.38791732909379967}
{"id": 3407, "code": "def multifn ( dispatch : Dispatch Function , default = None ) -> Multi Function [ T ] : name = sym . symbol ( dispatch . qualname , ns = dispatch . module ) return Multi Function ( name , dispatch , default )", "predictions": ["decorator to dispatches an dispatch function ."], "references": ["decorator function which can be used to make python multi functions ."], "bleu": 0.11434175042957104, "rouge_l": 0.30148270181219106}
{"id": 3408, "code": "def add method ( m : lmap . Map , key : T , method : Method ) -> lmap . Map : return m . assoc ( key , method )", "predictions": ["add a method to the current user ."], "references": ["swap the methods atom to include method with key ."], "bleu": 0.157044754112095, "rouge_l": 0.21785714285714283}
{"id": 3409, "code": "def remove method ( m : lmap . Map , key : T ) -> lmap . Map : return m . dissoc ( key )", "predictions": ["remove a method from the lmap ."], "references": ["swap the methods atom to remove method with key ."], "bleu": 0.15215596197411094, "rouge_l": 0.34205607476635513}
{"id": 3410, "code": "def remove method ( self , key : T ) -> Optional [ Method ] : method = self . methods . entry ( key , None ) if method : self . methods . swap ( Multi Function . remove method , key ) return method", "predictions": ["remove a method from the methods ."], "references": ["remove the method defined for this key and return it ."], "bleu": 0.1319006407505858, "rouge_l": 0.32049036777583184}
{"id": 3411, "code": "def is macro ( v : Var ) -> bool : return ( Maybe ( v . meta ) . map ( lambda m : m . entry ( SYM MACRO META KEY , None ) ) . or else get ( False ) )", "predictions": ["return true if the entry is a macro macro ."], "references": ["return true if the var holds a macro function ."], "bleu": 0.41722614486115056, "rouge_l": 0.7000000000000001}
{"id": 3412, "code": "def clean meta ( meta : Optional [ lmap . Map ] ) -> Optional [ lmap . Map ] : if meta is None : return None else : new meta = meta . dissoc ( reader . READER LINE KW , reader . READER COL KW ) return None if len ( new meta ) == 0 else new meta", "predictions": ["clean the meta data object ."], "references": ["remove reader metadata from the form s meta map ."], "bleu": 0.13487005099534619, "rouge_l": 0.3588235294117647}
{"id": 3413, "code": "def deftype impls ( ctx : Parser Context , form : I Seq ) -> Tuple [ List [ Def Type Base ] , List [ Method ] ] : current interface sym : Optional [ sym . Symbol ] = None current interface : Optional [ Def Type Base ] = None interfaces = [ ] methods : List [ Method ] = [ ] interface methods : Mutable Mapping [ sym . Symbol , List [ Method ] ] = { } for elem in form : if isinstance ( elem , sym . Symbol ) : if current interface is not None : if current interface sym in interface methods : raise Parser Exception ( f\"deftype* forms may only implement an interface once\" , form = elem , ) assert ( current interface sym is not None ) , \"Symbol must be defined with interface\" interface methods [ current interface sym ] = methods current interface sym = elem current interface = parse ast ( ctx , elem ) methods = [ ] if not isinstance ( current interface , ( Maybe Class , Maybe Host Form , Var Ref ) ) : raise Parser Exception ( f\"deftype* interface implementation must be an existing interface\" , form = elem , ) interfaces . append ( current interface ) elif isinstance ( elem , I Seq ) : if current interface is None : raise Parser Exception ( f\"deftype* method cannot be declared without interface\" , form = elem ) methods . append ( deftype method ( ctx , elem , current interface ) ) else : raise Parser Exception ( f\"deftype* must consist of interface or protocol names and methods\" , form = elem , ) if current interface is not None : if len ( methods ) > 0 : if current interface sym in interface methods : raise Parser Exception ( f\"deftype* forms may only implement an interface once\" , form = current interface sym , ) assert ( current interface sym is not None ) , \"Symbol must be defined with interface\" interface methods [ current interface sym ] = methods else : raise Parser Exception ( f\"deftype* may not declare interface without at least one method\" , form = current interface sym , ) return interfaces , list ( chain . from iterable ( interface methods . values ( ) ) )", "predictions": ["return an interfaces of an ast interface ."], "references": ["roll up deftype * declared bases and method implementations ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 3414, "code": "def resolve sym ( ctx : Parser Context , form : sym . Symbol ) -> Union [ Maybe Class , Maybe Host Form , Var Ref ] : if form . ns is None and form . name . endswith ( \".\" ) : try : ns , name = form . name [ : - 1 ] . rsplit ( \".\" , maxsplit = 1 ) form = sym . symbol ( name , ns = ns ) except Value Error : form = sym . symbol ( form . name [ : - 1 ] ) if form . ns is not None : return resolve namespaced symbol ( ctx , form ) else : return resolve bare symbol ( ctx , form )", "predictions": ["resolves a form symbol to an appropriate symbol ."], "references": ["resolve a basilisp symbol as a var or python name ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 3415, "code": "def bootstrap module ( gctx : Generator Context , optimizer : Python AST Optimizer , mod : types . Module Type , collect bytecode : Optional [ Bytecode Collector ] = None , ) -> None : incremental compile module ( optimizer , py module preamble ( gctx ) , mod , source filename = gctx . filename , collect bytecode = collect bytecode , ) mod . basilisp bootstrapped = True", "predictions": ["bootstrap the module into the gctx ."], "references": ["bootstrap a new module with imports and other boilerplate ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 3416, "code": "def sequence ( s : Iterable ) -> I Seq [ Any ] : try : i = iter ( s ) return Sequence ( i , next ( i ) ) except Stop Iteration : return EMPTY", "predictions": ["parse a sequence into a sequence ."], "references": ["create a sequence from iterable s ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 3417, "code": "def fraction ( numerator : int , denominator : int ) -> Fraction : return Fraction ( numerator = numerator , denominator = denominator )", "predictions": ["return a fraction of fraction with denominator ."], "references": ["create a fraction from a numerator and denominator ."], "bleu": 0.23420197753909952, "rouge_l": 0.465648854961832}
{"id": 3418, "code": "def get handler ( level : str , fmt : str ) -> logging . Handler : handler : logging . Handler = logging . Null Handler ( ) if os . getenv ( \"BASILISP USE DEV LOGGER\" ) == \"true\" : handler = logging . Stream Handler ( ) handler . set Formatter ( logging . Formatter ( fmt ) ) handler . set Level ( level ) return handler", "predictions": ["returns the logger handler for the logger ."], "references": ["get the default logging handler for basilisp ."], "bleu": 0.239802967618271, "rouge_l": 0.5}
{"id": 3419, "code": "def map ( kvs : Mapping [ K , V ] , meta = None ) -> Map [ K , V ] : return Map ( pmap ( initial = kvs ) , meta = meta )", "predictions": ["map function to kvs ."], "references": ["creates a new map ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 3420, "code": "def partition ( coll , n : int ) : assert n > 0 start = 0 stop = n while stop <= len ( coll ) : yield tuple ( e for e in coll [ start : stop ] ) start += n stop += n if start < len ( coll ) < stop : stop = len ( coll ) yield tuple ( e for e in coll [ start : stop ] )", "predictions": ["partition n n - sized chunks from coll ."], "references": ["partition coll into groups of size n ."], "bleu": 0.17747405280050263, "rouge_l": 0.35672514619883033}
{"id": 3421, "code": "def read namespaced ( ctx : Reader Context , allowed suffix : Optional [ str ] = None ) -> Tuple [ Optional [ str ] , str ] : ns : List [ str ] = [ ] name : List [ str ] = [ ] reader = ctx . reader has ns = False while True : token = reader . peek ( ) if token == \"/\" : reader . next token ( ) if has ns : raise Syntax Error ( \"Found '/'; expected word character\" ) elif len ( name ) == 0 : name . append ( \"/\" ) else : if \"/\" in name : raise Syntax Error ( \"Found '/' after '/'\" ) has ns = True ns = name name = [ ] elif ns name chars . match ( token ) : reader . next token ( ) name . append ( token ) elif allowed suffix is not None and token == allowed suffix : reader . next token ( ) name . append ( token ) else : break ns str = None if not has ns else \"\" . join ( ns ) name str = \"\" . join ( name ) if ns str is None : if \"/\" in name str and name str != \"/\" : raise Syntax Error ( \"'/' character disallowed in names\" ) assert ns str is None or len ( ns str ) > 0 return ns str , name str", "predictions": ["read a word - style word from a str ."], "references": ["read a namespaced token from the input stream ."], "bleu": 0.18850319022747347, "rouge_l": 0.42508710801393734}
{"id": 3422, "code": "def read list ( ctx : Reader Context ) -> llist . List : start = ctx . reader . advance ( ) assert start == \"(\" return read coll ( ctx , llist . list , \")\" , \"list\" )", "predictions": ["read a list of strings from the reader ."], "references": ["read a list element from the input stream ."], "bleu": 0.32466791547509893, "rouge_l": 0.6666666666666666}
{"id": 3423, "code": "def read vector ( ctx : Reader Context ) -> vector . Vector : start = ctx . reader . advance ( ) assert start == \"[\" return read coll ( ctx , vector . vector , \"]\" , \"vector\" )", "predictions": ["read a vector from an vector ."], "references": ["read a vector element from the input stream ."], "bleu": 0.28751742289713444, "rouge_l": 0.6112224448897796}
{"id": 3424, "code": "def read set ( ctx : Reader Context ) -> lset . Set : start = ctx . reader . advance ( ) assert start == \"{\" def set if valid ( s : Collection ) -> lset . Set : if len ( s ) != len ( set ( s ) ) : raise Syntax Error ( \"Duplicated values in set\" ) return lset . set ( s ) return read coll ( ctx , set if valid , \"}\" , \"set\" )", "predictions": ["empty a value from the reader"], "references": ["return a set from the input stream ."], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 3425, "code": "def read map ( ctx : Reader Context ) -> lmap . Map : reader = ctx . reader start = reader . advance ( ) assert start == \"{\" d : Mutable Mapping [ Any , Any ] = { } while True : if reader . peek ( ) == \"}\" : reader . next token ( ) break k = read next ( ctx ) if k is COMMENT : continue while True : if reader . peek ( ) == \"}\" : raise Syntax Error ( \"Unexpected token '}'; expected map value\" ) v = read next ( ctx ) if v is COMMENT : continue if k in d : raise Syntax Error ( f\"Duplicate key '{k}' in map literal\" ) break d [ k ] = v return lmap . map ( d )", "predictions": ["create a metrics from the reader metrics . . . . . . . . ."], "references": ["return a map from the input stream ."], "bleu": 0.11502783619900048, "rouge_l": 0.3546511627906977}
{"id": 3426, "code": "def read kw ( ctx : Reader Context ) -> keyword . Keyword : start = ctx . reader . advance ( ) assert start == \":\" ns , name = read namespaced ( ctx ) if \".\" in name : raise Syntax Error ( \"Found '.' in keyword name\" ) return keyword . keyword ( name , ns = ns )", "predictions": ["configure a bool from a bool = bool = 0 = 1 = 0 = 0 = 1 = 1 = 0 = 1 = 1 = 0 = 0 ="], "references": ["return a keyword from the input stream ."], "bleu": 0.04317900023606586, "rouge_l": 0.1147695202257761}
{"id": 3427, "code": "def read function ( ctx : Reader Context ) -> llist . List : if ctx . is in anon fn : raise Syntax Error ( f\"Nested #() definitions not allowed\" ) with ctx . in anon fn ( ) : form = read list ( ctx ) arg set = set ( ) def arg suffix ( arg num ) : if arg num is None : return \"1\" elif arg num == \"&\" : return \"rest\" else : return arg num def sym replacement ( arg num ) : suffix = arg suffix ( arg num ) return symbol . symbol ( f\"arg-{suffix}\" ) def identify and replace ( f ) : if isinstance ( f , symbol . Symbol ) : if f . ns is None : match = fn macro args . match ( f . name ) if match is not None : arg num = match . group ( 2 ) suffix = arg suffix ( arg num ) arg set . add ( suffix ) return sym replacement ( arg num ) return f body = walk . postwalk ( identify and replace , form ) if len ( form ) > 0 else None arg list : List [ symbol . Symbol ] = [ ] numbered args = sorted ( map ( int , filter ( lambda k : k != \"rest\" , arg set ) ) ) if len ( numbered args ) > 0 : max arg = max ( numbered args ) arg list = [ sym replacement ( str ( i ) ) for i in range ( 1 , max arg + 1 ) ] if \"rest\" in arg set : arg list . append ( AMPERSAND ) arg list . append ( sym replacement ( \"rest\" ) ) return llist . l ( FN , vector . vector ( arg list ) , body )", "predictions": ["create a replacement object from the symbol"], "references": ["read a function reader macro from the input stream ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 3428, "code": "def read quoted ( ctx : Reader Context ) -> llist . List : start = ctx . reader . advance ( ) assert start == \"'\" next form = read next consuming comment ( ctx ) return llist . l ( QUOTE , next form )", "predictions": ["get the metrics from the metrics . . . . . . . . . ."], "references": ["read a quoted form from the input stream ."], "bleu": 0.10878661088699644, "rouge_l": 0.2527624309392265}
{"id": 3429, "code": "def read syntax quoted ( ctx : Reader Context ) -> Reader Form : start = ctx . reader . advance ( ) assert start == \"`\" with ctx . syntax quoted ( ) : return process syntax quoted form ( ctx , read next consuming comment ( ctx ) )", "predictions": ["def handle to be used in the content of the content ."], "references": ["read a syntax - quote and set the syntax - quoting state in the reader ."], "bleu": 0.1113283703518327, "rouge_l": 0.2089041095890411}
{"id": 3430, "code": "def read deref ( ctx : Reader Context ) -> Lisp Form : start = ctx . reader . advance ( ) assert start == \"@\" next form = read next consuming comment ( ctx ) return llist . l ( DEREF , next form )", "predictions": ["def to def ."], "references": ["read a derefed form from the input stream ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 3431, "code": "def read regex ( ctx : Reader Context ) -> Pattern : s = read str ( ctx , allow arbitrary escapes = True ) try : return langutil . regex from str ( s ) except re . error : raise Syntax Error ( f\"Unrecognized regex pattern syntax: {s}\" )", "predictions": ["info about the regex pattern . ."], "references": ["read a regex reader macro from the input stream ."], "bleu": 0.14390022429682173, "rouge_l": 0.22803738317757008}
{"id": 3432, "code": "def read next ( ctx : Reader Context ) -> Lisp Reader Form : reader = ctx . reader token = reader . peek ( ) if token == \"(\" : return read list ( ctx ) elif token == \"[\" : return read vector ( ctx ) elif token == \"{\" : return read map ( ctx ) elif begin num chars . match ( token ) : return read num ( ctx ) elif whitespace chars . match ( token ) : reader . next token ( ) return read next ( ctx ) elif token == \":\" : return read kw ( ctx ) elif token == '\"' : return read str ( ctx ) elif token == \"'\" : return read quoted ( ctx ) elif token == \"\\\\\" : return read character ( ctx ) elif ns name chars . match ( token ) : return read sym ( ctx ) elif token == \"#\" : return read reader macro ( ctx ) elif token == \"^\" : return read meta ( ctx ) elif token == \";\" : return read comment ( ctx ) elif token == \"`\" : return read syntax quoted ( ctx ) elif token == \"~\" : return read unquote ( ctx ) elif token == \"@\" : return read deref ( ctx ) elif token == \"\" : return ctx . eof else : raise Syntax Error ( \"Unexpected token '{token}'\" . format ( token = token ) )", "predictions": ["results from the next or results"], "references": ["read the next full form from the input stream ."], "bleu": 0.17749896924055253, "rouge_l": 0.23921568627450981}
{"id": 3433, "code": "def basilisp bytecode ( mtime : int , source size : int , code : List [ types . Code Type ] ) -> bytes : data = bytearray ( MAGIC NUMBER ) data . extend ( w long ( mtime ) ) data . extend ( w long ( source size ) ) data . extend ( marshal . dumps ( code ) ) return data", "predictions": ["return a vector from an mtime"], "references": ["return the bytes for a basilisp bytecode cache file ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 3434, "code": "def exec cached module ( self , fullname : str , loader state : Mapping [ str , str ] , path stats : Mapping [ str , int ] , module : types . Module Type , ) : filename = loader state [ \"filename\" ] cache filename = loader state [ \"cache filename\" ] with timed ( lambda duration : logger . debug ( f\"Loaded cached Basilisp module '{fullname}' in {duration / 1000000}ms\" ) ) : logger . debug ( f\"Checking for cached Basilisp module '{fullname}''\" ) cache data = self . get data ( cache filename ) cached code = get basilisp bytecode ( fullname , path stats [ \"mtime\" ] , path stats [ \"size\" ] , cache data ) compiler . compile bytecode ( cached code , compiler . Generator Context ( filename = filename ) , compiler . Python AST Optimizer ( ) , module , )", "predictions": ["compile the cached * fullname * for the given fullname . * . * . * . * ."], "references": ["load and execute a cached basilisp module ."], "bleu": 0.0712695567709093, "rouge_l": 0.1598951507208388}
{"id": 3435, "code": "def exec module ( self , fullname : str , loader state : Mapping [ str , str ] , path stats : Mapping [ str , int ] , module : types . Module Type , ) : filename = loader state [ \"filename\" ] cache filename = loader state [ \"cache filename\" ] with timed ( lambda duration : logger . debug ( f\"Loaded Basilisp module '{fullname}' in {duration / 1000000}ms\" ) ) : all bytecode = [ ] def add bytecode ( bytecode : types . Code Type ) : all bytecode . append ( bytecode ) logger . debug ( f\"Reading and compiling Basilisp module '{fullname}'\" ) forms = reader . read file ( filename , resolver = runtime . resolve alias ) compiler . compile module ( forms , compiler . Compiler Context ( filename = filename ) , module , collect bytecode = add bytecode , ) cache file bytes = basilisp bytecode ( path stats [ \"mtime\" ] , path stats [ \"size\" ] , all bytecode ) self . cache bytecode ( filename , cache filename , cache file bytes )", "predictions": ["execute a file in the cached ."], "references": ["load and execute a non - cached basilisp module ."], "bleu": 0.18094495256969623, "rouge_l": 0.45607476635514016}
{"id": 3436, "code": "def symbol ( name : str , ns : Optional [ str ] = None , meta = None ) -> Symbol : return Symbol ( name , ns = ns , meta = meta )", "predictions": ["create an eval eval function ."], "references": ["create a new symbol ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 3437, "code": "def complete ( text : str , kw cache : atom . Atom [ \"P Map[int, Keyword]\" ] = INTERN ) -> Iterable [ str ] : assert text . startswith ( \":\" ) interns = kw cache . deref ( ) text = text [ 1 : ] if \"/\" in text : prefix , suffix = text . split ( \"/\" , maxsplit = 1 ) results = filter ( lambda kw : ( kw . ns is not None and kw . ns == prefix ) and kw . name . startswith ( suffix ) , interns . itervalues ( ) , ) else : results = filter ( lambda kw : kw . name . startswith ( text ) or ( kw . ns is not None and kw . ns . startswith ( text ) ) , interns . itervalues ( ) , ) return map ( str , results )", "predictions": ["autocomplete autocomplete in text"], "references": ["return an iterable of possible completions for the given text ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 3438, "code": "def keyword ( name : str , ns : Optional [ str ] = None , kw cache : atom . Atom [ \"P Map[int, Keyword]\" ] = INTERN , ) -> Keyword : h = hash ( ( name , ns ) ) return kw cache . swap ( get or create , h , name , ns ) [ h ]", "predictions": ["create a run function . . ."], "references": ["create a new keyword ."], "bleu": 0.2626909894424158, "rouge_l": 0.5154929577464789}
{"id": 3439, "code": "def chain py ast ( * genned : Generated Py AST , ) -> Tuple [ Py AST Stream , Py AST Stream ] : deps = chain . from iterable ( map ( lambda n : n . dependencies , genned ) ) nodes = map ( lambda n : n . node , genned ) return deps , nodes", "predictions": ["build an dispatch tree from an dispatch chain symbol symbol symbol symbol symbol symbol symbol symbol symbol symbol ."], "references": ["chain a sequence of generated python asts into a tuple of dependency nodes"], "bleu": 0.06439931429457924, "rouge_l": 0.06468716861081655}
{"id": 3440, "code": "def simple ast generator ( gen ast ) : @ wraps ( gen ast ) def wrapped ast generator ( ctx : Generator Context , form : Lisp Form ) -> Generated Py AST : return Generated Py AST ( node = gen ast ( ctx , form ) ) return wrapped ast generator", "predictions": ["generate method to add method to method ."], "references": ["wrap simpler ast generators to return a generatedpyast ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 3441, "code": "def collection ast ( ctx : Generator Context , form : Iterable [ Node ] ) -> Tuple [ Py AST Stream , Py AST Stream ] : return chain py ast ( * map ( partial ( gen py ast , ctx ) , form ) )", "predictions": ["build an method definition ."], "references": ["turn a collection of lisp forms into python ast nodes ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 3442, "code": "def clean meta ( form : I Meta ) -> Lisp Form : assert form . meta is not None , \"Form must have non-null 'meta' attribute\" meta = form . meta . dissoc ( reader . READER LINE KW , reader . READER COL KW ) if len ( meta ) == 0 : return None return cast ( lmap . Map , meta )", "predictions": ["remove the method from the self . . ."], "references": ["remove reader metadata from the form s meta map ."], "bleu": 0.18885888592159467, "rouge_l": 0.41709401709401706}
{"id": 3443, "code": "def is redefable ( v : Var ) -> bool : return ( Maybe ( v . meta ) . map ( lambda m : m . get ( SYM REDEF META KEY , None ) ) . or else get ( False ) )", "predictions": ["return true if the value is a macro macro ."], "references": ["return true if the var can be redefined ."], "bleu": 0.3672056269893592, "rouge_l": 0.5313588850174217}
{"id": 3444, "code": "def should warn on redef ( ctx : Generator Context , defsym : sym . Symbol , safe name : str , def meta : lmap . Map ) -> bool : no warn on redef = def meta . entry ( SYM NO WARN ON REDEF META KEY , False ) if no warn on redef : return False elif safe name in ctx . current ns . module . dict : return True elif defsym in ctx . current ns . interns : var = ctx . current ns . find ( defsym ) assert var is not None , f\"Var {defsym} cannot be none here\" if var . meta is not None and var . meta . entry ( SYM REDEF META KEY ) : return False elif var . is bound : return True else : return False else : return False", "predictions": ["return for redef ."], "references": ["return true if the compiler should emit a warning about this name being redefined ."], "bleu": 0.025419978385188596, "rouge_l": 0.190625}
{"id": 3445, "code": "def deftype to py ast ( ctx : Generator Context , node : Def Type ) -> Generated Py AST : assert node . op == Node Op . DEFTYPE type name = munge ( node . name ) ctx . symbol table . new symbol ( sym . symbol ( node . name ) , type name , Local Type . DEFTYPE ) bases = [ ] for base in node . interfaces : base node = gen py ast ( ctx , base ) assert ( count ( base node . dependencies ) == 0 ) , \"Class and host form nodes do not have dependencies\" bases . append ( base node . node ) decorator = ast . Call ( func = ATTR CLASS DECORATOR NAME , args = [ ] , keywords = [ ast . keyword ( arg = \"cmp\" , value = ast . Name Constant ( False ) ) , ast . keyword ( arg = \"frozen\" , value = ast . Name Constant ( node . is frozen ) ) , ast . keyword ( arg = \"slots\" , value = ast . Name Constant ( True ) ) , ] , ) with ctx . new symbol table ( node . name ) : type nodes = [ ] for field in node . fields : safe field = munge ( field . name ) type nodes . append ( ast . Assign ( targets = [ ast . Name ( id = safe field , ctx = ast . Store ( ) ) ] , value = ast . Call ( func = ATTRIB FIELD FN NAME , args = [ ] , keywords = [ ] ) , ) ) ctx . symbol table . new symbol ( sym . symbol ( field . name ) , safe field , field . local ) type deps : List [ ast . AST ] = [ ] for method in node . methods : type ast = deftype method to py ast ( ctx , method ) type nodes . append ( type ast . node ) type deps . extend ( type ast . dependencies ) return Generated Py AST ( node = ast . Name ( id = type name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( type deps , [ ast . Class Def ( name = type name , bases = bases , keywords = [ ] , body = type nodes , decorator list = [ decorator ] , ) ] , ) ) , )", "predictions": ["convert an ctx interfaces to a interfaces methods . ."], "references": ["return a python ast node for a deftype * expression ."], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 3446, "code": "def do to py ast ( ctx : Generator Context , node : Do ) -> Generated Py AST : assert node . op == Node Op . DO assert not node . is body body ast = Generated Py AST . reduce ( * map ( partial ( gen py ast , ctx ) , chain ( node . statements , [ node . ret ] ) ) ) fn body ast : List [ ast . AST ] = [ ] do result name = genname ( DO PREFIX ) fn body ast . extend ( map ( statementize , body ast . dependencies ) ) fn body ast . append ( ast . Assign ( targets = [ ast . Name ( id = do result name , ctx = ast . Store ( ) ) ] , value = body ast . node ) ) return Generated Py AST ( node = ast . Name ( id = do result name , ctx = ast . Load ( ) ) , dependencies = fn body ast )", "predictions": ["convert an ctx node to an ctx ctx ."], "references": ["return a python ast node for a do expression ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 3447, "code": "def fn args to py ast ( ctx : Generator Context , params : Iterable [ Binding ] , body : Do ) -> Tuple [ List [ ast . arg ] , Optional [ ast . arg ] , List [ ast . AST ] ] : fn args , varg = [ ] , None fn body ast : List [ ast . AST ] = [ ] for binding in params : assert binding . init is None , \":fn nodes cannot have bindint :inits\" assert varg is None , \"Must have at most one variadic arg\" arg name = genname ( munge ( binding . name ) ) if not binding . is variadic : fn args . append ( ast . arg ( arg = arg name , annotation = None ) ) ctx . symbol table . new symbol ( sym . symbol ( binding . name ) , arg name , Local Type . ARG ) else : varg = ast . arg ( arg = arg name , annotation = None ) safe local = genname ( munge ( binding . name ) ) fn body ast . append ( ast . Assign ( targets = [ ast . Name ( id = safe local , ctx = ast . Store ( ) ) ] , value = ast . Call ( func = COLLECT ARGS FN NAME , args = [ ast . Name ( id = arg name , ctx = ast . Load ( ) ) ] , keywords = [ ] , ) , ) ) ctx . symbol table . new symbol ( sym . symbol ( binding . name ) , safe local , Local Type . ARG ) body ast = synthetic do to py ast ( ctx , body ) fn body ast . extend ( map ( statementize , body ast . dependencies ) ) fn body ast . append ( ast . Return ( value = body ast . node ) ) return fn args , varg , fn body ast", "predictions": ["convert an ast to a list of ast objects ."], "references": ["generate a list of python ast nodes from function method parameters ."], "bleu": 0.21258637840736228, "rouge_l": 0.44721407624633425}
{"id": 3448, "code": "def single arity fn to py ast ( ctx : Generator Context , node : Fn , method : Fn Method , def name : Optional [ str ] = None , meta node : Optional [ Meta Node ] = None , ) -> Generated Py AST : assert node . op == Node Op . FN assert method . op == Node Op . FN METHOD lisp fn name = node . local . name if node . local is not None else None py fn name = fn name ( lisp fn name ) if def name is None else munge ( def name ) py fn node = ast . Async Function Def if node . is async else ast . Function Def with ctx . new symbol table ( py fn name ) , ctx . new recur point ( method . loop id , Recur Type . FN , is variadic = node . is variadic ) : if lisp fn name is not None : ctx . symbol table . new symbol ( sym . symbol ( lisp fn name ) , py fn name , Local Type . FN ) fn args , varg , fn body ast = fn args to py ast ( ctx , method . params , method . body ) meta deps , meta decorators = fn meta ( ctx , meta node ) return Generated Py AST ( node = ast . Name ( id = py fn name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( meta deps , [ py fn node ( name = py fn name , args = ast . arguments ( args = fn args , kwarg = None , vararg = varg , kwonlyargs = [ ] , defaults = [ ] , kw defaults = [ ] , ) , body = fn body ast , decorator list = list ( chain ( meta decorators , [ BASILISP FN FN NAME ] , [ TRAMPOLINE FN NAME ] if ctx . recur point . has recur else [ ] , ) ) , returns = None , ) ] , ) ) , )", "predictions": ["generates an ast table table table ."], "references": ["return a python ast node for a function with a single arity ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 3449, "code": "def multi arity fn to py ast ( ctx : Generator Context , node : Fn , methods : Collection [ Fn Method ] , def name : Optional [ str ] = None , meta node : Optional [ Meta Node ] = None , ) -> Generated Py AST : assert node . op == Node Op . FN assert all ( [ method . op == Node Op . FN METHOD for method in methods ] ) lisp fn name = node . local . name if node . local is not None else None py fn name = fn name ( lisp fn name ) if def name is None else munge ( def name ) py fn node = ast . Async Function Def if node . is async else ast . Function Def arity to name = { } rest arity name : Optional [ str ] = None fn defs = [ ] for method in methods : arity name = f\"{py fn name} arity{' rest' if method.is variadic else method.fixed arity}\" if method . is variadic : rest arity name = arity name else : arity to name [ method . fixed arity ] = arity name with ctx . new symbol table ( arity name ) , ctx . new recur point ( method . loop id , Recur Type . FN , is variadic = node . is variadic ) : if lisp fn name is not None : ctx . symbol table . new symbol ( sym . symbol ( lisp fn name ) , py fn name , Local Type . FN ) fn args , varg , fn body ast = fn args to py ast ( ctx , method . params , method . body ) fn defs . append ( py fn node ( name = arity name , args = ast . arguments ( args = fn args , kwarg = None , vararg = varg , kwonlyargs = [ ] , defaults = [ ] , kw defaults = [ ] , ) , body = fn body ast , decorator list = [ TRAMPOLINE FN NAME ] if ctx . recur point . has recur else [ ] , returns = None , ) ) dispatch fn ast = multi arity dispatch fn ( ctx , py fn name , arity to name , default name = rest arity name , max fixed arity = node . max fixed arity , meta node = meta node , is async = node . is async , ) return Generated Py AST ( node = dispatch fn ast . node , dependencies = list ( chain ( fn defs , dispatch fn ast . dependencies ) ) , )", "predictions": ["convert an ast to an ast table ."], "references": ["return a python ast node for a function with multiple arities ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 3450, "code": "def fn to py ast ( ctx : Generator Context , node : Fn , def name : Optional [ str ] = None , meta node : Optional [ Meta Node ] = None , ) -> Generated Py AST : assert node . op == Node Op . FN if len ( node . methods ) == 1 : return single arity fn to py ast ( ctx , node , next ( iter ( node . methods ) ) , def name = def name , meta node = meta node ) else : return multi arity fn to py ast ( ctx , node , node . methods , def name = def name , meta node = meta node )", "predictions": ["convert an level of level to an level of the level ."], "references": ["return a python ast node for a fn expression ."], "bleu": 0.10390302174233558, "rouge_l": 0.09242424242424242}
{"id": 3451, "code": "def import to py ast ( ctx : Generator Context , node : Import ) -> Generated Py AST : assert node . op == Node Op . IMPORT last = None deps : List [ ast . AST ] = [ ] for alias in node . aliases : safe name = munge ( alias . name ) try : module = importlib . import module ( safe name ) if alias . alias is not None : ctx . add import ( sym . symbol ( alias . name ) , module , sym . symbol ( alias . alias ) ) else : ctx . add import ( sym . symbol ( alias . name ) , module ) except Module Not Found Error as e : raise Import Error ( f\"Python module '{alias.name}' not found\" , node . form , node ) from e py import alias = ( munge ( alias . alias ) if alias . alias is not None else safe name . split ( \".\" , maxsplit = 1 ) [ 0 ] ) deps . append ( ast . Assign ( targets = [ ast . Name ( id = py import alias , ctx = ast . Store ( ) ) ] , value = ast . Call ( func = load attr ( \"builtins. import \" ) , args = [ ast . Str ( safe name ) ] , keywords = [ ] , ) , ) ) last = ast . Name ( id = py import alias , ctx = ast . Load ( ) ) deps . append ( ast . Call ( func = load attr ( f\"{ NS VAR VALUE}.add import\" ) , args = [ ast . Call ( func = NEW SYM FN NAME , args = [ ast . Str ( safe name ) ] , keywords = [ ] ) , last , ] , keywords = [ ] , ) ) assert last is not None , \"import* node must have at least one import\" return Generated Py AST ( node = last , dependencies = deps )", "predictions": ["convert an ast to to an ast instance ."], "references": ["return a python ast node for a basilisp import * expression ."], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 3452, "code": "def invoke to py ast ( ctx : Generator Context , node : Invoke ) -> Generated Py AST : assert node . op == Node Op . INVOKE fn ast = gen py ast ( ctx , node . fn ) args deps , args nodes = collection ast ( ctx , node . args ) return Generated Py AST ( node = ast . Call ( func = fn ast . node , args = list ( args nodes ) , keywords = [ ] ) , dependencies = list ( chain ( fn ast . dependencies , args deps ) ) , )", "predictions": ["partition to to to"], "references": ["return a python ast node for a basilisp function invocation ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 3453, "code": "def let to py ast ( ctx : Generator Context , node : Let ) -> Generated Py AST : assert node . op == Node Op . LET with ctx . new symbol table ( \"let\" ) : let body ast : List [ ast . AST ] = [ ] for binding in node . bindings : init node = binding . init assert init node is not None init ast = gen py ast ( ctx , init node ) binding name = genname ( munge ( binding . name ) ) let body ast . extend ( init ast . dependencies ) let body ast . append ( ast . Assign ( targets = [ ast . Name ( id = binding name , ctx = ast . Store ( ) ) ] , value = init ast . node , ) ) ctx . symbol table . new symbol ( sym . symbol ( binding . name ) , binding name , Local Type . LET ) let result name = genname ( \"let result\" ) body ast = synthetic do to py ast ( ctx , node . body ) let body ast . extend ( map ( statementize , body ast . dependencies ) ) let body ast . append ( ast . Assign ( targets = [ ast . Name ( id = let result name , ctx = ast . Store ( ) ) ] , value = body ast . node , ) ) return Generated Py AST ( node = ast . Name ( id = let result name , ctx = ast . Load ( ) ) , dependencies = let body ast )", "predictions": ["convert read read namespaced to python list ."], "references": ["return a python ast node for a let * expression ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 3454, "code": "def loop to py ast ( ctx : Generator Context , node : Loop ) -> Generated Py AST : assert node . op == Node Op . LOOP with ctx . new symbol table ( \"loop\" ) : binding names = [ ] init bindings : List [ ast . AST ] = [ ] for binding in node . bindings : init node = binding . init assert init node is not None init ast = gen py ast ( ctx , init node ) init bindings . extend ( init ast . dependencies ) binding name = genname ( munge ( binding . name ) ) binding names . append ( binding name ) init bindings . append ( ast . Assign ( targets = [ ast . Name ( id = binding name , ctx = ast . Store ( ) ) ] , value = init ast . node , ) ) ctx . symbol table . new symbol ( sym . symbol ( binding . name ) , binding name , Local Type . LOOP ) loop result name = genname ( \"loop\" ) with ctx . new recur point ( node . loop id , Recur Type . LOOP , binding names = binding names ) : loop body ast : List [ ast . AST ] = [ ] body ast = synthetic do to py ast ( ctx , node . body ) loop body ast . extend ( body ast . dependencies ) loop body ast . append ( ast . Assign ( targets = [ ast . Name ( id = loop result name , ctx = ast . Store ( ) ) ] , value = body ast . node , ) ) loop body ast . append ( ast . Break ( ) ) return Generated Py AST ( node = load attr ( loop result name ) , dependencies = list ( chain ( [ ast . Assign ( targets = [ ast . Name ( id = loop result name , ctx = ast . Store ( ) ) ] , value = ast . Name Constant ( None ) , ) ] , init bindings , [ ast . While ( test = ast . Name Constant ( True ) , body = loop body ast , orelse = [ ] , ) ] , ) ) , )", "predictions": ["convert an ctx read read to an ctx table ."], "references": ["return a python ast node for a loop * expression ."], "bleu": 0.11406351620367239, "rouge_l": 0.09442724458204334}
{"id": 3455, "code": "def quote to py ast ( ctx : Generator Context , node : Quote ) -> Generated Py AST : assert node . op == Node Op . QUOTE return const node to py ast ( ctx , node . expr )", "predictions": ["convert an ctx node to an astroid return ."], "references": ["return a python ast node for a quote expression ."], "bleu": 0.15019394384099988, "rouge_l": 0.20854700854700853}
{"id": 3456, "code": "def fn recur to py ast ( ctx : Generator Context , node : Recur ) -> Generated Py AST : assert node . op == Node Op . RECUR assert ctx . recur point . is variadic is not None recur nodes : List [ ast . AST ] = [ ] recur deps : List [ ast . AST ] = [ ] for expr in node . exprs : expr ast = gen py ast ( ctx , expr ) recur nodes . append ( expr ast . node ) recur deps . extend ( expr ast . dependencies ) return Generated Py AST ( node = ast . Call ( func = TRAMPOLINE ARGS FN NAME , args = list ( chain ( [ ast . Name Constant ( ctx . recur point . is variadic ) ] , recur nodes ) ) , keywords = [ ] , ) , dependencies = recur deps , )", "predictions": ["convert an ast to a list of ast ."], "references": ["return a python ast node for recur occurring inside a fn * ."], "bleu": 0.10761866342063775, "rouge_l": 0.26406926406926406}
{"id": 3457, "code": "def deftype method recur to py ast ( ctx : Generator Context , node : Recur ) -> Generated Py AST : assert node . op == Node Op . RECUR recur nodes : List [ ast . AST ] = [ ] recur deps : List [ ast . AST ] = [ ] for expr in node . exprs : expr ast = gen py ast ( ctx , expr ) recur nodes . append ( expr ast . node ) recur deps . extend ( expr ast . dependencies ) this entry = ctx . symbol table . find symbol ( ctx . current this ) assert this entry is not None , \"Field type local must have this\" return Generated Py AST ( node = ast . Call ( func = TRAMPOLINE ARGS FN NAME , args = list ( chain ( [ ast . Name Constant ( ctx . recur point . is variadic ) , ast . Name ( id = this entry . munged , ctx = ast . Load ( ) ) , ] , recur nodes , ) ) , keywords = [ ] , ) , dependencies = recur deps , )", "predictions": ["convert an ast to a deftype instance ."], "references": ["return a python ast node for recur occurring inside a deftype * method ."], "bleu": 0.11327490115090784, "rouge_l": 0.346590909090909}
{"id": 3458, "code": "def loop recur to py ast ( ctx : Generator Context , node : Recur ) -> Generated Py AST : assert node . op == Node Op . RECUR recur deps : List [ ast . AST ] = [ ] recur targets : List [ ast . Name ] = [ ] recur exprs : List [ ast . AST ] = [ ] for name , expr in zip ( ctx . recur point . binding names , node . exprs ) : expr ast = gen py ast ( ctx , expr ) recur deps . extend ( expr ast . dependencies ) recur targets . append ( ast . Name ( id = name , ctx = ast . Store ( ) ) ) recur exprs . append ( expr ast . node ) if len ( recur targets ) == 1 : assert len ( recur exprs ) == 1 recur deps . append ( ast . Assign ( targets = recur targets , value = recur exprs [ 0 ] ) ) else : recur deps . append ( ast . Assign ( targets = [ ast . Tuple ( elts = recur targets , ctx = ast . Store ( ) ) ] , value = ast . Tuple ( elts = recur exprs , ctx = ast . Load ( ) ) , ) ) recur deps . append ( ast . Continue ( ) ) return Generated Py AST ( node = ast . Name Constant ( None ) , dependencies = recur deps )", "predictions": ["convert an ast node to a list of ast nodes ."], "references": ["return a python ast node for recur occurring inside a loop ."], "bleu": 0.15553014371537452, "rouge_l": 0.34512022630834516}
{"id": 3459, "code": "def set bang to py ast ( ctx : Generator Context , node : Set Bang ) -> Generated Py AST : assert node . op == Node Op . SET BANG val temp name = genname ( \"set bang val\" ) val ast = gen py ast ( ctx , node . val ) target = node . target assert isinstance ( target , ( Host Field , Local , Var Ref ) ) , f\"invalid set! target type {type(target)}\" if isinstance ( target , Host Field ) : target ast = interop prop to py ast ( ctx , target , is assigning = True ) elif isinstance ( target , Var Ref ) : target ast = var sym to py ast ( ctx , target , is assigning = True ) elif isinstance ( target , Local ) : target ast = local sym to py ast ( ctx , target , is assigning = True ) else : raise Generator Exception ( f\"invalid set! target type {type(target)}\" , lisp ast = target ) return Generated Py AST ( node = ast . Name ( id = val temp name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( val ast . dependencies , [ ast . Assign ( targets = [ ast . Name ( id = val temp name , ctx = ast . Store ( ) ) ] , value = val ast . node , ) ] , target ast . dependencies , [ ast . Assign ( targets = [ target ast . node ] , value = val ast . node ) ] , ) ) , )", "predictions": ["converts an ast to an ast ."], "references": ["return a python ast node for a set! expression ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 3460, "code": "def throw to py ast ( ctx : Generator Context , node : Throw ) -> Generated Py AST : assert node . op == Node Op . THROW throw fn = genname ( THROW PREFIX ) exc ast = gen py ast ( ctx , node . exception ) raise body = ast . Raise ( exc = exc ast . node , cause = None ) return Generated Py AST ( node = ast . Call ( func = ast . Name ( id = throw fn , ctx = ast . Load ( ) ) , args = [ ] , keywords = [ ] ) , dependencies = [ ast . Function Def ( name = throw fn , args = ast . arguments ( args = [ ] , kwarg = None , vararg = None , kwonlyargs = [ ] , defaults = [ ] , kw defaults = [ ] , ) , body = list ( chain ( exc ast . dependencies , [ raise body ] ) ) , decorator list = [ ] , returns = None , ) ] , )", "predictions": ["convert an ast to a ast ."], "references": ["return a python ast node for a throw expression ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 3461, "code": "def try to py ast ( ctx : Generator Context , node : Try ) -> Generated Py AST : assert node . op == Node Op . TRY try expr name = genname ( \"try expr\" ) body ast = synthetic do to py ast ( ctx , node . body ) catch handlers = list ( map ( partial ( catch to py ast , ctx , try expr name = try expr name ) , node . catches ) ) finallys : List [ ast . AST ] = [ ] if node . finally is not None : finally ast = synthetic do to py ast ( ctx , node . finally ) finallys . extend ( map ( statementize , finally ast . dependencies ) ) finallys . append ( statementize ( finally ast . node ) ) return Generated Py AST ( node = ast . Name ( id = try expr name , ctx = ast . Load ( ) ) , dependencies = [ ast . Try ( body = list ( chain ( body ast . dependencies , [ ast . Assign ( targets = [ ast . Name ( id = try expr name , ctx = ast . Store ( ) ) ] , value = body ast . node , ) ] , ) ) , handlers = catch handlers , orelse = [ ] , finalbody = finallys , ) ] , )", "predictions": ["convert an ast node to a synthetic ast ."], "references": ["return a python ast node for a try expression ."], "bleu": 0.18885888592159467, "rouge_l": 0.41709401709401706}
{"id": 3462, "code": "def local sym to py ast ( ctx : Generator Context , node : Local , is assigning : bool = False ) -> Generated Py AST : assert node . op == Node Op . LOCAL sym entry = ctx . symbol table . find symbol ( sym . symbol ( node . name ) ) assert sym entry is not None if node . local == Local Type . FIELD : this entry = ctx . symbol table . find symbol ( ctx . current this ) assert this entry is not None , \"Field type local must have this\" return Generated Py AST ( node = load attr ( f\"{this entry.munged}.{sym entry.munged}\" , ctx = ast . Store ( ) if is assigning else ast . Load ( ) , ) ) else : return Generated Py AST ( node = ast . Name ( id = sym entry . munged , ctx = ast . Store ( ) if is assigning else ast . Load ( ) ) )", "predictions": ["converts an ast to a local symbol ."], "references": ["generate a python ast node for accessing a locally defined python variable ."], "bleu": 0.10207878682119532, "rouge_l": 0.2739520958083832}
{"id": 3463, "code": "def var find to py ast ( var name : str , ns name : str , py var ctx : ast . AST ) -> Generated Py AST : return Generated Py AST ( node = ast . Attribute ( value = ast . Call ( func = FIND VAR FN NAME , args = [ ast . Call ( func = NEW SYM FN NAME , args = [ ast . Str ( var name ) ] , keywords = [ ast . keyword ( arg = \"ns\" , value = ast . Str ( ns name ) ) ] , ) ] , keywords = [ ] , ) , attr = \"value\" , ctx = py var ctx , ) )", "predictions": ["convert python code to function function ."], "references": ["generate var . find calls for the named symbol ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 3464, "code": "def interop call to py ast ( ctx : Generator Context , node : Host Call ) -> Generated Py AST : assert node . op == Node Op . HOST CALL target ast = gen py ast ( ctx , node . target ) args deps , args nodes = collection ast ( ctx , node . args ) return Generated Py AST ( node = ast . Call ( func = ast . Attribute ( value = target ast . node , attr = munge ( node . method , allow builtins = True ) , ctx = ast . Load ( ) , ) , args = list ( args nodes ) , keywords = [ ] , ) , dependencies = list ( chain ( target ast . dependencies , args deps ) ) , )", "predictions": ["finds an ast to a interop ."], "references": ["generate a python ast node for python interop method calls ."], "bleu": 0.1319006407505858, "rouge_l": 0.32049036777583184}
{"id": 3465, "code": "def interop prop to py ast ( ctx : Generator Context , node : Host Field , is assigning : bool = False ) -> Generated Py AST : assert node . op == Node Op . HOST FIELD target ast = gen py ast ( ctx , node . target ) return Generated Py AST ( node = ast . Attribute ( value = target ast . node , attr = munge ( node . field ) , ctx = ast . Store ( ) if is assigning else ast . Load ( ) , ) , dependencies = target ast . dependencies , )", "predictions": ["converts an ast node to a ast ."], "references": ["generate a python ast node for python interop property access ."], "bleu": 0.16481400866629634, "rouge_l": 0.3070469798657718}
{"id": 3466, "code": "def with meta to py ast ( ctx : Generator Context , node : With Meta , * * kwargs ) -> Generated Py AST : assert node . op == Node Op . WITH META handle expr = WITH META EXPR HANDLER . get ( node . expr . op ) assert ( handle expr is not None ) , \"No expression handler for with-meta child node type\" return handle expr ( ctx , node . expr , meta node = node . meta , * * kwargs )", "predictions": ["see an ast . py ."], "references": ["generate a python ast node for python interop method calls ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 3467, "code": "def py module preamble ( ctx : Generator Context , ) -> Generated Py AST : preamble : List [ ast . AST ] = [ ] preamble . extend ( module imports ( ctx ) ) preamble . append ( from module import ( ) ) preamble . append ( ns var ( ) ) return Generated Py AST ( node = ast . Name Constant ( None ) , dependencies = preamble )", "predictions": ["return list of preamble imports ."], "references": ["bootstrap a new module with imports and other boilerplate ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 3468, "code": "def set ( members : Iterable [ T ] , meta = None ) -> Set [ T ] : return Set ( pset ( members ) , meta = meta )", "predictions": ["set members of members ."], "references": ["creates a new set ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 3469, "code": "def s ( * members : T , meta = None ) -> Set [ T ] : return Set ( pset ( members ) , meta = meta )", "predictions": ["return a string with the first meta of the given meta string ."], "references": ["creates a new set from members ."], "bleu": 0.10571070857151538, "rouge_l": 0.21143847487001732}
{"id": 3470, "code": "def visit Except Handler ( self , node : ast . Except Handler ) -> Optional [ ast . AST ] : new node = self . generic visit ( node ) assert isinstance ( new node , ast . Except Handler ) return ast . copy location ( ast . Except Handler ( type = new node . type , name = new node . name , body = filter dead code ( new node . body ) , ) , new node , )", "predictions": ["visit an ast node ."], "references": ["eliminate dead code from except handler bodies ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 3471, "code": "def visit Function Def ( self , node : ast . Function Def ) -> Optional [ ast . AST ] : new node = self . generic visit ( node ) assert isinstance ( new node , ast . Function Def ) return ast . copy location ( ast . Function Def ( name = new node . name , args = new node . args , body = filter dead code ( new node . body ) , decorator list = new node . decorator list , returns = new node . returns , ) , new node , )", "predictions": ["visit an ast node ."], "references": ["eliminate dead code from function bodies ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 3472, "code": "def visit While ( self , node : ast . While ) -> Optional [ ast . AST ] : new node = self . generic visit ( node ) assert isinstance ( new node , ast . While ) return ast . copy location ( ast . While ( test = new node . test , body = filter dead code ( new node . body ) , orelse = filter dead code ( new node . orelse ) , ) , new node , )", "predictions": ["visit an ast node ."], "references": ["eliminate dead code from while bodies ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 3473, "code": "def visit Try ( self , node : ast . Try ) -> Optional [ ast . AST ] : new node = self . generic visit ( node ) assert isinstance ( new node , ast . Try ) return ast . copy location ( ast . Try ( body = filter dead code ( new node . body ) , handlers = new node . handlers , orelse = filter dead code ( new node . orelse ) , finalbody = filter dead code ( new node . finalbody ) , ) , new node , )", "predictions": ["visit the try node and assemble it into the ast node ."], "references": ["eliminate dead code from except try bodies ."], "bleu": 0.11498759556447223, "rouge_l": 0.2074829931972789}
{"id": 3474, "code": "def nthrest ( coll , i : int ) : while True : if coll is None : return None if i == 0 : return coll i -= 1 coll = rest ( coll )", "predictions": ["return the first value of a number or none if not found ."], "references": ["returns the nth rest sequence of coll or coll if i is 0 ."], "bleu": 0.11640435130573178, "rouge_l": 0.36791314837153194}
{"id": 3475, "code": "def nthnext ( coll , i : int ) -> Optional [ I Seq ] : while True : if coll is None : return None if i == 0 : return to seq ( coll ) i -= 1 coll = next ( coll )", "predictions": ["return the first item of i i . e . 0 if i ."], "references": ["returns the nth next sequence of coll ."], "bleu": 0.10511846841633776, "rouge_l": 0.28683385579937304}
{"id": 3476, "code": "def to seq ( o ) -> Optional [ I Seq ] : if o is None : return None if isinstance ( o , I Seq ) : return seq or nil ( o ) if isinstance ( o , I Seqable ) : return seq or nil ( o . seq ( ) ) return seq or nil ( lseq . sequence ( o ) )", "predictions": ["convert a sequence to an seq ."], "references": ["coerce the argument o to a iseq . if o is none return none ."], "bleu": 0.0704451546128839, "rouge_l": 0.17062937062937064}
{"id": 3477, "code": "def concat ( * seqs ) -> I Seq : allseqs = lseq . sequence ( itertools . chain ( * filter ( None , map ( to seq , seqs ) ) ) ) if allseqs is None : return lseq . EMPTY return allseqs", "predictions": ["concatenate two sequences together"], "references": ["concatenate the sequences given by seqs into a single iseq ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 3478, "code": "def partial ( f , * args ) : @ functools . wraps ( f ) def partial f ( * inner args ) : return f ( * itertools . chain ( args , inner args ) ) return partial f", "predictions": ["decorator to partial partial partial partial ."], "references": ["return a function which is the partial application of f with args ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 3479, "code": "def contains ( coll , k ) : if isinstance ( coll , I Associative ) : return coll . contains ( k ) return k in coll", "predictions": ["returns true if coll contains coll ."], "references": ["return true if o contains the key k ."], "bleu": 0.20873176328735715, "rouge_l": 0.48897795591182364}
{"id": 3480, "code": "def get ( m , k , default = None ) : if isinstance ( m , I Associative ) : return m . entry ( k , default = default ) try : return m [ k ] except ( Key Error , Index Error , Type Error ) as e : logger . debug ( \"Ignored %s: %s\" , type ( e ) . name , e ) return default", "predictions": ["get the value of a k - style entry ."], "references": ["return the value of k in m . return default if k not found in m ."], "bleu": 0.12894015634133696, "rouge_l": 0.3538283062645011}
{"id": 3481, "code": "def to lisp ( o , keywordize keys : bool = True ) : if not isinstance ( o , ( dict , frozenset , list , set , tuple ) ) : return o else : return to lisp backup ( o , keywordize keys = keywordize keys )", "predictions": ["helper function to convert a list of keys to a lisp"], "references": ["recursively convert python collections into lisp collections ."], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 3482, "code": "def to py ( o , keyword fn : Callable [ [ kw . Keyword ] , Any ] = kw name ) : if isinstance ( o , I Seq ) : return to py list ( o , keyword fn = keyword fn ) elif not isinstance ( o , ( I Persistent List , I Persistent Map , I Persistent Set , I Persistent Vector ) ) : return o else : return to py backup ( o , keyword fn = keyword fn )", "predictions": ["helper function to convert an argument to a keyword argument ."], "references": ["recursively convert lisp collections into python collections ."], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 3483, "code": "def collect args ( args ) -> I Seq : if isinstance ( args , tuple ) : return llist . list ( args ) raise Type Error ( \"Python variadic arguments should always be a tuple\" )", "predictions": ["collect arguments from the command line ."], "references": ["collect python starred arguments into a basilisp list ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 3484, "code": "def init ns var ( which ns : str = CORE NS , ns var name : str = NS VAR NAME ) -> Var : core sym = sym . Symbol ( which ns ) core ns = Namespace . get or create ( core sym ) ns var = Var . intern ( core sym , sym . Symbol ( ns var name ) , core ns , dynamic = True ) logger . debug ( f\"Created namespace variable {sym.symbol(ns var name, ns=which ns)}\" ) return ns var", "predictions": ["create a namespace variable variable ."], "references": ["initialize the dynamic * ns * variable in the namespace which_ns ."], "bleu": 0.09663861439684919, "rouge_l": 0.20962199312714777}
{"id": 3485, "code": "def set current ns ( ns name : str , module : types . Module Type = None , ns var name : str = NS VAR NAME , ns var ns : str = NS VAR NS , ) -> Var : symbol = sym . Symbol ( ns name ) ns = Namespace . get or create ( symbol , module = module ) ns var sym = sym . Symbol ( ns var name , ns = ns var ns ) ns var = Maybe ( Var . find ( ns var sym ) ) . or else raise ( lambda : Runtime Exception ( f\"Dynamic Var {sym.Symbol(ns var name, ns=ns var ns)} not bound!\" ) ) ns var . push bindings ( ns ) logger . debug ( f\"Setting {ns var sym} to {ns}\" ) return ns var", "predictions": ["set a module in the current module ."], "references": ["set the value of the dynamic variable * ns * in the current thread ."], "bleu": 0.13769637489372183, "rouge_l": 0.41216216216216217}
{"id": 3486, "code": "def get current ns ( ns var name : str = NS VAR NAME , ns var ns : str = NS VAR NS ) -> Namespace : ns sym = sym . Symbol ( ns var name , ns = ns var ns ) ns : Namespace = Maybe ( Var . find ( ns sym ) ) . map ( lambda v : v . value ) . or else raise ( lambda : Runtime Exception ( f\"Dynamic Var {ns sym} not bound!\" ) ) return ns", "predictions": ["get a namespace namespace namespace ."], "references": ["get the value of the dynamic variable * ns * in the current thread ."], "bleu": 0.054546736148076896, "rouge_l": 0.17681159420289855}
{"id": 3487, "code": "def resolve alias ( s : sym . Symbol , ns : Optional [ Namespace ] = None ) -> sym . Symbol : if s in SPECIAL FORMS : return s ns = Maybe ( ns ) . or else ( get current ns ) if s . ns is not None : aliased ns = ns . get alias ( sym . symbol ( s . ns ) ) if aliased ns is not None : return sym . symbol ( s . name , aliased ns . name ) else : return s else : which var = ns . find ( sym . symbol ( s . name ) ) if which var is not None : return sym . symbol ( which var . name . name , which var . ns . name ) else : return sym . symbol ( s . name , ns = ns . name )", "predictions": ["resolve an alias to a symbol ."], "references": ["resolve the aliased symbol in the current namespace ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 3488, "code": "def add generated python ( generated python : str , var name : str = GENERATED PYTHON VAR NAME , which ns : Optional [ str ] = None , ) -> None : if which ns is None : which ns = get current ns ( ) . name ns sym = sym . Symbol ( var name , ns = which ns ) v = Maybe ( Var . find ( ns sym ) ) . or else ( lambda : Var . intern ( sym . symbol ( which ns ) , sym . symbol ( var name ) , \"\" , dynamic = True , meta = lmap . map ( { PRIVATE META KEY : True } ) , ) ) v . value = v . value + generated python", "predictions": ["fn to fn for a to a to a to a to fn is added to the expr expr is a to the expr is not a to the expr expr"], "references": ["add generated python code to a dynamic variable in which_ns ."], "bleu": 0.0513487742994337, "rouge_l": 0.10418445772843724}
{"id": 3489, "code": "def print generated python ( var name : str = PRINT GENERATED PY VAR NAME , core ns name : str = CORE NS ) -> bool : ns sym = sym . Symbol ( var name , ns = core ns name ) return ( Maybe ( Var . find ( ns sym ) ) . map ( lambda v : v . value ) . or else raise ( lambda : Runtime Exception ( f\"Dynamic Var {ns sym} not bound!\" ) ) )", "predictions": ["print for debugging ."], "references": ["return the value of the * print - generated - python * dynamic variable ."], "bleu": 0.025419978385188596, "rouge_l": 0.190625}
{"id": 3490, "code": "def intern ( ns : sym . Symbol , name : sym . Symbol , val , dynamic : bool = False , meta = None ) -> \"Var\" : var ns = Namespace . get or create ( ns ) var = var ns . intern ( name , Var ( var ns , name , dynamic = dynamic , meta = meta ) ) var . root = val return var", "predictions": ["create a loop ."], "references": ["intern the value bound to the symbol name in namespace ns ."], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 3491, "code": "def intern unbound ( ns : sym . Symbol , name : sym . Symbol , dynamic : bool = False , meta = None ) -> \"Var\" : var ns = Namespace . get or create ( ns ) return var ns . intern ( name , Var ( var ns , name , dynamic = dynamic , meta = meta ) )", "predictions": ["create a new set ctx ctx ctx ctx"], "references": ["create a new unbound var instance to the symbol name in namespace ns ."], "bleu": 0.1409894129706051, "rouge_l": 0.25994318181818177}
{"id": 3492, "code": "def add alias ( self , alias : sym . Symbol , namespace : \"Namespace\" ) -> None : self . aliases . swap ( lambda m : m . assoc ( alias , namespace ) )", "predictions": ["throw an to the list of aliases . . . . . . . . . ."], "references": ["add a symbol alias for the given namespace ."], "bleu": 0.07994607499472013, "rouge_l": 0.1628838451268358}
{"id": 3493, "code": "def add refer ( self , sym : sym . Symbol , var : Var ) -> None : if not var . is private : self . refers . swap ( lambda s : s . assoc ( sym , var ) )", "predictions": ["try to try to try to try to try to try to try to try to try to try to try to try to try to try to try to try"], "references": ["refer var in this namespace under the name sym ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3494, "code": "def get refer ( self , sym : sym . Symbol ) -> Optional [ Var ] : return self . refers . entry ( sym , None )", "predictions": ["local method to local db"], "references": ["get the var referred by symbol or none if it does not exist ."], "bleu": 0.037948473198912445, "rouge_l": 0.0}
{"id": 3495, "code": "def refer all ( cls , refers : lmap . Map , other ns interns : lmap . Map ) -> lmap . Map : final refers = refers for entry in other ns interns : s : sym . Symbol = entry . key var : Var = entry . value if not var . is private : final refers = final refers . assoc ( s , var ) return final refers", "predictions": ["return find of refers"], "references": ["refer all _public_ interns from another namespace ."], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 3496, "code": "def refer all ( self , other ns : \"Namespace\" ) : self . refers . swap ( Namespace . refer all , other ns . interns )", "predictions": ["swap all ns objects"], "references": ["refer all the vars in the other namespace ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 3497, "code": "def list ( members , meta = None ) -> List : return List ( plist ( iterable = members ) , meta = meta )", "predictions": ["node list of to list of to to to list of to to to node ."], "references": ["creates a new list ."], "bleu": 0.08513012360883544, "rouge_l": 0.21034482758620687}
{"id": 3498, "code": "def l ( * members , meta = None ) -> List : return List ( plist ( iterable = members ) , meta = meta )", "predictions": ["factory function for list of py py ."], "references": ["creates a new list from members ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 3499, "code": "def change style ( style , representer ) : def new representer ( dumper , data ) : scalar = representer ( dumper , data ) scalar . style = style return scalar return new representer", "predictions": ["py module with ctx"], "references": ["this function is used to format the key value as a multi - line string maintaining the line breaks"], "bleu": 0.0071055938730635, "rouge_l": 0.0}
{"id": 3500, "code": "def delete ( self , * args ) : cache = get cache ( ) key = self . get cache key ( * args ) if key in cache : del cache [ key ]", "predictions": ["set the cache key ."], "references": ["remove the key from the request cache and from memcache ."], "bleu": 0.10339832360529193, "rouge_l": 0.35124760076775424}
{"id": 3501, "code": "async def connect ( self ) : self . reader , self . writer = await asyncio . open connection ( self . host , self . port , loop = self . loop ) self . welcome msg = await self . reader . read ( self . buffer size )", "predictions": ["connect - . - . - . connection ."], "references": ["connect to lasaf through a cam - socket ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 3502, "code": "async def receive ( self ) : try : incomming = await self . reader . read ( self . buffer size ) except OS Error : return [ ] return parse receive ( incomming )", "predictions": ["read from socket . . . . . . . . . . . ."], "references": ["receive message from socket interface as list of ordereddict ."], "bleu": 0.11633270842295028, "rouge_l": 0.2489795918367347}
{"id": 3503, "code": "def dump ( ndb model , fp , * * kwargs ) : for chunk in Ndb Encoder ( * * kwargs ) . iterencode ( ndb model ) : fp . write ( chunk )", "predictions": ["visit a ndb instance to a file - like object"], "references": ["custom json dump using the custom encoder above ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 3504, "code": "def object hook handler ( self , val ) : return { k : self . decode date ( v ) for k , v in val . iteritems ( ) }", "predictions": ["decode handler handler handler"], "references": ["handles decoding of nested date strings ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 3505, "code": "def decode date ( self , val ) : if isinstance ( val , basestring ) and val . count ( '-' ) == 2 and len ( val ) > 9 : try : dt = dateutil . parser . parse ( val ) if val . endswith ( ( '+00:00' , '-00:00' , 'Z' ) ) : dt = dt . replace ( tzinfo = None ) return dt except ( Type Error , Value Error ) : pass return val", "predictions": ["visit a date from a date"], "references": ["tries to decode strings that look like dates into datetime objects ."], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 3506, "code": "def decode ( self , val ) : new val = self . decode date ( val ) if val != new val : return new val return json . JSON Decoder . decode ( self , val )", "predictions": ["decode is a if the value is not a if it is not none ."], "references": ["override of the default decode method that also uses decode_date ."], "bleu": 0.09782375748961449, "rouge_l": 0.1582360570687419}
{"id": 3507, "code": "def default ( self , obj ) : obj type = type ( obj ) if obj type not in self . ndb type encoding : if hasattr ( obj , ' metaclass ' ) : obj type = obj . metaclass else : for ndb type in NDB TYPES : if isinstance ( obj , ndb type ) : obj type = ndb type break fn = self . ndb type encoding . get ( obj type ) if fn : return fn ( obj ) return json . JSON Encoder . default ( self , obj )", "predictions": ["default the ndb type"], "references": ["overriding the default jsonencoder . default for ndb support ."], "bleu": 0.09534061816653486, "rouge_l": 0.26521739130434785}
{"id": 3508, "code": "def validate version ( ) : import leicacam version string = leicacam . version versions = version string . split ( '.' , 3 ) try : for ver in versions : int ( ver ) except Value Error : print ( 'Only integers are allowed in release version, ' 'please adjust current version {}' . format ( version string ) ) return None return version string", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["validate version before release ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3509, "code": "def robust topological sort ( graph : Graph ) -> list : assert check argument types ( ) components = strongly connected components ( graph ) node component = { } for component in components : for node in component : node component [ node ] = component component graph = { } for component in components : component graph [ component ] = [ ] for node in graph : node c = node component [ node ] for successor in graph [ node ] : successor c = node component [ successor ] if node c != successor c : component graph [ node c ] . append ( successor c ) return topological sort ( component graph )", "predictions": ["* * * * * * * * * * * * * * * * form * ."], "references": ["identify strongly connected components then perform a topological sort of those components ."], "bleu": 0.06439931429457924, "rouge_l": 0.06468716861081655}
{"id": 3510, "code": "def logger ( function ) : @ functools . wraps ( function ) def wrapper ( * args , * * kwargs ) : \"\"\"Wrap function.\"\"\" sep = kwargs . get ( 'sep' , ' ' ) end = kwargs . get ( 'end' , '' ) out = sep . join ( [ repr ( x ) for x in args ] ) out = out + end LOGGER . debug ( out ) return function ( * args , * * kwargs ) return wrapper", "predictions": ["decorator to partial a partial partial partial . ."], "references": ["decorate passed in function and log message to module logger ."], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 3511, "code": "def connect ( self ) : self . socket = socket . socket ( ) self . socket . connect ( ( self . host , self . port ) ) self . socket . settimeout ( False ) sleep ( self . delay ) self . welcome msg = self . socket . recv ( self . buffer size )", "predictions": ["contains the websocket connection to the server . ."], "references": ["connect to lasaf through a cam - socket ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 3512, "code": "def flush ( self ) : debug ( 'flushing incomming socket messages' ) try : while True : msg = self . socket . recv ( self . buffer size ) debug ( b'< ' + msg ) except socket . error : pass", "predictions": ["get the socket socket socket socket"], "references": ["flush incomming socket messages ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 3513, "code": "def receive ( self ) : try : incomming = self . socket . recv ( self . buffer size ) except socket . error : return [ ] return parse receive ( incomming )", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["receive message from socket interface as list of ordereddict ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3514, "code": "def enable ( self , slide = 0 , wellx = 1 , welly = 1 , fieldx = 1 , fieldy = 1 ) : cmd = [ ( 'cmd' , 'enable' ) , ( 'slide' , str ( slide ) ) , ( 'wellx' , str ( wellx ) ) , ( 'welly' , str ( welly ) ) , ( 'fieldx' , str ( fieldx ) ) , ( 'fieldy' , str ( fieldy ) ) , ( 'value' , 'true' ) ] self . send ( cmd ) return self . wait for ( * cmd [ 0 ] )", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["enable a given scan field ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3515, "code": "def save template ( self , filename = \"{Scanning Template}leicacam.xml\" ) : cmd = [ ( 'sys' , '0' ) , ( 'cmd' , 'save' ) , ( 'fil' , str ( filename ) ) ] self . send ( cmd ) return self . wait for ( * cmd [ 0 ] )", "predictions": ["collect a args from the remote host"], "references": ["save scanning template to filename ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3516, "code": "def get information ( self , about = 'stage' ) : cmd = [ ( 'cmd' , 'getinfo' ) , ( 'dev' , str ( about ) ) ] self . send ( cmd ) return self . wait for ( * cmd [ 1 ] )", "predictions": ["init the ns command"], "references": ["get information about given keyword . defaults to stage ."], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 3517, "code": "def locate package json ( ) : directory = settings . SYSTEMJS PACKAGE JSON DIR if not directory : raise Improperly Configured ( \"Could not locate 'package.json'. Set SYSTEMJS PACKAGE JSON DIR \" \"to the directory that holds 'package.json'.\" ) path = os . path . join ( directory , 'package.json' ) if not os . path . isfile ( path ) : raise Improperly Configured ( \"'package.json' does not exist, tried looking in %s\" % path ) return path", "predictions": ["return the create current current current current current current current current current current current current current current current current current current current current current current current current current current current current"], "references": ["find and return the location of package . json ."], "bleu": 0.0513487742994337, "rouge_l": 0.10748898678414096}
{"id": 3518, "code": "def parse package json ( ) : with open ( locate package json ( ) ) as pjson : data = json . loads ( pjson . read ( ) ) return data", "predictions": ["get the ns from the ns"], "references": ["extract the jspm configuration from package . json ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 3519, "code": "def validate yourls response ( response , data ) : try : response . raise for status ( ) except HTTP Error as http exc : http error info = sys . exc info ( ) reraise = False try : jsondata = response . json ( ) except Value Error : reraise = True else : logger . debug ( 'Received error {response} with JSON {json}' , response = response , json = jsondata ) handle api error with json ( http exc , jsondata , response ) if reraise : six . reraise ( * http error info ) else : jsondata = response . json ( ) logger . debug ( 'Received {response} with JSON {json}' , response = response , json = jsondata ) if { 'status' , 'code' , 'message' } <= set ( jsondata . keys ( ) ) : status = jsondata [ 'status' ] code = jsondata [ 'code' ] message = jsondata [ 'message' ] if status == 'fail' : if code == 'error:keyword' : raise YOURLS Keyword Exists Error ( message , keyword = data [ 'keyword' ] ) elif code == 'error:url' : url = json to shortened url ( jsondata [ 'url' ] , jsondata [ 'shorturl' ] ) raise YOURLSURL Exists Error ( message , url = url ) else : raise YOURLSAPI Error ( message ) else : return jsondata else : return jsondata", "predictions": ["resolve the response and return to the response"], "references": ["validate response from yourls server ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 3520, "code": "def interp dep vector ( wave , indep vector ) : dep vector is int = wave . dep vector . dtype . name . startswith ( \"int\" ) dep vector is complex = wave . dep vector . dtype . name . startswith ( \"complex\" ) if ( wave . interp , wave . indep scale ) == ( \"CONTINUOUS\" , \"LOG\" ) : wave interp func = scipy . interpolate . interp1d ( np . log10 ( wave . indep vector ) , wave . dep vector ) ret = wave interp func ( np . log10 ( indep vector ) ) elif ( wave . interp , wave . indep scale ) == ( \"CONTINUOUS\" , \"LINEAR\" ) : dep vector = ( wave . dep vector . astype ( np . float64 ) if not dep vector is complex else wave . dep vector ) wave interp func = scipy . interpolate . interp1d ( wave . indep vector , dep vector ) ret = wave interp func ( indep vector ) else : wave interp func = scipy . interpolate . interp1d ( wave . indep vector , wave . dep vector , kind = \"zero\" ) ret = wave interp func ( indep vector ) eq comp = np . all ( np . isclose ( wave . indep vector [ - 1 ] , indep vector [ - 1 ] , FP RTOL , FP ATOL ) ) if eq comp : ret [ - 1 ] = wave . dep vector [ - 1 ] round ret = np . round ( ret , 0 ) return ( round ret . astype ( \"int\" ) if ( dep vector is int and np . all ( np . isclose ( round ret , ret , FP RTOL , FP ATOL ) ) ) else ret )", "predictions": ["wrap the dep vector for the given wave ."], "references": ["create new dependent variable vector ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 3521, "code": "def get indep vector ( wave a , wave b ) : exobj = pexdoc . exh . addex ( Runtime Error , \"Independent variable ranges do not overlap\" ) min bound = max ( np . min ( wave a . indep vector ) , np . min ( wave b . indep vector ) ) max bound = min ( np . max ( wave a . indep vector ) , np . max ( wave b . indep vector ) ) exobj ( bool ( min bound > max bound ) ) raw range = np . unique ( np . concatenate ( ( wave a . indep vector , wave b . indep vector ) ) ) return raw range [ np . logical and ( min bound <= raw range , raw range <= max bound ) ]", "predictions": ["get the indep vector for the given wave ."], "references": ["create new independent variable vector ."], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 3522, "code": "def verify compatibility ( wave a , wave b , check dep units = True ) : exobj = pexdoc . exh . addex ( Runtime Error , \"Waveforms are not compatible\" ) ctuple = ( bool ( wave a . indep scale != wave b . indep scale ) , bool ( wave a . dep scale != wave b . dep scale ) , bool ( wave a . indep units != wave b . indep units ) , ( bool ( wave a . dep units != wave b . dep units ) if check dep units else False ) , bool ( wave a . interp != wave b . interp ) , ) exobj ( any ( ctuple ) )", "predictions": ["verify that the compatibility has a compatibility ."], "references": ["verify that two waveforms can be combined with various mathematical functions ."], "bleu": 0.13755608571892394, "rouge_l": 0.28955696202531644}
{"id": 3523, "code": "def trace pars ( mname ) : pickle fname = os . path . join ( os . path . dirname ( file ) , \"{0}.pkl\" . format ( mname ) ) ddir = os . path . dirname ( os . path . dirname ( file ) ) moddb fname = os . path . join ( ddir , \"moddb.json\" ) in callables fname = moddb fname if os . path . exists ( moddb fname ) else None out callables fname = os . path . join ( ddir , \"{0}.json\" . format ( mname ) ) noption = os . environ . get ( \"NOPTION\" , None ) exclude = [ \" pytest\" , \"execnet\" ] partuple = collections . namedtuple ( \"Par Tuple\" , [ \"pickle fname\" , \"in callables fname\" , \"out callables fname\" , \"noption\" , \"exclude\" , ] , ) return partuple ( pickle fname , in callables fname , out callables fname , noption , exclude )", "predictions": ["return the namedtuple of the trace ."], "references": ["define trace parameters ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 3524, "code": "def run trace ( mname , fname , module prefix , callable names , no print , module exclude = None , callable exclude = None , debug = False , ) : module exclude = [ ] if module exclude is None else module exclude callable exclude = [ ] if callable exclude is None else callable exclude par = trace pars ( mname ) start time = datetime . datetime . now ( ) with pexdoc . exdoc . Ex Doc Cxt ( exclude = par . exclude + module exclude , pickle fname = par . pickle fname , in callables fname = par . in callables fname , out callables fname = par . out callables fname , no print = no print , ) as exdoc obj : fname = os . path . realpath ( os . path . join ( os . path . dirname ( file ) , \"..\" , \"..\" , \"tests\" , \"test {0}.py\" . format ( fname ) , ) ) test cmd = ( [ \"--color=yes\" ] + ( [ \"-s\" , \"-vv\" ] if debug else [ \"-q\" , \"-q\" , \"-q\" ] ) + [ \"--disable-warnings\" ] + [ \"-x\" ] + ( [ par . noption ] if par . noption else [ ] ) + [ \"-m \" + mname ] + [ fname ] ) with warnings . catch warnings ( ) : warnings . filterwarnings ( \"ignore\" , category = Pytest Warning ) if pytest . main ( test cmd ) : raise Runtime Error ( \"Tracing did not complete successfully\" ) stop time = datetime . datetime . now ( ) if not no print : print ( \"Auto-generation of exceptions documentation time: {0}\" . format ( pmisc . elapsed time string ( start time , stop time ) ) ) for callable name in callable names : callable name = module prefix + callable name print ( \"\\n Callable: {0}\" . format ( callable name ) ) print ( exdoc obj . get sphinx doc ( callable name , exclude = callable exclude ) ) print ( \"\\n\" ) return copy . copy ( exdoc obj )", "predictions": ["run a trace script"], "references": ["run module tracing ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 3525, "code": "def flatten ( iterable , map2iter = None ) : if map2iter and isinstance ( iterable ) : iterable = map2iter ( iterable ) for item in iterable : if isinstance ( item , str ) or not isinstance ( item , abc . Iterable ) : yield item else : yield from flatten ( item , map2iter )", "predictions": ["flatten an iterable into a list of strings ."], "references": ["recursively flatten nested objects"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 3526, "code": "def printtsv ( table , sep = \"\\t\" , file = sys . stdout ) : for record in table : print ( * record , sep = sep , file = file )", "predictions": ["print all the rows in a table ."], "references": ["stupidly print an iterable of iterables in tsv format"], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 3527, "code": "def mkdummy ( name , * * attrs ) : return type ( name , ( ) , dict ( repr = ( lambda self : \"<%s>\" % name ) , * * attrs ) ) ( )", "predictions": ["return a string representing the given name ."], "references": ["make a placeholder object that uses its own name for its repr"], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 3528, "code": "def from str ( cls , human readable str , decimal = False , bits = False ) : divisor = 1000 if decimal else 1024 num = [ ] c = \"\" for c in human readable str : if c not in cls . digits : break num . append ( c ) num = \"\" . join ( num ) try : num = int ( num ) except Value Error : num = float ( num ) if bits : num /= 8 return cls ( round ( num * divisor ** cls . key [ c . lower ( ) ] ) )", "predictions": ["convert a string to a human - readable string ."], "references": ["attempt to parse a size in bytes from a human - readable string ."], "bleu": 0.4073398045860038, "rouge_l": 0.5663129973474801}
{"id": 3529, "code": "def trace module ( no print = True ) : mname = \"wave core\" fname = \"peng\" module prefix = \"peng.{0}.Waveform.\" . format ( mname ) callable names = ( \" init \" , ) return docs . support . trace support . run trace ( mname , fname , module prefix , callable names , no print )", "predictions": ["trace trace for the trace ."], "references": ["trace eng wave module exceptions ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 3530, "code": "def def links ( mobj ) : fdict = json load ( os . path . join ( \"data\" , \"requirements.json\" ) ) sdeps = sorted ( fdict . keys ( ) ) olines = [ ] for item in sdeps : olines . append ( \"..  {name}: {url}\\n\" . format ( name = fdict [ item ] [ \"name\" ] , url = fdict [ item ] [ \"url\" ] ) ) ret = [ ] for line in olines : wobj = textwrap . wrap ( line , width = LINE WIDTH , subsequent indent = \"   \" ) ret . append ( \"\\n\" . join ( [ item for item in wobj ] ) ) mobj . out ( \"\\n\" . join ( ret ) )", "predictions": ["print links from json file"], "references": ["define sphinx requirements links ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 3531, "code": "def make common entry ( plist , pyver , suffix , req ver ) : prefix = \"Python {pyver}.x{suffix}\" . format ( pyver = pyver , suffix = suffix ) plist . append ( \"{prefix}{ver}\" . format ( prefix = prefix , ver = ops to words ( req ver ) ) )", "predictions": ["make a common entry for the given plist ."], "references": ["generate python interpreter version entries for 2 . x or 3 . x series ."], "bleu": 0.08019421212222273, "rouge_l": 0.15947712418300655}
{"id": 3532, "code": "def make multi entry ( plist , pkg pyvers , ver dict ) : for pyver in pkg pyvers : pver = pyver [ 2 ] + \".\" + pyver [ 3 : ] plist . append ( \"Python {0}: {1}\" . format ( pver , ops to words ( ver dict [ pyver ] ) ) )", "predictions": ["make words in multi - multi - entry dictionary"], "references": ["generate python interpreter version entries ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3533, "code": "def ops to words ( item ) : unsupp ops = [ \"~=\" , \"===\" ] supp ops = [ \">=\" , \">\" , \"==\" , \"<=\" , \"<\" , \"!=\" ] tokens = sorted ( item . split ( \",\" ) , reverse = True ) actual tokens = [ ] for req in tokens : for op in unsupp ops : if req . startswith ( op ) : raise Runtime Error ( \"Unsupported version specification: {0}\" . format ( op ) ) for op in supp ops : if req . startswith ( op ) : actual tokens . append ( op ) break else : raise Runtime Error ( \"Illegal comparison operator: {0}\" . format ( op ) ) if len ( list ( set ( actual tokens ) ) ) != len ( actual tokens ) : raise Runtime Error ( \"Multiple comparison operators of the same type\" ) if \"!=\" in actual tokens : return ( \" and \" . join ( [ op to words ( token ) for token in tokens [ : - 1 ] ] ) + \" \" + op to words ( tokens [ - 1 ] ) ) return \" and \" . join ( [ op to words ( token ) for token in tokens ] )", "predictions": ["convert ops to words ."], "references": ["translate requirement specification to words ."], "bleu": 0.41602390756021224, "rouge_l": 0.5366568914956013}
{"id": 3534, "code": "def chunk noise ( noise ) : data = zip ( noise [ \"freq\" ] , noise [ \"nf\" ] , np . abs ( noise [ \"rc\" ] ) , np . angle ( noise [ \"rc\" ] ) , noise [ \"res\" ] , ) for freq , nf , rcmag , rcangle , res in data : yield freq , nf , rcmag , rcangle , res", "predictions": ["chunk noise noise noise"], "references": ["chunk input noise data into valid touchstone file rows ."], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 3535, "code": "def chunk pars ( freq vector , data matrix , pformat ) : pformat = pformat . upper ( ) length = 4 for freq , data in zip ( freq vector , data matrix ) : data = data . flatten ( ) for index in range ( 0 , data . size , length ) : fpoint = [ freq ] if not index else [ None ] cdata = data [ index : index + length ] if pformat == \"MA\" : vector1 = np . abs ( cdata ) vector2 = np . rad2deg ( np . angle ( cdata ) ) elif pformat == \"RI\" : vector1 = np . real ( cdata ) vector2 = np . imag ( cdata ) else : vector1 = 20.0 * np . log10 ( np . abs ( cdata ) ) vector2 = np . rad2deg ( np . angle ( cdata ) ) sep data = np . array ( [ ] ) for item1 , item2 in zip ( vector1 , vector2 ) : sep data = np . concatenate ( ( sep data , np . array ( [ item1 , item2 ] ) ) ) ret = np . concatenate ( ( np . array ( fpoint ) , sep data ) ) yield ret", "predictions": ["generate the chunk of the data pars for the given vector ."], "references": ["chunk input data into valid touchstone file rows ."], "bleu": 0.1235622127262679, "rouge_l": 0.2932692307692307}
{"id": 3536, "code": "def bound waveform ( wave , indep min , indep max ) : indep min , indep max = validate min max ( wave , indep min , indep max ) indep vector = copy . copy ( wave . indep vector ) if ( isinstance ( indep min , float ) or isinstance ( indep max , float ) ) and indep vector . dtype . name . startswith ( \"int\" ) : indep vector = indep vector . astype ( float ) min pos = np . searchsorted ( indep vector , indep min ) if not np . isclose ( indep min , indep vector [ min pos ] , FP RTOL , FP ATOL ) : indep vector = np . insert ( indep vector , min pos , indep min ) max pos = np . searchsorted ( indep vector , indep max ) if not np . isclose ( indep max , indep vector [ max pos ] , FP RTOL , FP ATOL ) : indep vector = np . insert ( indep vector , max pos , indep max ) dep vector = interp dep vector ( wave , indep vector ) wave . indep vector = indep vector [ min pos : max pos + 1 ] wave . dep vector = dep vector [ min pos : max pos + 1 ]", "predictions": ["r r subclasses may override this method ."], "references": ["add independent variable vector bounds if they are not in vector ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 3537, "code": "def build units ( indep units , dep units , op ) : if ( not dep units ) and ( not indep units ) : return \"\" if dep units and ( not indep units ) : return dep units if ( not dep units ) and indep units : return ( remove extra delims ( \"1{0}({1})\" . format ( op , indep units ) ) if op == \"/\" else remove extra delims ( \"({0})\" . format ( indep units ) ) ) return remove extra delims ( \"({0}){1}({2})\" . format ( dep units , op , indep units ) )", "predictions": ["build and remove units from units ."], "references": ["build unit math operations ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 3538, "code": "def operation ( wave , desc , units , fpointer ) : ret = copy . copy ( wave ) ret . dep units = units ret . dep name = \"{0}({1})\" . format ( desc , ret . dep name ) ret . dep vector = fpointer ( ret . dep vector ) return ret", "predictions": ["return a copy of the given wave operation ."], "references": ["perform generic operation on a waveform object ."], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 3539, "code": "def running area ( indep vector , dep vector ) : rect height = np . minimum ( dep vector [ : - 1 ] , dep vector [ 1 : ] ) rect base = np . diff ( indep vector ) rect area = np . multiply ( rect height , rect base ) triang height = np . abs ( np . diff ( dep vector ) ) triang area = 0.5 * np . multiply ( triang height , rect base ) return np . cumsum ( np . concatenate ( ( np . array ( [ 0.0 ] ) , triang area + rect area ) ) )", "predictions": ["return the running area of the vector ."], "references": ["calculate running area under curve ."], "bleu": 0.22679164443904004, "rouge_l": 0.43990384615384615}
{"id": 3540, "code": "def validate min max ( wave , indep min , indep max ) : imin , imax = False , False if indep min is None : indep min = wave . indep vector [ 0 ] imin = True if indep max is None : indep max = wave . indep vector [ - 1 ] imax = True if imin and imax : return indep min , indep max exminmax = pexdoc . exh . addex ( Runtime Error , \"Incongruent `indep min` and `indep max` arguments\" ) exmin = pexdoc . exh . addai ( \"indep min\" ) exmax = pexdoc . exh . addai ( \"indep max\" ) exminmax ( bool ( indep min >= indep max ) ) exmin ( bool ( ( indep min < wave . indep vector [ 0 ] ) and ( not np . isclose ( indep min , wave . indep vector [ 0 ] , FP RTOL , FP ATOL ) ) ) ) exmax ( bool ( ( indep max > wave . indep vector [ - 1 ] ) and ( not np . isclose ( indep max , wave . indep vector [ - 1 ] , FP RTOL , FP ATOL ) ) ) ) return indep min , indep max", "predictions": ["validate that the max max and vector is within the vector vector ."], "references": ["validate min and max bounds are within waveform s independent variable vector ."], "bleu": 0.1553712569276035, "rouge_l": 0.38461538461538464}
{"id": 3541, "code": "def get short desc ( long desc ) : found = False olines = [ ] for line in [ item . rstrip ( ) for item in long desc . split ( \"\\n\" ) ] : if found and ( ( ( not line ) and ( not olines ) ) or ( line and olines ) ) : olines . append ( line ) elif found and olines and ( not line ) : return ( \" \" . join ( olines ) . split ( \".\" ) [ 0 ] ) . strip ( ) found = line == \".. [[[end]]]\" if not found else found return \"\"", "predictions": ["returns the short description of the long"], "references": ["get first sentence of first paragraph of long description ."], "bleu": 0.14390022429682173, "rouge_l": 0.22803738317757008}
{"id": 3542, "code": "def render ( self , context ) : module path = self . path . resolve ( context ) if not settings . SYSTEMJS ENABLED : if settings . SYSTEMJS DEFAULT JS EXTENSIONS : name , ext = posixpath . splitext ( module path ) if not ext : module path = '{}.js' . format ( module path ) if settings . SYSTEMJS SERVER URL : tpl = \"\"\"<script src=\"{url}{app}\" type=\"text/javascript\"></script>\"\"\" else : tpl = \"\"\"<script type=\"text/javascript\">System.import('{app}');</script>\"\"\" return tpl . format ( app = module path , url = settings . SYSTEMJS SERVER URL ) rel path = System . get bundle path ( module path ) url = staticfiles storage . url ( rel path ) tag attrs = { 'type' : 'text/javascript' } for key , value in self . tag attrs . items ( ) : if not isinstance ( value , bool ) : value = value . resolve ( context ) tag attrs [ key ] = value return \"\"\"<script{attrs} src=\"{url}\"></script>\"\"\" . format ( url = url , attrs = flatatt ( tag attrs ) )", "predictions": ["render the html representation of the element ."], "references": ["build the filepath by appending the extension ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 3543, "code": "def build expr ( tokens , higher oplevel = - 1 , ldelim = \"(\" , rdelim = \")\" ) : if isinstance ( tokens , str ) : return tokens if len ( tokens ) == 2 : return \"\" . join ( tokens ) oplevel = get op level ( tokens [ 1 ] ) stoken = \"\" for num , item in enumerate ( tokens ) : if num % 2 == 0 : stoken += build expr ( item , oplevel , ldelim = ldelim , rdelim = rdelim ) else : stoken += item if ( oplevel < higher oplevel ) or ( ( oplevel == higher oplevel ) and ( oplevel in OP PREC PAR ) ) : stoken = ldelim + stoken + rdelim return stoken", "predictions": ["build string from higher ."], "references": ["build mathematical expression from hierarchical list ."], "bleu": 0.21763141204756337, "rouge_l": 0.48541114058355433}
{"id": 3544, "code": "def next rdelim ( items , pos ) : for num , item in enumerate ( items ) : if item > pos : break else : raise Runtime Error ( \"Mismatched delimiters\" ) del items [ num ] return item", "predictions": ["returns the next item in the list ."], "references": ["return position of next matching closing delimiter ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 3545, "code": "def get functions ( expr , ldelim = \"(\" , rdelim = \")\" ) : tpars = pair delims ( expr , ldelim = ldelim , rdelim = rdelim ) alphas = \"abcdefghijklmnopqrstuvwxyz\" \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" fchars = \"abcdefghijklmnopqrstuvwxyz\" \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \"0123456789\" \" \" tfuncs = [ ] for lnum , rnum in tpars : if lnum and expr [ lnum - 1 ] in fchars : for cnum , char in enumerate ( reversed ( expr [ : lnum ] ) ) : if char not in fchars : break else : cnum = lnum tfuncs . append ( { \"fname\" : expr [ lnum - cnum : lnum ] , \"expr\" : expr [ lnum + 1 : rnum ] , \"start\" : lnum - cnum , \"stop\" : rnum , } ) if expr [ lnum - cnum ] not in alphas : raise Runtime Error ( \"Function name `{0}` is not valid\" . format ( expr [ lnum - cnum : lnum ] ) ) return tfuncs", "predictions": ["get the list of functions from the expression ."], "references": ["parse function calls ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 3546, "code": "def parse expr ( text , ldelim = \"(\" , rdelim = \")\" ) : var = pyparsing . Word ( pyparsing . alphas + \" \" , pyparsing . alphanums + \" \" ) point = pyparsing . Literal ( \".\" ) exp = pyparsing . Caseless Literal ( \"E\" ) number = pyparsing . Combine ( pyparsing . Word ( \"+-\" + pyparsing . nums , pyparsing . nums ) + pyparsing . Optional ( point + pyparsing . Optional ( pyparsing . Word ( pyparsing . nums ) ) ) + pyparsing . Optional ( exp + pyparsing . Word ( \"+-\" + pyparsing . nums , pyparsing . nums ) ) ) atom = var | number oplist = [ ( pyparsing . Literal ( \"**\" ) , 2 , pyparsing . op Assoc . RIGHT ) , ( pyparsing . one Of ( \"+ - ~\" ) , 1 , pyparsing . op Assoc . RIGHT ) , ( pyparsing . one Of ( \"* / // %\" ) , 2 , pyparsing . op Assoc . LEFT ) , ( pyparsing . one Of ( \"+ -\" ) , 2 , pyparsing . op Assoc . LEFT ) , ( pyparsing . one Of ( \"<< >>\" ) , 2 , pyparsing . op Assoc . LEFT ) , ( pyparsing . Literal ( \"&\" ) , 2 , pyparsing . op Assoc . LEFT ) , ( pyparsing . Literal ( \"^\" ) , 2 , pyparsing . op Assoc . LEFT ) , ( pyparsing . Literal ( \"|\" ) , 2 , pyparsing . op Assoc . LEFT ) , ] expr = pyparsing . infix Notation ( atom , oplist , lpar = pyparsing . Suppress ( ldelim ) , rpar = pyparsing . Suppress ( rdelim ) ) return expr . parse String ( text ) [ 0 ]", "predictions": ["parse an \"e\" text"], "references": ["parse mathematical expression using pyparsing ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 3547, "code": "def remove consecutive delims ( expr , ldelim = \"(\" , rdelim = \")\" ) : tpars = pair delims ( expr , ldelim = ldelim , rdelim = rdelim ) ddelim = [ ] for ctuple , ntuple in zip ( tpars , tpars [ 1 : ] ) : if ctuple == ( ntuple [ 0 ] - 1 , ntuple [ 1 ] + 1 ) : ddelim . extend ( ntuple ) ddelim . sort ( ) for num , item in enumerate ( ddelim ) : expr = expr [ : item - num ] + expr [ item - num + 1 : ] return expr", "predictions": ["remove consecutive delims delims ."], "references": ["remove consecutive delimiters ."], "bleu": 0.3860973950960897, "rouge_l": 0.6802973977695167}
{"id": 3548, "code": "def needs ext ( self ) : if settings . SYSTEMJS DEFAULT JS EXTENSIONS : name , ext = posixpath . splitext ( self . app ) if not ext : return True return False", "predictions": ["return true if the extension is needs to be executed ."], "references": ["check whether self . app is missing the . js extension and if it needs it ."], "bleu": 0.09029890847277784, "rouge_l": 0.2750845546786922}
{"id": 3549, "code": "def bundle ( self ) : outfile , rel path = self . get paths ( ) options = self . opts if self . system . has jspm log ( ) : self . command += ' --log {log}' options . setdefault ( 'log' , 'err' ) if options . get ( 'minify' ) : self . command += ' --minify' if options . get ( 'skip source maps' ) : self . command += ' --skip-source-maps' try : cmd = self . command . format ( app = self . app , outfile = outfile , * * options ) proc = subprocess . Popen ( cmd , shell = True , cwd = self . system . cwd , stdout = self . stdout , stdin = self . stdin , stderr = self . stderr ) result , err = proc . communicate ( ) if err and self . system . has jspm log ( ) : fmt = 'Could not bundle \\'%s\\': \\n%s' logger . warn ( fmt , self . app , err ) raise Bundle Error ( fmt % ( self . app , err ) ) if result . strip ( ) : logger . info ( result ) except ( IO Error , OS Error ) as e : if isinstance ( e , Bundle Error ) : raise raise Bundle Error ( 'Unable to apply %s (%r): %s' % ( self . class . name , cmd , e ) ) else : if not options . get ( 'sfx' ) : sourcemap = find sourcemap comment ( outfile ) with open ( outfile , 'a' ) as of : of . write ( \"\\n System.import('{app}{ext}');\\n{sourcemap}\" . format ( app = self . app , ext = '.js' if self . needs ext ( ) else '' , sourcemap = sourcemap if sourcemap else '' , ) ) return rel path", "predictions": ["return the bundle path to the system"], "references": ["bundle the app and return the static url to the bundle ."], "bleu": 0.1692447266569478, "rouge_l": 0.40197693574958815}
{"id": 3550, "code": "def parse docstring ( doc ) : doc = inspect . cleandoc ( doc ) lines = doc . split ( '\\n' ) section = None section indent = None params = { } returns = None for line in lines : line = line . rstrip ( ) if len ( line ) == 0 : continue elif str ( line ) == 'Args:' : section = 'args' section indent = None continue elif str ( line ) == 'Returns:' : section = 'return' section indent = None continue if section is not None : stripped = line . lstrip ( ) margin = len ( line ) - len ( stripped ) if section indent is None : section indent = margin if margin != section indent : continue if section == 'args' : param name , type info = parse param ( stripped ) params [ param name ] = type info elif section == 'return' : returns = parse return ( stripped ) return params , returns", "predictions": ["parse the docstring into a dict"], "references": ["parse a docstring into parameterinfo and returninfo objects ."], "bleu": 0.20034704329441452, "rouge_l": 0.3860759493670886}
{"id": 3551, "code": "def split line ( self , line ) : parts = shlex . split ( line , posix = self . posix lex ) if not self . posix lex : parts = [ self . remove quotes ( x ) for x in parts ] return parts", "predictions": ["split a line into a line ."], "references": ["split a line into arguments using shlex and a dequoting routine ."], "bleu": 0.27528950557113974, "rouge_l": 0.6029654036243821}
{"id": 3552, "code": "def builtin help ( self , args ) : if len ( args ) == 0 : return self . list dir ( self . contexts [ - 1 ] ) if len ( args ) == 1 : func = self . find function ( self . contexts [ - 1 ] , args [ 0 ] ) return annotate . get help ( func ) help text = \"Too many arguments: \" + str ( args ) + \"\\n\" help text += \"Usage: help [function]\" return help text", "predictions": [". dep dep for the interp"], "references": ["return help information for a context or function ."], "bleu": 0.14827340167306757, "rouge_l": 0.12869198312236285}
{"id": 3553, "code": "def extract arg value ( cls , arg name , arg type , remaining ) : next arg = None should consume = False if len ( remaining ) > 0 : next arg = remaining [ 0 ] should consume = True if next arg == '--' : next arg = None if arg type == \"bool\" : if next arg is None or next arg . startswith ( '-' ) : next arg = True should consume = False else : if next arg is None : raise Argument Error ( \"Could not find value for keyword argument\" , argument = arg name ) if should consume : remaining . pop ( 0 ) return next arg", "predictions": ["get the argument vector from an argument ."], "references": ["try to find the value for a keyword argument ."], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 3554, "code": "def parse param ( param , include desc = False ) : param def , colon , desc = param . partition ( ':' ) if not include desc : desc = None else : desc = desc . lstrip ( ) if colon == \"\" : raise Validation Error ( \"Invalid parameter declaration in docstring, missing colon\" , declaration = param ) param name , space , param type = param def . partition ( ' ' ) if len ( param type ) < 2 or param type [ 0 ] != '(' or param type [ - 1 ] != ')' : raise Validation Error ( \"Invalid parameter type string not enclosed in ( ) characters\" , param string = param def , type string = param type ) param type = param type [ 1 : - 1 ] return param name , Parameter Info ( param type , [ ] , desc )", "predictions": ["verify that the parameter parameter is a parameter parameter ."], "references": ["parse a single typed parameter statement ."], "bleu": 0.14991106946711685, "rouge_l": 0.36454183266932266}
{"id": 3555, "code": "def classify section ( cls , section ) : name = section . lower ( ) if name in frozenset ( [ 'args' , 'arguments' , \"params\" , \"parameters\" ] ) : return cls . ARGS SECTION if name in frozenset ( [ 'returns' , 'return' ] ) : return cls . RETURN SECTION if name in frozenset ( [ 'main' ] ) : return cls . MAIN SECTION return None", "predictions": ["trace trace for the 'arguments' os os os os os os os os os os os os os os os os os os os os os os os os os os"], "references": ["attempt to find the canonical name of this section ."], "bleu": 0.03901663112717908, "rouge_l": 0.05374449339207048}
{"id": 3556, "code": "def classify line ( cls , line ) : line = line . rstrip ( ) if len ( line ) == 0 : return Blank Line ( '' ) if ' ' not in line and line . endswith ( ':' ) : name = line [ : - 1 ] return Section Header ( name ) if line . startswith ( '  ' ) : return Continuation Line ( line . lstrip ( ) ) if line . startswith ( ' - ' ) : return List Item ( '-' , line [ 3 : ] . lstrip ( ) ) if line . startswith ( '- ' ) : return List Item ( '-' , line [ 2 : ] . lstrip ( ) ) return Line ( line )", "predictions": ["run a trace trace"], "references": ["classify a line into a type of object ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 3557, "code": "def join paragraphs ( cls , lines , use indent = False , leading blanks = False , trailing blanks = False ) : curr para = [ ] paragraphs = [ ] for line in lines : if use indent : if line . startswith ( ' ' ) : curr para . append ( line . lstrip ( ) ) continue elif line == '' : continue else : if len ( curr para ) > 0 : paragraphs . append ( cls . join paragraph ( curr para , leading blanks , trailing blanks ) ) curr para = [ line . lstrip ( ) ] else : if len ( line ) != 0 : curr para . append ( line ) else : paragraphs . append ( cls . join paragraph ( curr para , leading blanks , trailing blanks ) ) curr para = [ ] if len ( curr para ) > 0 : paragraphs . append ( cls . join paragraph ( curr para , leading blanks , trailing blanks ) ) return paragraphs", "predictions": ["flatten paragraphs = false"], "references": ["join adjacent lines together into paragraphs using either a blank line or indent as separator ."], "bleu": 0.017888698387160718, "rouge_l": 0.09023668639053255}
{"id": 3558, "code": "def split type ( self , typename ) : name = self . canonicalize type ( typename ) if '(' not in name : return name , False , [ ] base , sub = name . split ( '(' ) if len ( sub ) == 0 or sub [ - 1 ] != ')' : raise Argument Error ( \"syntax error in complex type, no matching ) found\" , passed type = typename , basetype = base , subtype string = sub ) sub = sub [ : - 1 ] subs = sub . split ( ',' ) return base , True , subs", "predictions": ["split the type of the complex"], "references": ["given a potentially complex type split it into its base type and specializers"], "bleu": 0.08180282100568384, "rouge_l": 0.19741100323624597}
{"id": 3559, "code": "def instantiate type ( self , typename , base , subtypes ) : if base not in self . type factories : raise Argument Error ( \"unknown complex base type specified\" , passed type = typename , base type = base ) base type = self . type factories [ base ] #Make sure all of the subtypes are valid for sub type in subtypes : try : self . get type ( sub type ) except Key Value Exception as exc : raise Argument Error ( \"could not instantiate subtype for complex type\" , passed type = typename , sub type = sub type , error = exc ) typeobj = base type . Build ( * subtypes , type system = self ) self . inject type ( typename , typeobj )", "predictions": ["instantiate the type type"], "references": ["instantiate a complex type ."], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 3560, "code": "def short description ( func ) : doc = inspect . getdoc ( func ) if doc is not None : doc = inspect . cleandoc ( doc ) lines = doc . splitlines ( ) return lines [ 0 ] return \"\"", "predictions": ["decorator to num from a function ."], "references": ["given an object with a docstring return the first line of the docstring"], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 3561, "code": "def load ( ) : autodiscover modules ( 'cron' ) if PROJECT MODULE : if '.' in PROJECT MODULE . name : try : import module ( '%s.cron' % '.' . join ( PROJECT MODULE . name . split ( '.' ) [ 0 : - 1 ] ) ) except Import Error as e : if 'No module named' not in str ( e ) : print ( e ) for cmd , app in get commands ( ) . items ( ) : try : load command class ( app , cmd ) except django . core . exceptions . Improperly Configured : pass", "predictions": ["trace trace for all = = 0"], "references": ["load cron modules for applications listed in installed_apps ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 3562, "code": "def install ( ) : load ( ) tab = crontab . Cron Tab ( user = True ) for task in registry : tab . new ( task . command , KRONOS BREADCRUMB ) . setall ( task . schedule ) tab . write ( ) return len ( registry )", "predictions": ["links all the json files to the json server load the json registry load"], "references": ["register tasks with cron ."], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 3563, "code": "def uninstall ( ) : tab = crontab . Cron Tab ( user = True ) count = len ( list ( tab . find comment ( KRONOS BREADCRUMB ) ) ) tab . remove all ( comment = KRONOS BREADCRUMB ) tab . write ( ) return count", "predictions": ["make a crontab from the crontab"], "references": ["uninstall tasks from cron ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 3564, "code": "def kind ( self ) : optics = [ Equality , Isomorphism , Prism , Review , Lens , Traversal , Getter , Setter , Fold , ] for optic in optics : if self . is kind ( optic ) : return optic", "predictions": ["return list of make make sure this widget is a string"], "references": ["returns a class representing the kind of optic ."], "bleu": 0.12605968092174913, "rouge_l": 0.1018363939899833}
{"id": 3565, "code": "def play ( ) : ai = { 'X' : player move , 'O' : random move } board = Board ( ) while not board . winner : x , y = ai [ board . player ] ( board ) board = board . make move ( x , y ) print ( board , end = '\\n\\n' ) print ( board . winner )", "predictions": ["ops to the previous = 0 ."], "references": ["play a game of naughts and crosses against the computer ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 3566, "code": "def winner ( self ) : for potential win in self . potential wins ( ) : if potential win == tuple ( 'XXX' ) : return Outcome . win for crosses elif potential win == tuple ( 'OOO' ) : return Outcome . win for naughts if self . count ( ' ' ) == 0 : return Outcome . draw return Outcome . ongoing", "predictions": ["abs chunk of the = chunk"], "references": ["the winner of this board if one exists ."], "bleu": 0.14827340167306757, "rouge_l": 0.12869198312236285}
{"id": 3567, "code": "def open spider ( self , spider ) : self . ts = datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) . replace ( ':' , '-' )", "predictions": ["chunk the underlying pars pars pars matrix"], "references": ["callback function when spider is open ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3568, "code": "def upload chunk ( self , spider ) : if not self . items : return f = self . make fileobj ( ) object key = self . object key template . format ( * * self . get uri params ( spider ) ) try : self . s3 . upload fileobj ( f , self . bucket name , object key ) except Client Error : self . stats . inc value ( 'pipeline/s3/fail' ) raise else : self . stats . inc value ( 'pipeline/s3/success' ) finally : self . chunk number += len ( self . items ) self . items = [ ]", "predictions": ["bound a waveform waveform to the np float = 0 = 0 = 1 = 0"], "references": ["do upload items to s3 ."], "bleu": 0.07692375026049747, "rouge_l": 0.09902597402597402}
{"id": 3569, "code": "def make fileobj ( self ) : bio = Bytes IO ( ) f = gzip . Gzip File ( mode = 'wb' , fileobj = bio ) if self . use gzip else bio exporter = Json Lines Item Exporter ( f ) exporter . start exporting ( ) for item in self . items : exporter . export item ( item ) exporter . finish exporting ( ) if f is not bio : f . close ( ) bio . seek ( 0 ) return bio", "predictions": ["build the op from the exporter return a list of op return a list of op ."], "references": ["build file object from items ."], "bleu": 0.0859076483566362, "rouge_l": 0.28549141965678626}
{"id": 3570, "code": "def call ( self , method , params = None , request id = None ) : params = params or [ ] rid = request id or self . id counter if request id is None : self . id counter += 1 payload = { 'jsonrpc' : '2.0' , 'method' : method , 'params' : params , 'id' : rid } headers = { 'Content-Type' : 'application/json' } scheme = 'https' if self . tls else 'http' url = '{}://{}:{}' . format ( scheme , self . host , self . port ) try : response = self . session . post ( url , headers = headers , data = json . dumps ( payload ) ) response . raise for status ( ) except HTTP Error : raise Transport Error ( 'Got unsuccessful response from server (status code: {})' . format ( response . status code ) , response = response ) try : response data = response . json ( ) except Value Error as e : raise Protocol Error ( 'Unable to deserialize response body: {}' . format ( e ) , response = response ) if response data . get ( 'error' ) : code = response data [ 'error' ] . get ( 'code' , '' ) message = response data [ 'error' ] . get ( 'message' , '' ) raise Protocol Error ( 'Error[{}] {}' . format ( code , message ) , response = response , data = response data ) elif 'result' not in response data : raise Protocol Error ( 'Response is empty (result field is missing)' , response = response , data = response data ) return response data [ 'result' ]", "predictions": ["do a http request to the api"], "references": ["calls the json - rpc endpoint ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 3571, "code": "def is hash256 ( s ) : if not s or not isinstance ( s , str ) : return False return re . match ( '^[0-9A-F]{64}$' , s . strip ( ) , re . IGNORECASE )", "predictions": ["np . for area ."], "references": ["returns true if the considered string is a valid sha256 hash ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 3572, "code": "def is hash160 ( s ) : if not s or not isinstance ( s , str ) : return False if not len ( s ) == 40 : return False for c in s : if ( c < '0' or c > '9' ) and ( c < 'A' or c > 'F' ) and ( c < 'a' or c > 'f' ) : return False return True", "predictions": ["return must be a string or a hex string"], "references": ["returns true if the considered string is a valid ripemd160 hash ."], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 3573, "code": "def encode invocation params ( params ) : final params = [ ] for p in params : if isinstance ( p , bool ) : final params . append ( { 'type' : Contract Parameter Types . BOOLEAN . value , 'value' : p } ) elif isinstance ( p , int ) : final params . append ( { 'type' : Contract Parameter Types . INTEGER . value , 'value' : p } ) elif is hash256 ( p ) : final params . append ( { 'type' : Contract Parameter Types . HASH256 . value , 'value' : p } ) elif is hash160 ( p ) : final params . append ( { 'type' : Contract Parameter Types . HASH160 . value , 'value' : p } ) elif isinstance ( p , bytearray ) : final params . append ( { 'type' : Contract Parameter Types . BYTE ARRAY . value , 'value' : p } ) elif isinstance ( p , str ) : final params . append ( { 'type' : Contract Parameter Types . STRING . value , 'value' : p } ) elif isinstance ( p , list ) : innerp = encode invocation params ( p ) final params . append ( { 'type' : Contract Parameter Types . ARRAY . value , 'value' : innerp } ) return final params", "predictions": ["get all parameters from the desc parameters ."], "references": ["returns a list of paramaters meant to be passed to json - rpc endpoints ."], "bleu": 0.06685045700482882, "rouge_l": 0.08243243243243244}
{"id": 3574, "code": "def decode invocation result ( result ) : if 'stack' not in result : return result result = copy . deepcopy ( result ) result [ 'stack' ] = decode invocation result stack ( result [ 'stack' ] ) return result", "predictions": ["render the self not encoded as a if it is not none not ."], "references": ["tries to decode the values embedded in an invocation result dictionary ."], "bleu": 0.09782375748961449, "rouge_l": 0.15601023017902813}
{"id": 3575, "code": "def connect ( cls , settings ) : server = serializer ( 'json' ) . loads ( settings [ 'kvs.perlsess' ] ) server . setdefault ( 'key prefix' , 'perlsess::' ) server . setdefault ( 'codec' , 'storable' ) cls . cookie name = server . pop ( 'cookie name' , 'session id' ) cls . client = KVS ( * * server )", "predictions": ["build connection to wherever . . ."], "references": ["call that method in the pyramid configuration phase ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 3576, "code": "def basic ( username , password ) : none ( ) config . username = username config . password = password", "predictions": ["generate a next next next user ."], "references": ["add basic authentication to the requests of the clients ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 3577, "code": "def api key ( api key ) : none ( ) config . api key prefix [ \"Authorization\" ] = \"api-key\" config . api key [ \"Authorization\" ] = \"key=\" + b64encode ( api key . encode ( ) ) . decode ( )", "predictions": ["encode get the get functions . for the get functions ."], "references": ["authenticate via an api key ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 3578, "code": "def get json content from folder ( folder ) : for dirpath , dirnames , filenames in os . walk ( folder ) : for filename in filenames : if filename . lower ( ) . endswith ( \".json\" ) : filepath = os . path . join ( dirpath , filename ) with open ( filepath , \"rb\" ) as file : yield json . loads ( file . read ( ) . decode ( \"UTF-8\" ) )", "predictions": ["return expr from folder . ."], "references": ["yield objects from json files in the folder and subfolders ."], "bleu": 0.1141650334026257, "rouge_l": 0.33516483516483514}
{"id": 3579, "code": "def get schema ( self ) : path = os . path . join ( self . get schema folder ( ) , self . name + \".json\" ) with open ( path , \"rb\" ) as file : schema = json . loads ( file . read ( ) . decode ( \"UTF-8\" ) ) return schema", "predictions": ["0 - schema consecutive to the consecutive consecutive consecutive consecutive consecutive"], "references": ["return the schema ."], "bleu": 0.12605968092174913, "rouge_l": 0.14558472553699284}
{"id": 3580, "code": "def get valid examples ( self ) : path = os . path . join ( self . get schema folder ( ) , \"examples\" , \"valid\" ) return list ( get json content from folder ( path ) )", "predictions": ["returns a list of all examples in the name of the name of the name of the name"], "references": ["return a list of valid examples for the given schema ."], "bleu": 0.14025775160081475, "rouge_l": 0.3605200945626478}
{"id": 3581, "code": "def get invalid examples ( self ) : path = os . path . join ( self . get schema folder ( ) , \"examples\" , \"invalid\" ) return list ( get json content from folder ( path ) )", "predictions": ["returns a list of invalid self . \"examples\" ."], "references": ["return a list of examples which violate the schema ."], "bleu": 0.24855227187657006, "rouge_l": 0.41709401709401706}
{"id": 3582, "code": "def auth user get url ( self , scope = None ) : if not self . client id : raise Auth Missing Error ( 'No client id specified' ) return '{}?{}' . format ( self . auth url user , urllib . urlencode ( dict ( client id = self . client id , scope = ' ' . join ( scope or self . auth scope ) , response type = 'code' , redirect uri = self . auth redirect uri ) ) )", "predictions": ["gets the doc doc for the docstring . ."], "references": ["build authorization url for user agent ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 3583, "code": "def auth user process url ( self , url ) : url = urlparse . urlparse ( url ) url qs = dict ( it . chain . from iterable ( urlparse . parse qsl ( v ) for v in [ url . query , url . fragment ] ) ) if url qs . get ( 'error' ) : raise API Auth Error ( '{} :: {}' . format ( url qs [ 'error' ] , url qs . get ( 'error description' ) ) ) self . auth code = url qs [ 'code' ] return self . auth code", "predictions": ["authenticate against self posix posix posix posix posix posix posix posix posix posix posix posix posix posix posix posix posix posix posix posix posix posix posix posix posix posix posix posix"], "references": ["process tokens and errors from redirect_uri ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3584, "code": "def auth get token ( self , check scope = True ) : res = self . auth access data raw = self . auth token request ( ) return self . auth token process ( res , check scope = check scope )", "predictions": ["get oauth token from api ."], "references": ["refresh or acquire access_token ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 3585, "code": "def get user id ( self ) : if self . user id is None : self . user id = self . get user data ( ) [ 'id' ] return self . user id", "predictions": ["return the user id of the user ."], "references": ["returns id of a onedrive user ."], "bleu": 0.2653856085536222, "rouge_l": 0.5398230088495575}
{"id": 3586, "code": "def listdir ( self , folder id = 'me/skydrive' , limit = None , offset = None ) : return self ( self . api url join ( folder id , 'files' ) , dict ( limit = limit , offset = offset ) )", "predictions": ["lists the api url ."], "references": ["get onedrive object representing list of objects in a folder ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 3587, "code": "def comment add ( self , obj id , message ) : return self ( self . api url join ( obj id , 'comments' ) , method = 'post' , data = dict ( message = message ) , auth header = True )", "predictions": ["adds a comment to the current user ."], "references": ["add comment message to a specified object ."], "bleu": 0.20164945583740668, "rouge_l": 0.375}
{"id": 3588, "code": "def decode obj ( obj , force = False ) : if isinstance ( obj , unicode ) : return obj elif isinstance ( obj , bytes ) : if force encoding is not None : return obj . decode ( force encoding ) if chardet : enc guess = chardet . detect ( obj ) if enc guess [ 'confidence' ] > 0.7 : return obj . decode ( enc guess [ 'encoding' ] ) return obj . decode ( 'utf-8' ) else : return obj if not force else repr ( obj )", "predictions": ["decode obj into unicode strings ."], "references": ["convert or dump object to unicode ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 3589, "code": "def set drop target ( obj , root , designer , inspector ) : if obj . meta . container : dt = Tool Box Drop Target ( obj , root , designer = designer , inspector = inspector ) obj . drop target = dt for child in obj : set drop target ( child , root , designer , inspector )", "predictions": ["set the target target target target ."], "references": ["recursively create and set the drop target for obj and childs"], "bleu": 0.14834636222628117, "rouge_l": 0.32049036777583184}
{"id": 3590, "code": "def start drag opperation ( self , evt ) : ctrl = self . menu ctrl map [ evt . Get Tool Id ( ) ] ldata = wx . Custom Data Object ( \"gui\" ) ldata . Set Data ( ctrl . meta . name ) bmp = ctrl . image . Get Bitmap ( ) bdata = wx . Bitmap Data Object ( bmp ) data = wx . Data Object Composite ( ) data . Add ( ldata ) data . Add ( bdata ) drop Source = wx . Drop Source ( self ) drop Source . Set Data ( data ) if DEBUG : print ( \"Begining Drag Drop\\n\" ) result = drop Source . Do Drag Drop ( wx . Drag Allow Move ) if DEBUG : print ( \"Drag Drop completed: %d\\n\" % result ) if result == wx . Drag Move : if DEBUG : print \"dragmove!\" self . Refresh ( )", "predictions": ["start drag for drag"], "references": ["event handler for drag&drop functionality"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 3591, "code": "def set default tlw ( self , tlw , designer , inspector ) : self . designer = designer self . inspector = inspector", "predictions": ["set the default tlw for the given tlw ."], "references": ["track default top level window for toolbox menu default action"], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 3592, "code": "def inspect ( obj ) : from gui . tools . inspector import Inspector Tool inspector = Inspector Tool ( ) inspector . show ( obj ) return inspector", "predictions": ["show contents of an object ."], "references": ["open the inspector windows for a given object"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 3593, "code": "def migrate window ( bg ) : ret = { } for k , v in bg . items ( ) : if k == 'type' : v = WIN MAP [ v ] . meta . name elif k == 'menubar' : menus = v [ 'menus' ] v = [ migrate control ( menu ) for menu in menus ] elif k == 'components' : v = [ migrate control ( comp ) for comp in v ] else : k = SPEC MAP [ 'Widget' ] . get ( k , k ) ret [ k ] = v return ret", "predictions": ["migrate all menus names to the window"], "references": ["take a pythoncard background resource and convert to a gui2py window"], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 3594, "code": "def migrate control ( comp ) : ret = { } for k , v in comp . items ( ) : if k == 'type' : v = CTRL MAP [ v ] . meta . name elif k == 'menubar' : pass elif k == 'components' : v = [ migrate control ( comp ) for comp in v ] else : k = SPEC MAP [ 'Widget' ] . get ( k , k ) if comp [ 'type' ] in SPEC MAP : k = SPEC MAP [ comp [ 'type' ] ] . get ( k , k ) if k == 'font' : v = migrate font ( v ) ret [ k ] = v return ret", "predictions": ["migrate all control variables to the font"], "references": ["take a pythoncard background resource and convert to a gui2py window"], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 3595, "code": "def migrate font ( font ) : if 'face Name' in font : font [ 'face' ] = font . pop ( 'face Name' ) if 'family' in font and font [ 'family' ] == 'sans Serif' : font [ 'family' ] = 'sans serif' return font", "predictions": ["migrate font font to font"], "references": ["convert pythoncard font description to gui2py style"], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 3596, "code": "def load page ( self , location ) : if not location : self . wx obj . Set Page ( \"\" ) else : self . wx obj . Load Page ( location )", "predictions": ["load the page from the wx file"], "references": ["loads html page from location and then displays it"], "bleu": 0.18370727471078332, "rouge_l": 0.24448897795591182}
{"id": 3597, "code": "def Get Param ( tag , param , default = SENTINEL ) : if tag . Has Param ( param ) : return tag . Get Param ( param ) else : if default == SENTINEL : raise Key Error else : return default", "predictions": ["returns an tag from a param tag ."], "references": ["convenience function for accessing tag parameters"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 3598, "code": "def send ( evt ) : msg = ctrl input . value gui . alert ( msg , \"Message\" ) log ( msg ) ctrl input . value = \"\" ctrl input . set focus ( )", "predictions": ["send focus to gui ."], "references": ["process an outgoing communication"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 3599, "code": "def wellcome tip ( wx obj ) : msg = ( \"Close the main window to exit & save.\\n\" \"Drag & Drop / Click the controls from the Tool Box to create new ones.\\n\" \"Left click on the created controls to select them.\\n\" \"Double click to edit the default property.\\n\" \"Right click to pop-up the context menu.\\n\" ) stt = STT . Super Tool Tip ( msg ) stt . Set Header ( \"Welcome to gui2py designer!\" ) stt . Set Draw Header Line ( True ) stt . Apply Style ( \"Office 2007 Blue\" ) stt . Set Drop Shadow ( True ) stt . Set Header Bitmap ( images . designer . Get Bitmap ( ) ) stt . Set End Delay ( 15000 ) tip = Custom Tool Tip Window ( wx obj , stt ) tip . Calculate Best Size ( ) tip . Calculate Best Position ( wx obj ) tip . Drop Shadow ( stt . Get Drop Shadow ( ) ) if stt . Get Use Fade ( ) : show = lambda : tip . Start Alpha ( True ) else : show = lambda : tip . Show ( ) wx . Call Later ( 1000 , show ) wx . Call Later ( 30000 , tip . Destroy )", "predictions": ["tip tip for a wx window"], "references": ["show a tip message"], "bleu": 0.24446151121745047, "rouge_l": 0.2074829931972789}
{"id": 3600, "code": "def mouse down ( self , evt ) : if DEBUG : print \"down!\" if ( not evt . Control Down ( ) and not evt . Shift Down ( ) ) or evt . Alt Down ( ) : for obj in self . selection : if obj . sel marker : obj . sel marker . show ( False ) obj . sel marker . destroy ( ) obj . sel marker = None self . selection = [ ] wx obj = evt . Get Event Object ( ) if wx obj . Parent is None or evt . Alt Down ( ) : if not evt . Alt Down ( ) : evt . Skip ( ) self . current = wx obj self . overlay = wx . Overlay ( ) self . pos = evt . Get Position ( ) self . parent . wx obj . Capture Mouse ( ) #if self.inspector and hasattr(wx obj, \"obj\"): #self.dclick = False else : obj = wx obj . obj self . overlay = None if DEBUG : print wx obj sx , sy = wx obj . Screen To Client ( wx obj . Get Position Tuple ( ) ) dx , dy = wx obj . Screen To Client ( wx . Get Mouse Position ( ) ) self . pos = wx obj . Screen To Client ( wx . Get Mouse Position ( ) ) self . start = ( sx - dx , sy - dy ) self . current = wx obj if DEBUG : print \"capture...\" if not isinstance ( wx obj , wx . Notebook ) : self . parent . wx obj . Capture Mouse ( ) self . select ( obj , keep selection = True )", "predictions": ["mouse down a new down"], "references": ["get the selected object and store start position"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 3601, "code": "def mouse move ( self , evt ) : if DEBUG : print \"move!\" if self . current and not self . overlay : wx obj = self . current sx , sy = self . start x , y = wx . Get Mouse Position ( ) x , y = ( x + sx , y + sy ) if evt . Shift Down ( ) : x = x / GRID SIZE [ 0 ] * GRID SIZE [ 0 ] y = y / GRID SIZE [ 1 ] * GRID SIZE [ 1 ] ox , oy = wx obj . obj . pos dx , dy = ( x - ox ) , ( y - oy ) for obj in self . selection : x , y = obj . pos x = x + dx y = y + dy obj . pos = ( wx . Point ( x , y ) ) elif self . overlay : wx obj = self . current pos = evt . Get Position ( ) if evt . Get Event Object ( ) != wx obj : pos = evt . Get Event Object ( ) . Client To Screen ( pos ) pos = wx obj . Screen To Client ( pos ) rect = wx . Rect PP ( self . pos , pos ) dc = wx . Client DC ( wx obj ) odc = wx . DC Overlay ( self . overlay , dc ) odc . Clear ( ) dc . Set Pen ( wx . Pen ( \"blue\" , 2 ) ) if 'wx Mac' in wx . Platform Info : dc . Set Brush ( wx . Brush ( wx . Colour ( 0x C0 , 0x C0 , 0x C0 , 0x80 ) ) ) else : dc . Set Brush ( wx . TRANSPARENT BRUSH ) dc . Draw Rectangle Rect ( rect ) del odc", "predictions": ["mouse window to be done"], "references": ["move the selected object"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 3602, "code": "def key press ( self , event ) : key = event . Get Key Code ( ) if key in ( wx . WXK LEFT , wx . WXK UP , wx . WXK RIGHT , wx . WXK DOWN ) : for obj in self . selection : x , y = obj . pos if event . Shift Down ( ) : if key == wx . WXK LEFT : x = ( x - GRID SIZE [ 0 ] ) / GRID SIZE [ 0 ] * GRID SIZE [ 0 ] elif key == wx . WXK RIGHT : x = ( x + GRID SIZE [ 0 ] ) / GRID SIZE [ 0 ] * GRID SIZE [ 0 ] elif key == wx . WXK UP : y = ( y - GRID SIZE [ 1 ] ) / GRID SIZE [ 1 ] * GRID SIZE [ 1 ] elif key == wx . WXK DOWN : y = ( y + GRID SIZE [ 1 ] ) / GRID SIZE [ 1 ] * GRID SIZE [ 1 ] else : if key == wx . WXK LEFT : x = x - 1 elif key == wx . WXK RIGHT : x = x + 1 elif key == wx . WXK UP : y = y - 1 elif key == wx . WXK DOWN : y = y + 1 obj . pos = ( x , y ) elif key == wx . WXK DELETE : self . delete ( event ) elif key == wx . WXK INSERT : self . duplicate ( event ) else : if DEBUG : print \"KEY:\" , key", "predictions": ["handles key events events"], "references": ["support cursor keys to move components one pixel at a time"], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 3603, "code": "def delete ( self , event ) : for obj in self . selection : if obj : if DEBUG : print \"deleting\" , obj . name obj . destroy ( ) self . selection = [ ] self . inspector . load object ( )", "predictions": ["delete an object from the inspector inspector"], "references": ["delete all of the selected objects"], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 3604, "code": "def duplicate ( self , event ) : new selection = [ ] for obj in self . selection : if obj : if DEBUG : print \"duplicating\" , obj . name obj . sel marker . destroy ( ) obj . sel marker = None obj2 = obj . duplicate ( ) obj2 . sel marker = Selection Marker ( obj2 ) obj2 . sel marker . show ( True ) new selection . append ( obj2 ) self . selection = new selection self . inspector . load object ( )", "predictions": ["duplicate an event in the selection"], "references": ["create a copy of each selected object"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 3605, "code": "def refresh ( self ) : self . bmp = self . obj . snapshot ( ) self . Raise ( ) self . Show ( ) self . Refresh ( )", "predictions": ["refresh the object from the snapshot ."], "references": ["capture the new control superficial image after an update"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 3606, "code": "def Calculate Best Position ( self , widget ) : if isinstance ( widget , wx . Frame ) : screen = wx . Client Display Rect ( ) [ 2 : ] left , top = widget . Client To Screen XY ( 0 , 0 ) right , bottom = widget . Client To Screen XY ( * widget . Get Client Rect ( ) [ 2 : ] ) size = self . Get Size ( ) xpos = right ypos = bottom - size [ 1 ] self . Set Position ( ( xpos , ypos ) ) else : STT . Tool Tip Window . Calculate Best Position ( self , widget )", "predictions": ["draw a sub - widget widget"], "references": ["when dealing with a top - level window position it absolute lower - right"], "bleu": 0.06443935473636557, "rouge_l": 0.18654434250764526}
{"id": 3607, "code": "def Get Py Data ( self , item ) : wx data = self . Get Item Data ( item ) py data = self . py data map . get ( wx data ) return py data", "predictions": ["return the data source for the given item ."], "references": ["returns the pyth item data associated with the item"], "bleu": 0.17747405280050263, "rouge_l": 0.4444444444444444}
{"id": 3608, "code": "def Set Py Data ( self , item , py data ) : wx data = wx . New Id ( ) self . Set Item Data ( item , wx data ) self . py data map [ wx data ] = py data self . wx data map [ py data ] = wx data return wx data", "predictions": ["set the data for the specified item"], "references": ["set the python item data associated wit the wx item"], "bleu": 0.18938334565508194, "rouge_l": 0.5700934579439253}
{"id": 3609, "code": "def Find Py Data ( self , start , py data ) : wx data = self . wx data map [ py data ] if wx . VERSION < ( 3 , 0 , 0 ) or 'classic' in wx . version ( ) : data = self . Find Item Data ( start , wx data ) else : data = self . Find Item ( start , wx data ) return data", "predictions": ["search the source source"], "references": ["do a reverse look up for an item containing the requested data"], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 3610, "code": "def Delete Item ( self , item ) : wx data = self . Get Item Data ( item ) py data = self . py data map [ wx data ] del self . py data map [ wx data ] del self . wx data map [ py data ] wx . List Ctrl . Delete Item ( self , item )", "predictions": ["delete item from the data object"], "references": ["remove the item from the list and unset the related data"], "bleu": 0.18892891214792823, "rouge_l": 0.4468864468864468}
{"id": 3611, "code": "def Delete All Items ( self ) : self . py data map . clear ( ) self . wx data map . clear ( ) wx . List Ctrl . Delete All Items ( self )", "predictions": ["delete all data from the data store ."], "references": ["remove all the item from the list and unset the related data"], "bleu": 0.14544785215055717, "rouge_l": 0.3860759493670886}
{"id": 3612, "code": "def delete ( self , a position ) : key = self . wx obj . Get Py Data ( a position ) del self . items [ key ]", "predictions": ["delete the object at the given position ."], "references": ["deletes the item at the zero - based index n from the control ."], "bleu": 0.11327490115090784, "rouge_l": 0.346590909090909}
{"id": 3613, "code": "def clear all ( self ) : self . clear ( ) for ch in reversed ( self . columns ) : del self [ ch . name ]", "predictions": ["clears all columns from the queue"], "references": ["remove all items and column headings"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 3614, "code": "def clear ( self ) : dict . clear ( self ) self . key = 0 if hasattr ( self . list view , \"wx obj\" ) : self . list view . wx obj . Delete All Items ( )", "predictions": ["clears the contents of the object"], "references": ["remove all items and reset internal structures"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 3615, "code": "def set selection ( self , index , dummy = False ) : if index is None : self . wx obj . Set Selection ( - 1 ) if hasattr ( self . wx obj , \"Set Value\" ) : self . wx obj . Set Value ( \"\" ) else : self . wx obj . Set Selection ( index ) wx event = Item Container Control Select Event ( self . commandtype , index , self . wx obj ) if hasattr ( self , \"onchange\" ) and self . onchange : event = Form Event ( name = \"change\" , wx event = wx event ) self . onchange ( event )", "predictions": ["set a selection to the wx"], "references": ["sets the item at index n to be the selected item ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 3616, "code": "def get string selection ( self ) : if self . multiselect : return [ self . wx obj . Get String ( i ) for i in self . wx obj . Get Selections ( ) ] else : return self . wx obj . Get String Selection ( )", "predictions": ["returns a get get the get get get the res token"], "references": ["returns the label of the selected item or an empty string if none"], "bleu": 0.11294012253658708, "rouge_l": 0.24629878869448185}
{"id": 3617, "code": "def set data ( self , n , data ) : self . wx obj . Set Client Data ( n , data ) self . items dict [ data ] = self . get string ( n )", "predictions": ["sets the user s user s user s user attributes"], "references": ["associate the given client data with the item at position n ."], "bleu": 0.10320893749383378, "rouge_l": 0.08944281524926685}
{"id": 3618, "code": "def append ( self , a string , data = None ) : self . wx obj . Append ( a string , data ) self . items dict [ data ] = a string", "predictions": ["listdir listdir to add a new id"], "references": ["adds the item to the control associating the given data if not none ."], "bleu": 0.06833381956448398, "rouge_l": 0.08983799705449189}
{"id": 3619, "code": "def delete ( self , a position ) : self . wx obj . Delete ( a position ) data = self . get data ( ) if data in self . items dict : del self . items dict [ data ]", "predictions": ["comment for testing ."], "references": ["deletes the item at the zero - based index n from the control ."], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 3620, "code": "def represent ( obj , prefix , parent = \"\" , indent = 0 , context = False , max cols = 80 ) : try : name = getattr ( obj , \"name\" , \"\" ) class name = \"%s.%s\" % ( prefix , obj . class . name ) padding = len ( class name ) + 1 + indent * 4 + ( 5 if context else 0 ) params = [ ] for ( k , spec ) in sorted ( obj . meta . specs . items ( ) , key = get sort key ) : if k == \"index\" : continue if k == \"parent\" and parent != \"\" : v = parent else : v = getattr ( obj , k , \"\" ) if ( not isinstance ( spec , Internal Spec ) and v != spec . default and ( k != 'id' or v > 0 ) and isinstance ( v , ( basestring , int , long , float , bool , dict , list , decimal . Decimal , datetime . datetime , datetime . date , datetime . time , Font , Color ) ) and repr ( v ) != 'None' ) : v = repr ( v ) else : v = None if v is not None : params . append ( \"%s=%s\" % ( k , v ) ) param lines = [ ] line = \"\" for param in params : if len ( line + param ) + 3 > max cols - padding : param lines . append ( line ) line = \"\" line += param + \", \" param lines . append ( line ) param str = ( \"\\n%s\" % ( \" \" * padding ) ) . join ( param lines ) return \"%s(%s)\" % ( class name , param str ) except : raise return object . repr ( obj )", "predictions": ["decode a string into a string of force - value pairs . ."], "references": ["construct a string representing the object"], "bleu": 0.12571192676522522, "rouge_l": 0.22550831792975967}
{"id": 3621, "code": "def get ( obj name , init = False ) : wx parent = None if isinstance ( obj name , basestring ) : obj parent = COMPONENTS . get ( obj name ) if not obj parent : wx parent = wx . Find Window By Name ( obj name ) if wx parent : obj parent = getattr ( wx parent , \"obj\" ) else : for obj in COMPONENTS . values ( ) : if obj . name == obj name : obj parent = obj else : obj parent = obj name return obj parent or wx parent", "predictions": ["return up the object s if it exists . is not found"], "references": ["find an object already created"], "bleu": 0.10390302174233558, "rouge_l": 0.12708333333333333}
{"id": 3622, "code": "def duplicate ( self , new parent = None ) : kwargs = { } for spec name , spec in self . meta . specs . items ( ) : value = getattr ( self , spec name ) if isinstance ( value , Color ) : print \"COLOR\" , value , value . default if value . default : value = None if value is not None : kwargs [ spec name ] = value del kwargs [ 'parent' ] new id = wx . New Id ( ) kwargs [ 'id' ] = new id kwargs [ 'name' ] = \"%s %s\" % ( kwargs [ 'name' ] , new id ) new obj = self . class ( new parent or self . get parent ( ) , * * kwargs ) for child in self : child . duplicate ( new obj ) return new obj", "predictions": ["start a new object with the same attributes"], "references": ["create a new object exactly similar to self"], "bleu": 0.2984745896009823, "rouge_l": 0.375}
{"id": 3623, "code": "def sizer add ( self , child ) : if self . sizer : if DEBUG : print \"adding to sizer:\" , child . name border = None if not border : border = child . sizer border flags = child . sizer flags if child . sizer align : flags |= child . sizer align if child . sizer expand : flags |= wx . EXPAND if 'grid' in self . sizer : self . sizer . Add ( child . wx obj , flag = flags , border = border , pos = ( child . sizer row , child . sizer col ) , span = ( child . sizer rowspan , child . sizer colspan ) ) else : self . sizer . Add ( child . wx obj , 0 , flags , border )", "predictions": ["default set the child to the set of child"], "references": ["called when adding a control to the window"], "bleu": 0.18575057999133596, "rouge_l": 0.2378167641325536}
{"id": 3624, "code": "def set parent ( self , new parent , init = False ) : Component . set parent ( self , new parent , init ) if not init : if DEBUG : print \"reparenting\" , ctrl . name if hasattr ( self . wx obj , \"Reparent\" ) : self . wx obj . Reparent ( self . parent . wx obj )", "predictions": ["inspect the parent object"], "references": ["re - parent a child control with the new wx_obj parent"], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 3625, "code": "def resize ( self , evt = None ) : if DEBUG : print \"RESIZE!\" , self . name , self . width , self . height if not isinstance ( self . wx obj , wx . Top Level Window ) : if self . left and self . left [ - 1 ] == \"%\" or self . top and self . top [ - 1 ] == \"%\" : if DEBUG : print \"MOVING\" , self . name , self . width self . set pos ( ( self . left , self . top ) ) if self . width and self . width [ - 1 ] == \"%\" or self . height and self . height [ - 1 ] == \"%\" : if DEBUG : print \"RESIZING\" , self . name , self . width , self . height self . set size ( ( self . width , self . height ) ) for child in self : if isinstance ( child , Control ) : child . resize ( evt ) if evt : evt . Skip ( )", "predictions": ["resizes the contents of the editor"], "references": ["automatically adjust relative pos and size of children controls"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 3626, "code": "def tile background ( self , dc ) : sz = self . wx obj . Get Client Size ( ) bmp = self . bitmap . get bits ( ) w = bmp . Get Width ( ) h = bmp . Get Height ( ) if isinstance ( self , wx . Scrolled Window ) : spx , spy = self . wx obj . Get Scroll Pixels Per Unit ( ) vsx , vsy = self . wx obj . Get View Start ( ) dx , dy = ( spx * vsx ) % w , ( spy * vsy ) % h else : dx , dy = ( w , h ) x = - dx while x < sz . width : y = - dy while y < sz . height : dc . Draw Bitmap ( bmp , x , y ) y = y + h x = x + w", "predictions": ["control the control of the if it is not a control"], "references": ["make several copies of the background bitmap"], "bleu": 0.14991106946711685, "rouge_l": 0.2314990512333966}
{"id": 3627, "code": "def on erase background ( self , evt ) : if self . bitmap : dc = evt . Get DC ( ) if not dc : dc = wx . Client DC ( self ) r = self . wx obj . Get Update Region ( ) . Get Box ( ) dc . Set Clipping Region ( r . x , r . y , r . width , r . height ) if self . background tiling : self . tile background ( dc ) else : dc . Draw Bitmap Point ( self . bitmap . get bits ( ) , ( 0 , 0 ) )", "predictions": ["re - font background the bitmap background"], "references": ["draw the image as background"], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 3628, "code": "def on paint ( self , event ) : dc = wx . GCDC ( wx . Paint DC ( self . wx obj ) ) dc . Set Font ( self . wx obj . Get Font ( ) ) dc . Set Text Foreground ( self . wx obj . Get Foreground Colour ( ) ) dc . Draw Text ( self . wx obj . Get Label ( ) , 0 , 0 )", "predictions": ["called when a window is being pressed"], "references": ["custom draws the label when transparent background is needed"], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 3629, "code": "def get column headings ( self ) : headers = [ ctrl for ctrl in self if isinstance ( ctrl , Grid Column ) ] return sorted ( headers , key = lambda ch : ch . index )", "predictions": ["returns a list of column objects"], "references": ["return a list of children sub - components that are column headings"], "bleu": 0.1599248714366856, "rouge_l": 0.41924398625429554}
{"id": 3630, "code": "def Reset View ( self , grid ) : grid . Begin Batch ( ) for current , new , delmsg , addmsg in [ ( self . rows , self . Get Number Rows ( ) , gridlib . GRIDTABLE NOTIFY ROWS DELETED , gridlib . GRIDTABLE NOTIFY ROWS APPENDED ) , ( self . cols , self . Get Number Cols ( ) , gridlib . GRIDTABLE NOTIFY COLS DELETED , gridlib . GRIDTABLE NOTIFY COLS APPENDED ) , ] : if new < current : msg = gridlib . Grid Table Message ( self , delmsg , new , current - new ) grid . Process Table Message ( msg ) elif new > current : msg = gridlib . Grid Table Message ( self , addmsg , new - current ) grid . Process Table Message ( msg ) self . Update Values ( grid ) grid . End Batch ( ) self . rows = self . Get Number Rows ( ) self . cols = self . Get Number Cols ( ) self . update Col Attrs ( grid ) grid . Adjust Scrollbars ( ) grid . Force Refresh ( )", "predictions": ["reset all msg to the msg"], "references": ["update the grid if rows and columns have been added or deleted"], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 3631, "code": "def Update Values ( self , grid ) : msg = gridlib . Grid Table Message ( self , gridlib . GRIDTABLE REQUEST VIEW GET VALUES ) grid . Process Table Message ( msg )", "predictions": ["update the grid grid"], "references": ["update all displayed values"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 3632, "code": "def update Col Attrs ( self , grid ) : col = 0 for column in self . columns : attr = gridlib . Grid Cell Attr ( ) if False : attr . Set Read Only ( ) if False : attr . Set Renderer ( renderer ) grid . Set Col Size ( col , column . width ) grid . Set Col Attr ( col , attr ) col += 1", "predictions": ["updates the destroy attribute of the specified grid"], "references": ["update the column attributes to add the appropriate renderer"], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 3633, "code": "def clear ( self ) : for i in range ( len ( self ) - 1 , - 1 , - 1 ) : del self [ i ] self . key = 0 if hasattr ( self . grid view , \"wx obj\" ) : self . grid view . wx obj . Clear Grid ( )", "predictions": ["clears all start and start the start of the start of the start"], "references": ["remove all rows and reset internal structures"], "bleu": 0.10571070857151538, "rouge_l": 0.21143847487001732}
{"id": 3634, "code": "def Create ( self , parent , id , evt Handler ) : self . tc = wx . Combo Box ( parent , id , \"\" , ( 100 , 50 ) ) self . Set Control ( self . tc ) self . tc . Push Event Handler ( wx . Evt Handler ( ) ) self . tc . Bind ( wx . EVT COMBOBOX , self . On Change )", "predictions": ["create a new root window"], "references": ["called to create the control which must derive from wxcontrol ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 3635, "code": "def Begin Edit ( self , row , col , grid ) : self . start Value = grid . Get Table ( ) . Get Value ( row , col ) choices = grid . Get Table ( ) . columns [ col ] . choices self . tc . Clear ( ) self . tc . Append Items ( choices ) self . tc . Set String Selection ( self . start Value ) self . tc . Set Focus ( )", "predictions": ["update all rows in this column ."], "references": ["fetch the value from the table and prepare the edit control"], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 3636, "code": "def End Edit ( self , row , col , grid , val = None ) : changed = False val = self . tc . Get String Selection ( ) print \"val\" , val , row , col , self . start Value if val != self . start Value : changed = True grid . Get Table ( ) . Set Value ( row , col , val ) self . start Value = '' self . tc . Set String Selection ( '' ) return changed", "predictions": ["returns a column with a column value"], "references": ["complete the editing of the current cell . returns true if changed"], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 3637, "code": "def Is Accepted Key ( self , evt ) : return ( not ( evt . Control Down ( ) or evt . Alt Down ( ) ) and evt . Get Key Code ( ) != wx . WXK SHIFT )", "predictions": ["returns true if the cursor is a c ++"], "references": ["return true to allow the given key to start editing"], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 3638, "code": "def Starting Key ( self , evt ) : key = evt . Get Key Code ( ) ch = None if key in [ wx . WXK NUMPAD0 , wx . WXK NUMPAD1 , wx . WXK NUMPAD2 , wx . WXK NUMPAD3 , wx . WXK NUMPAD4 , wx . WXK NUMPAD5 , wx . WXK NUMPAD6 , wx . WXK NUMPAD7 , wx . WXK NUMPAD8 , wx . WXK NUMPAD9 ] : ch = ch = chr ( ord ( '0' ) + key - wx . WXK NUMPAD0 ) elif key < 256 and key >= 0 and chr ( key ) in string . printable : ch = chr ( key ) if not evt . Shift Down ( ) : ch = ch . lower ( ) if ch is not None : self . tc . Set String Selection ( ch ) else : evt . Skip ( )", "predictions": ["follow the image from the system"], "references": ["this will be called to let the editor do something with the first key"], "bleu": 0.06443935473636557, "rouge_l": 0.18654434250764526}
{"id": 3639, "code": "def Enable ( self , value ) : for i in range ( self . Get Menu Item Count ( ) ) : it = self . Find Item By Position ( i ) it . Enable ( value )", "predictions": ["updates the field values"], "references": ["enable or disable all menu items"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 3640, "code": "def Is Enabled ( self , * args , * * kwargs ) : for i in range ( self . Get Menu Item Count ( ) ) : it = self . Find Item By Position ( i ) if not it . Is Enabled ( ) : return False return True", "predictions": ["returns true if any of the lock is not a subclass of the object"], "references": ["check if all menu items are enabled"], "bleu": 0.08839374326825923, "rouge_l": 0.10132890365448505}
{"id": 3641, "code": "def Enable ( self , value ) : for i in range ( self . Get Menu Count ( ) ) : self . Enable Top ( i , value )", "predictions": ["sets the value of the field to the given self wx wx wx wx"], "references": ["enable or disable all top menus"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 3642, "code": "def Is Enabled ( self , * args , * * kwargs ) : for i in range ( self . Get Menu Count ( ) ) : if not self . Is Enabled Top ( i ) : return False return True", "predictions": ["returns true if the registry is a dimension"], "references": ["check if all top menus are enabled"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 3643, "code": "def Remove Item ( self , menu ) : menus = self . Get Menus ( ) menus = [ submenu for submenu in menus if submenu [ 0 ] != menu ] self . Set Menus ( menus )", "predictions": ["removes all py from the menu"], "references": ["helper method to remove a menu avoiding using its position"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 3644, "code": "def set Object Tag ( self , object , tag ) : object . attributes = { } object . name = tag . Get Name ( ) . lower ( ) for name in self . attributes : object . attributes [ \" %s\" % name ] = tag . Get Param ( name ) if object . attributes [ \" %s\" % name ] == \"\" : object . attributes [ \" %s\" % name ] = None", "predictions": ["sets the wx attribute of the given object"], "references": ["add a tag attribute to the wx window"], "bleu": 0.22679164443904004, "rouge_l": 0.25}
{"id": 3645, "code": "def autosummary table visit html ( self , node ) : try : tbody = node [ 0 ] [ 0 ] [ - 1 ] for row in tbody : col1 entry = row [ 0 ] par = col1 entry [ 0 ] for j , subnode in enumerate ( list ( par ) ) : if isinstance ( subnode , nodes . Text ) : new text = unicode ( subnode . astext ( ) ) new text = new text . replace ( u\" \" , u\"\\u00a0\" ) par [ j ] = nodes . Text ( new text ) except Index Error : pass", "predictions": ["visit for entry ast for entry ast . ."], "references": ["make the first column of the table non - breaking ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 3646, "code": "def mangle signature ( sig , max chars = 30 ) : s = re . sub ( r\"^\\((.*)\\)$\" , r\"\\1\" , sig ) . strip ( ) s = re . sub ( r\"\\\\\\\\\" , \"\" , s ) s = re . sub ( r\"\\\\'\" , \"\" , s ) s = re . sub ( r\"'[^']*'\" , \"\" , s ) args = [ ] opts = [ ] opt re = re . compile ( r\"^(.*, |)([a-z A-Z0-9 *]+)=\" ) while s : m = opt re . search ( s ) if not m : args = s . split ( ', ' ) break opts . insert ( 0 , m . group ( 2 ) ) s = m . group ( 1 ) [ : - 2 ] sig = limited join ( \", \" , args , max chars = max chars - 2 ) if opts : if not sig : sig = \"[%s]\" % limited join ( \", \" , opts , max chars = max chars - 4 ) elif len ( sig ) < max chars - 4 - 2 - 3 : sig += \"[, %s]\" % limited join ( \", \" , opts , max chars = max chars - len ( sig ) - 4 - 2 ) return u\"(%s)\" % sig", "predictions": ["clear the signature signature signature"], "references": ["reformat a function signature to a more compact form ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 3647, "code": "def import by name ( name ) : try : name parts = name . split ( '.' ) modname = '.' . join ( name parts [ : - 1 ] ) if modname : try : import ( modname ) mod = sys . modules [ modname ] return getattr ( mod , name parts [ - 1 ] ) , mod except ( Import Error , Index Error , Attribute Error ) : pass last j = 0 modname = None for j in reversed ( range ( 1 , len ( name parts ) + 1 ) ) : last j = j modname = '.' . join ( name parts [ : j ] ) try : import ( modname ) except : continue if modname in sys . modules : break if last j < len ( name parts ) : parent = None obj = sys . modules [ modname ] for obj name in name parts [ last j : ] : parent = obj obj = getattr ( obj , obj name ) return obj , parent else : return sys . modules [ modname ] , None except ( Value Error , Import Error , Attribute Error , Key Error ) , e : raise Import Error ( * e . args )", "predictions": ["set up the module module selection"], "references": ["import a python object given its full name ."], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 3648, "code": "def alert ( message , title = \"\" , parent = None , scrolled = False , icon = \"exclamation\" ) : if not scrolled : icons = { 'exclamation' : wx . ICON EXCLAMATION , 'error' : wx . ICON ERROR , 'question' : wx . ICON QUESTION , 'info' : wx . ICON INFORMATION } style = wx . OK | icons [ icon ] result = dialogs . message Dialog ( parent , message , title , style ) else : result = dialogs . scrolled Message Dialog ( parent , message , title )", "predictions": ["alert a message to a message"], "references": ["show a simple pop - up modal dialog"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 3649, "code": "def prompt ( message = \"\" , title = \"\" , default = \"\" , multiline = False , password = None , parent = None ) : if password : style = wx . TE PASSWORD | wx . OK | wx . CANCEL result = dialogs . text Entry Dialog ( parent , message , title , default , style ) elif multiline : style = wx . TE MULTILINE | wx . OK | wx . CANCEL result = dialogs . text Entry Dialog ( parent , message , title , default , style ) result . text = '\\n' . join ( result . text . splitlines ( ) ) else : result = dialogs . text Entry Dialog ( parent , message , title , default ) if result . accepted : return result . text", "predictions": ["displays a message with the user s password"], "references": ["modal dialog asking for an input returns string or none if cancelled"], "bleu": 0.08179133792443427, "rouge_l": 0.0}
{"id": 3650, "code": "def select font ( message = \"\" , title = \"\" , font = None , parent = None ) : if font is not None : wx font = font . get wx font ( ) else : wx font = None font = Font ( ) result = dialogs . font Dialog ( parent , font = wx font ) if result . accepted : font data = result . font Data result . color = result . font Data . Get Colour ( ) . Get ( ) wx font = result . font Data . Get Chosen Font ( ) font . set wx font ( wx font ) wx font = None return font", "predictions": ["select a font from a message"], "references": ["show a dialog to select a font"], "bleu": 0.36798327352994814, "rouge_l": 0.45522388059701485}
{"id": 3651, "code": "def select color ( message = \"\" , title = \"\" , color = None , parent = None ) : result = dialogs . color Dialog ( parent , color = color ) return result . accepted and result . color", "predictions": ["returns a color with the given color ."], "references": ["show a dialog to pick a color"], "bleu": 0.21105340631872638, "rouge_l": 0.26991150442477874}
{"id": 3652, "code": "def choose directory ( message = 'Choose a directory' , path = \"\" , parent = None ) : result = dialogs . directory Dialog ( parent , message , path ) return result . path", "predictions": ["returns a directory to a directory ."], "references": ["show a dialog to choose a directory"], "bleu": 0.2777619034011791, "rouge_l": 0.5714285714285714}
{"id": 3653, "code": "def find ( default = '' , whole words = 0 , case sensitive = 0 , parent = None ) : result = dialogs . find Dialog ( parent , default , whole words , case sensitive ) return { 'text' : result . search Text , 'whole words' : result . whole Words Only , 'case sensitive' : result . case Sensitive }", "predictions": ["returns a dictionary with the default case of the default words ."], "references": ["shows a find text dialog"], "bleu": 0.10390302174233558, "rouge_l": 0.12708333333333333}
{"id": 3654, "code": "def clear ( self ) : dict . clear ( self ) self . key = 0 if hasattr ( self . tree view , \"wx obj\" ) : self . tree view . wx obj . Delete All Items ( )", "predictions": ["clears the contents of this tree ."], "references": ["remove all items and reset internal structures"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3655, "code": "def set has children ( self , has children = True ) : self . tree model . tree view . wx obj . Set Item Has Children ( self . wx item , has children )", "predictions": ["sets the children of the tree"], "references": ["force appearance of the button next to the item"], "bleu": 0.1894765350842885, "rouge_l": 0.2573839662447257}
{"id": 3656, "code": "def set icon ( self , icon = None ) : if icon is not None : try : wx icon = wx . Icon ( icon , wx . BITMAP TYPE ICO ) self . wx obj . Set Icon ( wx icon ) except : pass", "predictions": ["set the icon icon"], "references": ["set icon based on resource values"], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 3657, "code": "def show ( self , value = True , modal = None ) : self . wx obj . Show ( value ) if modal : disabler = wx . Window Disabler ( self . wx obj ) eventloop = wx . Event Loop ( ) def on close modal ( evt ) : evt . Skip ( ) eventloop . Exit ( ) self . wx obj . Bind ( wx . EVT CLOSE , on close modal ) eventloop . Run ( ) del disabler", "predictions": ["show the window to the wx"], "references": ["display or hide the window optionally disabling all other windows"], "bleu": 0.14925824694560996, "rouge_l": 0.23921568627450981}
{"id": 3658, "code": "def resize ( self , evt = None ) : for child in self : if isinstance ( child , Control ) : child . resize ( evt ) if evt : evt . Skip ( )", "predictions": ["resize all children of this widget ."], "references": ["automatically adjust relative pos and size of children controls"], "bleu": 0.15447878876032708, "rouge_l": 0.12224448897795591}
{"id": 3659, "code": "def parse ( filename = \"\" ) : s = open ( filename ) . read ( ) ##s.decode(\"latin1\").encode(\"utf8\") import datetime , decimal rsrc = eval ( s ) return rsrc", "predictions": ["parse a datetime from a string"], "references": ["open read and eval the resource from the source file"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 3660, "code": "def save ( filename , rsrc ) : s = pprint . pformat ( rsrc ) open ( filename , \"w\" ) . write ( s )", "predictions": ["save the values to a file ."], "references": ["save the resource to the source file"], "bleu": 0.2777619034011791, "rouge_l": 0.5714285714285714}
{"id": 3661, "code": "def build window ( res ) : kwargs = dict ( res . items ( ) ) wintype = kwargs . pop ( 'type' ) menubar = kwargs . pop ( 'menubar' , None ) components = kwargs . pop ( 'components' ) panel = kwargs . pop ( 'panel' , { } ) from gui import registry import gui winclass = registry . WINDOWS [ wintype ] win = winclass ( * * kwargs ) if False and panel is not None : panel [ 'name' ] = 'panel' p = gui . Panel ( win , * * panel ) else : p = win if components : for comp in components : build component ( comp , parent = p ) if menubar : mb = gui . Menu Bar ( name = \"menu\" , parent = win ) for menu in menubar : build component ( menu , parent = mb ) return win", "predictions": ["build and return a window object from components ."], "references": ["create a gui2py window based on the python resource"], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 3662, "code": "def build component ( res , parent = None ) : kwargs = dict ( res . items ( ) ) comtype = kwargs . pop ( 'type' ) if 'components' in res : components = kwargs . pop ( 'components' ) elif comtype == 'Menu' and 'items' in res : components = kwargs . pop ( 'items' ) else : components = [ ] from gui import registry if comtype in registry . CONTROLS : comclass = registry . CONTROLS [ comtype ] elif comtype in registry . MENU : comclass = registry . MENU [ comtype ] elif comtype in registry . MISC : comclass = registry . MISC [ comtype ] else : raise Runtime Error ( \"%s not in registry\" % comtype ) com = comclass ( parent = parent , * * kwargs ) for comp in components : build component ( comp , parent = com ) return com", "predictions": ["build the component from the components of the components"], "references": ["create a gui2py control based on the python resource"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 3663, "code": "def convert ( self , name ) : new name = PYTHONCARD PROPERTY MAP . get ( name ) if new name : print \"WARNING: property %s should be %s (%s)\" % ( name , new name , self . obj . name ) return new name else : return name", "predictions": ["convert property to new name ."], "references": ["translate gui2py attribute name from pythoncard legacy code"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 3664, "code": "def set data ( data ) : try : if wx . The Clipboard . Open ( ) : if isinstance ( data , ( str , unicode ) ) : do = wx . Text Data Object ( ) do . Set Text ( data ) wx . The Clipboard . Set Data ( do ) elif isinstance ( data , wx . Bitmap ) : do = wx . Bitmap Data Object ( ) do . Set Bitmap ( data ) wx . The Clipboard . Set Data ( do ) wx . The Clipboard . Close ( ) except : pass", "predictions": ["set data type from data"], "references": ["write content to the clipboard data can be either a string or a bitmap"], "bleu": 0.04512859433163675, "rouge_l": 0.09697933227344992}
{"id": 3665, "code": "def load object ( self , obj = None ) : if obj : self . root obj = obj else : obj = self . root obj self . tree . Delete All Items ( ) self . root = self . tree . Add Root ( \"application\" ) self . tree . Set Item Text ( self . root , \"App\" , 1 ) self . tree . Set Item Text ( self . root , \"col 2 root\" , 2 ) #self.tree.Set Item Image(self.root, fldridx, which = wx.Tree Item Icon Normal) #self.tree.Set Item Image(self.root, fldropenidx, which = wx.Tree Item Icon Expanded) self . build tree ( self . root , obj ) self . tree . Expand ( self . root )", "predictions": ["loads an object from the referenceset referenceset ."], "references": ["add the object and all their childs"], "bleu": 0.17747405280050269, "rouge_l": 0.13495575221238937}
{"id": 3666, "code": "def inspect ( self , obj , context menu = False , edit prop = False , mouse pos = None ) : child = self . tree . Find Item ( self . root , obj . name ) if DEBUG : print \"inspect child\" , child if child : self . tree . Scroll To ( child ) self . tree . Set Current Item ( child ) self . tree . Select Item ( child ) child . Selected = True self . activate item ( child , edit prop ) if context menu : self . show context menu ( child , mouse pos )", "predictions": ["inspect the object in the tree"], "references": ["select the object and show its properties"], "bleu": 0.24608524656663955, "rouge_l": 0.3034825870646766}
{"id": 3667, "code": "def activate item ( self , child , edit prop = False , select = False ) : d = self . tree . Get Item Data ( child ) if d : o = d . Get Data ( ) self . selected obj = o callback = lambda o = o , * * kwargs : self . update ( o , * * kwargs ) self . propeditor . load object ( o , callback ) if edit prop : wx . Call After ( self . propeditor . edit ) if select and self . designer : self . designer . select ( o ) else : self . selected obj = None", "predictions": ["activate an item on the tree"], "references": ["load the selected item in the property editor"], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 3668, "code": "def update ( self , obj , * * kwargs ) : child = self . tree . Find Item ( self . root , kwargs [ 'name' ] ) if DEBUG : print \"update child\" , child , kwargs if child : self . tree . Scroll To ( child ) self . tree . Set Current Item ( child ) self . tree . Select Item ( child ) child . Selected = True self . tree . Set Item Text ( child , obj . name , 0 )", "predictions": ["updates the object with the given obj ."], "references": ["update the tree item when the object name changes"], "bleu": 0.20014292374951972, "rouge_l": 0.232824427480916}
{"id": 3669, "code": "def show context menu ( self , item , mouse pos = None ) : if item : d = self . tree . Get Item Data ( item ) if d : obj = d . Get Data ( ) if obj : self . highlight ( obj . wx obj ) self . obj = obj menu = wx . Menu ( ) id del , id dup , id raise , id lower = [ wx . New Id ( ) for i in range ( 4 ) ] menu . Append ( id del , \"Delete\" ) menu . Append ( id dup , \"Duplicate\" ) menu . Append ( id raise , \"Bring to Front\" ) menu . Append ( id lower , \"Send to Back\" ) sm = wx . Menu ( ) for ctrl in sorted ( obj . meta . valid children , key = lambda c : registry . ALL . index ( c . meta . name ) ) : new id = wx . New Id ( ) sm . Append ( new id , ctrl . meta . name ) self . Bind ( wx . EVT MENU , lambda evt , ctrl = ctrl : self . add child ( ctrl , mouse pos ) , id = new id ) menu . Append Menu ( wx . New Id ( ) , \"Add child\" , sm ) self . Bind ( wx . EVT MENU , self . delete , id = id del ) self . Bind ( wx . EVT MENU , self . duplicate , id = id dup ) self . Bind ( wx . EVT MENU , self . bring to front , id = id raise ) self . Bind ( wx . EVT MENU , self . send to back , id = id lower ) self . Popup Menu ( menu ) menu . Destroy ( ) self . load object ( self . root obj )", "predictions": ["show the context menu"], "references": ["open a popup menu with options regarding the selected object"], "bleu": 0.08872444253557525, "rouge_l": 0.13260869565217392}
{"id": 3670, "code": "def select option ( self ) : if self . disabled : warn ( \"Attempt to select disabled option: {}\" . format ( self . value or self . text ) ) self . base . select option ( )", "predictions": ["select the current option ."], "references": ["select this node if it is an option element inside a select tag ."], "bleu": 0.053667245469253895, "rouge_l": 0.2909379968203497}
{"id": 3671, "code": "def raise server error ( self ) : if self . server and self . server . error : try : if capybara . raise server errors : raise self . server . error finally : self . server . reset error ( )", "predictions": ["raise the server error if the server is not enabled ."], "references": ["raise errors encountered by the server ."], "bleu": 0.17033186037639278, "rouge_l": 0.4629981024667932}
{"id": 3672, "code": "def traceback ( self ) -> str : if not self . log traceback : return \"\" exc info = sys . exc info ( ) stack = traceback . extract stack ( ) exc tb = traceback . extract tb ( exc info [ 2 ] ) full tb = stack [ : 1 ] + exc tb exc line : typing . List [ str ] = traceback . format exception only ( * exc info [ : 2 ] ) tb text = \"\\n Traceback (most recent call last):\\n\" + \"\" . join ( traceback . format list ( full tb ) ) + \"\" . join ( exc line ) return tb text", "predictions": ["extract the traceback from the exception ."], "references": ["get outer traceback text for logging ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 3673, "code": "def get obj source ( self , instance : typing . Any , owner : typing . Optional [ type ] = None ) -> str : if self . log object repr : return f\"{instance!r}\" return f\"<{owner. name  if owner is not None else instance. class . name }() at 0x{id(instance):X}>\"", "predictions": ["return the source source name of the given owner ."], "references": ["get object repr block ."], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 3674, "code": "def logger ( self , logger : typing . Union [ logging . Logger , str , None ] ) -> None : if logger is None or isinstance ( logger , logging . Logger ) : self . logger = logger else : self . logger = logging . get Logger ( logger )", "predictions": ["override asyncio method ."], "references": ["logger instance to use as override ."], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 3675, "code": "def channels ( self ) : if not self . channels : self . channels = self . call api ( 'channels.list' ) [ 'channels' ] return self . channels", "predictions": ["return the channel channel ."], "references": ["list of channels of this slack team"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 3676, "code": "def users ( self ) : if not self . users : self . users = self . call api ( 'users.list' ) [ 'members' ] return self . users", "predictions": ["list of users ."], "references": ["list of users of this slack team"], "bleu": 0.3158905525406873, "rouge_l": 0.5198863636363635}
{"id": 3677, "code": "def channel from name ( self , name ) : try : channel = [ channel for channel in self . channels if channel [ 'name' ] == name ] [ 0 ] except Index Error : raise Value Error ( 'Unknown channel for name: \"{}\"' . format ( name ) ) return channel", "predictions": ["return a channel from its name ."], "references": ["return the channel dict given by human - readable { name }"], "bleu": 0.1081377510275021, "rouge_l": 0.30148270181219106}
{"id": 3678, "code": "def translate ( self , message ) : try : user id = message . pop ( 'user' ) user = self . slack . user from id ( user id ) message [ u'user' ] = user [ 'name' ] except ( Key Error , Index Error , Value Error ) : pass try : if type ( message [ 'channel' ] ) == str : channel id = message . pop ( 'channel' ) else : channel id = message . pop ( 'channel' ) [ 'id' ] self . slack . reload channels ( ) channel = self . slack . channel from id ( channel id ) message [ u'channel' ] = channel [ 'name' ] except ( Key Error , Index Error , Value Error ) : pass return message", "predictions": ["translate a user to a channel ."], "references": ["translate machine identifiers into human - readable"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 3679, "code": "def send Slack ( self , message ) : channel = message . get ( 'channel' , 'general' ) self . send Message ( self . make message ( message [ 'text' ] , channel ) )", "predictions": ["send a message to the websocket"], "references": ["send message to slack"], "bleu": 0.31239399369202553, "rouge_l": 0.6224489795918368}
{"id": 3680, "code": "def read channel ( self ) : channel , message = self . protocol . channel layer . receive many ( [ u'slack.send' ] , block = False ) delay = 0.1 if channel : self . protocols [ 0 ] . send Slack ( message ) reactor . call Later ( delay , self . read channel )", "predictions": ["alert a channel from the server scrolled"], "references": ["get available messages and send through to the protocol"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 3681, "code": "def run ( self , args ) : args = self . parser . parse args ( args ) if not args . token : raise Value Error ( 'Supply the slack token through --token or setting DJANGOBOT TOKEN' ) sys . path . insert ( 0 , \".\" ) module path , object path = args . channel layer . split ( ':' , 1 ) channel layer = importlib . import module ( module path ) for part in object path . split ( '.' ) : channel layer = getattr ( channel layer , part ) Client ( channel layer = channel layer , token = args . token , ) . run ( )", "predictions": ["import . py multiline"], "references": ["pass in raw arguments instantiate slack api and begin client ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 3682, "code": "def dict diff ( prv , nxt ) : keys = set ( prv . keys ( ) + nxt . keys ( ) ) result = { } for k in keys : if prv . get ( k ) != nxt . get ( k ) : result [ k ] = ( prv . get ( k ) , nxt . get ( k ) ) return result", "predictions": ["return the font of the dictionary of the keys"], "references": ["return a dict of keys that differ with another config object ."], "bleu": 0.12026590852507517, "rouge_l": 0.2785388127853881}
{"id": 3683, "code": "def colorize ( msg , color ) : if DONT COLORIZE : return msg else : return \"{}{}{}\" . format ( COLORS [ color ] , msg , COLORS [ \"endc\" ] )", "predictions": ["select the message with the given = = 0 dialogs dialogs dialogs dialogs dialogs dialogs"], "references": ["given a string add necessary codes to format the string ."], "bleu": 0.09103526405546068, "rouge_l": 0.07911802853437094}
{"id": 3684, "code": "def v2 playbook on task start ( self , task , * * kwargs ) : self . last task name = task . get name ( ) self . printed last task = False", "predictions": ["called when the message is started"], "references": ["run when a task starts ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 3685, "code": "def v2 runner on ok ( self , result , * * kwargs ) : failed = \"failed\" in result . result unreachable = \"unreachable\" in result . result if ( \"print action\" in result . task . tags or failed or unreachable or self . display . verbosity > 1 ) : self . print task ( ) self . last skipped = False msg = unicode ( result . result . get ( \"msg\" , \"\" ) ) or unicode ( result . result . get ( \"reason\" , \"\" ) ) or unicode ( result . result . get ( \"message\" , \"\" ) ) stderr = [ result . result . get ( \"exception\" , None ) , result . result . get ( \"module stderr\" , None ) , ] stderr = \"\\n\" . join ( [ e for e in stderr if e ] ) . strip ( ) self . print host or item ( result . host , result . result . get ( \"changed\" , False ) , msg , result . result . get ( \"diff\" , None ) , is host = True , error = failed or unreachable , stdout = result . result . get ( \"module stdout\" , None ) , stderr = stderr . strip ( ) , ) if \"results\" in result . result : for r in result . result [ \"results\" ] : failed = \"failed\" in r stderr = [ r . get ( \"exception\" , None ) , r . get ( \"module stderr\" , None ) ] stderr = \"\\n\" . join ( [ e for e in stderr if e ] ) . strip ( ) self . print host or item ( r [ \"item\" ] , r . get ( \"changed\" , False ) , unicode ( r . get ( \"msg\" , \"\" ) ) , r . get ( \"diff\" , None ) , is host = False , error = failed , stdout = r . get ( \"module stdout\" , None ) , stderr = stderr . strip ( ) , ) else : self . last skipped = True print ( \".\" , end = \"\" )", "predictions": ["the main function of the find runner"], "references": ["run when a task finishes correctly ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3686, "code": "def v2 playbook on stats ( self , stats ) : print ( ) self . printed last task = False self . print task ( \"STATS\" ) hosts = sorted ( stats . processed . keys ( ) ) for host in hosts : s = stats . summarize ( host ) if s [ \"failures\" ] or s [ \"unreachable\" ] : color = \"failed\" elif s [ \"changed\" ] : color = \"changed\" else : color = \"ok\" msg = \"{}    : ok={}\\tchanged={}\\tfailed={}\\tunreachable={}\" . format ( host , s [ \"ok\" ] , s [ \"changed\" ] , s [ \"failures\" ] , s [ \"unreachable\" ] ) print ( colorize ( msg , color ) )", "predictions": ["show playbook self self if enabled if not found"], "references": ["display info about playbook statistics ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 3687, "code": "def v2 runner on skipped ( self , result , * * kwargs ) : if self . display . verbosity > 1 : self . print task ( ) self . last skipped = False line length = 120 spaces = \" \" * ( 31 - len ( result . host . name ) - 4 ) line = \"  * {}{}- {}\" . format ( colorize ( result . host . name , \"not so bold\" ) , spaces , colorize ( \"skipped\" , \"skipped\" ) , ) reason = result . result . get ( \"skipped reason\" , \"\" ) or result . result . get ( \"skip reason\" , \"\" ) if len ( reason ) < 50 : line += \" -- {}\" . format ( reason ) print ( \"{} {}---------\" . format ( line , \"-\" * ( line length - len ( line ) ) ) ) else : print ( \"{} {}\" . format ( line , \"-\" * ( line length - len ( line ) ) ) ) print ( self . indent text ( reason , 8 ) ) print ( reason )", "predictions": ["print set the result of the result"], "references": ["run when a task is skipped ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3688, "code": "def load filters ( ) : all filters = { } for m in JINJA FILTERS : if hasattr ( m , \"filters\" ) : all filters . update ( m . filters ( ) ) return all filters", "predictions": ["set all icon icon"], "references": ["loads and returns all filters ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 3689, "code": "def get authorization ( self ) : auth = self . authorization class ( ) header = self . get authorization header ( ) if not header or not header . split : return auth header = header . split ( ) if len ( header ) > 1 and header [ 0 ] == 'Bearer' : auth . is oauth = True access token = header [ 1 ] self . validate access token ( access token , auth ) if not auth . is valid : auth . error = 'access denied' return auth", "predictions": ["gets the authorization object from the request . . . ."], "references": ["get authorization object representing status of authentication ."], "bleu": 0.16108992769687397, "rouge_l": 0.32504440497335696}
{"id": 3690, "code": "def open ( self , bus ) : if self . device is not None : self . close ( ) self . device = open ( '/dev/i2c-{0}' . format ( bus ) , 'r+b' , buffering = 0 )", "predictions": ["resize the for the evt for reading . . . . . ."], "references": ["open the smbus interface on the specified bus ."], "bleu": 0.1135935489027116, "rouge_l": 0.2819722650231125}
{"id": 3691, "code": "def read byte ( self , addr ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' self . select device ( addr ) return ord ( self . device . read ( 1 ) )", "predictions": ["parse an made from the read read or return ."], "references": ["read a single byte from the specified device ."], "bleu": 0.18850319022747347, "rouge_l": 0.31881533101045295}
{"id": 3692, "code": "def read bytes ( self , addr , number ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' self . select device ( addr ) return self . device . read ( number )", "predictions": ["save bytes bytes bytes ."], "references": ["read many bytes from the specified device ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 3693, "code": "def read byte data ( self , addr , cmd ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' reg = c uint8 ( cmd ) result = c uint8 ( ) request = make i2c rdwr data ( [ ( addr , 0 , 1 , pointer ( reg ) ) , ( addr , I2C M RD , 1 , pointer ( result ) ) ] ) ioctl ( self . device . fileno ( ) , I2C RDWR , request ) return result . value", "predictions": ["build a window data from the device . ."], "references": ["read a single byte from the specified cmd register of the device ."], "bleu": 0.20030090221863772, "rouge_l": 0.4401154401154401}
{"id": 3694, "code": "def write quick ( self , addr ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' request = make i2c rdwr data ( [ ( addr , 0 , 0 , None ) , ] ) ioctl ( self . device . fileno ( ) , I2C RDWR , request )", "predictions": ["build the component == == the device . . . . . . . . ."], "references": ["write a single byte to the specified device ."], "bleu": 0.10878661088699644, "rouge_l": 0.2527624309392265}
{"id": 3695, "code": "def write byte ( self , addr , val ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' self . select device ( addr ) data = bytearray ( 1 ) data [ 0 ] = val & 0x FF self . device . write ( data )", "predictions": ["convert a byte to the device . . . . . . . . ."], "references": ["write a single byte to the specified device ."], "bleu": 0.1892240568795935, "rouge_l": 0.5236051502145923}
{"id": 3696, "code": "def write bytes ( self , addr , buf ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' self . select device ( addr ) self . device . write ( buf )", "predictions": ["set the device device to the device . . . . . . . ."], "references": ["write many bytes to the specified device . buf is a bytearray"], "bleu": 0.1361294711534851, "rouge_l": 0.30235439900867406}
{"id": 3697, "code": "def write byte data ( self , addr , cmd , val ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' data = bytearray ( 2 ) data [ 0 ] = cmd & 0x FF data [ 1 ] = val & 0x FF self . select device ( addr ) self . device . write ( data )", "predictions": ["load a object in the redis language root ."], "references": ["write a byte of data to the specified cmd register of the device ."], "bleu": 0.09630141125179911, "rouge_l": 0.2510288065843621}
{"id": 3698, "code": "def write i2c block data ( self , addr , cmd , vals ) : assert self . device is not None , 'Bus must be opened before operations are made against it!' data = bytearray ( len ( vals ) + 1 ) data [ 0 ] = cmd & 0x FF data [ 1 : ] = vals [ 0 : ] self . select device ( addr ) self . device . write ( data )", "predictions": ["inspect an initial self . data data . ."], "references": ["write a buffer of data to the specified cmd register of the device ."], "bleu": 0.08961856124931385, "rouge_l": 0.1673525377229081}
{"id": 3699, "code": "def datetime created ( self ) : if self . info ( ) . get ( 'datetime created' ) : return dateutil . parser . parse ( self . info ( ) [ 'datetime created' ] )", "predictions": ["return s activate item prop prop prop prop prop prop prop prop prop prop prop prop prop prop prop prop prop prop prop prop prop prop prop prop"], "references": ["returns file group s create aware * datetime * in utc format ."], "bleu": 0.04327969719414172, "rouge_l": 0.05222602739726027}
{"id": 3700, "code": "def construct from ( cls , group info ) : group = cls ( group info [ 'id' ] ) group . info cache = group info return group", "predictions": ["update a * * * * * * * * * * * * * * * * . * . * . * . * . * . * ."], "references": ["constructs filegroup instance from group information ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 3701, "code": "def base opration ( self , method ) : uuids = self . uuids ( ) while True : chunk = list ( islice ( uuids , 0 , self . chunk size ) ) if not chunk : return rest request ( method , self . storage url , chunk )", "predictions": ["obj show the show - context from the supplied method . . . . ."], "references": ["base method for storage operations ."], "bleu": 0.09103526405546068, "rouge_l": 0.2064297800338409}
{"id": 3702, "code": "def uuids ( self ) : for f in self . seq : if isinstance ( f , File ) : yield f . uuid elif isinstance ( f , six . string types ) : yield f else : raise Value Error ( 'Invalid type for sequence item: {0}' . format ( type ( f ) ) )", "predictions": ["generator for all select select select strings"], "references": ["extract uuid from each item of specified seq ."], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 3703, "code": "def list ( api list class , arg namespace , * * extra ) : if arg namespace . starting point : ordering field = ( arg namespace . ordering or '' ) . lstrip ( '-' ) if ordering field in ( '' , 'datetime uploaded' , 'datetime created' ) : arg namespace . starting point = parser . parse ( arg namespace . starting point ) items = api list class ( starting point = arg namespace . starting point , ordering = arg namespace . ordering , limit = arg namespace . limit , request limit = arg namespace . request limit , * * extra ) items . constructor = lambda x : x try : pprint ( list ( items ) ) except Value Error as e : print ( e )", "predictions": ["raise a raise exception on the given error errors errors errors errors errors errors errors errors errors errors errors errors errors errors errors errors errors errors errors errors errors errors in"], "references": ["a common function for building methods of the list showing ."], "bleu": 0.04317900023606586, "rouge_l": 0.10418445772843724}
{"id": 3704, "code": "def bar ( iter content , parts , title = '' ) : parts = max ( float ( parts ) , 1.0 ) cells = 10 progress = 0 step = cells / parts draw = lambda progress : sys . stdout . write ( '\\r[{0:10}] {1:.2f}% {2}' . format ( '#' * int ( progress ) , progress * cells , title ) ) for chunk in iter content : yield chunk progress += step draw ( progress ) sys . stdout . flush ( ) draw ( cells ) print ( '' )", "predictions": ["generate a traceback of the file full traceback full data full"], "references": ["iterates over the iter_content and draws a progress bar to stdout ."], "bleu": 0.11510518494396255, "rouge_l": 0.08628005657708629}
{"id": 3705, "code": "def home mode set state ( self , state , * * kwargs ) : if state not in ( HOME MODE ON , HOME MODE OFF ) : raise Value Error ( 'Invalid home mode state' ) api = self . api info [ 'home mode' ] payload = dict ( { 'api' : api [ 'name' ] , 'method' : 'Switch' , 'version' : api [ 'version' ] , 'on' : state , ' sid' : self . sid , } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) if response [ 'success' ] : return True return False", "predictions": ["source state state and retry"], "references": ["set the state of home mode"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 3706, "code": "def home mode status ( self , * * kwargs ) : api = self . api info [ 'home mode' ] payload = dict ( { 'api' : api [ 'name' ] , 'method' : 'Get Info' , 'version' : api [ 'version' ] , ' sid' : self . sid } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) return response [ 'data' ] [ 'on' ]", "predictions": ["logger the mode self ."], "references": ["returns the status of home mode"], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 3707, "code": "def camera list ( self , * * kwargs ) : api = self . api info [ 'camera' ] payload = dict ( { ' sid' : self . sid , 'api' : api [ 'name' ] , 'method' : 'List' , 'version' : api [ 'version' ] , } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) cameras = [ ] for data in response [ 'data' ] [ 'cameras' ] : cameras . append ( Camera ( data , self . video stream url ) ) return cameras", "predictions": ["return list of available cameras"], "references": ["return a list of cameras ."], "bleu": 0.3342454302942773, "rouge_l": 0.7155425219941348}
{"id": 3708, "code": "def camera info ( self , camera ids , * * kwargs ) : api = self . api info [ 'camera' ] payload = dict ( { ' sid' : self . sid , 'api' : api [ 'name' ] , 'method' : 'Get Info' , 'version' : api [ 'version' ] , 'camera Ids' : ', ' . join ( str ( id ) for id in camera ids ) , } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) cameras = [ ] for data in response [ 'data' ] [ 'cameras' ] : cameras . append ( Camera ( data , self . video stream url ) ) return cameras", "predictions": ["list users in a users stream"], "references": ["return a list of cameras matching camera_ids ."], "bleu": 0.17516432701748888, "rouge_l": 0.13926940639269406}
{"id": 3709, "code": "def camera snapshot ( self , camera id , * * kwargs ) : api = self . api info [ 'camera' ] payload = dict ( { ' sid' : self . sid , 'api' : api [ 'name' ] , 'method' : 'Get Snapshot' , 'version' : api [ 'version' ] , 'camera Id' : camera id , } , * * kwargs ) response = self . get ( api [ 'url' ] , payload ) return response . content", "predictions": ["send a channel to a channel"], "references": ["return bytes of camera image ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3710, "code": "def camera event motion enum ( self , camera id , * * kwargs ) : api = self . api info [ 'camera event' ] payload = dict ( { ' sid' : self . sid , 'api' : api [ 'name' ] , 'method' : 'Motion Enum' , 'version' : api [ 'version' ] , 'cam Id' : camera id , } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) return Motion Setting ( camera id , response [ 'data' ] [ 'MD Param' ] )", "predictions": ["translate translate event enum to translate a translate"], "references": ["return motion settings matching camera_id ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3711, "code": "def camera event md param save ( self , camera id , * * kwargs ) : api = self . api info [ 'camera event' ] payload = dict ( { ' sid' : self . sid , 'api' : api [ 'name' ] , 'method' : 'MD Param Save' , 'version' : api [ 'version' ] , 'cam Id' : camera id , } , * * kwargs ) response = self . get json with retry ( api [ 'url' ] , payload ) return response [ 'data' ] [ 'cam Id' ]", "predictions": ["send send send send send send send to send"], "references": ["update motion settings matching camera_id with keyword args ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3712, "code": "def update ( self ) : cameras = self . api . camera list ( ) self . cameras by id = { v . camera id : v for i , v in enumerate ( cameras ) } motion settings = [ ] for camera id in self . cameras by id . keys ( ) : motion setting = self . api . camera event motion enum ( camera id ) motion settings . append ( motion setting ) self . motion settings by id = { v . camera id : v for i , v in enumerate ( motion settings ) }", "predictions": ["update camera cameras list"], "references": ["update cameras and motion settings with latest from api ."], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 3713, "code": "def set home mode ( self , state ) : state parameter = HOME MODE OFF if state : state parameter = HOME MODE ON return self . api . home mode set state ( state parameter )", "predictions": ["set the home mode to the given state ."], "references": ["set the state of home mode"], "bleu": 0.24446151121745052, "rouge_l": 0.5532879818594103}
{"id": 3714, "code": "def is last li ( li , meta data , current num Id ) : if not is li ( li , meta data ) : return False w namespace = get namespace ( li , 'w' ) next el = li while True : if next el is None : return True next el = next el . getnext ( ) if not is li ( next el , meta data ) : continue new num Id = get num Id ( next el , w namespace ) if current num Id != new num Id : return True return False", "predictions": ["return true if the data is the last li li ."], "references": ["determine if li is the last list item for a given list"], "bleu": 0.21423488883339475, "rouge_l": 0.34512022630834516}
{"id": 3715, "code": "def get single list nodes data ( li , meta data ) : yield li w namespace = get namespace ( li , 'w' ) current num Id = get num Id ( li , w namespace ) starting ilvl = get ilvl ( li , w namespace ) el = li while True : el = el . getnext ( ) if el is None : break if not has text ( el ) : continue if is top level upper roman ( el , meta data ) : break if ( is li ( el , meta data ) and ( starting ilvl > get ilvl ( el , w namespace ) ) ) : break new num Id = get num Id ( el , w namespace ) if new num Id is None or new num Id == - 1 : yield el continue if current num Id != new num Id : break if is last li ( el , meta data , current num Id ) : yield el break yield el", "predictions": ["return a generator of all nodes in the single list ."], "references": ["find consecutive li tags that have content that have the same list id ."], "bleu": 0.10312570678516415, "rouge_l": 0.2349165596919127}
{"id": 3716, "code": "def is bold ( r ) : w namespace = get namespace ( r , 'w' ) rpr = r . find ( '%sr Pr' % w namespace ) if rpr is None : return False bold = rpr . find ( '%sb' % w namespace ) return style is false ( bold )", "predictions": ["return true if the namespace is an bold bold"], "references": ["the function will return true if the r tag passed in is considered bold ."], "bleu": 0.2193764638240152, "rouge_l": 0.47843137254901963}
{"id": 3717, "code": "def build tr ( tr , meta data , row spans ) : tr el = etree . Element ( 'tr' ) w namespace = get namespace ( tr , 'w' ) visited nodes = [ ] for el in tr : if el in visited nodes : continue visited nodes . append ( el ) if el . tag == '%stc' % w namespace : v merge = get v merge ( el ) if ( v merge is not None and v merge . get ( '%sval' % w namespace ) != 'restart' ) : continue texts = [ ] for td content in el : if td content in visited nodes : continue if is li ( td content , meta data ) : li nodes = get single list nodes data ( td content , meta data , ) list el , list visited nodes = build list ( li nodes , meta data , ) visited nodes . extend ( list visited nodes ) texts . append ( etree . tostring ( list el ) ) elif td content . tag == '%stbl' % w namespace : table el , table visited nodes = build table ( td content , meta data , ) visited nodes . extend ( table visited nodes ) texts . append ( etree . tostring ( table el ) ) elif td content . tag == '%stc Pr' % w namespace : visited nodes . append ( td content ) continue else : text = get element content ( td content , meta data , is td = True , ) texts . append ( text ) data = '<br />' . join ( t for t in texts if t is not None ) td el = etree . XML ( '<td>%s</td>' % data ) colspan = get grid span ( el ) if colspan > 1 : td el . set ( 'colspan' , '%d' % colspan ) v merge = get v merge ( el ) if ( v merge is not None and v merge . get ( '%sval' % w namespace ) == 'restart' ) : rowspan = next ( row spans ) td el . set ( 'rowspan' , '%d' % rowspan ) tr el . append ( td el ) return tr el", "predictions": ["build the list of nodes from the data ."], "references": ["this will return a single tr element with all tds already populated ."], "bleu": 0.09049614828481034, "rouge_l": 0.08802308802308802}
{"id": 3718, "code": "def build table ( table , meta data ) : table el = etree . Element ( 'table' ) w namespace = get namespace ( table , 'w' ) row spans = get rowspan data ( table ) for el in table : if el . tag == '%str' % w namespace : tr el = build tr ( el , meta data , row spans , ) table el . append ( tr el ) visited nodes = list ( table . iter ( ) ) return table el , visited nodes", "predictions": ["build a table from a table ."], "references": ["this returns a table object with all rows and cells correctly populated ."], "bleu": 0.1114789227233716, "rouge_l": 0.2846034214618974}
{"id": 3719, "code": "def get t tag content ( t , parent , remove bold , remove italics , meta data ) : if t is None or t . text is None : return '' text = cgi . escape ( t . text ) el is bold = not remove bold and ( is bold ( parent ) or is underlined ( parent ) ) el is italics = not remove italics and is italics ( parent ) if el is bold : text = '<strong>%s</strong>' % text if el is italics : text = '<em>%s</em>' % text return text", "predictions": ["return the content of a t t ."], "references": ["generate the string data that for this particular t tag ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 3720, "code": "def strip tag ( tree , tag ) : for el in tree . iter ( ) : if el . tag == tag : el . getparent ( ) . remove ( el )", "predictions": ["strip the tag with the given tag ."], "references": ["remove all tags that have the tag name tag"], "bleu": 0.20014292374951972, "rouge_l": 0.34923664122137404}
{"id": 3721, "code": "def find ( dataset , url ) : fn = os . path . join ( DATASETS , dataset ) dn = os . path . dirname ( fn ) if not os . path . exists ( dn ) : print ( 'creating dataset directory: %s' , dn ) os . makedirs ( dn ) if not os . path . exists ( fn ) : if sys . version info < ( 3 , ) : urllib . urlretrieve ( url , fn ) else : urllib . request . urlretrieve ( url , fn ) return fn", "predictions": ["find a dataset in the given url ."], "references": ["find the location of a dataset on disk downloading if needed ."], "bleu": 0.15223083300988077, "rouge_l": 0.3860759493670886}
{"id": 3722, "code": "def load mnist ( flatten = True , labels = False ) : fn = find ( 'mnist.pkl.gz' , 'http://deeplearning.net/data/mnist/mnist.pkl.gz' ) h = gzip . open ( fn , 'rb' ) if sys . version info < ( 3 , ) : ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h ) else : ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h , encoding = 'bytes' ) h . close ( ) if not flatten : timg = timg . reshape ( ( - 1 , 28 , 28 , 1 ) ) vimg = vimg . reshape ( ( - 1 , 28 , 28 , 1 ) ) simg = simg . reshape ( ( - 1 , 28 , 28 , 1 ) ) if labels : return ( ( timg , tlab . astype ( 'i' ) ) , ( vimg , vlab . astype ( 'i' ) ) , ( simg , slab . astype ( 'i' ) ) ) return ( timg , ) , ( vimg , ) , ( simg , )", "predictions": ["load mnist from a file"], "references": ["load the mnist digits dataset ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 3723, "code": "def load cifar ( flatten = True , labels = False ) : def extract ( name ) : print ( 'extracting data from {}' . format ( name ) ) h = tar . extractfile ( name ) if sys . version info < ( 3 , ) : d = pickle . load ( h ) else : d = pickle . load ( h , encoding = 'bytes' ) for k in list ( d ) : d [ k . decode ( 'utf8' ) ] = d [ k ] h . close ( ) img = d [ 'data' ] . reshape ( ( - 1 , 3 , 32 , 32 ) ) . transpose ( ( 0 , 2 , 3 , 1 ) ) . astype ( 'f' ) / 128 - 1 if flatten : img = img . reshape ( ( - 1 , 32 * 32 * 3 ) ) d [ 'data' ] = img return d fn = find ( 'cifar10.tar.gz' , 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz' ) tar = tarfile . open ( fn ) imgs = [ ] labs = [ ] for i in range ( 1 , 6 ) : d = extract ( 'cifar-10-batches-py/data batch {}' . format ( i ) ) imgs . extend ( d [ 'data' ] ) labs . extend ( d [ 'labels' ] ) timg = np . asarray ( imgs [ : 40000 ] ) tlab = np . asarray ( labs [ : 40000 ] , 'i' ) vimg = np . asarray ( imgs [ 40000 : ] ) vlab = np . asarray ( labs [ 40000 : ] , 'i' ) d = extract ( 'cifar-10-batches-py/test batch' ) simg = d [ 'data' ] slab = d [ 'labels' ] tar . close ( ) if labels : return ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) return ( timg , ) , ( vimg , ) , ( simg , )", "predictions": ["load cifar data from a numpy array"], "references": ["load the cifar10 image dataset ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 3724, "code": "def plot layers ( weights , tied weights = False , channels = 1 ) : if hasattr ( weights [ 0 ] , 'get value' ) : weights = [ w . get value ( ) for w in weights ] k = min ( len ( weights ) , 9 ) imgs = np . eye ( weights [ 0 ] . shape [ 0 ] ) for i , weight in enumerate ( weights [ : - 1 ] ) : imgs = np . dot ( weight . T , imgs ) plot images ( imgs , 100 + 10 * k + i + 1 , channels = channels , title = 'Layer {}' . format ( i + 1 ) ) weight = weights [ - 1 ] n = weight . shape [ 1 ] / channels if int ( np . sqrt ( n ) ) ** 2 != n : return if tied weights : imgs = np . dot ( weight . T , imgs ) plot images ( imgs , 100 + 10 * k + k , channels = channels , title = 'Layer {}' . format ( k ) ) else : plot images ( weight , 100 + 10 * k + k , channels = channels , title = 'Decoding weights' )", "predictions": ["plot the layers of a single tied ."], "references": ["create a plot of weights visualized as bottom - level pixel arrays ."], "bleu": 0.10793517579160734, "rouge_l": 0.2739520958083832}
{"id": 3725, "code": "def plot filters ( filters ) : imgs = filters . get value ( ) N , channels , x , y = imgs . shape n = int ( np . sqrt ( N ) ) assert n * n == N , 'filters must contain a square number of rows!' assert channels == 1 or channels == 3 , 'can only plot grayscale or rgb filters!' img = np . zeros ( ( ( y + 1 ) * n - 1 , ( x + 1 ) * n - 1 , channels ) , dtype = imgs [ 0 ] . dtype ) for i , pix in enumerate ( imgs ) : r , c = divmod ( i , n ) img [ r * ( y + 1 ) : ( r + 1 ) * ( y + 1 ) - 1 , c * ( x + 1 ) : ( c + 1 ) * ( x + 1 ) - 1 ] = pix . transpose ( ( 1 , 2 , 0 ) ) img -= img . min ( ) img /= img . max ( ) ax = plt . gcf ( ) . add subplot ( 111 ) ax . xaxis . set visible ( False ) ax . yaxis . set visible ( False ) ax . set frame on ( False ) ax . imshow ( img . squeeze ( ) , cmap = plt . cm . gray )", "predictions": ["plot the filters of the filters ."], "references": ["create a plot of conv filters visualized as pixel arrays ."], "bleu": 0.1319006407505858, "rouge_l": 0.4273204903677758}
{"id": 3726, "code": "def batches ( dataset ) : seq lengths = dataset . variables [ 'seq Lengths' ] . data seq begins = np . concatenate ( ( [ 0 ] , np . cumsum ( seq lengths ) [ : - 1 ] ) ) def sample ( ) : chosen = np . random . choice ( list ( range ( len ( seq lengths ) ) ) , BATCH SIZE , replace = False ) return batch at ( dataset . variables [ 'inputs' ] . data , dataset . variables [ 'target Classes' ] . data , seq begins [ chosen ] , seq lengths [ chosen ] ) return sample", "predictions": ["generate batches of batches of batches ."], "references": ["returns a callable that chooses sequences from netcdf data ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 3727, "code": "def variables ( self ) : result = [ self . target ] if self . weights is not None : result . append ( self . weights ) return result", "predictions": ["return the list of variables ."], "references": ["a list of theano variables used in this loss ."], "bleu": 0.16959011078459055, "rouge_l": 0.47843137254901963}
{"id": 3728, "code": "def reservoir ( xs , n , rng ) : pool = [ ] for i , x in enumerate ( xs ) : if len ( pool ) < n : pool . append ( x / np . linalg . norm ( x ) ) continue j = rng . randint ( i + 1 ) if j < n : pool [ j ] = x / np . linalg . norm ( x ) L = len ( pool ) S = np . std ( pool , axis = 0 ) while len ( pool ) < n : x = pool [ rng . randint ( L ) ] pool . append ( x + S * rng . randn ( * x . shape ) ) return np . array ( pool , dtype = pool [ 0 ] . dtype )", "predictions": ["return the reservoir of a list of xs"], "references": ["select a random sample of n items from xs ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 3729, "code": "def inputs ( self ) : return [ l . input for l in self . layers if isinstance ( l , layers . Input ) ]", "predictions": ["return a list of all inputs in the list of all layers ."], "references": ["a list of theano variables for feedforward computations ."], "bleu": 0.18798317647335086, "rouge_l": 0.37596302003081655}
{"id": 3730, "code": "def variables ( self ) : result = self . inputs seen = set ( i . name for i in result ) for loss in self . losses : for v in loss . variables : if v . name not in seen : result . append ( v ) seen . add ( v . name ) return result", "predictions": ["return the list of all variables in this model ."], "references": ["a list of theano variables for loss computations ."], "bleu": 0.18850319022747347, "rouge_l": 0.42508710801393734}
{"id": 3731, "code": "def output size ( self ) : shape = self . output shape if shape is None : raise util . Configuration Error ( 'undefined output size for layer \"{}\"' . format ( self . name ) ) return shape [ - 1 ]", "predictions": ["the output size of this layer ."], "references": ["number of neurons in this layer s default output ."], "bleu": 0.18938334565508194, "rouge_l": 0.45607476635514016}
{"id": 3732, "code": "def resolve outputs ( self ) : input shape = None for i , shape in enumerate ( self . input shapes . values ( ) ) : if i == 0 : input shape = shape if len ( input shape ) != len ( shape ) or any ( a is not None and b is not None and a != b for a , b in zip ( input shape [ : - 1 ] , shape [ : - 1 ] ) ) : raise util . Configuration Error ( 'layer \"{}\" incompatible input shapes {}' . format ( self . name , self . input shapes ) ) size = self . kwargs . get ( 'size' ) shape = self . kwargs . get ( 'shape' ) if shape is not None : pass elif size is not None : shape = tuple ( input shape [ : - 1 ] ) + ( size , ) else : raise util . Configuration Error ( 'layer \"{}\" does not specify a size' . format ( self . name ) ) self . output shapes [ 'out' ] = shape", "predictions": ["resolves the outputs for the input shapes ."], "references": ["resolve the names of outputs for this layer into shape tuples ."], "bleu": 0.14544785215055717, "rouge_l": 0.3860759493670886}
{"id": 3733, "code": "def log ( self ) : inputs = ', ' . join ( '\"{0}\" {1}' . format ( * ns ) for ns in self . input shapes . items ( ) ) util . log ( 'layer {0. class . name } \"{0.name}\" {0.output shape} {1} from {2}' , self , getattr ( self . activate , 'name' , self . activate ) , inputs ) util . log ( 'learnable parameters: {}' , self . log params ( ) )", "predictions": ["logs the contents of the input shapes ."], "references": ["log some information about this layer ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 3734, "code": "def log params ( self ) : total = 0 for p in self . params : shape = p . get value ( ) . shape util . log ( 'parameter \"{}\" {}' , p . name , shape ) total += np . prod ( shape ) return total", "predictions": ["log parameters for this parameter ."], "references": ["log information about this layer s parameters ."], "bleu": 0.19902510067151713, "rouge_l": 0.4178082191780822}
{"id": 3735, "code": "def fmt ( self , string ) : if '{' not in string : string = '{}.' + string return string . format ( self . name )", "predictions": ["return the string representation of the string ."], "references": ["helper method to format our name into a string ."], "bleu": 0.1643685581109115, "rouge_l": 0.21785714285714283}
{"id": 3736, "code": "def get all intervals ( self ) : ints = sorted ( self . get intervals ( True ) ) if self . tier type == 'Interval Tier' : if not ints : ints . append ( ( self . xmin , self . xmax , '' ) ) else : if ints [ 0 ] [ 0 ] > self . xmin : ints . insert ( 0 , ( self . xmin , ints [ 0 ] [ 0 ] , '' ) ) if ints [ - 1 ] [ 1 ] < self . xmax : ints . append ( ( ints [ - 1 ] [ 1 ] , self . xmax , '' ) ) p = ints [ - 1 ] for index , i in reversed ( list ( enumerate ( ints [ : - 1 ] , 1 ) ) ) : if p [ 0 ] - i [ 1 ] != 0 : ints . insert ( index , ( i [ 1 ] , p [ 0 ] , '' ) ) p = i return ints", "predictions": ["get all intervals from the tier ."], "references": ["returns the true list of intervals including the empty intervals ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 3737, "code": "def main ( ) : import optparse import sys import codecs import locale import six from . algorithm import get display parser = optparse . Option Parser ( ) parser . add option ( '-e' , '--encoding' , dest = 'encoding' , default = 'utf-8' , type = 'string' , help = 'Text encoding (default: utf-8)' ) parser . add option ( '-u' , '--upper-is-rtl' , dest = 'upper is rtl' , default = False , action = 'store true' , help = \"Treat upper case chars as strong 'R' \" 'for debugging (default: False).' ) parser . add option ( '-d' , '--debug' , dest = 'debug' , default = False , action = 'store true' , help = \"Output to stderr steps taken with the algorithm\" ) parser . add option ( '-b' , '--base-dir' , dest = 'base dir' , default = None , type = 'string' , help = \"Override base direction [L|R]\" ) options , rest = parser . parse args ( ) if options . base dir and options . base dir not in 'LR' : parser . error ( 'option -b can be L or R' ) if six . PY2 : sys . stdout = codecs . getwriter ( locale . getpreferredencoding ( ) ) ( sys . stdout ) if rest : lines = rest else : lines = sys . stdin for line in lines : display = get display ( line , options . encoding , options . upper is rtl , options . base dir , options . debug ) if not isinstance ( display , six . text type ) : display = display . decode ( options . encoding ) six . print ( display , end = '' )", "predictions": ["main function for the script"], "references": ["will be used to create the console script"], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 3738, "code": "def debug storage ( storage , base info = False , chars = True , runs = False ) : import codecs import locale import sys if six . PY2 : stderr = codecs . getwriter ( locale . getpreferredencoding ( ) ) ( sys . stderr ) else : stderr = sys . stderr caller = inspect . stack ( ) [ 1 ] [ 3 ] stderr . write ( 'in %s\\n' % caller ) if base info : stderr . write ( u'  base level  : %d\\n' % storage [ 'base level' ] ) stderr . write ( u'  base dir    : %s\\n' % storage [ 'base dir' ] ) if runs : stderr . write ( u'  runs        : %s\\n' % list ( storage [ 'runs' ] ) ) if chars : output = u'  Chars       : ' for ch in storage [ 'chars' ] : if ch != '\\n' : output += ch [ 'ch' ] else : output += 'C' stderr . write ( output + u'\\n' ) output = u'  Res. levels : %s\\n' % u'' . join ( [ six . text type ( ch [ 'level' ] ) for ch in storage [ 'chars' ] ] ) stderr . write ( output ) types = [ ch [ 'type' ] . ljust ( 3 ) for ch in storage [ 'chars' ] ] for i in range ( 3 ) : if i : output = u'                %s\\n' else : output = u'  Res. types  : %s\\n' stderr . write ( output % u'' . join ( [ t [ i ] for t in types ] ) )", "predictions": ["print the storage information to stdout ."], "references": ["display debug information for the storage"], "bleu": 0.2626909894424158, "rouge_l": 0.31202046035805625}
{"id": 3739, "code": "def reorder resolved levels ( storage , debug ) : should reset = True chars = storage [ 'chars' ] for ch in chars [ : : - 1 ] : if ch [ 'orig' ] in ( 'B' , 'S' ) : ch [ 'level' ] = storage [ 'base level' ] should reset = True elif should reset and ch [ 'orig' ] in ( 'BN' , 'WS' ) : ch [ 'level' ] = storage [ 'base level' ] else : should reset = False max len = len ( chars ) line start = line end = 0 highest level = 0 lowest odd level = EXPLICIT LEVEL LIMIT for idx in range ( max len ) : ch = chars [ idx ] char level = ch [ 'level' ] if char level > highest level : highest level = char level if char level % 2 and char level < lowest odd level : lowest odd level = char level if ch [ 'orig' ] == 'B' or idx == max len - 1 : line end = idx if ch [ 'orig' ] == 'B' : line end -= 1 reverse contiguous sequence ( chars , line start , line end , highest level , lowest odd level ) line start = idx + 1 highest level = 0 lowest odd level = EXPLICIT LEVEL LIMIT if debug : debug storage ( storage )", "predictions": ["reorder the storage levels in the storage storage ."], "references": ["l1 and l2 rules"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3740, "code": "def process ( self , context ) : import os from maya import cmds current file = cmds . file ( scene Name = True , query = True ) normalised = os . path . normpath ( current file ) context . set data ( 'current File' , value = normalised ) context . set data ( 'current file' , value = normalised )", "predictions": ["process the current scene ."], "references": ["inject the current working file"], "bleu": 0.35930411196308426, "rouge_l": 0.4}
{"id": 3741, "code": "def add ( object , name , value ) : self . added . append ( name ) setattr ( object , name , value )", "predictions": ["add an object to the added model ."], "references": ["append to self accessible via qt . qtcompat"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 3742, "code": "def cli ( args ) : import argparse parser = argparse . Argument Parser ( ) parser . add argument ( \"--convert\" , help = \"Path to compiled Python module, e.g. my ui.py\" ) parser . add argument ( \"--compile\" , help = \"Accept raw .ui file and compile with native \" \"Py Side2 compiler.\" ) parser . add argument ( \"--stdout\" , help = \"Write to stdout instead of file\" , action = \"store true\" ) parser . add argument ( \"--stdin\" , help = \"Read from stdin instead of file\" , action = \"store true\" ) args = parser . parse args ( args ) if args . stdout : raise Not Implemented Error ( \"--stdout\" ) if args . stdin : raise Not Implemented Error ( \"--stdin\" ) if args . compile : raise Not Implemented Error ( \"--compile\" ) if args . convert : sys . stdout . write ( \"#\\n\" \"#\\n\" ) # # with open ( args . convert ) as f : lines = convert ( f . readlines ( ) ) backup = \"%s backup%s\" % os . path . splitext ( args . convert ) sys . stdout . write ( \"Creating \\\"%s\\\"..\\n\" % backup ) shutil . copy ( args . convert , backup ) # # with open ( args . convert , \"w\" ) as f : f . write ( \"\" . join ( lines ) ) sys . stdout . write ( \"Successfully converted \\\"%s\\\"\\n\" % args . convert )", "predictions": ["entry point for the application script"], "references": ["qt . py command - line interface"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 3743, "code": "def discover gui ( ) : guis = reversed ( pyblish . api . registered guis ( ) ) for gui in guis : try : gui = import ( gui ) . show except ( Import Error , Attribute Error ) : continue else : return gui", "predictions": ["discover the gui available guis"], "references": ["return the most desirable of the currently registered guis"], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 3744, "code": "def get single axis values ( self , axis , dataset ) : data index = getattr ( self , '%s data index' % axis ) return [ p [ data index ] for p in dataset [ 'data' ] ]", "predictions": ["return the list of self values values"], "references": ["return all the values for a single axis of the data ."], "bleu": 0.11434175042957104, "rouge_l": 0.30148270181219106}
{"id": 3745, "code": "def draw constant line ( self , value label style ) : value , label , style = value label style start = self . transform output coordinates ( ( 0 , value ) ) [ 1 ] stop = self . graph width path = etree . Sub Element ( self . graph , 'path' , { 'd' : 'M 0 %(start)s h%(stop)s' % locals ( ) , 'class' : 'constant Line' } ) if style : path . set ( 'style' , style ) text = etree . Sub Element ( self . graph , 'text' , { 'x' : str ( 2 ) , 'y' : str ( start - 2 ) , 'class' : 'constant Line' } ) text . text = label", "predictions": ["set the home mode to the home mode ."], "references": ["draw a constant line on the y - axis with the label"], "bleu": 0.11192003885776355, "rouge_l": 0.1856925418569254}
{"id": 3746, "code": "def load transform parameters ( self ) : x min , x max , x div = self . x range ( ) y min , y max , y div = self . y range ( ) x step = ( float ( self . graph width ) - self . font size * 2 ) / ( x max - x min ) y step = ( float ( self . graph height ) - self . font size * 2 ) / ( y max - y min ) self . transform parameters = dict ( locals ( ) ) del self . transform parameters [ 'self' ]", "predictions": ["is the last last last last last last last last last last last last last last last last last last last last last last last last value"], "references": ["cache the parameters necessary to transform x & y coordinates"], "bleu": 0.044915755686574035, "rouge_l": 0.05893719806763285}
{"id": 3747, "code": "def add popup ( self , x , y , label ) : txt width = len ( label ) * self . font size * 0.6 + 10 tx = x + [ 5 , - 5 ] [ int ( x + txt width > self . width ) ] anchor = [ 'start' , 'end' ] [ x + txt width > self . width ] style = 'fill: #000; text-anchor: %s;' % anchor id = 'label-%s' % self . w3c name ( label ) attrs = { 'x' : str ( tx ) , 'y' : str ( y - self . font size ) , 'visibility' : 'hidden' , 'style' : style , 'text' : label , 'id' : id , } etree . Sub Element ( self . foreground , 'text' , attrs ) vis tmpl = ( \"document.get Element By Id('{id}').set Attribute('visibility', {val})\" ) attrs = { 'cx' : str ( x ) , 'cy' : str ( y ) , 'r' : str ( 10 ) , 'style' : 'opacity: 0;' , 'onmouseover' : vis tmpl . format ( val = 'visible' , id = id ) , 'onmouseout' : vis tmpl . format ( val = 'hidden' , id = id ) , } etree . Sub Element ( self . foreground , 'circle' , attrs )", "predictions": ["get the single single single single single single single single single single single single single single single single single single single single single single single single single single single single single"], "references": ["add pop - up information to a point on the graph ."], "bleu": 0.03901663112717908, "rouge_l": 0.05053852526926264}
{"id": 3748, "code": "def make datapoint text ( self , x , y , value , style = None ) : if not self . show data values : return e = etree . Sub Element ( self . foreground , 'text' , { 'x' : str ( x ) , 'y' : str ( y ) , 'class' : 'data Point Label' , 'style' : '%(style)s stroke: #fff; stroke-width: 2;' % vars ( ) , } ) e . text = str ( value ) e = etree . Sub Element ( self . foreground , 'text' , { 'x' : str ( x ) , 'y' : str ( y ) , 'class' : 'data Point Label' } ) e . text = str ( value ) if style : e . set ( 'style' , style )", "predictions": ["is the bold a bold"], "references": ["add text for a datapoint"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 3749, "code": "def draw x labels ( self ) : if self . show x labels : labels = self . get x labels ( ) count = len ( labels ) labels = enumerate ( iter ( labels ) ) start = int ( not self . step include first x label ) labels = itertools . islice ( labels , start , None , self . step x labels ) list ( map ( self . draw x label , labels ) ) self . draw x guidelines ( self . field width ( ) , count )", "predictions": ["build the tr labels labels"], "references": ["draw the x axis labels"], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 3750, "code": "def draw y labels ( self ) : if not self . show y labels : return labels = self . get y labels ( ) count = len ( labels ) labels = enumerate ( iter ( labels ) ) start = int ( not self . step include first y label ) labels = itertools . islice ( labels , start , None , self . step y labels ) list ( map ( self . draw y label , labels ) ) self . draw y guidelines ( self . field height ( ) , count )", "predictions": ["build the table labels labels labels"], "references": ["draw the y axis labels"], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 3751, "code": "def draw x guidelines ( self , label height , count ) : if not self . show x guidelines : return for count in range ( 1 , count ) : move = 'M {start} 0 v{stop}' . format ( start = label height * count , stop = self . graph height , ) path = { 'd' : move , 'class' : 'guide Lines' } etree . Sub Element ( self . graph , 'path' , path )", "predictions": ["get t tag for the given parent . . . . . . . ."], "references": ["draw the x - axis guidelines"], "bleu": 0.08225964699966554, "rouge_l": 0.10321489001692045}
{"id": 3752, "code": "def draw y guidelines ( self , label height , count ) : if not self . show y guidelines : return for count in range ( 1 , count ) : move = 'M 0 {start} h{stop}' . format ( start = self . graph height - label height * count , stop = self . graph width , ) path = { 'd' : move , 'class' : 'guide Lines' } etree . Sub Element ( self . graph , 'path' , path )", "predictions": ["strip tag guidelines to screen"], "references": ["draw the y - axis guidelines"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 3753, "code": "def draw titles ( self ) : if self . show graph title : self . draw graph title ( ) if self . show graph subtitle : self . draw graph subtitle ( ) if self . show x title : self . draw x title ( ) if self . show y title : self . draw y title ( )", "predictions": ["find titles to os"], "references": ["draws the graph title and subtitle"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 3754, "code": "def render inline styles ( self ) : if not self . css inline : return styles = self . parse css ( ) for node in self . root . xpath ( '//*[@class]' ) : cl = '.' + node . attrib [ 'class' ] if cl not in styles : continue style = styles [ cl ] if 'style' in node . attrib : style += node . attrib [ 'style' ] node . attrib [ 'style' ] = style", "predictions": ["renders the mnist styles styles"], "references": ["hard - code the styles into the svg xml if style sheets are not used ."], "bleu": 0.03347779366253814, "rouge_l": 0.17403708987161198}
{"id": 3755, "code": "def start svg ( self ) : SVG NAMESPACE = 'http://www.w3.org/2000/svg' SVG = '{%s}' % SVG NAMESPACE NSMAP = { None : SVG NAMESPACE , 'xlink' : 'http://www.w3.org/1999/xlink' , 'a3' : 'http://ns.adobe.com/Adobe SVG Viewer Extensions/3.0/' , } root attrs = self . get root attributes ( ) self . root = etree . Element ( SVG + \"svg\" , attrib = root attrs , nsmap = NSMAP ) if hasattr ( self , 'style sheet href' ) : pi = etree . Processing Instruction ( 'xml-stylesheet' , 'href=\"%s\" type=\"text/css\"' % self . style sheet href ) self . root . addprevious ( pi ) comment strings = ( ' Created with SVG.Graph ' , ' SVG.Graph by Jason R. Coombs ' , ' Based on SVG::Graph by Sean E. Russel ' , ' Based on Perl SVG:TT:Graph by Leo Lapworth & Stephan Morgan ' , ' ' + '/' * 66 , ) list ( map ( self . root . append , map ( etree . Comment , comment strings ) ) ) defs = etree . Sub Element ( self . root , 'defs' ) self . add defs ( defs ) if not hasattr ( self , 'style sheet href' ) and not self . css inline : self . root . append ( etree . Comment ( ' include default stylesheet if none specified ' ) ) style = etree . Sub Element ( defs , 'style' , type = 'text/css' ) style . text = self . get stylesheet ( ) . css Text self . root . append ( etree . Comment ( 'SVG Background' ) ) etree . Sub Element ( self . root , 'rect' , { 'width' : str ( self . width ) , 'height' : str ( self . height ) , 'x' : '0' , 'y' : '0' , 'class' : 'svg Background' } )", "predictions": ["load cifar - list cifar version of cifar version version version version version version version version ."], "references": ["base svg document creation"], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 3756, "code": "def get stylesheet resources ( self ) : class vars = class dict ( self ) loader = functools . partial ( self . load resource stylesheet , subs = class vars ) sheets = list ( map ( loader , self . stylesheet names ) ) return sheets", "predictions": ["returns a dictionary of all resources resources . . . . . ."], "references": ["get the stylesheets for this instance"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 3757, "code": "def send validation email ( self ) : if self . email verified : raise Value Error ( ( 'Cannot validate already active user.' ) ) site = Site . objects . get current ( ) self . validation notification ( user = self , site = site ) . notify ( )", "predictions": ["plot the filters of the filters get the filters get get the filters get the filters get the filters get get must be value"], "references": ["send a validation email to the user s email address ."], "bleu": 0.050661968099322066, "rouge_l": 0.061244979919678706}
{"id": 3758, "code": "def send password reset ( self ) : site = Site . objects . get current ( ) self . password reset notification ( user = self , site = site ) . notify ( )", "predictions": ["batches password password password"], "references": ["send a password reset to the user s email address ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 3759, "code": "def allow request ( self , request , view ) : if request . method != 'POST' : return True return super ( Post Request Throttle Mixin , self ) . allow request ( request , view )", "predictions": ["variables that the request should be logged target target is variables target target target target target target target target target target"], "references": ["throttle post requests only ."], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 3760, "code": "def client ( self ) : cls = self . class if cls . client is None : kwargs = { } if self . tls config : kwargs [ 'tls' ] = docker . tls . TLS Config ( * * self . tls config ) kwargs . update ( kwargs from env ( ) ) client = docker . API Client ( version = 'auto' , * * kwargs ) cls . client = client return cls . client", "predictions": ["build an instance of the . reservoir . . . . . . . . . ."], "references": ["single global client instance"], "bleu": 0.07223943354597204, "rouge_l": 0.10720562390158171}
{"id": 3761, "code": "def poll ( self ) : service = yield self . get service ( ) if not service : self . log . warn ( \"Docker service not found\" ) return 0 task filter = { 'service' : service [ 'Spec' ] [ 'Name' ] } tasks = yield self . docker ( 'tasks' , task filter ) running task = None for task in tasks : task state = task [ 'Status' ] [ 'State' ] self . log . debug ( \"Task %s of Docker service %s status: %s\" , task [ 'ID' ] [ : 7 ] , self . service id [ : 7 ] , pformat ( task state ) , ) if task state == 'running' : running task = task if running task is not None : return None else : return 1", "predictions": ["inputs for the docker return available tasks ."], "references": ["check for a task state like docker service ps id"], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 3762, "code": "def filter queryset ( self , value , queryset ) : return super ( Unique Email Validator , self ) . filter queryset ( value . lower ( ) , queryset , )", "predictions": ["variables queryset to a queryset for the given result for the given result for the given result for the given result for the given result for the given result for the"], "references": ["check lower - cased email is unique ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3763, "code": "def update ( self , instance , validated data ) : if not instance . check password ( validated data [ 'old password' ] ) : msg = ( 'Invalid password.' ) raise serializers . Validation Error ( { 'old password' : msg } ) instance . set password ( validated data [ 'new password' ] ) instance . save ( ) return instance", "predictions": ["output an instance s util"], "references": ["check the old password is valid and set the new password ."], "bleu": 0.0566124695559154, "rouge_l": 0.0}
{"id": 3764, "code": "def update ( self , instance , validated data ) : instance . set password ( validated data [ 'new password' ] ) instance . save ( ) return instance", "predictions": ["resolve an existing instance instance"], "references": ["set the new password for the user ."], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 3765, "code": "def delete ( self , request , * args , * * kwargs ) : auth = get authorization header ( request ) . split ( ) if not auth or auth [ 0 ] . lower ( ) != b'token' : return response . Response ( status = status . HTTP 400 BAD REQUEST ) if len ( auth ) == 1 : msg = 'Invalid token header. No credentials provided.' return response . Response ( msg , status = status . HTTP 400 BAD REQUEST ) elif len ( auth ) > 2 : msg = 'Invalid token header. Token string should not contain spaces.' return response . Response ( msg , status = status . HTTP 400 BAD REQUEST ) try : token = self . model . objects . get ( key = auth [ 1 ] ) except self . model . Does Not Exist : pass else : token . delete ( ) signals . user logged out . send ( type ( self ) , user = token . user , request = request , ) return response . Response ( status = status . HTTP 204 NO CONTENT )", "predictions": ["do a log log - token"], "references": ["delete auth token when delete request was issued ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 3766, "code": "def initial ( self , request , * args , * * kwargs ) : email = request . data . get ( 'email' ) if request . user . is authenticated ( ) and email != request . user . email : raise Permission Denied ( ) return super ( Resend Confirmation Email , self ) . initial ( request , * args , * * kwargs )", "predictions": ["set up log and handles log requests ."], "references": ["disallow users other than the user whose email is being reset ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 3767, "code": "def post ( self , request , * args , * * kwargs ) : serializer = self . serializer class ( data = request . data ) if not serializer . is valid ( ) : return response . Response ( serializer . errors , status = status . HTTP 400 BAD REQUEST , ) serializer . user . send validation email ( ) msg = ( 'Email confirmation sent.' ) return response . Response ( msg , status = status . HTTP 204 NO CONTENT )", "predictions": ["handles fmt email but do not process"], "references": ["validate email and send a request to confirm it ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 3768, "code": "def update expiry ( self , commit = True ) : self . expires = update expiry ( self . created ) if commit : self . save ( )", "predictions": ["get or set an all if the if the if not already set"], "references": ["update token s expiration datetime on every auth action ."], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 3769, "code": "def password reset email context ( notification ) : return { 'protocol' : 'https' , 'uid' : notification . user . generate uid ( ) , 'token' : notification . user . generate token ( ) , 'site' : notification . site , }", "predictions": ["reset a new main add email context to the parser display the parser display ."], "references": ["email context to reset a user password ."], "bleu": 0.1892240568795935, "rouge_l": 0.36802413273001505}
{"id": 3770, "code": "def email handler ( notification , email context ) : incuna mail . send ( to = notification . user . email , subject = notification . email subject , template name = notification . text email template , html template name = notification . html email template , context = email context ( notification ) , headers = getattr ( notification , 'headers' , { } ) , )", "predictions": ["process an debug notification notification runs on the client runs runs runs runs runs runs runs runs runs runs runs runs runs runs runs runs runs runs runs runs runs in"], "references": ["send a notification by email ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 3771, "code": "def password reset email handler ( notification ) : base subject = ( '{domain} password reset' ) . format ( domain = notification . site . domain ) subject = getattr ( settings , 'DUM PASSWORD RESET SUBJECT' , base subject ) notification . email subject = subject email handler ( notification , password reset email context )", "predictions": ["resets the reorder levels for the given levels chars chars chars chars chars chars chars chars chars ."], "references": ["password reset email handler ."], "bleu": 0.06809398432036522, "rouge_l": 0.09682539682539681}
{"id": 3772, "code": "def validation email handler ( notification ) : base subject = ( '{domain} account validate' ) . format ( domain = notification . site . domain ) subject = getattr ( settings , 'DUM VALIDATE EMAIL SUBJECT' , base subject ) notification . email subject = subject email handler ( notification , validation email context )", "predictions": ["validate the process email context file file file file file file file file file file file file self file file file file email file file file email file file file email"], "references": ["validation email handler ."], "bleu": 0.03901663112717908, "rouge_l": 0.06637649619151251}
{"id": 3773, "code": "def authenticate credentials ( self , key ) : user , token = super ( Token Authentication , self ) . authenticate credentials ( key ) if token . expires < timezone . now ( ) : msg = ( 'Token has expired.' ) raise exceptions . Authentication Failed ( msg ) token . update expiry ( ) return ( user , token )", "predictions": ["add credentials and ."], "references": ["custom authentication to check if auth token has expired ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 3774, "code": "def notebook show ( obj , doc , comm ) : target = obj . ref [ 'id' ] load mime = 'application/vnd.holoviews load.v0+json' exec mime = 'application/vnd.holoviews exec.v0+json' bokeh script , bokeh div , = bokeh . embed . notebook . notebook content ( obj , comm . id ) publish display data ( data = { 'text/html' : encode utf8 ( bokeh div ) } ) JS = '\\n' . join ( [ PYVIZ PROXY , Jupyter Comm Manager . js manager ] ) publish display data ( data = { load mime : JS , 'application/javascript' : JS } ) msg handler = bokeh msg handler . format ( plot id = target ) comm js = comm . js template . format ( plot id = target , comm id = comm . id , msg handler = msg handler ) bokeh js = '\\n' . join ( [ comm js , bokeh script ] ) publish display data ( data = { exec mime : '' , 'text/html' : '' , 'application/javascript' : bokeh js } , metadata = { exec mime : { 'id' : target } } )", "predictions": ["show the details of a cli"], "references": ["displays bokeh output inside a notebook ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 3775, "code": "def process hv plots ( widgets , plots ) : bokeh plots = [ ] for plot in plots : if hasattr ( plot , ' update callbacks' ) : for subplot in plot . traverse ( lambda x : x ) : subplot . comm = widgets . server comm for cb in subplot . callbacks : for c in cb . callbacks : c . code = c . code . replace ( plot . id , widgets . plot id ) plot = plot . state bokeh plots . append ( plot ) return bokeh plots", "predictions": ["discover all plots plots plots with a list of gui . . . . . . ."], "references": ["temporary fix to patch holoviews plot comms"], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 3776, "code": "def widget ( self , param name ) : if param name not in self . widgets : self . widgets [ param name ] = self . make widget ( param name ) return self . widgets [ param name ]", "predictions": ["create a widget widget ."], "references": ["get widget for param_name"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 3777, "code": "def render function ( obj , view ) : try : import holoviews as hv except : hv = None if hv and isinstance ( obj , hv . core . Dimensioned ) : renderer = hv . renderer ( 'bokeh' ) if not view . notebook : renderer = renderer . instance ( mode = 'server' ) plot = renderer . get plot ( obj , doc = view . document ) if view . notebook : plot . comm = view . comm plot . document = view . document return plot . state return obj", "predictions": ["render a function or plot of a function"], "references": ["the default renderer function which handles holoviews objects ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 3778, "code": "def Text Widget ( * args , * * kw ) : kw [ 'value' ] = str ( kw [ 'value' ] ) kw . pop ( 'options' , None ) return Text Input ( * args , * * kw )", "predictions": ["remove an item from the given document ."], "references": ["forces a parameter value to be text"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3779, "code": "def ping ( self , params = None ) : try : self . transport . perform request ( 'HEAD' , '/' , params = params ) except Transport Error : raise gen . Return ( False ) raise gen . Return ( True )", "predictions": ["ping the transport ."], "references": ["returns true if the cluster is up false otherwise ."], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 3780, "code": "def bytes to readable ( num ) : if num < 512 : return \"0 Kb\" elif num < 1024 : return \"1 Kb\" for unit in [ '' , 'Kb' , 'Mb' , 'Gb' , 'Tb' , 'Pb' , 'Eb' , 'Zb' ] : if abs ( num ) < 1024.0 : return \"%3.1f%s\" % ( num , unit ) num /= 1024.0 return \"%.1f%s\" % ( num , 'Yb' )", "predictions": ["convert bytes to readable string ."], "references": ["converts bytes to a human readable format"], "bleu": 0.2644358066258934, "rouge_l": 0.45522388059701485}
{"id": 3781, "code": "def cpu total load ( self ) : system load = self . cpu system load user load = self . cpu user load other load = self . cpu other load if system load is not None and user load is not None and other load is not None : return system load + user load + other load", "predictions": ["returns the total system load system load ."], "references": ["total cpu load for synology dsm"], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 3782, "code": "def memory size ( self , human readable = True ) : if self . data is not None : return data = int ( self . data [ \"memory\" ] [ \"memory size\" ] ) * 1024 if human readable : return Syno Format Helper . bytes to readable ( return data ) else : return return data", "predictions": ["return the memory size of the memory ."], "references": ["total memory size of synology dsm"], "bleu": 0.2984745896009823, "rouge_l": 0.43990384615384615}
{"id": 3783, "code": "def network up ( self , human readable = True ) : network = self . get network ( \"total\" ) if network is not None : return data = int ( network [ \"tx\" ] ) if human readable : return Syno Format Helper . bytes to readable ( return data ) else : return return data", "predictions": ["return the network up to the network ."], "references": ["total upload speed being used"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3784, "code": "def volumes ( self ) : if self . data is not None : volumes = [ ] for volume in self . data [ \"volumes\" ] : volumes . append ( volume [ \"id\" ] ) return volumes", "predictions": ["list of all volumes in this collection ."], "references": ["returns all available volumes"], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 3785, "code": "def get volume ( self , volume id ) : if self . data is not None : for volume in self . data [ \"volumes\" ] : if volume [ \"id\" ] == volume id : return volume", "predictions": ["return the volume of the given volume ."], "references": ["returns a specific volume"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 3786, "code": "def volume size total ( self , volume , human readable = True ) : volume = self . get volume ( volume ) if volume is not None : return data = int ( volume [ \"size\" ] [ \"total\" ] ) if human readable : return Syno Format Helper . bytes to readable ( return data ) else : return return data", "predictions": ["return the total size of the volume"], "references": ["total size of volume"], "bleu": 0.3655552228545123, "rouge_l": 0.7648902821316614}
{"id": 3787, "code": "def volume percentage used ( self , volume ) : volume = self . get volume ( volume ) if volume is not None : total = int ( volume [ \"size\" ] [ \"total\" ] ) used = int ( volume [ \"size\" ] [ \"used\" ] ) if used is not None and used > 0 and total is not None and total > 0 : return round ( ( float ( used ) / float ( total ) ) * 100.0 , 1 )", "predictions": ["volume used for percentage"], "references": ["total used size in percentage for volume"], "bleu": 0.21341568174752815, "rouge_l": 0.346590909090909}
{"id": 3788, "code": "def volume disk temp avg ( self , volume ) : volume = self . get volume ( volume ) if volume is not None : vol disks = volume [ \"disks\" ] if vol disks is not None : total temp = 0 total disks = 0 for vol disk in vol disks : disk temp = self . disk temp ( vol disk ) if disk temp is not None : total disks += 1 total temp += disk temp if total temp > 0 and total disks > 0 : return round ( total temp / total disks , 0 )", "predictions": ["return the disk disk for a disk"], "references": ["average temperature of all disks making up the volume"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 3789, "code": "def volume disk temp max ( self , volume ) : volume = self . get volume ( volume ) if volume is not None : vol disks = volume [ \"disks\" ] if vol disks is not None : max temp = 0 for vol disk in vol disks : disk temp = self . disk temp ( vol disk ) if disk temp is not None and disk temp > max temp : max temp = disk temp return max temp", "predictions": ["return maximum disk disk for a disk volume ."], "references": ["maximum temperature of all disks making up the volume"], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 3790, "code": "def get disk ( self , disk id ) : if self . data is not None : for disk in self . data [ \"disks\" ] : if disk [ \"id\" ] == disk id : return disk", "predictions": ["return the disk object for the given disk id ."], "references": ["returns a specific disk"], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 3791, "code": "def login ( self ) : api path = \"%s/auth.cgi?api=SYNO.API.Auth&version=2\" % ( self . base url , ) login path = \"method=login&%s\" % ( self . encode credentials ( ) ) url = \"%s&%s&session=Core&format=cookie\" % ( api path , login path ) result = self . execute get url ( url , False ) if result is not None : self . access token = result [ \"data\" ] [ \"sid\" ] self . debuglog ( \"Authentication Succesfull, token: \" + str ( self . access token ) ) return True else : self . debuglog ( \"Authentication Failed\" ) return False", "predictions": ["gets the login form"], "references": ["build and execute login request"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 3792, "code": "def get url ( self , url , retry on error = True ) : if self . access token is None or self . session is None or self . session error : self . access token = None self . session error = False if self . session is not None : self . session = None self . debuglog ( \"Creating New Session\" ) self . session = requests . Session ( ) if self . use https : self . session . verify = False if self . login ( ) is False : self . session error = True self . debuglog ( \"Login Failed, unable to process request\" ) return response = self . execute get url ( url ) if ( self . session error or response is None ) and retry on error : self . debuglog ( \"Error occured, retrying...\" ) self . get url ( url , False ) return response", "predictions": ["get the url of the url"], "references": ["function to handle sessions for a get request"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 3793, "code": "def execute get url ( self , request url , append sid = True ) : self . debuglog ( \"Requesting URL: '\" + request url + \"'\" ) if append sid : self . debuglog ( \"Appending access token (SID: \" + self . access token + \") to url\" ) request url = \"%s& sid=%s\" % ( request url , self . access token ) try : resp = self . session . get ( request url ) self . debuglog ( \"Request executed: \" + str ( resp . status code ) ) if resp . status code == 200 : json data = json . loads ( resp . text ) if json data [ \"success\" ] : self . debuglog ( \"Succesfull returning data\" ) self . debuglog ( str ( json data ) ) return json data else : if json data [ \"error\" ] [ \"code\" ] in { 105 , 106 , 107 , 119 } : self . debuglog ( \"Session error: \" + str ( json data [ \"error\" ] [ \"code\" ] ) ) self . session error = True else : self . debuglog ( \"Failed: \" + resp . text ) else : return None except : return None", "predictions": ["execute the get url and return the response ."], "references": ["function to execute and handle a get request"], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 3794, "code": "def update ( self ) : if self . utilisation is not None : api = \"SYNO.Core.System.Utilization\" url = \"%s/entry.cgi?api=%s&version=1&method=get& sid=%s\" % ( self . base url , api , self . access token ) self . utilisation . update ( self . get url ( url ) ) if self . storage is not None : api = \"SYNO.Storage.CGI.Storage\" url = \"%s/entry.cgi?api=%s&version=1&method=load info& sid=%s\" % ( self . base url , api , self . access token ) self . storage . update ( self . get url ( url ) )", "predictions": ["update the api token"], "references": ["updates the various instanced modules"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 3795, "code": "def utilisation ( self ) : if self . utilisation is None : api = \"SYNO.Core.System.Utilization\" url = \"%s/entry.cgi?api=%s&version=1&method=get\" % ( self . base url , api ) self . utilisation = Syno Utilization ( self . get url ( url ) ) return self . utilisation", "predictions": ["returns the utilisation utilisation"], "references": ["getter for various utilisation variables"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 3796, "code": "def storage ( self ) : if self . storage is None : api = \"SYNO.Storage.CGI.Storage\" url = \"%s/entry.cgi?api=%s&version=1&method=load info\" % ( self . base url , api ) self . storage = Syno Storage ( self . get url ( url ) ) return self . storage", "predictions": ["return the storage object ."], "references": ["getter for various storage variables"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 3797, "code": "def for request ( request , body = None ) : tenant , jwt data = Tenant . objects . for request ( request , body ) webhook sender id = jwt data . get ( 'sub' ) sender data = None if body and 'item' in body : if 'sender' in body [ 'item' ] : sender data = body [ 'item' ] [ 'sender' ] elif 'message' in body [ 'item' ] and 'from' in body [ 'item' ] [ 'message' ] : sender data = body [ 'item' ] [ 'message' ] [ 'from' ] if sender data is None : if webhook sender id is None : raise Bad Tenant Error ( 'Cannot identify sender in tenant' ) sender data = { 'id' : webhook sender id } return Context ( tenant = tenant , sender = Hipchat User ( id = sender data . get ( 'id' ) , name = sender data . get ( 'name' ) , mention name = sender data . get ( 'mention name' ) , ) , signed request = request . GET . get ( 'signed request' ) , context = jwt data . get ( 'context' ) or { } , )", "predictions": ["returns a list of all the data structures ."], "references": ["creates the context for a specific request ."], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 3798, "code": "def tenant token ( self ) : rv = getattr ( self , ' tenant token' , None ) if rv is None : rv = self . tenant token = self . tenant . get token ( ) return rv", "predictions": ["returns the tenant token for the tenant ."], "references": ["the cached token of the current tenant ."], "bleu": 0.25098621243978964, "rouge_l": 0.625}
{"id": 3799, "code": "def build attrs ( self , extra attrs = None , * * kwargs ) : self . attrs = self . widget . build attrs ( extra attrs = None , * * kwargs ) return self . attrs", "predictions": ["build the attrs dictionary for this sample ."], "references": ["helper function for building an attribute dictionary ."], "bleu": 0.19070828081828378, "rouge_l": 0.25}
{"id": 3800, "code": "def get global settings ( self ) : return dict ( ( key , getattr ( global settings , key ) ) for key in dir ( global settings ) if key . isupper ( ) )", "predictions": ["return the global settings dict from the global settings ."], "references": ["return a dictionary of all global_settings values ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 3801, "code": "def do GET ( self ) : parsed url = urlparse ( self . path ) if parsed url [ 2 ] == \"/\" + SERVER REDIRECT PATH : parsed query = parse qs ( parsed url [ 4 ] ) if \"code\" not in parsed query : self . send response ( 200 ) self . send header ( \"Content-Type\" , \"text/plain\" ) self . end headers ( ) self . wfile . write ( \"No code found, try again!\" . encode ( \"utf-8\" ) ) return self . server . response code = parsed query [ \"code\" ] [ 0 ] self . send response ( 200 ) self . send header ( \"Content-Type\" , \"text/plain\" ) self . end headers ( ) self . wfile . write ( \"Thank you for using O Auth2Util. The authorization was successful, \" \"you can now close this window.\" . encode ( \"utf-8\" ) ) elif parsed url [ 2 ] == \"/\" + SERVER LINK PATH : self . send response ( 200 ) self . send header ( \"Content-Type\" , \"text/html\" ) self . end headers ( ) self . wfile . write ( \"<html><body>Hey there!<br/>Click <a href=\\\"{0}\\\">here</a> to claim your prize.</body></html>\" . format ( self . server . authorize url ) . encode ( \"utf-8\" ) ) else : self . send response ( 404 ) self . send header ( \"Content-Type\" , \"text/plain\" ) self . end headers ( ) self . wfile . write ( \"404 not found\" . encode ( \"utf-8\" ) )", "predictions": ["handle the actual packet"], "references": ["handle the retrieval of the code"], "bleu": 0.2868106410131918, "rouge_l": 0.3860759493670886}
{"id": 3802, "code": "def get value ( self , key , func = None , split val = None , as boolean = False , exception default = None ) : try : if as boolean : return self . config . getboolean ( key [ 0 ] , key [ 1 ] ) value = self . config . get ( key [ 0 ] , key [ 1 ] ) if split val is not None : value = value . split ( split val ) if func is not None : return func ( value ) return value except ( Key Error , configparser . No Section Error , configparser . No Option Error ) as e : if exception default is not None : return exception default raise Key Error ( e )", "predictions": ["get the value from the config object ."], "references": ["helper method to get a value from the config"], "bleu": 0.41224781842476826, "rouge_l": 0.5820610687022901}
{"id": 3803, "code": "def change value ( self , key , value ) : if not self . config . has section ( key [ 0 ] ) : self . config . add section ( key [ 0 ] ) self . config . set ( key [ 0 ] , key [ 1 ] , str ( value ) ) with open ( self . configfile , \"w\" ) as f : self . config . write ( f )", "predictions": ["change the value of a section ."], "references": ["change the value of the given key in the given file to the given value"], "bleu": 0.16486612875739623, "rouge_l": 0.3412587412587413}
{"id": 3804, "code": "def migrate config ( self , oldname = DEFAULT CONFIG , newname = DEFAULT CONFIG ) : self . log ( \"Your O Auth2Util config file is in an old format and needs \" \"to be changed. I tried as best as I could to migrate it.\" , logging . WARNING ) with open ( oldname , \"r\" ) as old : with open ( newname , \"w\" ) as new : new . write ( \"[app]\\n\" ) new . write ( old . read ( ) )", "predictions": ["migrate the current config file to the old config file ."], "references": ["migrates the old config file format to the new one"], "bleu": 0.3646285861936466, "rouge_l": 0.4803149606299213}
{"id": 3805, "code": "def start webserver ( self , authorize url = None ) : server address = ( SERVER URL , SERVER PORT ) self . server = HTTP Server ( server address , O Auth2Util Request Handler ) self . server . response code = None self . server . authorize url = authorize url t = Thread ( target = self . server . serve forever ) t . daemon = True t . start ( )", "predictions": ["start the server ."], "references": ["start the webserver that will receive the code"], "bleu": 0.1739594473063345, "rouge_l": 0.31443298969072164}
{"id": 3806, "code": "def wait for response ( self ) : while not self . server . response code : time . sleep ( 2 ) time . sleep ( 5 ) self . server . shutdown ( )", "predictions": ["wait for the response to finish"], "references": ["wait until the user accepted or rejected the request"], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 3807, "code": "def get new access information ( self ) : if not self . r . has oauth app info : self . log ( 'Cannot obtain authorize url from PRAW. Please check your configuration.' , logging . ERROR ) raise Attribute Error ( 'Reddit Session invalid, please check your designated config file.' ) url = self . r . get authorize url ( 'Using O Auth2Util' , self . get value ( CONFIGKEY SCOPE , set , split val = ',' ) , self . get value ( CONFIGKEY REFRESHABLE , as boolean = True ) ) self . start webserver ( url ) if not self . get value ( CONFIGKEY SERVER MODE , as boolean = True ) : webbrowser . open ( url ) else : print ( \"Webserver is waiting for you :D. Please open {0}:{1}/{2} \" \"in your browser\" . format ( SERVER URL , SERVER PORT , SERVER LINK PATH ) ) self . wait for response ( ) try : access information = self . r . get access information ( self . server . response code ) except praw . errors . O Auth Exception : self . log ( \"Can not authenticate, maybe the app infos (e.g. secret) are wrong.\" , logging . ERROR ) raise self . change value ( CONFIGKEY TOKEN , access information [ \"access token\" ] ) self . change value ( CONFIGKEY REFRESH TOKEN , access information [ \"refresh token\" ] ) self . change value ( CONFIGKEY VALID UNTIL , time . time ( ) + TOKEN VALID DURATION )", "predictions": ["get new access access access information from oauth app"], "references": ["request new access information from reddit using the built in webserver"], "bleu": 0.2389997677137507, "rouge_l": 0.3929146537842191}
{"id": 3808, "code": "def check token present ( self ) : try : self . get value ( CONFIGKEY TOKEN ) self . get value ( CONFIGKEY REFRESH TOKEN ) self . get value ( CONFIGKEY REFRESHABLE ) except Key Error : self . log ( \"Request new Token (CTP)\" ) self . get new access information ( )", "predictions": ["widget widget if token is not self if not self if not self if not self if not"], "references": ["check whether the tokens are set and request new ones if not"], "bleu": 0.08961672320242714, "rouge_l": 0.13832199546485258}
{"id": 3809, "code": "def set access credentials ( self , retry = 0 ) : if retry >= 5 : raise Connection Aborted Error ( 'Reddit is not accessible right now, cannot refresh O Auth2 tokens.' ) self . check token present ( ) try : self . r . set access credentials ( self . get value ( CONFIGKEY SCOPE , set , split val = \",\" ) , self . get value ( CONFIGKEY TOKEN ) , self . get value ( CONFIGKEY REFRESH TOKEN ) ) except ( praw . errors . O Auth Invalid Token , praw . errors . HTTP Exception ) as e : self . log ( \"Request new Token (SAC)\" ) self . get new access information ( )", "predictions": ["render the function credentials credentials credentials"], "references": ["set the token on the reddit object again"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 3810, "code": "def fix schema ( prefix , schema ) : schema dict = extract schema ( schema ) snake case organization = schema dict [ 'vendor' ] . replace ( '.' , ' ' ) . lower ( ) snake case name = re . sub ( '([^A-Z ])([A-Z])' , '\\g<1> \\g<2>' , schema dict [ 'name' ] ) . lower ( ) model = schema dict [ 'version' ] . split ( '-' ) [ 0 ] return \"{} {} {} {}\" . format ( prefix , snake case organization , snake case name , model )", "predictions": ["fix schema schema schema"], "references": ["create an elasticsearch field name from a schema string"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 3811, "code": "def transform ( line , known fields = ENRICHED EVENT FIELD TYPES , add geolocation data = True ) : return jsonify good event ( line . split ( '\\t' ) , known fields , add geolocation data )", "predictions": ["ping the gen gen gen events"], "references": ["convert a snowplow enriched event tsv into a json"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 3812, "code": "def jsonify good event ( event , known fields = ENRICHED EVENT FIELD TYPES , add geolocation data = True ) : if len ( event ) != len ( known fields ) : raise Snowplow Event Transformation Exception ( [ \"Expected {} fields, received {} fields.\" . format ( len ( known fields ) , len ( event ) ) ] ) else : output = { } errors = [ ] if add geolocation data and event [ LATITUDE INDEX ] != '' and event [ LONGITUDE INDEX ] != '' : output [ 'geo location' ] = event [ LATITUDE INDEX ] + ',' + event [ LONGITUDE INDEX ] for i in range ( len ( event ) ) : key = known fields [ i ] [ 0 ] if event [ i ] != '' : try : kvpairs = known fields [ i ] [ 1 ] ( key , event [ i ] ) for kvpair in kvpairs : output [ kvpair [ 0 ] ] = kvpair [ 1 ] except Snowplow Event Transformation Exception as sete : errors += sete . error messages except Exception as e : errors += [ \"Unexpected exception parsing field with key {} and value {}: {}\" . format ( known fields [ i ] [ 0 ] , event [ i ] , repr ( e ) ) ] if errors : raise Snowplow Event Transformation Exception ( errors ) else : return output", "predictions": ["bytes for to to be used in the module"], "references": ["convert a snowplow enriched event in the form of an array of fields into a json"], "bleu": 0.08533861327767082, "rouge_l": 0.1523096129837703}
{"id": 3813, "code": "def print context ( self , context ) : text = [ CONTEXT TITLE ] for i , context scope in enumerate ( context ) : dump1 = linebreaksbr ( pformat django context html ( context scope ) ) dump2 = pformat dict summary html ( context scope ) if len ( context scope ) <= 3 and dump1 . count ( '<br />' ) > 20 : ( dump1 , dump2 ) = ( dump2 , dump1 ) text . append ( CONTEXT BLOCK . format ( style = PRE STYLE , num = i , dump1 = dump1 , dump2 = dump2 ) ) return u'' . join ( text )", "predictions": ["cpu total total number of django and return"], "references": ["print the entire template context"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3814, "code": "def print variables ( self , context ) : text = [ ] for name , expr in self . variables : data = '' try : if isinstance ( expr . var , Variable ) : data = expr . var . resolve ( context ) else : data = expr . resolve ( context ) except Variable Does Not Exist as e : keys = [ ] for scope in context : keys += scope . keys ( ) keys = sorted ( set ( keys ) ) return ERROR TYPE BLOCK . format ( style = PRE ALERT STYLE , error = escape ( u\"Variable '{0}' not found!  Available context variables are:\\n\\n{1}\" . format ( expr , u', ' . join ( keys ) ) ) ) else : textdata = linebreaksbr ( pformat django context html ( data ) ) if isinstance ( data , SHORT NAME TYPES ) : text . append ( BASIC TYPE BLOCK . format ( style = PRE STYLE , name = name , value = textdata ) ) else : text . append ( OBJECT TYPE BLOCK . format ( style = PRE STYLE , name = name , type = data . class . name , value = textdata ) ) return u'' . join ( text )", "predictions": ["memory html of the human - readable human readable form . . ."], "references": ["print a set of variables"], "bleu": 0.09552040806823771, "rouge_l": 0.12079207920792079}
{"id": 3815, "code": "def pformat sql html ( sql ) : sql = escape ( sql ) sql = RE SQL NL . sub ( u'<br>\\n\\\\1' , sql ) sql = RE SQL . sub ( u'<strong>\\\\1</strong>' , sql ) return sql", "predictions": ["replace up up up up up up up to html ."], "references": ["highlight common sql words in a string ."], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 3816, "code": "def pformat dict summary html ( dict ) : if not dict : return '   {}' html = [ ] for key , value in sorted ( six . iteritems ( dict ) ) : if not isinstance ( value , DICT EXPANDED TYPES ) : value = '...' html . append ( format dict item ( key , value ) ) return mark safe ( u'<br/>' . join ( html ) )", "predictions": ["convert a dict dict to json"], "references": ["briefly print the dictionary keys ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3817, "code": "def format ( self , object , stream , indent , allowance , context , level ) : try : Pretty Printer . format ( self , object , stream , indent , allowance , context , level ) except Exception as e : stream . write ( format exception ( e ) )", "predictions": ["get the formatted output of the given object in the id in the id"], "references": ["recursive part of the formatting"], "bleu": 0.11633270842295028, "rouge_l": 0.23018867924528305}
{"id": 3818, "code": "def get organisation information ( self , query params = None ) : return self . fetch json ( uri path = self . base uri , query params = query params or { } )", "predictions": ["gets all size of a human readable human readable form get the size of the size of the human readable get the size of the size of the size of the"], "references": ["get information fot this organisation . returns a dictionary of values ."], "bleu": 0.046398855339878003, "rouge_l": 0.10107705053852528}
{"id": 3819, "code": "def get list information ( self , query params = None ) : return self . fetch json ( uri path = self . base uri , query params = query params or { } )", "predictions": ["gets all used used used in a percentage not found in the current query"], "references": ["get information for this list . returns a dictionary of values ."], "bleu": 0.08839374326825923, "rouge_l": 0.07800511508951406}
{"id": 3820, "code": "def add card ( self , query params = None ) : card json = self . fetch json ( uri path = self . base uri + '/cards' , http method = 'POST' , query params = query params or { } ) return self . create card ( card json )", "predictions": ["adds a disk to the current disk if it exists ."], "references": ["create a card for this list . returns a card object ."], "bleu": 0.11510518494396255, "rouge_l": 0.17256011315417258}
{"id": 3821, "code": "def get label information ( self , query params = None ) : return self . fetch json ( uri path = self . base uri , query params = query params or { } )", "predictions": ["gets all disk temp temp temp temp is temp is not a query"], "references": ["get all information for this label . returns a dictionary of values ."], "bleu": 0.10571070857151538, "rouge_l": 0.15384615384615383}
{"id": 3822, "code": "def update label name ( self , name ) : label json = self . fetch json ( uri path = self . base uri , http method = 'PUT' , query params = { 'name' : name } ) return self . create label ( label json )", "predictions": ["updates a disk name name ."], "references": ["update the current label s name . returns a new label object ."], "bleu": 0.09728049676725326, "rouge_l": 0.19741100323624597}
{"id": 3823, "code": "def update label dict ( self , query params = { } ) : label json = self . fetch json ( uri path = self . base uri , http method = 'PUT' , query params = query params ) return self . create label ( label json )", "predictions": ["updates a label self . ."], "references": ["update the current label . returns a new label object ."], "bleu": 0.12071482560966854, "rouge_l": 0.33516483516483514}
{"id": 3824, "code": "def get card information ( self , query params = None ) : return self . fetch json ( uri path = self . base uri , query params = query params or { } )", "predictions": ["gets all url information ."], "references": ["get information for this card . returns a dictionary of values ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 3825, "code": "def add comment ( self , comment text ) : return self . fetch json ( uri path = self . base uri + '/actions/comments' , http method = 'POST' , query params = { 'text' : comment text } )", "predictions": ["execute a get get request"], "references": ["adds a comment to this card by the current user ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 3826, "code": "def add attachment ( self , filename , open file ) : fields = { 'api key' : self . client . api key , 'token' : self . client . user auth token } content type , body = self . encode multipart formdata ( fields = fields , filename = filename , file values = open file ) return self . fetch json ( uri path = self . base uri + '/attachments' , http method = 'POST' , body = body , headers = { 'Content-Type' : content type } , )", "predictions": ["adds an attachment to the specified utilisation . . . . . . . . . ."], "references": ["adds an attachment to this card ."], "bleu": 0.21042990347620458, "rouge_l": 0.4505169867060562}
{"id": 3827, "code": "def add checklist ( self , query params = None ) : checklist json = self . fetch json ( uri path = self . base uri + '/checklists' , http method = 'POST' , query params = query params or { } ) return self . create checklist ( checklist json )", "predictions": ["utilisation a new checklist url url url url url url url url url url ."], "references": ["add a checklist to this card . returns a checklist object ."], "bleu": 0.09782375748961449, "rouge_l": 0.22676579925650556}
{"id": 3828, "code": "def add label from dict ( self , query params = None ) : return self . fetch json ( uri path = self . base uri + '/labels' , http method = 'POST' , query params = query params or { } )", "predictions": ["storage for a is a label self . ."], "references": ["add a label to this card from a dictionary ."], "bleu": 0.18885888592159467, "rouge_l": 0.31282051282051276}
{"id": 3829, "code": "def add label from class ( self , label = None ) : return self . fetch json ( uri path = self . base uri + '/id Labels' , http method = 'POST' , query params = { 'value' : label . id } )", "predictions": ["adds a request to this class ."], "references": ["add an existing label to this card ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 3830, "code": "def add member ( self , member id ) : members = self . fetch json ( uri path = self . base uri + '/id Members' , http method = 'POST' , query params = { 'value' : member id } ) members list = [ ] for member json in members : members list . append ( self . create member ( member json ) ) return members list", "predictions": ["tenant a token for a token"], "references": ["add a member to this card . returns a list of member objects ."], "bleu": 0.06443935473636557, "rouge_l": 0.18654434250764526}
{"id": 3831, "code": "def create checklist item ( self , card id , checklist id , checklistitem json , * * kwargs ) : return self . client . create checklist item ( card id , checklist id , checklistitem json , * * kwargs )", "predictions": ["creates an attrs for a attrs . . ."], "references": ["create a checklistitem object from json object"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 3832, "code": "def get board information ( self , query params = None ) : return self . fetch json ( uri path = '/boards/' + self . id , query params = query params or { } )", "predictions": ["get all global global global global global global global global global global global global global global global global global global settings in the query in this method in the query in"], "references": ["get all information for this board . returns a dictionary of values ."], "bleu": 0.055177848898164926, "rouge_l": 0.14722445695897024}
{"id": 3833, "code": "def get checklists ( self ) : checklists = self . get Checklists Json ( self . base uri ) checklists list = [ ] for checklist json in checklists : checklists list . append ( self . create Checklist ( checklist json ) ) return checklists list", "predictions": ["returns a list of checklists url url"], "references": ["get the checklists for this board . returns a list of checklist objects ."], "bleu": 0.19905304276733932, "rouge_l": 0.35935198821796754}
{"id": 3834, "code": "def update board ( self , query params = None ) : board json = self . fetch json ( uri path = self . base uri , http method = 'PUT' , query params = query params or { } ) return self . create board ( board json )", "predictions": ["updates a key ."], "references": ["update this board s information . returns a new board ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 3835, "code": "def add list ( self , query params = None ) : list json = self . fetch json ( uri path = self . base uri + '/lists' , http method = 'POST' , query params = query params or { } ) return self . create list ( list json )", "predictions": ["method to change a value of a key has been added to the current key has been created has been authenticated value"], "references": ["create a list for a board . returns a new list object ."], "bleu": 0.0612957497932821, "rouge_l": 0.11984282907662083}
{"id": 3836, "code": "def add label ( self , query params = None ) : list json = self . fetch json ( uri path = self . base uri + '/labels' , http method = 'POST' , query params = query params or { } ) return self . create label ( list json )", "predictions": ["adds a config to the current query log log log log log log log log log log log log log log log log log log log log log log log log"], "references": ["create a label for a board . returns a new label object ."], "bleu": 0.03901663112717908, "rouge_l": 0.04907481898632341}
{"id": 3837, "code": "def get checklist information ( self , query params = None ) : return self . fetch json ( uri path = self . base uri , query params = query params or { } )", "predictions": ["returns all webserver information information . ."], "references": ["get all information for this checklist . returns a dictionary of values ."], "bleu": 0.10374282717383708, "rouge_l": 0.3794712286158632}
{"id": 3838, "code": "def get card ( self ) : card id = self . get checklist information ( ) . get ( 'id Card' , None ) if card id : return self . client . get card ( card id )", "predictions": ["wait for a for a for the for the for the for this for this for the given for the given for the given for the for the given for the"], "references": ["get card this checklist is on ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 3839, "code": "def get item objects ( self , query params = None ) : card = self . get card ( ) checklistitems list = [ ] for checklistitem json in self . get items ( query params ) : checklistitems list . append ( self . create checklist item ( card . id , self . id , checklistitem json ) ) return checklistitems list", "predictions": ["returns all new new new new new new new new new new new new new new new new new new new new new new new new new new new new new"], "references": ["get the items for this checklist . returns a list of checklistitem objects ."], "bleu": 0.03901663112717908, "rouge_l": 0.04769351055512119}
{"id": 3840, "code": "def update checklist ( self , name ) : checklist json = self . fetch json ( uri path = self . base uri , http method = 'PUT' , query params = { 'name' : name } ) return self . create checklist ( checklist json )", "predictions": ["updates an checklist for a given name ."], "references": ["update the current checklist . returns a new checklist object ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 3841, "code": "def remove item ( self , item id ) : return self . fetch json ( uri path = self . base uri + '/check Items/' + item id , http method = 'DELETE' )", "predictions": ["remove an item from the base uri ."], "references": ["deletes an item from this checklist ."], "bleu": 0.3155984539112945, "rouge_l": 0.5398230088495575}
{"id": 3842, "code": "def update name ( self , name ) : checklistitem json = self . fetch json ( uri path = self . base uri + '/name' , http method = 'PUT' , query params = { 'value' : name } ) return self . create checklist item ( self . id Card , self . id Checklist , checklistitem json )", "predictions": ["update an item name ."], "references": ["rename the current checklist item . returns a new checklistitem object ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 3843, "code": "def update state ( self , state ) : checklistitem json = self . fetch json ( uri path = self . base uri + '/state' , http method = 'PUT' , query params = { 'value' : 'complete' if state else 'incomplete' } ) return self . create checklist item ( self . id Card , self . id Checklist , checklistitem json )", "predictions": ["updates this state ."], "references": ["set the state of the current checklist item . returns a new checklistitem object ."], "bleu": 0.025419978385188596, "rouge_l": 0.190625}
{"id": 3844, "code": "def add authorisation ( self , query params ) : query params [ 'key' ] = self . api key if self . user auth token : query params [ 'token' ] = self . user auth token return query params", "predictions": ["add user to query query"], "references": ["adds the api key and user auth token to the query parameters"], "bleu": 0.08006212224540951, "rouge_l": 0.3285457809694794}
{"id": 3845, "code": "def check errors ( self , uri , response ) : if response . status == 401 : raise trolly . Unauthorised ( uri , response ) if response . status != 200 : raise trolly . Resource Unavailable ( uri , response )", "predictions": ["check if the response is valid ."], "references": ["check http reponse for known errors"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 3846, "code": "def build uri ( self , path , query params ) : url = 'https://api.trello.com/1' + self . clean path ( path ) url += '?' + urlencode ( query params ) return url", "predictions": ["return the url to use for the query ."], "references": ["build the uri for the api call ."], "bleu": 0.21105340631872635, "rouge_l": 0.4756335282651072}
{"id": 3847, "code": "def create checklist item ( self , card id , checklist id , checklistitem json ) : return trolly . checklist . Checklist Item ( trello client = self , card id = card id , checklist id = checklist id , checklistitem id = checklistitem json [ 'id' ] . encode ( 'utf-8' ) , name = checklistitem json [ 'name' ] . encode ( 'utf-8' ) , state = checklistitem json [ 'state' ] . encode ( 'utf-8' ) )", "predictions": ["creates a checklist item in the given card ."], "references": ["create a checklistitem object from json object"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 3848, "code": "def set password ( self , service , username , password ) : assoc = self . generate assoc ( service , username ) password encrypted = self . encrypt ( password . encode ( 'utf-8' ) , assoc ) password base64 = '\\n' + encodebytes ( password encrypted ) . decode ( ) self . write config value ( service , username , password base64 )", "predictions": ["write the password of the service ."], "references": ["write the password in the file ."], "bleu": 0.38260294162784475, "rouge_l": 0.7142857142857143}
{"id": 3849, "code": "def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] cli = Command Line Tool ( ) try : return cli . run ( argv ) except Keyboard Interrupt : print ( 'Canceled' ) return 3", "predictions": ["entry point for the application script ."], "references": ["main command line interface ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 3850, "code": "def create cipher ( self , password , salt , nonce = None ) : from argon2 . low level import hash secret raw , Type from Crypto . Cipher import AES aesmode = self . get mode ( self . aesmode ) if aesmode is None : raise Value Error ( 'invalid AES mode: %s' % self . aesmode ) key = hash secret raw ( secret = password . encode ( self . password encoding ) , salt = salt , time cost = self . time cost , memory cost = self . memory cost , parallelism = self . parallelism , hash len = 16 , type = Type . ID ) return AES . new ( key , aesmode , nonce )", "predictions": ["create the cipher cipher ."], "references": ["create the cipher object to encrypt or decrypt a payload ."], "bleu": 0.1618271218800702, "rouge_l": 0.4683301343570058}
{"id": 3851, "code": "def get mode ( mode = None ) : from Crypto . Cipher import AES AES Mode Map = { 'CCM' : AES . MODE CCM , 'EAX' : AES . MODE EAX , 'GCM' : AES . MODE GCM , 'OCB' : AES . MODE OCB , } if mode is None : return AES Mode Map . keys ( ) return AES Mode Map . get ( mode )", "predictions": ["return the path to the mode"], "references": ["return the aes mode or a list of valid aes modes if mode == none"], "bleu": 0.06970452184190923, "rouge_l": 0.26521739130434785}
{"id": 3852, "code": "def connect To Broker ( self , protocol ) : self . protocol = protocol self . protocol . on Publish = self . on Publish self . protocol . on Disconnection = self . on Disconnection self . protocol . set Window Size ( 3 ) try : yield self . protocol . connect ( \"Twisted MQTT-subs\" , keepalive = 60 ) yield self . subscribe ( ) except Exception as e : log . error ( \"Connecting to {broker} raised {excp!s}\" , broker = BROKER , excp = e ) else : log . info ( \"Connected and subscribed to {broker}\" , broker = BROKER )", "predictions": ["connect to the specified protocol ."], "references": ["connect to mqtt broker"], "bleu": 0.2907153684841096, "rouge_l": 0.4149659863945578}
{"id": 3853, "code": "def on Publish ( self , topic , payload , qos , dup , retain , msg Id ) : log . debug ( \"msg={payload}\" , payload = payload )", "predictions": ["internal helper to the payload of a topic"], "references": ["callback receiving messages from publisher"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3854, "code": "def connect To Broker ( self , protocol ) : self . protocol = protocol self . protocol . on Publish = self . on Publish self . protocol . on Disconnection = self . on Disconnection self . protocol . set Window Size ( 3 ) self . task = task . Looping Call ( self . publish ) self . task . start ( 5.0 , now = False ) try : yield self . protocol . connect ( \"Twisted MQTT-pubsubs\" , keepalive = 60 ) yield self . subscribe ( ) except Exception as e : log . error ( \"Connecting to {broker} raised {excp!s}\" , broker = BROKER , excp = e ) else : log . info ( \"Connected and subscribed to {broker}\" , broker = BROKER )", "predictions": ["connect to the given protocol ."], "references": ["connect to mqtt broker"], "bleu": 0.2907153684841096, "rouge_l": 0.4149659863945578}
{"id": 3855, "code": "def make Id ( self ) : self . id = ( self . id + 1 ) % 65536 self . id = self . id or 1 return self . id", "predictions": ["make a new id ."], "references": ["produce ids for protocol packets outliving their sessions"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 3856, "code": "def connect ( self , request ) : state = self . class . name return defer . fail ( MQTT State Error ( \"Unexpected connect() operation\" , state ) )", "predictions": ["connect to the given state ."], "references": ["send a connect control packet ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 3857, "code": "def handle CONNACK ( self , response ) : state = self . class . name log . error ( \"Unexpected {packet:7} packet received in {log source}\" , packet = \"CONNACK\" )", "predictions": ["handle incoming connack from server ."], "references": ["handles connack packet from the server"], "bleu": 0.2626909894424158, "rouge_l": 0.5}
{"id": 3858, "code": "def encode ( self ) : header = bytearray ( 2 ) header [ 0 ] = 0x E0 self . encoded = header return str ( header ) if PY2 else bytes ( header )", "predictions": ["return the byte representation of the packet ."], "references": ["encode and store a disconnect control packet ."], "bleu": 0.21105340631872638, "rouge_l": 0.25}
{"id": 3859, "code": "def decode ( self , packet ) : self . encoded = packet len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] version str , packet remaining = decode String ( packet remaining ) version id = int ( packet remaining [ 0 ] ) if version id == v31 [ 'level' ] : self . version = v31 else : self . version = v311 flags = packet remaining [ 1 ] self . clean Start = ( flags & 0x02 ) != 0 will Flag = ( flags & 0x04 ) != 0 will Qo S = ( flags >> 3 ) & 0x03 will Retain = ( flags & 0x20 ) != 0 user Flag = ( flags & 0x80 ) != 0 pass Flag = ( flags & 0x40 ) != 0 packet remaining = packet remaining [ 2 : ] self . keepalive = decode16Int ( packet remaining ) packet remaining = packet remaining [ 2 : ] self . client Id , packet remaining = decode String ( packet remaining ) if will Flag : self . will Retain = will Retain self . will Qo S = will Qo S self . will Topic , packet remaining = decode String ( packet remaining ) self . will Message , packet remaining = decode String ( packet remaining ) if user Flag : self . username , packet remaining = decode String ( packet remaining ) if pass Flag : l = decode16Int ( packet remaining ) self . password = packet remaining [ 2 : 2 + l ]", "predictions": ["decode packet string into list of sensors ."], "references": ["decode a connect control packet ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 3860, "code": "def encode ( self ) : header = bytearray ( 1 ) var Header = bytearray ( 2 ) header [ 0 ] = 0x20 var Header [ 0 ] = self . session var Header [ 1 ] = self . result Code header . extend ( encode Length ( len ( var Header ) ) ) header . extend ( var Header ) self . encoded = header return str ( header ) if PY2 else bytes ( header )", "predictions": ["return a compressed header ."], "references": ["encode and store a connack control packet ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 3861, "code": "def decode ( self , packet ) : self . encoded = packet len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] self . session = ( packet remaining [ 0 ] & 0x01 ) == 0x01 self . result Code = int ( packet remaining [ 1 ] )", "predictions": ["decode a packet ."], "references": ["decode a connack control packet ."], "bleu": 0.36064528799877893, "rouge_l": 0.7721518987341772}
{"id": 3862, "code": "def decode ( self , packet ) : self . encoded = packet len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] self . msg Id = decode16Int ( packet remaining [ 0 : 2 ] ) self . topics = [ ] packet remaining = packet remaining [ 2 : ] while len ( packet remaining ) : topic , packet remaining = decode String ( packet remaining ) qos = int ( packet remaining [ 0 ] ) & 0x03 self . topics . append ( ( topic , qos ) ) packet remaining = packet remaining [ 1 : ]", "predictions": ["decode packet string into list of topics ."], "references": ["decode a subscribe control packet ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 3863, "code": "def encode ( self ) : header = bytearray ( 1 ) payload = bytearray ( ) var Header = encode16Int ( self . msg Id ) header [ 0 ] = 0x90 for code in self . granted : payload . append ( code [ 0 ] | ( 0x80 if code [ 1 ] == True else 0x00 ) ) header . extend ( encode Length ( len ( var Header ) + len ( payload ) ) ) header . extend ( var Header ) header . extend ( payload ) self . encoded = header return str ( header ) if PY2 else bytes ( header )", "predictions": ["encodes the message payload ."], "references": ["encode and store a suback control packet ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 3864, "code": "def decode ( self , packet ) : self . encoded = packet len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] self . msg Id = decode16Int ( packet remaining [ 0 : 2 ] ) self . topics = [ ] packet remaining = packet remaining [ 2 : ] while len ( packet remaining ) : l = decode16Int ( packet remaining [ 0 : 2 ] ) topic = packet remaining [ 2 : 2 + l ] . decode ( encoding = 'utf-8' ) self . topics . append ( topic ) packet remaining = packet remaining [ 2 + l : ]", "predictions": ["decode packet string into list of topics ."], "references": ["decode a unsuback control packet ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 3865, "code": "def encode ( self ) : header = bytearray ( 1 ) var Header = encode16Int ( self . msg Id ) header [ 0 ] = 0x B0 header . extend ( encode Length ( len ( var Header ) ) ) header . extend ( var Header ) self . encoded = header return str ( header ) if PY2 else bytes ( header )", "predictions": ["encodes the header of the variable ."], "references": ["encode and store an unsuback control packet"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3866, "code": "def decode ( self , packet ) : self . encoded = packet len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] self . dup = ( packet [ 0 ] & 0x08 ) == 0x08 self . qos = ( packet [ 0 ] & 0x06 ) >> 1 self . retain = ( packet [ 0 ] & 0x01 ) == 0x01 self . topic , = decode String ( packet remaining ) topic Len = decode16Int ( packet remaining ) if self . qos : self . msg Id = decode16Int ( packet remaining [ topic Len + 2 : topic Len + 4 ] ) self . payload = packet remaining [ topic Len + 4 : ] else : self . msg Id = None self . payload = packet remaining [ topic Len + 2 : ]", "predictions": ["decode the packet into the payload ."], "references": ["decode a publish control packet ."], "bleu": 0.22089591134157885, "rouge_l": 0.4680306905370844}
{"id": 3867, "code": "def decode ( self , packet ) : self . encoded = packet len Len = 1 while packet [ len Len ] & 0x80 : len Len += 1 packet remaining = packet [ len Len + 1 : ] self . msg Id = decode16Int ( packet remaining ) self . dup = ( packet [ 0 ] & 0x08 ) == 0x08", "predictions": ["decode a packet ."], "references": ["decode a pubrel control packet ."], "bleu": 0.36064528799877893, "rouge_l": 0.7721518987341772}
{"id": 3868, "code": "def refresh ( self ) : if self . comm . rank == 0 : self . blocks = self . list blocks ( ) else : self . blocks = None self . blocks = self . comm . bcast ( self . blocks )", "predictions": ["refresh the comm from the comm ."], "references": ["refresh the list of blocks to the disk collectively"], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 3869, "code": "def get total time span ( d ) : tmax = 0 for di in d . values ( ) : if di . u Time . max ( ) > tmax : tmax = di . u Time . max ( ) return tmax", "predictions": ["return the total time span span of the given time ."], "references": ["returns total length of analysis ."], "bleu": 0.1354599427337814, "rouge_l": 0.3727087576374745}
{"id": 3870, "code": "def get defined srms ( srm file ) : srms = read table ( srm file ) return np . asanyarray ( srms . index . unique ( ) )", "predictions": ["return the number of srms in the srm ."], "references": ["returns list of srms defined in the srm database"], "bleu": 0.31239399369202553, "rouge_l": 0.5555555555555556}
{"id": 3871, "code": "def read configuration ( config = 'DEFAULT' ) : , conf = read latoolscfg ( ) if config == 'DEFAULT' : config = conf [ 'DEFAULT' ] [ 'config' ] conf = dict ( conf [ config ] ) conf [ 'config' ] = config return conf", "predictions": ["read configuration from configuration file"], "references": ["read latools configuration file and return parameters as dict ."], "bleu": 0.14203729394569906, "rouge_l": 0.37731958762886597}
{"id": 3872, "code": "def print all ( ) : , conf = read latoolscfg ( ) default = conf [ 'DEFAULT' ] [ 'config' ] pstr = '\\n Currently defined L Atools configurations:\\n\\n' for s in conf . sections ( ) : if s == default : pstr += s + ' [DEFAULT]\\n' elif s == 'REPRODUCE' : pstr += s + ' [DO NOT ALTER]\\n' else : pstr += s + '\\n' for k , v in conf [ s ] . items ( ) : if k != 'config' : if v [ : 9 ] == 'resources' : v = pkgrs . resource filename ( 'latools' , v ) pstr += '   ' + k + ': ' + v + '\\n' pstr += '\\n' print ( pstr ) return", "predictions": ["update all the values in a string"], "references": ["prints all currently defined configurations ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 3873, "code": "def change default ( config ) : config file , cf = read latoolscfg ( ) if config not in cf . sections ( ) : raise Value Error ( \"\\n'{:s}' is not a defined configuration.\" . format ( config ) ) if config == 'REPRODUCE' : pstr = ( 'Are you SURE you want to set REPRODUCE as your default configuration?\\n' + '     ... this is an odd thing to be doing.' ) else : pstr = ( 'Are you sure you want to change the default configuration from {:s}' . format ( cf [ 'DEFAULT' ] [ 'config' ] ) + 'to {:s}?' . format ( config ) ) response = input ( pstr + '\\n> [N/y]: ' ) if response . lower ( ) == 'y' : cf . set ( 'DEFAULT' , 'config' , config ) with open ( config file , 'w' ) as f : cf . write ( f ) print ( '  Default changed!' ) else : print ( '  Done nothing.' )", "predictions": ["remove the item from the item base base self base base self base base"], "references": ["change the default configuration ."], "bleu": 0.08839374326825923, "rouge_l": 0.11509433962264153}
{"id": 3874, "code": "def autorange plot ( self , analyte = 'total counts' , gwin = 7 , swin = None , win = 20 , on mult = [ 1.5 , 1. ] , off mult = [ 1. , 1.5 ] , transform = 'log' ) : if analyte is None : sig = self . data [ 'total counts' ] elif analyte == 'total counts' : sig = self . data [ 'total counts' ] elif analyte in self . analytes : sig = self . focus [ analyte ] else : raise Value Error ( 'Invalid analyte.' ) if transform == 'log' : sig = np . log10 ( sig ) fig , axs = plot . autorange plot ( t = self . Time , sig = sig , gwin = gwin , swin = swin , win = win , on mult = on mult , off mult = off mult ) return fig , axs", "predictions": ["private method to name the update of the update"], "references": ["plot a detailed autorange report for this sample ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 3875, "code": "def rangecalc ( x , y = None , pad = 0.05 ) : mn = np . nanmin ( [ np . nanmin ( x ) , np . nanmin ( y ) ] ) mx = np . nanmax ( [ np . nanmax ( x ) , np . nanmax ( y ) ] ) rn = mx - mn return ( mn - pad * rn , mx + pad * rn )", "predictions": ["r update the update - vector of the segment segment"], "references": ["calculate padded range limits for axes ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 3876, "code": "def rangecalcx ( x , pad = 0.05 ) : mn = np . nanmin ( x ) mx = np . nanmax ( x ) rn = mx - mn return ( mn - pad * rn , mx + pad * rn )", "predictions": ["r add a vector vector"], "references": ["calculate padded range limits for axes ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 3877, "code": "def gen keywords ( * args : Union [ ANSI Colors , ANSI Styles ] , * * kwargs : Union [ ANSI Colors , ANSI Styles ] ) -> tuple : fields : tuple = tuple ( ) values : tuple = tuple ( ) for tpl in args : fields += tpl . fields values += tpl for prefix , tpl in kwargs . items ( ) : fields += tuple ( map ( lambda x : ' ' . join ( [ prefix , x ] ) , tpl . fields ) ) values += tpl return namedtuple ( 'ANSI Sequences' , fields ) ( * values )", "predictions": ["generate namedtuple errors from multiple fields ."], "references": ["generate single escape sequence mapping ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 3878, "code": "def dedup ( stack : tuple ) -> tuple : reducer = lambda x , y : x if y in x else x + ( y , ) return reduce ( reducer , stack , tuple ( ) )", "predictions": ["urlencode a stack stack ."], "references": ["remove duplicates from the stack in first - seen order ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 3879, "code": "def stderr ( a ) : return np . nanstd ( a ) / np . sqrt ( sum ( np . isfinite ( a ) ) )", "predictions": ["return - vector product for item - vector"], "references": ["calculate the standard error of a ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 3880, "code": "def filter nremoved ( self , filt = True , quiet = False ) : rminfo = { } for n in self . subsets [ 'All Samples' ] : s = self . data [ n ] rminfo [ n ] = s . filt nremoved ( filt ) if not quiet : max L = max ( [ len ( s ) for s in rminfo . keys ( ) ] ) print ( '{string:{number}s}' . format ( string = 'Sample ' , number = max L + 3 ) + '{total:4s}' . format ( total = 'tot' ) + '{removed:4s}' . format ( removed = 'flt' ) + '{percent:4s}' . format ( percent = '%rm' ) ) for k , ( ntot , nfilt , pcrm ) in rminfo . items ( ) : print ( '{string:{number}s}' . format ( string = k , number = max L + 3 ) + '{total:4.0f}' . format ( total = ntot ) + '{removed:4.0f}' . format ( removed = nfilt ) + '{percent:4.0f}' . format ( percent = pcrm ) ) return rminfo", "predictions": ["set the password in the table"], "references": ["report how many data are removed by the active filters ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 3881, "code": "def getstats ( self , save = True , filename = None , samples = None , subset = None , ablation time = False ) : slst = [ ] if samples is not None : subset = self . make subset ( samples ) samples = self . get samples ( subset ) for s in self . stats calced : for nm in [ n for n in samples if self . srm identifier not in n ] : if self . stats [ nm ] [ s ] . ndim == 2 : reps = np . arange ( self . stats [ nm ] [ s ] . shape [ - 1 ] ) ss = np . array ( [ s ] * reps . size ) nms = np . array ( [ nm ] * reps . size ) stdf = pd . Data Frame ( self . stats [ nm ] [ s ] . T , columns = self . stats [ nm ] [ 'analytes' ] , index = [ ss , nms , reps ] ) stdf . index . set names ( [ 'statistic' , 'sample' , 'rep' ] , inplace = True ) else : stdf = pd . Data Frame ( self . stats [ nm ] [ s ] , index = self . stats [ nm ] [ 'analytes' ] , columns = [ [ s ] , [ nm ] ] ) . T stdf . index . set names ( [ 'statistic' , 'sample' ] , inplace = True ) slst . append ( stdf ) out = pd . concat ( slst ) if ablation time : ats = self . ablation times ( samples = samples , subset = subset ) ats [ 'statistic' ] = 'nanmean' ats . set index ( 'statistic' , append = True , inplace = True ) ats = ats . reorder levels ( [ 'statistic' , 'sample' , 'rep' ] ) out = out . join ( ats ) out . drop ( self . internal standard , 1 , inplace = True ) if save : if filename is None : filename = 'stat export.csv' out . to csv ( self . export dir + '/' + filename ) self . stats df = out return out", "predictions": ["reorder the data of the data ."], "references": ["return pandas dataframe of all sample statistics ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 3882, "code": "def minimal export traces ( self , outdir = None , analytes = None , samples = None , subset = 'All Analyses' ) : if analytes is None : analytes = self . analytes elif isinstance ( analytes , str ) : analytes = [ analytes ] if samples is not None : subset = self . make subset ( samples ) samples = self . get samples ( subset ) focus stage = 'rawdata' if not os . path . isdir ( outdir ) : os . mkdir ( outdir ) for s in samples : d = self . data [ s ] . data [ focus stage ] out = Bunch ( ) for a in analytes : out [ a ] = d [ a ] out = pd . Data Frame ( out , index = self . data [ s ] . Time ) out . index . name = 'Time' d = dateutil . parser . parse ( self . data [ s ] . meta [ 'date' ] ) header = [ % ( time . strftime ( '%Y:%m:%d %H:%M:%S' ) ) , , , '#' , % ( s ) , + d . strftime ( '%Y-%m-%d %H:%M:%S' ) ] header = '\\n' . join ( header ) + '\\n' csv = out . to csv ( ) with open ( '%s/%s.csv' % ( outdir , s ) , 'w' ) as f : f . write ( header ) f . write ( csv ) return", "predictions": ["cipher the cipher traces traces"], "references": ["used for exporting minimal dataset . don t use ."], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 3883, "code": "def save log ( self , directory = None , logname = None , header = None ) : if directory is None : directory = self . export dir if not os . path . isdir ( directory ) : directory = os . path . dirname ( directory ) if logname is None : logname = 'analysis.lalog' if header is None : header = self . log header ( ) loc = logging . write logfile ( self . log , header , os . path . join ( directory , logname ) ) return loc", "predictions": ["get the mode of the mode"], "references": ["save analysis . lalog in specified location"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 3884, "code": "def calc windows ( fn , s , min points ) : max points = np . sum ( ~ np . isnan ( s ) ) n points = max points - min points out = np . full ( ( n points , s . size ) , np . nan ) ind = ~ np . isnan ( s ) s = s [ ind ] for i , w in enumerate ( range ( min points , s . size ) ) : r = rolling window ( s , w , pad = np . nan ) out [ i , ind ] = np . apply along axis ( fn , 1 , r ) return out", "predictions": ["connect windows to the windows points"], "references": ["apply fn to all contiguous regions in s that have at least min_points ."], "bleu": 0.05822753005110548, "rouge_l": 0.09327217125382263}
{"id": 3885, "code": "def calc window mean std ( s , min points , ind = None ) : max points = np . sum ( ~ np . isnan ( s ) ) n points = max points - min points mean = np . full ( ( n points , s . size ) , np . nan ) std = np . full ( ( n points , s . size ) , np . nan ) if ind is None : ind = ~ np . isnan ( s ) else : ind = ind & ~ np . isnan ( s ) s = s [ ind ] for i , w in enumerate ( range ( min points , s . size ) ) : r = rolling window ( s , w , pad = np . nan ) mean [ i , ind ] = r . sum ( 1 ) / w std [ i , ind ] = ( ( ( r - mean [ i , ind ] [ : , np . newaxis ] ) ** 2 ) . sum ( 1 ) / ( w - 1 ) ) ** 0.5 return mean , std", "predictions": ["calculates the mean mean mean dup"], "references": ["apply fn to all contiguous regions in s that have at least min_points ."], "bleu": 0.048963321289052536, "rouge_l": 0.0}
{"id": 3886, "code": "def bayes scale ( s ) : if sum ( ~ np . isnan ( s ) ) > 1 : bm , bv , bs = bayes mvs ( s [ ~ np . isnan ( s ) ] ) return ( s - bm . statistic ) / bs . statistic else : return np . full ( s . shape , np . nan )", "predictions": ["scale scale scale scale"], "references": ["remove mean and divide by standard deviation using bayes_kvm statistics ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 3887, "code": "def median scaler ( s ) : if sum ( ~ np . isnan ( s ) ) > 2 : ss = s [ ~ np . isnan ( s ) ] median = np . median ( ss ) IQR = np . diff ( np . percentile ( ss , [ 25 , 75 ] ) ) return ( s - median ) / IQR else : return np . full ( s . shape , np . nan )", "predictions": ["make the make scaler scaler scaler scaler"], "references": ["remove median divide by iqr ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 3888, "code": "def clear ( self ) : self . components = { } self . info = { } self . params = { } self . switches = { } self . keys = { } self . index = { } self . sets = { } self . maxset = - 1 self . n = 0 for a in self . analytes : self . switches [ a ] = { } return", "predictions": ["connect to all the components and analytes"], "references": ["clear all filters ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 3889, "code": "def clean ( self ) : for f in sorted ( self . components . keys ( ) ) : unused = not any ( self . switches [ a ] [ f ] for a in self . analytes ) if unused : self . remove ( f )", "predictions": ["remove all name of the ."], "references": ["remove unused filters ."], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 3890, "code": "def get info ( self ) : out = '' for k in sorted ( self . components . keys ( ) ) : out += '{:s}: {:s}' . format ( k , self . info [ k ] ) + '\\n' return ( out )", "predictions": ["return the current . info"], "references": ["get info for all filters ."], "bleu": 0.24736929544091937, "rouge_l": 0.1788856304985337}
{"id": 3891, "code": "def log ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : a = func ( self , * args , * * kwargs ) self . log . append ( func . name + ' :: args={} kwargs={}' . format ( args , kwargs ) ) return a return wrapper", "predictions": ["decorator to decode a decode method ."], "references": ["function for logging method calls and parameters"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 3892, "code": "def autologin ( function , timeout = TIMEOUT ) : @ wraps ( function ) async def wrapper ( self , * args , * * kwargs ) : \"\"\"Wrap a function with timeout.\"\"\" try : async with async timeout . timeout ( timeout ) : return await function ( self , * args , * * kwargs ) except ( asyncio . Timeout Error , Client Error , Error ) : pass LOGGER . debug ( \"autologin\" ) try : async with async timeout . timeout ( timeout ) : await self . login ( ) return await function ( self , * args , * * kwargs ) except ( asyncio . Timeout Error , Client Error , Error ) : raise Error ( str ( function ) ) return wrapper", "predictions": ["simple decorator to make a self len len len len len len len len len len len len len len len len len len len len len len len len len"], "references": ["decorator that will try to login and redo an action before failing ."], "bleu": 0.04317900023606586, "rouge_l": 0.09814963797264682}
{"id": 3893, "code": "async def get information ( ) : jar = aiohttp . Cookie Jar ( unsafe = True ) websession = aiohttp . Client Session ( cookie jar = jar ) modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) await modem . login ( password = sys . argv [ 2 ] ) result = await modem . information ( ) for sms in result . sms : pprint . pprint ( sms ) await modem . logout ( ) await websession . close ( )", "predictions": ["get self = 0 = 1 = 0 = 0 = 0 = 1 = 1 = 0 = 1 = 0 = 0 = 1 = 1 = 0 ="], "references": ["example of printing the inbox ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3894, "code": "async def send message ( ) : jar = aiohttp . Cookie Jar ( unsafe = True ) websession = aiohttp . Client Session ( cookie jar = jar ) modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) await modem . login ( password = sys . argv [ 2 ] ) await modem . sms ( phone = sys . argv [ 3 ] , message = sys . argv [ 4 ] ) await modem . logout ( ) await websession . close ( )", "predictions": ["qos to modem and logout = ."], "references": ["example of sending a message ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 3895, "code": "async def get information ( ) : jar = aiohttp . Cookie Jar ( unsafe = True ) websession = aiohttp . Client Session ( cookie jar = jar ) try : modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) await modem . login ( password = sys . argv [ 2 ] ) result = await modem . information ( ) print ( \"upstream: {}\" . format ( result . upstream ) ) print ( \"serial number: {}\" . format ( result . serial number ) ) print ( \"wire connected: {}\" . format ( result . wire connected ) ) print ( \"mobile connected: {}\" . format ( result . mobile connected ) ) print ( \"connection text: {}\" . format ( result . connection text ) ) print ( \"connection type: {}\" . format ( result . connection type ) ) print ( \"current nw service type: {}\" . format ( result . current nw service type ) ) print ( \"current ps service type: {}\" . format ( result . current ps service type ) ) print ( \"register network display: {}\" . format ( result . register network display ) ) print ( \"roaming: {}\" . format ( result . roaming ) ) print ( \"radio quality: {}\" . format ( result . radio quality ) ) print ( \"rx level: {}\" . format ( result . rx level ) ) print ( \"tx level: {}\" . format ( result . tx level ) ) print ( \"current band: {}\" . format ( result . current band ) ) print ( \"cell id: {}\" . format ( result . cell id ) ) await modem . logout ( ) except eternalegypt . Error : print ( \"Could not login\" ) await websession . close ( )", "predictions": ["get modem self payload payload payload payload payload payload payload payload payload payload payload payload payload payload payload payload payload payload"], "references": ["example of printing the current upstream ."], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 3896, "code": "async def set failover mode ( mode ) : jar = aiohttp . Cookie Jar ( unsafe = True ) websession = aiohttp . Client Session ( cookie jar = jar ) try : modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) await modem . login ( password = sys . argv [ 2 ] ) await modem . set failover mode ( mode ) await modem . logout ( ) except eternalegypt . Error : print ( \"Could not login\" ) await websession . close ( )", "predictions": ["set mode mode mode"], "references": ["example of printing the current upstream ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 3897, "code": "def nbviewer link ( url ) : if six . PY2 : from urlparse import urlparse as urlsplit else : from urllib . parse import urlsplit info = urlsplit ( url ) domain = info . netloc url type = 'github' if domain == 'github.com' else 'url' return 'https://nbviewer.jupyter.org/%s%s' % ( url type , info . path )", "predictions": ["return a link link"], "references": ["return the link to the jupyter nbviewer for the given notebook url"], "bleu": 0.0538140946637381, "rouge_l": 0.22932330827067668}
{"id": 3898, "code": "def thumbnail div ( self ) : return self . THUMBNAIL TEMPLATE . format ( snippet = self . get description ( ) [ 1 ] , thumbnail = self . thumb file , ref name = self . reference )", "predictions": ["returns the decode + decode + config"], "references": ["the string for creating the thumbnail of this example"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 3899, "code": "def code div ( self ) : code example = self . code example if code example is None : return None return self . CODE TEMPLATE . format ( snippet = self . get description ( ) [ 1 ] , code = code example , ref name = self . reference )", "predictions": ["the decode decode decode to the & example"], "references": ["the string for creating a code example for the gallery"], "bleu": 0.1485237584394808, "rouge_l": 0.21785714285714283}
{"id": 3900, "code": "def code example ( self ) : if self . code example is not None : return self . code example return getattr ( self . nb . metadata , 'code example' , None )", "predictions": ["the refresh of the refresh refresh refresh refresh comm comm comm comm comm comm comm comm"], "references": ["the code example out of the notebook metadata"], "bleu": 0.10878661088699644, "rouge_l": 0.2659883720930233}
{"id": 3901, "code": "def supplementary files ( self ) : if self . supplementary files is not None : return self . supplementary files return getattr ( self . nb . metadata , 'supplementary files' , None )", "predictions": ["get the get total total total total . . . . . . ."], "references": ["the supplementary files of this notebook"], "bleu": 0.08839374326825923, "rouge_l": 0.10777385159010601}
{"id": 3902, "code": "def other supplementary files ( self ) : if self . other supplementary files is not None : return self . other supplementary files return getattr ( self . nb . metadata , 'other supplementary files' , None )", "predictions": ["np - defined defined files = defined in this node"], "references": ["the supplementary files of this notebook"], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 3903, "code": "def url ( self ) : if self . url is not None : url = self . url else : url = getattr ( self . nb . metadata , 'url' , None ) if url is not None : return nbviewer link ( url )", "predictions": ["returns the read read read read - only read - only read - only - only read read - only read read - only read - only link for this resource"], "references": ["the url on jupyter nbviewer for this notebook or none if unknown"], "bleu": 0.055177848898164926, "rouge_l": 0.1516155758077879}
{"id": 3904, "code": "def get out file ( self , ending = 'rst' ) : return os . path . splitext ( self . outfile ) [ 0 ] + os . path . extsep + ending", "predictions": ["return the path to the out of the file"], "references": ["get the output file with the specified ending"], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 3905, "code": "def create py ( self , nb , force = False ) : if list ( map ( int , re . findall ( '\\d+' , nbconvert . version ) ) ) >= [ 4 , 2 ] : py file = os . path . basename ( self . py file ) else : py file = self . py file try : level = logger . logger . level except Attribute Error : level = logger . level spr . call ( [ 'jupyter' , 'nbconvert' , '--to=python' , '--output=' + py file , '--log-level=%s' % level , self . outfile ] ) with open ( self . py file ) as f : py content = f . read ( ) py content = re . sub ( '^\\s*get ipython\\(\\).magic.*' , , py content , flags = re . MULTILINE ) with open ( self . py file , 'w' ) as f : f . write ( py content )", "predictions": ["create the config file ."], "references": ["create the python script from the notebook node"], "bleu": 0.1971902775417715, "rouge_l": 0.2953995157384988}
{"id": 3906, "code": "def data download ( self , files ) : if len ( files ) > 1 : return self . DATA DOWNLOAD % ( ( '\\n\\n' + ' ' * 8 ) + ( '\\n' + ' ' * 8 ) . join ( '* :download:`%s`' % f for f in files ) ) return self . DATA DOWNLOAD % ':download:`%s`' % files [ 0 ]", "predictions": ["download data from the given files ."], "references": ["create the rst string to download supplementary data"], "bleu": 0.19148978368719022, "rouge_l": 0.2634989200863931}
{"id": 3907, "code": "def create thumb ( self ) : thumbnail figure = self . copy thumbnail figure ( ) if thumbnail figure is not None : if isinstance ( thumbnail figure , six . string types ) : pic = thumbnail figure else : pic = self . pictures [ thumbnail figure ] self . save thumbnail ( pic ) else : for pic in self . pictures [ : : - 1 ] : if pic . endswith ( 'png' ) : self . save thumbnail ( pic ) return", "predictions": ["create the thumb figure out the thumbnail figure ."], "references": ["create the thumbnail for html output"], "bleu": 0.22089591134157885, "rouge_l": 0.4149659863945578}
{"id": 3908, "code": "def get description ( self ) : def split header ( s , get header = True ) : s = s . lstrip ( ) . rstrip ( ) parts = s . splitlines ( ) if parts [ 0 ] . startswith ( '#' ) : if get header : header = re . sub ( '#+\\s*' , '' , parts . pop ( 0 ) ) if not parts : return header , '' else : header = '' rest = '\\n' . join ( parts ) . lstrip ( ) . split ( '\\n\\n' ) desc = rest [ 0 ] . replace ( '\\n' , ' ' ) return header , desc else : if get header : if parts [ 0 ] . startswith ( ( '=' , '-' ) ) : parts = parts [ 1 : ] header = parts . pop ( 0 ) if parts and parts [ 0 ] . startswith ( ( '=' , '-' ) ) : parts . pop ( 0 ) if not parts : return header , '' else : header = '' rest = '\\n' . join ( parts ) . lstrip ( ) . split ( '\\n\\n' ) desc = rest [ 0 ] . replace ( '\\n' , ' ' ) return header , desc first cell = self . nb [ 'cells' ] [ 0 ] if not first cell [ 'cell type' ] == 'markdown' : return '' , '' header , desc = split header ( first cell [ 'source' ] ) if not desc and len ( self . nb [ 'cells' ] ) > 1 : second cell = self . nb [ 'cells' ] [ 1 ] if second cell [ 'cell type' ] == 'markdown' : , desc = split header ( second cell [ 'source' ] , False ) return header , desc", "predictions": ["get the description of the cell"], "references": ["get summary and description of this notebook"], "bleu": 0.2644358066258934, "rouge_l": 0.45522388059701485}
{"id": 3909, "code": "def save thumbnail ( self , image path ) : thumb dir = os . path . join ( os . path . dirname ( image path ) , 'thumb' ) create dirs ( thumb dir ) thumb file = os . path . join ( thumb dir , '%s thumb.png' % self . reference ) if os . path . exists ( image path ) : logger . info ( 'Scaling %s to thumbnail %s' , image path , thumb file ) self . scale image ( image path , thumb file , 400 , 280 ) self . thumb file = thumb file", "predictions": ["save the thumbnail to the given path"], "references": ["save the thumbnail image"], "bleu": 0.345720784641941, "rouge_l": 0.5736677115987461}
{"id": 3910, "code": "def copy thumbnail figure ( self ) : ret = None if self . thumbnail figure is not None : if not isstring ( self . thumbnail figure ) : ret = self . thumbnail figure else : ret = osp . join ( osp . dirname ( self . outfile ) , osp . basename ( self . thumbnail figure ) ) copyfile ( self . thumbnail figure , ret ) return ret elif hasattr ( self . nb . metadata , 'thumbnail figure' ) : if not isstring ( self . nb . metadata . thumbnail figure ) : ret = self . nb . metadata . thumbnail figure else : ret = osp . join ( osp . dirname ( self . outfile ) , 'images' , osp . basename ( self . nb . metadata . thumbnail figure ) ) copyfile ( osp . join ( osp . dirname ( self . infile ) , self . nb . metadata . thumbnail figure ) , ret ) return ret", "predictions": ["copies the thumbnail figure to the thumbnail figure ."], "references": ["the integer of the thumbnail figure"], "bleu": 0.2777619034011791, "rouge_l": 0.5532879818594103}
{"id": 3911, "code": "def get db change languages ( self , field name , db table fields ) : for lang code , lang name in get languages ( ) : if get real fieldname ( field name , lang code ) not in db table fields : yield lang code for db table field in db table fields : pattern = re . compile ( '^%s (?P<lang>\\w{2})$' % field name ) m = pattern . match ( db table field ) if not m : continue lang = m . group ( 'lang' ) yield lang", "predictions": ["get all languages change from db ."], "references": ["get only db changes fields"], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 3912, "code": "def pre save ( self , model instance , add ) : file = getattr ( model instance , self . attname ) if file and not file . committed : image file = file if self . resize source to : file . seek ( 0 ) image file = processors . process ( file , self . resize source to ) image file = post processors . process ( image file , self . resize source to ) filename = str ( shortuuid . uuid ( ) ) + os . path . splitext ( file . name ) [ 1 ] file . save ( filename , image file , save = False ) return file", "predictions": ["saves the image and returns its value ."], "references": ["process the source image through the defined processors ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 3913, "code": "def refresh cache ( self ) : self . thumbnails = { } metadatas = self . metadata backend . get thumbnails ( self . source image . name ) for metadata in metadatas : self . thumbnails [ metadata . size ] = Thumbnail ( metadata = metadata , storage = self . storage )", "predictions": ["load the cache from the source image ."], "references": ["populate self . _thumbnails ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 3914, "code": "def all ( self ) : if self . thumbnails is not None : return self . thumbnails self . refresh cache ( ) return self . thumbnails", "predictions": ["return all thumbnails in this collection ."], "references": ["return all thumbnails in a dict format ."], "bleu": 0.46905226098954195, "rouge_l": 0.6587473002159828}
{"id": 3915, "code": "def create ( self , size ) : thumbnail = images . create ( self . source image . name , size , self . metadata backend , self . storage ) return thumbnail", "predictions": ["create a new thumbnail ."], "references": ["creates and return a thumbnail of a given size ."], "bleu": 0.11943865131127647, "rouge_l": 0.37731958762886597}
{"id": 3916, "code": "def delete ( self , size ) : images . delete ( self . source image . name , size , self . metadata backend , self . storage ) del ( self . thumbnails [ size ] )", "predictions": ["delete this image ."], "references": ["deletes a thumbnail of a given size"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 3917, "code": "def get ( source name , size , metadata backend = None , storage backend = None ) : if storage backend is None : storage backend = backends . storage . get backend ( ) if metadata backend is None : metadata backend = backends . metadata . get backend ( ) metadata = metadata backend . get thumbnail ( source name , size ) if metadata is None : return None else : return Thumbnail ( metadata = metadata , storage = storage backend )", "predictions": ["return the backends for the given source name ."], "references": ["returns a thumbnail instance or none if thumbnail does not yet exist ."], "bleu": 0.09049614828481034, "rouge_l": 0.08802308802308802}
{"id": 3918, "code": "def delete ( source name , size , metadata backend = None , storage backend = None ) : if storage backend is None : storage backend = backends . storage . get backend ( ) if metadata backend is None : metadata backend = backends . metadata . get backend ( ) storage backend . delete ( get thumbnail name ( source name , size ) ) metadata backend . delete thumbnail ( source name , size )", "predictions": ["delete a source ."], "references": ["deletes a thumbnail file and its relevant metadata ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 3919, "code": "def jsonex api ( f ) : @ wraps ( f ) def wrapper ( * args , * * kwargs ) : try : code , res = 200 , f ( * args , * * kwargs ) except HTTP Exception as e : code , res = e . code , { 'error' : e } except Exception as e : code , res = 500 , { 'error' : e } logger . exception ( 'Method error' ) response = make response ( jsonex dumps ( res ) , code ) response . headers [ 'Content-Type' ] = 'application/json' return response return wrapper", "predictions": ["decorator to add jsonex api api ."], "references": ["view wrapper for jsonex responses . catches exceptions as well"], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 3920, "code": "def estimate tx gas with web3 ( self , safe address : str , to : str , value : int , data : bytes ) -> int : return self . ethereum client . estimate gas ( safe address , to , value , data , block identifier = 'pending' )", "predictions": ["estimate a tx with a tx ."], "references": ["estimate tx gas using web3"], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 3921, "code": "def has bad headers ( self , default from = None ) : sender = self . sender or default from reply to = self . reply to or '' for val in [ self . subject , sender , reply to ] + self . recipients : for c in '\\r\\n' : if c in val : return True return False", "predictions": ["returns true if any of the reply has any bad headers ."], "references": ["checks for bad headers i . e . newlines in subject sender or recipients ."], "bleu": 0.11443781455557568, "rouge_l": 0.21785714285714283}
{"id": 3922, "code": "def from module ( module name ) : d = importlib . import module ( module name ) config = { } for key in dir ( d ) : if key . isupper ( ) : config [ key ] = getattr ( d , key ) return Config ( config )", "predictions": ["return a module from a module name ."], "references": ["load a configuration module and return a config"], "bleu": 0.239802967618271, "rouge_l": 0.375}
{"id": 3923, "code": "def register resources ( self , * * resources ) : for key , resource in resources . items ( ) : if key in self . resources : raise Already Exists Exception ( 'A Service for {} is already registered.' . format ( key ) ) self . init resource ( key , resource )", "predictions": ["register all resources in the resource ."], "references": ["register resources with the resourcemanager ."], "bleu": 0.23356898886410002, "rouge_l": 0.6240409207161125}
{"id": 3924, "code": "def require ( self , key ) : value = self . get ( key ) if not value : raise Value Error ( '\"{}\" is empty.' . format ( key ) ) return value", "predictions": ["require the value of the key ."], "references": ["raises an exception if value for key is empty ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 3925, "code": "def exit ( self , obj , type , value , traceback ) : if type is None : try : obj . next ( ) except Stop Iteration : return else : raise Runtime Error ( '{} yielded more than once.' . format ( obj ) ) else : try : obj . throw ( type , value , traceback ) raise Runtime Error ( '{} did not close after throw()' . format ( obj ) ) except Stop Iteration as exc : return exc is not value except : # if sys . exc info ( ) [ 1 ] is not value : raise", "predictions": ["exit the traceback of the given type ."], "references": ["teardown a resource or middleware ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 3926, "code": "def samefile ( path1 , path2 ) : info1 = fs . getfileinfo ( path1 ) info2 = fs . getfileinfo ( path2 ) return ( info1 . dw Volume Serial Number == info2 . dw Volume Serial Number and info1 . n File Index High == info2 . n File Index High and info1 . n File Index Low == info2 . n File Index Low )", "predictions": ["check if two image objects are still still samefile"], "references": ["returns true if path1 and path2 refer to the same file ."], "bleu": 0.10113117135596685, "rouge_l": 0.0928462709284627}
{"id": 3927, "code": "def create ( source , link name ) : success = False if not os . path . isdir ( source ) : raise Exception ( \"%s is not a directory\" % source ) if os . path . exists ( link name ) : raise Exception ( \"%s: junction link name already exists\" % link name ) link name = os . path . abspath ( link name ) os . mkdir ( link name ) hlink = Create File ( link name , fs . GENERIC WRITE , fs . FILE SHARE READ | fs . FILE SHARE WRITE , None , fs . OPEN EXISTING , fs . FILE FLAG OPEN REPARSE POINT | fs . FILE FLAG BACKUP SEMANTICS , None ) try : if hlink == fs . INVALID HANDLE VALUE : raise Win Error ( ) srcvolpath = unparsed convert ( source ) ( junctioninfo , infolen ) = new junction reparse buffer ( srcvolpath ) dummy = DWORD ( 0 ) res = Device Io Control ( hlink , FSCTL SET REPARSE POINT , byref ( junctioninfo ) , infolen , None , 0 , byref ( dummy ) , None ) if res == 0 : raise Win Error ( ) success = True finally : if hlink != fs . INVALID HANDLE VALUE : Close Handle ( hlink ) if not success : os . rmdir ( link name )", "predictions": ["create a new junction link"], "references": ["create a junction at link_name pointing to source ."], "bleu": 0.17348474258688365, "rouge_l": 0.40757238307349664}
{"id": 3928, "code": "def initialize logger ( args ) : global log filename log filename = os . path . join ( os . getcwd ( ) , \"jacquard.log\" ) if args . log file : validate log file ( args . log file ) log filename = args . log file logging . basic Config ( format = FILE LOG FORMAT , level = \"DEBUG\" , datefmt = DATE FORMAT , filename = log filename ) global verbose if args . verbose : verbose = args . verbose start time = datetime . now ( ) . strftime ( DATE FORMAT ) global logging dict logging dict = { 'user' : getpass . getuser ( ) , 'host' : socket . gethostname ( ) , 'start time' : start time , 'tool' : args . subparser name }", "predictions": ["initialize the logger with the logging information ."], "references": ["sets command name and formatting for subsequent calls to logger"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 3929, "code": "def error ( self , message ) : message = self . remessage invalid subparser ( message ) raise utils . Usage Error ( message )", "predictions": ["set the error message"], "references": ["suppress default exit behavior"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 3930, "code": "def read ( * paths ) : with open ( os . path . join ( * paths ) , 'r' ) as filename : return filename . read ( )", "predictions": ["read the contents of a file ."], "references": ["build a file path from * paths * and return the contents ."], "bleu": 0.13653323887370866, "rouge_l": 0.2846034214618974}
{"id": 3931, "code": "def prefix line terminator ( self , data ) : for t in self . LINE TERMINATORS : if data . startswith ( t ) : return t return None", "predictions": ["return the prefix of the given data line ."], "references": ["return line terminator data begins with or none ."], "bleu": 0.17747405280050263, "rouge_l": 0.3333333333333333}
{"id": 3932, "code": "def suffix line terminator ( self , data ) : for t in self . LINE TERMINATORS : if data . endswith ( t ) : return t return None", "predictions": ["returns the suffix of the given data line ."], "references": ["return line terminator data ends with or none ."], "bleu": 0.16784459625186196, "rouge_l": 0.2222222222222222}
{"id": 3933, "code": "def tail ( self , lines = 10 ) : self . file . seek ( 0 , SEEK END ) for i in range ( lines ) : if self . seek previous line ( ) == - 1 : break data = self . file . read ( ) for t in self . LINE TERMINATORS : if data . endswith ( t ) : data = data [ : - len ( t ) ] break if data : return self . splitlines ( data ) else : return [ ]", "predictions": ["read lines from the file ."], "references": ["return the last lines of the file ."], "bleu": 0.31149111610852515, "rouge_l": 0.5570776255707762}
{"id": 3934, "code": "def head ( self , lines = 10 ) : self . file . seek ( 0 ) for i in range ( lines ) : if self . seek next line ( ) == - 1 : break end pos = self . file . tell ( ) self . file . seek ( 0 ) data = self . file . read ( end pos ) for t in self . LINE TERMINATORS : if data . endswith ( t ) : data = data [ : - len ( t ) ] break if data : return self . splitlines ( data ) else : return [ ]", "predictions": ["read a line from the file ."], "references": ["return the top lines of the file ."], "bleu": 0.29969770769039067, "rouge_l": 0.3952483801295896}
{"id": 3935, "code": "def format tags ( self ) : tags = Vcf Record . EMPTY SET if self . sample tag values : first sample = list ( self . sample tag values . keys ( ) ) [ 0 ] tags = set ( self . sample tag values [ first sample ] . keys ( ) ) return tags", "predictions": ["return formatted tags as a string ."], "references": ["returns set of format tags ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 3936, "code": "def join info fields ( self ) : if self . info dict : info fields = [ ] if len ( self . info dict ) > 1 : self . info dict . pop ( \".\" , None ) for field , value in self . info dict . items ( ) : if field == value : info fields . append ( value ) else : info fields . append ( \"=\" . join ( [ field , value ] ) ) self . info = \";\" . join ( info fields ) else : self . info = \".\"", "predictions": ["get the out of the out file file ."], "references": ["updates info attribute from info dict ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 3937, "code": "def format field ( self ) : format field = \".\" if self . sample tag values : first sample = list ( self . sample tag values . keys ( ) ) [ 0 ] tag names = self . sample tag values [ first sample ] . keys ( ) if tag names : format field = \":\" . join ( tag names ) return format field", "predictions": ["create string with if it is not passed to the sample"], "references": ["returns string representation of format field ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 3938, "code": "def text ( self ) : stringifier = [ self . chrom , self . pos , self . vcf id , self . ref , self . alt , self . qual , self . filter , self . info , self . format field ( ) ] for sample in self . sample tag values : stringifier . append ( self . sample field ( sample ) ) return \"\\t\" . join ( stringifier ) + \"\\n\"", "predictions": ["in the return of the return data if any"], "references": ["returns tab - delimited newline terminated string of vcfrecord ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 3939, "code": "def add or replace filter ( self , new filter ) : if self . filter . lower ( ) in self . FILTERS TO REPLACE : self . filter = new filter elif new filter not in self . filter . split ( \";\" ) : self . filter = \";\" . join ( [ self . filter , new filter ] )", "predictions": ["replace a figure to the self if it doesn t already exist if there is a figure is true"], "references": ["replaces null or blank filter or adds filter to existing list ."], "bleu": 0.06439931429457924, "rouge_l": 0.06725468577728776}
{"id": 3940, "code": "def add product error ( self , product , error ) : self . add error ( self . field name ( product ) , error )", "predictions": ["get the error error error"], "references": ["adds an error to the given product s field"], "bleu": 0.13575914775035755, "rouge_l": 0.1358574610244989}
{"id": 3941, "code": "def model fields form factory ( model ) : fields = model . meta . get fields ( ) choices = [ ] for field in fields : if hasattr ( field , \"verbose name\" ) : choices . append ( ( field . name , field . verbose name ) ) class Model Fields Form ( forms . Form ) : fields = forms . Multiple Choice Field ( choices = choices , required = False , ) return Model Fields Form", "predictions": ["returns a list of thumbnail thumbnail thumbnail thumbnail . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["creates a form for specifying fields from a model to display ."], "bleu": 0.04317900023606586, "rouge_l": 0.10107705053852528}
{"id": 3942, "code": "def items pending or purchased ( self ) : status = [ commerce . Cart . STATUS PAID , commerce . Cart . STATUS ACTIVE ] return self . items ( status )", "predictions": ["outfile thumbnail thumbnail figure figure if any is thumbnail is thumbnail is thumbnail is thumbnail is thumbnail is thumbnail is thumbnail is thumbnail is thumbnail is thumbnail is thumbnail is thumbnail"], "references": ["returns the items that this user has purchased or has pending ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 3943, "code": "def iter osm notes ( feed limit = 25 , interval = 60 , parse timestamps = True ) : last seen guid = None while True : u = urllib2 . urlopen ( 'https://www.openstreetmap.org/api/0.6/notes/feed?limit=%d' % feed limit ) tree = etree . parse ( u ) new notes = [ ] for note item in tree . xpath ( '/rss/channel/item' ) : title = note item . xpath ( 'title' ) [ 0 ] . text if title . startswith ( 'new note (' ) : action = 'create' elif title . startswith ( 'new comment (' ) : action = 'comment' elif title . startswith ( 'closed note (' ) : action = 'close' guid = note item . xpath ( 'link' ) [ 0 ] . text if last seen guid == guid : break elif last seen guid == None : last seen guid = guid else : note id = int ( guid . split ( '/' ) [ - 1 ] . split ( '#c' ) [ 0 ] ) new notes . append ( ( action , get note ( note id , parse timestamps ) ) ) for note in reversed ( new notes ) : yield note yield model . Finished ( None , None ) time . sleep ( interval )", "predictions": ["return an iterator over all db change in the db = table = table"], "references": ["parses the global osm notes feed and yields as much note information as possible ."], "bleu": 0.0823001243157892, "rouge_l": 0.06853932584269662}
{"id": 3944, "code": "def passes filter ( self , user ) : cls = type ( self . condition ) qs = cls . objects . filter ( pk = self . condition . id ) return self . condition in self . pre filter ( qs , user )", "predictions": ["return the queryset to all the if any of the model is pre - filtered save ."], "references": ["returns true if the condition passes the filter"], "bleu": 0.0859076483566362, "rouge_l": 0.17110799438990182}
{"id": 3945, "code": "def apply voucher ( self , voucher code ) : voucher = inventory . Voucher . objects . get ( code = voucher code . upper ( ) ) if voucher in self . cart . vouchers . all ( ) : return self . test voucher ( voucher ) self . cart . vouchers . add ( voucher )", "predictions": ["refresh all cache cache cache"], "references": ["applies the voucher with the given code to this cart ."], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 3946, "code": "def recalculate discounts ( self ) : commerce . Discount Item . objects . filter ( cart = self . cart ) . delete ( ) product items = self . cart . productitem set . all ( ) . select related ( \"product\" , \"product category\" ) . order by ( \"-product price\" ) products = [ i . product for i in product items ] discounts = Discount Controller . available discounts ( self . cart . user , [ ] , products , ) for item in product items : self . add discount ( item . product , item . quantity , discounts )", "predictions": ["refreshes all related related items"], "references": ["calculates all of the discounts available for this product ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 3947, "code": "def rows ( self , content type ) : for row in self . data : yield [ self . cell text ( content type , i , cell ) for i , cell in enumerate ( row ) ]", "predictions": ["generate create create create create create create create create strings source source source source source source ."], "references": ["returns the data rows for the table ."], "bleu": 0.07223943354597204, "rouge_l": 0.08555399719495091}
{"id": 3948, "code": "def get form ( self , request ) : if self . form type is not None : form = self . form type ( request . GET ) form . is valid ( ) else : form = None return form", "predictions": ["return form form form"], "references": ["creates an instance of self . form_type using request . get"], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 3949, "code": "def reports list ( request ) : reports = [ ] for report in get all reports ( ) : reports . append ( { \"name\" : report . name , \"url\" : reverse ( report ) , \"description\" : report . doc , } ) reports . sort ( key = lambda report : report [ \"name\" ] ) ctx = { \"reports\" : reports , } return render ( request , \"registrasion/reports list.html\" , ctx )", "predictions": ["print the list get details of the storage is available"], "references": ["lists all of the reports currently available ."], "bleu": 0.17827531042796255, "rouge_l": 0.34014869888475835}
{"id": 3950, "code": "def sales payment summary ( ) : def value or zero ( aggregate , key ) : return aggregate [ key ] or 0 def sum amount ( payment set ) : a = payment set . values ( \"amount\" ) . aggregate ( total = Sum ( \"amount\" ) ) return value or zero ( a , \"total\" ) headings = [ \"Category\" , \"Total\" ] data = [ ] sales = commerce . Line Item . objects . filter ( invoice status = commerce . Invoice . STATUS PAID , ) . values ( \"price\" , \"quantity\" ) . aggregate ( total = Sum ( F ( \"price\" ) * F ( \"quantity\" ) , output field = CURRENCY ( ) ) , ) sales = value or zero ( sales , \"total\" ) all payments = sum amount ( commerce . Payment Base . objects . all ( ) ) all credit notes = 0 - sum amount ( commerce . Credit Note . objects . all ( ) ) unclaimed credit notes = 0 - sum amount ( commerce . Credit Note . unclaimed ( ) ) claimed credit notes = sum amount ( commerce . Credit Note Application . objects . all ( ) ) refunded credit notes = 0 - sum amount ( commerce . Credit Note . refunded ( ) ) data . append ( [ \"Items on paid invoices\" , sales ] ) data . append ( [ \"All payments\" , all payments ] ) data . append ( [ \"Sales - Payments \" , sales - all payments ] ) data . append ( [ \"All credit notes\" , all credit notes ] ) data . append ( [ \"Credit notes paid on invoices\" , claimed credit notes ] ) data . append ( [ \"Credit notes refunded\" , refunded credit notes ] ) data . append ( [ \"Unclaimed credit notes\" , unclaimed credit notes ] ) data . append ( [ \"Credit notes - (claimed credit notes + unclaimed credit notes)\" , all credit notes - claimed credit notes - refunded credit notes - unclaimed credit notes ] ) return List Report ( \"Sales and Payments Summary\" , headings , data )", "predictions": ["get all the payments source source payment"], "references": ["summarises paid items and payments ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 3951, "code": "def payments ( ) : payments = commerce . Payment Base . objects . all ( ) return Queryset Report ( \"Payments\" , [ \"invoice id\" , \"id\" , \"reference\" , \"amount\" ] , payments , link view = views . invoice , )", "predictions": ["render payments payments and ."], "references": ["shows the history of payments into the system"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 3952, "code": "def credit note refunds ( ) : notes refunded = commerce . Credit Note . refunded ( ) return Queryset Report ( \"Credit note refunds\" , [ \"id\" , \"creditnoterefund reference\" , \"amount\" ] , notes refunded , link view = views . credit note , )", "predictions": ["returns the estimate of the estimate tx ."], "references": ["shows all of the credit notes that have been generated ."], "bleu": 0.15587146574232644, "rouge_l": 0.3070469798657718}
{"id": 3953, "code": "def discount status ( request , form ) : discounts = form . cleaned data [ \"discount\" ] items = commerce . Discount Item . objects . filter ( Q ( discount in = discounts ) , ) . select related ( \"cart\" , \"product\" , \"product category\" ) items = group by cart status ( items , [ \"discount\" ] , [ \"discount\" , \"discount description\" ] , ) headings = [ \"Discount\" , \"Paid\" , \"Reserved\" , \"Unreserved\" , \"Refunded\" , ] data = [ ] for item in items : data . append ( [ item [ \"discount description\" ] , item [ \"total paid\" ] , item [ \"total reserved\" ] , item [ \"total unreserved\" ] , item [ \"total refunded\" ] , ] ) return List Report ( \"Usage by item\" , headings , data )", "predictions": ["return the bad bad bad bad bad bad bad bad bad"], "references": ["summarises the usage of a given discount ."], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 3954, "code": "def credit notes ( request , form ) : notes = commerce . Credit Note . objects . all ( ) . select related ( \"creditnoterefund\" , \"creditnoteapplication\" , \"invoice\" , \"invoice user attendee attendeeprofilebase\" , ) return Queryset Report ( \"Credit Notes\" , [ \"id\" , \"invoice user attendee attendeeprofilebase invoice recipient\" , \"status\" , \"value\" ] , notes , headings = [ \"id\" , \"Owner\" , \"Status\" , \"Value\" ] , link view = views . credit note , )", "predictions": ["displays the from the from the from the from from the from the from the from the from the from the from the from the from the from the from the"], "references": ["shows all of the credit notes in the system ."], "bleu": 0.04317900023606586, "rouge_l": 0.10748898678414096}
{"id": 3955, "code": "def invoices ( request , form ) : invoices = commerce . Invoice . objects . all ( ) . order by ( \"status\" , \"id\" ) return Queryset Report ( \"Invoices\" , [ \"id\" , \"recipient\" , \"value\" , \"get status display\" ] , invoices , headings = [ \"id\" , \"Recipient\" , \"Value\" , \"Status\" ] , link view = views . invoice , )", "predictions": ["render a register object"], "references": ["shows all of the invoices in the system ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 3956, "code": "def attendee list ( request ) : attendees = people . Attendee . objects . select related ( \"attendeeprofilebase\" , \"user\" , ) profiles = Attendee Profile . objects . filter ( attendee in = attendees ) . select related ( \"attendee\" , \"attendee user\" , ) profiles by attendee = dict ( ( i . attendee , i ) for i in profiles ) attendees = attendees . annotate ( has registered = Count ( Q ( user invoice status = commerce . Invoice . STATUS PAID ) ) , ) headings = [ \"User ID\" , \"Name\" , \"Email\" , \"Has registered\" , ] data = [ ] for a in attendees : data . append ( [ a . user . id , ( profiles by attendee [ a ] . attendee name ( ) if a in profiles by attendee else \"\" ) , a . user . email , a . has registered > 0 , ] ) data . sort ( key = lambda a : ( - a [ 3 ] , a [ 0 ] ) ) return Attendee List Report ( \"Attendees\" , headings , data , link view = attendee )", "predictions": ["list list of all raise raise connectionfailure if the user is not registered"], "references": ["returns a list of all attendees ."], "bleu": 0.1777835117834348, "rouge_l": 0.31715771230502604}
{"id": 3957, "code": "def speaker registrations ( request , form ) : kinds = form . cleaned data [ \"kind\" ] presentations = schedule models . Presentation . objects . filter ( proposal base kind in = kinds , ) . exclude ( cancelled = True , ) users = User . objects . filter ( Q ( speaker profile presentations in = presentations ) | Q ( speaker profile copresentations in = presentations ) ) paid carts = commerce . Cart . objects . filter ( status = commerce . Cart . STATUS PAID ) paid carts = Case ( When ( cart in = paid carts , then = Value ( 1 ) ) , default = Value ( 0 ) , output field = models . Integer Field ( ) , ) users = users . annotate ( paid carts = Sum ( paid carts ) ) users = users . order by ( \"paid carts\" ) return Queryset Report ( \"Speaker Registration Status\" , [ \"id\" , \"speaker profile name\" , \"email\" , \"paid carts\" ] , users , link view = attendee , ) return [ ]", "predictions": ["return exit with paid exit if the exit is not available if not found if not ."], "references": ["shows registration status for speakers with a given proposal kind ."], "bleu": 0.07994607499472013, "rouge_l": 0.14859926918392205}
{"id": 3958, "code": "def missing categories ( context ) : user = user for context ( context ) categories available = set ( Category Controller . available categories ( user ) ) items = Item Controller ( user ) . items pending or purchased ( ) categories held = set ( ) for product , quantity in items : categories held . add ( product . category ) return categories available - categories held", "predictions": ["returns a list of categories that are missing fs return the user return the user return the user return the user return the user return the user return the user fs"], "references": ["adds the categories that the user does not currently have ."], "bleu": 0.06757878745244751, "rouge_l": 0.20836891545687447}
{"id": 3959, "code": "def voucher code ( request ) : VOUCHERS FORM PREFIX = \"vouchers\" v = handle voucher ( request , VOUCHERS FORM PREFIX ) voucher form , voucher handled = v if voucher handled : messages . success ( request , \"Your voucher code was accepted.\" ) return redirect ( \"dashboard\" ) data = { \"voucher form\" : voucher form , } return render ( request , \"registrasion/voucher code.html\" , data )", "predictions": ["exists in the create create a create create a create code . . . . . . . . . . . . . . . . . . . ."], "references": ["a view * just * for entering a voucher form ."], "bleu": 0.046398855339878003, "rouge_l": 0.15627668659265584}
{"id": 3960, "code": "def amend registration ( request , user id ) : user = User . objects . get ( id = int ( user id ) ) current cart = Cart Controller . for user ( user ) items = commerce . Product Item . objects . filter ( cart = current cart . cart , ) . select related ( \"product\" ) initial = [ { \"product\" : i . product , \"quantity\" : i . quantity } for i in items ] Staff Products Form Set = forms . staff products formset factory ( user ) formset = Staff Products Form Set ( request . POST or None , initial = initial , prefix = \"products\" , ) for item , form in zip ( items , formset ) : queryset = inventory . Product . objects . filter ( id = item . product . id ) form . fields [ \"product\" ] . queryset = queryset voucher form = forms . Voucher Form ( request . POST or None , prefix = \"voucher\" , ) if request . POST and formset . is valid ( ) : pq = [ ( f . cleaned data [ \"product\" ] , f . cleaned data [ \"quantity\" ] ) for f in formset if \"product\" in f . cleaned data and f . cleaned data [ \"product\" ] is not None ] try : current cart . set quantities ( pq ) return redirect ( amend registration , user id ) except Validation Error as ve : for ve field in ve . error list : product , message = ve field . message for form in formset : if \"product\" not in form . cleaned data : continue if form . cleaned data [ \"product\" ] == product : form . add error ( \"quantity\" , message ) if request . POST and voucher form . has changed ( ) and voucher form . is valid ( ) : try : current cart . apply voucher ( voucher form . cleaned data [ \"voucher\" ] ) return redirect ( amend registration , user id ) except Validation Error as ve : voucher form . add error ( None , ve ) ic = Item Controller ( user ) data = { \"user\" : user , \"paid\" : ic . items purchased ( ) , \"cancelled\" : ic . items released ( ) , \"form\" : formset , \"voucher form\" : voucher form , } return render ( request , \"registrasion/amend registration.html\" , data )", "predictions": ["page for viewing the logger ."], "references": ["allows staff to amend a user s current registration cart and etc etc ."], "bleu": 0.05822753005110548, "rouge_l": 0.09327217125382263}
{"id": 3961, "code": "def extend reservation ( request , user id , days = 7 ) : user = User . objects . get ( id = int ( user id ) ) cart = Cart Controller . for user ( user ) cart . extend reservation ( datetime . timedelta ( days = days ) ) return redirect ( request . META [ \"HTTP REFERER\" ] )", "predictions": ["redirect a reservation utils utils utils"], "references": ["allows staff to extend the reservation on a given user s cart ."], "bleu": 0.07612610271614867, "rouge_l": 0.09870550161812298}
{"id": 3962, "code": "def invoice mailout ( request ) : category = request . GET . getlist ( \"category\" , [ ] ) product = request . GET . getlist ( \"product\" , [ ] ) status = request . GET . get ( \"status\" ) form = forms . Invoice Email Form ( request . POST or None , category = category , product = product , status = status , ) emails = [ ] if form . is valid ( ) : emails = [ ] for invoice in form . cleaned data [ \"invoice\" ] : from email = form . cleaned data [ \"from email\" ] subject = form . cleaned data [ \"subject\" ] body = Template ( form . cleaned data [ \"body\" ] ) . render ( Context ( { \"invoice\" : invoice , \"user\" : invoice . user , } ) ) recipient list = [ invoice . user . email ] emails . append ( Email ( subject , body , from email , recipient list ) ) if form . cleaned data [ \"action\" ] == forms . Invoice Email Form . ACTION SEND : send mass mail ( emails ) messages . info ( request , \"The e-mails have been sent.\" ) data = { \"form\" : form , \"emails\" : emails , } return render ( request , \"registrasion/invoice mailout.html\" , data )", "predictions": ["send view for read read read read os os os os os os os"], "references": ["allows staff to send emails to users based on their invoice status ."], "bleu": 0.08839374326825923, "rouge_l": 0.0745721271393643}
{"id": 3963, "code": "def render badge ( user ) : data = { \"user\" : user , } t = loader . get template ( 'registrasion/badge.svg' ) return t . render ( data )", "predictions": ["prefix the line of a line"], "references": ["renders a single user s badge ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 3964, "code": "def generate from cart ( cls , cart ) : cart . refresh from db ( ) product items = commerce . Product Item . objects . filter ( cart = cart ) product items = product items . select related ( \"product\" , \"product category\" , ) product items = product items . order by ( \"product category order\" , \"product order\" ) if len ( product items ) == 0 : raise Validation Error ( \"Your cart is empty.\" ) discount items = commerce . Discount Item . objects . filter ( cart = cart ) discount items = discount items . select related ( \"discount\" , \"product\" , \"product category\" , ) def format product ( product ) : return \"%s - %s\" % ( product . category . name , product . name ) def format discount ( discount , product ) : description = discount . description return \"%s (%s)\" % ( description , format product ( product ) ) line items = [ ] for item in product items : product = item . product line item = commerce . Line Item ( description = format product ( product ) , quantity = item . quantity , price = product . price , product = product , ) line items . append ( line item ) for item in discount items : line item = commerce . Line Item ( description = format discount ( item . discount , item . product ) , quantity = item . quantity , price = cls . resolve discount value ( item ) * - 1 , product = item . product , ) line items . append ( line item ) min due time = cart . reservation duration + cart . time last updated return cls . generate ( cart . user , cart , min due time , line items )", "predictions": ["suffix to suffix for a cart cart"], "references": ["generates an invoice for the given cart ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 3965, "code": "def apply credit notes ( cls , invoice ) : invoices = commerce . Invoice . objects . filter ( user = invoice . user , status = commerce . Invoice . STATUS UNPAID , ) if invoices . count ( ) > 1 : return notes = commerce . Credit Note . unclaimed ( ) . filter ( invoice user = invoice . user ) for note in notes : try : Credit Note Controller ( note ) . apply to invoice ( invoice ) except Validation Error : break invoice . refresh from db ( )", "predictions": ["tail all self self 0 to 10"], "references": ["applies the user s credit notes to the given invoice on creation ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 3966, "code": "def refresh ( self ) : self . invoice . refresh from db ( ) if self . invoice . cart : self . invoice . cart . refresh from db ( )", "predictions": ["head the 10 10 values from the 10"], "references": ["refreshes the underlying invoice and cart objects ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 3967, "code": "def void ( self ) : if self . invoice . total payments ( ) > 0 : raise Validation Error ( \"Invoices with payments must be refunded.\" ) elif self . invoice . is refunded : raise Validation Error ( \"Refunded invoices may not be voided.\" ) if self . invoice . is paid : self . release cart ( ) self . mark void ( )", "predictions": ["raises an exception if the invoice is not already not been not defined = true = false = false = false"], "references": ["voids the invoice if it is valid to do so ."], "bleu": 0.08687475782716618, "rouge_l": 0.19869706840390877}
{"id": 3968, "code": "def update ( self , data ) : fields = [ 'id' , 'status' , 'type' , 'persistence' , 'date start' , 'date finish' , 'date created' , 'date modified' , 'checksum' , 'processor name' , 'input' , 'input schema' , 'output' , 'output schema' , 'static' , 'static schema' , 'var' , 'var template' , ] self . annotation = { } for f in fields : setattr ( self , f , data [ f ] ) self . name = data [ 'static' ] [ 'name' ] if 'name' in data [ 'static' ] else '' self . annotation . update ( self . flatten field ( data [ 'input' ] , data [ 'input schema' ] , 'input' ) ) self . annotation . update ( self . flatten field ( data [ 'output' ] , data [ 'output schema' ] , 'output' ) ) self . annotation . update ( self . flatten field ( data [ 'static' ] , data [ 'static schema' ] , 'static' ) ) self . annotation . update ( self . flatten field ( data [ 'var' ] , data [ 'var template' ] , 'var' ) )", "predictions": ["update the data in the format of the data"], "references": ["update the object with new data ."], "bleu": 0.19960198807747329, "rouge_l": 0.38364779874213834}
{"id": 3969, "code": "def flatten field ( self , field , schema , path ) : flat = { } for field schema , fields , path in iterate schema ( field , schema , path ) : name = field schema [ 'name' ] typ = field schema [ 'type' ] label = field schema [ 'label' ] value = fields [ name ] if name in fields else None flat [ path ] = { 'name' : name , 'value' : value , 'type' : typ , 'label' : label } return flat", "predictions": ["flatten a field into a flat flat dictionary ."], "references": ["reduce dicts of dicts to dot separated keys ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 3970, "code": "def print downloads ( self ) : for path , ann in self . annotation . items ( ) : if path . startswith ( 'output' ) and ann [ 'type' ] == 'basic:file:' : print ( \"{}: {}\" . format ( path , ann [ 'value' ] [ 'file' ] ) )", "predictions": ["print the downloads of all the downloads in the annotation ."], "references": ["print file fields to standard output ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 3971, "code": "def data ( self , * * query ) : objects = self . cache [ 'objects' ] data = self . api . data . get ( * * query ) [ 'objects' ] data objects = [ ] for d in data : id = d [ 'id' ] if id in objects : objects [ id ] . update ( d ) else : objects [ id ] = Gen Data ( d , self ) data objects . append ( objects [ id ] ) for d in data objects : count += 1 while True : ref annotation = { } remove annotation = [ ] for path , ann in d . annotation . items ( ) : if ann [ 'type' ] . startswith ( 'data:' ) : id = ann [ 'value' ] if id not in objects : try : d tmp = self . api . data ( id ) . get ( ) except slumber . exceptions . Http Client Error as ex : if ex . response . status code == 404 : continue else : raise ex objects [ id ] = Gen Data ( d tmp , self ) annotation = objects [ id ] . annotation ref annotation . update ( { path + '.' + k : v for k , v in annotation . items ( ) } ) remove annotation . append ( path ) if ref annotation : d . annotation . update ( ref annotation ) for path in remove annotation : del d . annotation [ path ] else : break return data objects", "predictions": ["remove all objects from the cache"], "references": ["query for data object annotation ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 3972, "code": "def rundata ( self , strjson ) : d = json . loads ( strjson ) return self . api . data . post ( d )", "predictions": ["retrieve data from the api"], "references": ["post json data object to server"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 3973, "code": "def get subclasses ( c ) : subclasses = c . subclasses ( ) for d in list ( subclasses ) : subclasses . extend ( get subclasses ( d ) ) return subclasses", "predictions": ["return all subclasses of all subclasses of * c * c * c * ."], "references": ["gets the subclasses of a class ."], "bleu": 0.11633270842295028, "rouge_l": 0.291866028708134}
{"id": 3974, "code": "def get repo and project ( self ) : app = self . app repo = app . data . apply ( 'github-repo' , app . args . github repo , app . prompt repo , on load = app . github . get repo , on save = lambda r : r . id ) assert repo , \"repository not found.\" project = app . data . apply ( 'asana-project' , app . args . asana project , app . prompt project , on load = app . asana . projects . find by id , on save = lambda p : p [ 'id' ] ) assert project , \"project not found.\" first issue = app . data . apply ( 'first-issue' , app . args . first issue , \"set the first issue to sync with [1 for new repos]\" , on save = int ) assert first issue assert first issue >= 0 , \"issue must be positive\" app . sync data ( ) return repo , project", "predictions": ["sync the repo and project project ."], "references": ["returns repository and project ."], "bleu": 0.29071536848410967, "rouge_l": 0.5154929577464789}
{"id": 3975, "code": "def get variant phenotypes with suggested changes ( variant id list ) : variants = civic . get variants by ids ( variant id list ) evidence = list ( ) for variant in variants : evidence . extend ( variant . evidence ) for e in evidence : suggested changes url = f'https://civicdb.org/api/evidence items/{e.id}/suggested changes' resp = requests . get ( suggested changes url ) resp . raise for status ( ) suggested changes = dict ( ) for suggested change in resp . json ( ) : pheno changes = suggested change [ 'suggested changes' ] . get ( 'phenotype ids' , None ) if pheno changes is None : continue a , b = pheno changes added = set ( b ) - set ( a ) deleted = set ( a ) - set ( b ) rid = suggested change [ 'id' ] suggested changes [ rid ] = { 'added' : added , 'deleted' : deleted } yield e , { 'suggested changes' : suggested changes , 'current' : set ( [ x . id for x in e . phenotypes ] ) }", "predictions": ["get all variant phenotypes with suggested changes"], "references": ["for each variant yields evidence and associated phenotypes both current and suggested"], "bleu": 0.1081377510275021, "rouge_l": 0.30148270181219106}
{"id": 3976, "code": "def get variant phenotypes with suggested changes merged ( variant id list ) : for evidence , phenotype status in get variant phenotypes with suggested changes ( variant id list ) : final = phenotype status [ 'current' ] for rid in sorted ( phenotype status [ 'suggested changes' ] ) : changes = phenotype status [ 'suggested changes' ] [ rid ] final = final - changes [ 'deleted' ] final = final | changes [ 'added' ] if final : yield evidence , final", "predictions": ["get all variant phenotypes with suggested changes"], "references": ["for each variant yields evidence and merged phenotype from applying suggested changes to current"], "bleu": 0.09663861439684919, "rouge_l": 0.2695139911634757}
{"id": 3977, "code": "def update ( self , allow partial = True , force = False , * * kwargs ) : if kwargs : self . init ( partial = allow partial , force = force , * * kwargs ) return not self . partial if not force and CACHE . get ( hash ( self ) ) : cached = CACHE [ hash ( self ) ] for field in self . SIMPLE FIELDS | self . COMPLEX FIELDS : v = getattr ( cached , field ) setattr ( self , field , v ) self . partial = False logging . info ( f'Loading {str(self)} from cache' ) return True resp dict = element lookup by id ( self . type , self . id ) self . init ( partial = False , * * resp dict ) return True", "predictions": ["update an object with the given id ."], "references": ["updates record and returns true if record is complete after update else false ."], "bleu": 0.08383280652235028, "rouge_l": 0.1732954545454545}
{"id": 3978, "code": "def uniqify ( cls , seq ) : seen = set ( ) seen add = seen . add return [ x for x in seq if x not in seen and not seen add ( x ) ]", "predictions": ["return a list of all seq in order ."], "references": ["returns a unique list of seq"], "bleu": 0.21105340631872635, "rouge_l": 0.5532879818594103}
{"id": 3979, "code": "def authenticate ( self ) : if self . oauth : return False self . settings . apply ( 'api-asana' , self . args . asana api , \"enter asana api key\" ) self . settings . apply ( 'api-github' , self . args . github api , \"enter github.com token\" ) logging . debug ( \"authenticating asana api.\" ) self . asana = Client . basic auth ( self . settings [ 'api-asana' ] ) self . asana errors = asana errors self . asana me = self . asana . users . me ( ) logging . debug ( \"authenticating github api\" ) self . github = Github ( self . settings [ 'api-github' ] ) self . github user = self . github . get user ( ) self . oauth = True", "predictions": ["perform the github github user ."], "references": ["connects to github and asana and authenticates via oauth ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 3980, "code": "def list select ( cls , lst , prompt , offset = 0 ) : inp = raw input ( \"select %s: \" % prompt ) assert inp , \"value required.\" try : return lst [ int ( inp ) + offset ] except Value Error : return inp except Index Error : assert False , \"bad value.\"", "predictions": ["list the input parameter ."], "references": ["given a list of values and names accepts the index value or name ."], "bleu": 0.053667245469253895, "rouge_l": 0.2909379968203497}
{"id": 3981, "code": "def move saved issue data ( self , issue , ns , other ns ) : if isinstance ( issue , int ) : issue number = str ( issue ) elif isinstance ( issue , basestring ) : issue number = issue else : issue number = issue . number issue data key = self . issue data key ( ns ) other issue data key = self . issue data key ( other ns ) issue data = self . data . get ( issue data key , { } ) other issue data = self . data . get ( other issue data key , { } ) id = issue data . pop ( issue number , None ) if id : other issue data [ issue number ] = id self . data [ other issue data key ] = other issue data self . data [ issue data key ] = issue data", "predictions": ["move saved issue data to the issue"], "references": ["moves an issue_data from one namespace to another ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 3982, "code": "def get asana task ( self , asana task id ) : try : return self . asana . tasks . find by id ( asana task id ) except asana errors . Not Found Error : return None except asana errors . Forbidden Error : return None", "predictions": ["get the asana task object by id"], "references": ["retrieves a task from asana ."], "bleu": 0.20556680845025982, "rouge_l": 0.15601023017902813}
{"id": 3983, "code": "def transport task ( func ) : def wrapped func ( * args , * * kwargs ) : tries = 0 while True : try : try : return func ( * args , * * kwargs ) except ( asana errors . Invalid Request Error , asana errors . Not Found Error ) , exc : logging . warn ( \"warning: invalid request: %r\" , exc ) except asana errors . Forbidden Error , exc : logging . warn ( \"forbidden error: %r\" , exc ) except asana errors . Not Found Error , exc : logging . warn ( \"not found error: %r\" , exc ) return None except asana errors . Retryable Asana Error , retry exc : tries += 1 logging . warn ( \"retry exception %r on try %d\" , retry exc , tries ) if tries >= 3 : raise except Exception , exc : logging . exception ( \"Exception in transport.\" ) return return wrapped func", "predictions": ["decorator to try to try to try to try to the transport task ."], "references": ["decorator for retrying tasks with special cases ."], "bleu": 0.09782375748961449, "rouge_l": 0.19122257053291536}
{"id": 3984, "code": "def flush ( callback = None ) : while True : if shutdown event . is set ( ) : return if callable ( callback ) : callback ( ) try : item = queue . get ( timeout = 1 ) queue . put ( item ) except Queue . Empty : return", "predictions": ["flush any pending item from the queue ."], "references": ["waits until queue is empty ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 3985, "code": "def format task numbers with links ( tasks ) : project id = data . get ( 'asana-project' , None ) def task format ( task id ) : if project id : asana url = tool . Tool App . make asana url ( project id , task id ) return \"[#%d](%s)\" % ( task id , asana url ) else : return \"#%d\" % task id return \"\\n\" . join ( [ task format ( tid ) for tid in tasks ] )", "predictions": ["format the task numbers to be printed"], "references": ["returns formatting for the tasks section of asana ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 3986, "code": "def create missing task ( self , asana workspace id , name , assignee , projects , completed , issue number , issue html url , issue state , issue body , tasks , labels , label tag map ) : task = self . asana . tasks . create in workspace ( asana workspace id , { 'name' : name , 'notes' : issue body , 'assignee' : assignee , 'projects' : projects , 'completed' : completed , } ) task id = task [ 'id' ] put ( \"create story\" , task id = task id , text = \"Git Issue #%d: \\n\" \"%s\" % ( issue number , issue html url , ) ) put ( \"apply tasks to issue\" , tasks = [ task id ] , issue number = issue number , issue body = issue body , ) put setting ( \"save issue data task\" , issue = issue number , task id = task id , namespace = issue state ) tasks . append ( task id ) put ( \"sync tags\" , tasks = tasks , labels = labels , label tag map = label tag map )", "predictions": ["create missing task for a workspace ."], "references": ["creates a missing task ."], "bleu": 0.2777619034011791, "rouge_l": 0.5154929577464789}
{"id": 3987, "code": "def apply tasks to issue ( self , tasks , issue number , issue body ) : issue body = issue body task numbers = format task numbers with links ( tasks ) if task numbers : new body = ASANA SECTION RE . sub ( '' , issue body ) new body = new body + % task numbers put ( \"issue edit\" , issue number = issue number , body = new body ) return new body return issue body", "predictions": ["apply tasks to issue ."], "references": ["applies task numbers to an issue ."], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 3988, "code": "def data types ( self ) : data = self . gencloud . project data ( self . id ) return sorted ( set ( d . type for d in data ) )", "predictions": ["return the list of all data types types of this project ."], "references": ["return a list of data types ."], "bleu": 0.1870361278311548, "rouge_l": 0.6630434782608696}
{"id": 3989, "code": "def data ( self , * * query ) : data = self . gencloud . project data ( self . id ) query [ 'case ids contains' ] = self . id ids = set ( d [ 'id' ] for d in self . gencloud . api . dataid . get ( * * query ) [ 'objects' ] ) return [ d for d in data if d . id in ids ]", "predictions": ["return all data for the project"], "references": ["query for data object annotation ."], "bleu": 0.24446151121745047, "rouge_l": 0.16666666666666666}
{"id": 3990, "code": "def init Port ( self ) : try : self . m ser = serial . Serial ( port = self . m ttyport , baudrate = self . m baudrate , timeout = 0 , parity = serial . PARITY EVEN , stopbits = serial . STOPBITS ONE , bytesize = serial . SEVENBITS , rtscts = False ) ekm log ( \"Pyserial version = \" + serial . VERSION ) ekm log ( \"Port = \" + self . m ttyport ) ekm log ( \"Rate = \" + str ( self . m baudrate ) ) time . sleep ( self . m init wait ) return True except : ekm log ( traceback . format exc ( sys . exc info ( ) ) ) return False", "predictions": ["init the serial serial port ."], "references": ["required initialization call wraps pyserial constructor ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 3991, "code": "def combine AB ( self ) : v4definition meter = V4Meter ( ) v4definition meter . make AB ( ) defv4 = v4definition meter . get Read Buffer ( ) v3definition meter = V3Meter ( ) v3definition meter . make Return Format ( ) defv3 = v3definition meter . get Read Buffer ( ) for fld in defv3 : if fld not in self . m all fields : compare fld = fld . upper ( ) if not \"RESERVED\" in compare fld and not \"CRC\" in compare fld : self . m all fields [ fld ] = defv3 [ fld ] for fld in defv4 : if fld not in self . m all fields : compare fld = fld . upper ( ) if not \"RESERVED\" in compare fld and not \"CRC\" in compare fld : self . m all fields [ fld ] = defv4 [ fld ] pass", "predictions": ["combine the ab and combine the fields"], "references": ["use the serial block definitions in v3 and v4 to create one field list ."], "bleu": 0.06555660318294844, "rouge_l": 0.17062937062937064}
{"id": 3992, "code": "def update Observers ( self ) : for observer in self . m observers : try : observer . update ( self . m req ) except : ekm log ( traceback . format exc ( sys . exc info ( ) ) )", "predictions": ["update all observers observer"], "references": ["fire update method in all attached observers in order of attachment ."], "bleu": 0.05782700803395868, "rouge_l": 0.34398496240601506}
{"id": 3993, "code": "def init Lcd Lookup ( self ) : self . m lcd lookup [ \"k Wh Tot\" ] = LCD Items . k Wh Tot self . m lcd lookup [ \"Rev k Wh Tot\" ] = LCD Items . Rev k Wh Tot self . m lcd lookup [ \"RMS Volts Ln 1\" ] = LCD Items . RMS Volts Ln 1 self . m lcd lookup [ \"RMS Volts Ln 2\" ] = LCD Items . RMS Volts Ln 2 self . m lcd lookup [ \"RMS Volts Ln 3\" ] = LCD Items . RMS Volts Ln 3 self . m lcd lookup [ \"Amps Ln 1\" ] = LCD Items . Amps Ln 1 self . m lcd lookup [ \"Amps Ln 2\" ] = LCD Items . Amps Ln 2 self . m lcd lookup [ \"Amps Ln 3\" ] = LCD Items . Amps Ln 3 self . m lcd lookup [ \"RMS Watts Ln 1\" ] = LCD Items . RMS Watts Ln 1 self . m lcd lookup [ \"RMS Watts Ln 2\" ] = LCD Items . RMS Watts Ln 2 self . m lcd lookup [ \"RMS Watts Ln 3\" ] = LCD Items . RMS Watts Ln 3 self . m lcd lookup [ \"RMS Watts Tot\" ] = LCD Items . RMS Watts Tot self . m lcd lookup [ \"Power Factor Ln 1\" ] = LCD Items . Power Factor Ln 1 self . m lcd lookup [ \"Power Factor Ln 2\" ] = LCD Items . Power Factor Ln 2 self . m lcd lookup [ \"Power Factor Ln 3\" ] = LCD Items . Power Factor Ln 3 self . m lcd lookup [ \"k Wh Tariff 1\" ] = LCD Items . k Wh Tariff 1 self . m lcd lookup [ \"k Wh Tariff 2\" ] = LCD Items . k Wh Tariff 2 self . m lcd lookup [ \"k Wh Tariff 3\" ] = LCD Items . k Wh Tariff 3 self . m lcd lookup [ \"k Wh Tariff 4\" ] = LCD Items . k Wh Tariff 4 self . m lcd lookup [ \"Rev k Wh Tariff 1\" ] = LCD Items . Rev k Wh Tariff 1 self . m lcd lookup [ \"Rev k Wh Tariff 2\" ] = LCD Items . Rev k Wh Tariff 2 self . m lcd lookup [ \"Rev k Wh Tariff 3\" ] = LCD Items . Rev k Wh Tariff 3 self . m lcd lookup [ \"Rev k Wh Tariff 4\" ] = LCD Items . Rev k Wh Tariff 4 self . m lcd lookup [ \"Reactive Pwr Ln 1\" ] = LCD Items . Reactive Pwr Ln 1 self . m lcd lookup [ \"Reactive Pwr Ln 2\" ] = LCD Items . Reactive Pwr Ln 2 self . m lcd lookup [ \"Reactive Pwr Ln 3\" ] = LCD Items . Reactive Pwr Ln 3 self . m lcd lookup [ \"Reactive Pwr Tot\" ] = LCD Items . Reactive Pwr Tot self . m lcd lookup [ \"Line Freq\" ] = LCD Items . Line Freq self . m lcd lookup [ \"Pulse Cnt 1\" ] = LCD Items . Pulse Cnt 1 self . m lcd lookup [ \"Pulse Cnt 2\" ] = LCD Items . Pulse Cnt 2 self . m lcd lookup [ \"Pulse Cnt 3\" ] = LCD Items . Pulse Cnt 3 self . m lcd lookup [ \"k Wh Ln 1\" ] = LCD Items . k Wh Ln 1 self . m lcd lookup [ \"Rev k Wh Ln 1\" ] = LCD Items . Rev k Wh Ln 1 self . m lcd lookup [ \"k Wh Ln 2\" ] = LCD Items . k Wh Ln 2 self . m lcd lookup [ \"Rev k Wh Ln 2\" ] = LCD Items . Rev k Wh Ln 2 self . m lcd lookup [ \"k Wh Ln 3\" ] = LCD Items . k Wh Ln 3 self . m lcd lookup [ \"Rev k Wh Ln 3\" ] = LCD Items . Rev k Wh Ln 3 self . m lcd lookup [ \"Reactive Energy Tot\" ] = LCD Items . Reactive Energy Tot self . m lcd lookup [ \"Max Demand Rst\" ] = LCD Items . Max Demand Rst self . m lcd lookup [ \"Rev k Wh Rst\" ] = LCD Items . Rev k Wh Rst self . m lcd lookup [ \"State Inputs\" ] = LCD Items . State Inputs self . m lcd lookup [ \"Max Demand\" ] = LCD Items . Max Demand", "predictions": ["initialize lcd . ."], "references": ["initialize lookup table for string input of lcd fields"], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 3994, "code": "def make AB ( self ) : for fld in self . m blk a : compare fld = fld . upper ( ) if not \"RESERVED\" in compare fld and not \"CRC\" in compare fld : self . m req [ fld ] = self . m blk a [ fld ] for fld in self . m blk b : compare fld = fld . upper ( ) if not \"RESERVED\" in compare fld and not \"CRC\" in compare fld : self . m req [ fld ] = self . m blk b [ fld ] pass", "predictions": ["make a ab object for the blk"], "references": ["munge a and b reads into single serial block with only unique fields ."], "bleu": 0.06833381956448398, "rouge_l": 0.08983799705449189}
{"id": 3995, "code": "def serial Post End ( self ) : ekm log ( \"Termination string sent (\" + self . m context + \")\" ) try : self . m serial port . write ( \"0142300375\" . decode ( \"hex\" ) ) except : ekm log ( traceback . format exc ( sys . exc info ( ) ) ) pass", "predictions": ["handler for the serial serial serial port"], "references": ["send termination string to implicit current meter ."], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 3996, "code": "def apply tasks to issue ( self , issue , tasks , issue body = None ) : issue body = issue body or issue . body task numbers = transport . format task numbers with links ( tasks ) if task numbers : new body = transport . ASANA SECTION RE . sub ( '' , issue body ) new body = new body + % task numbers transport . issue edit ( issue , body = new body ) return new body return issue body", "predictions": ["apply tasks to issue ."], "references": ["applies task numbers to an issue ."], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 3997, "code": "def statistics ( self , elapsed , result ) : return \"\\n\" . join ( ( self . timing ( elapsed ) , self . result summary ( result ) ) )", "predictions": ["return the statistics for the given elapsed ."], "references": ["return output for the combined time and result summary statistics ."], "bleu": 0.17250013293422076, "rouge_l": 0.4093959731543625}
{"id": 3998, "code": "def color ( self , color , text ) : return \"{escape}{text}{reset}\" . format ( escape = self . ANSI [ color ] , text = text , reset = self . ANSI [ \"reset\" ] , )", "predictions": ["print a color string for the given color ."], "references": ["color some text in the given ansi color ."], "bleu": 0.24446151121745052, "rouge_l": 0.5555555555555556}
{"id": 3999, "code": "def show ( self , text ) : self . stream . write ( text ) self . stream . flush ( )", "predictions": ["write the text to the stream ."], "references": ["write the text to the stream and flush immediately ."], "bleu": 0.5663883102178479, "rouge_l": 0.7981308411214955}
{"id": 4000, "code": "def result summary ( self , result ) : return \"{} examples, {} errors, {} failures\\n\" . format ( result . tests Run , len ( result . errors ) , len ( result . failures ) , )", "predictions": ["fields for the update update"], "references": ["return a summary of the results ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 4001, "code": "def parse ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] if not argv or argv [ 0 ] not in { \"run\" , \"transform\" } : argv = [ \"run\" ] + argv arguments = clean ( parser . parse args ( argv ) ) return arguments", "predictions": ["flatten the command line arguments = 1 = 0 = 1 = 0 = 1 = 0"], "references": ["parse some arguments using the parser ."], "bleu": 0.07994607499472013, "rouge_l": 0.09010339734121123}
{"id": 4002, "code": "def setup ( config ) : formatter = config . Formatter ( ) if config . verbose : formatter = result . Verbose ( formatter ) if config . color : formatter = result . Colored ( formatter ) current result = result . Example Result ( formatter ) ivoire . current result = ivoire . manager . result = current result", "predictions": ["print the . . . formatter object path path path path path path path path path path"], "references": ["setup the environment for an example run ."], "bleu": 0.07994607499472013, "rouge_l": 0.17110799438990182}
{"id": 4003, "code": "def run ( config ) : setup ( config ) if config . exitfirst : ivoire . current result . failfast = True ivoire . current result . start Test Run ( ) for spec in config . specs : try : load by name ( spec ) except Exception : ivoire . current result . add Error ( Example Not Running ( ) , sys . exc info ( ) ) ivoire . current result . stop Test Run ( ) sys . exit ( not ivoire . current result . was Successful ( ) )", "predictions": ["data update the ."], "references": ["time to run ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 4004, "code": "def transform ( config ) : if transform possible : Example Loader . register ( ) args , sys . argv [ 1 : ] = sys . argv [ 1 : ] , config . args try : return runpy . run path ( config . runner , run name = \" main \" ) finally : sys . argv [ 1 : ] = args", "predictions": ["rundata command line arguments json"], "references": ["run in transform mode ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 4005, "code": "def takes only self ( self ) : return ast . arguments ( args = [ ast . arg ( arg = \"self\" ) ] , defaults = [ ] , kw defaults = [ ] , kwonlyargs = [ ] , )", "predictions": ["= default d list of d objects"], "references": ["return an argument list node that takes only self ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 4006, "code": "def register ( cls ) : cls . finder = File Finder . path hook ( ( cls , [ cls . suffix ] ) ) sys . path hooks . append ( cls . finder )", "predictions": ["get the template finder"], "references": ["register the path hook ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 4007, "code": "def source to code ( self , source bytes , source path ) : node = ast . parse ( source bytes ) transformed = Example Transformer ( ) . transform ( node ) return compile ( transformed , source path , \"exec\" , dont inherit = True )", "predictions": ["convert get get to phenotypes phenotypes phenotypes ."], "references": ["transform the source code then return the code object ."], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 4008, "code": "def apply argument parser ( arguments Parser , options = None ) : if options is not None : args = arguments Parser . parse args ( options ) else : args = arguments Parser . parse args ( ) return args", "predictions": ["get the variant phenotypes phenotypes status status status ."], "references": ["apply the argument parser ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 4009, "code": "def load by name ( name ) : if os . path . exists ( name ) : load from path ( name ) else : import ( name )", "predictions": ["update all config from file"], "references": ["load a spec from either a file path or a fully qualified name ."], "bleu": 0.04994299940831281, "rouge_l": 0.19395866454689983}
{"id": 4010, "code": "def load from path ( path ) : if os . path . isdir ( path ) : paths = discover ( path ) else : paths = [ path ] for path in paths : name = os . path . basename ( os . path . splitext ( path ) [ 0 ] ) imp . load source ( name , path )", "predictions": ["load from from from"], "references": ["load a spec from a given path discovering specs if a directory is given ."], "bleu": 0.025419978385188596, "rouge_l": 0.190625}
{"id": 4011, "code": "def delimit ( values , delimiter = ', ' ) : toks = [ ] if not values : return toks if not isinstance ( delimiter , ( list , tuple ) ) : delimiter = [ delimiter ] last = len ( values ) - 1 for i , value in enumerate ( values ) : toks . append ( value ) if i < last : toks . extend ( delimiter ) return toks", "predictions": ["asana a list of self into a list of self ."], "references": ["returns a list of tokens interleaved with the delimiter ."], "bleu": 0.22416933501922287, "rouge_l": 0.384251968503937}
{"id": 4012, "code": "def exists ( value ) : if not isinstance ( value , Token ) : raise Type Error ( 'value must be a token' ) if not hasattr ( value , 'identifier' ) : raise Type Error ( 'value must support an identifier' ) if not value . identifier : value = value . class ( * * value . dict ) value . identifier = 'v' ident = Identifier ( value . identifier ) return Query ( [ Optional Match ( value ) , Return ( Predicate ( ident , 'IS NOT NULL' ) ) , Limit ( 1 ) , ] )", "predictions": ["check that value is a valid identifier return none if not found ."], "references": ["query to test if a value exists ."], "bleu": 0.12011055432195765, "rouge_l": 0.19902120717781402}
{"id": 4013, "code": "def get ( value ) : if not isinstance ( value , Token ) : raise Type Error ( 'value must be a token' ) if not hasattr ( value , 'identifier' ) : raise Type Error ( 'value must support an identifier' ) if not value . identifier : value = value . class ( * * value . dict ) value . identifier = 'v' ident = Identifier ( value . identifier ) return Query ( [ Match ( value ) , Return ( ident ) ] )", "predictions": ["convert issue issue issue to } { } { % } { % } { % } { % } { % } { % } { % } { %"], "references": ["query to get the value ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 4014, "code": "def check ( self ) : if self . closed : raise Value Error ( \"Cannot check a closed state\" ) self . maybe Reset ( ) if self . url is None : return False return self . maybe Check ( )", "predictions": ["get the errors from the request id id id"], "references": ["check the state of http"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 4015, "code": "def wrap Heart ( service ) : master = taservice . Multi Service ( ) service . set Service Parent ( master ) maybe Add Heart ( master ) return master", "predictions": ["transport s task task . ."], "references": ["wrap a service in a multiservice with a heart"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 4016, "code": "def freeze from checkpoint ( input checkpoint , output file path , output node names ) : check input checkpoint ( input checkpoint ) output node names = output node names string as list ( output node names ) with tf . Session ( ) as sess : restore from checkpoint ( sess , input checkpoint ) freeze graph . freeze graph with def protos ( input graph def = sess . graph def , input saver def = None , input checkpoint = input checkpoint , output node names = ',' . join ( output node names ) , restore op name = 'save/restore all' , filename tensor name = 'save/Const:0' , output graph = output file path , clear devices = True , initializer nodes = '' )", "predictions": ["flush except for callback ."], "references": ["freeze and shrink the graph based on a checkpoint and the output node names ."], "bleu": 0.0369481680224917, "rouge_l": 0.09172932330827067}
{"id": 4017, "code": "def freeze ( sess , output file path , output node names ) : with Temporary Directory ( ) as temp dir name : checkpoint path = os . path . join ( temp dir name , 'model.ckpt' ) tf . train . Saver ( ) . save ( sess , checkpoint path ) freeze from checkpoint ( checkpoint path , output file path , output node names )", "predictions": ["format and format the if it is not a if it doesn t exist tool tool tool tool ."], "references": ["freeze and shrink the graph based on a session and the output node names ."], "bleu": 0.08097785064266201, "rouge_l": 0.24039408866995077}
{"id": 4018, "code": "def save graph only ( sess , output file path , output node names , as text = False ) : for node in sess . graph def . node : node . device = '' graph def = graph util . extract sub graph ( sess . graph def , output node names ) output dir , output filename = os . path . split ( output file path ) graph io . write graph ( graph def , output dir , output filename , as text = as text )", "predictions": ["create a missing missing missing missing missing missing missing missing missing missing missing missing missing missing ."], "references": ["save a small version of the graph based on a session and the output node names ."], "bleu": 0.07994607499472013, "rouge_l": 0.1176470588235294}
{"id": 4019, "code": "def save graph only from checkpoint ( input checkpoint , output file path , output node names , as text = False ) : check input checkpoint ( input checkpoint ) output node names = output node names string as list ( output node names ) with tf . Session ( ) as sess : restore from checkpoint ( sess , input checkpoint ) save graph only ( sess , output file path , output node names , as text = as text )", "predictions": ["apply tasks to checkpoint . . ."], "references": ["save a small version of the graph based on a checkpoint and the output node names ."], "bleu": 0.04926429870313275, "rouge_l": 0.1550190597204574}
{"id": 4020, "code": "def save weights ( sess , output path , conv var names = None , conv transpose var names = None ) : if not conv var names : conv var names = [ ] if not conv transpose var names : conv transpose var names = [ ] for var in tf . trainable variables ( ) : filename = '{}-{}' . format ( output path , var . name . replace ( ':' , '-' ) . replace ( '/' , '-' ) ) if var . name in conv var names : var = tf . transpose ( var , perm = [ 3 , 0 , 1 , 2 ] ) elif var . name in conv transpose var names : var = tf . transpose ( var , perm = [ 3 , 1 , 0 , 2 ] ) value = sess . run ( var ) with open ( filename , 'w' ) as file : value . tofile ( file )", "predictions": ["data for tf . . . . . . . . . . . . ."], "references": ["save the weights of the trainable variables each one in a different file in output_path ."], "bleu": 0.07692375026049747, "rouge_l": 0.0625}
{"id": 4021, "code": "def save weights from checkpoint ( input checkpoint , output path , conv var names = None , conv transpose var names = None ) : check input checkpoint ( input checkpoint ) with tf . Session ( ) as sess : restore from checkpoint ( sess , input checkpoint ) save weights ( sess , output path , conv var names = conv var names , conv transpose var names = conv transpose var names )", "predictions": ["data self . self . . . ."], "references": ["save the weights of the trainable variables given a checkpoint each one in a different file in output_path ."], "bleu": 0.04054685178922986, "rouge_l": 0.06900452488687782}
{"id": 4022, "code": "def restore from checkpoint ( sess , input checkpoint ) : saver = tf . train . import meta graph ( '{}.meta' . format ( input checkpoint ) ) saver . restore ( sess , input checkpoint ) return saver", "predictions": ["init the serial baudrate from the checkpoint . . . . . . . . . . . ."], "references": ["return a tensorflow saver from a checkpoint containing the metagraph ."], "bleu": 0.08097785064266201, "rouge_l": 0.21010332950631458}
{"id": 4023, "code": "def render tag ( self , context , * tag args , * * tag kwargs ) : raise Not Implemented Error ( \"{0}.render tag() is not implemented!\" . format ( self . class . name ) )", "predictions": ["combine the tag and make it up to the console ."], "references": ["render the tag with all arguments resolved to their actual values ."], "bleu": 0.15553014371537452, "rouge_l": 0.34512022630834516}
{"id": 4024, "code": "def validate args ( cls , tag name , * args , * * kwargs ) : if cls . min args is not None and len ( args ) < cls . min args : if cls . min args == 1 : raise Template Syntax Error ( \"'{0}' tag requires at least {1} argument\" . format ( tag name , cls . min args ) ) else : raise Template Syntax Error ( \"'{0}' tag requires at least {1} arguments\" . format ( tag name , cls . min args ) ) if cls . max args is not None and len ( args ) > cls . max args : if cls . max args == 0 : if cls . allowed kwargs : raise Template Syntax Error ( \"'{0}' tag only allows keywords arguments, for example {1}=\\\"...\\\".\" . format ( tag name , cls . allowed kwargs [ 0 ] ) ) else : raise Template Syntax Error ( \"'{0}' tag doesn't support any arguments\" . format ( tag name ) ) elif cls . max args == 1 : raise Template Syntax Error ( \"'{0}' tag only allows {1} argument.\" . format ( tag name , cls . max args ) ) else : raise Template Syntax Error ( \"'{0}' tag only allows {1} arguments.\" . format ( tag name , cls . max args ) )", "predictions": ["update tag tag tag tag"], "references": ["validate the syntax of the template tag ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4025, "code": "def get context data ( self , parent context , * tag args , * * tag kwargs ) : raise Not Implemented Error ( \"{0}.get context data() is not implemented.\" . format ( self . class . name ) )", "predictions": ["returns the context data data for the given m . . . . . . ."], "references": ["return the context data for the included template ."], "bleu": 0.207061938283276, "rouge_l": 0.505524861878453}
{"id": 4026, "code": "def render tag ( self , context , * tag args , * * tag kwargs ) : if self . as var : context [ self . as var ] = self . get value ( context , * tag args , * * tag kwargs ) return u''", "predictions": ["make a tag tag"], "references": ["rendering of the tag . it either assigns the value as variable or renders it ."], "bleu": 0.017888698387160718, "rouge_l": 0.09023668639053255}
{"id": 4027, "code": "def parse ( cls , parser , token ) : bits , as var = parse as var ( parser , token ) tag name , args , kwargs = parse token kwargs ( parser , bits , ( 'template' , ) + cls . allowed kwargs , compile args = cls . compile args , compile kwargs = cls . compile kwargs ) cls . validate args ( tag name , * args ) return cls ( tag name , as var , * args , * * kwargs )", "predictions": ["serial class to serial class ."], "references": ["parse the as var syntax ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 4028, "code": "def render tag ( self , context , * tag args , * * tag kwargs ) : if self . as var : return Base Assignment Node . render tag ( self , context , * tag args , * * tag kwargs ) else : return Base Inclusion Node . render tag ( self , context , * tag args , * * tag kwargs )", "predictions": ["apply the tasks to the self or tasks or tasks or tasks"], "references": ["rendering of the tag . it either assigns the value as variable or renders it ."], "bleu": 0.08853619422215007, "rouge_l": 0.2089041095890411}
{"id": 4029, "code": "def caffe to tensorflow session ( caffe def path , caffemodel path , inputs , graph name = 'Graph' , conversion out dir path = None , use padding same = False ) : try : from caffeflow import convert except Import Error : raise Exception ( \"caffeflow package needs to be installed to freeze Caffe models. Check out the README file.\" ) with ( dummy context mgr ( conversion out dir path ) or util . Temporary Directory ( ) ) as dir path : params values output path = os . path . join ( dir path , 'params values.npy' ) network output path = os . path . join ( dir path , 'network.py' ) convert . convert ( caffe def path , caffemodel path , params values output path , network output path , False , use padding same = use padding same ) network module = imp . load source ( 'module.name' , network output path ) network class = getattr ( network module , graph name ) network = network class ( inputs ) sess = tf . Session ( ) network . load ( params values output path , sess ) return sess", "predictions": ["convert a statistics to a self . session ."], "references": ["create a tensorflow session from a caffe model ."], "bleu": 0.17747405280050263, "rouge_l": 0.3333333333333333}
{"id": 4030, "code": "def freeze ( caffe def path , caffemodel path , inputs , output file path , output node names , graph name = 'Graph' , conversion out dir path = None , checkpoint out path = None , use padding same = False ) : with caffe to tensorflow session ( caffe def path , caffemodel path , inputs , graph name = graph name , conversion out dir path = conversion out dir path , use padding same = use padding same ) as sess : saver = tf . train . Saver ( ) with ( dummy context mgr ( checkpoint out path ) or util . Temporary Directory ( ) ) as temp dir path : checkpoint path = checkpoint out path or os . path . join ( temp dir path , 'pose.ckpt' ) saver . save ( sess , checkpoint path ) output node names = util . output node names string as list ( output node names ) tf freeze . freeze from checkpoint ( checkpoint path , output file path , output node names )", "predictions": ["color a self . ."], "references": ["freeze and shrink the graph based on a caffe model the input tensors and the output node names ."], "bleu": 0.018373002712755784, "rouge_l": 0.1508034610630408}
{"id": 4031, "code": "def save graph only ( caffe def path , caffemodel path , inputs , output file path , output node names , graph name = 'Graph' , use padding same = False ) : with caffe to tensorflow session ( caffe def path , caffemodel path , inputs , graph name = graph name , use padding same = use padding same ) as sess : tf freeze . save graph only ( sess , output file path , output node names )", "predictions": ["show graph self . ."], "references": ["save a small version of the graph based on a caffe model the input tensors and the output node names ."], "bleu": 0.012315792024227385, "rouge_l": 0.13847900113507378}
{"id": 4032, "code": "def save weights ( caffe def path , caffemodel path , inputs , output path , graph name = 'Graph' , conv var names = None , conv transpose var names = None , use padding same = False ) : with caffe to tensorflow session ( caffe def path , caffemodel path , inputs , graph name = graph name , use padding same = use padding same ) as sess : tf freeze . save weights ( sess , output path , conv var names = conv var names , conv transpose var names = conv transpose var names )", "predictions": ["save weights in a caffe object ."], "references": ["save the weights of the trainable variables each one in a different file in output_path ."], "bleu": 0.08036914931946858, "rouge_l": 0.40612516644474045}
{"id": 4033, "code": "def descendant ( self , chain path ) : public child = self . hdkeychain chain step bytes = 4 max bits per step = 2 ** 31 chain steps = [ int ( chain path [ i : i + chain step bytes * 2 ] , 16 ) % max bits per step for i in range ( 0 , len ( chain path ) , chain step bytes * 2 ) ] for step in chain steps : public child = public child . get child ( step ) return Public Keychain ( public child )", "predictions": ["return the descendant public descendant of the given chain ."], "references": ["a descendant is a child many steps down ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 4034, "code": "def object iter ( obj , parent = None , parent key = None , idx = None , siblings = None ) : obj node = Node ( value = obj , parent = parent , parent key = parent key , siblings = siblings , idx = idx ) if isinstance ( obj , list ) : siblings = len ( obj ) for i , elem in enumerate ( obj ) : for node in object iter ( elem , obj node , None , i + 1 , siblings ) : yield node elif isinstance ( obj , collections . Mapping ) : for key in obj : for node in object iter ( obj [ key ] , obj node , key ) : yield node yield obj node", "predictions": ["iterate over all nodes of a object ."], "references": ["yields each node of object graph in postorder ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 4035, "code": "def parse ( self , selector ) : log . debug ( self . obj ) tokens = lex ( selector ) if self . peek ( tokens , 'operator' ) == '*' : self . match ( tokens , 'operator' ) results = list ( object iter ( self . obj ) ) else : results = self . selector production ( tokens ) results = [ node . value for node in results ] if len ( results ) == 1 : return results [ 0 ] elif not len ( results ) : return None return results", "predictions": ["parse the selector ."], "references": ["accept a list of tokens . returns matched nodes of self . obj ."], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 4036, "code": "def selector production ( self , tokens ) : validators = [ ] if self . peek ( tokens , 'type' ) : type = self . match ( tokens , 'type' ) validators . append ( self . type production ( type ) ) if self . peek ( tokens , 'identifier' ) : key = self . match ( tokens , 'identifier' ) validators . append ( self . key production ( key ) ) if self . peek ( tokens , 'pclass' ) : pclass = self . match ( tokens , 'pclass' ) validators . append ( self . pclass production ( pclass ) ) if self . peek ( tokens , 'nth func' ) : nth func = self . match ( tokens , 'nth func' ) validators . append ( self . nth child production ( nth func , tokens ) ) if self . peek ( tokens , 'pclass func' ) : pclass func = self . match ( tokens , 'pclass func' ) validators . append ( self . pclass func production ( pclass func , tokens ) ) if not len ( validators ) : raise Selector Syntax Error ( 'no selector recognized.' ) results = self . match nodes ( validators , self . obj ) if self . peek ( tokens , 'operator' ) : operator = self . match ( tokens , 'operator' ) rvals = self . selector production ( tokens ) if operator == ',' : results . extend ( rvals ) elif operator == '>' : results = self . parents ( results , rvals ) elif operator == '~' : results = self . siblings ( results , rvals ) elif operator == ' ' : results = self . ancestors ( results , rvals ) else : raise Selector Syntax Error ( \"unrecognized operator '%s'\" % operator ) else : if len ( tokens ) : rvals = self . selector production ( tokens ) results = self . ancestors ( results , rvals ) return results", "predictions": ["process nth child production ."], "references": ["production for a full selector ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 4037, "code": "def parents ( self , lhs , rhs ) : return [ node for node in rhs if node . parent in lhs ]", "predictions": ["return the list of parents of lhs parents ."], "references": ["find nodes in rhs which have parents in lhs ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 4038, "code": "def ancestors ( self , lhs , rhs ) : def search ( node ) : if node in lhs : return True if not node . parent : return False return search ( node . parent ) return [ node for node in rhs if search ( node ) ]", "predictions": ["returns true if lhs is ancestors of the given lhs ."], "references": ["return nodes from rhs which have ancestors in lhs ."], "bleu": 0.16108992769687397, "rouge_l": 0.28818897637795277}
{"id": 4039, "code": "def siblings ( self , lhs , rhs ) : parents = [ node . parent for node in lhs ] return [ node for node in rhs if node . parent in parents ]", "predictions": ["return the list of siblings of the given rhs ."], "references": ["find nodes in rhs having common parents in lhs ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 4040, "code": "def nth child production ( self , lexeme , tokens ) : args = self . match ( tokens , 'expr' ) pat = self . nth child pat . match ( args ) if pat . group ( 5 ) : a = 2 b = 1 if pat . group ( 5 ) == 'odd' else 0 elif pat . group ( 6 ) : a = 0 b = int ( pat . group ( 6 ) ) else : sign = pat . group ( 1 ) if pat . group ( 1 ) else '+' coef = pat . group ( 2 ) if pat . group ( 2 ) else '1' a = eval ( sign + coef ) b = eval ( pat . group ( 3 ) + pat . group ( 4 ) ) if pat . group ( 3 ) else 0 reverse = False if lexeme == 'nth-last-child' : reverse = True def validate ( node ) : \"\"\"This crazy function taken from jsonselect.js:444.\"\"\" if not node . siblings : return False idx = node . idx - 1 tot = node . siblings if reverse : idx = tot - idx else : idx += 1 if a == 0 : m = b == idx else : mod = ( idx - b ) % a m = not mod and ( idx * a + b ) >= 0 return m return validate", "predictions": ["nth child production production function"], "references": ["parse args and pass them to pclass_func_validator ."], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 4041, "code": "def get Body ( self , url , method = 'GET' , headers = { } , data = None , socket = None ) : if not 'User-Agent' in headers : headers [ 'User-Agent' ] = [ 'Tensor HTTP checker' ] return self . request ( url , method , headers , data , socket )", "predictions": ["get requests from url ."], "references": ["make an http request and return the body"], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 4042, "code": "def expire ( self , age ) : now = time . time ( ) cache = self . acquire cache ( ) expired = [ k for k , v in cache . items ( ) if ( now - v [ 0 ] ) > age ] for k in expired : if k in cache : del cache [ k ] if k in self . store : del self . store [ k ] self . write cache ( cache )", "predictions": ["expire all expired expired expired expired expired ."], "references": ["expire any items in the cache older than age seconds"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 4043, "code": "def set ( self , k , v ) : self . store [ k ] = ( time . time ( ) , v ) self . persist ( )", "predictions": ["set the current value of the field ."], "references": ["set a key k to value v"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 4044, "code": "def get ( self , k ) : if self . changed ( ) : self . read ( ) if k in self . store : return tuple ( self . store [ k ] ) else : return None", "predictions": ["get a value from the store ."], "references": ["returns key contents and modify time"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4045, "code": "def contains ( self , k ) : if self . changed ( ) : self . read ( ) return k in self . store . keys ( )", "predictions": ["return whether this document contains the store k ."], "references": ["return true if key k exists"], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 4046, "code": "def rendered content ( self ) : template = self . resolve template ( self . template name ) if django . VERSION [ 1 ] < 8 : if template . name . endswith ( '.min' ) : return super ( Minified Js Template Response , self ) . rendered content else : if template . template . name . endswith ( '.min' ) : return super ( Minified Js Template Response , self ) . rendered content content = super ( Minified Js Template Response , self ) . rendered content content = jsmin . jsmin ( content ) return content", "predictions": ["return the content of the rendered template ."], "references": ["returns a minified version of the javascript content"], "bleu": 0.22679164443904004, "rouge_l": 0.25}
{"id": 4047, "code": "def get ( self , max lines = None ) : rows = [ ] self . get fn ( lambda row : rows . append ( row ) , max lines = max lines ) return rows", "predictions": ["return list of rows ."], "references": ["returns a big list of all log lines since the last run"], "bleu": 0.08860330314183162, "rouge_l": 0.2190305206463196}
{"id": 4048, "code": "def engine ( self ) : if not hasattr ( self , ' engine' ) : from cryptography . fernet import Fernet from cryptography . hazmat . backends import default backend from cryptography . hazmat . primitives import hashes digest = hashes . Hash ( hashes . SHA256 ( ) , backend = default backend ( ) ) digest . update ( current app . config [ 'SECRET KEY' ] . encode ( 'utf8' ) ) fernet key = urlsafe b64encode ( digest . finalize ( ) ) self . engine = Fernet ( fernet key ) return self . engine", "predictions": ["the engine - engine engine ."], "references": ["get cryptographic engine ."], "bleu": 0.2907153684841096, "rouge_l": 0.4149659863945578}
{"id": 4049, "code": "def create token ( self , obj id , extra data ) : return self . engine . encrypt ( super ( Encrypted Token Mix In , self ) . create token ( obj id , extra data ) )", "predictions": ["create a new token ."], "references": ["create a token referencing the object id with extra data ."], "bleu": 0.12296202211076697, "rouge_l": 0.4683301343570058}
{"id": 4050, "code": "def compat validate token ( cls , * args , * * kwargs ) : data = None for algorithm in SUPPORTED DIGEST ALGORITHMS : data = cls ( algorithm name = algorithm ) . validate token ( * args , * * kwargs ) if not data : continue return data", "predictions": ["validate that all algorithm are valid ."], "references": ["multiple algorithm - compatible token validation ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 4051, "code": "def create token ( cls , obj id , data , expires at = None ) : if expires at : s = Timed Secret Link Serializer ( expires at = expires at ) else : s = Secret Link Serializer ( ) return s . create token ( obj id , data )", "predictions": ["create a new token ."], "references": ["create the secret link token ."], "bleu": 0.31610981104846864, "rouge_l": 0.5366568914956013}
{"id": 4052, "code": "def Counter32 ( a , b , delta ) : if b < a : c = 4294967295 - a return ( c + b ) / float ( delta ) return ( b - a ) / float ( delta )", "predictions": ["return a single value of a and b ."], "references": ["32bit counter aggregator with wrapping"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4053, "code": "def Counter64 ( a , b , delta ) : if b < a : c = 18446744073709551615 - a return ( c + b ) / float ( delta ) return ( b - a ) / float ( delta )", "predictions": ["return a single value of a and b ."], "references": ["64bit counter aggregator with wrapping"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4054, "code": "def average duration ( total duration , visits ) : if not visits : seconds = 0 else : seconds = int ( round ( total duration / Decimal ( visits ) ) ) duration = timedelta ( seconds = seconds ) return str ( duration )", "predictions": ["return the average duration in seconds"], "references": ["method to calculate and format an average duration safely"], "bleu": 0.17632778423526832, "rouge_l": 0.2573839662447257}
{"id": 4055, "code": "def setup Sources ( self , config ) : sources = config . get ( 'sources' , [ ] ) for source in sources : src = self . create Source ( source ) self . setup Triggers ( source , src ) self . sources . append ( src )", "predictions": ["setup sources for the source model ."], "references": ["sets up source objects from the given config"], "bleu": 0.17820132316770915, "rouge_l": 0.13174946004319654}
{"id": 4056, "code": "def validate expires at ( form , field ) : if form . accept . data : if not field . data or datetime . utcnow ( ) . date ( ) >= field . data : raise validators . Stop Validation ( ( \"Please provide a future date.\" ) ) if not field . data or datetime . utcnow ( ) . date ( ) + timedelta ( days = 365 ) < field . data : raise validators . Stop Validation ( ( \"Please provide a date no more than 1 year into the future.\" ) )", "predictions": ["validate that the expires field is of the expires year ."], "references": ["validate that date is in the future ."], "bleu": 0.17827531042796255, "rouge_l": 0.5417406749555951}
{"id": 4057, "code": "def validate accept ( form , field ) : if field . data and form . reject . data : raise validators . Validation Error ( ( \"Both reject and accept cannot be set at the same time.\" ) )", "predictions": ["validate that the data field is not a accept form ."], "references": ["validate that accept have not been set ."], "bleu": 0.17827531042796255, "rouge_l": 0.433392539964476}
{"id": 4058, "code": "def validate reject ( form , field ) : if field . data and form . accept . data : raise validators . Validation Error ( ( \"Both reject and accept cannot be set at the same time.\" ) )", "predictions": ["validate that the reject field is not a reject ."], "references": ["validate that accept have not been set ."], "bleu": 0.18850319022747347, "rouge_l": 0.4535315985130111}
{"id": 4059, "code": "def verify token ( ) : try : from . models import Secret Link token = request . args [ 'token' ] if token and Secret Link . validate token ( token , { } ) : session [ 'accessrequests-secret-token' ] = token except Key Error : pass", "predictions": ["verify that the token is valid ."], "references": ["verify token and save in session if it s valid ."], "bleu": 0.15685718045401453, "rouge_l": 0.4273204903677758}
{"id": 4060, "code": "def init app ( self , app ) : app . before request ( verify token ) self . init config ( app ) state = App State ( app = app ) app . extensions [ 'zenodo-accessrequests' ] = state", "predictions": ["flask application initialization ."], "references": ["flask application initialization ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 4061, "code": "def name ( self ) : if ( self . device type and self . device type . code in ( Device Type . MOBILE , Device Type . TABLET ) ) : return self . device else : return self . browser", "predictions": ["return the device name of the device ."], "references": ["return a basic meaningful name based on device type"], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 4062, "code": "def warn node ( self , msg , * args , * * kwargs ) : if not msg . startswith ( 'nonlocal image URI found:' ) : warn node old ( self , msg , * args , * * kwargs )", "predictions": ["wrapper for node s node"], "references": ["do not warn on external images ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 4063, "code": "def connect receivers ( ) : request created . connect ( send email validation ) request confirmed . connect ( send confirmed notifications ) request rejected . connect ( send reject notification ) request accepted . connect ( create secret link ) request accepted . connect ( send accept notification )", "predictions": ["connect to the receivers ."], "references": ["connect receivers to signals ."], "bleu": 0.34329452398451965, "rouge_l": 0.6}
{"id": 4064, "code": "def create secret link ( request , message = None , expires at = None ) : pid , record = get record ( request . recid ) if not record : raise Record Not Found ( request . recid ) description = render template ( \"zenodo accessrequests/link description.tpl\" , request = request , record = record , pid = pid , expires at = expires at , message = message , ) request . create secret link ( record [ \"title\" ] , description = description , expires at = expires at )", "predictions": ["save a weights link link"], "references": ["receiver for request - accepted signal ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 4065, "code": "def send accept notification ( request , message = None , expires at = None ) : pid , record = get record ( request . recid ) send notification ( request . sender email , ( \"Access request accepted\" ) , \"zenodo accessrequests/emails/accepted.tpl\" , request = request , record = record , pid = pid , record link = request . link . get absolute url ( 'invenio records ui.recid' ) , message = message , expires at = expires at , )", "predictions": ["descendant for sending an accept self max self max max max - mail self max to the int max bits max"], "references": ["receiver for request - accepted signal to send email notification ."], "bleu": 0.0690889519686715, "rouge_l": 0.19869706840390877}
{"id": 4066, "code": "def send confirmed notifications ( request ) : pid , record = get record ( request . recid ) if record is None : current app . logger . error ( \"Cannot retrieve record %s. Emails not sent\" % request . recid ) return title = ( \"Access request: %(record)s\" , record = record [ \"title\" ] ) send notification ( request . receiver . email , title , \"zenodo accessrequests/emails/new request.tpl\" , request = request , record = record , pid = pid , ) send notification ( request . sender email , title , \"zenodo accessrequests/emails/confirmation.tpl\" , request = request , record = record , pid = pid , )", "predictions": ["object for sending iter notifications"], "references": ["receiver for request - confirmed signal to send email notification ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 4067, "code": "def send email validation ( request ) : token = Email Confirmation Serializer ( ) . create token ( request . id , dict ( email = request . sender email ) ) pid , record = get record ( request . recid ) send notification ( request . sender email , ( \"Access request verification\" ) , \"zenodo accessrequests/emails/validate email.tpl\" , request = request , record = record , pid = pid , days = timedelta ( seconds = current app . config [ \"ACCESSREQUESTS CONFIRMLINK EXPIRES IN\" ] ) . days , confirm link = url for ( \"invenio records ui.recid access request email confirm\" , pid value = request . recid , token = token , external = True , ) )", "predictions": ["parse email self = context = 0 = 1 = 0 = 1 = 1 = 0 = 0 = 1 = 1 = 0 = 1 = 0 = 1"], "references": ["receiver for request - created signal to send email notification ."], "bleu": 0.03901663112717908, "rouge_l": 0.05209222886421862}
{"id": 4068, "code": "def send reject notification ( request , message = None ) : pid , record = get record ( request . recid ) send notification ( request . sender email , ( \"Access request rejected\" ) , \"zenodo accessrequests/emails/rejected.tpl\" , request = request , record = record , pid = pid , message = message , )", "predictions": ["production production notification notification"], "references": ["receiver for request - rejected signal to send email notification ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 4069, "code": "def send notification ( to , subject , template , * * ctx ) : msg = Message ( subject , sender = current app . config . get ( 'SUPPORT EMAIL' ) , recipients = [ to ] ) msg . body = render template ( template , * * ctx ) send email . delay ( msg . dict )", "predictions": ["parents to the template ."], "references": ["render a template and send as email ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 4070, "code": "def create ( cls , title , owner , extra data , description = \"\" , expires at = None ) : if isinstance ( expires at , date ) : expires at = datetime . combine ( expires at , datetime . min . time ( ) ) with db . session . begin nested ( ) : obj = cls ( owner = owner , title = title , description = description , expires at = expires at , token = '' , ) db . session . add ( obj ) with db . session . begin nested ( ) : obj . token = Secret Link Factory . create token ( obj . id , extra data , expires at = expires at ) . decode ( 'utf8' ) link created . send ( obj ) return obj", "predictions": ["ancestors a new token"], "references": ["create a new secret link ."], "bleu": 0.2868106410131918, "rouge_l": 0.3860759493670886}
{"id": 4071, "code": "def revoke ( self ) : if self . revoked at is None : with db . session . begin nested ( ) : self . revoked at = datetime . utcnow ( ) link revoked . send ( self ) return True return False", "predictions": ["siblings siblings for this link . ."], "references": ["revoken a secret link ."], "bleu": 0.24446151121745047, "rouge_l": 0.34366197183098596}
{"id": 4072, "code": "def get by receiver ( cls , request id , user ) : return cls . query . filter by ( id = request id , receiver user id = user . id ) . first ( )", "predictions": ["nth nth user ."], "references": ["get access request for a specific receiver ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 4073, "code": "def confirm email ( self ) : with db . session . begin nested ( ) : if self . status != Request Status . EMAIL VALIDATION : raise Invalid Request State Error ( Request Status . EMAIL VALIDATION ) self . status = Request Status . PENDING request confirmed . send ( self )", "predictions": ["get the email email . . ."], "references": ["confirm that senders email is valid ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 4074, "code": "def create secret link ( self , title , description = None , expires at = None ) : self . link = Secret Link . create ( title , self . receiver , extra data = dict ( recid = self . recid ) , description = description , expires at = expires at , ) return self . link", "predictions": ["expire a secret self ."], "references": ["create a secret link from request ."], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 4075, "code": "def is embargoed ( record ) : return record . get ( 'access right' ) == 'embargoed' and record . get ( 'embargo date' ) and record . get ( 'embargo date' ) > datetime . utcnow ( ) . date ( )", "predictions": ["set the record record record"], "references": ["template filter to check if a record is embargoed ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 4076, "code": "def access request ( pid , record , template , * * kwargs ) : recid = int ( pid . pid value ) datastore = Local Proxy ( lambda : current app . extensions [ 'security' ] . datastore ) if record . get ( 'access right' ) != 'restricted' or not record . get ( 'access conditions' ) : abort ( 404 ) owners = record . get ( 'owners' , [ ] ) record owners = [ datastore . find user ( id = owner id ) for owner id in owners ] if not record owners : abort ( 404 ) sender = None initialdata = dict ( ) if current user . is authenticated : sender = current user initialdata [ 'email' ] = current user . email if current user . profile : initialdata [ 'full name' ] = current user . profile . full name form = Access Request Form ( formdata = request . form , * * initialdata ) if form . validate on submit ( ) : accreq = Access Request . create ( recid = recid , receiver = record owners [ 0 ] , sender full name = form . data [ 'full name' ] , sender email = form . data [ 'email' ] , justification = form . data [ 'justification' ] , sender = sender ) db . session . commit ( ) if accreq . status == Request Status . EMAIL VALIDATION : flash ( ( \"Email confirmation needed: We have sent you an email to \" \"verify your address. Please check the email and follow the \" \"instructions to complete the access request.\" ) , category = 'info' ) else : flash ( ( \"Access request submitted.\" ) , category = 'info' ) return redirect ( url for ( 'invenio records ui.recid' , pid value = recid ) ) return render template ( template , pid = pid , record = record , form = form , owners = record owners , )", "predictions": ["handle get request get request tuple tuple tuple tuple tuple tuple tuple tuple tuple tuple tuple tuple tuple tuple tuple tuple tuple tuple tuple tuple tuple tuple tuple tuple tuple tuple"], "references": ["create an access request ."], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 4077, "code": "def confirm ( pid , record , template , * * kwargs ) : recid = int ( pid . pid value ) token = request . view args [ 'token' ] data = Email Confirmation Serializer . compat validate token ( token ) if data is None : flash ( ( \"Invalid confirmation link.\" ) , category = 'danger' ) return redirect ( url for ( \"invenio records ui.recid\" , pid value = recid ) ) r = Access Request . query . get ( data [ 'id' ] ) if not r : abort ( 404 ) if r . status != Request Status . EMAIL VALIDATION : abort ( 404 ) r . confirm email ( ) db . session . commit ( ) flash ( ( \"Email validated and access request submitted.\" ) , category = 'info' ) return redirect ( url for ( \"invenio records ui.recid\" , pid value = recid ) )", "predictions": ["view function that handles confirmation records and redirect ."], "references": ["confirm email address ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 4078, "code": "def get endpoint ( self ) : return SSH Command Client Endpoint . new Connection ( reactor , b'/bin/cat' , self . username , self . hostname , port = self . port , keys = self . keys , password = self . password , known Hosts = self . known Hosts )", "predictions": ["rendered content for proxied proxied connection"], "references": ["creates a generic endpoint connection that doesn t finish"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4079, "code": "def reverse ( self , col ) : if col in self . options : if self . is selected ( col ) : return col if not self . asc else '-{0}' . format ( col ) else : return col return None", "predictions": ["returns the column in this column"], "references": ["get reverse direction of ordering ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4080, "code": "def selected ( self ) : if self . selected : return self . selected if self . asc else \"-{0}\" . format ( self . selected ) return None", "predictions": ["the engine - side side of the engine not engine not engine"], "references": ["get column which is being order by ."], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 4081, "code": "def items ( self ) : if self . asc is not None : if self . selected and self . asc : return self . query . order by ( self . selected ) elif self . selected and not self . asc : return self . query . order by ( desc ( self . selected ) ) return self . query", "predictions": ["returns the list of create create create create create create create create create create create key from the encrypt id id id id id id id"], "references": ["get query with correct ordering ."], "bleu": 0.03925345689749394, "rouge_l": 0.0}
{"id": 4082, "code": "def records ( ) : import uuid from invenio records . api import Record from invenio pidstore . models import Persistent Identifier , PID Status create test user ( ) indexer = Record Indexer ( ) with db . session . begin nested ( ) : rec uuid = uuid . uuid4 ( ) pid1 = Persistent Identifier . create ( 'recid' , '1' , object type = 'rec' , object uuid = rec uuid , status = PID Status . REGISTERED ) Record . create ( { 'title' : 'Registered' , 'description' : 'This is an awesome description' , 'control number' : '1' , 'access right' : 'restricted' , 'access conditions' : 'fuu' , 'owners' : [ 1 , 2 ] , 'recid' : 1 } , id = rec uuid ) indexer . index by id ( pid1 . object uuid ) db . session . commit ( ) sleep ( 3 )", "predictions": ["load compat data from an kwargs kwargs kwargs kwargs kwargs kwargs kwargs ."], "references": ["load test data fixture ."], "bleu": 0.1135935489027116, "rouge_l": 0.36237623762376237}
{"id": 4083, "code": "def init ssh ( self ) : self . ssh host = self . config . get ( 'ssh host' , self . hostname ) self . known hosts = self . config . get ( 'ssh knownhosts file' , self . tensor . config . get ( 'ssh knownhosts file' , None ) ) self . ssh keyfile = self . config . get ( 'ssh keyfile' , self . tensor . config . get ( 'ssh keyfile' , None ) ) self . ssh key = self . config . get ( 'ssh key' , self . tensor . config . get ( 'ssh key' , None ) ) self . ssh keypass = self . config . get ( 'ssh keypass' , self . tensor . config . get ( 'ssh keypass' , None ) ) self . ssh user = self . config . get ( 'ssh username' , self . tensor . config . get ( 'ssh username' , None ) ) self . ssh password = self . config . get ( 'ssh password' , self . tensor . config . get ( 'ssh password' , None ) ) self . ssh port = self . config . get ( 'ssh port' , self . tensor . config . get ( 'ssh port' , 22 ) ) if not ( self . ssh key or self . ssh keyfile or self . ssh password ) : raise Exception ( \"To use SSH you must specify *one* of ssh key,\" \" ssh keyfile or ssh password for this source\" \" check or globally\" ) if not self . ssh user : raise Exception ( \"ssh username must be set\" ) self . ssh keydb = [ ] c Hash = hashlib . sha1 ( ':' . join ( ( self . ssh host , self . ssh user , str ( self . ssh port ) , str ( self . ssh password ) , str ( self . ssh key ) , str ( self . ssh keyfile ) ) ) . encode ( ) ) . hexdigest ( ) if c Hash in self . tensor . host Connector Cache : self . ssh client = self . tensor . host Connector Cache . get ( c Hash ) self . ssh connector = False else : self . ssh connector = True self . ssh client = ssh . SSH Client ( self . ssh host , self . ssh user , self . ssh port , password = self . ssh password , knownhosts = self . known hosts ) if self . ssh keyfile : self . ssh client . add Key File ( self . ssh keyfile , self . ssh keypass ) if self . ssh key : self . ssh client . add Key String ( self . ssh key , self . ssh keypass ) self . tensor . host Connector Cache [ c Hash ] = self . ssh client", "predictions": ["setup the token for the token"], "references": ["configure ssh client options"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4084, "code": "def start Timer ( self ) : self . td = self . t . start ( self . inter ) if self . use ssh and self . ssh connector : self . ssh client . connect ( )", "predictions": ["start - . events ."], "references": ["starts the timer for this source"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 4085, "code": "def create Event ( self , state , description , metric , prefix = None , hostname = None , aggregation = None , evtime = None ) : if prefix : service name = self . service + \".\" + prefix else : service name = self . service return Event ( state , service name , description , metric , self . ttl , hostname = hostname or self . hostname , aggregation = aggregation , evtime = evtime , tags = self . tags , attributes = self . attributes )", "predictions": ["creates a new float with the given description and description"], "references": ["creates an event object from the source configuration"], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 4086, "code": "def create Log ( self , type , data , evtime = None , hostname = None ) : return Event ( None , type , data , 0 , self . ttl , hostname = hostname or self . hostname , evtime = evtime , tags = self . tags , type = 'log' )", "predictions": ["creates a new seconds for the given visits ."], "references": ["creates an event object from the source configuration"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 4087, "code": "def index ( ) : query = request . args . get ( 'query' , '' ) order = request . args . get ( 'sort' , '-created' ) try : page = int ( request . args . get ( 'page' , 1 ) ) per page = int ( request . args . get ( 'per page' , 20 ) ) except ( Type Error , Value Error ) : abort ( 404 ) form = Delete Form ( request . form ) if form . validate on submit ( ) : link = Secret Link . query by owner ( current user ) . filter by ( id = form . link . data ) . first ( ) if link . revoke ( ) : flash ( ( \"Shared link revoked.\" ) , category = 'success' ) db . session . commit ( ) links = Secret Link . query by owner ( current user ) . filter ( Secret Link . revoked at . is ( None ) ) if query : lquery = \"%{0}%\" . format ( query ) links = links . filter ( Secret Link . title . like ( lquery ) | Secret Link . description . like ( lquery ) ) ordering = Query Ordering ( links , [ 'title' , 'created' , 'expires at' ] , order ) links = ordering . items ( ) requests = Access Request . query by receiver ( current user ) . filter by ( status = Request Status . PENDING ) . order by ( 'created' ) return render template ( \"zenodo accessrequests/settings/index.html\" , links pagination = links . paginate ( page , per page = per page ) , requests = requests , query = query , order = ordering , get record = get record , form = Delete Form ( ) , )", "predictions": ["show a link setup . . . . ."], "references": ["list pending access requests and shared links ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 4088, "code": "def create Client ( self ) : server = self . config . get ( 'server' , 'localhost' ) port = self . config . get ( 'port' , 5555 ) failover = self . config . get ( 'failover' , False ) self . factory = riemann . Riemann Client Factory ( server , failover = failover ) if failover : initial = random . choice ( server ) else : initial = server log . msg ( 'Connecting to Riemann on %s:%s' % ( initial , port ) ) if self . tls : if SSL : self . connector = reactor . connect SSL ( initial , port , self . factory , Client TLS Context ( self . key , self . cert ) ) else : log . msg ( '[FATAL] SSL support not available!' ' Please install Py Open SSL. Exiting now' ) reactor . stop ( ) else : self . connector = reactor . connect TCP ( initial , port , self . factory ) d = defer . Deferred ( ) def cb ( ) : if hasattr ( self . factory , 'proto' ) and self . factory . proto : self . t . start ( self . inter ) d . callback ( None ) else : reactor . call Later ( 0.01 , cb ) cb ( ) return d", "predictions": ["validate the field ."], "references": ["create a tcp connection to riemann with automatic reconnection"], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 4089, "code": "def stop ( self ) : self . t . stop ( ) self . factory . stop Trying ( ) self . connector . disconnect ( )", "predictions": ["validate the timer field field field field field field field field field field field field field field field field field field field field field field field field field field field field"], "references": ["stop this client ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4090, "code": "def tick ( self ) : if self . factory . proto : if ( self . pressure < 0 ) or ( self . factory . proto . pressure <= self . pressure ) : self . empty Queue ( ) elif self . expire : for i , e in enumerate ( self . events ) : if ( time . time ( ) - e . time ) > e . ttl : self . events . pop ( i )", "predictions": ["close the . validate the ."], "references": ["clock tick called every self . inter"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4091, "code": "def empty Queue ( self ) : if self . events : if self . queue Depth and ( len ( self . events ) > self . queue Depth ) : events = self . events [ : self . queue Depth ] self . events = self . events [ self . queue Depth : ] else : events = self . events self . events = [ ] if self . allow nan : self . factory . proto . send Events ( events ) else : self . factory . proto . send Events ( [ e for e in events if e . metric is not None ] )", "predictions": ["re - verify the queue of the queue"], "references": ["remove all or self . queuedepth events from the queue"], "bleu": 0.1643685581109115, "rouge_l": 0.21785714285714283}
{"id": 4092, "code": "def create Client ( self ) : server = self . config . get ( 'server' , '127.0.0.1' ) port = self . config . get ( 'port' , 5555 ) def connect ( ip ) : self . protocol = riemann . Riemann UDP ( ip , port ) self . endpoint = reactor . listen UDP ( 0 , self . protocol ) d = reactor . resolve ( server ) d . add Callback ( connect ) return d", "predictions": ["init a connection to the server before creating it if not already connected before the reactor"], "references": ["create a udp connection to riemann"], "bleu": 0.10878661088699644, "rouge_l": 0.2970779220779221}
{"id": 4093, "code": "def create Client ( self ) : server = self . config . get ( 'server' , 'localhost' ) port = int ( self . config . get ( 'port' , 9200 ) ) self . client = elasticsearch . Elastic Search ( self . url , self . user , self . password , self . index ) self . t . start ( self . inter )", "predictions": ["name of the browser connection type type type type type type type type type type type type type type type type type type type type type type type type type type"], "references": ["sets up http connector and starts queue timer"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4094, "code": "def tick ( self ) : if self . events : if self . queue Depth and ( len ( self . events ) > self . queue Depth ) : events = self . events [ : self . queue Depth ] self . events = self . events [ self . queue Depth : ] else : events = self . events self . events = [ ] try : result = yield self . send Events ( events ) if result . get ( 'errors' , False ) : log . msg ( repr ( result ) ) self . events . extend ( events ) except Exception as e : log . msg ( 'Could not connect to elasticsearch ' + str ( e ) ) self . events . extend ( events )", "predictions": ["connect the events of the events"], "references": ["clock tick called every self . inter"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 4095, "code": "def encode Event ( self , event ) : pbevent = proto pb2 . Event ( time = int ( event . time ) , state = event . state , service = event . service , host = event . hostname , description = event . description , tags = event . tags , ttl = event . ttl , ) if event . metric is not None : if isinstance ( event . metric , int ) : pbevent . metric sint64 = event . metric pbevent . metric f = float ( event . metric ) else : pbevent . metric d = float ( event . metric ) pbevent . metric f = float ( event . metric ) if event . attributes is not None : for key , value in event . attributes . items ( ) : attribute = pbevent . attributes . add ( ) attribute . key , attribute . value = key , value return pbevent", "predictions": ["connect the request object to the request object notifications notifications notifications notifications notifications notifications notifications notifications notifications ."], "references": ["adapts an event object to a riemann protobuf event event"], "bleu": 0.08961672320242714, "rouge_l": 0.1506172839506173}
{"id": 4096, "code": "def encode Message ( self , events ) : message = proto pb2 . Msg ( events = [ self . encode Event ( e ) for e in events if e . type == 'riemann' ] ) return message . Serialize To String ( )", "predictions": ["encodes events into json"], "references": ["encode a list of tensor events with protobuf"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 4097, "code": "def decode Message ( self , data ) : message = proto pb2 . Msg ( ) message . Parse From String ( data ) return message", "predictions": ["decode a message into a byte array ."], "references": ["decode a protobuf message into a list of tensor events"], "bleu": 0.27643386259177455, "rouge_l": 0.5446428571428571}
{"id": 4098, "code": "def send Events ( self , events ) : self . pressure += 1 self . send String ( self . encode Message ( events ) )", "predictions": ["send a message to the socket ."], "references": ["send a tensor event to riemann"], "bleu": 0.2626909894424158, "rouge_l": 0.4680306905370844}
{"id": 4099, "code": "def generate ( ctx , url , * args , * * kwargs ) : file previews = ctx . obj [ 'file previews' ] options = { } metadata = kwargs [ 'metadata' ] width = kwargs [ 'width' ] height = kwargs [ 'height' ] output format = kwargs [ 'format' ] if metadata : options [ 'metadata' ] = metadata . split ( ',' ) if width : options . setdefault ( 'size' , { } ) options [ 'size' ] [ 'width' ] = width if height : options . setdefault ( 'size' , { } ) options [ 'size' ] [ 'height' ] = height if output format : options [ 'format' ] = output format results = file previews . generate ( url , * * options ) click . echo ( results )", "predictions": ["generate an url for a given url ."], "references": ["generate preview for url ."], "bleu": 0.239802967618271, "rouge_l": 0.6421052631578947}
{"id": 4100, "code": "def retrieve ( ctx , preview id , * args , * * kwargs ) : file previews = ctx . obj [ 'file previews' ] results = file previews . retrieve ( preview id ) click . echo ( results )", "predictions": ["retrieve details for a preview preview preview ."], "references": ["retreive preview results for id ."], "bleu": 0.19070828081828378, "rouge_l": 0.2932692307692307}
{"id": 4101, "code": "def message loop ( self , t q , r q ) : t msg = { } while t msg . get ( \"state\" , \"\" ) != \" DIE \" : try : t msg = t q . get ( True , self . cycle sleep ) self . task = t msg . get ( \"task\" , \"\" ) if self . task != \"\" : self . task . task start = time . time ( ) self . r q send ( { \"w id\" : self . w id , \"task\" : self . task , \"state\" : \" ACK \" } ) self . cycle sleep = self . task . worker loop delay self . task . result = self . task . run ( ) self . task . task stop = time . time ( ) self . r q send ( { \"w id\" : self . w id , \"task\" : self . task , \"state\" : \" FINISHED \" } ) self . task = None except Empty : pass except Full : time . sleep ( 0.1 ) except : if self . task is not None : self . task . task stop = time . time ( ) tb str = \"\" . join ( tb . format exception ( * ( sys . exc info ( ) ) ) ) self . r q send ( { \"w id\" : self . w id , \"task\" : self . task , \"error\" : tb str , \"state\" : \" ERROR \" , } ) return", "predictions": ["message loop for the message loop"], "references": ["loop through messages and execute tasks"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 4102, "code": "def log time ( self ) : if self . hot loop and self . time delta >= self . log interval : return True return False", "predictions": ["return true if the time should be executed ."], "references": ["return true if it s time to log"], "bleu": 0.2777619034011791, "rouge_l": 0.4756335282651072}
{"id": 4103, "code": "def log message ( self ) : time delta = deepcopy ( self . time delta ) total work time = self . worker count * time delta time worked = sum ( self . exec times ) pct busy = time worked / total work time * 100.0 min task time = min ( self . exec times ) avg task time = sum ( self . exec times ) / len ( self . exec times ) max task time = max ( self . exec times ) min queue time = min ( self . queue times ) avg queue time = sum ( self . queue times ) / len ( self . queue times ) max queue time = max ( self . queue times ) time delta = self . time delta total tasks = len ( self . exec times ) avg task rate = total tasks / time delta self . reset ( ) task msg = \"\"\"Ran {0} tasks, {1} tasks/s; {2} workers {3}% busy\"\"\" . format ( total tasks , round ( avg task rate , 1 ) , self . worker count , round ( pct busy , 1 ) ) task mam = \"\"\"     Task run times: {0}/{1}/{2} (min/avg/max)\"\"\" . format ( round ( min task time , 3 ) , round ( avg task time , 3 ) , round ( max task time , 3 ) ) queue mam = \"\"\"     Time in queue: {0}/{1}/{2} (min/avg/max)\"\"\" . format ( round ( min queue time , 6 ) , round ( avg queue time , 6 ) , round ( max queue time , 6 ) ) return \"\"\"{0}\\n{1}\\n{2}\"\"\" . format ( task msg , task mam , queue mam )", "predictions": ["log the message in the queue"], "references": ["build a log message and reset the stats"], "bleu": 0.18822631894109965, "rouge_l": 0.4178082191780822}
{"id": 4104, "code": "def post Construction ( self ) : self . set Window Title ( 'Filesystem Browser' ) self . filesystem Widget . sort By Column ( 0 , Qt Core . Qt . Ascending Order ) self . bookmarks Widget . hide ( ) self . accept Button . set Default ( True ) self . accept Button . set Disabled ( True ) self . accept Button . clicked . connect ( self . accept ) self . cancel Button . clicked . connect ( self . reject ) self . configure Shortcuts ( ) self . set Location ( self . root ) self . filesystem Widget . horizontal Header ( ) . set Resize Mode ( Qt Gui . Q Header View . Resize To Contents ) self . filesystem Widget . horizontal Header ( ) . set Resize Mode ( 0 , Qt Gui . Q Header View . Stretch ) self . up Button . clicked . connect ( self . on Navigate Up Button Clicked ) self . location Widget . current Index Changed . connect ( self . on Navigate ) self . filesystem Widget . activated . connect ( self . on Activate Item ) selection Model = self . filesystem Widget . selection Model ( ) selection Model . current Row Changed . connect ( self . on Select Item )", "predictions": ["create the qtableview for when the bookmarks button is construction ."], "references": ["perform post - construction operations ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 4105, "code": "def configure Shortcuts ( self ) : self . up Shortcut = Qt Gui . Q Shortcut ( Qt Gui . Q Key Sequence ( 'Backspace' ) , self ) self . up Shortcut . set Auto Repeat ( False ) self . up Shortcut . activated . connect ( self . on Navigate Up Button Clicked )", "predictions": ["configure the basic basic widget ."], "references": ["add keyboard shortcuts to navigate the filesystem ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4106, "code": "def on Activate Item ( self , index ) : item = self . filesystem Widget . model ( ) . item ( index ) if not isinstance ( item , riffle . model . File ) : self . accept Button . set Disabled ( True ) self . set Location ( item . path , interactive = True )", "predictions": ["called when an item is interactive"], "references": ["handle activation of item in listing ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4107, "code": "def on Select Item ( self , selection , previous Selection ) : self . accept Button . set Enabled ( True ) del self . selected [ : ] item = self . filesystem Widget . model ( ) . item ( selection ) self . selected . append ( item . path )", "predictions": ["memorizes an item in the model"], "references": ["handle selection of item in listing ."], "bleu": 0.24608524656663955, "rouge_l": 0.3034825870646766}
{"id": 4108, "code": "def on Navigate ( self , index ) : if index > 0 : self . set Location ( self . location Widget . item Data ( index ) , interactive = True )", "predictions": ["set the value of the item in the location"], "references": ["handle selection of path segment ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 4109, "code": "def segment Path ( self , path ) : parts = [ ] model = self . filesystem Widget . model ( ) remainder = path while True : if remainder == model . root . path : break if remainder : parts . append ( remainder ) head , tail = os . path . split ( remainder ) if head == remainder : break remainder = head parts . append ( model . root . path ) return parts", "predictions": ["return the path to the segment of the given path ."], "references": ["return list of valid * path * segments ."], "bleu": 0.14323145079400493, "rouge_l": 0.4073455759599332}
{"id": 4110, "code": "def finalize options ( self ) : self . resource source path = os . path . join ( RESOURCE PATH , 'resource.qrc' ) self . resource target path = RESOURCE TARGET PATH", "predictions": ["finalize the resource options ."], "references": ["finalize options to be used ."], "bleu": 0.2658156069371863, "rouge_l": 0.5366568914956013}
{"id": 4111, "code": "def add Child ( self , item ) : if item . parent and item . parent != self : item . parent . remove Child ( item ) self . children . append ( item ) item . parent = self", "predictions": ["add an item to the parent list"], "references": ["add * item * as child of this item ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 4112, "code": "def fetch Children ( self ) : children = [ ] for entry in Q Dir . drives ( ) : path = os . path . normpath ( entry . canonical File Path ( ) ) children . append ( Mount ( path ) ) return children", "predictions": ["fetch the directory under the canonical environment ."], "references": ["fetch and return new child items ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 4113, "code": "def fetch Children ( self ) : children = [ ] paths = [ ] for name in os . listdir ( self . path ) : paths . append ( os . path . normpath ( os . path . join ( self . path , name ) ) ) collections , remainder = clique . assemble ( paths , [ clique . PATTERNS [ 'frames' ] ] ) for path in remainder : try : child = Item Factory ( path ) except Value Error : pass else : children . append ( child ) for collection in collections : children . append ( Collection ( collection ) ) return children", "predictions": ["fetch all the children of the current folder ."], "references": ["fetch and return new child items ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 4114, "code": "def fetch Children ( self ) : children = [ ] for path in self . collection : try : child = Item Factory ( path ) except Value Error : pass else : children . append ( child ) return children", "predictions": ["returns a list of all children in the current collection ."], "references": ["fetch and return new child items ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 4115, "code": "def row Count ( self , parent ) : if parent . column ( ) > 0 : return 0 if parent . is Valid ( ) : item = parent . internal Pointer ( ) else : item = self . root return len ( item . children )", "predictions": ["return number of children of given parent ."], "references": ["return number of children * parent * index has ."], "bleu": 0.37810137198038724, "rouge_l": 0.6535714285714286}
{"id": 4116, "code": "def index ( self , row , column , parent ) : if not self . has Index ( row , column , parent ) : return Q Model Index ( ) if not parent . is Valid ( ) : item = self . root else : item = parent . internal Pointer ( ) try : child = item . children [ row ] except Index Error : return Q Model Index ( ) else : return self . create Index ( row , column , child )", "predictions": ["index a row from the parent ."], "references": ["return index for * row * and * column * under * parent * ."], "bleu": 0.07448668213629092, "rouge_l": 0.3412587412587413}
{"id": 4117, "code": "def path Index ( self , path ) : if path == self . root . path : return Q Model Index ( ) if not path . startswith ( self . root . path ) : return Q Model Index ( ) parts = [ ] while True : if path == self . root . path : break head , tail = os . path . split ( path ) if head == path : if path : parts . append ( path ) break parts . append ( tail ) path = head parts . reverse ( ) if parts : item = self . root count = 0 for count , part in enumerate ( parts ) : matched = False for child in item . children : if child . name == part : item = child matched = True break if not matched : break if count + 1 == len ( parts ) : return self . create Index ( item . row , 0 , item ) return Q Model Index ( )", "predictions": ["return the path to the path of the path ."], "references": ["return index of item with * path * ."], "bleu": 0.15851165692617156, "rouge_l": 0.42508710801393734}
{"id": 4118, "code": "def parent ( self , index ) : if not index . is Valid ( ) : return Q Model Index ( ) item = index . internal Pointer ( ) if not item : return Q Model Index ( ) parent = item . parent if not parent or parent == self . root : return Q Model Index ( ) return self . create Index ( parent . row , 0 , parent )", "predictions": ["return the parent item with the given index ."], "references": ["return parent of * index * ."], "bleu": 0.17747405280050263, "rouge_l": 0.5115303983228512}
{"id": 4119, "code": "def data ( self , index , role ) : if not index . is Valid ( ) : return None column = index . column ( ) item = index . internal Pointer ( ) if role == self . ITEM ROLE : return item elif role == Qt . Display Role : if column == 0 : return item . name elif column == 1 : if item . size : return item . size elif column == 2 : return item . type elif column == 3 : if item . modified is not None : return item . modified . strftime ( '%c' ) elif role == Qt . Decoration Role : if column == 0 : return self . icon Factory . icon ( item ) elif role == Qt . Text Alignment Role : if column == 1 : return Qt . Align Right else : return Qt . Align Left return None", "predictions": ["return the data for the given role"], "references": ["return data for * index * according to * role * ."], "bleu": 0.13597602315271134, "rouge_l": 0.40197693574958815}
{"id": 4120, "code": "def header Data ( self , section , orientation , role ) : if orientation == Qt . Horizontal : if section < len ( self . columns ) : column = self . columns [ section ] if role == Qt . Display Role : return column return None", "predictions": ["get the column column for the given orientation ."], "references": ["return label for * section * according to * orientation * and * role * ."], "bleu": 0.07711214194298908, "rouge_l": 0.22846441947565538}
{"id": 4121, "code": "def can Fetch More ( self , index ) : if not index . is Valid ( ) : item = self . root else : item = index . internal Pointer ( ) return item . can Fetch More ( )", "predictions": ["return whether the item can be fetch or not"], "references": ["return if more data available for * index * ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 4122, "code": "def fetch More ( self , index ) : if not index . is Valid ( ) : item = self . root else : item = index . internal Pointer ( ) if item . can Fetch More ( ) : start Index = len ( item . children ) additional Children = item . fetch Children ( ) end Index = start Index + len ( additional Children ) - 1 if end Index >= start Index : self . begin Insert Rows ( index , start Index , end Index ) for new Child in additional Children : item . add Child ( new Child ) self . end Insert Rows ( )", "predictions": ["fetch a more from the given index ."], "references": ["fetch additional data under * index * ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 4123, "code": "def less Than ( self , left , right ) : source Model = self . source Model ( ) if source Model : left Item = source Model . item ( left ) right Item = source Model . item ( right ) if ( isinstance ( left Item , Directory ) and not isinstance ( right Item , Directory ) ) : return self . sort Order ( ) == Qt . Ascending Order elif ( not isinstance ( left Item , Directory ) and isinstance ( right Item , Directory ) ) : return self . sort Order ( ) == Qt . Descending Order return super ( Filesystem Sort Proxy , self ) . less Than ( left , right )", "predictions": ["sorts the list of columns in the list of objects"], "references": ["return ordering of * left * vs * right * ."], "bleu": 0.11406351620367239, "rouge_l": 0.09442724458204334}
{"id": 4124, "code": "def path Index ( self , path ) : source Model = self . source Model ( ) if not source Model : return Q Model Index ( ) return self . map From Source ( source Model . path Index ( path ) )", "predictions": ["return the path to the source file at the given path ."], "references": ["return index of item with * path * ."], "bleu": 0.1235622127262679, "rouge_l": 0.2932692307692307}
{"id": 4125, "code": "def item ( self , index ) : source Model = self . source Model ( ) if not source Model : return None return source Model . item ( self . map To Source ( index ) )", "predictions": ["return item from model"], "references": ["return item at * index * ."], "bleu": 0.22336835181428535, "rouge_l": 0.346590909090909}
{"id": 4126, "code": "def icon ( self , index ) : source Model = self . source Model ( ) if not source Model : return None return source Model . icon ( self . map To Source ( index ) )", "predictions": ["return the icon icon from the given index"], "references": ["return icon for index ."], "bleu": 0.19070828081828378, "rouge_l": 0.48157894736842105}
{"id": 4127, "code": "def has Children ( self , index ) : source Model = self . source Model ( ) if not source Model : return False return source Model . has Children ( self . map To Source ( index ) )", "predictions": ["return true if index is a children index ."], "references": ["return if * index * has children ."], "bleu": 0.18575057999133596, "rouge_l": 0.594541910331384}
{"id": 4128, "code": "def can Fetch More ( self , index ) : source Model = self . source Model ( ) if not source Model : return False return source Model . can Fetch More ( self . map To Source ( index ) )", "predictions": ["returns true if index is a = false otherwise ."], "references": ["return if more data available for * index * ."], "bleu": 0.14991106946711685, "rouge_l": 0.3}
{"id": 4129, "code": "def fetch More ( self , index ) : source Model = self . source Model ( ) if not source Model : return False return source Model . fetch More ( self . map To Source ( index ) )", "predictions": ["decode this data source for the given data data"], "references": ["fetch additional data under * index * ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 4130, "code": "def type ( self , item ) : icon Type = Icon Type . Unknown if isinstance ( item , riffle . model . Computer ) : icon Type = Icon Type . Computer elif isinstance ( item , riffle . model . Mount ) : icon Type = Icon Type . Mount elif isinstance ( item , riffle . model . Directory ) : icon Type = Icon Type . Directory elif isinstance ( item , riffle . model . File ) : icon Type = Icon Type . File elif isinstance ( item , riffle . model . Collection ) : icon Type = Icon Type . Collection return icon Type", "predictions": ["return the send send send to the model 1 1 1 if any"], "references": ["return appropriate icon type for * item * ."], "bleu": 0.09552040806823771, "rouge_l": 0.09399075500770414}
{"id": 4131, "code": "def get max fd ( self ) : limits = resource . getrlimit ( resource . RLIMIT NOFILE ) result = limits [ 1 ] if result == resource . RLIM INFINITY : result = maxfd return result", "predictions": ["return - . the resource - ctx - ctx - ctx - kwargs file file file file file file file file file file file file file file file file file file"], "references": ["return the maximum file descriptor value ."], "bleu": 0.04906081629292276, "rouge_l": 0.17818889970788704}
{"id": 4132, "code": "def close fd ( self , fd ) : try : os . close ( fd ) except OS Error , exc : if exc . errno != errno . EBADF : msg = \"Failed to close file descriptor {}: {}\" . format ( fd , exc ) raise Error ( msg )", "predictions": ["retrieve the echo echo echo echo the given echo echo echo the given echo . . ."], "references": ["close a file descriptor if it is open ."], "bleu": 0.07223943354597204, "rouge_l": 0.0814419225634179}
{"id": 4133, "code": "def close open fds ( self ) : maxfd = self . get max fd ( ) for fd in reversed ( range ( maxfd ) ) : if fd not in self . exclude fds : self . close fd ( fd )", "predictions": ["message message to message . ."], "references": ["close open file descriptors ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 4134, "code": "def redirect ( self , stream , target ) : if target is None : target fd = os . open ( os . devnull , os . O RDWR ) else : target fd = target . fileno ( ) os . dup2 ( target fd , stream . fileno ( ) )", "predictions": ["log the file to a stream stream"], "references": ["redirect a system stream to the provided target ."], "bleu": 0.1755217914979255, "rouge_l": 0.24448897795591182}
{"id": 4135, "code": "def is valid s3 url ( url ) : if url . startswith ( 'source:' ) : return True scheme , netloc , path , , , = urlparse ( url ) port except = Remote Port Validation Error ( 'Port value %s is not a valid s3 location' % url ) if len ( scheme ) < 2 : raise port except if 's3' in scheme or 's3' in netloc or 's3' in path : return True else : raise port except", "predictions": ["check if a self self = true"], "references": ["checks if the url contains s3 . not an accurate validation of the url"], "bleu": 0.06833381956448398, "rouge_l": 0.08983799705449189}
{"id": 4136, "code": "def get template abs path ( filename ) : if os . path . isabs ( filename ) and os . path . isfile ( filename ) : return filename else : return os . path . join ( os . getcwd ( ) , filename )", "predictions": ["sort the absolute path of a file ."], "references": ["return a valid absolute path . filename can be relative or absolute ."], "bleu": 0.1283572790104489, "rouge_l": 0.2739520958083832}
{"id": 4137, "code": "def list ( self , s3 folder = '' , full key data = False ) : if not s3 folder . startswith ( '/' ) : s3 folder = '/' + s3 folder s3 prefix = self . prefix + s3 folder bucket data = self . client . list objects ( Bucket = self . bucket , Prefix = s3 prefix ) if full key data : return bucket data [ 'Contents' ] else : return [ k [ 'Key' ] for k in bucket data [ 'Contents' ] ]", "predictions": ["configure s3 in s3 folder ."], "references": ["get a list of keys for the accounts"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 4138, "code": "def build worklfow json ( self ) : wf json = { 'tasks' : [ ] , 'name' : 'cloud-harness %s' % str ( uuid . uuid4 ( ) ) } task def = json . loads ( self . task template . json ( ) ) d = { \"name\" : task def [ 'name' ] , \"outputs\" : [ ] , \"inputs\" : [ ] , \"task Type\" : task def [ 'task Type' ] } for port in self . task template . input ports : port value = port . value if port value is False : port value = 'false' if port value is True : port value = 'true' d [ 'inputs' ] . append ( { \"name\" : port . name , \"value\" : port value } ) for port in self . task template . output ports : d [ 'outputs' ] . append ( { \"name\" : port . name } ) wf json [ 'tasks' ] . append ( d ) for port in self . task template . output ports : if hasattr ( port , 'stage To S3' ) and port . stage To S3 : save location = '{customer storage}/{run name}/{port}' . format ( customer storage = self . storage . location , run name = self . task template . run name , port = port . name ) new task = dict ( * * self . STAGE TO S3 ) new task [ 'inputs' ] = [ { 'name' : 'data' , 'source' : '%s:%s' % ( task def [ 'name' ] , port . name ) } , { 'name' : 'destination' , 'value' : save location } ] wf json [ 'tasks' ] . append ( new task ) return wf json", "predictions": ["on the worklfow on the worklfow"], "references": ["build a workflow definition from the cloud_harness task ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4139, "code": "def execute ( self , override wf json = None ) : r = self . gbdx . post ( self . URL , json = self . json if override wf json is None else override wf json ) try : r . raise for status ( ) except : print ( \"GBDX API Status Code: %s\" % r . status code ) print ( \"GBDX API Response: %s\" % r . text ) self . id = None return self . id = r . json ( ) [ 'id' ] self . refresh status ( )", "predictions": ["on a shell - rpc request accept it"], "references": ["execute the cloud_harness task ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4140, "code": "def archive ( folder , dry run = False ) : for f in folder : if not os . path . exists ( f ) : bail ( 'folder does not exist: ' + f ) archive safe ( folder , PROJ ARCHIVE , dry run = dry run )", "predictions": ["check if the folder is an on the folder directory"], "references": ["move an active project to the archive ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 4141, "code": "def mkdir ( p ) : isdir = os . path . isdir stack = [ os . path . abspath ( p ) ] while not isdir ( stack [ - 1 ] ) : parent dir = os . path . dirname ( stack [ - 1 ] ) stack . append ( parent dir ) while stack : p = stack . pop ( ) if not isdir ( p ) : os . mkdir ( p )", "predictions": ["create a directory if it doesn t exist"], "references": ["the equivalent of mkdir - p in shell ."], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 4142, "code": "def list ( pattern = ( ) ) : globs = [ '*{0}*' . format ( p ) for p in pattern ] + [ '*' ] matches = [ ] offset = len ( PROJ ARCHIVE ) + 1 for suffix in globs : glob pattern = os . path . join ( PROJ ARCHIVE , '*' , '*' , suffix ) matches . append ( set ( f [ offset : ] for f in glob . glob ( glob pattern ) ) ) matches = reduce ( lambda x , y : x . intersection ( y ) , matches ) for m in sorted ( matches ) : print ( m )", "predictions": ["finalize all . . py files path path path path path path path path path path path path path path path path path path path path path path path path path"], "references": ["list the contents of the archive directory ."], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 4143, "code": "def restore ( folder ) : if os . path . isdir ( folder ) : bail ( 'a folder of the same name already exists!' ) pattern = os . path . join ( PROJ ARCHIVE , '*' , '*' , folder ) matches = glob . glob ( pattern ) if not matches : bail ( 'no project matches: ' + folder ) if len ( matches ) > 1 : print ( 'Warning: multiple matches, picking the most recent' , file = sys . stderr ) source = sorted ( matches ) [ - 1 ] print ( source , '-->' , folder ) shutil . move ( source , '.' )", "predictions": ["add remove remove remove remove remove remove remove from folder . . . ."], "references": ["restore a project from the archive ."], "bleu": 0.09782375748961449, "rouge_l": 0.2026578073089701}
{"id": 4144, "code": "def validate storage path ( cls , path , projects allowed = True ) : if not path or not isinstance ( path , str ) or path [ 0 ] != '/' or path == '/' : raise Storage Argument Exception ( 'The path must be a string, start with a slash (/), and be longer' ' than 1 character.' ) if not projects allowed and len ( [ elem for elem in path . split ( '/' ) if elem ] ) == 1 : raise Storage Argument Exception ( 'This method does not accept projects in the path.' )", "predictions": ["fetch the path path path"], "references": ["validate a string as a valid storage path"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4145, "code": "def new ( cls , access token , environment = 'prod' ) : return cls ( storage client = Storage Client . new ( access token , environment = environment ) )", "predictions": ["create a new access listdir listdir listdir listdir listdir listdir listdir listdir listdir listdir listdir listdir listdir listdir listdir listdir listdir listdir listdir listdir listdir listdir listdir listdir listdir listdir children"], "references": ["creates a new cross - service client ."], "bleu": 0.0513487742994337, "rouge_l": 0.1147695202257761}
{"id": 4146, "code": "def emit ( self , record ) : msg = self . format ( record ) if not isinstance ( msg , dict ) : msg = json . loads ( msg ) self . collection . insert ( msg )", "predictions": ["fetch the record record"], "references": ["pymongo expects a dict"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 4147, "code": "def sort ( self , f = lambda d : d [ \"t\" ] ) : list . sort ( self , key = f ) return self", "predictions": ["row - aware row - first"], "references": ["sort here works by sorting by timestamp by default"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 4148, "code": "def sum ( self ) : raw = self . raw ( ) s = 0 for i in range ( len ( raw ) ) : s += raw [ i ] [ \"d\" ] return s", "predictions": ["index of the distribution"], "references": ["gets the sum of the data portions of all datapoints within"], "bleu": 0.08217262444082543, "rouge_l": 0.24596774193548387}
{"id": 4149, "code": "def rfxcom ( device ) : if device is None : device = app . config . get ( 'DEVICE' ) if device is None : print ( \"The serial device needs to be passed in as --device or \" \"set in the config as DEVICE.\" ) return rfxcom collect ( device )", "predictions": ["count the parts of the parts of the parts device root root device root root device"], "references": ["start the event loop to collect data from the serial device ."], "bleu": 0.09147827112247602, "rouge_l": 0.21995192307692307}
{"id": 4150, "code": "def create user ( username ) : password = prompt pass ( \"Enter password\" ) user = User ( username = username , password = password ) db . session . add ( user ) db . session . commit ( )", "predictions": ["parent to parent user"], "references": ["create a new user ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 4151, "code": "def refresh ( self ) : self . metadata = self . db . read ( self . path ) . json ( )", "predictions": ["data from the database role role role role role role role role role role role role role"], "references": ["refresh reloads data from the server . it raises an error if it fails to get the object s metadata"], "bleu": 0.11270143366181852, "rouge_l": 0.159825327510917}
{"id": 4152, "code": "def streams ( self ) : result = self . db . read ( self . path , { \"q\" : \"ls\" } ) if result is None or result . json ( ) is None : return [ ] streams = [ ] for s in result . json ( ) : strm = self [ s [ \"name\" ] ] strm . metadata = s streams . append ( strm ) return streams", "predictions": ["return list of all header header header role header role role role role role role role"], "references": ["returns the list of streams that belong to the device"], "bleu": 0.10123734869668824, "rouge_l": 0.16052631578947368}
{"id": 4153, "code": "def users ( self ) : result = self . db . read ( \"\" , { \"q\" : \"ls\" } ) if result is None or result . json ( ) is None : return [ ] users = [ ] for u in result . json ( ) : usr = self ( u [ \"name\" ] ) usr . metadata = u users . append ( usr ) return users", "predictions": ["return list of can be used in user home directory"], "references": ["returns the list of users in the database"], "bleu": 0.17827531042796255, "rouge_l": 0.34014869888475835}
{"id": 4154, "code": "def connectordb ( self ) : if self . cdb is None : logging . debug ( \"Logger: Connecting to \" + self . serverurl ) self . cdb = Connector DB ( self . apikey , url = self . serverurl ) return self . cdb", "predictions": ["returns the len representation of the site . . . ."], "references": ["returns the connectordb object that the logger uses . raises an error if logger isn t able to connect"], "bleu": 0.0823086270595964, "rouge_l": 0.2544316996871741}
{"id": 4155, "code": "def sync ( self ) : logging . debug ( \"Logger: Syncing...\" ) failed = False try : cdb = self . connectordb cdb . ping ( ) with self . synclock : c = self . database . cursor ( ) for stream in self . streams : s = cdb [ stream ] c . execute ( \"SELECT * FROM cache WHERE stream=? ORDER BY timestamp ASC;\" , ( stream , ) ) datapoint Array = [ ] for dp in c . fetchall ( ) : datapoint Array . append ( { \"t\" : dp [ 1 ] , \"d\" : json . loads ( dp [ 2 ] ) } ) if len ( s ) > 0 : newtime = s [ - 1 ] [ \"t\" ] while ( len ( datapoint Array ) > 0 and datapoint Array [ 0 ] [ \"t\" ] < newtime ) : logging . debug ( \"Datapoint exists with older timestamp. Removing the datapoint.\" ) datapoint Array = datapoint Array [ 1 : ] if len ( datapoint Array ) > 0 : logging . debug ( \"%s: syncing %i datapoints\" % ( stream , len ( datapoint Array ) ) ) while ( len ( datapoint Array ) > DATAPOINT INSERT LIMIT ) : s . insert array ( datapoint Array [ : DATAPOINT INSERT LIMIT ] ) datapoint Array = datapoint Array [ DATAPOINT INSERT LIMIT : ] c . execute ( \"DELETE FROM cache WHERE stream=? AND timestamp <?\" , ( stream , datapoint Array [ 0 ] [ \"t\" ] ) ) s . insert array ( datapoint Array ) c . execute ( \"DELETE FROM cache WHERE stream=? AND timestamp <=?\" , ( stream , datapoint Array [ - 1 ] [ \"t\" ] ) ) self . lastsynctime = time . time ( ) if self . onsync is not None : self . onsync ( ) except Exception as e : falied = True reraise = self . syncraise if self . onsyncfail is not None : reraise = self . onsyncfail ( e ) if reraise : raise", "predictions": ["syncs the cache from the return data to the database right ."], "references": ["attempt to sync with the connectordb server"], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 4156, "code": "def stop ( self ) : with self . synclock : if self . syncthread is not None : self . syncthread . cancel ( ) self . syncthread = None", "predictions": ["path to the timer ."], "references": ["stops the background synchronization thread"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 4157, "code": "def read ( * paths ) : filename = os . path . join ( * paths ) with codecs . open ( filename , mode = 'r' , encoding = 'utf-8' ) as handle : return handle . read ( )", "predictions": ["item from file = path = path = value = 1 = 1 = 1 = 0 = 1 = 1 = 0 = 1 = 1 = 1 = 1"], "references": ["build a file path from * paths * and return the contents ."], "bleu": 0.046398855339878003, "rouge_l": 0.09814963797264682}
{"id": 4158, "code": "def download url job ( job , url , name = None , s3 key path = None , cghub key path = None ) : work dir = job . file Store . get Local Temp Dir ( ) fpath = download url ( job = job , url = url , work dir = work dir , name = name , s3 key path = s3 key path , cghub key path = cghub key path ) return job . file Store . write Global File ( fpath )", "predictions": ["icon a self . to the correct s3 directory"], "references": ["job version of download_url"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4159, "code": "def s3am upload job ( job , file id , file name , s3 dir , s3 key path = None ) : work dir = job . file Store . get Local Temp Dir ( ) fpath = job . file Store . read Global File ( file id , os . path . join ( work dir , file name ) ) s3am upload ( job = job , fpath = fpath , s3 dir = s3 dir , num cores = job . cores , s3 key path = s3 key path )", "predictions": ["upload a job to a specific source job"], "references": ["job version of s3am_upload"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 4160, "code": "def labels ( ontology , output , ols base ) : for label in get labels ( ontology = ontology , ols base = ols base ) : click . echo ( label , file = output )", "predictions": ["print labels of an ontology"], "references": ["output the names to the given file"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 4161, "code": "def tree ( ontology , output , ols base ) : for parent , child in get hierarchy ( ontology = ontology , ols base = ols base ) : click . echo ( '{}\\t{}' . format ( parent , child ) , file = output )", "predictions": ["print an ontology tree"], "references": ["output the parent - child relations to the given file"], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 4162, "code": "def get mean insert size ( work dir , bam name ) : cmd = \"docker run --log-driver=none --rm -v {}:/data quay.io/ucsc cgl/samtools \" \"view -f66 {}\" . format ( work dir , os . path . join ( work dir , bam name ) ) process = subprocess . Popen ( args = cmd , shell = True , stdout = subprocess . PIPE ) b sum = 0.0 b count = 0.0 while True : line = process . stdout . readline ( ) if not line : break tmp = line . split ( \"\\t\" ) if abs ( long ( tmp [ 8 ] ) ) < 10000 : b sum += abs ( long ( tmp [ 8 ] ) ) b count += 1 process . wait ( ) try : mean = b sum / b count except Zero Division Error : mean = 150 print \"Using insert size: %d\" % mean return int ( mean )", "predictions": ["get the size of a bam bam file"], "references": ["function taken from mc3 pipeline"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4163, "code": "def device ( self ) : splitted path = self . path . split ( \"/\" ) return Device ( self . db , splitted path [ 0 ] + \"/\" + splitted path [ 1 ] )", "predictions": ["device device name ."], "references": ["returns the device which owns the given stream"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 4164, "code": "def get empty config ( self ) : self . generate config ( ) path = self . get config path ( ) with open ( path , 'r' ) as readable : contents = readable . read ( ) os . remove ( path ) return contents", "predictions": ["reads the empty config file ."], "references": ["returns the config file contents as a string . the config file is generated and then deleted ."], "bleu": 0.04470346785554712, "rouge_l": 0.30576441102756885}
{"id": 4165, "code": "def create pipeline command ( self , args , workdir path , config path ) : return ( [ self . name , 'run' , os . path . join ( workdir path , 'job Store' ) , '--config' , config path , '--work Dir' , workdir path , '--retry Count' , '1' ] + ( [ '--restart' ] if args . restart else [ ] ) )", "predictions": ["create a pipeline command"], "references": ["creates and returns a list that represents a command for running the pipeline ."], "bleu": 0.03507385333204471, "rouge_l": 0.20198675496688742}
{"id": 4166, "code": "def delete ( self , path ) : return self . handleresult ( self . r . delete ( urljoin ( self . url + CRUD PATH , path ) ) )", "predictions": ["delete a file from the remote transifex server ."], "references": ["send a delete request to the given path of the crud api . this deletes the object . or at least tries to ."], "bleu": 0.03352051871072022, "rouge_l": 0.1680440771349862}
{"id": 4167, "code": "def subscribe ( self , stream , callback , transform = \"\" ) : return self . ws . subscribe ( stream , callback , transform )", "predictions": ["subscribe to a stream ."], "references": ["subscribe to the given stream with the callback"], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 4168, "code": "def devices ( self ) : result = self . db . read ( self . path , { \"q\" : \"ls\" } ) if result is None or result . json ( ) is None : return [ ] devices = [ ] for d in result . json ( ) : dev = self [ d [ \"name\" ] ] dev . metadata = d devices . append ( dev ) return devices", "predictions": ["return a list of devices ."], "references": ["returns the list of devices that belong to the user"], "bleu": 0.21108303712651422, "rouge_l": 0.3588235294117647}
{"id": 4169, "code": "def send ( self , cmd ) : with self . ws sendlock : self . ws . send ( json . dumps ( cmd ) )", "predictions": ["send a cmd command to the server ."], "references": ["send the given command thru the websocket"], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 4170, "code": "def subscribe ( self , stream , callback , transform = \"\" ) : if self . status == \"disconnected\" or self . status == \"disconnecting\" or self . status == \"connecting\" : self . connect ( ) if self . status is not \"connected\" : return False logging . debug ( \"Subscribing to %s\" , stream ) self . send ( { \"cmd\" : \"subscribe\" , \"arg\" : stream , \"transform\" : transform } ) with self . subscription lock : self . subscriptions [ stream + \":\" + transform ] = callback return True", "predictions": ["subscribe to a stream ."], "references": ["given a stream a callback and an optional transform sets up the subscription"], "bleu": 0.07254224910650854, "rouge_l": 0.20573355817875214}
{"id": 4171, "code": "def reconnect ( self ) : self . status = \"reconnecting\" if self . disconnected time - self . connected time > 15 * 60 : self . reconnect time = self . reconnect time starting seconds else : self . reconnect time *= self . reconnect time backoff multiplier if self . reconnect time > self . reconnect time max seconds : self . reconnect time = self . reconnect time max seconds self . reconnect time *= 1 + random . uniform ( - 0.2 , 0.2 ) if self . reconnect time < self . reconnect time starting seconds : self . reconnect time = self . reconnect time starting seconds logging . warn ( \"Connector DB:WS: Attempting to reconnect in %fs\" , self . reconnect time ) self . reconnector = threading . Timer ( self . reconnect time , self . reconnect fnc ) self . reconnector . daemon = True self . reconnector . start ( )", "predictions": ["reconnect to rabbitmq server"], "references": ["this is called when a connection is lost - it attempts to reconnect to the server"], "bleu": 0.025298439872324396, "rouge_l": 0.27071005917159763}
{"id": 4172, "code": "def on open ( self , ws ) : logging . debug ( \"Connector DB: Websocket opened\" ) self . reconnect time /= self . reconnect time backoff multiplier self . status = \"connected\" self . lastpingtime = time . time ( ) self . ensure ping ( ) self . connected time = time . time ( ) self . ws openlock . release ( )", "predictions": ["opens the ping connection with the device ."], "references": ["called when the websocket is opened"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 4173, "code": "def on close ( self , ws ) : if self . status == \"disconnected\" : return logging . debug ( \"Connector DB:WS: Websocket closed\" ) if self . pingtimer is not None : self . pingtimer . cancel ( ) self . disconnected time = time . time ( ) if self . status == \"disconnecting\" : self . status = \"disconnected\" elif self . status == \"connected\" : self . reconnect ( )", "predictions": ["called when the connection is closed ."], "references": ["called when the websocket is closed"], "bleu": 0.4111336169005197, "rouge_l": 0.7800511508951408}
{"id": 4174, "code": "def on error ( self , ws , err ) : logging . debug ( \"Connector DB:WS: Connection Error\" ) if self . status == \"connecting\" : self . status = \"errored\" self . ws openlock . release ( )", "predictions": ["called when the connection is closed ."], "references": ["called when there is an error in the websocket"], "bleu": 0.20873176328735715, "rouge_l": 0.3667334669338677}
{"id": 4175, "code": "def on message ( self , ws , msg ) : msg = json . loads ( msg ) logging . debug ( \"Connector DB:WS: Msg '%s'\" , msg [ \"stream\" ] ) stream key = msg [ \"stream\" ] + \":\" if \"transform\" in msg : stream key += msg [ \"transform\" ] self . subscription lock . acquire ( ) if stream key in self . subscriptions : subscription function = self . subscriptions [ stream key ] self . subscription lock . release ( ) fresult = subscription function ( msg [ \"stream\" ] , msg [ \"data\" ] ) if fresult is True : fresult = msg [ \"data\" ] if fresult is not False and fresult is not None and msg [ \"stream\" ] . endswith ( \"/downlink\" ) and msg [ \"stream\" ] . count ( \"/\" ) == 3 : self . insert ( msg [ \"stream\" ] [ : - 9 ] , fresult ) else : self . subscription lock . release ( ) logging . warn ( \"Connector DB:WS: Msg '%s' not subscribed! Subscriptions: %s\" , msg [ \"stream\" ] , list ( self . subscriptions . keys ( ) ) )", "predictions": ["handle an incoming subscription message ."], "references": ["this function is called whenever there is a message received from the server"], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 4176, "code": "def write config ( configuration ) : with open ( CONFIG PATH , 'w' ) as f : json . dump ( configuration , f , indent = 2 , sort keys = True )", "predictions": ["write the current configuration to a file"], "references": ["helper to write the json configuration to a file"], "bleu": 0.44683107184405757, "rouge_l": 0.7334669338677354}
{"id": 4177, "code": "def check ( self ) : status = check Container Status ( self . spark Container ID , self . hdfs Container ID , spark Noun = 'worker' , hdfs Noun = 'datanode' ) return status", "predictions": ["check the spark status of this spark"], "references": ["checks to see if spark worker and hdfs datanode are still running ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 4178, "code": "def base tokenizer ( fp ) : if isinstance ( fp , String IO ) : template file = fp size = template file . len else : #empty file check if os . fstat ( fp . fileno ( ) ) . st size == 0 : yield TOKEN EOF , 'EOF' , 0 , 0 return template file = mmap . mmap ( fp . fileno ( ) , 0 , access = mmap . ACCESS READ ) size = template file . size ( ) lineno = 0 while 1 : lineno += 1 pos = 1 if template file . tell ( ) == size : yield TOKEN EOF , 'EOF' , lineno , 0 break line = template file . readline ( ) . decode ( 'utf-8' ) line = line . replace ( '\\r\\n' , '' ) line = line . replace ( '\\n' , '' ) if re comment . match ( line ) : continue last text = deque ( ) while line : line len = len ( line ) for token in tokens : m = token . regex . match ( line ) if m : if last text : yield TOKEN TEXT , '' . join ( last text ) , lineno , pos pos += len ( last text ) last text . clear ( ) offset , value = m . end ( ) , m . group ( ) line = line [ offset : ] yield token , value , lineno , pos pos += offset break if line len == len ( line ) : last text . append ( line [ 0 ] ) line = line [ 1 : ] if last text : yield TOKEN TEXT , '' . join ( last text ) , lineno , pos pos += len ( last text ) last text . clear ( ) yield TOKEN NEWLINE , '\\n' , lineno , pos template file . close ( )", "predictions": ["yield lines of a file ."], "references": ["tokenizer . generates tokens stream from text"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4179, "code": "def fitness ( self ) : if len ( self . members ) != 0 : if self . num processes > 1 : members = [ m . get ( ) for m in self . members ] else : members = self . members return sum ( m . fitness score for m in members ) / len ( members ) else : return None", "predictions": ["the fitness of the processes ."], "references": ["population fitness == average member fitness score"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4180, "code": "def ave cost fn val ( self ) : if len ( self . members ) != 0 : if self . num processes > 1 : members = [ m . get ( ) for m in self . members ] else : members = self . members return sum ( m . cost fn val for m in members ) / len ( members ) else : return None", "predictions": ["return the ave cost of the ave ave ."], "references": ["returns average cost function return value for all members"], "bleu": 0.15619699684601276, "rouge_l": 0.1111111111111111}
{"id": 4181, "code": "def med cost fn val ( self ) : if len ( self . members ) != 0 : if self . num processes > 1 : members = [ m . get ( ) for m in self . members ] else : members = self . members return median ( [ m . cost fn val for m in members ] ) else : return None", "predictions": ["return the cost of the med cost ."], "references": ["returns median cost function return value for all members"], "bleu": 0.15662030188557857, "rouge_l": 0.116412213740458}
{"id": 4182, "code": "def parameters ( self ) : if len ( self . members ) != 0 : if self . num processes > 1 : members = [ m . get ( ) for m in self . members ] else : members = self . members params = { } for p in self . parameters : params [ p . name ] = sum ( m . parameters [ p . name ] for m in members ) / len ( members ) return params else : return None", "predictions": ["return the list of parameters in this model ."], "references": ["population parameter vals == average member parameter vals"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4183, "code": "def members ( self ) : if self . num processes > 1 : return [ m . get ( ) for m in self . members ] else : return self . members", "predictions": ["return list of members of this processes ."], "references": ["returns member objects of population"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 4184, "code": "def get environ vars ( self ) : for key , val in os . environ . items ( ) : if environ prefix re . search ( key ) : yield ( environ prefix re . sub ( \"\" , key ) . lower ( ) , val )", "predictions": ["return the environment variables that are not defined in the environment ."], "references": ["returns a generator with all environmental vars with prefix pip_"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 4185, "code": "def transform result ( typ , result ) : if issubclass ( typ , bytes ) : return tostring ( result , encoding = 'utf-8' ) elif issubclass ( typ , unicode ) : return tostring ( result , encoding = 'unicode' ) else : return result", "predictions": ["transform typ into unicode string ."], "references": ["convert the result back into the input type ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 4186, "code": "def is single class ( ) : ret = False counts = get counts ( ) if counts [ \"classes\" ] < 1 and counts [ \"modules\" ] < 1 : ret = counts [ \"tests\" ] > 0 else : ret = counts [ \"classes\" ] <= 1 and counts [ \"modules\" ] <= 1 return ret", "predictions": ["return true if the class is a single class"], "references": ["returns true if only a single class is being run or some tests within a single class"], "bleu": 0.14592352870481634, "rouge_l": 0.5101553166069295}
{"id": 4187, "code": "def is single module ( ) : ret = False counts = get counts ( ) if counts [ \"modules\" ] == 1 : ret = True elif counts [ \"modules\" ] < 1 : ret = is single class ( ) return ret", "predictions": ["return whether the module is a single module"], "references": ["returns true if only a module is being run"], "bleu": 0.20014292374951972, "rouge_l": 0.232824427480916}
{"id": 4188, "code": "def validate params ( request ) : if 'params' in request : correct params = isinstance ( request [ 'params' ] , ( list , dict ) ) error = 'Incorrect parameter values' assert correct params , error", "predictions": ["validate that the request params is correct ."], "references": ["validate request params ."], "bleu": 0.239802967618271, "rouge_l": 0.7093023255813954}
{"id": 4189, "code": "def validate id ( request ) : if 'id' in request : correct id = isinstance ( request [ 'id' ] , ( string types , int , None ) , ) error = 'Incorrect identifier' assert correct id , error", "predictions": ["validate that the request id is valid ."], "references": ["validate request id ."], "bleu": 0.239802967618271, "rouge_l": 0.7093023255813954}
{"id": 4190, "code": "def escape argspec ( obj , iterable , escape ) : for key , value in iterable : if hasattr ( value , ' html ' ) or isinstance ( value , string types ) : obj [ key ] = escape ( value ) return obj", "predictions": ["escapes all html - safe characters ."], "references": ["helper for various string - wrapped functions ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 4191, "code": "def sub symbols ( pattern , code , symbol ) : return pattern . replace ( '\u00a4\u00a4',  c de). r e place(' \u00a4 ', s y bol)", "predictions": ["return the sub - character symbols in a pattern"], "references": ["substitutes symbols in cldr number pattern ."], "bleu": 0.19960198807747329, "rouge_l": 0.38364779874213834}
{"id": 4192, "code": "def amount converter ( obj ) : if isinstance ( obj , Decimal ) : return obj elif isinstance ( obj , ( str , int , float ) ) : return Decimal ( str ( obj ) ) else : raise Value Error ( 'do not know how to convert: {}' . format ( type ( obj ) ) )", "predictions": ["returns the labels of a labels"], "references": ["converts amount value from several types into decimal ."], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 4193, "code": "def exception ( self ) : buf = traceback . format exception only ( self . exc type , self . exc value ) rv = '' . join ( buf ) . strip ( ) return rv . decode ( 'utf-8' , 'replace' ) if PY2 else rv", "predictions": ["return tree of the tree"], "references": ["string representation of the exception ."], "bleu": 0.2941733261715515, "rouge_l": 0.3577712609970674}
{"id": 4194, "code": "def render summary ( self , include title = True ) : title = '' frames = [ ] classes = [ 'traceback' ] if not self . frames : classes . append ( 'noframe-traceback' ) if include title : if self . is syntax error : title = u'Syntax Error' else : title = u'Traceback <em>(most recent call last)</em>:' for frame in self . frames : frames . append ( u'<li%s>%s' % ( frame . info and u' title=\"%s\"' % escape ( frame . info ) or u'' , frame . render ( ) ) ) if self . is syntax error : description wrapper = u'<pre class=syntaxerror>%s</pre>' else : description wrapper = u'<blockquote>%s</blockquote>' return SUMMARY HTML % { 'classes' : u' ' . join ( classes ) , 'title' : title and u'<h3>%s</h3>' % title or u'' , 'frames' : u'\\n' . join ( frames ) , 'description' : description wrapper % escape ( self . exception ) }", "predictions": ["get the mean mean of the interactive console ."], "references": ["render the traceback for the interactive console ."], "bleu": 0.4111336169005197, "rouge_l": 0.594541910331384}
{"id": 4195, "code": "def generate plaintext traceback ( self ) : yield u'Traceback (most recent call last):' for frame in self . frames : yield u'  File \"%s\", line %s, in %s' % ( frame . filename , frame . lineno , frame . function name ) yield u'    ' + frame . current line . strip ( ) yield self . exception", "predictions": ["generates the plaintext self 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"], "references": ["like the plaintext attribute but returns a generator"], "bleu": 0.053091875602826286, "rouge_l": 0.11753371868978806}
{"id": 4196, "code": "def get annotated lines ( self ) : lines = [ Line ( idx + 1 , x ) for idx , x in enumerate ( self . sourcelines ) ] if hasattr ( self . code , 'co firstlineno' ) : lineno = self . code . co firstlineno - 1 while lineno > 0 : if funcdef re . match ( lines [ lineno ] . code ) : break lineno -= 1 try : offset = len ( inspect . getblock ( [ x . code + '\\n' for x in lines [ lineno : ] ] ) ) except Token Error : offset = 0 for line in lines [ lineno : lineno + offset ] : line . in frame = True try : lines [ self . lineno - 1 ] . current = True except Index Error : pass return lines", "predictions": ["return config config from the empty string"], "references": ["helper function that returns lines with extra information ."], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 4197, "code": "def render source ( self ) : return SOURCE TABLE HTML % u'\\n' . join ( line . render ( ) for line in self . get annotated lines ( ) )", "predictions": ["create a pipeline return the pipeline return the pipeline return it ."], "references": ["render the sourcecode ."], "bleu": 0.11498759556447223, "rouge_l": 0.27477477477477474}
{"id": 4198, "code": "def get content type ( url , session ) : scheme , netloc , path , query , fragment = urllib parse . urlsplit ( url ) if scheme not in ( 'http' , 'https' ) : return '' resp = session . head ( url , allow redirects = True ) resp . raise for status ( ) return resp . headers . get ( \"Content-Type\" , \"\" )", "predictions": ["parses the content self . . ."], "references": ["get the content - type of the given url using a head request"], "bleu": 0.10374282717383708, "rouge_l": 0.1897356143079316}
{"id": 4199, "code": "def links ( self ) : for anchor in self . parsed . findall ( \".//a\" ) : if anchor . get ( \"href\" ) : href = anchor . get ( \"href\" ) url = self . clean link ( urllib parse . urljoin ( self . base url , href ) ) internal = None if self . api version and self . api version >= 2 : internal = bool ( anchor . get ( \"rel\" ) and \"internal\" in anchor . get ( \"rel\" ) . split ( ) ) yield Link ( url , self , internal = internal )", "predictions": ["return all subscribe subscribe to the page . ."], "references": ["yields all links in the page"], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 4200, "code": "def find data files ( self , package , src dir ) : globs = ( self . package data . get ( '' , [ ] ) + self . package data . get ( package , [ ] ) ) files = self . manifest files . get ( package , [ ] ) [ : ] for pattern in globs : files . extend ( glob ( os . path . join ( src dir , convert path ( pattern ) ) ) ) return self . exclude data files ( package , src dir , files )", "predictions": ["devices for a given = path to a directory . ."], "references": ["return filenames for package s data files in src_dir"], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 4201, "code": "def check package ( self , package , package dir ) : try : return self . packages checked [ package ] except Key Error : pass init py = orig . build py . check package ( self , package , package dir ) self . packages checked [ package ] = init py if not init py or not self . distribution . namespace packages : return init py for pkg in self . distribution . namespace packages : if pkg == package or pkg . startswith ( package + '.' ) : break else : return init py f = open ( init py , 'rb U' ) if 'declare namespace' . encode ( ) not in f . read ( ) : from distutils . errors import Distutils Error raise Distutils Error ( \"Namespace package problem: %s is a namespace package, but \" \"its\\n init .py does not call declare namespace()! Please \" 'fix it.\\n(See the setuptools manual under ' '\"Namespace Packages\" for details.)\\n\"' % ( package , ) ) f . close ( ) return init py", "predictions": ["send the namespace command to the package . ."], "references": ["check namespace packages __init__ for declare_namespace"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 4202, "code": "def exclude data files ( self , package , src dir , files ) : globs = ( self . exclude package data . get ( '' , [ ] ) + self . exclude package data . get ( package , [ ] ) ) bad = [ ] for pattern in globs : bad . extend ( fnmatch . filter ( files , os . path . join ( src dir , convert path ( pattern ) ) ) ) bad = dict . fromkeys ( bad ) seen = { } return [ f for f in files if f not in bad and f not in seen and seen . setdefault ( f , 1 ) ]", "predictions": ["subscribe to all self self status status status status status status status status status status status status status status status status status status status status status status status status status self"], "references": ["filter filenames for package s data files in src_dir"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4203, "code": "def ignore comments ( iterator ) : for line in iterator : line = COMMENT RE . sub ( '' , line ) line = line . strip ( ) if line : yield line", "predictions": ["strips comments comments from a string - like object - safe way - separated string - encoded iterator - encoded string - encoded iterator - produce comments - produce produce comments"], "references": ["strips and filters empty or commented lines ."], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 4204, "code": "def skip regex ( lines , options ) : skip regex = options . skip requirements regex if options else None if skip regex : lines = filterfalse ( re . compile ( skip regex ) . search , lines ) return lines", "predictions": ["on the open open open self . ws ."], "references": ["optionally exclude lines that match -- skip - requirements - regex"], "bleu": 0.09503475972243516, "rouge_l": 0.0}
{"id": 4205, "code": "def compile ( marker ) : try : return cache [ marker ] except Key Error : pass if not marker . strip ( ) : def marker fn ( environment = None , override = None ) : \"\"\"\"\"\" return True else : compiled marker = compile marker ( parse marker ( marker ) ) def marker fn ( environment = None , override = None ) : \"\"\"override updates environment\"\"\" if override is None : override = { } if environment is None : environment = default environment ( ) environment . update ( override ) return eval ( compiled marker , environment ) marker fn . doc = marker cache [ marker ] = marker fn return cache [ marker ]", "predictions": ["on a callable return returns a cache object"], "references": ["return compiled marker as a function accepting an environment dict ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 4206, "code": "def visit ( self , node ) : if not isinstance ( node , self . ALLOWED ) : raise Syntax Error ( 'Not allowed in environment markers.\\n%s\\n%s' % ( self . statement , ( ' ' * node . col offset ) + '^' ) ) return ast . Node Transformer . visit ( self , node )", "predictions": ["on python code on the ast and assemble it to the ast ."], "references": ["ensure statement only contains allowed nodes ."], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 4207, "code": "def visit Attribute ( self , node ) : new node = ast . Name ( \"%s.%s\" % ( node . value . id , node . attr ) , node . ctx ) return ast . copy location ( new node , node )", "predictions": ["return an astroid . json ws as string"], "references": ["flatten one level of attribute access ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4208, "code": "def push ( self ) : self . refcnt += 1 app ctx stack . push ( self ) appcontext pushed . send ( self . app )", "predictions": ["write current stack to stack"], "references": ["binds the app context to the current context ."], "bleu": 0.13575914775035755, "rouge_l": 0.1358574610244989}
{"id": 4209, "code": "def pop ( self , exc = None ) : self . refcnt -= 1 if self . refcnt <= 0 : if exc is None : exc = sys . exc info ( ) [ 1 ] self . app . do teardown appcontext ( exc ) rv = app ctx stack . pop ( ) assert rv is self , 'Popped wrong app context.  (%r instead of %r)' % ( rv , self ) appcontext popped . send ( self . app )", "predictions": ["remove an instance of this app ."], "references": ["pops the app context ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 4210, "code": "def push ( self ) : top = request ctx stack . top if top is not None and top . preserved : top . pop ( top . preserved exc ) app ctx = app ctx stack . top if app ctx is None or app ctx . app != self . app : app ctx = self . app . app context ( ) app ctx . push ( ) self . implicit app ctx stack . append ( app ctx ) else : self . implicit app ctx stack . append ( None ) request ctx stack . push ( self ) self . session = self . app . open session ( self . request ) if self . session is None : self . session = self . app . make null session ( )", "predictions": ["base class to st ."], "references": ["binds the request context to the current context ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 4211, "code": "def dist in usersite ( dist ) : norm path = normalize path ( dist location ( dist ) ) return norm path . startswith ( normalize path ( user site ) )", "predictions": ["returns the full path to the given self processes processes"], "references": ["return true if given distribution is installed in user site ."], "bleu": 0.11406351620367239, "rouge_l": 0.09442724458204334}
{"id": 4212, "code": "def dist is editable ( dist ) : from pip import Frozen Requirement req = Frozen Requirement . from dist ( dist , [ ] ) return req . editable", "predictions": ["1 if a ave record is an fn fn 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"], "references": ["is distribution an editable install?"], "bleu": 0.04317900023606586, "rouge_l": 0.12774869109947642}
{"id": 4213, "code": "def run ( self , options , args ) : shells = COMPLETION SCRIPTS . keys ( ) shell options = [ '--' + shell for shell in sorted ( shells ) ] if options . shell in shells : script = COMPLETION SCRIPTS . get ( options . shell , '' ) print ( BASE COMPLETION % { 'script' : script , 'shell' : options . shell } ) else : sys . stderr . write ( 'ERROR: You must pass %s\\n' % ' or ' . join ( shell options ) )", "predictions": ["run the num ."], "references": ["prints the completion code of the given shell"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 4214, "code": "def root is purelib ( name , wheeldir ) : name folded = name . replace ( \"-\" , \" \" ) for item in os . listdir ( wheeldir ) : match = dist info re . match ( item ) if match and match . group ( 'name' ) == name folded : with open ( os . path . join ( wheeldir , item , 'WHEEL' ) ) as wheel : for line in wheel : line = line . lower ( ) . rstrip ( ) if line == \"root-is-purelib: true\" : return True return False", "predictions": ["return is is . ."], "references": ["return true if the extracted wheel in wheeldir should go into purelib ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 4215, "code": "def iter symbols ( code ) : for name in code . co names : yield name for const in code . co consts : if isinstance ( const , basestring ) : yield const elif isinstance ( const , Code Type ) : for name in iter symbols ( const ) : yield name", "predictions": ["return all symbols of code"], "references": ["yield names and strings used by code and its nested code objects"], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 4216, "code": "def ensure fresh rates ( func ) : def wrapper ( self , * args , * * kwargs ) : if self . last updated + timedelta ( minutes = 5 ) < zulu . now ( ) : self . refresh ( ) return func ( self , * args , * * kwargs ) return wrapper", "predictions": ["get a decorator that wraps the if it s not already environ items items items items items items items items items items"], "references": ["decorator for backend that ensures rates are fresh within last 5 mins"], "bleu": 0.0612957497932821, "rouge_l": 0.12423625254582485}
{"id": 4217, "code": "def write delete marker file ( directory ) : filepath = os . path . join ( directory , PIP DELETE MARKER FILENAME ) with open ( filepath , 'w' ) as marker fp : marker fp . write ( DELETE MARKER MESSAGE )", "predictions": ["transform the marker marker to the given directory"], "references": ["write the pip delete marker file into this directory ."], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 4218, "code": "def running under virtualenv ( ) : if hasattr ( sys , 'real prefix' ) : return True elif sys . prefix != getattr ( sys , \"base prefix\" , sys . prefix ) : return True return False", "predictions": ["returns true if the class is is is is is is is is is is is is is is is is is is is is is is is is is is"], "references": ["return true if we re running inside a virtualenv false otherwise ."], "bleu": 0.0513487742994337, "rouge_l": 0.10107705053852528}
{"id": 4219, "code": "def get username ( ) : if WINDOWS : return getpass . getuser ( ) import pwd return pwd . getpwuid ( os . geteuid ( ) ) . pw name", "predictions": ["= { % } counts }"], "references": ["returns the effective username of the current process ."], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 4220, "code": "def distutils scheme ( dist name , user = False , home = None , root = None , isolated = False ) : from distutils . dist import Distribution scheme = { } if isolated : extra dist args = { \"script args\" : [ \"--no-user-cfg\" ] } else : extra dist args = { } dist args = { 'name' : dist name } dist args . update ( extra dist args ) d = Distribution ( dist args ) d . parse config files ( ) i = d . get command obj ( 'install' , create = True ) i . user = user or i . user i . home = home or i . home i . root = root or i . root i . finalize options ( ) for key in SCHEME KEYS : scheme [ key ] = getattr ( i , 'install ' + key ) if i . install lib is not None : scheme . update ( dict ( purelib = i . install lib , platlib = i . install lib ) ) if running under virtualenv ( ) : scheme [ 'headers' ] = os . path . join ( sys . prefix , 'include' , 'site' , 'python' + sys . version [ : 3 ] , dist name , ) if root is not None : scheme [ \"headers\" ] = os . path . join ( root , os . path . abspath ( scheme [ \"headers\" ] ) [ 1 : ] , ) return scheme", "predictions": ["return params for validate the validate builder dict dict dict dict dict dict dict dict dict"], "references": ["return a distutils install scheme"], "bleu": 0.07692375026049747, "rouge_l": 0.10517241379310344}
{"id": 4221, "code": "def install script ( self , dist , script name , script text , dev path = None ) : spec = str ( dist . as requirement ( ) ) is script = is python script ( script text , script name ) if is script : script text = ( Script Writer . get header ( script text ) + self . load template ( dev path ) % locals ( ) ) self . write script ( script name , to ascii ( script text ) , 'b' )", "predictions": ["validate a id of the given id"], "references": ["generate a legacy script wrapper and install it"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4222, "code": "def install site py ( self ) : if self . sitepy installed : return sitepy = os . path . join ( self . install dir , \"site.py\" ) source = resource string ( \"setuptools\" , \"site-patch.py\" ) current = \"\" if os . path . exists ( sitepy ) : log . debug ( \"Checking existing site.py in %s\" , self . install dir ) f = open ( sitepy , 'rb' ) current = f . read ( ) if PY3 : current = current . decode ( ) f . close ( ) if not current . startswith ( 'def  boot():' ) : raise Distutils Error ( \"%s is not a setuptools-generated site.py; please\" \" remove it.\" % sitepy ) if current != source : log . info ( \"Creating %s\" , sitepy ) if not self . dry run : ensure directory ( sitepy ) f = open ( sitepy , 'wb' ) f . write ( source ) f . close ( ) self . byte compile ( [ sitepy ] ) self . sitepy installed = True", "predictions": ["escape the argspec for the argspec"], "references": ["make sure there s a site . py in the target dir if needed"], "bleu": 0.05822753005110548, "rouge_l": 0.09327217125382263}
{"id": 4223, "code": "def save ( self ) : if not self . dirty : return data = '\\n' . join ( map ( self . make relative , self . paths ) ) if data : log . debug ( \"Saving %s\" , self . filename ) data = ( \"import sys; sys. plen = len(sys.path)\\n\" \"%s\\n\" \"import sys; new=sys.path[sys. plen:];\" \" del sys.path[sys. plen:];\" \" p=getattr(sys,' egginsert',0); sys.path[p:p]=new;\" \" sys. egginsert = p+len(new)\\n\" ) % data if os . path . islink ( self . filename ) : os . unlink ( self . filename ) f = open ( self . filename , 'wt' ) f . write ( data ) f . close ( ) elif os . path . exists ( self . filename ) : log . debug ( \"Deleting empty %s\" , self . filename ) os . unlink ( self . filename ) self . dirty = False", "predictions": ["sub - sub - sub - dirty"], "references": ["write changed . pth file back to disk"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 4224, "code": "def add filters ( self , filterer , filters ) : for f in filters : try : filterer . add Filter ( self . config [ 'filters' ] [ f ] ) except Standard Error as e : raise Value Error ( 'Unable to add filter %r: %s' % ( f , e ) )", "predictions": ["add filters to config ."], "references": ["add filters to a filterer from a list of names ."], "bleu": 0.1618271218800702, "rouge_l": 0.4683301343570058}
{"id": 4225, "code": "def configure handler ( self , config ) : formatter = config . pop ( 'formatter' , None ) if formatter : try : formatter = self . config [ 'formatters' ] [ formatter ] except Standard Error as e : raise Value Error ( 'Unable to set formatter ' '%r: %s' % ( formatter , e ) ) level = config . pop ( 'level' , None ) filters = config . pop ( 'filters' , None ) if '()' in config : c = config . pop ( '()' ) if not hasattr ( c , ' call ' ) and hasattr ( types , 'Class Type' ) and type ( c ) != types . Class Type : c = self . resolve ( c ) factory = c else : klass = self . resolve ( config . pop ( 'class' ) ) if issubclass ( klass , logging . handlers . Memory Handler ) and 'target' in config : try : config [ 'target' ] = self . config [ 'handlers' ] [ config [ 'target' ] ] except Standard Error as e : raise Value Error ( 'Unable to set target handler ' '%r: %s' % ( config [ 'target' ] , e ) ) elif issubclass ( klass , logging . handlers . SMTP Handler ) and 'mailhost' in config : config [ 'mailhost' ] = self . as tuple ( config [ 'mailhost' ] ) elif issubclass ( klass , logging . handlers . Sys Log Handler ) and 'address' in config : config [ 'address' ] = self . as tuple ( config [ 'address' ] ) factory = klass kwargs = dict ( ( k , config [ k ] ) for k in config if valid ident ( k ) ) try : result = factory ( * * kwargs ) except Type Error as te : if \"'stream'\" not in str ( te ) : raise #(e.g. by Django) kwargs [ 'strm' ] = kwargs . pop ( 'stream' ) result = factory ( * * kwargs ) if formatter : result . set Formatter ( formatter ) if level is not None : result . set Level ( check Level ( level ) ) if filters : self . add filters ( result , filters ) return result", "predictions": ["configure a handler from a given config ."], "references": ["configure a handler from a dictionary ."], "bleu": 0.6104735835807844, "rouge_l": 0.8097345132743362}
{"id": 4226, "code": "def add handlers ( self , logger , handlers ) : for h in handlers : try : logger . add Handler ( self . config [ 'handlers' ] [ h ] ) except Standard Error as e : raise Value Error ( 'Unable to add handler %r: %s' % ( h , e ) )", "predictions": ["add handlers to config ."], "references": ["add handlers to a logger from a list of names ."], "bleu": 0.1618271218800702, "rouge_l": 0.4683301343570058}
{"id": 4227, "code": "def common logger config ( self , logger , config , incremental = False ) : level = config . get ( 'level' , None ) if level is not None : logger . set Level ( check Level ( level ) ) if not incremental : for h in logger . handlers [ : ] : logger . remove Handler ( h ) handlers = config . get ( 'handlers' , None ) if handlers : self . add handlers ( logger , handlers ) filters = config . get ( 'filters' , None ) if filters : self . add filters ( logger , filters )", "predictions": ["common code for setting up a logger"], "references": ["perform configuration which is common to root and non - root loggers ."], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 4228, "code": "def execfile ( filename , globals , locals = None ) : mode = 'rb' with open ( filename , mode ) as stream : script = stream . read ( ) if sys . version info [ : 2 ] < ( 2 , 7 ) or sys . version info [ : 2 ] >= ( 3 , 0 ) and sys . version info [ : 2 ] < ( 3 , 2 ) : script = script . replace ( b'\\r\\n' , b'\\n' ) script = script . replace ( b'\\r' , b'\\n' ) if locals is None : locals = globals code = compile ( script , filename , 'exec' ) exec ( code , globals , locals )", "predictions": ["run execfile on the given script ."], "references": ["python 3 implementation of execfile ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 4229, "code": "def override temp ( replacement ) : if not os . path . isdir ( replacement ) : os . makedirs ( replacement ) saved = tempfile . tempdir tempfile . tempdir = replacement try : yield finally : tempfile . tempdir = saved", "predictions": ["context manager to override the temporary directory ."], "references": ["monkey - patch tempfile . tempdir with replacement ensuring it exists"], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 4230, "code": "def run setup ( setup script , args ) : setup dir = os . path . abspath ( os . path . dirname ( setup script ) ) with setup context ( setup dir ) : try : sys . argv [ : ] = [ setup script ] + list ( args ) sys . path . insert ( 0 , setup dir ) working set . init ( ) working set . callbacks . append ( lambda dist : dist . activate ( ) ) def runner ( ) : ns = dict ( file = setup script , name = ' main ' ) execfile ( setup script , ns ) Directory Sandbox ( setup dir ) . run ( runner ) except System Exit as v : if v . args and v . args [ 0 ] : raise", "predictions": ["run the setup . py file ."], "references": ["run a distutils setup script sandboxed in its directory"], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 4231, "code": "def getitem ( self , obj , argument ) : try : return obj [ argument ] except ( Type Error , Lookup Error ) : if isinstance ( argument , string types ) : try : attr = str ( argument ) except Exception : pass else : try : return getattr ( obj , attr ) except Attribute Error : pass return self . undefined ( obj = obj , name = argument )", "predictions": ["returns the object s object ."], "references": ["get an item or attribute of an object but prefer the item ."], "bleu": 0.08180282100568384, "rouge_l": 0.19741100323624597}
{"id": 4232, "code": "def find eggs in zip ( importer , path item , only = False ) : if importer . archive . endswith ( '.whl' ) : return metadata = Egg Metadata ( importer ) if metadata . has metadata ( 'PKG-INFO' ) : yield Distribution . from filename ( path item , metadata = metadata ) if only : return for subitem in metadata . resource listdir ( '/' ) : if subitem . endswith ( '.egg' ) : subpath = os . path . join ( path item , subitem ) for dist in find eggs in zip ( zipimport . zipimporter ( subpath ) , subpath ) : yield dist", "predictions": ["find eggs in zip files ."], "references": ["find eggs in zip files ; possibly multiple nested eggs ."], "bleu": 0.36545207560928844, "rouge_l": 0.6703296703296703}
{"id": 4233, "code": "def find on path ( importer , path item , only = False ) : path item = normalize cached ( path item ) if os . path . isdir ( path item ) and os . access ( path item , os . R OK ) : if path item . lower ( ) . endswith ( '.egg' ) : yield Distribution . from filename ( path item , metadata = Path Metadata ( path item , os . path . join ( path item , 'EGG-INFO' ) ) ) else : for entry in os . listdir ( path item ) : lower = entry . lower ( ) if lower . endswith ( '.egg-info' ) or lower . endswith ( '.dist-info' ) : fullpath = os . path . join ( path item , entry ) if os . path . isdir ( fullpath ) : metadata = Path Metadata ( path item , fullpath ) else : metadata = File Metadata ( fullpath ) yield Distribution . from location ( path item , entry , metadata , precedence = DEVELOP DIST ) elif not only and lower . endswith ( '.egg' ) : dists = find distributions ( os . path . join ( path item , entry ) ) for dist in dists : yield dist elif not only and lower . endswith ( '.egg-link' ) : with open ( os . path . join ( path item , entry ) ) as entry file : entry lines = entry file . readlines ( ) for line in entry lines : if not line . strip ( ) : continue path = os . path . join ( path item , line . rstrip ( ) ) dists = find distributions ( path ) for item in dists : yield item break", "predictions": ["find all distributions in a directory ."], "references": ["yield distributions accessible on a sys . path directory"], "bleu": 0.1755217914979255, "rouge_l": 0.3667334669338677}
{"id": 4234, "code": "def declare namespace ( package Name ) : imp . acquire lock ( ) try : if package Name in namespace packages : return path , parent = sys . path , None if '.' in package Name : parent = '.' . join ( package Name . split ( '.' ) [ : - 1 ] ) declare namespace ( parent ) if parent not in namespace packages : import ( parent ) try : path = sys . modules [ parent ] . path except Attribute Error : raise Type Error ( \"Not a package:\" , parent ) namespace packages . setdefault ( parent , [ ] ) . append ( package Name ) namespace packages . setdefault ( package Name , [ ] ) for path item in path : handle ns ( package Name , path item ) finally : imp . release lock ( )", "predictions": ["declare a namespace namespace ."], "references": ["declare that package packagename is a namespace package"], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 4235, "code": "def get mro ( cls ) : if not isinstance ( cls , type ) : class cls ( cls , object ) : pass return cls . mro [ 1 : ] return cls . mro", "predictions": ["return the mro for the given type ."], "references": ["get an mro for a type or classic class"], "bleu": 0.20014292374951972, "rouge_l": 0.34923664122137404}
{"id": 4236, "code": "def find adapter ( registry , ob ) : for t in get mro ( getattr ( ob , ' class ' , type ( ob ) ) ) : if t in registry : return registry [ t ]", "predictions": ["return the first adapter of the given registry ."], "references": ["return an adapter factory for ob from registry"], "bleu": 0.16784459625186196, "rouge_l": 0.35672514619883033}
{"id": 4237, "code": "def ensure directory ( path ) : dirname = os . path . dirname ( path ) if not os . path . isdir ( dirname ) : os . makedirs ( dirname )", "predictions": ["ensure that the directory exists"], "references": ["ensure that the parent directory of path exists"], "bleu": 0.3086194627209984, "rouge_l": 0.7384987893462469}
{"id": 4238, "code": "def insert on ( self , path , loc = None ) : loc = loc or self . location if not loc : return nloc = normalize cached ( loc ) bdir = os . path . dirname ( nloc ) npath = [ ( p and normalize cached ( p ) or p ) for p in path ] for p , item in enumerate ( npath ) : if item == nloc : break elif item == bdir and self . precedence == EGG DIST : if path is sys . path : self . check version conflict ( ) path . insert ( p , loc ) npath . insert ( p , nloc ) break else : if path is sys . path : self . check version conflict ( ) path . append ( loc ) return while True : try : np = npath . index ( nloc , p + 1 ) except Value Error : break else : del npath [ np ] , path [ np ] p = np return", "predictions": ["insert a new path into the location ."], "references": ["insert self . location in path before its nearest parent directory"], "bleu": 0.13859150907108325, "rouge_l": 0.20469798657718125}
{"id": 4239, "code": "def parse pattern ( pattern ) : if isinstance ( pattern , Number Pattern ) : return pattern def match number ( pattern ) : rv = number re . search ( pattern ) if rv is None : raise Value Error ( 'Invalid number pattern %r' % pattern ) return rv . groups ( ) pos pattern = pattern if ';' in pattern : pos pattern , neg pattern = pattern . split ( ';' , 1 ) pos prefix , number , pos suffix = match number ( pos pattern ) neg prefix , , neg suffix = match number ( neg pattern ) else : pos prefix , number , pos suffix = match number ( pos pattern ) neg prefix = '-' + pos prefix neg suffix = pos suffix if 'E' in number : number , exp = number . split ( 'E' , 1 ) else : exp = None if '@' in number : if '.' in number and '0' in number : raise Value Error ( 'Significant digit patterns can not contain ' '\"@\" or \"0\"' ) if '.' in number : integer , fraction = number . rsplit ( '.' , 1 ) else : integer = number fraction = '' def parse precision ( p ) : \"\"\"Calculate the min and max allowed digits\"\"\" min = max = 0 for c in p : if c in '@0' : min += 1 max += 1 elif c == '#' : max += 1 elif c == ',' : continue else : break return min , max int prec = parse precision ( integer ) frac prec = parse precision ( fraction ) if exp : exp plus = exp . startswith ( '+' ) exp = exp . lstrip ( '+' ) exp prec = parse precision ( exp ) else : exp plus = None exp prec = None grouping = babel . numbers . parse grouping ( integer ) return Number Pattern ( pattern , ( pos prefix , neg prefix ) , ( pos suffix , neg suffix ) , grouping , int prec , frac prec , exp prec , exp plus )", "predictions": ["parse a number of pattern into a precision ."], "references": ["parse number format patterns"], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 4240, "code": "def get decimal quantum ( precision ) : assert isinstance ( precision , ( int , decimal . Decimal ) ) return decimal . Decimal ( 10 ) ** ( - precision )", "predictions": ["return the decimal quantum quantum number of bytes"], "references": ["return minimal quantum of a number as defined by precision ."], "bleu": 0.13859150907108325, "rouge_l": 0.3070469798657718}
{"id": 4241, "code": "def scientific notation elements ( self , value , locale ) : exp = value . adjusted ( ) value = value * get decimal quantum ( exp ) assert value . adjusted ( ) == 0 lead shift = max ( [ 1 , min ( self . int prec ) ] ) - 1 exp = exp - lead shift value = value * get decimal quantum ( - lead shift ) exp sign = '' if exp < 0 : exp sign = babel . numbers . get minus sign symbol ( locale ) elif self . exp plus : exp sign = babel . numbers . get plus sign symbol ( locale ) exp = abs ( exp ) return value , exp , exp sign", "predictions": ["gets the value of the scientific notation ."], "references": ["returns normalized scientific notation components of a value ."], "bleu": 0.22149455506955362, "rouge_l": 0.34923664122137404}
{"id": 4242, "code": "def total seconds ( td ) : if hasattr ( td , 'total seconds' ) : return td . total seconds ( ) ms = td . microseconds secs = ( td . seconds + td . days * 24 * 3600 ) return ( ms + secs * 10 ** 6 ) / 10 ** 6", "predictions": ["returns the total number of seconds in seconds"], "references": ["python 2 . 6 compatability"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4243, "code": "def check extras ( dist , attr , value ) : try : for k , v in value . items ( ) : if ':' in k : k , m = k . split ( ':' , 1 ) if pkg resources . invalid marker ( m ) : raise Distutils Setup Error ( \"Invalid environment marker: \" + m ) list ( pkg resources . parse requirements ( v ) ) except ( Type Error , Value Error , Attribute Error ) : raise Distutils Setup Error ( \"'extras require' must be a dictionary whose values are \" \"strings or lists of strings containing valid project/version \" \"requirement specifiers.\" )", "predictions": ["verify that the value is valid ."], "references": ["verify that extras_require mapping is valid"], "bleu": 0.3073940764756322, "rouge_l": 0.6240409207161125}
{"id": 4244, "code": "def check requirements ( dist , attr , value ) : try : list ( pkg resources . parse requirements ( value ) ) except ( Type Error , Value Error ) as error : tmpl = ( \"{attr!r} must be a string or list of strings \" \"containing valid project/version requirement specifiers; {error}\" ) raise Distutils Setup Error ( tmpl . format ( attr = attr , error = error ) )", "predictions": ["verify that value is valid ."], "references": ["verify that install_requires is a valid requirements list"], "bleu": 0.236682065782701, "rouge_l": 0.5570776255707762}
{"id": 4245, "code": "def fetch build egg ( self , req ) : try : cmd = self . egg fetcher cmd . package index . to scan = [ ] except Attribute Error : from setuptools . command . easy install import easy install dist = self . class ( { 'script args' : [ 'easy install' ] } ) dist . parse config files ( ) opts = dist . get option dict ( 'easy install' ) keep = ( 'find links' , 'site dirs' , 'index url' , 'optimize' , 'site dirs' , 'allow hosts' ) for key in list ( opts ) : if key not in keep : del opts [ key ] if self . dependency links : links = self . dependency links [ : ] if 'find links' in opts : links = opts [ 'find links' ] [ 1 ] . split ( ) + links opts [ 'find links' ] = ( 'setup' , links ) install dir = self . get egg cache dir ( ) cmd = easy install ( dist , args = [ \"x\" ] , install dir = install dir , exclude scripts = True , always copy = False , build directory = None , editable = False , upgrade = False , multi version = True , no report = True , user = False ) cmd . ensure finalized ( ) self . egg fetcher = cmd return cmd . easy install ( req )", "predictions": ["fetch an egg from a package"], "references": ["fetch an egg needed for building"], "bleu": 0.4111336169005197, "rouge_l": 0.5}
{"id": 4246, "code": "def do dice roll ( ) : options = get options ( ) dice = Dice ( options . sides ) rolls = [ dice . roll ( ) for n in range ( options . number ) ] for roll in rolls : print ( 'rolled' , roll ) if options . number > 1 : print ( 'total' , sum ( rolls ) )", "predictions": ["rolls all roll roll roll options ."], "references": ["roll n - sided dice and return each result and the total"], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 4247, "code": "def price converter ( obj ) : if isinstance ( obj , str ) : obj = Price Class . parse ( obj ) return obj", "predictions": ["convert an iso - terminated price to a string ."], "references": ["ensures that string prices are converted into price objects ."], "bleu": 0.14991106946711685, "rouge_l": 0.2}
{"id": 4248, "code": "def get method ( self , args ) : try : method = self . app [ args [ 'method' ] ] except Key Error : method not found ( args [ 'id' ] ) else : return method", "predictions": ["get the method method for this app ."], "references": ["get request method for service application ."], "bleu": 0.239802967618271, "rouge_l": 0.5398230088495575}
{"id": 4249, "code": "def apply ( self , method , args ) : try : params = args [ 'params' ] if isinstance ( params , dict ) : result = method ( * * params ) else : result = method ( * params ) except Exception as error : server error ( args [ 'id' ] , error ) else : return result", "predictions": ["apply method to method"], "references": ["apply application method ."], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 4250, "code": "def blueprint ( self ) : if self . url rule and '.' in self . url rule . endpoint : return self . url rule . endpoint . rsplit ( '.' , 1 ) [ 0 ]", "predictions": ["return the blueprint object of this route ."], "references": ["the name of the current blueprint"], "bleu": 0.19070828081828378, "rouge_l": 0.2932692307692307}
{"id": 4251, "code": "def cleanup files ( self ) : logger . debug ( 'Cleaning up...' ) with indent log ( ) : for req in self . reqs to cleanup : req . remove temporary source ( )", "predictions": ["remove all files created by the source ."], "references": ["clean up files remove builds ."], "bleu": 0.19070828081828378, "rouge_l": 0.2932692307692307}
{"id": 4252, "code": "def get all ns packages ( self ) : nsp = set ( ) for pkg in self . distribution . namespace packages or [ ] : pkg = pkg . split ( '.' ) while pkg : nsp . add ( '.' . join ( pkg ) ) pkg . pop ( ) return sorted ( nsp )", "predictions": ["get all ns packages ."], "references": ["return sorted list of all package namespaces"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 4253, "code": "def default ( self , obj ) : if isinstance ( obj , models . Model ) : return self . encode ( model to dict ( obj ) ) elif isinstance ( obj , models . query . Query Set ) : return serializers . serialize ( 'json' , obj ) else : return super ( Json Response Encoder , self ) . default ( obj )", "predictions": ["convert json object into a dict"], "references": ["convert queryset objects to their list counter - parts"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4254, "code": "def tokenize annotated ( doc , annotation ) : tokens = tokenize ( doc , include hrefs = False ) for tok in tokens : tok . annotation = annotation return tokens", "predictions": ["tokenize an annotation doc into a list of annotated tokens ."], "references": ["tokenize a document and add an annotation attribute to each token"], "bleu": 0.17033186037639278, "rouge_l": 0.2727272727272727}
{"id": 4255, "code": "def copy annotations ( src , dest ) : assert len ( src ) == len ( dest ) for src tok , dest tok in zip ( src , dest ) : dest tok . annotation = src tok . annotation", "predictions": ["copy annotations from src to dest"], "references": ["copy annotations from the tokens listed in src to the tokens in dest"], "bleu": 0.1582341759519314, "rouge_l": 0.5922330097087379}
{"id": 4256, "code": "def fixup chunks ( chunks ) : tag accum = [ ] cur word = None result = [ ] for chunk in chunks : if isinstance ( chunk , tuple ) : if chunk [ 0 ] == 'img' : src = chunk [ 1 ] tag , trailing whitespace = split trailing whitespace ( chunk [ 2 ] ) cur word = tag token ( 'img' , src , html repr = tag , pre tags = tag accum , trailing whitespace = trailing whitespace ) tag accum = [ ] result . append ( cur word ) elif chunk [ 0 ] == 'href' : href = chunk [ 1 ] cur word = href token ( href , pre tags = tag accum , trailing whitespace = \" \" ) tag accum = [ ] result . append ( cur word ) continue if is word ( chunk ) : chunk , trailing whitespace = split trailing whitespace ( chunk ) cur word = token ( chunk , pre tags = tag accum , trailing whitespace = trailing whitespace ) tag accum = [ ] result . append ( cur word ) elif is start tag ( chunk ) : tag accum . append ( chunk ) elif is end tag ( chunk ) : if tag accum : tag accum . append ( chunk ) else : assert cur word , ( \"Weird state, cur word=%r, result=%r, chunks=%r of %r\" % ( cur word , result , chunk , chunks ) ) cur word . post tags . append ( chunk ) else : assert ( 0 ) if not result : return [ token ( '' , pre tags = tag accum ) ] else : result [ - 1 ] . post tags . extend ( tag accum ) return result", "predictions": ["add filters to filters . . . . . . . . ."], "references": ["this function takes a list of chunks and produces a list of tokens ."], "bleu": 0.08844818008721958, "rouge_l": 0.0735826296743064}
{"id": 4257, "code": "def start tag ( el ) : return '<%s%s>' % ( el . tag , '' . join ( [ ' %s=\"%s\"' % ( name , html escape ( value , True ) ) for name , value in el . attrib . items ( ) ] ) )", "predictions": ["return an html handler ."], "references": ["the text representation of the start tag for a tag ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 4258, "code": "def fixup ins del tags ( doc ) : for tag in [ 'ins' , 'del' ] : for el in doc . xpath ( 'descendant-or-self::%s' % tag ) : if not contains block level tag ( el ) : continue move el inside block ( el , tag = tag ) el . drop tag ( )", "predictions": ["remove self . from logger . ."], "references": ["fixup_ins_del_tags that works on an lxml document in - place"], "bleu": 0.10175282441454783, "rouge_l": 0.0}
{"id": 4259, "code": "def cache url ( self , * * kwargs ) : query = { 'Operation' : self . Operation , 'Service' : \"AWSE Commerce Service\" , 'Version' : self . Version , } query . update ( kwargs ) service domain = SERVICE DOMAINS [ self . Region ] [ 0 ] return \"http://\" + service domain + \"/onca/xml?\" + quote query ( query )", "predictions": ["returns the logger logger for the current page get ."], "references": ["a simplified url to be used for caching the given query ."], "bleu": 0.12273680279953825, "rouge_l": 0.2683284457478006}
{"id": 4260, "code": "def document fromstring ( html , guess charset = True , parser = None ) : if not isinstance ( html , strings ) : raise Type Error ( 'string required' ) if parser is None : parser = html parser return parser . parse ( html , use Chardet = guess charset ) . getroot ( )", "predictions": ["return html instance ."], "references": ["parse a whole document into a string ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 4261, "code": "def export ( self , location ) : url , rev = self . get url rev ( ) rev options = get rev options ( url , rev ) logger . info ( 'Exporting svn repository %s to %s' , url , location ) with indent log ( ) : if os . path . exists ( location ) : rmtree ( location ) self . run command ( [ 'export' ] + rev options + [ url , location ] , show stdout = False )", "predictions": ["override the tempdir try to the destination try to the destination try to the destination location path path path path path path path path path path path path path path path"], "references": ["export the svn repository at the url to the destination location"], "bleu": 0.11811895313352072, "rouge_l": 0.3125533731853117}
{"id": 4262, "code": "def get revision ( self , location ) : revision = 0 for base , dirs , files in os . walk ( location ) : if self . dirname not in dirs : dirs [ : ] = [ ] continue dirs . remove ( self . dirname ) entries fn = os . path . join ( base , self . dirname , 'entries' ) if not os . path . exists ( entries fn ) : continue dirurl , localrev = self . get svn url rev ( base ) if base == location : base url = dirurl + '/' elif not dirurl or not dirurl . startswith ( base url ) : dirs [ : ] = [ ] continue revision = max ( revision , localrev ) return revision", "predictions": ["return the setup directory at the given args"], "references": ["return the maximum revision for all files under a given location"], "bleu": 0.15587146574232644, "rouge_l": 0.3070469798657718}
{"id": 4263, "code": "def unique ( iterable ) : seen = set ( ) for value in iterable : if not value in seen : seen . add ( value ) yield value", "predictions": ["yields all getitem in an self . . ."], "references": ["yield unique values in iterable preserving order ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 4264, "code": "def handle requires ( metadata , pkg info , key ) : may requires = defaultdict ( list ) for value in pkg info . get all ( key ) : extra match = EXTRA RE . search ( value ) if extra match : groupdict = extra match . groupdict ( ) condition = groupdict [ 'condition' ] extra = groupdict [ 'extra' ] package = groupdict [ 'package' ] if condition . endswith ( ' and ' ) : condition = condition [ : - 5 ] else : condition , extra = None , None package = value key = May Requires Key ( condition , extra ) may requires [ key ] . append ( package ) if may requires : metadata [ 'run requires' ] = [ ] for key , value in may requires . items ( ) : may requirement = { 'requires' : value } if key . extra : may requirement [ 'extra' ] = key . extra if key . condition : may requirement [ 'environment' ] = key . condition metadata [ 'run requires' ] . append ( may requirement ) if not 'extras' in metadata : metadata [ 'extras' ] = [ ] metadata [ 'extras' ] . extend ( [ key . extra for key in may requires . keys ( ) if key . extra ] )", "predictions": ["find all metadata for the given path"], "references": ["place the runtime requirements from pkg_info into metadata ."], "bleu": 0.15447878876032708, "rouge_l": 0.12224448897795591}
{"id": 4265, "code": "def requires to requires dist ( requirement ) : requires dist = [ ] for op , ver in requirement . specs : requires dist . append ( op + ver ) if not requires dist : return '' return \" (%s)\" % ',' . join ( requires dist )", "predictions": ["returns a string representation of a importer os os os os os os os os os os os os os os os os os os os os os os os os"], "references": ["compose the version predicates for requirement in pep 345 fashion ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4266, "code": "def modules ( self ) : sys . path . insert ( 0 , self . basedir ) for p in self . paths ( ) : try : module name = self . module path ( p ) logger . debug ( \"Importing {} from path {}\" . format ( module name , p ) ) m = importlib . import module ( module name ) yield m except Exception as e : logger . warning ( 'Caught exception while importing {}: {}' . format ( p , e ) ) logger . warning ( e , exc info = True ) error info = getattr ( self , 'error info' , None ) if not error info : exc info = sys . exc info ( ) #raise e. class , e, exc info[2] #self.error info = (e, exc info) self . error info = exc info continue sys . path . pop ( 0 )", "predictions": ["not thread - safe ."], "references": ["return modules that match module_name"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 4267, "code": "def classes ( self ) : for module in self . modules ( ) : cs = inspect . getmembers ( module , inspect . isclass ) class name = getattr ( self , 'class name' , '' ) class regex = '' if class name : if class name . startswith ( \"*\" ) : class name = class name . strip ( \"*\" ) class regex = re . compile ( r'.*?{}' . format ( class name ) , re . I ) else : class regex = re . compile ( r'^{}' . format ( class name ) , re . I ) for c name , c in cs : can yield = True if class regex and not class regex . match ( c name ) : #if class name and class name not in c name: can yield = False if can yield and issubclass ( c , unittest . Test Case ) : if c is not unittest . Test Case : logger . debug ( 'class: {} matches {}' . format ( c name , class name ) ) yield c", "predictions": ["return all get get get get get get get get get class get class get class get class get class get class ."], "references": ["the partial self . class_name will be used to find actual testcase classes"], "bleu": 0.05291907393644996, "rouge_l": 0.05848513902205177}
{"id": 4268, "code": "def method names ( self ) : for c in self . classes ( ) : #ms = inspect.getmembers(c, inspect.ismethod) ms = inspect . getmembers ( c , lambda f : inspect . ismethod ( f ) or inspect . isfunction ( f ) ) method name = getattr ( self , 'method name' , '' ) method regex = '' if method name : if method name . startswith ( self . method prefix ) : method regex = re . compile ( r'^{}' . format ( method name ) , flags = re . I ) else : if method name . startswith ( \"*\" ) : method name = method name . strip ( \"*\" ) method regex = re . compile ( r'^{}[ ]{{0,1}}.*?{}' . format ( self . method prefix , method name ) , flags = re . I ) else : method regex = re . compile ( r'^{}[ ]{{0,1}}{}' . format ( self . method prefix , method name ) , flags = re . I ) for m name , m in ms : if not m name . startswith ( self . method prefix ) : continue can yield = True if method regex and not method regex . match ( m name ) : can yield = False if can yield : logger . debug ( 'method: {} matches {}' . format ( m name , method name ) ) yield c , m name", "predictions": ["find all find find find find matching the find find find all find find find ."], "references": ["return the actual test methods that matched self . method_name"], "bleu": 0.08513012360883544, "rouge_l": 0.16052631578947368}
{"id": 4269, "code": "def dump arg defaults ( kwargs ) : if current app : kwargs . setdefault ( 'cls' , current app . json encoder ) if not current app . config [ 'JSON AS ASCII' ] : kwargs . setdefault ( 'ensure ascii' , False ) kwargs . setdefault ( 'sort keys' , current app . config [ 'JSON SORT KEYS' ] ) else : kwargs . setdefault ( 'sort keys' , True ) kwargs . setdefault ( 'cls' , JSON Encoder )", "predictions": ["ensure that the default values are defined"], "references": ["inject default arguments for dump functions ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4270, "code": "def load arg defaults ( kwargs ) : if current app : kwargs . setdefault ( 'cls' , current app . json decoder ) else : kwargs . setdefault ( 'cls' , JSON Decoder )", "predictions": ["insert the default values of the . if it is not none . it"], "references": ["inject default arguments for load functions ."], "bleu": 0.09782375748961449, "rouge_l": 0.2026578073089701}
{"id": 4271, "code": "def get dist ( self ) : egg info = self . egg info path ( '' ) . rstrip ( '/' ) base dir = os . path . dirname ( egg info ) metadata = pkg resources . Path Metadata ( base dir , egg info ) dist name = os . path . splitext ( os . path . basename ( egg info ) ) [ 0 ] return pkg resources . Distribution ( os . path . dirname ( egg info ) , project name = dist name , metadata = metadata )", "predictions": ["parse the pattern in the current directory . ."], "references": ["return a pkg_resources . distribution built from self . egg_info_path"], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 4272, "code": "def to text ( s , blank if none = True ) : if s is None : if blank if none : return \"\" else : return None elif isinstance ( s , text type ) : return s else : return text type ( s )", "predictions": ["convert string to decimal ."], "references": ["wrapper around six . text_type to convert none to empty string"], "bleu": 0.10339832360529193, "rouge_l": 0.2341650671785029}
{"id": 4273, "code": "def find ca bundle ( ) : if os . name == 'nt' : return get win certfile ( ) else : for cert path in cert paths : if os . path . isfile ( cert path ) : return cert path try : return pkg resources . resource filename ( 'certifi' , 'cacert.pem' ) except ( Import Error , Resolution Error , Extraction Error ) : return None", "predictions": [". utility to scientific elements ."], "references": ["return an existing ca bundle path or none"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 4274, "code": "def parse ( doc , treebuilder = \"etree\" , encoding = None , namespace HTML Elements = True ) : tb = treebuilders . get Tree Builder ( treebuilder ) p = HTML Parser ( tb , namespace HTML Elements = namespace HTML Elements ) return p . parse ( doc , encoding = encoding )", "predictions": ["total parsing of the document s stats + treebuilder"], "references": ["parse a string or file - like object into a tree"], "bleu": 0.09503475972243516, "rouge_l": 0.0}
{"id": 4275, "code": "def bind ( self ) : HTTP Server . init ( self , ( self . host , self . port ) , HTTP Request Handler ) self . port = self . server port", "predictions": ["check if the items is ready to be created"], "references": ["bind and activate http server ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4276, "code": "def report ( self ) : print ( self . report message . format ( service = self . service , host = self . host , port = self . port , ) ) sys . stdout . flush ( )", "predictions": ["check the check of the try to the console value"], "references": ["report startup info to stdout ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 4277, "code": "def load bytecode ( self , f ) : magic = f . read ( len ( bc magic ) ) if magic != bc magic : self . reset ( ) return checksum = pickle . load ( f ) if self . checksum != checksum : self . reset ( ) return self . code = marshal load ( f )", "predictions": ["fetch a build from file"], "references": ["loads bytecode from a file or file like object ."], "bleu": 0.11943865131127647, "rouge_l": 0.2515463917525773}
{"id": 4278, "code": "def get impl ver ( ) : impl ver = sysconfig . get config var ( \"py version nodot\" ) if not impl ver : impl ver = '' . join ( map ( str , sys . version info [ : 2 ] ) ) return impl ver", "predictions": ["return the rolls rolls rolls get the current rolls rolls get it ."], "references": ["return implementation version ."], "bleu": 0.10571070857151538, "rouge_l": 0.2601279317697228}
{"id": 4279, "code": "def distros for location ( location , basename , metadata = None ) : if basename . endswith ( '.egg.zip' ) : basename = basename [ : - 4 ] if basename . endswith ( '.egg' ) and '-' in basename : return [ Distribution . from location ( location , basename , metadata ) ] if basename . endswith ( '.exe' ) : win base , py ver , platform = parse bdist wininst ( basename ) if win base is not None : return interpret distro name ( location , win base , metadata , py ver , BINARY DIST , platform ) # for ext in EXTENSIONS : if basename . endswith ( ext ) : basename = basename [ : - len ( ext ) ] return interpret distro name ( location , basename , metadata ) return [ ]", "predictions": ["convert a source source into a list of source objects ."], "references": ["yield egg or source distribution objects based on basename"], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 4280, "code": "def find external links ( url , page ) : for match in REL . finditer ( page ) : tag , rel = match . groups ( ) rels = set ( map ( str . strip , rel . lower ( ) . split ( ',' ) ) ) if 'homepage' in rels or 'download' in rels : for match in HREF . finditer ( tag ) : yield urljoin ( url , htmldecode ( match . group ( 1 ) ) ) for tag in ( \"<th>Home Page\" , \"<th>Download URL\" ) : pos = page . find ( tag ) if pos != - 1 : match = HREF . search ( page , pos ) if match : yield urljoin ( url , htmldecode ( match . group ( 1 ) ) )", "predictions": ["get method to get method links"], "references": ["find rel = homepage and rel = download links in page yielding urls"], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 4281, "code": "def local open ( url ) : scheme , server , path , param , query , frag = urlparse ( url ) filename = url2pathname ( path ) if os . path . isfile ( filename ) : return urllib2 . urlopen ( url ) elif path . endswith ( '/' ) and os . path . isdir ( filename ) : files = [ ] for f in os . listdir ( filename ) : if f == 'index.html' : with open ( os . path . join ( filename , f ) , 'r' ) as fp : body = fp . read ( ) break elif os . path . isdir ( os . path . join ( filename , f ) ) : f += '/' files . append ( \"<a href=%r>%s</a>\" % ( f , f ) ) else : body = ( \"<html><head><title>%s</title>\" % url ) + \"</head><body>%s</body></html>\" % '\\n' . join ( files ) status , message = 200 , \"OK\" else : status , message , body = 404 , \"Path not found\" , \"Not found\" headers = { 'content-type' : 'text/html' } return HTTP Error ( url , status , message , headers , String IO ( body ) )", "predictions": ["open open with apply scheme to scheme"], "references": ["read a local path with special support for directories"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 4282, "code": "def process url ( self , url , retrieve = False ) : if url in self . scanned urls and not retrieve : return self . scanned urls [ url ] = True if not URL SCHEME ( url ) : self . process filename ( url ) return else : dists = list ( distros for url ( url ) ) if dists : if not self . url ok ( url ) : return self . debug ( \"Found link: %s\" , url ) if dists or not retrieve or url in self . fetched urls : list ( map ( self . add , dists ) ) return if not self . url ok ( url ) : self . fetched urls [ url ] = True return self . info ( \"Reading %s\" , url ) self . fetched urls [ url ] = True f = self . open url ( url , \"Download error on %s: %%s -- Some packages may not be found!\" % url ) if f is None : return self . fetched urls [ f . url ] = True if 'html' not in f . headers . get ( 'content-type' , '' ) . lower ( ) : f . close ( ) return base = f . url page = f . read ( ) if not isinstance ( page , str ) : if isinstance ( f , HTTP Error ) : charset = 'latin-1' else : charset = f . headers . get param ( 'charset' ) or 'latin-1' page = page . decode ( charset , \"ignore\" ) f . close ( ) for match in HREF . finditer ( page ) : link = urljoin ( base , htmldecode ( match . group ( 1 ) ) ) self . process url ( link ) if url . startswith ( self . index url ) and getattr ( f , 'code' , None ) != 404 : page = self . process index ( url , page )", "predictions": ["blueprint a url from a url endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint endpoint"], "references": ["evaluate a url as a possible download and maybe retrieve it"], "bleu": 0.055177848898164926, "rouge_l": 0.15627668659265584}
{"id": 4283, "code": "def init pathinfo ( ) : d = set ( ) for dir in sys . path : try : if os . path . isdir ( dir ) : dir , dircase = makepath ( dir ) d . add ( dircase ) except Type Error : continue return d", "predictions": ["return with all paths in the current directory indent indent indent indent indent indent"], "references": ["return a set containing all existing directory entries from sys . path"], "bleu": 0.10511846841633776, "rouge_l": 0.2340153452685422}
{"id": 4284, "code": "def setcopyright ( ) : builtins . copyright = Printer ( \"copyright\" , sys . copyright ) if is jython : builtins . credits = Printer ( \"credits\" , \"Jython is maintained by the Jython developers (www.jython.org).\" ) elif is pypy : builtins . credits = Printer ( \"credits\" , \"Py Py is maintained by the Py Py developers: http://pypy.org/\" ) else : builtins . credits = Printer ( \"credits\" , ) here = os . path . dirname ( os . file ) builtins . license = Printer ( \"license\" , \"See http://www.python.org/%.3s/license.html\" % sys . version , [ \"LICENSE.txt\" , \"LICENSE\" ] , [ os . path . join ( here , os . pardir ) , here , os . curdir ] )", "predictions": ["setup the distribution of the distribution self self self self self self self self self self self self self self self self self self self self self self self self self"], "references": ["set copyright and credits in __builtin__"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4285, "code": "def have pyrex ( ) : pyrex impls = 'Cython.Distutils.build ext' , 'Pyrex.Distutils.build ext' for pyrex impl in pyrex impls : try : import ( pyrex impl , fromlist = [ 'build ext' ] ) . build ext return True except Exception : pass return False", "predictions": ["check if the pyrex is pyrex"], "references": ["return true if cython or pyrex can be imported ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 4286, "code": "def debug application ( self , environ , start response ) : app iter = None try : app iter = self . app ( environ , start response ) for item in app iter : yield item if hasattr ( app iter , 'close' ) : app iter . close ( ) except Exception : if hasattr ( app iter , 'close' ) : app iter . close ( ) traceback = get current traceback ( skip = 1 , show hidden frames = self . show hidden frames , ignore system exceptions = True ) for frame in traceback . frames : self . frames [ frame . id ] = frame self . tracebacks [ traceback . id ] = traceback try : start response ( '500 INTERNAL SERVER ERROR' , [ ( 'Content-Type' , 'text/html; charset=utf-8' ) , ( 'X-XSS-Protection' , '0' ) , ] ) except Exception : environ [ 'wsgi.errors' ] . write ( 'Debugging middleware caught exception in streamed ' 'response at a point where response headers were already ' 'sent.\\n' ) else : yield traceback . render full ( evalex = self . evalex , secret = self . secret ) . encode ( 'utf-8' , 'replace' ) traceback . log ( environ [ 'wsgi.errors' ] )", "predictions": ["tokenize the annotated annotated in the full in the full in the include include"], "references": ["run the application and conserve the traceback frames ."], "bleu": 0.09782375748961449, "rouge_l": 0.18100890207715134}
{"id": 4287, "code": "def get resource ( self , request , filename ) : filename = join ( dirname ( file ) , 'shared' , basename ( filename ) ) if isfile ( filename ) : mimetype = mimetypes . guess type ( filename ) [ 0 ] or 'application/octet-stream' f = open ( filename , 'rb' ) try : return Response ( f . read ( ) , mimetype = mimetype ) finally : f . close ( ) return Response ( 'Not Found' , status = 404 )", "predictions": ["return the annotations of the given filename"], "references": ["return a static resource from the shared folder ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 4288, "code": "def user agent ( ) : data = { \"installer\" : { \"name\" : \"pip\" , \"version\" : pip . version } , \"python\" : platform . python version ( ) , \"implementation\" : { \"name\" : platform . python implementation ( ) , } , } if data [ \"implementation\" ] [ \"name\" ] == 'C Python' : data [ \"implementation\" ] [ \"version\" ] = platform . python version ( ) elif data [ \"implementation\" ] [ \"name\" ] == 'Py Py' : if sys . pypy version info . releaselevel == 'final' : pypy version info = sys . pypy version info [ : 3 ] else : pypy version info = sys . pypy version info data [ \"implementation\" ] [ \"version\" ] = \".\" . join ( [ str ( x ) for x in pypy version info ] ) elif data [ \"implementation\" ] [ \"name\" ] == 'Jython' : data [ \"implementation\" ] [ \"version\" ] = platform . python version ( ) elif data [ \"implementation\" ] [ \"name\" ] == 'Iron Python' : data [ \"implementation\" ] [ \"version\" ] = platform . python version ( ) if sys . platform . startswith ( \"linux\" ) : distro = dict ( filter ( lambda x : x [ 1 ] , zip ( [ \"name\" , \"version\" , \"id\" ] , platform . linux distribution ( ) ) , ) ) libc = dict ( filter ( lambda x : x [ 1 ] , zip ( [ \"lib\" , \"version\" ] , platform . libc ver ( ) ) , ) ) if libc : distro [ \"libc\" ] = libc if distro : data [ \"distro\" ] = distro if sys . platform . startswith ( \"darwin\" ) and platform . mac ver ( ) [ 0 ] : data [ \"distro\" ] = { \"name\" : \"OS X\" , \"version\" : platform . mac ver ( ) [ 0 ] } if platform . system ( ) : data . setdefault ( \"system\" , { } ) [ \"name\" ] = platform . system ( ) if platform . release ( ) : data . setdefault ( \"system\" , { } ) [ \"release\" ] = platform . release ( ) if platform . machine ( ) : data [ \"cpu\" ] = platform . machine ( ) return \"{data[installer][name]}/{data[installer][version]} {json}\" . format ( data = data , json = json . dumps ( data , separators = ( \",\" , \":\" ) , sort keys = True ) , )", "predictions": ["return the user agent agent ."], "references": ["return a string representing the user agent ."], "bleu": 0.35032865403497776, "rouge_l": 0.6963470319634703}
{"id": 4289, "code": "def is url ( name ) : if ':' not in name : return False scheme = name . split ( ':' , 1 ) [ 0 ] . lower ( ) return scheme in [ 'http' , 'https' , 'file' , 'ftp' ] + vcs . all schemes", "predictions": ["return true if the name is an url ."], "references": ["returns true if the name looks like a url"], "bleu": 0.4111336169005197, "rouge_l": 0.5555555555555556}
{"id": 4290, "code": "def download http url ( link , session , temp dir ) : target url = link . url . split ( '#' , 1 ) [ 0 ] try : resp = session . get ( target url , headers = { \"Accept-Encoding\" : \"identity\" } , stream = True , ) resp . raise for status ( ) except requests . HTTP Error as exc : logger . critical ( \"HTTP error %s while getting %s\" , exc . response . status code , link , ) raise content type = resp . headers . get ( 'content-type' , '' ) filename = link . filename content disposition = resp . headers . get ( 'content-disposition' ) if content disposition : type , params = cgi . parse header ( content disposition ) filename = params . get ( 'filename' ) or filename ext = splitext ( filename ) [ 1 ] if not ext : ext = mimetypes . guess extension ( content type ) if ext : filename += ext if not ext and link . url != resp . url : ext = os . path . splitext ( resp . url ) [ 1 ] if ext : filename += ext file path = os . path . join ( temp dir , filename ) with open ( file path , 'wb' ) as content file : download url ( resp , link , content file ) return file path , content type", "predictions": ["download http url from http ."], "references": ["download link url into temp_dir using provided session"], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4291, "code": "def currency Format ( context , code , symbol , format , currency digits = True , decimal quantization = True , name = '' ) : context . action ( discriminator = ( 'currency' , name , code ) , callable = register currency , args = ( name , code , symbol , format , currency digits , decimal quantization ) )", "predictions": ["formats an action to be called with another currency"], "references": ["handle currencyformat subdirectives ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4292, "code": "def exchange ( context , component , backend , base , name = '' ) : context . action ( discriminator = ( 'currency' , 'exchange' , component ) , callable = register exchange , args = ( name , component , backend , base ) )", "predictions": ["registers an exchange exchange ."], "references": ["handle exchange subdirectives ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 4293, "code": "def print results ( distributions , list all files ) : results printed = False for dist in distributions : results printed = True logger . info ( \"---\" ) logger . info ( \"Metadata-Version: %s\" % dist . get ( 'metadata-version' ) ) logger . info ( \"Name: %s\" % dist [ 'name' ] ) logger . info ( \"Version: %s\" % dist [ 'version' ] ) logger . info ( \"Summary: %s\" % dist . get ( 'summary' ) ) logger . info ( \"Home-page: %s\" % dist . get ( 'home-page' ) ) logger . info ( \"Author: %s\" % dist . get ( 'author' ) ) logger . info ( \"Author-email: %s\" % dist . get ( 'author-email' ) ) logger . info ( \"License: %s\" % dist . get ( 'license' ) ) logger . info ( \"Location: %s\" % dist [ 'location' ] ) logger . info ( \"Requires: %s\" % ', ' . join ( dist [ 'requires' ] ) ) if list all files : logger . info ( \"Files:\" ) if dist [ 'files' ] is not None : for line in dist [ 'files' ] : logger . info ( \"  %s\" % line . strip ( ) ) else : logger . info ( \"Cannot locate installed-files.txt\" ) if 'entry points' in dist : logger . info ( \"Entry-points:\" ) for line in dist [ 'entry points' ] : logger . info ( \"  %s\" % line . strip ( ) ) return results printed", "predictions": ["print all files in a list of distributions"], "references": ["print the informations from installed distributions found ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 4294, "code": "def decode ( self , data , decode content , flush decoder ) : try : if decode content and self . decoder : data = self . decoder . decompress ( data ) except ( IO Error , zlib . error ) as e : content encoding = self . headers . get ( 'content-encoding' , '' ) . lower ( ) raise Decode Error ( \"Received response with content-encoding: %s, but \" \"failed to decode it.\" % content encoding , e ) if flush decoder and decode content and self . decoder : buf = self . decoder . decompress ( binary type ( ) ) data += buf + self . decoder . flush ( ) return data", "predictions": ["decode data from binary stream ."], "references": ["decode the data passed in and potentially flush the decoder ."], "bleu": 0.1141650334026257, "rouge_l": 0.33516483516483514}
{"id": 4295, "code": "def render ( template , context , app ) : rv = template . render ( context ) template rendered . send ( app , template = template , context = context ) return rv", "predictions": ["render a template ."], "references": ["renders the template and fires the signal"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4296, "code": "def parse version ( version ) : global parse version try : from pkg resources import parse version except Import Error : from distutils . version import Loose Version as parse version return parse version ( version )", "predictions": ["parse version string ."], "references": ["use parse_version from pkg_resources or distutils as available ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 4297, "code": "def is declared ( self , name ) : if name in self . declared locally or name in self . declared parameter : return True return name in self . declared", "predictions": ["return true if the name is declared"], "references": ["check if a name is declared in this or an outer scope ."], "bleu": 0.15513171017484098, "rouge_l": 0.3794712286158632}
{"id": 4298, "code": "def visit Name ( self , node ) : if node . ctx == 'store' : self . identifiers . declared locally . add ( node . name ) elif node . ctx == 'param' : self . identifiers . declared parameter . add ( node . name ) elif node . ctx == 'load' and not self . identifiers . is declared ( node . name ) : self . identifiers . undeclared . add ( node . name )", "predictions": ["register an undeclared node ."], "references": ["all assignments to names go through this function ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 4299, "code": "def visit From Import ( self , node , frame ) : self . newline ( node ) self . write ( 'included template = environment.get template(' ) self . visit ( node . template , frame ) self . write ( ', %r).' % self . name ) if node . with context : self . write ( 'make module(context.parent, True)' ) else : self . write ( 'module' ) var names = [ ] discarded names = [ ] for name in node . names : if isinstance ( name , tuple ) : name , alias = name else : alias = name self . writeline ( 'l %s = getattr(included template, ' '%r, missing)' % ( alias , name ) ) self . writeline ( 'if l %s is missing:' % alias ) self . indent ( ) self . writeline ( 'l %s = environment.undefined(%r %% ' 'included template. name , ' 'name=%r)' % ( alias , 'the template %%r (imported on %s) does ' 'not export the requested name %s' % ( self . position ( node ) , repr ( name ) ) , name ) ) self . outdent ( ) if frame . toplevel : var names . append ( alias ) if not alias . startswith ( ' ' ) : discarded names . append ( alias ) frame . assigned names . add ( alias ) if var names : if len ( var names ) == 1 : name = var names [ 0 ] self . writeline ( 'context.vars[%r] = l %s' % ( name , name ) ) else : self . writeline ( 'context.vars.update({%s})' % ', ' . join ( '%r: l %s' % ( name , name ) for name in var names ) ) if discarded names : if len ( discarded names ) == 1 : self . writeline ( 'context.exported vars.discard(%r)' % discarded names [ 0 ] ) else : self . writeline ( 'context.exported vars.difference ' 'update((%s))' % ', ' . join ( imap ( repr , discarded names ) ) )", "predictions": ["visit the newline ."], "references": ["visit named imports ."], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 4300, "code": "def populate requirement set ( requirement set , args , options , finder , session , name , wheel cache ) : for req in args : requirement set . add requirement ( Install Requirement . from line ( req , None , isolated = options . isolated mode , wheel cache = wheel cache ) ) for req in options . editables : requirement set . add requirement ( Install Requirement . from editable ( req , default vcs = options . default vcs , isolated = options . isolated mode , wheel cache = wheel cache ) ) found req in file = False for filename in options . requirements : for req in parse requirements ( filename , finder = finder , options = options , session = session , wheel cache = wheel cache ) : found req in file = True requirement set . add requirement ( req ) if not ( args or options . editables or found req in file ) : opts = { 'name' : name } if options . find links : msg = ( 'You must give at least one requirement to ' '%(name)s (maybe you meant \"pip %(name)s ' '%(links)s\"?)' % dict ( opts , links = ' ' . join ( options . find links ) ) ) else : msg = ( 'You must give at least one requirement ' 'to %(name)s (see \"pip help %(name)s\")' % opts ) logger . warning ( msg )", "predictions": ["populate the requirement set with all requirements from the specified options ."], "references": ["marshal cmd line args into a requirement set ."], "bleu": 0.14694106251955755, "rouge_l": 0.2932692307692307}
{"id": 4301, "code": "def export ( self , location ) : temp dir = tempfile . mkdtemp ( '-export' , 'pip-' ) self . unpack ( temp dir ) if os . path . exists ( location ) : rmtree ( location ) try : self . run command ( [ 'export' , location ] , cwd = temp dir , show stdout = False ) finally : rmtree ( temp dir )", "predictions": ["export the tray tray data to disk ."], "references": ["export the bazaar repository at the url to the destination location"], "bleu": 0.15587146574232644, "rouge_l": 0.3070469798657718}
{"id": 4302, "code": "def verify signature ( self , key , value , sig ) : return constant time compare ( sig , self . get signature ( key , value ) )", "predictions": ["verify the signature of the constant"], "references": ["verifies the given signature matches the expected signature"], "bleu": 0.18822631894109965, "rouge_l": 0.4178082191780822}
{"id": 4303, "code": "def get signature ( self , value ) : value = want bytes ( value ) key = self . derive key ( ) sig = self . algorithm . get signature ( key , value ) return base64 encode ( sig )", "predictions": ["get base64 signature from the base64 encoded string ."], "references": ["returns the signature for the given value"], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 4304, "code": "def sign ( self , value ) : return value + want bytes ( self . sep ) + self . get signature ( value )", "predictions": ["sign a string using the given value ."], "references": ["signs the given string ."], "bleu": 0.239802967618271, "rouge_l": 0.48157894736842105}
{"id": 4305, "code": "def verify signature ( self , value , sig ) : key = self . derive key ( ) try : sig = base64 decode ( sig ) except Exception : return False return self . algorithm . verify signature ( key , value , sig )", "predictions": ["verify the signature signature"], "references": ["verifies the signature for the given value ."], "bleu": 0.1739594473063345, "rouge_l": 0.31443298969072164}
{"id": 4306, "code": "def unsign ( self , signed value ) : signed value = want bytes ( signed value ) sep = want bytes ( self . sep ) if sep not in signed value : raise Bad Signature ( 'No %r found in value' % self . sep ) value , sig = signed value . rsplit ( sep , 1 ) if self . verify signature ( value , sig ) : return value raise Bad Signature ( 'Signature %r does not match' % sig , payload = value )", "predictions": ["verifies a signed value ."], "references": ["unsigns the given string ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 4307, "code": "def sign ( self , value ) : value = want bytes ( value ) timestamp = base64 encode ( int to bytes ( self . get timestamp ( ) ) ) sep = want bytes ( self . sep ) value = value + sep + timestamp return value + sep + self . get signature ( value )", "predictions": ["sign the value using the base64 encoded string ."], "references": ["signs the given string and also attaches a time information ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 4308, "code": "def all dirs ( base path ) : for root , dirs , files in os . walk ( base path , followlinks = True ) : for dir in dirs : yield os . path . relpath ( os . path . join ( root , dir ) , base path )", "predictions": ["return all directories under base path ."], "references": ["return all dirs in base_path relative to base_path"], "bleu": 0.21191828141393895, "rouge_l": 0.2634989200863931}
{"id": 4309, "code": "def install scripts ( distributions ) : try : from setuptools . command import easy install import pkg resources except Import Error : raise Runtime Error ( \"'wheel install scripts' needs setuptools.\" ) for dist in distributions : pkg resources dist = pkg resources . get distribution ( dist ) install = wheel . paths . get install command ( dist ) command = easy install . easy install ( install . distribution ) command . args = [ 'wheel' ] command . finalize options ( ) command . install egg scripts ( pkg resources dist )", "predictions": ["install scripts scripts in setuptools"], "references": ["regenerate the entry_points console_scripts for the named distribution ."], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 4310, "code": "def get node ( self , ID ) : node = super ( Graph , self ) . get node ( ID ) if node is not None : return node for graph in self . all graphs : for each node in graph . nodes : if each node . ID == ID : return each node else : return None", "predictions": ["return the node from the graph ."], "references": ["returns a node given an id or none if no such node exists ."], "bleu": 0.07562380261607851, "rouge_l": 0.17967599410898377}
{"id": 4311, "code": "def directed changed ( self , new ) : if new : conn = \"->\" else : conn = \"--\" for edge in [ e for g in self . all graphs for e in g . edges ] : edge . conn = conn", "predictions": ["called when a new directed has changed ."], "references": ["sets the connection string for all edges ."], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 4312, "code": "def on edges ( self , object , name , old , new ) : if name == \"edges items\" : edges = new . added elif name == \"edges\" : edges = new else : edges = [ ] all nodes = [ n for g in self . all graphs for n in g . nodes ] for each edge in edges : if each edge . tail node not in all nodes : object . nodes . append ( each edge . tail node ) if each edge . head node not in all nodes : object . nodes . append ( each edge . head node ) each edge . nodes = all nodes", "predictions": ["create edges from old object ."], "references": ["handles the list of edges for any graph changing ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 4313, "code": "def component changed ( self , old , new ) : canvas = self . canvas if old is not None : canvas . remove ( old ) if new is not None : canvas . add ( new )", "predictions": ["changes the component of a component"], "references": ["handles the component being changed ."], "bleu": 0.2907153684841096, "rouge_l": 0.3333333333333333}
{"id": 4314, "code": "def diagram canvas changed ( self , new ) : logger . debug ( \"Diagram canvas changed!\" ) canvas = self . diagram canvas for tool in self . tools : if canvas is not None : print \"Adding tool: %s\" % tool canvas . tools . append ( tool ( canvas ) )", "predictions": ["called when a canvas has changed ."], "references": ["handles the diagram canvas being set"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 4315, "code": "def clear canvas ( self ) : logger . debug ( \"Clearing the diagram canvas!\" ) old canvas = self . diagram canvas new canvas = Canvas ( ) new canvas . copy traits ( old canvas , [ \"bgcolor\" , \"draw axes\" ] ) self . diagram canvas = new canvas self . viewport . component = new canvas self . viewport . request redraw ( ) return", "predictions": ["clear the canvas canvas ."], "references": ["removes all components from the canvas"], "bleu": 0.2941733261715515, "rouge_l": 0.3577712609970674}
{"id": 4316, "code": "def domain model changed for diagram ( self , obj , name , old , new ) : if old is not None : self . unmap model ( old ) if new is not None : self . map model ( new )", "predictions": ["changes the value of a model for the given object ."], "references": ["handles the domain model changing"], "bleu": 0.12605968092174913, "rouge_l": 0.2681318681318681}
{"id": 4317, "code": "def map model ( self , new ) : logger . debug ( \"Mapping the domain model!\" ) dot = Dot ( ) self . diagram . clear canvas ( ) for node mapping in self . nodes : ct = node mapping . containment trait logger . debug ( \"Mapping elements contained by the '%s' trait\" % ct ) if hasattr ( new , ct ) : elements = getattr ( new , ct ) logger . debug ( \"%d element(s) found\" % len ( elements ) ) for element in elements : pydot node = Node ( str ( id ( element ) ) ) dot attrs = node mapping . dot node if dot attrs is not None : self . style node ( pydot node , dot attrs ) dot . add node ( pydot node ) new . on trait change ( self . map element , ct + \" items\" ) logger . debug ( \"Retrieving xdot data and forming pydot graph!\" ) xdot = graph from dot data ( dot . create ( self . program , \"xdot\" ) ) parser = X Dot Parser ( ) for node in xdot . get node list ( ) : diagram node = parser . parse node ( node ) logger . debug ( \"Parsed node [%s] and received diagram node [%s]\" % ( node , diagram node ) ) if diagram node is not None : for node mapping in self . nodes : ct = node mapping . containment trait for element in getattr ( new , ct ) : if str ( id ( element ) ) == diagram node . dot node . get name ( ) : logger . debug ( \"Referencing element [%s] from diagram node [%s]\" % ( element , diagram node ) ) diagram node . element = element break if isinstance ( diagram node . element , node mapping . element ) : for tool in node mapping . tools : logger . debug ( \"Adding tool [%s] to diagram node [%s]\" % ( tool , diagram node ) ) diagram node . tools . append ( tool ( diagram node ) ) else : if diagram node . element is None : logger . warning ( \"Diagram node not referenced to element\" ) self . diagram . diagram canvas . add ( diagram node ) del parser", "predictions": ["copies the model data from pydot to the dot node ."], "references": ["maps a domain model to the diagram"], "bleu": 0.16108992769687397, "rouge_l": 0.3472485768500949}
{"id": 4318, "code": "def unmap model ( self , old ) : for node mapping in self . nodes : ct = node mapping . containment trait if hasattr ( old , ct ) : old elements = getattr ( old , ct ) for old element in old elements : old . on trait change ( self . map element , ct + \" items\" , remove = True )", "predictions": ["unmap the class trait after the class"], "references": ["removes listeners from a domain model"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4319, "code": "def map element ( self , obj , name , event ) : canvas = self . diagram . diagram canvas parser = X Dot Parser ( ) for element in event . added : logger . debug ( \"Mapping new element [%s] to diagram node\" % element ) for node mapping in self . nodes : ct = name [ : - 6 ] #strip ' items' if node mapping . containment trait == ct : dot attrs = node mapping . dot node dot = Dot ( ) graph node = Node ( str ( id ( element ) ) ) self . style node ( graph node , dot attrs ) dot . add node ( graph node ) xdot = graph from dot data ( dot . create ( self . program , \"xdot\" ) ) diagram nodes = parser . parse nodes ( xdot ) #.get node list()) for dn in diagram nodes : if dn is not None : dn . element = element for tool in node mapping . tools : dn . tools . append ( tool ( dn ) ) canvas . add ( dn ) canvas . request redraw ( ) for element in event . removed : logger . debug ( \"Unmapping element [%s] from diagram\" % element ) for component in canvas . components : if element == component . element : canvas . remove ( component ) canvas . request redraw ( ) break", "predictions": ["map a new element to the dot canvas ."], "references": ["handles mapping elements to diagram components"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 4320, "code": "def parse xdot data ( self , data ) : parser = self . parser if data : return parser . parse String ( data ) else : return [ ]", "predictions": ["user - point data"], "references": ["parses xdot data and returns the associated components ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 4321, "code": "def proc font ( self , tokens ) : size = int ( tokens [ \"s\" ] ) self . pen . font = \"%s %d\" % ( tokens [ \"b\" ] , size ) return [ ]", "predictions": ["process url url ."], "references": ["sets the font ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 4322, "code": "def proc ellipse ( self , tokens , filled ) : component = Ellipse ( pen = self . pen , x origin = tokens [ \"x0\" ] , y origin = tokens [ \"y0\" ] , e width = tokens [ \"w\" ] , e height = tokens [ \"h\" ] , filled = filled ) return component", "predictions": ["process a component component component"], "references": ["returns the components of an ellipse ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 4323, "code": "def proc polygon ( self , tokens , filled ) : pts = [ ( p [ \"x\" ] , p [ \"y\" ] ) for p in tokens [ \"points\" ] ] component = Polygon ( pen = self . pen , points = pts , filled = filled ) return component", "predictions": ["process one polygon discriminator ."], "references": ["returns the components of a polygon ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 4324, "code": "def proc polyline ( self , tokens ) : pts = [ ( p [ \"x\" ] , p [ \"y\" ] ) for p in tokens [ \"points\" ] ] component = Polyline ( pen = self . pen , points = pts ) return component", "predictions": ["process one or more backend ."], "references": ["returns the components of a polyline ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4325, "code": "def proc text ( self , tokens ) : component = Text ( pen = self . pen , text x = tokens [ \"x\" ] , text y = tokens [ \"y\" ] , justify = tokens [ \"j\" ] , text w = tokens [ \"w\" ] , text = tokens [ \"b\" ] ) return component", "predictions": ["process one or more list ."], "references": ["returns text components ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4326, "code": "def proc image ( self , tokens ) : print \"IMAGE:\" , tokens , tokens . as List ( ) , tokens . keys ( ) raise Not Implemented Error", "predictions": ["process an image image object try to a string try to a list of content try to the image ."], "references": ["returns the components of an image ."], "bleu": 0.10580331550093845, "rouge_l": 0.2433510638297872}
{"id": 4327, "code": "def render grid file ( context , f ) : f . seek ( 0 ) response = context . response if debug : response . headers [ 'Grid-ID' ] = str ( f . id ) log . debug ( \"Serving Grid FS file.\" , extra = dict ( identifier = str ( f . id ) , filename = f . filename , length = f . length , mimetype = f . content type ) ) response . conditional response = True response . accept ranges = 'bytes' response . content type = f . content type response . content length = f . length response . content md5 = response . etag = f . md5 response . last modified = f . metadata . get ( 'modified' , None ) response . content disposition = 'attachment; filename=' + f . name if context . request . if range . match response ( response ) : response . body file = f else : response . app iter = iter ( f ) return True", "predictions": ["render the grid template template"], "references": ["allow direct use of gridout gridfs file wrappers as endpoint responses ."], "bleu": 0.0566124695559154, "rouge_l": 0.0}
{"id": 4328, "code": "def save ( self , obj ) : fd = None try : fd = open ( self . dot file . absolute path , \"wb\" ) obj . save dot ( fd ) finally : if fd is not None : fd . close ( ) return", "predictions": ["parse an . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["save to file ."], "bleu": 0.03901663112717908, "rouge_l": 0.06637649619151251}
{"id": 4329, "code": "def load ( self ) : fd = None try : obj = parse dot file ( self . dot file . absolute path ) finally : if fd is not None : fd . close ( ) return obj", "predictions": ["is the locally - bus return the object"], "references": ["load the file ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 4330, "code": "def is in ( self , point x , point y ) : x = self . x origin y = self . y origin a = self . e width b = self . e height #/2 return ( ( point x - x ) ** 2 / ( a ** 2 ) ) + ( ( point y - y ) ** 2 / ( b ** 2 ) ) < 1.0", "predictions": ["returns true if the node is in false otherwise"], "references": ["test if the point is within this ellipse"], "bleu": 0.19960198807747329, "rouge_l": 0.35672514619883033}
{"id": 4331, "code": "def draw bounds ( self , gc ) : dx , dy = self . bounds x , y = self . position gc . rect ( x , y , dx , dy ) gc . stroke path ( )", "predictions": ["visit the bounds at the specified gc"], "references": ["draws the component bounds for testing purposes"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 4332, "code": "def perform ( self , event ) : wizard = New Dot Graph Wizard ( parent = self . window . control , window = self . window , title = \"New Graph\" ) if wizard . open ( ) == OK : wizard . finished = True", "predictions": ["populate the finder finder with the content of the req"], "references": ["perform the action ."], "bleu": 0.12605968092174913, "rouge_l": 0.1548223350253807}
{"id": 4333, "code": "def start ( self , context ) : if debug : log . info ( \"Connecting SQL Alchemy database layer.\" , extra = dict ( uri = redact uri ( self . uri ) , config = self . config , alias = self . alias , ) ) engine = self . engine = create engine ( self . uri , * * self . config ) self . Session = scoped session ( sessionmaker ( bind = engine ) ) engine . connect ( ) . close ( ) context . db [ self . alias ] = engine", "predictions": ["export the unpack unpack unpack unpack unpack tempfile tempfile tempfile tempfile tempfile tempfile tempfile tempfile tempfile tempfile tempfile tempfile tempfile tempfile tempfile tempfile tempfile tempfile tempfile tempfile tempfile tempfile tempfile"], "references": ["construct the sqlalchemy engine and session factory ."], "bleu": 0.04034110170120257, "rouge_l": 0.05876685934489403}
{"id": 4334, "code": "def parse dot code fired ( self ) : parser = Godot Data Parser ( ) graph = parser . parse dot data ( self . dot code ) if graph is not None : self . model = graph", "predictions": ["parses the signature of the signature file"], "references": ["parses the dot_code string and replaces the existing model ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 4335, "code": "def new model ( self , info ) : if info . initialized : retval = confirm ( parent = info . ui . control , message = \"Replace existing graph?\" , title = \"New Graph\" , default = YES ) if retval == YES : self . model = Graph ( )", "predictions": ["create a get signature"], "references": ["handles the new graph action ."], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 4336, "code": "def open file ( self , info ) : if not info . initialized : return dlg = File Dialog ( action = \"open\" , wildcard = \"Graphviz Files (*.dot, *.xdot, *.txt)|\" \"*.dot;*.xdot;*.txt|Dot Files (*.dot)|*.dot|\" \"All Files (*.*)|*.*|\" ) if dlg . open ( ) == OK : parser = Godot Data Parser ( ) model = parser . parse dot file ( dlg . path ) if model is not None : self . model = model else : print \"error parsing: %s\" % dlg . path self . save file = dlg . path del dlg", "predictions": ["sign a file to a dot"], "references": ["handles the open action ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4337, "code": "def save ( self , info ) : save file = self . save file if not isfile ( save file ) : self . save as ( info ) else : fd = None try : fd = open ( save file , \"wb\" ) dot code = str ( self . model ) fd . write ( dot code ) finally : if fd is not None : fd . close ( )", "predictions": ["verify the model model"], "references": ["handles saving the current model to the last file ."], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 4338, "code": "def save as ( self , info ) : if not info . initialized : return dlg = File Dialog ( action = \"save as\" , wildcard = \"Graphviz Files (*.dot, *.xdot, *.txt)|\" \"*.dot;*.xdot;*.txt|Dot Files (*.dot)|*.dot|\" \"All Files (*.*)|*.*|\" ) if dlg . open ( ) == OK : fd = None try : fd = open ( dlg . path , \"wb\" ) dot code = str ( self . model ) fd . write ( dot code ) self . save file = dlg . path except : error ( parent = info . ui . control , title = \"Save Error\" , message = \"An error was encountered when saving\\nto %s\" % self . file ) finally : if fd is not None : fd . close ( ) del dlg", "predictions": ["save the model ."], "references": ["handles saving the current model to file ."], "bleu": 0.1571901051328651, "rouge_l": 0.47164948453608246}
{"id": 4339, "code": "def configure graph ( self , info ) : if info . initialized : self . model . edit traits ( parent = info . ui . control , kind = \"live\" , view = attr view )", "predictions": ["sign the graph graph bytes bytes bytes bytes bytes bytes"], "references": ["handles display of the graph dot traits ."], "bleu": 0.16590387014219712, "rouge_l": 0.22676579925650556}
{"id": 4340, "code": "def configure nodes ( self , info ) : if info . initialized : self . model . edit traits ( parent = info . ui . control , kind = \"live\" , view = nodes view )", "predictions": ["all dirs for this in the in - memory in the in - memory in the in - place ."], "references": ["handles display of the nodes editor ."], "bleu": 0.06760229884571738, "rouge_l": 0.1622340425531915}
{"id": 4341, "code": "def configure edges ( self , info ) : if info . initialized : self . model . edit traits ( parent = info . ui . control , kind = \"live\" , view = edges view )", "predictions": ["install scripts for the import import"], "references": ["handles display of the edges editor ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4342, "code": "def about godot ( self , info ) : if info . initialized : self . edit traits ( parent = info . ui . control , kind = \"livemodal\" , view = about view )", "predictions": ["set the get get to the screen super super state super super super super super super super super super super super state super super super super super super super"], "references": ["handles displaying a view about godot ."], "bleu": 0.03511476270817333, "rouge_l": 0.0}
{"id": 4343, "code": "def add node ( self , info ) : if not info . initialized : return graph = self . request graph ( info . ui . control ) if graph is None : return I Ds = [ v . ID for v in graph . nodes ] node = Node ( ID = make unique name ( \"node\" , I Ds ) ) graph . nodes . append ( node ) retval = node . edit traits ( parent = info . ui . control , kind = \"livemodal\" ) if not retval . result : graph . nodes . remove ( node )", "predictions": ["directed a unique changed changed changed = true ."], "references": ["handles adding a node to the graph ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 4344, "code": "def add edge ( self , info ) : if not info . initialized : return graph = self . request graph ( info . ui . control ) if graph is None : return n nodes = len ( graph . nodes ) I Ds = [ v . ID for v in graph . nodes ] if n nodes == 0 : tail node = Node ( ID = make unique name ( \"node\" , I Ds ) ) head name = make unique name ( \"node\" , I Ds + [ tail node . ID ] ) head node = Node ( ID = head name ) elif n nodes == 1 : tail node = graph . nodes [ 0 ] head node = Node ( ID = make unique name ( \"node\" , I Ds ) ) else : tail node = graph . nodes [ 0 ] head node = graph . nodes [ 1 ] edge = Edge ( tail node , head node , nodes = graph . nodes ) retval = edge . edit traits ( parent = info . ui . control , kind = \"livemodal\" ) if retval . result : graph . edges . append ( edge )", "predictions": ["on a unique object for this not yet yet ."], "references": ["handles adding an edge to the graph ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 4345, "code": "def add subgraph ( self , info ) : if not info . initialized : return graph = self . request graph ( info . ui . control ) if graph is not None : subgraph = Subgraph ( ) #root=graph, parent=graph) retval = subgraph . edit traits ( parent = info . ui . control , kind = \"livemodal\" ) if retval . result : graph . subgraphs . append ( subgraph )", "predictions": ["component a changed changed is added to the current is ."], "references": ["handles adding a subgraph to the main graph ."], "bleu": 0.17033186037639278, "rouge_l": 0.4073455759599332}
{"id": 4346, "code": "def add cluster ( self , info ) : if not info . initialized : return graph = self . request graph ( info . ui . control ) if graph is not None : cluster = Cluster ( ) #root=graph, parent=graph) retval = cluster . edit traits ( parent = info . ui . control , kind = \"livemodal\" ) if retval . result : graph . clusters . append ( cluster )", "predictions": ["diagram a canvas to the graph graph"], "references": ["handles adding a cluster to the main graph ."], "bleu": 0.20873176328735715, "rouge_l": 0.48897795591182364}
{"id": 4347, "code": "def godot options ( self , info ) : if info . initialized : self . edit traits ( parent = info . ui . control , kind = \"livemodal\" , view = \"options view\" )", "predictions": ["clear the canvas canvas canvas ."], "references": ["handles display of the options menu ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 4348, "code": "def configure dot code ( self , info ) : if not info . initialized : return self . dot code = str ( self . model ) retval = self . edit traits ( parent = info . ui . control , kind = \"livemodal\" , view = \"dot code view\" )", "predictions": ["domain the model changed"], "references": ["handles display of the dot code in a text editor ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 4349, "code": "def on exit ( self , info ) : if self . prompt on exit : retval = confirm ( parent = info . ui . control , message = \"Exit Godot?\" , title = \"Confirm exit\" , default = YES ) if retval == YES : self . on close ( info ) else : self . on close ( info )", "predictions": ["model is called when the websocket connection is closed ."], "references": ["handles the user attempting to exit godot ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 4350, "code": "def save to file like ( self , flo , format = None , * * kwargs ) : format = self . format if format is None else format save = getattr ( self , \"save %s\" % format , None ) if save is None : raise Value Error ( \"Unknown format '%s'.\" % format ) save ( flo , * * kwargs )", "predictions": ["unmap the image to a file file"], "references": ["save the object to a given file like object in the given format ."], "bleu": 0.10218289380194193, "rouge_l": 0.35935198821796754}
{"id": 4351, "code": "def save to file ( self , filename , format = None , * * kwargs ) : if format is None : format = format from extension ( filename ) with file ( filename , 'wb' ) as fp : self . save to file like ( fp , format , * * kwargs )", "predictions": ["map the object to a file file"], "references": ["save the object to file given by filename ."], "bleu": 0.27470644934024185, "rouge_l": 0.48897795591182364}
{"id": 4352, "code": "def add node ( self , node or ID , * * kwds ) : if not isinstance ( node or ID , Node ) : node ID = str ( node or ID ) if node ID in self . nodes : node = self . nodes [ self . nodes . index ( node ID ) ] else : if self . default node is not None : node = self . default node . clone traits ( copy = \"deep\" ) node . ID = node ID else : node = Node ( node ID ) self . nodes . append ( node ) else : node = node or ID if node in self . nodes : node = self . nodes [ self . nodes . index ( node or ID ) ] else : self . nodes . append ( node ) node . set ( * * kwds ) return node", "predictions": ["add a node to the graph ."], "references": ["adds a node to the graph ."], "bleu": 0.8408964152537145, "rouge_l": 0.8571428571428571}
{"id": 4353, "code": "def delete node ( self , node or ID ) : if isinstance ( node or ID , Node ) : node = node or ID else : node = self . get node ( node or ID ) if node is None : raise Value Error ( \"Node %s does not exists\" % node or ID ) self . nodes . remove ( node )", "predictions": ["delete a node from the graph"], "references": ["removes a node from the graph ."], "bleu": 0.6848940448305334, "rouge_l": 0.7587064676616916}
{"id": 4354, "code": "def get node ( self , ID ) : for node in self . nodes : if node . ID == str ( ID ) : return node return None", "predictions": ["return the node object for the given node ."], "references": ["returns the node with the given id or none ."], "bleu": 0.2187537716852318, "rouge_l": 0.5213675213675214}
{"id": 4355, "code": "def delete edge ( self , tail node or ID , head node or ID ) : if isinstance ( tail node or ID , Node ) : tail node = tail node or ID else : tail node = self . get node ( tail node or ID ) if isinstance ( head node or ID , Node ) : head node = head node or ID else : head node = self . get node ( head node or ID ) if ( tail node is None ) or ( head node is None ) : return None for i , edge in enumerate ( self . edges ) : if ( edge . tail node == tail node ) and ( edge . head node == head node ) : edge = self . edges . pop ( i ) return edge return None", "predictions": ["delete the edge from the graph ."], "references": ["removes an edge from the graph . returns the deleted edge or none ."], "bleu": 0.2601300475114444, "rouge_l": 0.4491899852724595}
{"id": 4356, "code": "def add edge ( self , tail node or ID , head node or ID , * * kwds ) : tail node = self . add node ( tail node or ID ) head node = self . add node ( head node or ID ) if \"directed\" in self . trait names ( ) : directed = self . directed else : directed = False if self . default edge is not None : edge = self . default edge . clone traits ( copy = \"deep\" ) edge . tail node = tail node edge . head node = head node edge . conn = \"->\" if directed else \"--\" edge . set ( * * kwds ) else : edge = Edge ( tail node , head node , directed , * * kwds ) if \"strict\" in self . trait names ( ) : if not self . strict : self . edges . append ( edge ) else : self . edges . append ( edge ) else : self . edges . append ( edge )", "predictions": ["add an edge to the graph ."], "references": ["adds an edge to the graph ."], "bleu": 0.8408964152537145, "rouge_l": 0.8571428571428571}
{"id": 4357, "code": "def add subgraph ( self , subgraph or ID ) : if not isinstance ( subgraph or ID , ( godot . subgraph . Subgraph , godot . cluster . Cluster ) ) : subgraph ID = str ( subgraph or ID ) if subgraph or ID . startswith ( \"cluster\" ) : subgraph = godot . cluster . Cluster ( ID = subgraph ID ) else : subgraph = godot . subgraph . Subgraph ( ID = subgraph ID ) else : subgraph = subgraph or ID subgraph . default node = self . default node subgraph . default edge = self . default edge if isinstance ( subgraph , godot . subgraph . Subgraph ) : self . subgraphs . append ( subgraph ) elif isinstance ( subgraph , godot . cluster . Cluster ) : self . clusters . append ( subgraph ) else : raise return subgraph", "predictions": ["add a subgraph to the subgraph ."], "references": ["adds a subgraph to the graph ."], "bleu": 0.5410822690539396, "rouge_l": 0.7142857142857143}
{"id": 4358, "code": "def program changed ( self , new ) : progs = self . progs if not progs . has key ( prog ) : logger . warning ( 'Graph Viz\\'s executable \"%s\" not found' % prog ) if not os . path . exists ( progs [ prog ] ) or not os . path . isfile ( progs [ prog ] ) : logger . warning ( \"Graph Viz's executable '%s' is not a \" \"file or doesn't exist\" % progs [ prog ] )", "predictions": ["changes the executable of the program ."], "references": ["handles the graphviz layout program selection changing ."], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 4359, "code": "def set node lists ( self , new ) : for edge in self . edges : edge . nodes = self . nodes", "predictions": ["set the node lists of edges ."], "references": ["maintains each edge s list of available nodes ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 4360, "code": "def parse dot file ( filename ) : parser = Godot Data Parser ( ) graph = parser . parse dot file ( filename ) del parser return graph", "predictions": ["parse a dot file into a dot - string"], "references": ["parses a dot file and returns a godot graph ."], "bleu": 0.24855227187657006, "rouge_l": 0.41709401709401706}
{"id": 4361, "code": "def parse dot file ( self , file or filename ) : if isinstance ( file or filename , basestring ) : file = None try : file = open ( file or filename , \"rb\" ) data = file . read ( ) except : print \"Could not open %s.\" % file or filename return None finally : if file is not None : file . close ( ) else : file = file or filename data = file . read ( ) return self . parse dot data ( data )", "predictions": ["read a dot file and return a dot - formatted data"], "references": ["returns a graph given a file or a filename ."], "bleu": 0.1354599427337814, "rouge_l": 0.28818897637795277}
{"id": 4362, "code": "def build top graph ( self , tokens ) : strict = tokens [ 0 ] == 'strict' graphtype = tokens [ 1 ] directed = graphtype == 'digraph' graphname = tokens [ 2 ] graph = Graph ( ID = graphname , strict = strict , directed = directed ) self . graph = self . build graph ( graph , tokens [ 3 ] )", "predictions": ["build a top level graph ."], "references": ["build a godot graph instance from parsed data ."], "bleu": 0.20034704329441452, "rouge_l": 0.5147679324894514}
{"id": 4363, "code": "def build graph ( self , graph , tokens ) : subgraph = None for element in tokens : cmd = element [ 0 ] if cmd == ADD NODE : cmd , nodename , opts = element graph . add node ( nodename , * * opts ) elif cmd == ADD EDGE : cmd , src , dest , opts = element srcport = destport = \"\" if isinstance ( src , tuple ) : srcport = src [ 1 ] src = src [ 0 ] if isinstance ( dest , tuple ) : destport = dest [ 1 ] dest = dest [ 0 ] graph . add edge ( src , dest , tailport = srcport , headport = destport , * * opts ) elif cmd in [ ADD GRAPH TO NODE EDGE , ADD GRAPH TO GRAPH EDGE , ADD NODE TO GRAPH EDGE ] : cmd , src , dest , opts = element srcport = destport = \"\" if isinstance ( src , tuple ) : srcport = src [ 1 ] if isinstance ( dest , tuple ) : destport = dest [ 1 ] if not ( cmd == ADD NODE TO GRAPH EDGE ) : if cmd == ADD GRAPH TO NODE EDGE : src = subgraph else : src = prev subgraph dest = subgraph else : dest = subgraph src is graph = isinstance ( src , ( Subgraph , Cluster ) ) dst is graph = isinstance ( dst , ( Subgraph , Cluster ) ) if src is graph : src nodes = src . nodes else : src nodes = [ src ] if dst is graph : dst nodes = dst . nodes else : dst nodes = [ dst ] for src node in src nodes : for dst node in dst nodes : graph . add edge ( from node = src node , to node = dst node , tailport = srcport , headport = destport , * * kwds ) elif cmd == SET GRAPH ATTR : graph . set ( * * element [ 1 ] ) elif cmd == SET DEF NODE ATTR : graph . default node . set ( * * element [ 1 ] ) elif cmd == SET DEF EDGE ATTR : graph . default edge . set ( * * element [ 1 ] ) elif cmd == SET DEF GRAPH ATTR : graph . default graph . set ( * * element [ 1 ] ) elif cmd == ADD SUBGRAPH : cmd , name , elements = element if subgraph : prev subgraph = subgraph if name . startswith ( \"cluster\" ) : cluster = Cluster ( ID = name ) cluster = self . build graph ( cluster , elements ) graph . add cluster ( cluster ) else : subgraph = Subgraph ( ID = name ) subgraph = self . build graph ( subgraph , elements ) graph . add subgraph ( subgraph ) return graph", "predictions": ["build a graph from command line ."], "references": ["builds a godot graph ."], "bleu": 0.22089591134157885, "rouge_l": 0.5154929577464789}
{"id": 4364, "code": "def format duration ( seconds ) : units , divider = get time units and multiplier ( seconds ) seconds *= divider return \"%.3f %s\" % ( seconds , units )", "predictions": ["format a duration duration ."], "references": ["formats a number of seconds using the best units ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 4365, "code": "def on path ( self , new ) : self . name = basename ( new ) self . graph = self . editor input . load ( )", "predictions": ["load a new editor ."], "references": ["handle the file path changing ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 4366, "code": "def get children ( self , object ) : children = [ ] children . extend ( object . subgraphs ) children . extend ( object . clusters ) children . extend ( object . nodes ) children . extend ( object . edges ) return children", "predictions": ["return the children of an object ."], "references": ["gets the object s children ."], "bleu": 0.23356898886410002, "rouge_l": 0.4680306905370844}
{"id": 4367, "code": "def append child ( self , object , child ) : if isinstance ( child , Subgraph ) : object . subgraphs . append ( child ) elif isinstance ( child , Cluster ) : object . clusters . append ( child ) elif isinstance ( child , Node ) : object . nodes . append ( child ) elif isinstance ( child , Edge ) : object . edges . append ( child ) else : pass", "predictions": ["append a child to the graph"], "references": ["appends a child to the object s children ."], "bleu": 0.37288786399304175, "rouge_l": 0.5147679324894514}
{"id": 4368, "code": "def insert child ( self , object , index , child ) : if isinstance ( child , Subgraph ) : object . subgraphs . insert ( index , child ) elif isinstance ( child , Cluster ) : object . clusters . insert ( index , child ) elif isinstance ( child , Node ) : object . nodes . insert ( index , child ) elif isinstance ( child , Edge ) : object . edges . insert ( index , child ) else : pass", "predictions": ["insert a child into the list"], "references": ["inserts a child into the object s children ."], "bleu": 0.37288786399304175, "rouge_l": 0.5147679324894514}
{"id": 4369, "code": "def delete child ( self , object , index ) : if isinstance ( child , Subgraph ) : object . subgraphs . pop ( index ) elif isinstance ( child , Cluster ) : object . clusters . pop ( index ) elif isinstance ( child , Node ) : object . nodes . pop ( index ) elif isinstance ( child , Edge ) : object . edges . pop ( index ) else : pass", "predictions": ["delete a child from the list ."], "references": ["deletes a child at a specified index from the object s children ."], "bleu": 0.13653323887370866, "rouge_l": 0.474339035769829}
{"id": 4370, "code": "def get label ( self , object ) : label = self . label if label [ : 1 ] == '=' : return label [ 1 : ] label = xgetattr ( object , label , '' ) if self . formatter is None : return label return self . formatter ( object , label )", "predictions": ["return the label for the given object ."], "references": ["gets the label to display for a specified object ."], "bleu": 0.21632118787624222, "rouge_l": 0.5446428571428571}
{"id": 4371, "code": "def set label ( self , object , label ) : label name = self . label if label name [ : 1 ] != '=' : xsetattr ( object , label name , label )", "predictions": ["set the label of the label"], "references": ["sets the label for a specified object ."], "bleu": 0.20830666398386113, "rouge_l": 0.2785388127853881}
{"id": 4372, "code": "def add listeners ( self ) : object = self . value canvas = self . factory . canvas if canvas is not None : for name in canvas . node children : object . on trait change ( self . nodes replaced , name ) object . on trait change ( self . nodes changed , name + \" items\" ) for name in canvas . edge children : object . on trait change ( self . edges replaced , name ) object . on trait change ( self . edges changed , name + \" items\" ) else : raise Value Error ( \"Graph canvas not set for graph editor.\" )", "predictions": ["add listeners for the canvas"], "references": ["adds the event listeners for a specified object ."], "bleu": 0.17348474258688365, "rouge_l": 0.2717149220489978}
{"id": 4373, "code": "def nodes replaced ( self , object , name , old , new ) : self . delete nodes ( old ) self . add nodes ( new )", "predictions": ["add an object to the node ."], "references": ["handles a list of nodes being set ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4374, "code": "def nodes changed ( self , object , name , undefined , event ) : self . delete nodes ( event . removed ) self . add nodes ( event . added )", "predictions": ["called when an object is changed ."], "references": ["handles addition and removal of nodes ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4375, "code": "def delete nodes ( self , features ) : graph = self . graph if graph is not None : for feature in features : graph . delete node ( id ( feature ) ) graph . arrange all ( )", "predictions": ["delete all nodes in the graph ."], "references": ["removes the node corresponding to each item in features ."], "bleu": 0.14390022429682173, "rouge_l": 0.22803738317757008}
{"id": 4376, "code": "def edges replaced ( self , object , name , old , new ) : self . delete edges ( old ) self . add edges ( new )", "predictions": ["add an edges to the graph ."], "references": ["handles a list of edges being set ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 4377, "code": "def edges changed ( self , object , name , undefined , event ) : self . delete edges ( event . removed ) self . add edges ( event . added )", "predictions": ["called when an object is changed ."], "references": ["handles addition and removal of edges ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4378, "code": "def delete edges ( self , features ) : graph = self . graph if graph is not None : for feature in features : for graph edge in self . factory . edges : if feature . class in graph edge . edge for : tail feature = getattr ( feature , graph edge . tail name ) head feature = getattr ( feature , graph edge . head name ) graph . delete edge ( id ( tail feature ) , id ( head feature ) ) graph . arrange all ( )", "predictions": ["delete all edges from the graph ."], "references": ["removes the node corresponding to each item in features ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 4379, "code": "def arrange all ( self ) : import godot . dot data parser import godot . graph graph = godot . graph . Graph ( ID = \"g\" , directed = True ) self . conn = \"->\" graph . edges . append ( self ) xdot data = graph . create ( format = \"xdot\" ) parser = godot . dot data parser . Godot Data Parser ( ) ndata = xdot data . replace ( '\\\\\\n' , '' ) tokens = parser . dotparser . parse String ( ndata ) [ 0 ] for element in tokens [ 3 ] : cmd = element [ 0 ] if cmd == \"add edge\" : cmd , src , dest , opts = element self . set ( * * opts )", "predictions": ["create all data in the current data store"], "references": ["arrange the components of the node using graphviz ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 4380, "code": "def parse xdot directive ( self , name , new ) : parser = Xdot Attr Parser ( ) components = parser . parse xdot data ( new ) x1 = min ( [ c . x for c in components ] ) y1 = min ( [ c . y for c in components ] ) print \"X1/Y1:\" , name , x1 , y1 for c in components : if isinstance ( c , Ellipse ) : component . x origin -= x1 component . y origin -= y1 elif isinstance ( c , ( Polygon , B Spline ) ) : print \"Points:\" , c . points c . points = [ ( t [ 0 ] - x1 , t [ 1 ] - y1 ) for t in c . points ] print \"Points:\" , c . points elif isinstance ( c , Text ) : c . text x , c . text y = c . x - x1 , c . y - y1 container = Container ( auto size = True , position = [ x1 , y1 ] , bgcolor = \"yellow\" ) container . add ( * components ) if name == \" draw \" : self . drawing = container elif name == \" hdraw \" : self . arrowhead drawing = container else : raise", "predictions": ["parse xdot directive ."], "references": ["handles parsing xdot drawing directives ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 4381, "code": "def on drawing ( self , object , name , old , new ) : attrs = [ \"drawing\" , \"arrowhead drawing\" ] others = [ getattr ( self , a ) for a in attrs if ( a != name ) and ( getattr ( self , a ) is not None ) ] x , y = self . component . position print \"POS:\" , x , y , self . component . position abs x = [ d . x + x for d in others ] abs y = [ d . y + y for d in others ] print \"ABS:\" , abs x , abs y x1 = min ( abs x + [ new . x ] ) y1 = min ( abs y + [ new . y ] ) print \"DRAW:\" , new . position new . position = [ new . x - x1 , new . y - y1 ] print \"DRAW:\" , new . position if old is not None : self . component . remove ( old ) if new is not None : self . component . add ( new ) print \"POS NEW:\" , self . component . position self . component . position = [ x1 , y1 ] print \"POS NEW:\" , self . component . position self . component . request redraw ( ) print \"POS NEW:\" , self . component . position", "predictions": ["handle an drawing drawing"], "references": ["handles the containers of drawing components being set ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 4382, "code": "def node factory ( * * row factory kw ) : if \" table editor \" in row factory kw : graph = row factory kw [ \" table editor \" ] . object ID = make unique name ( \"n\" , [ node . ID for node in graph . nodes ] ) del row factory kw [ \" table editor \" ] return godot . node . Node ( ID ) else : return godot . node . Node ( uuid . uuid4 ( ) . hex [ : 6 ] )", "predictions": ["return a unique node for the node ."], "references": ["give new nodes a unique id ."], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 4383, "code": "def edge factory ( * * row factory kw ) : if \" table editor \" in row factory kw : table editor = row factory kw [ \" table editor \" ] graph = table editor . object ID = make unique name ( \"node\" , [ node . ID for node in graph . nodes ] ) n nodes = len ( graph . nodes ) I Ds = [ v . ID for v in graph . nodes ] if n nodes == 0 : tail node = godot . Node ( ID = make unique name ( \"n\" , I Ds ) ) head node = godot . Node ( ID = make unique name ( \"n\" , I Ds ) ) elif n nodes == 1 : tail node = graph . nodes [ 0 ] head node = godot . Node ( ID = make unique name ( \"n\" , I Ds ) ) else : tail node = graph . nodes [ 0 ] head node = graph . nodes [ 1 ] return godot . edge . Edge ( tail node , head node , nodes = graph . nodes ) else : return None", "predictions": ["return a unique edge factory that returns a unique edge ."], "references": ["give new edges a unique id ."], "bleu": 0.16108992769687397, "rouge_l": 0.3472485768500949}
{"id": 4384, "code": "def start ( self , context ) : self . config [ 'alias' ] = self . alias safe config = dict ( self . config ) del safe config [ 'host' ] log . info ( \"Connecting Mongo Engine database layer.\" , extra = dict ( uri = redact uri ( self . config [ 'host' ] ) , config = self . config , ) ) self . connection = connect ( * * self . config )", "predictions": ["add the else else * context *"], "references": ["initialize the database connection ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 4385, "code": "def prepare ( self , context ) : context . db [ self . alias ] = Mongo Engine Proxy ( self . connection )", "predictions": ["delete the database . ."], "references": ["attach this connection s default database to the context using our alias ."], "bleu": 0.06554932163900559, "rouge_l": 0.20573355817875214}
{"id": 4386, "code": "def arrange all ( self ) : import godot . dot data parser import godot . graph graph = godot . graph . Graph ( ID = \"g\" ) graph . add node ( self ) print \"GRAPH DOT:\\n\" , str ( graph ) xdot data = graph . create ( format = \"xdot\" ) print \"XDOT DATA:\\n\" , xdot data parser = godot . dot data parser . Godot Data Parser ( ) flat data = xdot data . replace ( '\\\\\\n' , '' ) tokens = parser . dotparser . parse String ( flat data ) [ 0 ] for element in tokens [ 3 ] : print \"TOK:\" , element cmd = element [ 0 ] if cmd == 'add node' : cmd , nodename , opts = element assert nodename == self . ID print \"OPTIONS:\" , opts self . set ( * * opts )", "predictions": ["create node . get node"], "references": ["arrange the components of the node using graphviz ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 4387, "code": "def parse xdot drawing directive ( self , new ) : components = Xdot Attr Parser ( ) . parse xdot data ( new ) max x = max ( [ c . bounds [ 0 ] for c in components ] + [ 1 ] ) max y = max ( [ c . bounds [ 1 ] for c in components ] + [ 1 ] ) pos x = min ( [ c . x for c in components ] ) pos y = min ( [ c . y for c in components ] ) move to origin ( components ) container = Container ( auto size = True , position = [ pos x - self . pos [ 0 ] , pos y - self . pos [ 1 ] ] , bgcolor = \"blue\" ) container . add ( * components ) self . drawing = container", "predictions": ["delete a edge self self = value"], "references": ["parses the drawing directive updating the node components ."], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 4388, "code": "def drawing changed ( self , old , new ) : if old is not None : self . component . remove ( old ) if new is not None : self . component . add ( new ) w , h = self . component . bounds self . component . position = [ self . pos [ 0 ] - ( w / 2 ) , self . pos [ 1 ] - ( h / 2 ) ] self . component . request redraw ( )", "predictions": ["changes the = value of a add add value"], "references": ["handles the container of drawing components changing ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 4389, "code": "def on position change ( self , new ) : w , h = self . component . bounds self . pos = tuple ( [ new [ 0 ] + ( w / 2 ) , new [ 1 ] + ( h / 2 ) ] )", "predictions": ["update the . ."], "references": ["handles the poition of the component changing ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 4390, "code": "def pos changed ( self , new ) : w , h = self . component . bounds self . component . position = [ new [ 0 ] - ( w / 2 ) , new [ 1 ] - ( h / 2 ) ] self . component . request redraw ( )", "predictions": ["handle changes in dconf"], "references": ["handles the graphviz position attribute changing ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 4391, "code": "def highlight info ( ctx , style ) : click . secho ( \"The following styles are available to choose from:\" , fg = \"green\" ) click . echo ( list ( pygments . styles . get all styles ( ) ) ) click . echo ( ) click . secho ( f'The following CSS for the \"{style}\" style can be customized:' , fg = \"green\" ) click . echo ( pygments . formatters . Html Formatter ( style = style ) . get style defs ( ) )", "predictions": ["print node node node node node for given style for the given style style for the given style style style style for the given style style style for the given style"], "references": ["outputs the css which can be customized for highlighted code"], "bleu": 0.04317900023606586, "rouge_l": 0.10748898678414096}
{"id": 4392, "code": "def draw mainlayer ( self , gc , view bounds = None , mode = \"default\" ) : gc . save state ( ) try : if len ( self . points ) >= 2 : gc . set fill color ( self . pen . fill color ) gc . set stroke color ( self . pen . color ) gc . set line width ( self . pen . line width ) gc . begin path ( ) gc . lines ( self . points ) gc . close path ( ) if self . filled : gc . draw path ( self . inside rule ) else : gc . stroke path ( ) finally : gc . restore state ( )", "predictions": ["parse the pen and close"], "references": ["draws a closed polygon"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 4393, "code": "def is in ( self , point x , point y ) : point array = array ( ( ( point x , point y ) , ) ) vertices = array ( self . points ) winding = self . inside rule == \"winding\" result = points in polygon ( point array , vertices , winding ) return result [ 0 ]", "predictions": ["returns true if the point is dot than the given point"], "references": ["test if a point is within this polygonal region"], "bleu": 0.16108992769687397, "rouge_l": 0.3055091819699499}
{"id": 4394, "code": "def draw mainlayer ( self , gc , view bounds = None , mode = \"default\" ) : if not self . points : return gc . save state ( ) try : gc . set fill color ( self . pen . fill color ) gc . set line width ( self . pen . line width ) gc . set stroke color ( self . pen . color ) gc . begin path ( ) start x , start y = self . points [ 0 ] gc . move to ( start x , start y ) for triple in nsplit ( self . points [ 1 : ] , 3 ) : x1 , y1 = triple [ 0 ] x2 , y2 = triple [ 1 ] end x , end y = triple [ 2 ] gc . curve to ( x1 , y1 , x2 , y2 , end x , end y ) gc . move to ( end x , end y ) gc . stroke path ( ) finally : gc . restore state ( )", "predictions": ["build the top - level top - level top - left top - level top - left left left left"], "references": ["draws the bezier component"], "bleu": 0.06108557268562171, "rouge_l": 0.09472049689440994}
{"id": 4395, "code": "def connect ( self , context ) : if debug : log . info ( \"Connecting \" + self . engine . partition ( ':' ) [ 0 ] + \" database layer.\" , extra = dict ( uri = redact uri ( self . uri , self . protect ) , config = self . config , alias = self . alias , ) ) self . connection = context . db [ self . alias ] = self . connector ( self . uri , * * self . config )", "predictions": ["build connection to the given context ."], "references": ["initialize the database connection ."], "bleu": 0.22089591134157885, "rouge_l": 0.34366197183098596}
{"id": 4396, "code": "def handle event ( self , event , * args , * * kw ) : for engine in self . engines . values ( ) : if hasattr ( engine , event ) : getattr ( engine , event ) ( * args , * * kw )", "predictions": ["format duration duration multiplier"], "references": ["broadcast an event to the database connections registered ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 4397, "code": "def get full page url ( self , page number , scheme = None ) : args = dict ( request . view args , external = True , ) if scheme is not None : args [ ' scheme' ] = scheme if page number != 1 : args [ 'page' ] = page number return url for ( request . endpoint , * * args )", "predictions": ["returns the path to the path to the path to the path"], "references": ["get the full external url for this page optinally with the passed in url scheme"], "bleu": 0.08955242946910898, "rouge_l": 0.14523809523809522}
{"id": 4398, "code": "def render prev next links ( self , scheme = None ) : output = '' if self . has prev : output += '<link rel=\"prev\" href=\"{}\" />\\n' . format ( self . get full page url ( self . prev , scheme = scheme ) ) if self . has next : output += '<link rel=\"next\" href=\"{}\" />\\n' . format ( self . get full page url ( self . next , scheme = scheme ) ) return Markup ( output )", "predictions": ["get the next version of the />\\n' nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes"], "references": ["render the rel = prev and rel = next links to a markup object for injection into a template"], "bleu": 0.04317900023606586, "rouge_l": 0.08361891706648389}
{"id": 4399, "code": "def render seo links ( self , scheme = None ) : out = self . render prev next links ( scheme = scheme ) if self . total pages == 1 : out += self . render canonical link ( scheme = scheme ) return out", "predictions": ["append the link to the pass subgraphs subgraphs subgraphs subgraphs subgraphs subgraphs subgraphs subgraphs subgraphs subgraphs ."], "references": ["render the rel = canonical rel = prev and rel = next links to a markup object for injection into a template"], "bleu": 0.05957492105018649, "rouge_l": 0.10024650780608053}
{"id": 4400, "code": "def content type matches ( candidate , pattern ) : def wildcard compare ( type spec , type pattern ) : return type pattern == '*' or type spec == type pattern return ( wildcard compare ( candidate . content type , pattern . content type ) and wildcard compare ( candidate . content subtype , pattern . content subtype ) )", "predictions": ["compare matches matches matches"], "references": ["is candidate an exact match or sub - type of pattern ?"], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 4401, "code": "def ensure dir ( path ) : try : log . info ( 'Ensuring directory exists: %s' % path ) os . makedirs ( path ) except OS Error : if not os . path . isdir ( path ) : raise", "predictions": ["delete the directory if it exists ."], "references": ["ensure that a needed directory exists creating it if it doesn t"], "bleu": 0.13597602315271134, "rouge_l": 0.30148270181219106}
{"id": 4402, "code": "def list dataset uris ( cls , base uri , config path ) : storage account name = generous parse uri ( base uri ) . netloc blobservice = get blob service ( storage account name , config path ) containers = blobservice . list containers ( include metadata = True ) uri list = [ ] for c in containers : admin metadata = c . metadata uri = cls . generate uri ( admin metadata [ 'name' ] , admin metadata [ 'uuid' ] , base uri ) uri list . append ( uri ) return uri list", "predictions": ["return the list of admin containers"], "references": ["return list containing uris with base uri ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4403, "code": "def list overlay names ( self ) : overlay names = [ ] for blob in self . blobservice . list blobs ( self . uuid , prefix = self . overlays key prefix ) : overlay file = blob . name . rsplit ( '/' , 1 ) [ - 1 ] overlay name , ext = overlay file . split ( '.' ) overlay names . append ( overlay name ) return overlay names", "predictions": ["set the label names names 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1"], "references": ["return list of overlay names ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 4404, "code": "def iter item handles ( self ) : blob generator = self . blobservice . list blobs ( self . uuid , include = 'metadata' ) for blob in blob generator : if 'type' in blob . metadata : if blob . metadata [ 'type' ] == 'item' : handle = blob . metadata [ 'relpath' ] yield handle", "predictions": ["iterate over all is in the listeners canvas canvas canvas canvas canvas canvas canvas canvas canvas canvas canvas canvas canvas canvas canvas canvas canvas canvas canvas"], "references": ["return iterator over item handles ."], "bleu": 0.04668049023095243, "rouge_l": 0.07043879907621246}
{"id": 4405, "code": "def luhn check ( card number ) : sum = 0 num digits = len ( card number ) oddeven = num digits & 1 for count in range ( 0 , num digits ) : digit = int ( card number [ count ] ) if not ( ( count & 1 ) ^ oddeven ) : digit *= 2 if digit > 9 : digit -= 9 sum += digit return ( sum % 10 ) == 0", "predictions": ["replaced a number number"], "references": ["checks to make sure that the card passes a luhn mod - 10 checksum"], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 4406, "code": "def remove namespaces ( root ) : for elem in root . getiterator ( ) : if not hasattr ( elem . tag , 'find' ) : continue i = elem . tag . find ( '}' ) if i >= 0 : elem . tag = elem . tag [ i + 1 : ] objectify . deannotate ( root , cleanup namespaces = True )", "predictions": ["nodes are in the add add add add add them to the self . . . . . . . . . . . . . . . . . ."], "references": ["call this on an lxml . etree document to remove all namespaces"], "bleu": 0.04317900023606586, "rouge_l": 0.05053852526926264}
{"id": 4407, "code": "def merge ( self , new dict ) : actions = new dict . pop ( \"actions\" ) for action in actions : self . add action ( action ) self . dict . update ( new dict )", "predictions": ["delete all graph in this features ."], "references": ["merges a dictionary into the rule object ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4408, "code": "def execute actions ( self , cwd ) : self . execute globals ( cwd ) for action in self . actions : logger . info ( \"executing {}\" . format ( action ) ) p = subprocess . Popen ( action , shell = True , cwd = cwd ) p . wait ( )", "predictions": ["edges all replaced replaced replaced replaced with the given object new ones new replaced new replaced new replaced"], "references": ["iterates over the actions and executes them in order ."], "bleu": 0.06809398432036522, "rouge_l": 0.07530864197530865}
{"id": 4409, "code": "def add details ( self , message ) : msg = message try : from flask import request url = request . url method = request . method endpoint = request . endpoint form dict = dict ( request . form ) for key in form dict : if key . lower ( ) in error reporting obscured fields : form dict [ key ] = '******' elif len ( form dict [ key ] ) == 1 : form dict [ key ] = form dict [ key ] [ 0 ] form = pprint . pformat ( form dict ) . replace ( '\\n' , '\\n          ' ) msg = '%s\\n Request:\\n\\nurl:      %s\\nmethod:   %s\\nendpoint: %s\\nform:     %s\\n' % ( msg , url , method , endpoint , form ) except Exception : traceback . print exc ( ) try : from flask import session from flask . json import JSON Encoder session str = json . dumps ( dict ( * * session ) , indent = 2 , cls = JSON Encoder ) msg = '%s\\n Session:\\n\\n%s\\n' % ( msg , session str ) except Exception : traceback . print exc ( ) return msg", "predictions": ["edges edges from delete add delete"], "references": ["add extra details to the message . separate so that it can be overridden"], "bleu": 0.05822753005110548, "rouge_l": 0.09327217125382263}
{"id": 4410, "code": "def get context ( self , value ) : context = super ( Rendition Aware Struct Block , self ) . get context ( value ) context [ 'image rendition' ] = self . rendition . image rendition or 'original' return context", "predictions": ["add in the class to the class"], "references": ["ensure image_rendition is added to the global context ."], "bleu": 0.18370727471078332, "rouge_l": 0.24448897795591182}
{"id": 4411, "code": "def set ( self , k , v ) : k = k . lstrip ( '/' ) url = '{}/{}' . format ( self . endpoint , k ) r = requests . put ( url , data = str ( v ) ) if r . status code != 200 or r . json ( ) is not True : raise KV Store Error ( 'PUT returned {}' . format ( r . status code ) )", "predictions": ["arrange an ordered graph"], "references": ["add or update a key value pair to the database"], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 4412, "code": "def get ( self , k , wait = False , wait index = False , timeout = '5m' ) : k = k . lstrip ( '/' ) url = '{}/{}' . format ( self . endpoint , k ) params = { } if wait : params [ 'index' ] = wait index params [ 'wait' ] = timeout r = requests . get ( url , params = params ) if r . status code == 404 : raise Key Does Not Exist ( \"Key \" + k + \" does not exist\" ) if r . status code != 200 : raise KV Store Error ( 'GET returned {}' . format ( r . status code ) ) try : return base64 . b64decode ( r . json ( ) [ 0 ] [ 'Value' ] ) except Type Error as e : return \"\"", "predictions": ["do a parse request"], "references": ["get the value of a given key"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4413, "code": "def recurse ( self , k , wait = False , wait index = None , timeout = '5m' ) : k = k . lstrip ( '/' ) url = '{}/{}' . format ( self . endpoint , k ) params = { } params [ 'recurse' ] = 'true' if wait : params [ 'wait' ] = timeout if not wait index : params [ 'index' ] = self . index ( k , recursive = True ) else : params [ 'index' ] = wait index r = requests . get ( url , params = params ) if r . status code == 404 : raise Key Does Not Exist ( \"Key \" + k + \" does not exist\" ) if r . status code != 200 : raise KV Store Error ( 'GET returned {}' . format ( r . status code ) ) entries = { } for e in r . json ( ) : if e [ 'Value' ] : entries [ e [ 'Key' ] ] = base64 . b64decode ( e [ 'Value' ] ) else : entries [ e [ 'Key' ] ] = '' return entries", "predictions": ["on the new new new k and returns the response"], "references": ["recursively get the tree below the given key"], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 4414, "code": "def delete ( self , k , recursive = False ) : k = k . lstrip ( '/' ) url = '{}/{}' . format ( self . endpoint , k ) params = { } if recursive : params [ 'recurse' ] = '' r = requests . delete ( url , params = params ) if r . status code != 200 : raise KV Store Error ( 'DELETE returned {}' . format ( r . status code ) )", "predictions": ["node is the hard endpoint"], "references": ["delete a given key or recursively delete the tree below it"], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 4415, "code": "def add months ( months , timestamp = datetime . datetime . utcnow ( ) ) : month = timestamp . month new month = month + months years = 0 while new month < 1 : new month += 12 years -= 1 while new month > 12 : new month -= 12 years += 1 year = timestamp . year + years try : return datetime . datetime ( year , new month , timestamp . day , timestamp . hour , timestamp . minute , timestamp . second ) except Value Error : if months > 0 : new month += 1 if new month > 12 : new month -= 12 year += 1 return datetime . datetime ( year , new month , 1 , timestamp . hour , timestamp . minute , timestamp . second ) else : new day = calendar . monthrange ( year , new month ) [ 1 ] return datetime . datetime ( year , new month , new day , timestamp . hour , timestamp . minute , timestamp . second )", "predictions": ["edge - time factory for a given row ."], "references": ["add a number of months to a timestamp"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 4416, "code": "def add months to date ( months , date ) : month = date . month new month = month + months years = 0 while new month < 1 : new month += 12 years -= 1 while new month > 12 : new month -= 12 years += 1 year = date . year + years try : return datetime . date ( year , new month , date . day ) except Value Error : if months > 0 : new month += 1 if new month > 12 : new month -= 12 year += 1 return datetime . datetime ( year , new month , 1 ) else : new day = calendar . monthrange ( year , new month ) [ 1 ] return datetime . datetime ( year , new month , new day )", "predictions": ["add months to date"], "references": ["add a number of months to a date"], "bleu": 0.19765609300943976, "rouge_l": 0.6288659793814433}
{"id": 4417, "code": "def is christmas period ( ) : now = datetime . date . today ( ) if now . month != 12 : return False if now . day < 15 : return False if now . day > 27 : return False return True", "predictions": ["returns true if the day is a christmas period"], "references": ["is this the christmas period?"], "bleu": 0.16784459625186196, "rouge_l": 0.3012345679012346}
{"id": 4418, "code": "def from csv ( self , label column = 'labels' ) : df = pd . read csv ( self . path , header = 0 ) X = df . loc [ : , df . columns != label column ] . to dict ( 'records' ) X = map dict list ( X , if func = lambda k , v : v and math . isfinite ( v ) ) y = list ( df [ label column ] . values ) return X , y", "predictions": ["reads the csv and returns a dataframe"], "references": ["read dataset from csv ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 4419, "code": "def from json ( self ) : with gzip . open ( '%s.gz' % self . path , 'rt' ) if self . gz else open ( self . path ) as file : return list ( map ( list , zip ( * json . load ( file ) ) ) ) [ : : - 1 ]", "predictions": ["return the object from a json file"], "references": ["reads dataset from json ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 4420, "code": "def restore data ( self , data dict ) : session [ self . base key ] = data dict self . data dict = session [ self . base key ]", "predictions": ["restore data from data dictionary"], "references": ["restore the data dict - update the flask session and this object"], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 4421, "code": "def verify block ( self , block type , block ) : if block type in self . registry : raise Already Registered ( \"A block has already been registered to the {} `block type` \" \"in the registry. Either unregister that block before trying \" \"to register this block under a different `block type`\" . format ( block type ) ) if not isinstance ( block , Block ) : raise Invalid Block ( \"The block you tried register to {} is invalid. Only \" \"instances of `wagtail.wagtailcore.blocks.Block` may be \" \"registered with the the block registry.\" . format ( block type ) )", "predictions": ["verify that the block is valid ."], "references": ["verifies a block prior to registration ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 4422, "code": "def register block ( self , block type , block ) : self . verify block ( block type , block ) self . registry [ block type ] = block", "predictions": ["register a block on the system ."], "references": ["registers block to block_type in the registry ."], "bleu": 0.19148978368719022, "rouge_l": 0.3952483801295896}
{"id": 4423, "code": "def connect ( self ) : SCOPES = 'https://www.googleapis.com/auth/drive' store = file . Storage ( 'drive credentials.json' ) creds = store . get ( ) if not creds or creds . invalid : try : flow = client . flow from clientsecrets ( 'client secret.json' , SCOPES ) except Invalid Client Secrets Error : log . error ( 'ERROR: Could not find client secret.json in current directory, please obtain it from the API console.' ) return creds = tools . run flow ( flow , store ) self . connection = build ( 'drive' , 'v3' , http = creds . authorize ( Http ( ) ) ) response = self . connection . files ( ) . list ( q = \"name='Music' and mime Type='application/vnd.google-apps.folder' and trashed=false\" ) . execute ( ) try : folder id = response . get ( 'files' , [ ] ) [ 0 ] [ 'id' ] except Index Error : log . warning ( 'Music folder is missing. Creating it.' ) folder metadata = { 'name' : 'Music' , 'mime Type' : 'application/vnd.google-apps.folder' } folder = self . connection . files ( ) . create ( body = folder metadata , fields = 'id' ) . execute ( )", "predictions": ["connect to the client ."], "references": ["creates connection to the google drive api sets the connection attribute to make requests and creates the music folder if it doesn t exist ."], "bleu": 0.007071620464460905, "rouge_l": 0.17853658536585365}
{"id": 4424, "code": "def connect ( self ) : if self . music folder is None : music folder = os . path . join ( os . path . expanduser ( '~' ) , 'Music' ) if not os . path . exists ( music folder ) : os . makedirs ( music folder ) self . music folder = music folder", "predictions": ["connect to the folder ."], "references": ["initializes the connection attribute with the path to the user home folder s music folder and creates it if it doesn t exist ."], "bleu": 0.009132829366636706, "rouge_l": 0.2467138523761375}
{"id": 4425, "code": "def write sky params to file ( self ) : inp file = self . sky file + ' params.txt' lg . info ( 'Writing Inputs to file : ' + inp file ) f = open ( inp file , 'w' ) f . write ( 'verbose= ' + str ( self . verbose ) + '\\n' ) f . write ( 'band count= ' + str ( self . num bands ) + '\\n' ) f . write ( 'band centres data= ' ) f . write ( \",\" . join ( [ str ( wave ) for wave in self . wavelengths ] ) + '\\n' ) f . write ( 'partition= ' + self . partition + '\\n' ) f . write ( 'vn= ' + str ( self . vn ) + '\\n' ) f . write ( 'hn= ' + str ( self . hn ) + '\\n' ) f . write ( 'rdif= ' + str ( self . sky r dif ) + '\\n' ) f . write ( 'theta points= ' ) f . write ( \",\" . join ( [ str ( theta ) for theta in self . theta points ] ) + '\\n' ) f . write ( 'type= ' + self . sky type + '\\n' ) f . write ( 'azimuth= ' + str ( self . sky azimuth ) + '\\n' ) f . write ( 'zenith= ' + str ( self . sky zenith ) + '\\n' ) f . write ( 'sky save fp= ' + inp file . strip ( ' params.txt' ) + '\\n' ) f . write ( 'sky image save fp= ' + self . sky file + '.ppm' + '\\n' ) f . write ( 'sky image size= 256' + '\\n' ) if self . sky type == 'hlideal' : f . write ( 'C= ' + str ( self . sky c ) + '\\n' ) f . write ( 'rdif= ' + str ( self . sky r dif ) + '\\n' ) f . flush ( ) f . close ( )", "predictions": ["write sky params to file ."], "references": ["writes the params to file that skytool_free needs to generate the sky radiance distribution ."], "bleu": 0.10152291541512103, "rouge_l": 0.3536231884057971}
{"id": 4426, "code": "def write surf params to file ( self ) : inp file = self . water surface file + ' params.txt' lg . info ( 'Writing Inputs to file : ' + inp file ) if self . surf state == 'flat' : lg . info ( 'Surface Type is :: flat' ) f = open ( inp file , 'w' ) f . write ( 'verbose= ' + str ( self . verbose ) + '\\n' ) f . write ( 'band count= ' + str ( self . num bands ) + '\\n' ) f . write ( 'band centres data= ' ) f . write ( \",\" . join ( [ str ( wave ) for wave in self . wavelengths ] ) + '\\n' ) f . write ( 'partition= ' + self . partition + '\\n' ) f . write ( 'vn= ' + str ( self . vn ) + '\\n' ) f . write ( 'hn= ' + str ( self . hn ) + '\\n' ) f . write ( 'theta points= ' ) f . write ( \",\" . join ( [ str ( theta ) for theta in self . theta points ] ) + '\\n' ) f . write ( 'type= ' + self . iface type + '\\n' ) f . write ( 'refrac index 0= ' + str ( self . iface 0 ri ) + '\\n' ) f . write ( 'refrac index 1= ' + str ( self . iface 1 ri ) + '\\n' ) f . write ( 'wind speed= ' + str ( self . wind speed ) + '\\n' ) f . write ( 'wind direc= ' + str ( self . wind direc ) + '\\n' ) f . write ( 'crosswind vertices= ' + str ( self . crosswind vertices ) + '\\n' ) f . write ( 'upwind vertices= ' + str ( self . upwind vertices ) + '\\n' ) f . write ( 'surface size= ' + str ( self . surface size ) + '\\n' ) f . write ( 'surface radius=' + str ( self . surface radius ) + '\\n' ) f . write ( 'target size= ' + str ( self . target size ) + '\\n' ) f . write ( 'rays per quad= ' + str ( self . rays per quad ) + '\\n' ) f . write ( 'surface count= ' + str ( self . surface count ) + '\\n' ) f . write ( 'azimuthally average= ' + str ( self . azimuthally average ) + '\\n' ) f . write ( 'surface save fp= ' + inp file . strip ( ' params.txt' ) + '\\n' ) f . flush ( ) f . close ( )", "predictions": ["write the surf parameters to a file ."], "references": ["write the params to file that surftool_free needs to generate the surface facets"], "bleu": 0.1283572790104489, "rouge_l": 0.3652694610778443}
{"id": 4427, "code": "def write phase params to file ( self ) : inp file = os . path . join ( os . path . join ( self . input path , 'phase files' ) , self . phase function file ) + ' params.txt' lg . info ( 'Writing Inputs to file : ' + inp file ) if self . iop type == 'isotropic' or 'isotropic integ' or 'petzold' or 'pure water ' : lg . info ( 'Iop type is :: ' + self . iop type ) f = open ( inp file , 'w' ) f . write ( 'verbose = ' + str ( self . verbose ) + '\\n' ) f . write ( 'band count = ' + str ( self . num bands ) + '\\n' ) f . write ( 'band centres data = ' ) f . write ( \",\" . join ( [ str ( wave ) for wave in self . wavelengths ] ) + '\\n' ) f . write ( 'partition = ' + self . partition + '\\n' ) f . write ( 'vn = ' + str ( self . vn ) + '\\n' ) f . write ( 'hn = ' + str ( self . hn ) + '\\n' ) f . write ( 'theta points = ' ) f . write ( \",\" . join ( [ str ( theta ) for theta in self . theta points ] ) + '\\n' ) f . write ( 'type = ' + self . iop type + '\\n' ) f . write ( 'phase func save fp = ' + inp file . strip ( ' params.txt' ) + '\\n' ) f . flush ( ) f . close ( )", "predictions": ["write the phase params to the file ."], "references": ["write the params to file that surftool_free needs to generate the surface facets"], "bleu": 0.15451666492113134, "rouge_l": 0.45658682634730546}
{"id": 4428, "code": "def update filenames ( self ) : self . sky file = os . path . abspath ( os . path . join ( os . path . join ( self . input path , 'sky files' ) , 'sky ' + self . sky state + ' z' + str ( self . sky zenith ) + ' a' + str ( self . sky azimuth ) + ' ' + str ( self . num bands ) + ' ' + self . ds code ) )", "predictions": ["update filenames of the sky file ."], "references": ["does nothing currently . may not need this method"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 4429, "code": "def string to float list ( string var ) : try : return [ float ( s ) for s in string var . strip ( '[' ) . strip ( ']' ) . split ( ', ' ) ] except : return [ float ( s ) for s in string var . strip ( '[' ) . strip ( ']' ) . split ( ',' ) ]", "predictions": ["convert a string to a float ."], "references": ["pull comma separated string values out of a text file and converts them to float list"], "bleu": 0.06457085856966725, "rouge_l": 0.2436750998668442}
{"id": 4430, "code": "def set handler ( self , signals , handler = signal . SIG DFL ) : for sig in signals : self . log . debug ( \"Creating handler for signal: {0}\" . format ( sig ) ) signal . signal ( sig , handler )", "predictions": ["sets the signal handler ."], "references": ["takes a list of signals and sets a handler for them"], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 4431, "code": "def pseudo handler ( self , signum , frame ) : self . log . warn ( \"Received sigal {0} but system is already busy processing a previous signal, current frame: {1}\" . format ( signum , str ( frame ) ) )", "predictions": ["handle an incoming system handler ."], "references": ["pseudo handler placeholder while signal is beind processed"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 4432, "code": "def default handler ( self , signum , frame ) : self . log . debug ( \"Signal handler called with signal: {0}\" . format ( signum ) ) if signum in self . restart signals : self . set handler ( self . handled signals , self . pseudo handler ) self . cleanup ( ) os . execl ( 'python' , 'python' , * sys . argv ) elif signum in self . abort signals : self . abort ( signum ) elif signum in self . pause signals : self . pause ( signum ) elif signum in self . resume signals : self . resume ( signum ) elif signum in self . status signals : self . status ( signum ) elif signum in self . error signals : self . log . error ( 'Signal handler received error signal from an external process, aborting' ) self . abort ( signum ) else : self . log . error ( \"Unhandled signal received: {0}\" . format ( signum ) ) raise", "predictions": ["handle the default handler ."], "references": ["default handler a generic callback method for signal processing"], "bleu": 0.1614457444314309, "rouge_l": 0.2717149220489978}
{"id": 4433, "code": "def status ( self , signum ) : self . log . debug ( 'Signal handler got status signal' ) new status callbacks = [ ] for status call in self . status callbacks : try : self . log . debug ( \"Calling {0}({1},{2})\" . format ( status call [ 'function' ] . name , status call [ 'args' ] , status call [ 'kwargs' ] ) ) except Attribute Error : self . log . debug ( \"Calling unbound function/method {0}\" . format ( str ( status call ) ) ) apply ( status call [ 'function' ] , status call [ 'args' ] , status call [ 'kwargs' ] ) if status call [ 'persistent' ] : new status callbacks . append ( status call ) self . status callbacks = new status callbacks self . resume ( signum )", "predictions": ["handle the status of the status of the chain ."], "references": ["run all status tasks then run all tasks in the resume queue"], "bleu": 0.11421946507590645, "rouge_l": 0.1788856304985337}
{"id": 4434, "code": "def unreg event ( self , event list , event ) : try : self . log . debug ( \"Removing event {0}({1},{2})\" . format ( event [ 'function' ] . name , event [ 'args' ] , event [ 'kwargs' ] ) ) except Attribute Error : self . log . debug ( \"Removing event {0}\" . format ( str ( event ) ) ) try : event list . remove ( event ) except Value Error : try : self . log . warn ( \"Unable to remove event {0}({1},{2}) , not found in list: {3}\" . format ( event [ 'function' ] . name , event [ 'args' ] , event [ 'kwargs' ] , event list ) ) except Attribute Error : self . log . debug ( \"Unable to remove event {0}\" . format ( str ( event ) ) ) raise Key Error ( 'Unable to unregister the specified event from the signals specified' )", "predictions": ["unreg an event from the event list ."], "references": ["tries to remove a registered event without triggering it"], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 4435, "code": "def sig from partial ( self , inst ) : self . pargl = list ( inst . pargl ) self . kargl = list ( inst . kargl ) self . def argv = inst . def argv . copy ( ) self . var pargs = inst . var pargs self . var kargs = inst . var kargs", "predictions": ["set the values of the object from a partial attribute"], "references": ["extract function signature from an existing partial instance ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 4436, "code": "def vlq2int ( data ) : byte = ord ( data . read ( 1 ) ) value = byte & 0x7F shift = 1 while byte & 0x80 != 0 : byte = ord ( data . read ( 1 ) ) value = ( ( byte & 0x7F ) << shift * 7 ) | value shift += 1 return value", "predictions": ["read a value from a binary string ."], "references": ["read one vlq - encoded integer value from an input data stream ."], "bleu": 0.1283572790104489, "rouge_l": 0.3652694610778443}
{"id": 4437, "code": "def parse header ( self ) : header = Ordered Dict ( ) user data header = self . archive . header [ 'user data header' ] [ 'content' ] if re . search ( r'Star Craft II replay' , user data header ) : user data header = String IO . String IO ( user data header ) user data header . seek ( 30 ) header . update ( read table ( user data header , [ 'release flag' , 'major version' , 'minor version' , 'maintenance version' , 'build number' , 'unknown' , 'unknown' , 'duration' ] ) ) header [ 'version' ] = '%s.%s.%s.%s' % ( header [ 'major version' ] , header [ 'minor version' ] , header [ 'maintenance version' ] , header [ 'build number' ] ) if not header [ 'release flag' ] : header [ 'version' ] += ' (dev)' header [ 'duration' ] /= 16 else : raise Value Error ( \"The given file is not a Star Craft II replay.\" ) return header", "predictions": ["parse the given user header ."], "references": ["parse the user data header portion of the replay ."], "bleu": 0.17749896924055253, "rouge_l": 0.5980392156862745}
{"id": 4438, "code": "def get duration ( self , seconds ) : duration = \"\" minutes , seconds = divmod ( seconds , 60 ) if minutes >= 60 : hours , minutes = divmod ( minutes , 60 ) duration = \"%sh \" % hours duration += \"%sm %ss\" % ( minutes , seconds ) return duration", "predictions": ["get duration in seconds"], "references": ["transform duration into a human - readable form ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 4439, "code": "def print details ( self ) : print 'Map      ' , self . map print 'Duration ' , self . duration print 'Version  ' , self . version print 'Team  Player       Race       Color' print '-----------------------------------' for player in self . players : print '{team:<5} {name:12} {race:10} {color}' . format ( * * player )", "predictions": ["print the details of the players players ."], "references": ["print a summary of the game details ."], "bleu": 0.25098621243978964, "rouge_l": 0.5}
{"id": 4440, "code": "def data ( self ) : self . batch name value = self . ui . batch name value . text ( ) self . saa values = self . ui . saa values . text ( ) self . sza values = self . ui . sza values . text ( ) self . p values = self . ui . p values . text ( ) self . x value = self . ui . x value . text ( ) self . y value = self . ui . y value . text ( ) self . g value = self . ui . g value . text ( ) self . s value = self . ui . s value . text ( ) self . z value = self . ui . z value . text ( ) self . wavelength values = self . ui . wavelength values . text ( ) self . verbose value = self . ui . verbose value . text ( ) self . phytoplankton path = self . ui . phyto path . text ( ) self . bottom path = self . ui . bottom path . text ( ) self . executive path = self . ui . exec path . text ( ) self . nb cpu = self . ui . nb cpu . current Text ( ) self . report parameter value = str ( self . ui . report parameter value . text ( ) )", "predictions": ["return the data from the batch"], "references": ["this function gets back data that the user typed ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 4441, "code": "def search file result ( self ) : if self . ui . tab Widget . current Index ( ) == Tab Widget . NORMAL MODE : self . result file = self . file dialog . get Open File Name ( caption = str ( \"Open Report File\" ) , directory = \"./outputs\" ) if not self . result file == '' : self . ui . show all curves . set Disabled ( False ) self . ui . show grid . set Disabled ( False ) self . data processing ( ) self . display the graphic ( self . num line , self . wavelength , self . data wanted , self . information ) self . authorized display = True", "predictions": ["search for file result result ."], "references": ["this function once the file found display data s file and the graphic associated ."], "bleu": 0.054546736148076896, "rouge_l": 0.17681159420289855}
{"id": 4442, "code": "def write to file ( self ) : bt = Batch File ( self . batch name value , self . p values , self . x value , self . y value , self . g value , self . s value , self . z value , self . wavelength values , self . verbose value , self . phytoplankton path , self . bottom path , self . nb cpu , self . executive path , self . saa values , self . sza values , self . report parameter value ) bt . write batch to file ( str ( self . batch name value + \" batch.txt\" ) )", "predictions": ["write the batch to a file ."], "references": ["this function calls gui_batch . py with inputs values to write the batch file ."], "bleu": 0.13626462819908836, "rouge_l": 0.42657342657342656}
{"id": 4443, "code": "def data processing ( self ) : the file name = str ( self . result file ) the file = open ( the file name , 'r' ) lines = the file . readlines ( ) lines array = [ ] for line in lines : line = line . split ( ',' ) lines array . append ( line ) labels line = lines array [ 0 ] cell labels line = 0 flag = True try : while flag : if \"wave length (nm)\" in labels line [ cell labels line ] : index = labels line . index ( labels line [ cell labels line ] ) flag = False else : cell labels line += 1 except Index Error : raise sys . exit ( \"Warning : There is no value named 'wavelength' in the file used to plot curves. \" \"So, I can't separate data to plot curves and data about tests linking with these curves.\" ) self . information = [ ] data wavelength = [ ] self . num line = 0 for line in lines array : cell line = 0 self . information . append ( [ ] ) data wavelength . append ( [ ] ) while cell line < len ( line ) : if cell line < index : self . information [ self . num line ] . append ( line [ cell line ] ) elif cell line > index : data wavelength [ self . num line ] . append ( line [ cell line ] ) cell line += 1 self . num line += 1 line wavelength = 0 for row data wavelength in data wavelength : row data wavelength = [ float ( item . strip ( '\\n' ) . strip ( '\\\"' ) ) for item in row data wavelength ] data wavelength [ line wavelength ] = row data wavelength line wavelength += 1 self . wavelength = data wavelength [ 0 ] self . data wanted = data wavelength [ 1 : ] the file . close ( )", "predictions": ["handles the data processing processing curves"], "references": ["this function separates data from the file to display curves and will put them in the good arrays ."], "bleu": 0.03009357607781848, "rouge_l": 0.1462829736211031}
{"id": 4444, "code": "def display error message ( self ) : self . ui . error label . set Scaled Contents ( True ) self . ui . error text label . show ( ) self . ui . error text label . set Style Sheet ( 'color: red' )", "predictions": ["display the error message ."], "references": ["this function displays an error message when a wrong value is typed ."], "bleu": 0.07795171967670728, "rouge_l": 0.3086003372681282}
{"id": 4445, "code": "def hide error message ( self ) : self . ui . error label . set Scaled Contents ( False ) self . ui . error text label . hide ( )", "predictions": ["function to hide the error message message ."], "references": ["this function hides the error message when all values are correct ."], "bleu": 0.20034704329441452, "rouge_l": 0.4825949367088607}
{"id": 4446, "code": "def run ( self ) : print ( 'Executing planarrad' ) if self . ui . tab Widget . current Index ( ) == Tab Widget . NORMAL MODE : self . data ( ) self . check values ( ) if self . without error == False : self . display error message ( ) elif self . without error == True : self . is running = True self . hide error message ( ) self . write to file ( ) os . chdir ( './' ) self . progress bar ( ) this dir = os . path . dirname ( os . path . realpath ( file ) ) . rstrip ( 'gui/' ) batch file = os . path . join ( this dir , \"inputs/batch files/\" + str ( self . batch name value ) + \" batch.txt\" ) print ( batch file ) self . p = subprocess . Popen ( [ \"./planarrad.py -i \" + batch file ] , shell = True ) if self . ui . progress Bar . value ( ) == 100 : self . display the graphic ( self . num line , self . wavelength , self . data wanted , self . information )", "predictions": ["run the current batch of the current process ."], "references": ["this function executes planarrad using the batch file ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 4447, "code": "def cancel planarrad ( self ) : if ( self . is running == True ) & ( self . ui . tab Widget . current Index ( ) == Tab Widget . NORMAL MODE ) : cancel = Qt Gui . Q Message Box . question ( self . ui . cancel , 'Cancel Planar Rad' , \"Are you sure to cancel ?\" , Qt Gui . Q Message Box . Yes , Qt Gui . Q Message Box . No ) if cancel == Qt Gui . Q Message Box . Yes : self . is running = False os . kill ( self . p . pid , signal . SIGTERM ) print ( \"Necessary to check if cancel planarrad works well !\" ) self . ui . progress Bar . reset ( ) else : pass", "predictions": ["cancel the planarrad widget ."], "references": ["this function cancels planarrad ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 4448, "code": "def quit ( self ) : if self . is running == True : warning planarrad running = Qt Gui . Q Message Box . warning ( self . ui . quit , 'Warning !' , \"Planar Rad is running. Stop it before quit !\" , Qt Gui . Q Message Box . Ok ) else : quit = Qt Gui . Q Message Box . question ( self . ui . quit , 'Quit Planar Rad' , \"Are you sure to quit ?\" , Qt Gui . Q Message Box . Yes , Qt Gui . Q Message Box . No ) if quit == Qt Gui . Q Message Box . Yes : Qt Gui . q App . quit ( )", "predictions": ["add the interface to the interface . ."], "references": ["this function quits planarrad checking if planarrad is running before ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 4449, "code": "def open log file ( self ) : f = open ( os . path . expanduser ( '~/.planarradpy/log/libplanarradpy.log' ) ) self . ui Log . text Edit . set Plain Text ( str ( f . read ( ) ) ) self . log window . show ( )", "predictions": ["is the root log to a file"], "references": ["the following opens the log file of planarrad ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 4450, "code": "def open documentation ( self ) : window = Window ( ) html = Qt Core . Q Url . from Local File ( os . path . join ( os . getcwd ( ) , './docs/ build/html/index.html' ) ) #open('./docs/ build/html/index.html').read() #window.show() window . view . load ( html ) window . show ( ) window . exec ( )", "predictions": ["from the csv file"], "references": ["the following opens the documentation file ."], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 4451, "code": "def prerequisite actions ( self ) : self . hide error message ( ) self . ui . show all curves . set Disabled ( True ) self . ui . sens . set Disabled ( True ) self . ui . show grid . set Disabled ( True ) pathname = os . path . dirname ( sys . argv [ 0 ] ) path = os . path . abspath ( pathname ) self . verbose value = self . ui . verbose value . set Text ( \"6\" ) self . report parameter value = self . ui . report parameter value . set Text ( \"Rrs\" ) self . ui . progress Bar . reset ( )", "predictions": ["enable or disable the from the from the from the from the from the from the from the from the from the from the from the from the from the from"], "references": ["this function does all required actions at the beginning when we run the gui ."], "bleu": 0.04317900023606586, "rouge_l": 0.09277566539923954}
{"id": 4452, "code": "def click ( self , event ) : if event . button == 3 : if self . ui . tab Widget . current Index ( ) == Tab Widget . NORMAL MODE : self . pos = Qt Gui . Q Cursor ( ) . pos ( ) self . graphic context menu ( self . pos )", "predictions": ["double context context menu"], "references": ["this function intercepts the mouse s right click and its position ."], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 4453, "code": "def mouse move ( self , event ) : if ( self . ui . tab Widget . current Index ( ) == Tab Widget . NORMAL MODE ) : self . pos X = event . xdata self . pos Y = event . ydata self . graphic target ( self . pos X , self . pos Y )", "predictions": ["moves verify cursor cursor to cursor position"], "references": ["the following gets back coordinates of the mouse on the canvas ."], "bleu": 0.07646493705380929, "rouge_l": 0.0}
{"id": 4454, "code": "def graphic target ( self , x , y ) : if self . authorized display == True : try : self . display the graphic ( self . num line , self . wavelength , self . data wanted , self . information ) self . ui . mouse coordinate . set Text ( \"(%0.3f, %0.3f)\" % ( x , y ) ) except : pass", "predictions": ["move register block block"], "references": ["the following update labels about mouse coordinates ."], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 4455, "code": "def sign ( self , privkey ) : if self . v : raise Invalid Signature ( \"already signed\" ) if privkey in ( 0 , '' , '\\x00' * 32 ) : raise Invalid Signature ( \"Zero privkey cannot sign\" ) rawhash = sha3 ( rlp . encode ( self , self . class . exclude ( [ 'v' , 'r' , 's' ] ) ) ) if len ( privkey ) == 64 : privkey = encode privkey ( privkey , 'bin' ) pk = Private Key ( privkey , raw = True ) signature = pk . ecdsa recoverable serialize ( pk . ecdsa sign recoverable ( rawhash , raw = True ) ) signature = signature [ 0 ] + chr ( signature [ 1 ] ) self . v = ord ( signature [ 64 ] ) + 27 self . r = big endian to int ( signature [ 0 : 32 ] ) self . s = big endian to int ( signature [ 32 : 64 ] ) self . sender = None return self", "predictions": ["connect to binary using ecdsa store store store ."], "references": ["sign this with a private key"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4456, "code": "def hash ( self ) : if self . sender is None : raise Missing Signature Error ( ) class Hash Serializable ( rlp . Serializable ) : fields = [ ( field , sedes ) for field , sedes in self . fields if field not in ( 'v' , 'r' , 's' ) ] + [ ( ' sender' , binary ) ] sedes = None return sha3 ( rlp . encode ( self , Hash Serializable ) )", "predictions": ["the connect connect to the music ."], "references": ["signatures are non deterministic"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4457, "code": "def check ( self ) : if not self . is valid : return True test = ( self . has quorum , self . has quorum possible , self . has noquorum ) assert 1 == len ( [ x for x in test if x is not None ] ) return True", "predictions": ["write out if any of the + inp . is available"], "references": ["either invalid or one of quorum noquorum quorumpossible"], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 4458, "code": "def validate votes ( self , validators H , validators prev H ) : assert self . sender def check ( lockset , validators ) : if not lockset . num eligible votes == len ( validators ) : raise Invalid Proposal Error ( 'lockset num eligible votes mismatch' ) for v in lockset : if v . sender not in validators : raise Invalid Proposal Error ( 'invalid signer' ) if self . round lockset : check ( self . round lockset , validators H ) check ( self . signing lockset , validators prev H ) return True", "predictions": ["write surf to signing"], "references": ["set of validators may change between heights"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 4459, "code": "def validate votes ( self , validators H ) : assert self . sender if not self . round lockset . num eligible votes == len ( validators H ) : raise Invalid Proposal Error ( 'round lockset num eligible votes mismatch' ) for v in self . round lockset : if v . sender not in validators H : raise Invalid Proposal Error ( 'invalid signer' )", "predictions": ["write phase phase phase"], "references": ["set of validators may change between heights"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 4460, "code": "def issue funds ( ctx , amount = 'uint256' , rtgs hash = 'bytes32' , returns = STATUS ) : ctx . accounts [ ctx . msg sender ] += amount ctx . issued amounts [ ctx . msg sender ] += amount ctx . Issuance ( ctx . msg sender , rtgs hash , amount ) return OK", "predictions": ["update the update filenames for an update"], "references": ["in the iou fungible the supply is set by issuer who issue funds ."], "bleu": 0.06833381956448398, "rouge_l": 0.08983799705449189}
{"id": 4461, "code": "def last lock ( self ) : rs = list ( self . rounds ) assert len ( rs ) < 2 or rs [ 0 ] > rs [ 1 ] for r in self . rounds : if self . rounds [ r ] . lock is not None : return self . rounds [ r ] . lock", "predictions": ["return of all rounds rounds return none if any"], "references": ["highest lock on height"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4462, "code": "def last voted blockproposal ( self ) : for r in self . rounds : if isinstance ( self . rounds [ r ] . proposal , Block Proposal ) : assert isinstance ( self . rounds [ r ] . lock , Vote ) if self . rounds [ r ] . proposal . blockhash == self . rounds [ r ] . lock . blockhash : return self . rounds [ r ] . proposal", "predictions": ["the set of rounds handler . or none if not found . ."], "references": ["the last block proposal node voted on"], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 4463, "code": "def last valid lockset ( self ) : for r in self . rounds : ls = self . rounds [ r ] . lockset if ls . is valid : return ls return None", "predictions": ["already handler for the pseudo - lockset"], "references": ["highest valid lockset on height"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 4464, "code": "def get timeout ( self ) : if self . timeout time is not None or self . proposal : return now = self . cm . chainservice . now round timeout = Consensus Manager . round timeout round timeout factor = Consensus Manager . round timeout factor delay = round timeout * round timeout factor ** self . round self . timeout time = now + delay return delay", "predictions": ["with the handler for the restart ."], "references": ["setup a timeout for waiting for a proposal"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4465, "code": "def on proposal ( self , proposal , proto ) : assert isinstance ( proto , HDC Protocol ) assert isinstance ( proposal , Proposal ) if proposal . height >= self . cm . height : assert proposal . lockset . is valid self . last active protocol = proto", "predictions": ["called when an try is received ."], "references": ["called to inform about synced peers"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 4466, "code": "def mk privkeys ( num ) : privkeys = [ ] assert num <= num colors for i in range ( num ) : j = 0 while True : k = sha3 ( str ( j ) ) a = privtoaddr ( k ) an = big endian to int ( a ) if an % num colors == i : break j += 1 privkeys . append ( k ) return privkeys", "predictions": ["returns a list of debug debug debug values from a string ."], "references": ["make privkeys that support coloring see utils . cstr"], "bleu": 0.10390302174233558, "rouge_l": 0.09775641025641024}
{"id": 4467, "code": "def delay ( self , sender , receiver , packet , add delay = 0 ) : bw = min ( sender . ul bandwidth , receiver . dl bandwidth ) delay = sender . base latency + receiver . base latency delay += len ( packet ) / bw delay += add delay return delay", "predictions": ["sig a inst argv argv argv argv argv argv argv argv argv argv argv argv argv argv argv argv argv argv argv argv argv argv argv argv argv argv argv argv"], "references": ["bandwidths are inaccurate as we don t account for parallel transfers here"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4468, "code": "def deliver ( self , sender , receiver , packet ) : to = Consensus Manager . round timeout assert to > 0 print \"in slow transport deliver\" super ( Slow Transport , self ) . deliver ( sender , receiver , packet , add delay = to )", "predictions": ["override asyncio to deliver"], "references": ["deliver on edge of timeout_window"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 4469, "code": "def chain nac proxy ( chain , sender , contract address , value = 0 ) : klass = registry [ contract address ] . im self assert issubclass ( klass , Native ABI Contract ) def mk method ( method ) : def m ( s , * args ) : data = abi encode args ( method , args ) block = chain . head candidate output = test call ( block , sender , contract address , data ) if output is not None : return abi decode return vals ( method , output ) return m class cproxy ( object ) : pass for m in klass . abi methods ( ) : setattr ( cproxy , m . func . func name , mk method ( m ) ) return cproxy ( )", "predictions": ["wrapper for header header . . ."], "references": ["create an object which acts as a proxy for the contract on the chain"], "bleu": 0.06833381956448398, "rouge_l": 0.08983799705449189}
{"id": 4470, "code": "def address to native contract class ( self , address ) : assert isinstance ( address , bytes ) and len ( address ) == 20 assert self . is instance address ( address ) nca = self . native contract address prefix + address [ - 4 : ] return self . native contracts [ nca ]", "predictions": ["convert get get get get get get get class class"], "references": ["returns class . _on_msg_unsafe use x . im_self to get class"], "bleu": 0.1613101715871968, "rouge_l": 0.18885448916408668}
{"id": 4471, "code": "def update ( self , data ) : if data not in self . filter : self . filter . append ( data ) if len ( self . filter ) > self . max items : self . filter . pop ( 0 ) return True else : self . filter . append ( self . filter . pop ( 0 ) ) return False", "predictions": ["print a new filter"], "references": ["returns true if unknown"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 4472, "code": "def on receive transactions ( self , proto , transactions ) : log . debug ( '----------------------------------' ) log . debug ( 'remote transactions received' , count = len ( transactions ) , remote id = proto ) def add txs ( ) : for tx in transactions : self . add transaction ( tx , origin = proto ) gevent . spawn ( add txs )", "predictions": ["receive self ui ui ui"], "references": ["receives rlp . decoded serialized"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 4473, "code": "def img from vgg ( x ) : x = x . transpose ( ( 1 , 2 , 0 ) ) x [ : , : , 0 ] += 103.939 x [ : , : , 1 ] += 116.779 x [ : , : , 2 ] += 123.68 x = x [ : , : , : : - 1 ] return x", "predictions": ["create a point from the result of the result of the result ."], "references": ["decondition an image from the vgg16 model ."], "bleu": 0.1350862565735141, "rouge_l": 0.2985318107667211}
{"id": 4474, "code": "def img to vgg ( x ) : x = x [ : , : , : : - 1 ] x [ : , : , 0 ] -= 103.939 x [ : , : , 1 ] -= 116.779 x [ : , : , 2 ] -= 123.68 x = x . transpose ( ( 2 , 0 , 1 ) ) return x", "predictions": ["convert write write write image to file y y ."], "references": ["condition an image for use with the vgg16 model ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 4475, "code": "def get f layer ( self , layer name ) : inputs = [ self . net input ] if self . learning phase is not None : inputs . append ( K . learning phase ( ) ) return K . function ( inputs , [ self . get layer output ( layer name ) ] )", "predictions": ["returns the layer of the layer result of the layer"], "references": ["create a function for the response of a layer ."], "bleu": 0.14991106946711685, "rouge_l": 0.3}
{"id": 4476, "code": "def get layer output ( self , name ) : if not name in self . f layer outputs : layer = self . net . get layer ( name ) self . f layer outputs [ name ] = layer . output return self . f layer outputs [ name ]", "predictions": ["display the message message for the given error . . . . . . . . . . . . . . . . . ."], "references": ["get symbolic output of a layer ."], "bleu": 0.04668049023095243, "rouge_l": 0.06762749445676275}
{"id": 4477, "code": "def get features ( self , x , layers ) : if not layers : return None inputs = [ self . net . input ] if self . learning phase is not None : inputs . append ( self . learning phase ) f = K . function ( inputs , [ self . get layer output ( layer name ) for layer name in layers ] ) feature outputs = f ( [ x ] ) features = dict ( zip ( layers , feature outputs ) ) return features", "predictions": ["returns the list of error layers for the given network x . . ."], "references": ["evaluate layer outputs for x"], "bleu": 0.09782375748961449, "rouge_l": 0.23018867924528305}
{"id": 4478, "code": "def fix compile ( remove flags ) : import distutils . ccompiler def fix compile ( self , sources , output dir = None , macros = None , include dirs = None , debug = 0 , extra preargs = None , extra postargs = None , depends = None ) : for flag in remove flags : if flag in self . compiler so : self . compiler so . remove ( flag ) macros , objects , extra postargs , pp opts , build = self . setup compile ( output dir , macros , include dirs , sources , depends , extra postargs ) cc args = self . get cc args ( pp opts , debug , extra preargs ) for obj in objects : try : src , ext = build [ obj ] except Key Error : continue self . compile ( obj , src , ext , cc args , extra postargs , pp opts ) return objects distutils . ccompiler . C Compiler . compile = fix compile", "predictions": ["add all macros and compile to the list of macros ."], "references": ["monkey - patch compiler to allow for removal of default compiler flags ."], "bleu": 0.11294012253658708, "rouge_l": 0.24629878869448185}
{"id": 4479, "code": "def do table ( self , line ) : if len ( line ) > 0 : if line . strip ( ) . lower ( ) == \"on\" : log . write ( \"Table ON\" ) self . table output = True return elif line . strip ( ) . lower ( ) == \"off\" : log . write ( \"Table OFF\" ) self . table output = False return log . write ( \"Table output: {}\" . format ( \"ON\" if self . table output else \"OFF\" ) )", "predictions": ["print a table table table"], "references": ["display results in table format"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 4480, "code": "def float with multiplier ( string ) : match = re float with multiplier . search ( string ) if not match or not match . group ( 'num' ) : raise Value Error ( 'String \"{}\" is not numeric!' . format ( string ) ) num = float ( match . group ( 'num' ) ) multi = match . group ( 'multi' ) if multi : try : num *= multipliers [ multi ] except Key Error : raise Value Error ( 'Unknown multiplier: {}' . format ( multi ) ) return num", "predictions": ["return a float with a float"], "references": ["convert string with optional k m g t multiplier to float"], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 4481, "code": "def specific gains ( string ) : if not string : return { } gains = { } for gain in string . split ( ',' ) : amp name , value = gain . split ( '=' ) gains [ amp name . strip ( ) ] = float ( value . strip ( ) ) return gains", "predictions": ["convert a string to a list of strings"], "references": ["convert string with gains of individual amplification elements to dict"], "bleu": 0.157044754112095, "rouge_l": 0.3267857142857143}
{"id": 4482, "code": "def device settings ( string ) : if not string : return { } settings = { } for setting in string . split ( ',' ) : setting name , value = setting . split ( '=' ) settings [ setting name . strip ( ) ] = value . strip ( ) return settings", "predictions": ["return a dict of device settings"], "references": ["convert string with soapysdr device settings to dict"], "bleu": 0.2238400777155271, "rouge_l": 0.2785388127853881}
{"id": 4483, "code": "def wrap ( text , indent = '    ' ) : wrapper = textwrap . Text Wrapper ( width = int ( os . environ . get ( 'COLUMNS' , 80 ) ) , initial indent = indent , subsequent indent = indent ) return '\\n' . join ( wrapper . wrap ( text ) )", "predictions": ["wrap text with unicode ."], "references": ["wrap text to terminal width with default indentation"], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 4484, "code": "def detect devices ( soapy args = '' ) : devices = simplesoapy . detect devices ( soapy args , as string = True ) text = [ ] text . append ( 'Detected Soapy SDR devices:' ) if devices : for i , d in enumerate ( devices ) : text . append ( '  {}' . format ( d ) ) else : text . append ( '  No devices found!' ) return ( devices , '\\n' . join ( text ) )", "predictions": ["detect devices on the system ."], "references": ["returns detected soapysdr devices"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4485, "code": "def set center freq ( self , center freq ) : psd state = { 'repeats' : 0 , 'freq array' : self . base freq array + self . lnb lo + center freq , 'pwr array' : None , 'update lock' : threading . Lock ( ) , 'futures' : [ ] , } return psd state", "predictions": ["sets the center of the psd to the given center ."], "references": ["set center frequency and clear averaged psd data"], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 4486, "code": "def result ( self , psd state ) : freq array = numpy . fft . fftshift ( psd state [ 'freq array' ] ) pwr array = numpy . fft . fftshift ( psd state [ 'pwr array' ] ) if self . crop factor : crop bins half = round ( ( self . crop factor * self . bins ) / 2 ) freq array = freq array [ crop bins half : - crop bins half ] pwr array = pwr array [ crop bins half : - crop bins half ] if psd state [ 'repeats' ] > 1 : pwr array = pwr array / psd state [ 'repeats' ] if self . log scale : pwr array = 10 * numpy . log10 ( pwr array ) return ( freq array , pwr array )", "predictions": ["return the result of the crop crop"], "references": ["return freqs and averaged psd for given center frequency"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 4487, "code": "def wait for result ( self , psd state ) : if len ( psd state [ 'futures' ] ) > 1 : concurrent . futures . wait ( psd state [ 'futures' ] ) elif psd state [ 'futures' ] : psd state [ 'futures' ] [ 0 ] . result ( ) return self . result ( psd state )", "predictions": ["wait for a psd to finish ."], "references": ["wait for all psd threads to finish and return result"], "bleu": 0.20958712452883563, "rouge_l": 0.5700934579439253}
{"id": 4488, "code": "def update ( self , psd state , samples array ) : freq array , pwr array = simplespectral . welch ( samples array , self . sample rate , nperseg = self . bins , window = self . fft window , noverlap = self . fft overlap bins , detrend = self . detrend ) if self . remove dc : pwr array [ 0 ] = ( pwr array [ 1 ] + pwr array [ - 1 ] ) / 2 with psd state [ 'update lock' ] : psd state [ 'repeats' ] += 1 if psd state [ 'pwr array' ] is None : psd state [ 'pwr array' ] = pwr array else : psd state [ 'pwr array' ] += pwr array", "predictions": ["updates the state of the psd state ."], "references": ["compute psd from samples and update average for given center frequency"], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 4489, "code": "def read ( self , f ) : magic = f . read ( len ( self . magic ) ) if not magic : return None if magic != self . magic : raise Value Error ( 'Magic bytes not found! Read data: {}' . format ( magic ) ) header = self . header . make ( self . header struct . unpack ( f . read ( self . header struct . size ) ) ) pwr array = numpy . fromstring ( f . read ( header . size ) , dtype = 'float32' ) return ( header , pwr array )", "predictions": ["read an array from the file - like object"], "references": ["read data from file - like object"], "bleu": 0.42728700639623407, "rouge_l": 0.7672955974842767}
{"id": 4490, "code": "def write ( self , f , time start , time stop , start , stop , step , samples , pwr array ) : f . write ( self . magic ) f . write ( self . header struct . pack ( self . version , time start , time stop , start , stop , step , samples , pwr array . nbytes ) ) #pwr array.tofile(f) f . write ( pwr array . tobytes ( ) ) f . flush ( )", "predictions": ["write the data to a file - like object f ."], "references": ["write data to file - like object"], "bleu": 0.37700638045494705, "rouge_l": 0.8102466793168881}
{"id": 4491, "code": "def write ( self , psd data or future , time start , time stop , samples ) : try : f array , pwr array = psd data or future . result ( ) except Attribute Error : f array , pwr array = psd data or future try : step = f array [ 1 ] - f array [ 0 ] self . formatter . write ( self . output , time start . timestamp ( ) , time stop . timestamp ( ) , f array [ 0 ] , f array [ - 1 ] + step , step , samples , pwr array ) except Exception as e : logging . exception ( 'Error writing to output file: {}' . format ( e ) )", "predictions": ["write data to the output file ."], "references": ["write psd of one frequency hop"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 4492, "code": "def write ( self , psd data or future , time start , time stop , samples ) : try : f array , pwr array = psd data or future . result ( ) except Attribute Error : f array , pwr array = psd data or future self . output . write ( ) self . output . write ( . format ( time start ) ) self . output . write ( . format ( time stop ) ) self . output . write ( '#\\n' ) self . output . write ( ) for f , pwr in zip ( f array , pwr array ) : self . output . write ( '{} {}\\n' . format ( f , pwr ) ) self . output . write ( '\\n' ) self . output . flush ( )", "predictions": ["write data to a file or file - like object ."], "references": ["write psd of one frequency hop"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 4493, "code": "def write ( self , psd data or future , time start , time stop , samples ) : try : f array , pwr array = psd data or future . result ( ) except Attribute Error : f array , pwr array = psd data or future try : step = f array [ 1 ] - f array [ 0 ] row = [ time stop . strftime ( '%Y-%m-%d' ) , time stop . strftime ( '%H:%M:%S' ) , f array [ 0 ] , f array [ - 1 ] + step , step , samples ] row += list ( pwr array ) self . output . write ( '{}\\n' . format ( ', ' . join ( str ( x ) for x in row ) ) ) self . output . flush ( ) except Exception as e : logging . exception ( 'Error writing to output file:' )", "predictions": ["write data to the output stream ."], "references": ["write psd of one frequency hop"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 4494, "code": "def time to repeats ( self , bins , integration time ) : return math . ceil ( ( self . device . sample rate * integration time ) / bins )", "predictions": ["convert time to repeats repeats repeats ."], "references": ["convert integration time to number of repeats"], "bleu": 0.2777619034011791, "rouge_l": 0.5714285714285714}
{"id": 4495, "code": "def freq plan ( self , min freq , max freq , bins , overlap = 0 , quiet = False ) : bin size = self . bins to bin size ( bins ) bins crop = round ( ( 1 - overlap ) * bins ) sample rate crop = ( 1 - overlap ) * self . device . sample rate freq range = max freq - min freq hopping = True if freq range >= sample rate crop else False hop size = self . nearest freq ( sample rate crop , bin size ) hops = math . ceil ( freq range / hop size ) if hopping else 1 min center freq = min freq + ( hop size / 2 ) if hopping else min freq + ( freq range / 2 ) max center freq = min center freq + ( ( hops - 1 ) * hop size ) freq list = [ min center freq + ( i * hop size ) for i in range ( hops ) ] if not quiet : logger . info ( 'overlap: {:.5f}' . format ( overlap ) ) logger . info ( 'bin size: {:.2f} Hz' . format ( bin size ) ) logger . info ( 'bins: {}' . format ( bins ) ) logger . info ( 'bins (after crop): {}' . format ( bins crop ) ) logger . info ( 'sample rate: {:.3f} M Hz' . format ( self . device . sample rate / 1e6 ) ) logger . info ( 'sample rate (after crop): {:.3f} M Hz' . format ( sample rate crop / 1e6 ) ) logger . info ( 'freq range: {:.3f} M Hz' . format ( freq range / 1e6 ) ) logger . info ( 'hopping: {}' . format ( 'YES' if hopping else 'NO' ) ) logger . info ( 'hop size: {:.3f} M Hz' . format ( hop size / 1e6 ) ) logger . info ( 'hops: {}' . format ( hops ) ) logger . info ( 'min center freq: {:.3f} M Hz' . format ( min center freq / 1e6 ) ) logger . info ( 'max center freq: {:.3f} M Hz' . format ( max center freq / 1e6 ) ) logger . info ( 'min freq (after crop): {:.3f} M Hz' . format ( ( min center freq - ( hop size / 2 ) ) / 1e6 ) ) logger . info ( 'max freq (after crop): {:.3f} M Hz' . format ( ( max center freq + ( hop size / 2 ) ) / 1e6 ) ) logger . debug ( 'Frequency hops table:' ) logger . debug ( '  {:8s}      {:8s}      {:8s}' . format ( 'Min:' , 'Center:' , 'Max:' ) ) for f in freq list : logger . debug ( '  {:8.3f} M Hz  {:8.3f} M Hz  {:8.3f} M Hz' . format ( ( f - ( self . device . sample rate / 2 ) ) / 1e6 , f / 1e6 , ( f + ( self . device . sample rate / 2 ) ) / 1e6 , ) ) return freq list", "predictions": ["sample plan plan in place"], "references": ["returns list of frequencies for frequency hopping"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 4496, "code": "def create buffer ( self , bins , repeats , base buffer size , max buffer size = 0 ) : samples = bins * repeats buffer repeats = 1 buffer size = math . ceil ( samples / base buffer size ) * base buffer size if not max buffer size : max buffer size = ( 100 * 1024 ** 2 ) / 8 if max buffer size > 0 : max buffer size = math . ceil ( max buffer size / base buffer size ) * base buffer size if buffer size > max buffer size : logger . warning ( 'Required buffer size ({}) will be shrinked to max buffer size ({})!' . format ( buffer size , max buffer size ) ) buffer repeats = math . ceil ( buffer size / max buffer size ) buffer size = max buffer size logger . info ( 'repeats: {}' . format ( repeats ) ) logger . info ( 'samples: {} (time: {:.5f} s)' . format ( samples , samples / self . device . sample rate ) ) if max buffer size > 0 : logger . info ( 'max buffer size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( max buffer size , max buffer size / bins , max buffer size / self . device . sample rate ) ) else : logger . info ( 'max buffer size (samples): UNLIMITED' ) logger . info ( 'buffer size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( buffer size , buffer size / bins , buffer size / self . device . sample rate ) ) logger . info ( 'buffer repeats: {}' . format ( buffer repeats ) ) return ( buffer repeats , zeros ( buffer size , numpy . complex64 ) )", "predictions": ["create a buffer in the device"], "references": ["create buffer for reading samples"], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 4497, "code": "def setup ( self , bins , repeats , base buffer size = 0 , max buffer size = 0 , fft window = 'hann' , fft overlap = 0.5 , crop factor = 0 , log scale = True , remove dc = False , detrend = None , lnb lo = 0 , tune delay = 0 , reset stream = False , max threads = 0 , max queue size = 0 ) : if self . device . is streaming : self . device . stop stream ( ) base buffer = self . device . start stream ( buffer size = base buffer size ) self . bins = bins self . repeats = repeats self . base buffer size = len ( base buffer ) self . max buffer size = max buffer size self . buffer repeats , self . buffer = self . create buffer ( bins , repeats , self . base buffer size , self . max buffer size ) self . tune delay = tune delay self . reset stream = reset stream self . psd = psd . PSD ( bins , self . device . sample rate , fft window = fft window , fft overlap = fft overlap , crop factor = crop factor , log scale = log scale , remove dc = remove dc , detrend = detrend , lnb lo = lnb lo , max threads = max threads , max queue size = max queue size ) self . writer = writer . formats [ self . output format ] ( self . output )", "predictions": ["setup the stream for the device ."], "references": ["prepare samples buffer and start streaming samples from device"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 4498, "code": "def stop ( self ) : if not self . device . is streaming : return self . device . stop stream ( ) self . writer . close ( ) self . bins = None self . repeats = None self . base buffer size = None self . max buffer size = None self . buffer repeats = None self . buffer = None self . tune delay = None self . reset stream = None self . psd = None self . writer = None", "predictions": ["stop the stream ."], "references": ["stop streaming samples from device and delete samples buffer"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 4499, "code": "def psd ( self , freq ) : if not self . device . is streaming : raise Runtime Error ( 'Streaming is not initialized, you must run setup() first!' ) logger . debug ( '  Frequency hop: {:.2f} Hz' . format ( freq ) ) t freq = time . time ( ) if self . device . freq != freq : if self . reset stream : self . device . device . deactivate Stream ( self . device . stream ) self . device . freq = freq if self . reset stream : self . device . device . activate Stream ( self . device . stream ) if self . tune delay : t delay = time . time ( ) while True : self . device . read stream ( ) t delay end = time . time ( ) if t delay end - t delay >= self . tune delay : break logger . debug ( '    Tune delay: {:.3f} s' . format ( t delay end - t delay ) ) else : logger . debug ( '    Same frequency as before, tuning skipped' ) psd state = self . psd . set center freq ( freq ) t freq end = time . time ( ) logger . debug ( '    Tune time: {:.3f} s' . format ( t freq end - t freq ) ) for repeat in range ( self . buffer repeats ) : logger . debug ( '    Repeat: {}' . format ( repeat + 1 ) ) t acq = time . time ( ) acq time start = datetime . datetime . utcnow ( ) self . device . read stream into buffer ( self . buffer ) acq time stop = datetime . datetime . utcnow ( ) t acq end = time . time ( ) logger . debug ( '      Acquisition time: {:.3f} s' . format ( t acq end - t acq ) ) self . psd . update async ( psd state , numpy . copy ( self . buffer ) ) t final = time . time ( ) if shutdown : break psd future = self . psd . result async ( psd state ) logger . debug ( '    Total hop time: {:.3f} s' . format ( t final - t freq ) ) return ( psd future , acq time start , acq time stop )", "predictions": ["run the device on the device ."], "references": ["tune to specified center frequency and compute power spectral density"], "bleu": 0.10175282441454783, "rouge_l": 0.0}
{"id": 4500, "code": "def sweep ( self , min freq , max freq , bins , repeats , runs = 0 , time limit = 0 , overlap = 0 , fft window = 'hann' , fft overlap = 0.5 , crop = False , log scale = True , remove dc = False , detrend = None , lnb lo = 0 , tune delay = 0 , reset stream = False , base buffer size = 0 , max buffer size = 0 , max threads = 0 , max queue size = 0 ) : self . setup ( bins , repeats , base buffer size , max buffer size , fft window = fft window , fft overlap = fft overlap , crop factor = overlap if crop else 0 , log scale = log scale , remove dc = remove dc , detrend = detrend , lnb lo = lnb lo , tune delay = tune delay , reset stream = reset stream , max threads = max threads , max queue size = max queue size ) try : freq list = self . freq plan ( min freq - lnb lo , max freq - lnb lo , bins , overlap ) t start = time . time ( ) run = 0 while not shutdown and ( runs == 0 or run < runs ) : run += 1 t run start = time . time ( ) logger . debug ( 'Run: {}' . format ( run ) ) for freq in freq list : psd future , acq time start , acq time stop = self . psd ( freq ) self . writer . write async ( psd future , acq time start , acq time stop , len ( self . buffer ) * self . buffer repeats ) if shutdown : break write next future = self . writer . write next async ( ) t run = time . time ( ) logger . debug ( '  Total run time: {:.3f} s' . format ( t run - t run start ) ) if time limit and ( time . time ( ) - t start ) >= time limit : logger . info ( 'Time limit of {} s exceeded, completed {} runs' . format ( time limit , run ) ) break write next future . result ( ) logging . debug ( 'Number of USB buffer overflow errors: {}' . format ( self . device . buffer overflow count ) ) logging . debug ( 'PSD worker threads: {}' . format ( self . psd . executor . max workers ) ) logging . debug ( 'Max. PSD queue size: {} / {}' . format ( self . psd . executor . max queue size reached , self . psd . executor . max queue size ) ) logging . debug ( 'Writer worker threads: {}' . format ( self . writer . executor . max workers ) ) logging . debug ( 'Max. Writer queue size: {} / {}' . format ( self . writer . executor . max queue size reached , self . writer . executor . max queue size ) ) finally : self . stop ( ) t stop = time . time ( ) logger . info ( 'Total time: {:.3f} s' . format ( t stop - t start ) )", "predictions": ["sweep the worker to sweep"], "references": ["sweep spectrum using frequency hopping"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 4501, "code": "def run cmake ( arg = \"\" ) : if ds . find executable ( 'cmake' ) is None : print \"C Make  is required to build zql\" print \"Please install cmake version >= 2.8 and re-run setup\" sys . exit ( - 1 ) print \"Configuring zql build with C Make.... \" cmake args = arg try : build dir = op . join ( op . split ( file ) [ 0 ] , 'build' ) dd . mkpath ( build dir ) os . chdir ( \"build\" ) ds . spawn ( [ 'cmake' , '..' ] + cmake args . split ( ) ) ds . spawn ( [ 'make' , 'clean' ] ) ds . spawn ( [ 'make' ] ) os . chdir ( \"..\" ) except ds . Distutils Exec Error : print \"Error while running cmake\" print \"run 'setup.py build --help' for build options\" print \"You may also try editing the settings in C Make Lists.txt file and re-running setup\" sys . exit ( - 1 )", "predictions": ["run cmake - cmake cmake"], "references": ["forcing to run cmake"], "bleu": 0.35930411196308426, "rouge_l": 0.4535315985130111}
{"id": 4502, "code": "def bring gpio interrupt into userspace ( ) : try : with open ( GPIO INTERRUPT DEVICE VALUE ) : return except IO Error : with open ( GPIO EXPORT FILE , 'w' ) as export file : export file . write ( str ( GPIO INTERRUPT PIN ) ) wait until file exists ( GPIO INTERRUPT DEVICE VALUE )", "predictions": ["write gpio interrupt into userspace"], "references": ["bring the interrupt pin on the gpio into linux userspace ."], "bleu": 0.10339832360529193, "rouge_l": 0.35124760076775424}
{"id": 4503, "code": "def gpio interrupts enable ( self ) : try : bring gpio interrupt into userspace ( ) set gpio interrupt edge ( ) except Timeout as e : raise Interrupt Enable Exception ( \"There was an error bringing gpio%d into userspace. %s\" % ( GPIO INTERRUPT PIN , e . message ) )", "predictions": ["enable or disable the gpio interrupt interrupt ."], "references": ["enables gpio interrupts ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 4504, "code": "def has errors ( self , form ) : return any ( [ fieldname error for fieldname error in form . errors . keys ( ) if fieldname error in self ] )", "predictions": ["returns true if any of the form has any errors ."], "references": ["find tab fields listed as invalid"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 4505, "code": "def get form kwargs ( self ) : kwargs = super ( Form Containers Mixin , self ) . get form kwargs ( ) kwargs . update ( { 'pack' : \"foundation-{}\" . format ( self . kwargs . get ( 'foundation version' ) ) } ) return kwargs", "predictions": ["returns the keyword arguments for instantiating the form ."], "references": ["pass template pack argument"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4506, "code": "def publish ( self ) : return self . publish ( self . args , self . server , self . URI )", "predictions": ["publish a message to the pool ."], "references": ["perform http session to transmit defined weather values ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 4507, "code": "def get ( data ) : crc = 0 for byte in array ( 'B' , data ) : crc = ( V Pro CRC . CRC TABLE [ ( crc >> 8 ) ^ byte ] ^ ( ( crc & 0x FF ) << 8 ) ) return crc", "predictions": ["get the crc from input string ."], "references": ["return crc calc value from raw serial data"], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 4508, "code": "def unpack storm date ( date ) : year = ( date & 0x7f ) + 2000 day = ( date >> 7 ) & 0x01f month = ( date >> 12 ) & 0x0f return \"%s-%s-%s\" % ( year , month , day )", "predictions": ["unpack storm date string to storm date"], "references": ["given a packed storm date field unpack and return yyyy - mm - dd string ."], "bleu": 0.07678812443288274, "rouge_l": 0.2436750998668442}
{"id": 4509, "code": "def use rev b archive ( self , records , offset ) : if type ( self . ARCHIVE REV B ) is bool : return self . ARCHIVE REV B data = Archive B Struct . unpack from ( records , offset ) if data [ 'Rec Type' ] == 0 : log . info ( 'detected archive rev. B' ) self . ARCHIVE REV B = True else : log . info ( 'detected archive rev. A' ) self . ARCHIVE REV B = False return self . ARCHIVE REV B", "predictions": ["check whether the archive is b ."], "references": ["return true if weather station returns rev . b archives"], "bleu": 0.13391424795650428, "rouge_l": 0.11401869158878504}
{"id": 4510, "code": "def wakeup ( self ) : log . info ( \"send: WAKEUP\" ) for i in xrange ( 3 ) : self . port . write ( '\\n' ) ack = self . port . read ( len ( self . WAKE ACK ) ) log raw ( 'read' , ack ) if ack == self . WAKE ACK : return raise No Device Exception ( 'Can not access weather station' )", "predictions": ["read the port from the port"], "references": ["issue wakeup command to device to take out of standby mode ."], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 4511, "code": "def dmpaft cmd ( self , time fields ) : records = [ ] tbuf = struct . pack ( '2H' , * time fields ) self . cmd ( 'DMPAFT' ) crc = V Pro CRC . get ( tbuf ) crc = struct . pack ( '>H' , crc ) log raw ( 'send' , tbuf + crc ) self . port . write ( tbuf + crc ) ack = self . port . read ( len ( self . ACK ) ) log raw ( 'read' , ack ) if ack != self . ACK : return raw = self . port . read ( Dmp Struct . size ) log raw ( 'read' , raw ) if not V Pro CRC . verify ( raw ) : log raw ( 'send ESC' , self . ESC ) self . port . write ( self . ESC ) return log raw ( 'send ACK' , self . ACK ) self . port . write ( self . ACK ) dmp = Dmp Struct . unpack ( raw ) log . info ( 'reading %d pages, start offset %d' % ( dmp [ 'Pages' ] , dmp [ 'Offset' ] ) ) for i in xrange ( dmp [ 'Pages' ] ) : raw = self . port . read ( Dmp Page Struct . size ) log raw ( 'read' , raw ) if not V Pro CRC . verify ( raw ) : log raw ( 'send ESC' , self . ESC ) self . port . write ( self . ESC ) return log raw ( 'send ACK' , self . ACK ) self . port . write ( self . ACK ) page = Dmp Page Struct . unpack ( raw ) offset = 0 if i == 0 : offset = dmp [ 'Offset' ] * Archive A Struct . size while offset < Archive A Struct . size * 5 : log . info ( 'page %d, reading record at offset %d' % ( page [ 'Index' ] , offset ) ) if self . use rev b archive ( page [ 'Records' ] , offset ) : a = Archive B Struct . unpack from ( page [ 'Records' ] , offset ) else : a = Archive A Struct . unpack from ( page [ 'Records' ] , offset ) if a [ 'Date Stamp' ] != 0xffff and a [ 'Time Stamp' ] != 0xffff : records . append ( a ) offset += Archive A Struct . size log . info ( 'read all pages' ) return records", "predictions": ["dmpaft command - line arguments ."], "references": ["issue a command to read the archive records after a known time stamp ."], "bleu": 0.06443935473636557, "rouge_l": 0.18654434250764526}
{"id": 4512, "code": "def weather update ( station , pub sites , interval ) : station . parse ( ) if station . fields [ 'Temp Out' ] > 200 : raise No Sensor Exception ( 'Out of range temperature value: %.1f, check sensors' % ( station . fields [ 'Temp Out' ] , ) ) gust , gust dir = Wind Gust . get ( station , interval ) for ps in pub sites : try : ps . set ( pressure = station . fields [ 'Pressure' ] , dewpoint = station . fields [ 'Dew Point' ] , humidity = station . fields [ 'Hum Out' ] , tempf = station . fields [ 'Temp Out' ] , rainin = station . fields [ 'Rain Rate' ] , rainday = station . fields [ 'Rain Day' ] , dateutc = station . fields [ 'Date Stamp Utc' ] , windspeed = station . fields [ 'Wind Speed10Min' ] , winddir = station . fields [ 'Wind Dir' ] , windgust = gust , windgustdir = gust dir , ) ps . publish ( ) except ( Exception ) as e : log . warn ( 'publisher %s: %s' % ( ps . class . name , e ) )", "predictions": ["with the float with the given match ."], "references": ["main execution loop . query weather data and post to online service ."], "bleu": 0.08583768591139128, "rouge_l": 0.09131736526946108}
{"id": 4513, "code": "def init log ( quiet , debug ) : from logging . handlers import Sys Log Handler fmt = logging . Formatter ( os . path . basename ( sys . argv [ 0 ] ) + \".%(name)s %(levelname)s - %(message)s\" ) facility = Sys Log Handler . LOG DAEMON syslog = Sys Log Handler ( address = '/dev/log' , facility = facility ) syslog . set Formatter ( fmt ) log . add Handler ( syslog ) if not quiet : console = logging . Stream Handler ( ) console . set Formatter ( fmt ) log . add Handler ( console ) log . set Level ( logging . INFO ) if debug : log . set Level ( logging . DEBUG )", "predictions": ["initialize { syslog } } } } } } } } }"], "references": ["setup system logging to desired verbosity ."], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 4514, "code": "def get pub services ( opts ) : sites = [ ] for p key in vars ( opts ) . keys ( ) : args = getattr ( opts , p key ) if p key in PUB SERVICES and args : if isinstance ( args , tuple ) : ps = PUB SERVICES [ p key ] ( * args ) else : ps = PUB SERVICES [ p key ] ( args ) sites . append ( ps ) return sites", "predictions": ["return list of not settings"], "references": ["use values in opts data to generate instances of publication services ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 4515, "code": "def get options ( parser ) : pub g = optparse . Option Group ( parser , \"Publication Services\" , , ) pub g . add option ( '-w' , '--wundergound' , nargs = 2 , type = 'string' , dest = 'wug' , help = 'Weather Underground service; WUG=[SID(station ID), PASSWORD]' ) pub g . add option ( '-p' , '--pws' , nargs = 2 , type = 'string' , dest = 'pws' , help = 'PWS service; PWS=[SID(station ID), PASSWORD]' ) pub g . add option ( '-f' , '--file' , nargs = 1 , type = 'string' , dest = 'file' , help = 'Local file; FILE=[FILE NAME]' ) parser . add option group ( pub g ) parser . add option ( '-d' , '--debug' , dest = 'debug' , action = \"store true\" , default = False , help = 'enable verbose debug logging' ) parser . add option ( '-q' , '--quiet' , dest = 'quiet' , action = \"store true\" , default = False , help = 'disable all console logging' ) parser . add option ( '-t' , '--tty' , dest = 'tty' , default = '/dev/tty S0' , help = 'set serial port device [/dev/tty S0]' ) parser . add option ( '-n' , '--interval' , dest = 'interval' , default = 60 , type = 'int' , help = 'polling/update interval in seconds [60]' ) return parser . parse args ( )", "predictions": ["wrap wrapper for wrapper ."], "references": ["read command line options to configure program behavior ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 4516, "code": "def set ( self , * * kw ) : self . args = kw log . debug ( self . args )", "predictions": ["detect the current configuration"], "references": ["store keyword args to be written to output file ."], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 4517, "code": "def publish ( self ) : with open ( self . file name , 'w' ) as fh : for k , v in self . args . iteritems ( ) : buf = String IO . String IO ( ) buf . write ( k ) self . append vals ( buf , v ) fh . write ( buf . getvalue ( ) + '\\n' ) buf . close ( )", "predictions": ["set the psd to a file ."], "references": ["write output file ."], "bleu": 0.24446151121745047, "rouge_l": 0.3824451410658307}
{"id": 4518, "code": "def init app ( self , app ) : if not hasattr ( app , \"extensions\" ) : app . extensions = { } app . extensions [ \"allows\" ] = self @ app . before request def start context ( * a , * * k ) : self . overrides . push ( Override ( ) ) self . additional . push ( Additional ( ) ) @ app . after request def cleanup ( response ) : self . clear all overrides ( ) self . clear all additional ( ) return response", "predictions": ["initialize the app and configure the log numpy numpy numpy extension numpy numpy numpy numpy application numpy extension numpy array numpy array numpy array numpy ."], "references": ["initializes the flask - allows object against the provided application"], "bleu": 0.05551277111446364, "rouge_l": 0.18118811881188118}
{"id": 4519, "code": "def unduplicate field names ( field names ) : res = [ ] for k in field names : if k in res : i = 1 while k + ' ' + str ( i ) in res : i += 1 k += ' ' + str ( i ) res . append ( k ) return res", "predictions": ["return for for wait for for for for for for for for for wait"], "references": ["append a number to duplicate field names to make them unique ."], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 4520, "code": "def get dataframe ( self ) : if pd is None : raise Import Error ( \"Try installing Pandas first.\" ) frame = pd . Data Frame ( self [ : ] , columns = ( self and self . keys ) or [ ] ) return frame", "predictions": ["0 - 1 version of the dataframe"], "references": ["returns a pandas dataframe instance built from the result set ."], "bleu": 0.1160873020151595, "rouge_l": 0.10683012259194395}
{"id": 4521, "code": "def get widgets sorted ( self ) : result = [ ] for widget name , widget in self . get widgets ( ) . items ( ) : result . append ( ( widget name , widget , widget . position ) ) result . sort ( key = lambda x : x [ 2 ] ) return result", "predictions": ["format widgets list of widgets self raise error if any"], "references": ["returns the widgets sorted by position ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 4522, "code": "def unregister widget ( self , widget cls ) : if widget cls . name in self . widgets : del self . widgets [ widget cls ( ) . get name ( ) ]", "predictions": ["remove the underlying widget widget from the underlying widget"], "references": ["unregisters the given widget ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 4523, "code": "def get last update ( self ) : instance , created = models . Dashboard Widget Last Update . objects . get or create ( widget name = self . get name ( ) ) return instance", "predictions": ["returns the last self . . ."], "references": ["gets or creates the last update object for this widget ."], "bleu": 0.14834636222628117, "rouge_l": 0.32049036777583184}
{"id": 4524, "code": "def save setting ( self , setting name , value ) : setting = self . get setting ( setting name ) if setting is None : setting = models . Dashboard Widget Settings . objects . create ( widget name = self . get name ( ) , setting name = setting name , value = value ) setting . value = value setting . save ( ) return setting", "predictions": ["write a setting to the collection"], "references": ["saves the setting value into the database ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4525, "code": "def format axes ( axes , shape ) : if isinstance ( axes , int ) : axes = ( axes , ) elif isinstance ( axes , list ) or hasattr ( axes , ' iter ' ) : axes = tuple ( axes ) if not isinstance ( axes , tuple ) : raise Value Error ( \"axes argument %s in the constructor not specified correctly\" % str ( axes ) ) if min ( axes ) < 0 or max ( axes ) > len ( shape ) - 1 : raise Value Error ( \"invalid key axes %s given shape %s\" % ( str ( axes ) , str ( shape ) ) ) return axes", "predictions": ["write the axes axes to a psd"], "references": ["format target axes given an array shape"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4526, "code": "def wrap ( func , shape , context = None , axis = ( 0 , ) , dtype = None , npartitions = None ) : if isinstance ( shape , int ) : shape = ( shape , ) key shape , value shape = get kv shape ( shape , Construct Spark . format axes ( axis , shape ) ) split = len ( key shape ) rdd = context . parallelize ( list ( product ( * [ arange ( x ) for x in key shape ] ) ) , npartitions ) rdd = rdd . map ( lambda x : ( x , func ( value shape , dtype , order = 'C' ) ) ) return Bolt Array Spark ( rdd , shape = shape , split = split , dtype = dtype )", "predictions": ["time rdd and self . rdd"], "references": ["wrap an existing numpy constructor in a parallelized construction"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 4527, "code": "def first ( self ) : from bolt . local . array import Bolt Array Local rdd = self . rdd if self . ordered else self . rdd . sort By Key ( ) return Bolt Array Local ( rdd . values ( ) . first ( ) )", "predictions": ["- first of this rdd"], "references": ["return the first element of an array"], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 4528, "code": "def zip with index ( rdd ) : starts = [ 0 ] if rdd . get Num Partitions ( ) > 1 : nums = rdd . map Partitions ( lambda it : [ sum ( 1 for in it ) ] ) . collect ( ) count = sum ( nums ) for i in range ( len ( nums ) - 1 ) : starts . append ( starts [ - 1 ] + nums [ i ] ) else : count = rdd . count ( ) def func ( k , it ) : for i , v in enumerate ( it , starts [ k ] ) : yield v , i return count , rdd . map Partitions With Index ( func )", "predictions": ["create an iterator over an rdd of an rdd"], "references": ["alternate version of spark s zipwithindex that eagerly returns count ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 4529, "code": "def wrapped ( f ) : import inspect def extract ( func ) : append = \"\" args = inspect . getargspec ( func ) for i , a in enumerate ( args . args ) : if i < ( len ( args ) - len ( args . defaults ) ) : append += str ( a ) + \", \" else : default = args . defaults [ i - len ( args . defaults ) ] if hasattr ( default , \" name \" ) : default = default . name else : default = str ( default ) append += str ( a ) + \"=\" + default + \", \" append = append [ : - 2 ] + \")\" return append doc = f . doc + \"\\n\" doc += \"    local -> array(\" + extract ( getattr ( Construct Local , f . name ) ) + \"\\n\" doc += \"    spark -> array(\" + extract ( getattr ( Construct Spark , f . name ) ) + \"\\n\" f . doc = doc return f", "predictions": ["decorator to setup a function window window window window window window window window window window window window window window window window"], "references": ["decorator to append routed docstrings"], "bleu": 0.07645949399477267, "rouge_l": 0.17304964539007092}
{"id": 4530, "code": "def plotcdf ( x , xmin , alpha ) : x = sort ( x ) n = len ( x ) xcdf = arange ( n , 0 , - 1 , dtype = 'float' ) / float ( n ) q = x [ x >= xmin ] fcdf = ( q / xmin ) ** ( 1 - alpha ) nc = xcdf [ argmax ( x >= xmin ) ] fcdf norm = nc * fcdf loglog ( x , xcdf ) loglog ( q , fcdf norm )", "predictions": ["stop the point - wise version of the given point self ."], "references": ["plots cdf and powerlaw"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 4531, "code": "def plotpdf ( x , xmin , alpha , nbins = 30 , dolog = False ) : x = sort ( x ) n = len ( x ) if dolog : hb = hist ( x , bins = logspace ( log10 ( min ( x ) ) , log10 ( max ( x ) ) , nbins ) , log = True ) alpha += 1 else : hb = hist ( x , bins = linspace ( ( min ( x ) ) , ( max ( x ) ) , nbins ) ) h , b = hb [ 0 ] , hb [ 1 ] b = b [ 1 : ] q = x [ x >= xmin ] px = ( alpha - 1 ) / xmin * ( q / xmin ) ** ( - alpha ) arg = argmin ( abs ( b - xmin ) ) norm = mean ( h [ b > xmin ] / ( ( alpha - 1 ) / xmin * ( b [ b > xmin ] / xmin ) ** ( - alpha ) ) ) px = px * norm loglog ( q , px ) gca ( ) . set xlim ( min ( x ) , max ( x ) )", "predictions": ["r set the xlim version of the given parameter ."], "references": ["plots pdf and powerlaw ...."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 4532, "code": "def discrete max likelihood arg ( data , xmin , alpharange = ( 1.5 , 3.5 ) , n alpha = 201 ) : likelihoods = discrete likelihood vector ( data , xmin , alpharange = alpharange , n alpha = n alpha ) Largmax = np . argmax ( likelihoods ) return Largmax", "predictions": ["calculate the sweep - self - self - self - self - self - self - self - self . arg"], "references": ["returns the * argument * of the max of the likelihood of the data given an input xmin"], "bleu": 0.05809665204409193, "rouge_l": 0.05200341005967604}
{"id": 4533, "code": "def discrete max likelihood ( data , xmin , alpharange = ( 1.5 , 3.5 ) , n alpha = 201 ) : likelihoods = discrete likelihood vector ( data , xmin , alpharange = alpharange , n alpha = n alpha ) Lmax = np . max ( likelihoods ) return Lmax", "predictions": ["compute the cmake likelihood likelihood likelihood likelihood"], "references": ["returns the * argument * of the max of the likelihood of the data given an input xmin"], "bleu": 0.04270613179243733, "rouge_l": 0.14823815309842042}
{"id": 4534, "code": "def most likely alpha ( data , xmin , alpharange = ( 1.5 , 3.5 ) , n alpha = 201 ) : alpha vector = np . linspace ( alpharange [ 0 ] , alpharange [ 1 ] , n alpha ) return alpha vector [ discrete max likelihood arg ( data , xmin , alpharange = alpharange , n alpha = n alpha ) ]", "predictions": ["compute the bring gpio gpio for the bring model ."], "references": ["return the most likely alpha for the data given an xmin"], "bleu": 0.1613101715871968, "rouge_l": 0.28328173374613}
{"id": 4535, "code": "def plotcdf ( self , x = None , xmin = None , alpha = None , pointcolor = 'k' , dolog = True , zoom = True , pointmarker = '+' , * * kwargs ) : if x is None : x = self . data if xmin is None : xmin = self . xmin if alpha is None : alpha = self . alpha x = np . sort ( x ) n = len ( x ) xcdf = np . arange ( n , 0 , - 1 , dtype = 'float' ) / float ( n ) q = x [ x >= xmin ] fcdf = ( q / xmin ) ** ( 1 - alpha ) nc = xcdf [ argmax ( x >= xmin ) ] fcdf norm = nc * fcdf D location = argmax ( xcdf [ x >= xmin ] - fcdf norm ) pylab . vlines ( q [ D location ] , xcdf [ x >= xmin ] [ D location ] , fcdf norm [ D location ] , color = 'm' , linewidth = 2 , zorder = 2 ) pylab . plot ( [ q [ D location ] ] * 2 , [ xcdf [ x >= xmin ] [ D location ] , fcdf norm [ D location ] ] , color = 'm' , marker = 's' , zorder = 3 ) #plotx = pylab.linspace(q.min(),q.max(),1000) #ploty = (plotx/xmin)**(1-alpha) * nc if dolog : pylab . loglog ( x , xcdf , marker = pointmarker , color = pointcolor , * * kwargs ) pylab . loglog ( q , fcdf norm , 'r' , * * kwargs ) else : pylab . semilogx ( x , xcdf , marker = pointmarker , color = pointcolor , * * kwargs ) pylab . semilogx ( q , fcdf norm , 'r' , * * kwargs ) if zoom : pylab . axis ( [ xmin , x . max ( ) , xcdf . min ( ) , nc ] )", "predictions": ["plot is the pylab of the pylab ."], "references": ["plots cdf and powerlaw"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4536, "code": "def plot lognormal pdf ( self , * * kwargs ) : if not hasattr ( self , 'lognormal dist' ) : return normalized pdf = self . lognormal dist . pdf ( self . data ) / self . lognormal dist . pdf ( self . data ) . max ( ) min Y , max Y = pylab . gca ( ) . get ylim ( ) pylab . plot ( self . data , normalized pdf * max Y , '.' , * * kwargs )", "predictions": ["plots the errors of the errors of the errors"], "references": ["plot the fitted lognormal distribution"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 4537, "code": "def plot lognormal cdf ( self , * * kwargs ) : if not hasattr ( self , 'lognormal dist' ) : return x = np . sort ( self . data ) n = len ( x ) xcdf = np . arange ( n , 0 , - 1 , dtype = 'float' ) / float ( n ) lcdf = self . lognormal dist . sf ( x ) D location = argmax ( xcdf - lcdf ) pylab . vlines ( x [ D location ] , xcdf [ D location ] , lcdf [ D location ] , color = 'm' , linewidth = 2 ) pylab . plot ( x , lcdf , ',' , * * kwargs )", "predictions": ["get the kwargs of the form kwargs"], "references": ["plot the fitted lognormal distribution"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 4538, "code": "def hash sha256 ( self ) : fp plain = hashlib . sha256 ( self . decoded key ) . digest ( ) return ( b\"SHA256:\" + base64 . b64encode ( fp plain ) . replace ( b\"=\" , b\"\" ) ) . decode ( \"utf-8\" )", "predictions": ["return sha256 sha256 sha256"], "references": ["calculate sha256 fingerprint ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 4539, "code": "def hash sha512 ( self ) : fp plain = hashlib . sha512 ( self . decoded key ) . digest ( ) return ( b\"SHA512:\" + base64 . b64encode ( fp plain ) . replace ( b\"=\" , b\"\" ) ) . decode ( \"utf-8\" )", "predictions": ["return sha512 for the get - sha512 get - sha512 get the get - sha512 get - sha512 get - sha512 get - sha512 get - sha512 get - sha512 get"], "references": ["calculates sha512 fingerprint ."], "bleu": 0.03901663112717908, "rouge_l": 0.06637649619151251}
{"id": 4540, "code": "def parse long ( cls , data ) : if sys . version < '3' : ret = long ( 0 ) for byte in data : ret = ( ret << 8 ) + ord ( byte ) else : ret = 0 for byte in data : ret = ( ret << 8 ) + byte return ret", "predictions": ["unpack data to storm bytes"], "references": ["calculate two s complement ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 4541, "code": "def decode key ( cls , pubkey content ) : try : decoded key = base64 . b64decode ( pubkey content . encode ( \"ascii\" ) ) except ( Type Error , binascii . Error ) : raise Malformed Data Error ( \"Unable to decode the key\" ) return decoded key", "predictions": ["use the decoded rev to use for testing ."], "references": ["decode base64 coded part of the key ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 4542, "code": "def parse options ( self , options ) : quote open = False parsed options = { } def parse add single option ( opt ) : \"\"\"Parses and validates a single option, and adds it to parsed options field.\"\"\" if \"=\" in opt : opt name , opt value = opt . split ( \"=\" , 1 ) opt value = opt value . replace ( '\"' , '' ) else : opt name = opt opt value = True if \" \" in opt name or not self . OPTION NAME RE . match ( opt name ) : raise Invalid Option Name Error ( \"%s is not valid option name.\" % opt name ) if self . strict mode : for valid opt name , value required in self . OPTIONS SPEC : if opt name . lower ( ) == valid opt name : if value required and opt value is True : raise Missing Mandatory Option Value Error ( \"%s is missing mandatory value.\" % opt name ) break else : raise Unknown Option Name Error ( \"%s is unrecognized option name.\" % opt name ) if opt name not in parsed options : parsed options [ opt name ] = [ ] parsed options [ opt name ] . append ( opt value ) start of current opt = 0 i = 1 for i , character in enumerate ( options ) : if character == '\"' : quote open = not quote open if quote open : continue if character == \",\" : opt = options [ start of current opt : i ] parse add single option ( opt ) start of current opt = i + 1 if start of current opt + 1 != i : opt = options [ start of current opt : ] parse add single option ( opt ) if quote open : raise Invalid Options Error ( \"Unbalanced quotes.\" ) return parsed options", "predictions": ["wakeup options command line options ."], "references": ["parses ssh options string ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 4543, "code": "def process ssh rsa ( self , data ) : current position , raw e = self . unpack by int ( data , 0 ) current position , raw n = self . unpack by int ( data , current position ) unpacked e = self . parse long ( raw e ) unpacked n = self . parse long ( raw n ) self . rsa = RSA Public Numbers ( unpacked e , unpacked n ) . public key ( default backend ( ) ) self . bits = self . rsa . key size if self . strict mode : min length = self . RSA MIN LENGTH STRICT max length = self . RSA MAX LENGTH STRICT else : min length = self . RSA MIN LENGTH LOOSE max length = self . RSA MAX LENGTH LOOSE if self . bits < min length : raise Too Short Key Error ( \"%s key data can not be shorter than %s bits (was %s)\" % ( self . key type , min length , self . bits ) ) if self . bits > max length : raise Too Long Key Error ( \"%s key data can not be longer than %s bits (was %s)\" % ( self . key type , max length , self . bits ) ) return current position", "predictions": ["process the cmd = value based on the cmd"], "references": ["parses ssh - rsa public keys ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4544, "code": "def process ssh dss ( self , data ) : data fields = { } current position = 0 for item in ( \"p\" , \"q\" , \"g\" , \"y\" ) : current position , value = self . unpack by int ( data , current position ) data fields [ item ] = self . parse long ( value ) q bits = self . bits in number ( data fields [ \"q\" ] ) p bits = self . bits in number ( data fields [ \"p\" ] ) if q bits != self . DSA N LENGTH : raise Invalid Key Error ( \"Incorrect DSA key parameters: bits(p)=%s, q=%s\" % ( self . bits , q bits ) ) if self . strict mode : min length = self . DSA MIN LENGTH STRICT max length = self . DSA MAX LENGTH STRICT else : min length = self . DSA MIN LENGTH LOOSE max length = self . DSA MAX LENGTH LOOSE if p bits < min length : raise Too Short Key Error ( \"%s key can not be shorter than %s bits (was %s)\" % ( self . key type , min length , p bits ) ) if p bits > max length : raise Too Long Key Error ( \"%s key data can not be longer than %s bits (was %s)\" % ( self . key type , max length , p bits ) ) dsa parameters = DSA Parameter Numbers ( data fields [ \"p\" ] , data fields [ \"q\" ] , data fields [ \"g\" ] ) self . dsa = DSA Public Numbers ( data fields [ \"y\" ] , dsa parameters ) . public key ( default backend ( ) ) self . bits = self . dsa . key size return current position", "predictions": ["process ssh dss data"], "references": ["parses ssh - dsa public keys ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4545, "code": "def process ecdsa sha ( self , data ) : current position , curve information = self . unpack by int ( data , 0 ) if curve information not in self . ECDSA CURVE DATA : raise Not Implemented Error ( \"Invalid curve type: %s\" % curve information ) curve , hash algorithm = self . ECDSA CURVE DATA [ curve information ] current position , key data = self . unpack by int ( data , current position ) try : ecdsa key = ecdsa . Verifying Key . from string ( key data [ 1 : ] , curve , hash algorithm ) except Assertion Error : raise Invalid Key Error ( \"Invalid ecdsa key\" ) self . bits = int ( curve information . replace ( b\"nistp\" , b\"\" ) ) self . ecdsa = ecdsa key return current position", "predictions": ["process ecdsa sha sha ."], "references": ["parses ecdsa - sha public keys ."], "bleu": 0.21763141204756337, "rouge_l": 0.48541114058355433}
{"id": 4546, "code": "def main ( properties = properties , options = options , * * custom options ) : return init ( * * dict ( options , * * custom options ) ) ( * * properties )", "predictions": ["main function for cli ."], "references": ["imports and runs setup function with given properties ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 4547, "code": "def create file ( ) : f = wave . open ( 'audio.wav' , mode = 'wb' ) f . setnchannels ( 2 ) p = pyaudio . Py Audio ( ) f . setsampwidth ( p . get sample size ( pyaudio . pa Int16 ) ) f . setframerate ( p . get default input device info ( ) [ 'default Sample Rate' ] ) try : yield f finally : f . close ( )", "predictions": ["create a new wave file ."], "references": ["returns a file handle which is used to record audio"], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 4548, "code": "def djfrontend jquery datatables css ( version = None ) : if version is None : if not getattr ( settings , 'DJFRONTEND JQUERY DATATABLES CSS' , False ) : version = getattr ( settings , 'DJFRONTEND JQUERY DATATABLES VERSION' , DJFRONTEND JQUERY DATATABLES VERSION DEFAULT ) else : version = getattr ( settings , 'DJFRONTEND JQUERY DATATABLES CSS' , DJFRONTEND JQUERY DATATABLES VERSION DEFAULT ) return format html ( '<link rel=\"stylesheet\" href=\"{static}djfrontend/css/jquery/jquery.data Tables/{v}/jquery.data Tables{min}.css\">' , static = static url , v = version , min = min )", "predictions": ["generate html datatables css css css css ."], "references": ["returns the jquery datatables css file according to version number ."], "bleu": 0.15587146574232644, "rouge_l": 0.3070469798657718}
{"id": 4549, "code": "def djfrontend jquery datatables themeroller ( version = None ) : if version is None : if not getattr ( settings , 'DJFRONTEND JQUERY DATATABLES THEMEROLLER' , False ) : version = getattr ( settings , 'DJFRONTEND JQUERY DATATABLES VERSION' , DJFRONTEND JQUERY DATATABLES VERSION DEFAULT ) else : version = getattr ( settings , 'DJFRONTEND JQUERY DATATABLES THEMEROLLER' , DJFRONTEND JQUERY DATATABLES VERSION DEFAULT ) return format html ( '<link rel=\"stylesheet\" href=\"href=\"{static}djfrontend/css/jquery/jquery.data Tables/{v}/jquery.data Tables themeroller.min.css\">' , static = static url , v = version )", "predictions": ["generate html to themeroller with themeroller"], "references": ["returns the jquery datatables themeroller css file according to version number ."], "bleu": 0.08993236413460196, "rouge_l": 0.10481099656357389}
{"id": 4550, "code": "def calc expiry time ( minutes valid ) : return ( timezone . now ( ) + datetime . timedelta ( minutes = minutes valid + 1 ) ) . replace ( second = 0 , microsecond = 0 )", "predictions": ["calculate the expiry time of a valid minutes time"], "references": ["return specific time an auth_hash will expire ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 4551, "code": "def get user token ( user , purpose , minutes valid ) : token = '' . join ( dumps ( [ user . get username ( ) , get auth hash ( user , purpose ) , ] ) . encode ( 'base64' ) . split ( '\\n' ) ) return { 'id' : get meteor id ( user ) , 'token' : token , 'token Expires' : calc expiry time ( minutes valid ) , }", "predictions": ["get authentication token from user ."], "references": ["return login token info for given user ."], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 4552, "code": "def serialize ( self , obj , * args , * * kwargs ) : data = super ( Users , self ) . serialize ( obj , * args , * * kwargs ) profile = data . pop ( 'fields' ) profile . setdefault ( 'name' , obj . get full name ( ) ) fields = data [ 'fields' ] = { 'username' : obj . get username ( ) , 'emails' : [ ] , 'profile' : profile , 'permissions' : sorted ( self . model . get all permissions ( obj ) ) , } for sensitive in [ 'password' , 'user permissions ids' , 'is active' , 'is staff' , 'is superuser' , 'groups ids' , ] : profile . pop ( sensitive , None ) try : fields [ 'created At' ] = profile . pop ( 'date joined' ) except Key Error : date joined = getattr ( obj , 'get date joined' , lambda : getattr ( obj , 'date joined' , None ) ) ( ) if date joined : fields [ 'created At' ] = date joined try : email = profile . pop ( 'email' ) except Key Error : email = getattr ( obj , 'get email' , lambda : getattr ( obj , 'email' , None ) ) ( ) if email : fields [ 'emails' ] . append ( { 'address' : email , 'verified' : True } ) return data", "predictions": ["serialize the email to a list of email ."], "references": ["serialize user as per meteor accounts serialization ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 4553, "code": "def deserialize profile ( profile , key prefix = '' , pop = False ) : result = { } if pop : getter = profile . pop else : getter = profile . get def prefixed ( name ) : \"\"\"Return name prefixed by `key prefix`.\"\"\" return '%s%s' % ( key prefix , name ) for key in profile . keys ( ) : val = getter ( key ) if key == prefixed ( 'name' ) : result [ 'full name' ] = val else : raise Meteor Error ( 400 , 'Bad profile key: %r' % key ) return result", "predictions": ["deserializes a profile into a dictionary ."], "references": ["de - serialize user profile fields into concrete model fields ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 4554, "code": "def update ( self , selector , update , options = None ) : del options user = get object ( self . model , selector [ ' id' ] , pk = this . user id , ) profile update = self . deserialize profile ( update [ '$set' ] , key prefix = 'profile.' , pop = True , ) if len ( update [ '$set' ] ) != 0 : raise Meteor Error ( 400 , 'Invalid update fields: %r' ) for key , val in profile update . items ( ) : setattr ( user , key , val ) user . save ( )", "predictions": ["update an existing user ."], "references": ["update user data ."], "bleu": 0.32466791547509893, "rouge_l": 0.6802973977695167}
{"id": 4555, "code": "def auth failed ( * * credentials ) : if credentials : user login failed . send robust ( sender = name , credentials = auth . clean credentials ( credentials ) , ) raise Meteor Error ( 403 , 'Authentication failed.' )", "predictions": ["authenticate against the failed user ."], "references": ["consistent fail so we don t provide attackers with valuable info ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 4556, "code": "def validated user ( cls , token , purpose , minutes valid ) : try : username , auth hash = loads ( token . decode ( 'base64' ) ) except ( Value Error , Error ) : cls . auth failed ( token = token ) try : user = cls . user model . objects . get ( * * { cls . user model . USERNAME FIELD : username , 'is active' : True , } ) user . backend = 'django.contrib.auth.backends.Model Backend' except cls . user model . Does Not Exist : cls . auth failed ( username = username , token = token ) if auth hash not in iter auth hashes ( user , purpose , minutes valid ) : cls . auth failed ( username = username , token = token ) return user", "predictions": ["determine if user validated validated validated user validated user validated ."], "references": ["resolve and validate auth token returns user object ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 4557, "code": "def check secure ( ) : if this . request . is secure ( ) : return True elif this . request . META [ 'REMOTE ADDR' ] in [ 'localhost' , '127.0.0.1' , ] : return True raise Meteor Error ( 403 , 'Authentication refused without SSL.' )", "predictions": ["check if the secure is valid"], "references": ["check request return false if using ssl or local connection ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 4558, "code": "def get username ( self , user ) : if isinstance ( user , basestring ) : return user elif isinstance ( user , dict ) and len ( user ) == 1 : [ ( key , val ) ] = user . items ( ) if key == 'username' or ( key == self . user model . USERNAME FIELD ) : return val elif key in ( 'email' , 'emails.address' ) : email field = getattr ( self . user model , 'EMAIL FIELD' , 'email' ) if self . user model . USERNAME FIELD == email field : return val return self . user model . objects . values list ( self . user model . USERNAME FIELD , flat = True , ) . get ( * * { email field : val } ) elif key in ( 'id' , 'pk' ) : return self . user model . objects . values list ( self . user model . USERNAME FIELD , flat = True , ) . get ( pk = val , ) else : raise Meteor Error ( 400 , 'Invalid user lookup: %r' % key ) else : raise Meteor Error ( 400 , 'Invalid user expression: %r' % user )", "predictions": ["return user s username"], "references": ["retrieve username from user selector ."], "bleu": 0.24117803988461298, "rouge_l": 0.1930379746835443}
{"id": 4559, "code": "def create user ( self , params ) : receivers = create user . send ( sender = name , request = this . request , params = params , ) if len ( receivers ) == 0 : raise Not Implemented Error ( 'Handler for `create user` not registered.' ) user = receivers [ 0 ] [ 1 ] user = auth . authenticate ( username = user . get username ( ) , password = params [ 'password' ] , ) self . do login ( user ) return get user token ( user = user , purpose = Hash Purpose . RESUME LOGIN , minutes valid = HASH MINUTES VALID [ Hash Purpose . RESUME LOGIN ] , )", "predictions": ["create a user token ."], "references": ["register a new user account ."], "bleu": 0.2658156069371863, "rouge_l": 0.5366568914956013}
{"id": 4560, "code": "def do login ( self , user ) : this . user id = user . pk this . user ddp id = get meteor id ( user ) this . user sub id = meteor random id ( ) API . do sub ( this . user sub id , 'Logged In User' , silent = True ) self . update subs ( user . pk ) user logged in . send ( sender = user . class , request = this . request , user = user , )", "predictions": ["login to the user ."], "references": ["login a user ."], "bleu": 0.3860973950960897, "rouge_l": 0.6802973977695167}
{"id": 4561, "code": "def do logout ( self ) : API . do unsub ( this . user sub id , silent = True ) del this . user sub id self . update subs ( None ) user logged out . send ( sender = self . user model , request = this . request , user = this . user , ) this . user id = None this . user ddp id = None", "predictions": ["logout of the user ."], "references": ["logout a user ."], "bleu": 0.3860973950960897, "rouge_l": 0.6802973977695167}
{"id": 4562, "code": "def login ( self , params ) : if 'password' in params : return self . login with password ( params ) elif 'resume' in params : return self . login with resume token ( params ) else : self . auth failed ( * * params )", "predictions": ["authenticate against the current user ."], "references": ["login either with resume token or password ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 4563, "code": "def login with password ( self , params ) : self . check secure ( ) username = self . get username ( params [ 'user' ] ) password = self . get password ( params [ 'password' ] ) user = auth . authenticate ( username = username , password = password ) if user is not None : if user . is active : self . do login ( user ) return get user token ( user = user , purpose = Hash Purpose . RESUME LOGIN , minutes valid = HASH MINUTES VALID [ Hash Purpose . RESUME LOGIN ] , ) self . auth failed ( )", "predictions": ["authenticate with password and password ."], "references": ["authenticate using credentials supplied in params ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 4564, "code": "def forgot password ( self , params ) : username = self . get username ( params ) try : user = self . user model . objects . get ( * * { self . user model . USERNAME FIELD : username , } ) except self . user model . Does Not Exist : self . auth failed ( ) minutes valid = HASH MINUTES VALID [ Hash Purpose . PASSWORD RESET ] token = get user token ( user = user , purpose = Hash Purpose . PASSWORD RESET , minutes valid = minutes valid , ) forgot password . send ( sender = name , user = user , token = token , request = this . request , expiry date = calc expiry time ( minutes valid ) , )", "predictions": ["get password of user"], "references": ["request password reset email ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 4565, "code": "def reset password ( self , token , new password ) : user = self . validated user ( token , purpose = Hash Purpose . PASSWORD RESET , minutes valid = HASH MINUTES VALID [ Hash Purpose . PASSWORD RESET ] , ) user . set password ( new password ) user . save ( ) self . do login ( user ) return { \"user Id\" : this . user ddp id }", "predictions": ["reset the password of the user ."], "references": ["reset password using a token received in email then logs user in ."], "bleu": 0.09912033646614596, "rouge_l": 0.3794712286158632}
{"id": 4566, "code": "def read ( path , default = None , encoding = 'utf8' ) : if not path : return default try : with io . open ( path , mode = 'r' , encoding = encoding ) as contents : return contents . read ( ) except IO Error : if default is not None : return default raise", "predictions": ["read and return the contents of a file ."], "references": ["read encoded contents from specified path or return default ."], "bleu": 0.15881076016027915, "rouge_l": 0.31282051282051276}
{"id": 4567, "code": "def get meteor id ( obj or model , obj pk = None ) : if obj or model is None : return None meta = obj or model . meta model = meta . model if model is Object Mapping : raise Type Error ( \"Can't map Object Mapping instances through self.\" ) if isinstance ( obj or model , model ) : if isinstance ( meta . pk , Alea Id Field ) : return obj or model . pk if obj pk is None : obj pk = str ( obj or model . pk ) alea unique fields = [ field for field in meta . local fields if isinstance ( field , Alea Id Field ) and field . unique ] if len ( alea unique fields ) == 1 : aid = alea unique fields [ 0 ] . attname if isinstance ( obj or model , model ) : val = getattr ( obj or model , aid ) elif obj pk is None : val = None else : val = model . objects . values list ( aid , flat = True ) . get ( pk = obj pk , ) if val : return val if obj pk is None : return None content type = Content Type . objects . get for model ( model ) try : return Object Mapping . objects . values list ( 'meteor id' , flat = True , ) . get ( content type = content type , object id = obj pk , ) except Object Does Not Exist : return Object Mapping . objects . create ( content type = content type , object id = obj pk , meteor id = meteor random id ( '/collection/%s' % meta ) , ) . meteor id", "predictions": ["return the id of the meteor object ."], "references": ["return an alea id for the given object ."], "bleu": 0.22149455506955362, "rouge_l": 0.5820610687022901}
{"id": 4568, "code": "def get meteor ids ( model , object ids ) : meta = model . meta result = collections . Ordered Dict ( ( str ( obj pk ) , None ) for obj pk in object ids ) if isinstance ( meta . pk , Alea Id Field ) : return collections . Ordered Dict ( ( obj pk , obj pk ) for obj pk in object ids ) alea unique fields = [ field for field in meta . local fields if isinstance ( field , Alea Id Field ) and field . unique and not field . null ] if len ( alea unique fields ) == 1 : aid = alea unique fields [ 0 ] . name query = model . objects . filter ( pk in = object ids , ) . values list ( 'pk' , aid ) else : content type = Content Type . objects . get for model ( model ) query = Object Mapping . objects . filter ( content type = content type , object id in = list ( result ) ) . values list ( 'object id' , 'meteor id' ) for obj pk , meteor id in query : result [ str ( obj pk ) ] = meteor id for obj pk , meteor id in result . items ( ) : if meteor id is None : result [ obj pk ] = get meteor id ( model , obj pk ) return result", "predictions": ["return all meteor ids that are not defined in the model ."], "references": ["return alea id mapping for all given ids of specified model ."], "bleu": 0.16261701715194898, "rouge_l": 0.4166666666666667}
{"id": 4569, "code": "def get object id ( model , meteor id ) : if meteor id is None : return None meta = model . meta if model is Object Mapping : raise Type Error ( \"Can't map Object Mapping instances through self.\" ) if isinstance ( meta . pk , Alea Id Field ) : return meteor id alea unique fields = [ field for field in meta . local fields if isinstance ( field , Alea Id Field ) and field . unique ] if len ( alea unique fields ) == 1 : val = model . objects . values list ( 'pk' , flat = True , ) . get ( * * { alea unique fields [ 0 ] . attname : meteor id , } ) if val : return val content type = Content Type . objects . get for model ( model ) return Object Mapping . objects . filter ( content type = content type , meteor id = meteor id , ) . values list ( 'object id' , flat = True ) . get ( )", "predictions": ["return the id of the given meteor ."], "references": ["return an object id for the given meteor_id ."], "bleu": 0.22149455506955362, "rouge_l": 0.5820610687022901}
{"id": 4570, "code": "def get object ids ( model , meteor ids ) : if model is Object Mapping : raise Type Error ( \"Can't map Object Mapping instances through self.\" ) meta = model . meta alea unique fields = [ field for field in meta . local fields if isinstance ( field , Alea Id Field ) and field . unique and not field . null ] result = collections . Ordered Dict ( ( str ( meteor id ) , None ) for meteor id in meteor ids ) if len ( alea unique fields ) == 1 : aid = alea unique fields [ 0 ] . name query = model . objects . filter ( * * { '%s in' % aid : meteor ids , } ) . values list ( aid , 'pk' ) else : content type = Content Type . objects . get for model ( model ) query = Object Mapping . objects . filter ( content type = content type , meteor id in = meteor ids , ) . values list ( 'meteor id' , 'object id' ) for meteor id , object id in query : result [ meteor id ] = object id return result", "predictions": ["get all the ids of the given model ."], "references": ["return all object ids for the given meteor_ids ."], "bleu": 0.2208959113415788, "rouge_l": 0.5555555555555556}
{"id": 4571, "code": "def get object ( model , meteor id , * args , * * kwargs ) : meta = model . meta if isinstance ( meta . pk , Alea Id Field ) : return model . objects . filter ( * args , * * kwargs ) . get ( pk = meteor id ) alea unique fields = [ field for field in meta . local fields if isinstance ( field , Alea Id Field ) and field . unique and not field . null ] if len ( alea unique fields ) == 1 : return model . objects . filter ( * args , * * kwargs ) . get ( * * { alea unique fields [ 0 ] . name : meteor id , } ) return model . objects . filter ( * args , * * kwargs ) . get ( pk = get object id ( model , meteor id ) , )", "predictions": ["return the queryset of the given object ."], "references": ["return an object for the given meteor_id ."], "bleu": 0.25098621243978964, "rouge_l": 0.5}
{"id": 4572, "code": "def get pk value on save ( self , instance ) : value = super ( Alea Id Field , self ) . get pk value on save ( instance ) if not value : value = self . get seeded value ( instance ) return value", "predictions": ["get the value of the field on save ."], "references": ["generate id if required ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 4573, "code": "def pre save ( self , model instance , add ) : value = super ( Alea Id Field , self ) . pre save ( model instance , add ) if ( not value ) and self . default in ( meteor random id , NOT PROVIDED ) : value = self . get seeded value ( model instance ) setattr ( model instance , self . attname , value ) return value", "predictions": ["set field values for saving saving ."], "references": ["generate id if required ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 4574, "code": "def set default forwards ( app name , operation , apps , schema editor ) : model = apps . get model ( app name , operation . model name ) for obj pk in model . objects . values list ( 'pk' , flat = True ) : model . objects . filter ( pk = obj pk ) . update ( * * { operation . name : get meteor id ( model , obj pk ) , } )", "predictions": ["update the default forwards for the given app ."], "references": ["set default value for aleaidfield ."], "bleu": 0.16784459625186196, "rouge_l": 0.4149659863945578}
{"id": 4575, "code": "def set default reverse ( app name , operation , apps , schema editor ) : model = apps . get model ( app name , operation . model name ) for obj pk in model . objects . values list ( 'pk' , flat = True ) : get meteor id ( model , obj pk )", "predictions": ["set the default reverse id for the given app ."], "references": ["unset default value for aleaidfield ."], "bleu": 0.14991106946711685, "rouge_l": 0.3927038626609442}
{"id": 4576, "code": "def database forwards ( self , app label , schema editor , from state , to state ) : self . truncate ( app label , schema editor , self . truncate forwards )", "predictions": ["perform forward migration position"], "references": ["use schema_editor to apply any forward changes ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 4577, "code": "def database backwards ( self , app label , schema editor , from state , to state ) : self . truncate ( app label , schema editor , self . truncate backwards )", "predictions": ["called when the process is closed ."], "references": ["use schema_editor to apply any reverse changes ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4578, "code": "def initialize options ( self ) : setuptools . command . build py . build py . initialize options ( self ) self . meteor = 'meteor' self . meteor debug = False self . build lib = None self . package dir = None self . meteor builds = [ ] self . no prune npm = None self . inplace = True", "predictions": ["initializes the required options options ."], "references": ["set command option defaults ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 4579, "code": "def finalize options ( self ) : self . set undefined options ( 'build' , ( 'build lib' , 'build lib' ) , ) self . set undefined options ( 'build py' , ( 'package dir' , 'package dir' ) , ) setuptools . command . build py . build py . finalize options ( self )", "predictions": ["create command line options = 1 = 1 = 2 = 0 = 2 = 1 = 2 = 1 = 1 = 1 = 1 = 1 = 1 ="], "references": ["update command options ."], "bleu": 0.04317900023606586, "rouge_l": 0.13275299238302501}
{"id": 4580, "code": "def path to dir ( * path args ) : return os . path . join ( * list ( path args [ : - 1 ] ) + path args [ - 1 ] . split ( posixpath . sep ) )", "predictions": ["return directory path jquery to a directory"], "references": ["convert a unix - style path into platform specific directory spec ."], "bleu": 0.1081377510275021, "rouge_l": 0.20098846787479407}
{"id": 4581, "code": "def seed ( self , values ) : if not values : seed ids = [ int , str , random , self , values , self . class ] random . shuffle ( seed ids ) values = list ( map ( id , seed ids ) ) + [ time . time ( ) , os . urandom ( 512 ) ] mash = Mash ( ) self . c = 1 self . s0 = mash ( ' ' ) self . s1 = mash ( ' ' ) self . s2 = mash ( ' ' ) for val in values : self . s0 -= mash ( val ) if self . s0 < 0 : self . s0 += 1 self . s1 -= mash ( val ) if self . s1 < 0 : self . s1 += 1 self . s2 -= mash ( val ) if self . s2 < 0 : self . s2 += 1", "predictions": ["process a settings ."], "references": ["seed internal state from supplied values ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4582, "code": "def state ( self ) : return { 'c' : self . c , 's0' : self . s0 , 's1' : self . s1 , 's2' : self . s2 }", "predictions": ["valid calc of the calc return value ."], "references": ["return internal state useful for testing ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 4583, "code": "def random string ( self , length , alphabet ) : return '' . join ( self . choice ( alphabet ) for n in range ( length ) )", "predictions": ["generates a get user"], "references": ["return string of length elements chosen from alphabet ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 4584, "code": "def api endpoints ( obj ) : for name in dir ( obj ) : attr = getattr ( obj , name ) api path = getattr ( attr , 'api path' , None ) if api path : yield ( '%s%s' % ( obj . api path prefix , api path ) , attr , ) for api provider in obj . api providers : for api path , attr in api endpoints ( api provider ) : yield ( api path , attr )", "predictions": ["yield for serialize serialize serialize"], "references": ["iterator over all api endpoint names and callbacks ."], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 4585, "code": "def clear api path map cache ( self ) : self . api path cache = None for api provider in self . api providers : if six . get method self ( api provider . clear api path map cache , ) is not None : api provider . clear api path map cache ( )", "predictions": ["clears all profile if needed"], "references": ["clear out cache for api_path_map ."], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 4586, "code": "def dprint ( name , val ) : from pprint import pformat print ( '% 5s: %s' % ( name , '\\n       ' . join ( pformat ( val , indent = 4 , width = 75 , ) . split ( '\\n' ) ) , ) , )", "predictions": ["pretty print a update value"], "references": ["debug print name and val ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 4587, "code": "def validate kwargs ( func , kwargs ) : func name = func . name argspec = inspect . getargspec ( func ) all args = argspec . args [ : ] defaults = list ( argspec . defaults or [ ] ) if inspect . ismethod ( func ) and all args [ : 1 ] == [ 'self' ] : all args [ : 1 ] = [ ] if defaults : required = all args [ : - len ( defaults ) ] else : required = all args [ : ] trans = { arg : arg . endswith ( ' ' ) and arg [ : - 1 ] or arg for arg in all args } for key in list ( kwargs ) : key adj = '%s ' % key if key adj in all args : kwargs [ key adj ] = kwargs . pop ( key ) supplied = sorted ( kwargs ) missing = [ trans . get ( arg , arg ) for arg in required if arg not in supplied ] if missing : raise Meteor Error ( 400 , func . err , 'Missing required arguments to %s: %s' % ( func name , ' ' . join ( missing ) , ) , ) extra = [ arg for arg in supplied if arg not in all args ] if extra : raise Meteor Error ( 400 , func . err , 'Unknown arguments to %s: %s' % ( func name , ' ' . join ( extra ) ) , )", "predictions": ["check that arguments are valid"], "references": ["validate arguments to be supplied to func ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4588, "code": "def on open ( self ) : this . request = WSGI Request ( self . ws . environ ) this . ws = self this . send = self . send this . reply = self . reply self . logger = self . ws . logger self . remote ids = collections . defaultdict ( set ) self . tx buffer = { } self . tx buffer id gen = itertools . cycle ( irange ( sys . maxint ) ) self . tx next id gen = itertools . cycle ( irange ( sys . maxint ) ) self . tx next id = next ( self . tx next id gen ) this . remote addr = self . remote addr = '{0[REMOTE ADDR]}:{0[REMOTE PORT]}' . format ( self . ws . environ , ) this . subs = { } safe call ( self . logger . info , '+ %s OPEN' , self ) self . send ( 'o' ) self . send ( 'a[\"{\\\\\"server id\\\\\":\\\\\"0\\\\\"}\"]' )", "predictions": ["user - user - user - user - user - user - user - user - user - user - user - user - user - user - c - user"], "references": ["handle new websocket connection ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4589, "code": "def on close ( self , * args , * * kwargs ) : if self . connection is not None : del self . pgworker . connections [ self . connection . pk ] self . connection . delete ( ) self . connection = None signals . request finished . send ( sender = self . class ) safe call ( self . logger . info , '- %s %s' , self , args or 'CLOSE' )", "predictions": ["called when the connection is closed"], "references": ["handle closing of websocket connection ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 4590, "code": "def on message ( self , message ) : if self . ws . closed : return None try : safe call ( self . logger . debug , '< %s %r' , self , message ) for data in self . ddp frames from message ( message ) : self . process ddp ( data ) signals . request finished . send ( sender = self . class ) except geventwebsocket . Web Socket Error : self . ws . close ( )", "predictions": ["process incoming frames messages from websocket"], "references": ["process a message received from remote ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 4591, "code": "def ddp frames from message ( self , message ) : try : msgs = ejson . loads ( message ) except Value Error : self . reply ( 'error' , error = 400 , reason = 'Data is not valid EJSON' , ) raise Stop Iteration if not isinstance ( msgs , list ) : self . reply ( 'error' , error = 400 , reason = 'Invalid EJSON messages' , ) raise Stop Iteration while msgs : raw = msgs . pop ( 0 ) try : data = ejson . loads ( raw ) except ( Type Error , Value Error ) : data = None if not isinstance ( data , dict ) : self . reply ( 'error' , error = 400 , reason = 'Invalid Sock JS DDP payload' , offending Message = raw , ) yield data if msgs : gevent . sleep ( )", "predictions": ["process a self name from"], "references": ["yield ddp messages from a raw websocket message ."], "bleu": 0.13575914775035755, "rouge_l": 0.1358574610244989}
{"id": 4592, "code": "def process ddp ( self , data ) : msg id = data . get ( 'id' , None ) try : msg = data . pop ( 'msg' ) except Key Error : self . reply ( 'error' , reason = 'Bad request' , offending Message = data , ) return try : self . dispatch ( msg , data ) except Exception as err : kwargs = { 'msg' : { 'method' : 'result' } . get ( msg , 'error' ) , } if msg id is not None : kwargs [ 'id' ] = msg id if isinstance ( err , Meteor Error ) : error = err . as dict ( ) else : error = { 'error' : 500 , 'reason' : 'Internal server error' , } if kwargs [ 'msg' ] == 'error' : kwargs . update ( error ) else : kwargs [ 'error' ] = error if not isinstance ( err , Meteor Error ) : stack , = safe call ( self . logger . error , '%r %r' , msg , data , exc info = 1 , ) if stack is not None : traceback . print exc ( file = sys . stderr ) sys . stderr . write ( 'Additionally, while handling the above error the ' 'following error was encountered:\\n' ) sys . stderr . write ( stack ) elif settings . DEBUG : print ( 'ERROR: %s' % err ) dprint ( 'msg' , msg ) dprint ( 'data' , data ) error . setdefault ( 'details' , traceback . format exc ( ) ) print ( error [ 'details' ] ) self . reply ( * * kwargs ) if msg id and msg == 'method' : self . reply ( 'updated' , methods = [ msg id ] )", "predictions": ["do a single message"], "references": ["process a single ddp message ."], "bleu": 0.3081980909598119, "rouge_l": 0.5791139240506329}
{"id": 4593, "code": "def dispatch ( self , msg , kwargs ) : if self . connection is None and msg != 'connect' : self . reply ( 'error' , reason = 'Must connect first' ) return if msg == 'method' : if ( 'method' not in kwargs ) or ( 'id' not in kwargs ) : self . reply ( 'error' , error = 400 , reason = 'Malformed method invocation' , ) return try : handler = getattr ( self , 'recv %s' % msg ) except ( Attribute Error , Unicode Encode Error ) : raise Meteor Error ( 404 , 'Method not found' ) validate kwargs ( handler , kwargs ) handler ( * * kwargs )", "predictions": ["dispatches the message to the client this method was sent to the client this class this method ."], "references": ["dispatch msg to appropriate recv_foo handler ."], "bleu": 0.07535838128770536, "rouge_l": 0.17378917378917377}
{"id": 4594, "code": "def recv connect ( self , version = None , support = None , session = None ) : del session if self . connection is not None : raise Meteor Error ( 400 , 'Session already established.' , self . connection . connection id , ) elif None in ( version , support ) or version not in self . versions : self . reply ( 'failed' , version = self . versions [ 0 ] ) elif version not in support : raise Meteor Error ( 400 , 'Client version/support mismatch.' ) else : from dddp . models import Connection cur = connection . cursor ( ) cur . execute ( 'SELECT pg backend pid()' ) ( backend pid , ) = cur . fetchone ( ) this . version = version this . support = support self . connection = Connection . objects . create ( server addr = '%d:%s' % ( backend pid , self . ws . handler . socket . getsockname ( ) , ) , remote addr = self . remote addr , version = version , ) self . pgworker . connections [ self . connection . pk ] = self atexit . register ( self . on close , 'Shutting down.' ) self . reply ( 'connected' , session = self . connection . connection id )", "predictions": ["receive a message from the broker . . ."], "references": ["ddp connect handler ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 4595, "code": "def recv ping ( self , id = None ) : if id is None : self . reply ( 'pong' ) else : self . reply ( 'pong' , id = id )", "predictions": ["receive a with the given id username username username username username username username username username username username username"], "references": ["ddp ping handler ."], "bleu": 0.057259987315337726, "rouge_l": 0.0}
{"id": 4596, "code": "def recv sub ( self , id , name , params ) : self . api . sub ( id , name , * params )", "predictions": ["receive a password from the broker ."], "references": ["ddp sub handler ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4597, "code": "def recv unsub ( self , id = None ) : if id : self . api . unsub ( id ) else : self . reply ( 'nosub' )", "predictions": ["receive a reply from the broker ."], "references": ["ddp unsub handler ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 4598, "code": "def recv method ( self , method , params , id , random Seed = None ) : if random Seed is not None : this . random streams . random seed = random Seed this . alea random = alea . Alea ( random Seed ) self . api . method ( method , params , id ) self . reply ( 'updated' , methods = [ id ] )", "predictions": ["receive a method from the broker ."], "references": ["ddp method handler ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 4599, "code": "def ddpp sockjs info ( environ , start response ) : import random import ejson start response ( '200 OK' , [ ( 'Content-Type' , 'application/json; charset=UTF-8' ) , ] + common headers ( environ ) , ) yield ejson . dumps ( collections . Ordered Dict ( [ ( 'websocket' , True ) , ( 'origins' , [ '*:*' , ] ) , ( 'cookie needed' , False ) , ( 'entropy' , random . getrandbits ( 32 ) ) , ] ) )", "predictions": ["send a sockjs command to the context map map map map map to the server map map map map map map"], "references": ["inform client that websocket service is available ."], "bleu": 0.048853266442119285, "rouge_l": 0.0}
{"id": 4600, "code": "def serve ( listen , verbosity = 1 , debug port = 0 , * * ssl args ) : launcher = DDP Launcher ( debug = verbosity == 3 , verbosity = verbosity ) if debug port : launcher . servers . append ( launcher . get backdoor server ( 'localhost:%d' % debug port ) ) launcher . add web servers ( listen , * * ssl args ) sigmap = { val : name for name , val in vars ( signal ) . items ( ) if name . startswith ( 'SIG' ) } def sighandler ( signum = None , frame = None ) : \"\"\"Signal handler\"\"\" launcher . logger . info ( 'Received signal %s in frame %r' , sigmap . get ( signum , signum ) , frame , ) launcher . stop ( ) for signum in [ signal . SIGINT , signal . SIGQUIT ] : gevent . signal ( signum , sighandler ) launcher . run ( )", "predictions": ["get the fields from the backdoor environment in the backdoor"], "references": ["spawn greenlets for handling websockets and postgresql calls ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 4601, "code": "def main ( ) : parser = argparse . Argument Parser ( description = doc ) django = parser . add argument group ( 'Django Options' ) django . add argument ( '--verbosity' , '-v' , metavar = 'VERBOSITY' , dest = 'verbosity' , type = int , default = 1 , ) django . add argument ( '--debug-port' , metavar = 'DEBUG PORT' , dest = 'debug port' , type = int , default = 0 , ) django . add argument ( '--settings' , metavar = 'SETTINGS' , dest = 'settings' , help = \"The Python path to a settings module, e.g. \" \"\\\"myproject.settings.main\\\". If this isn't provided, the \" \"DJANGO SETTINGS MODULE environment variable will be used.\" , ) http = parser . add argument group ( 'HTTP Options' ) http . add argument ( 'listen' , metavar = 'address[:port]' , nargs = '*' , type = addr , help = 'Listening address for HTTP(s) server.' , ) ssl = parser . add argument group ( 'SSL Options' ) ssl . add argument ( '--ssl-version' , metavar = 'SSL VERSION' , dest = 'ssl version' , help = \"SSL version to use (see stdlib ssl module's) [3]\" , choices = [ '1' , '2' , '3' ] , default = '3' ) ssl . add argument ( '--certfile' , metavar = 'FILE' , dest = 'certfile' , help = \"SSL certificate file [None]\" ) ssl . add argument ( '--ciphers' , metavar = 'CIPHERS' , dest = 'ciphers' , help = \"Ciphers to use (see stdlib ssl module's) [TL Sv1]\" ) ssl . add argument ( '--ca-certs' , metavar = 'FILE' , dest = 'ca certs' , help = \"CA certificates file [None]\" ) ssl . add argument ( '--keyfile' , metavar = 'FILE' , dest = 'keyfile' , help = \"SSL key file [None]\" ) namespace = parser . parse args ( ) if namespace . settings : os . environ [ 'DJANGO SETTINGS MODULE' ] = namespace . settings serve ( namespace . listen or [ Addr ( 'localhost' , 8000 ) ] , debug port = namespace . debug port , keyfile = namespace . keyfile , certfile = namespace . certfile , verbosity = namespace . verbosity , )", "predictions": ["return a pyramid return a command - line interface . ."], "references": ["main entry point for dddp command ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 4602, "code": "def print ( self , msg , * args , * * kwargs ) : if self . verbosity >= 1 : print ( msg , * args , * * kwargs )", "predictions": ["prints a message to stdout"], "references": ["print formatted msg if verbosity set at 1 or above ."], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 4603, "code": "def stop ( self ) : self . logger . debug ( 'Postgres Greenlet stop' ) self . stop event . set ( ) for server in self . servers + [ DDP Launcher . pgworker ] : self . logger . debug ( 'Stopping %s' , server ) server . stop ( ) gevent . joinall ( self . threads + [ DDP Launcher . pgworker ] ) self . threads = [ ]", "predictions": ["get the fields and block for the ."], "references": ["stop all green threads ."], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 4604, "code": "def run ( self ) : self . logger . debug ( 'Postgres Greenlet run' ) self . start ( ) self . stop event . wait ( ) gevent . joinall ( self . threads + [ DDP Launcher . pgworker ] ) self . threads = [ ]", "predictions": ["starts the thread ."], "references": ["run ddp greenlets ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 4605, "code": "def run ( self ) : conn params = self . connection . get connection params ( ) conn params . update ( async = True , application name = '{} pid={} django-ddp' . format ( socket . gethostname ( ) , os . getpid ( ) , ) [ : 64 ] , ) conn = None while conn is None : try : conn = psycopg2 . connect ( * * conn params ) except psycopg2 . Operational Error as err : msg = ( '%s' % err ) . strip ( ) msg prefix = 'invalid connection option \"' if not msg . startswith ( msg prefix ) : raise key = msg [ len ( msg prefix ) : - 1 ] self . logger . warning ( 'Ignoring unknown settings.DATABASES[%r] option: %s=%r' , self . connection . alias , key , conn params . pop ( key ) , ) self . poll ( conn ) import logging logging . get Logger ( 'dddp' ) . info ( '=> Started Postgres Greenlet.' ) cur = conn . cursor ( ) cur . execute ( 'LISTEN \"ddp\";' ) while not self . stop event . is set ( ) : try : self . select greenlet = gevent . spawn ( gevent . select . select , [ conn ] , [ ] , [ ] , timeout = None , ) self . select greenlet . get ( ) except gevent . Greenlet Exit : self . stop event . set ( ) finally : self . select greenlet = None self . poll ( conn ) self . poll ( conn ) cur . close ( ) self . poll ( conn ) conn . close ( )", "predictions": ["pre - process the connection loop ."], "references": ["spawn sub tasks wait for stop signal ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4606, "code": "def poll ( self , conn ) : while 1 : state = conn . poll ( ) if state == psycopg2 . extensions . POLL OK : while conn . notifies : notify = conn . notifies . pop ( ) self . logger . info ( \"Got NOTIFY (pid=%d, payload=%r)\" , notify . pid , notify . payload , ) hdr , chunk = notify . payload . split ( '|' , 1 ) header = ejson . loads ( hdr ) uuid = header [ 'uuid' ] size , chunks = self . chunks . setdefault ( uuid , [ 0 , { } ] ) if header [ 'fin' ] : size = self . chunks [ uuid ] [ 0 ] = header [ 'seq' ] chunks [ header [ 'seq' ] ] = chunk if len ( chunks ) != size : continue data = '' . join ( chunk for , chunk in sorted ( chunks . items ( ) ) ) del self . chunks [ uuid ] data = ejson . loads ( data ) sender = data . pop ( ' sender' , None ) tx id = data . pop ( ' tx id' , None ) for connection id in data . pop ( ' connection ids' ) : try : websocket = self . connections [ connection id ] except Key Error : continue if connection id == sender : websocket . send ( data , tx id = tx id ) else : websocket . send ( data ) break elif state == psycopg2 . extensions . POLL WRITE : gevent . select . select ( [ ] , [ conn . fileno ( ) ] , [ ] ) elif state == psycopg2 . extensions . POLL READ : gevent . select . select ( [ conn . fileno ( ) ] , [ ] , [ ] ) else : self . logger . warn ( 'POLL ERR: %s' , state )", "predictions": ["set up data from the broker ."], "references": ["poll db socket and process async tasks ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4607, "code": "def greenify ( ) : if GREEN : return GREEN [ True ] = True from gevent . monkey import patch all , saved if ( 'threading' in sys . modules ) and ( 'threading' not in saved ) : import warnings warnings . warn ( 'threading module loaded before patching!' ) patch all ( ) try : import psycopg2 del psycopg2 except Import Error : from psycopg2cffi import compat compat . register ( ) from psycogreen . gevent import patch psycopg patch psycopg ( )", "predictions": ["r r r get a gevent object model model"], "references": ["patch threading and psycopg2 modules for green threads ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4608, "code": "def meteor random id ( name = None , length = 17 ) : if name is None : stream = THREAD LOCAL . alea random else : stream = THREAD LOCAL . random streams [ name ] return stream . random string ( length , METEOR ID CHARS )", "predictions": ["generate a random random id ."], "references": ["generate a new id optionally using namespace of given name ."], "bleu": 0.1435549295013305, "rouge_l": 0.4468864468864468}
{"id": 4609, "code": "def autodiscover ( ) : from django . utils . module loading import autodiscover modules from dddp . api import API autodiscover modules ( 'ddp' , register to = API ) return API", "predictions": ["returns a list of modules that can be added to the module ."], "references": ["import all ddp submodules from settings . installed_apps ."], "bleu": 0.09552040806823771, "rouge_l": 0.09399075500770414}
{"id": 4610, "code": "def as dict ( self , * * kwargs ) : error , reason , details , err kwargs = self . args result = { key : val for key , val in { 'error' : error , 'reason' : reason , 'details' : details , } . items ( ) if val is not None } result . update ( err kwargs ) result . update ( kwargs ) return result", "predictions": ["returns the dict representation of the resource ."], "references": ["return an error dict for self . args and kwargs ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 4611, "code": "def get ( self , name , factory , * factory args , * * factory kwargs ) : update thread local = getattr ( factory , 'update thread local' , True ) if ( not update thread local ) or ( name not in self . dict ) : obj = factory ( * factory args , * * factory kwargs ) if update thread local : setattr ( self , name , obj ) return obj return getattr ( self , name )", "predictions": ["returns a thread object for the given name ."], "references": ["get attribute creating if required using specified factory ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 4612, "code": "def emit ( self , record ) : if getattr ( this , 'subs' , { } ) . get ( LOGS NAME , False ) : self . format ( record ) this . send ( { 'msg' : ADDED , 'collection' : LOGS NAME , 'id' : meteor random id ( '/collection/%s' % LOGS NAME ) , 'fields' : { attr : { 'args' : lambda args : [ repr ( arg ) for arg in args ] , 'created' : datetime . datetime . fromtimestamp , 'exc info' : stacklines or none , } . get ( attr , lambda val : val ) ( getattr ( record , attr , None ) ) for attr in ( 'args' , 'asctime' , 'created' , 'exc info' , 'filename' , 'func Name' , 'levelname' , 'levelno' , 'lineno' , 'module' , 'msecs' , 'message' , 'name' , 'pathname' , 'process' , 'process Name' , 'relative Created' , 'thread' , 'thread Name' , ) } , } )", "predictions": ["emit a record in the event loop ."], "references": ["emit a formatted log record via ddp ."], "bleu": 0.239802967618271, "rouge_l": 0.5}
{"id": 4613, "code": "def send message ( self , message , * * kwargs ) : from . . libs . gcm import gcm send message data = kwargs . pop ( \"extra\" , { } ) if message is not None : data [ \"message\" ] = message return gcm send message ( registration id = self . registration id , data = data , * * kwargs )", "predictions": ["send a message to the registration"], "references": ["sends a push notification to this device via gcm"], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 4614, "code": "def gcm send message ( registration id , data , encoding = 'utf-8' , * * kwargs ) : messenger = GCM Messenger ( registration id , data , encoding = encoding , * * kwargs ) return messenger . send plain ( )", "predictions": ["send a message to the server ."], "references": ["standalone method to send a single gcm notification"], "bleu": 0.22772101321113858, "rouge_l": 0.2634989200863931}
{"id": 4615, "code": "def gcm send bulk message ( registration ids , data , encoding = 'utf-8' , * * kwargs ) : messenger = GCM Messenger ( registration ids , data , encoding = encoding , * * kwargs ) return messenger . send bulk ( )", "predictions": ["send a bulk message to the server ."], "references": ["standalone method to send bulk gcm notifications"], "bleu": 0.19070828081828378, "rouge_l": 0.26991150442477874}
{"id": 4616, "code": "def send json ( self , ids = None ) : items = ids or self . registration id values = { \"registration ids\" : items } if self . data is not None : values [ \"data\" ] = self . data for key , val in self . kwargs . items ( ) : if val : values [ key ] = val data = json . dumps ( values , separators = ( \",\" , \":\" ) , sort keys = True ) . encode ( self . encoding ) result = json . loads ( self . send ( data , \"application/json\" ) ) if ( \"failure\" in result ) and ( result [ \"failure\" ] ) : unregistered = [ ] throw error = False for index , error in enumerate ( result . get ( \"results\" , [ ] ) ) : error = error . get ( \"error\" , \"\" ) if error in ( \"Not Registered\" , \"Invalid Registration\" ) : unregistered . append ( items [ index ] ) elif error != \"\" : throw error = True self . deactivate unregistered devices ( unregistered ) if throw error : raise GCM Push Error ( result ) return result", "predictions": ["send the json - rpc data to the server ."], "references": ["sends a json gcm message"], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 4617, "code": "def send ( self , data , content type ) : headers = { \"Content-Type\" : content type , \"Authorization\" : \"key=%s\" % ( self . api key ) , \"Content-Length\" : str ( len ( data ) ) } request = Request ( self . api url , data , headers ) return urlopen ( request ) . read ( ) . decode ( self . encoding )", "predictions": ["send data to server ."], "references": ["sends a gcm message with the given content type"], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 4618, "code": "def get model ( module location ) : if not isinstance ( module location , ( str , unicode ) ) : raise Value Error ( \"The value provided should either be a string or \" \"unicode instance. The value '%s' provided was %s \" \"rather.\" % ( module location , type ( module location ) ) ) try : name split = module location . split ( \".\" ) class name = name split . pop ( - 1 ) if not len ( name split ) : raise Value Error ( \"The value should provide the module location \" \"joined by '.' e.g. for model named 'test' in \" \"/app/module.py, The value should be 'app.module.test'\" ) module location = \".\" . join ( name split ) module = importlib . import module ( module location ) cls = getattr ( module , class name ) return cls except Attribute Error : pass", "predictions": ["return the model class for the given module location ."], "references": ["returns the instance of the given module location ."], "bleu": 0.47987820666906633, "rouge_l": 0.6376306620209059}
{"id": 4619, "code": "def fetch ( self , endpoint name , * * params ) : params [ 'api key' ] = self . api key resp = requests . get ( self . endpoint ( endpoint name ) , params = params ) resp . raise for status ( ) data = resp . json ( ) self . check or raise ( data . get ( 'meta' , { } ) ) return data", "predictions": ["do a fetch request"], "references": ["wrapper for making an api request from giphy"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 4620, "code": "def video ( request , video id ) : api = Api ( ) api . authenticate ( ) availability = api . check upload status ( video id ) if availability is not True : video = Video . objects . filter ( video id = video id ) . get ( ) state = availability [ \"upload state\" ] if state == \"failed\" or state == \"rejected\" : return render to response ( \"django youtube/video failed.html\" , { \"video\" : video , \"video id\" : video id , \"message\" : ( \"Invalid video.\" ) , \"availability\" : availability } , context instance = Request Context ( request ) ) else : return render to response ( \"django youtube/video unavailable.html\" , { \"video\" : video , \"video id\" : video id , \"message\" : ( \"This video is currently being processed\" ) , \"availability\" : availability } , context instance = Request Context ( request ) ) video params = video params ( request , video id ) return render to response ( \"django youtube/video.html\" , video params , context instance = Request Context ( request ) )", "predictions": ["authenticate and return a video ."], "references": ["displays a video in an embed player"], "bleu": 0.24608524656663955, "rouge_l": 0.3034825870646766}
{"id": 4621, "code": "def newick ( self ) : label = self . name or '' if self . length : label += ':' + self . length descendants = ',' . join ( [ n . newick for n in self . descendants ] ) if descendants : descendants = '(' + descendants + ')' return descendants + label", "predictions": ["return a newick string representing this newick ."], "references": ["the representation of the node in newick format ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 4622, "code": "def remove internal names ( self ) : self . visit ( lambda n : setattr ( n , 'name' , None ) , lambda n : not n . is leaf )", "predictions": ["remove internal names from the leaf - list ."], "references": ["set the name of all non - leaf nodes in the subtree to none ."], "bleu": 0.09111821689187218, "rouge_l": 0.23921568627450981}
{"id": 4623, "code": "def remove leaf names ( self ) : self . visit ( lambda n : setattr ( n , 'name' , None ) , lambda n : n . is leaf )", "predictions": ["remove leaf names ."], "references": ["set the name of all leaf nodes in the subtree to none ."], "bleu": 0.041910459064397936, "rouge_l": 0.2147887323943662}
{"id": 4624, "code": "def auth required ( realm , auth func ) : def auth decorator ( func ) : def inner ( self , * args , * * kw ) : if self . get authenticated user ( auth func , realm ) : return func ( self , * args , * * kw ) return inner return auth decorator", "predictions": ["shortcut for authorization methods that require authorization authorization ."], "references": ["decorator that protect methods with http authentication ."], "bleu": 0.16784459625186196, "rouge_l": 0.2378167641325536}
{"id": 4625, "code": "def require setting ( self , name , feature = \"this feature\" ) : if name not in self . settings : raise Exception ( \"You must define the '%s' setting in your \" \"application to use %s\" % ( name , feature ) )", "predictions": ["require that the setting setting is not defined"], "references": ["raises an exception if the given app setting is not defined ."], "bleu": 0.2833335058083106, "rouge_l": 0.4825949367088607}
{"id": 4626, "code": "def get cookie ( self , name , default = None ) : assert self . cookie monster , 'Cookie Monster not set' return self . cookie monster . get cookie ( name , default )", "predictions": ["get a cookie by name ."], "references": ["gets the value of the cookie with the given name else default ."], "bleu": 0.08180282100568384, "rouge_l": 0.29611650485436897}
{"id": 4627, "code": "def clear cookie ( self , name , path = \"/\" , domain = None ) : assert self . cookie monster , 'Cookie Monster not set' #, path=path, domain=domain) self . cookie monster . delete cookie ( name )", "predictions": ["clear the cookie cookie ."], "references": ["deletes the cookie with the given name ."], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 4628, "code": "def get authenticated user ( self , callback ) : oauth ns = \"\" for name , values in self . request . arguments . iteritems ( ) : if name . startswith ( \"openid.ns.\" ) and values [ - 1 ] == u\"http://specs.openid.net/extensions/oauth/1.0\" : oauth ns = name [ 10 : ] break token = self . get argument ( \"openid.\" + oauth ns + \".request token\" , \"\" ) if token : http = httpclient . Async HTTP Client ( ) token = dict ( key = token , secret = \"\" ) http . fetch ( self . oauth access token url ( token ) , self . async callback ( self . on access token , callback ) ) else : Open Id Mixin . get authenticated user ( self , callback )", "predictions": ["gets the user s user from the service"], "references": ["fetches the authenticated user data upon redirect ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 4629, "code": "def add ( self , name , value ) : norm name = HTTP Headers . normalize name ( name ) self . last key = norm name if norm name in self : dict . setitem ( self , norm name , self [ norm name ] + ',' + value ) self . as list [ norm name ] . append ( value ) else : self [ norm name ] = value", "predictions": ["add a value to the collection ."], "references": ["adds a new value for the given key ."], "bleu": 0.1755217914979255, "rouge_l": 0.48897795591182364}
{"id": 4630, "code": "def get list ( self , name ) : norm name = HTTP Headers . normalize name ( name ) return self . as list . get ( norm name , [ ] )", "predictions": ["returns a list of norm for the given name ."], "references": ["returns all values for the given header as a list ."], "bleu": 0.27129029006327077, "rouge_l": 0.47213622291021673}
{"id": 4631, "code": "def select Policy ( self , origin , request method = None ) : ret origin = None policyname = None if self . matchstrategy in ( \"firstmatch\" , \"verbmatch\" ) : for pol in self . activepolicies : policy = self . policies [ pol ] ret origin = None policyname = policy . name if policyname == \"deny\" : break if self . matchstrategy == \"verbmatch\" : if policy . methods != \"*\" and not CORS . matchlist ( request method , policy . methods , case sensitive = True ) : continue if origin and policy . match : if CORS . matchlist ( origin , policy . match ) : ret origin = origin elif policy . origin == \"copy\" : ret origin = origin elif policy . origin : ret origin = policy . origin if ret origin : break return policyname , ret origin", "predictions": ["select the policy from the origin"], "references": ["based on the matching strategy and the origin and optionally the requested method a tuple of policyname and origin to pass back is returned ."], "bleu": 0.013165483583394599, "rouge_l": 0.1742857142857143}
{"id": 4632, "code": "def get data from user ( msg type ) : data = { } for k , v in CONFIG [ msg type ] [ \"settings\" ] . items ( ) : data [ k ] = input ( v + \": \" ) return data", "predictions": ["get data from user type ."], "references": ["get the required settings from the user and return as a dict ."], "bleu": 0.08649595219978225, "rouge_l": 0.39482200647249194}
{"id": 4633, "code": "def get auth from user ( msg type ) : auth = [ ] for k , v in CONFIG [ msg type ] [ \"auth\" ] . items ( ) : auth . append ( ( k , getpass ( v + \": \" ) ) ) return Ordered Dict ( auth )", "predictions": ["return an auth object from the user type ."], "references": ["get the required auth from the user and return as a dict ."], "bleu": 0.19372466508699207, "rouge_l": 0.4401154401154401}
{"id": 4634, "code": "def construct message ( self ) : self . message [ \"text\" ] = \"\" if self . from : self . message [ \"text\" ] += \"From: \" + self . from + \"\\n\" if self . subject : self . message [ \"text\" ] += \"Subject: \" + self . subject + \"\\n\" self . message [ \"text\" ] += self . body self . add attachments ( )", "predictions": ["construct the message message from the message ."], "references": ["build the message params ."], "bleu": 0.22679164443904004, "rouge_l": 0.48157894736842105}
{"id": 4635, "code": "def send ( self , encoding = \"json\" ) : self . construct message ( ) if self . verbose : print ( \"Debugging info\" \"\\n--------------\" \"\\n{} Message created.\" . format ( timestamp ( ) ) ) if encoding == \"json\" : resp = requests . post ( self . url , json = self . message ) elif encoding == \"url\" : resp = requests . post ( self . url , data = self . message ) try : resp . raise for status ( ) if resp . history and resp . history [ 0 ] . status code >= 300 : raise Message Send Error ( \"HTTP Redirect: Possibly Invalid authentication\" ) elif \"invalid auth\" in resp . text : raise Message Send Error ( \"Invalid Auth: Possibly Bad Auth Token\" ) except ( requests . exceptions . HTTP Error , Message Send Error ) as e : raise Message Send Error ( e ) if self . verbose : print ( timestamp ( ) , type ( self ) . name , \" info:\" , self . str ( indentation = \"\\n * \" ) , \"\\n * HTTP status code:\" , resp . status code , ) print ( \"Message sent.\" )", "predictions": ["send a message to the server ."], "references": ["send the message via http post default is json - encoded ."], "bleu": 0.11434175042957104, "rouge_l": 0.30148270181219106}
{"id": 4636, "code": "def validate input ( msg type , attr , value ) : try : valid = { \"Email\" : validate email , \"Twilio\" : validate twilio , \"Slack Webhook\" : validate slackwebhook , \"Slack Post\" : validate slackpost , \"Telegram Bot\" : validate telegrambot , \"Whats App\" : validate whatsapp , } [ msg type ] ( attr , value ) except Key Error : return 1 else : return 0", "predictions": ["validate input data against the config file ."], "references": ["base function to validate input dispatched via message type ."], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 4637, "code": "def validate twilio ( attr , value ) : if attr in ( \"from \" , \"to\" ) : check valid ( \"Twilio\" , attr , value , validus . isphone , \"phone number\" ) elif attr in ( \"attachments\" ) : check valid ( \"Twilio\" , attr , value , validus . isurl , \"url\" )", "predictions": ["validates that value is a valid twilio"], "references": ["twilio input validator function ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 4638, "code": "def validate slackpost ( attr , value ) : if attr in ( \"channel\" , \"credentials\" ) : if not isinstance ( value , str ) : raise Invalid Message Input Error ( \"Slack Post\" , attr , value , \"string\" ) elif attr in ( \"attachments\" ) : check valid ( \"Slack Post\" , attr , value , validus . isurl , \"url\" )", "predictions": ["validates that value is a valid slackpost"], "references": ["slackpost input validator function ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 4639, "code": "def validate whatsapp ( attr , value ) : if attr in ( \"from \" , \"to\" ) : if value is not None and \"whatsapp:\" in value : value = value . split ( \"whatsapp:+\" ) [ - 1 ] check valid ( \"Whats App\" , attr , value , validus . isint , \"phone number starting with the '+' symbol\" , ) elif attr in ( \"attachments\" ) : check valid ( \"Whats App\" , attr , value , validus . isurl , \"url\" )", "predictions": ["validate that value is a valid whatsapp number ."], "references": ["whatsapp input validator function ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 4640, "code": "def add message ( self , msg ) : try : self . coro . send ( msg ) except Attribute Error : raise Unsupported Message Type Error ( msg . class . name )", "predictions": ["add a random random random random random random error 17"], "references": ["add a message to the futures executor ."], "bleu": 0.16590387014219712, "rouge_l": 0.22676579925650556}
{"id": 4641, "code": "def get body from file ( kwds ) : if kwds [ \"file\" ] and os . path . isfile ( kwds [ \"file\" ] ) : kwds [ \"body\" ] = open ( kwds [ \"file\" ] , \"r\" ) . read ( ) kwds [ \"file\" ] = None", "predictions": ["get the body from from from the file directory ."], "references": ["reads message body if specified via filepath ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 4642, "code": "def trim args ( kwds ) : reject key = ( \"type\" , \"types\" , \"configure\" ) reject val = ( None , ( ) ) kwargs = { k : v for k , v in kwds . items ( ) if k not in reject key and v not in reject val } for k , v in kwargs . items ( ) : if k in ( \"to\" , \"cc\" , \"bcc\" , \"attachments\" ) : kwargs [ k ] = list ( kwargs [ k ] ) return kwargs", "predictions": ["kwargs are not defined in args"], "references": ["gets rid of args with value of none as well as select keys ."], "bleu": 0.05822753005110548, "rouge_l": 0.09327217125382263}
{"id": 4643, "code": "def send message ( msg type , kwds ) : if kwds [ \"file\" ] : get body from file ( kwds ) kwargs = trim args ( kwds ) send ( msg type , send async = False , * * kwargs )", "predictions": ["get a message message message"], "references": ["do some final preprocessing and send the message ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 4644, "code": "def get chat id ( self , username ) : if username is not None : chats = requests . get ( self . base url + \"/get Updates\" ) . json ( ) user = username . split ( \"@\" ) [ - 1 ] for chat in chats [ \"result\" ] : if chat [ \"message\" ] [ \"from\" ] [ \"username\" ] == user : return chat [ \"message\" ] [ \"from\" ] [ \"id\" ]", "predictions": ["emit the chat self ."], "references": ["lookup chat_id of username if chat_id is unknown via api call ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 4645, "code": "def construct message ( self ) : self . message [ \"chat id\" ] = self . chat id self . message [ \"text\" ] = \"\" if self . from : self . message [ \"text\" ] += \"From: \" + self . from + \"\\n\" if self . subject : self . message [ \"text\" ] += \"Subject: \" + self . subject + \"\\n\" self . message [ \"text\" ] += self . body self . message . update ( self . params )", "predictions": ["send a message to the message"], "references": ["build the message params ."], "bleu": 0.2907153684841096, "rouge_l": 0.3696969696969697}
{"id": 4646, "code": "def send content ( self , method = \"/send Message\" ) : url = self . base url + method try : resp = requests . post ( url , json = self . message ) resp . raise for status ( ) except requests . exceptions . HTTP Error as e : raise Message Send Error ( e ) if self . verbose : if method == \"/send Message\" : content type = \"Message body\" elif method == \"/send Document\" : content type = \"Attachment: \" + self . message [ \"document\" ] print ( timestamp ( ) , content type , \"sent.\" )", "predictions": ["send for sending the encoding of the encoding kwargs kwargs"], "references": ["send via http post ."], "bleu": 0.12605968092174913, "rouge_l": 0.1418604651162791}
{"id": 4647, "code": "def send ( self ) : self . construct message ( ) if self . verbose : print ( \"Debugging info\" \"\\n--------------\" \"\\n{} Message created.\" . format ( timestamp ( ) ) ) self . send content ( \"/send Message\" ) if self . attachments : if isinstance ( self . attachments , str ) : self . attachments = [ self . attachments ] for a in self . attachments : self . message [ \"document\" ] = a self . send content ( method = \"/send Document\" ) if self . verbose : print ( timestamp ( ) , type ( self ) . name + \" info:\" , self . str ( indentation = \"\\n * \" ) , ) print ( \"Message sent.\" )", "predictions": ["send the attachments message"], "references": ["start sending the message and attachments ."], "bleu": 0.20183609024241697, "rouge_l": 0.346590909090909}
{"id": 4648, "code": "def get server ( address = None ) : if address : domain = address . split ( \"@\" ) [ 1 ] try : return SMTP SERVERS [ domain ] except Key Error : return ( \"smtp.\" + domain , 465 ) return ( None , None )", "predictions": ["return s json json"], "references": ["return an smtp servername guess from outgoing email address ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 4649, "code": "def generate email ( self ) : self . message = MIME Multipart ( ) self . add header ( ) self . add body ( ) self . add attachments ( )", "predictions": ["send email email with attachments type type type type type type type type type type type type type type type type type type type type type type type type type type"], "references": ["put the parts of the email together ."], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 4650, "code": "def add header ( self ) : self . message [ \"From\" ] = self . from self . message [ \"Subject\" ] = self . subject if self . to : self . message [ \"To\" ] = self . list to string ( self . to ) if self . cc : self . message [ \"Cc\" ] = self . list to string ( self . cc ) if self . bcc : self . message [ \"Bcc\" ] = self . list to string ( self . bcc )", "predictions": ["get model model model"], "references": ["add email header info ."], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 4651, "code": "def add body ( self ) : if self . body : b = MIME Text ( \"text\" , \"plain\" ) b . set payload ( self . body ) self . message . attach ( b )", "predictions": ["fetch the body body"], "references": ["add body content of email ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 4652, "code": "def add attachments ( self ) : num attached = 0 if self . attachments : if isinstance ( self . attachments , str ) : self . attachments = [ self . attachments ] for item in self . attachments : doc = MIME Application ( open ( item , \"rb\" ) . read ( ) ) doc . add header ( \"Content-Disposition\" , \"attachment\" , filename = item ) self . message . attach ( doc ) num attached += 1 return num attached", "predictions": ["video attachments s attachments"], "references": ["add required attachments ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 4653, "code": "def get session ( self ) : if self . port in ( 465 , \"465\" ) : session = self . get ssl ( ) elif self . port in ( 587 , \"587\" ) : session = self . get tls ( ) try : session . login ( self . from , self . auth ) except SMTP Response Exception as e : raise Message Send Error ( e . smtp error . decode ( \"unicode escape\" ) ) return session", "predictions": ["newick and returns a session session object"], "references": ["start session with email server ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 4654, "code": "def get ssl ( self ) : return smtplib . SMTP SSL ( self . server , self . port , context = ssl . create default context ( ) )", "predictions": ["remove an internal internal internal internal internal internal internal internal method ."], "references": ["get an smtp session with ssl ."], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 4655, "code": "def get tls ( self ) : session = smtplib . SMTP ( self . server , self . port ) session . ehlo ( ) session . starttls ( context = ssl . create default context ( ) ) session . ehlo ( ) return session", "predictions": ["return session instance lambda function lambda"], "references": ["get an smtp session with tls ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4656, "code": "def delete ( self , filename = None ) : if self . tags is not None : if filename is None : filename = self . filename else : warnings . warn ( \"delete(filename=...) is deprecated, reload the file\" , Deprecation Warning ) return self . tags . delete ( filename )", "predictions": ["auth method to auth file self self self self self self self self self self self self self self self self self self self self self self self self self self"], "references": ["remove tags from a file ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 4657, "code": "def save ( self , filename = None , * * kwargs ) : if filename is None : filename = self . filename else : warnings . warn ( \"save(filename=...) is deprecated, reload the file\" , Deprecation Warning ) if self . tags is not None : return self . tags . save ( filename , * * kwargs ) else : raise Value Error ( \"no tags in file\" )", "predictions": ["require must be a string or a callable"], "references": ["save metadata tags ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4658, "code": "def unload ( self ) : if self . handle != - 1 : lib . Unload Image ( self . handle ) self . handle = - 1", "predictions": ["get the correct position of the field default default default default default = 1 default default default default ."], "references": ["releases renderer resources associated with this image ."], "bleu": 0.06439931429457924, "rouge_l": 0.0799475753604194}
{"id": 4659, "code": "def clear ( self ) : for i in list ( self . internal ) : self . internal . remove ( i )", "predictions": ["clear all internal in the pool domain domain domain domain domain domain domain domain domain domain domain domain domain domain domain"], "references": ["clear all keys from the comment ."], "bleu": 0.0821610732492254, "rouge_l": 0.23552123552123552}
{"id": 4660, "code": "def read ( self ) : self . fileobj . seek ( self . data offset ) self . data = self . fileobj . read ( self . data size )", "predictions": ["get the contents of the data . . ."], "references": ["read the chunks data"], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 4661, "code": "def delete ( self ) : delete bytes ( self . fileobj , self . size , self . offset ) if self . parent chunk is not None : self . parent chunk . resize ( self . parent chunk . data size - self . size )", "predictions": ["add this normalize to the dict ."], "references": ["removes the chunk from the file"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 4662, "code": "def resize ( self , data size ) : self . fileobj . seek ( self . offset + 4 ) self . fileobj . write ( pack ( '>I' , data size ) ) if self . parent chunk is not None : size diff = self . data size - data size self . parent chunk . resize ( self . parent chunk . data size - size diff ) self . data size = data size self . size = data size + self . HEADER SIZE", "predictions": ["get the data from the file"], "references": ["update the size of the chunk"], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 4663, "code": "def insert chunk ( self , id ) : if not isinstance ( id , text type ) : id = id . decode ( 'ascii' ) if not is valid chunk id ( id ) : raise Key Error ( \"AIFF key must be four ASCII characters.\" ) self . fileobj . seek ( self . next offset ) self . fileobj . write ( pack ( '>4si' , id . ljust ( 4 ) . encode ( 'ascii' ) , 0 ) ) self . fileobj . seek ( self . next offset ) chunk = IFF Chunk ( self . fileobj , self [ u'FORM' ] ) self [ u'FORM' ] . resize ( self [ u'FORM' ] . data size + chunk . size ) self . chunks [ id ] = chunk self . next offset = chunk . offset + chunk . size", "predictions": ["select a chunk by its origin"], "references": ["insert a new chunk at the end of the iff file"], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 4664, "code": "def save ( self , filename = None , v2 version = 4 , v23 sep = '/' ) : framedata = self . prepare framedata ( v2 version , v23 sep ) framesize = len ( framedata ) if filename is None : filename = self . filename fileobj = open ( filename , 'rb+' ) iff file = IFF File ( fileobj ) try : if u'ID3' not in iff file : iff file . insert chunk ( u'ID3' ) chunk = iff file [ u'ID3' ] fileobj . seek ( chunk . data offset ) header = fileobj . read ( 10 ) header = self . prepare id3 header ( header , framesize , v2 version ) header , new size , = header data = header + framedata + ( b'\\x00' * ( new size - framesize ) ) new size += 10 if new size > chunk . size : insert at = chunk . offset + chunk . size insert size = new size - chunk . size + new size % 2 insert bytes ( fileobj , insert size , insert at ) chunk . resize ( new size ) fileobj . seek ( chunk . data offset ) fileobj . write ( data ) finally : fileobj . close ( )", "predictions": ["get the values of the id3 in the given file in the given environment"], "references": ["save id3v2 data to the aiff file"], "bleu": 0.09782375748961449, "rouge_l": 0.2026578073089701}
{"id": 4665, "code": "def delete ( self , filename = None ) : if filename is None : filename = self . filename delete ( filename ) self . clear ( )", "predictions": ["get the file from the cache ."], "references": ["completely removes the id3 chunk from the aiff file"], "bleu": 0.20873176328735715, "rouge_l": 0.3667334669338677}
{"id": 4666, "code": "def load ( self , filename , * * kwargs ) : self . filename = filename try : self . tags = IFFID3 ( filename , * * kwargs ) except ID3Error : self . tags = None try : fileobj = open ( filename , \"rb\" ) self . info = AIFF Info ( fileobj ) finally : fileobj . close ( )", "predictions": ["construct the object from disk"], "references": ["load stream and tag information from a file ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 4667, "code": "def parse file ( self , filename ) : self . reset ( ) self . filename = filename fileinput . close ( ) self . format = None self . lineno = 0 self . lines = [ ] for line in fileinput . input ( filename ) : if line [ - 1 ] == '\\012' : line = line [ 0 : - 1 ] if self . format == None : self . process normal line ( line ) else : if self . format . end . match ( line ) : self . lines . append ( line ) self . add block lines ( ) elif self . format . column . match ( line ) : self . lines . append ( line ) else : self . add block lines ( ) self . process normal line ( line ) self . add block lines ( )", "predictions": ["send a file command to a file"], "references": ["parse a c source file and add its blocks to the processor s list"], "bleu": 0.0812630644213965, "rouge_l": 0.2695139911634757}
{"id": 4668, "code": "def process normal line ( self , line ) : for f in re source block formats : if f . start . match ( line ) : self . add block lines ( ) self . format = f self . lineno = fileinput . filelineno ( ) self . lines . append ( line )", "predictions": ["validate a input line line"], "references": ["process a normal line and check whether it is the start of a new block"], "bleu": 0.040889869516541145, "rouge_l": 0.18345864661654135}
{"id": 4669, "code": "def add block lines ( self ) : if self . lines != [ ] : block = Source Block ( self , self . filename , self . lineno , self . lines ) self . blocks . append ( block ) self . format = None self . lines = [ ]", "predictions": ["validate the twilio lines lines"], "references": ["add the current accumulated lines and create a new block"], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 4670, "code": "def make html words ( self , words ) : line = \"\" if words : line = html quote ( words [ 0 ] ) for w in words [ 1 : ] : line = line + \" \" + html quote ( w ) return line", "predictions": ["validate html words and return ."], "references": ["convert a series of simple words into some html text"], "bleu": 0.1255107248036171, "rouge_l": 0.11960784313725491}
{"id": 4671, "code": "def make html word ( self , word ) : m = re crossref . match ( word ) if m : try : name = m . group ( 1 ) rest = m . group ( 2 ) block = self . identifiers [ name ] url = self . make block url ( block ) return '<a href=\"' + url + '\">' + name + '</a>' + rest except : sys . stderr . write ( \"WARNING: undefined cross reference '\" + name + \"'.\\n\" ) return '?' + name + '?' + rest m = re italic . match ( word ) if m : name = m . group ( 1 ) rest = m . group ( 3 ) return '<i>' + name + '</i>' + rest m = re bold . match ( word ) if m : name = m . group ( 1 ) rest = m . group ( 3 ) return '<b>' + name + '</b>' + rest return html quote ( word )", "predictions": ["transform whatsapp word into whatsapp word"], "references": ["analyze a simple word to detect cross - references and styling"], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 4672, "code": "def make html para ( self , words ) : line = \"\" if words : line = self . make html word ( words [ 0 ] ) for word in words [ 1 : ] : line = line + \" \" + self . make html word ( word ) line = re . sub ( r\"(^|\\W)`(.*?)'(\\W|$)\" , r'\\1&lsquo;\\2&rsquo;\\3' , line ) line = string . replace ( line , \"~\" , \"&nbsp;\" ) return para header + line + para footer", "predictions": ["make html para - para html from words ."], "references": ["convert words of a paragraph into tagged html text handle xrefs"], "bleu": 0.12507277759788113, "rouge_l": 0.09822866344605477}
{"id": 4673, "code": "def make html code ( self , lines ) : line = code header + '\\n' for l in lines : line = line + html quote ( l ) + '\\n' return line + code footer", "predictions": ["make a html code code from the given lines ."], "references": ["convert a code sequence to html"], "bleu": 0.14991106946711685, "rouge_l": 0.26180257510729615}
{"id": 4674, "code": "def make html items ( self , items ) : lines = [ ] for item in items : if item . lines : lines . append ( self . make html code ( item . lines ) ) else : lines . append ( self . make html para ( item . words ) ) return string . join ( lines , '\\n' )", "predictions": ["make html items from items ."], "references": ["convert a field s content into some valid html"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4675, "code": "def save ( self , filename ) : values = [ ] items = sorted ( self . items ( ) , key = MP4Tags . get sort stats ) for key , value in items : info = self . atoms . get ( key [ : 4 ] , ( None , type ( self ) . render text ) ) try : values . append ( info [ 1 ] ( self , key , value , * info [ 2 : ] ) ) except ( Type Error , Value Error ) as s : reraise ( MP4Metadata Value Error , s , sys . exc info ( ) [ 2 ] ) data = Atom . render ( b\"ilst\" , b\"\" . join ( values ) ) fileobj = open ( filename , \"rb+\" ) try : atoms = Atoms ( fileobj ) try : path = atoms . path ( b\"moov\" , b\"udta\" , b\"meta\" , b\"ilst\" ) except Key Error : self . save new ( fileobj , atoms , data ) else : self . save existing ( fileobj , atoms , path , data ) finally : fileobj . close ( )", "predictions": ["save the values to a file"], "references": ["save the metadata to the given filename ."], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 4676, "code": "def update parents ( self , fileobj , path , delta ) : for atom in path : fileobj . seek ( atom . offset ) size = cdata . uint be ( fileobj . read ( 4 ) ) if size == 1 : size = cdata . ulonglong be ( fileobj . read ( 12 ) [ 4 : ] ) fileobj . seek ( atom . offset + 8 ) fileobj . write ( cdata . to ulonglong be ( size + delta ) ) else : fileobj . seek ( atom . offset ) fileobj . write ( cdata . to uint be ( size + delta ) )", "predictions": ["update the parents of the given path to the specified path ."], "references": ["update all parent atoms with the new size ."], "bleu": 0.1235622127262679, "rouge_l": 0.2932692307692307}
{"id": 4677, "code": "def load ( self , filename ) : self . filename = filename fileobj = open ( filename , \"rb\" ) try : data = AP Ev2Data ( fileobj ) finally : fileobj . close ( ) if data . tag : self . clear ( ) self . casemap . clear ( ) self . parse tag ( data . tag , data . items ) else : raise APE No Header Error ( \"No APE tag found\" )", "predictions": ["load the ape from disk"], "references": ["load tags from a filename ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 4678, "code": "def delete ( self , filename = None ) : filename = filename or self . filename fileobj = open ( filename , \"r+b\" ) try : data = AP Ev2Data ( fileobj ) if data . start is not None and data . size is not None : delete bytes ( fileobj , data . end - data . start , data . start ) finally : fileobj . close ( ) self . clear ( )", "predictions": ["delete the file from disk ."], "references": ["remove tags from a file ."], "bleu": 0.2626909894424158, "rouge_l": 0.3333333333333333}
{"id": 4679, "code": "def size ( self ) : header size = 27 for datum in self . packets : quot , rem = divmod ( len ( datum ) , 255 ) header size += quot + 1 if not self . complete and rem == 0 : header size -= 1 header size += sum ( map ( len , self . packets ) ) return header size", "predictions": ["total size of the underlying packets ."], "references": ["total frame size ."], "bleu": 0.22089591134157885, "rouge_l": 0.5736677115987461}
{"id": 4680, "code": "def load ( self , filename ) : self . filename = filename fileobj = open ( filename , \"rb\" ) try : try : self . info = self . Info ( fileobj ) self . tags = self . Tags ( fileobj , self . info ) self . info . post tags ( fileobj ) except error as e : reraise ( self . Error , e , sys . exc info ( ) [ 2 ] ) except EOF Error : raise self . Error ( \"no appropriate stream found\" ) finally : fileobj . close ( )", "predictions": ["load the stream from disk"], "references": ["load file information from a filename ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 4681, "code": "def set section ( self , section name ) : if not self . sections . has key ( section name ) : section = Doc Section ( section name ) self . sections [ section name ] = section self . section = section else : self . section = self . sections [ section name ]", "predictions": ["set a section ."], "references": ["set current section during parsing"], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 4682, "code": "def add markup ( self ) : if self . markup and self . markup lines : marks = self . markup lines if len ( marks ) > 0 and not string . strip ( marks [ - 1 ] ) : self . markup lines = marks [ : - 1 ] m = Doc Markup ( self . markup , self . markup lines ) self . markups . append ( m ) self . markup = None self . markup lines = [ ]", "predictions": ["add markup markup to markup"], "references": ["add a new markup section"], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 4683, "code": "def get markup ( self , tag name ) : for m in self . markups : if m . tag == string . lower ( tag name ) : return m return None", "predictions": ["return the markup by tag name ."], "references": ["return the docmarkup corresponding to a given tag in a block"], "bleu": 0.14834636222628117, "rouge_l": 0.32049036777583184}
{"id": 4684, "code": "def utf8 ( data ) : if isinstance ( data , bytes ) : return data . decode ( \"utf-8\" , \"replace\" ) . encode ( \"utf-8\" ) elif isinstance ( data , text type ) : return data . encode ( \"utf-8\" ) else : raise Type Error ( \"only unicode/bytes types can be converted to UTF-8\" )", "predictions": ["convert utf8 data to binary ."], "references": ["convert a basestring to a valid utf - 8 str ."], "bleu": 0.1141650334026257, "rouge_l": 0.33516483516483514}
{"id": 4685, "code": "def delete ( self ) : cset = Change Set ( connection = self . connection , hosted zone id = self . zone id ) cset . add change ( 'DELETE' , self ) return self . connection . change resource record sets ( cset )", "predictions": ["delete the resource record ."], "references": ["deletes this record set ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 4686, "code": "def save ( self ) : cset = Change Set ( connection = self . connection , hosted zone id = self . zone id ) cset . add change ( 'DELETE' , self ) cset . add change ( 'CREATE' , self ) retval = self . connection . change resource record sets ( cset ) for key , val in self . initial vals . items ( ) : self . initial vals [ key ] = getattr ( self , key ) return retval", "predictions": ["saves the current resource ."], "references": ["saves any changes to this record set ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 4687, "code": "def Parse ID3v1 ( data ) : try : data = data [ data . index ( b'TAG' ) : ] except Value Error : return None if 128 < len ( data ) or len ( data ) < 124 : return None unpack fmt = \"3s30s30s30s%ds29s BB\" % ( len ( data ) - 124 ) try : tag , title , artist , album , year , comment , track , genre = unpack ( unpack fmt , data ) except Struct Error : return None if tag != b\"TAG\" : return None def fix ( data ) : return data . split ( b'\\x00' ) [ 0 ] . strip ( ) . decode ( 'latin1' ) title , artist , album , year , comment = map ( fix , [ title , artist , album , year , comment ] ) frames = { } if title : frames [ 'TIT2' ] = TIT2 ( encoding = 0 , text = title ) if artist : frames [ 'TPE1' ] = TPE1 ( encoding = 0 , text = [ artist ] ) if album : frames [ 'TALB' ] = TALB ( encoding = 0 , text = album ) if year : frames [ 'TDRC' ] = TDRC ( encoding = 0 , text = year ) if comment : frames [ 'COMM' ] = COMM ( encoding = 0 , lang = 'eng' , desc = \"ID3v1 Comment\" , text = comment ) if track and ( ( track != 32 ) or ( data [ - 3 ] == b'\\x00' [ 0 ] ) ) : frames [ 'TRCK' ] = TRCK ( encoding = 0 , text = str ( track ) ) if genre != 255 : frames [ 'TCON' ] = TCON ( encoding = 0 , text = str ( genre ) ) return frames", "predictions": ["parses the data into a list of frames ."], "references": ["parse an id3v1 tag returning a list of id3v2 . 4 frames ."], "bleu": 0.20030090221863772, "rouge_l": 0.4401154401154401}
{"id": 4688, "code": "def Make ID3v1 ( id3 ) : v1 = { } for v2id , name in { \"TIT2\" : \"title\" , \"TPE1\" : \"artist\" , \"TALB\" : \"album\" } . items ( ) : if v2id in id3 : text = id3 [ v2id ] . text [ 0 ] . encode ( 'latin1' , 'replace' ) [ : 30 ] else : text = b'' v1 [ name ] = text + ( b'\\x00' * ( 30 - len ( text ) ) ) if \"COMM\" in id3 : cmnt = id3 [ \"COMM\" ] . text [ 0 ] . encode ( 'latin1' , 'replace' ) [ : 28 ] else : cmnt = b'' v1 [ 'comment' ] = cmnt + ( b'\\x00' * ( 29 - len ( cmnt ) ) ) if \"TRCK\" in id3 : try : v1 [ \"track\" ] = chr ( + id3 [ \"TRCK\" ] ) except Value Error : v1 [ \"track\" ] = b'\\x00' else : v1 [ \"track\" ] = b'\\x00' if \"TCON\" in id3 : try : genre = id3 [ \"TCON\" ] . genres [ 0 ] except Index Error : pass else : if genre in TCON . GENRES : v1 [ \"genre\" ] = chr ( TCON . GENRES . index ( genre ) ) if \"genre\" not in v1 : v1 [ \"genre\" ] = b\"\\xff\" if \"TDRC\" in id3 : year = text type ( id3 [ \"TDRC\" ] ) . encode ( 'latin1' , 'replace' ) elif \"TYER\" in id3 : year = text type ( id3 [ \"TYER\" ] ) . encode ( 'latin1' , 'replace' ) else : year = b'' v1 [ 'year' ] = ( year + b'\\x00\\x00\\x00\\x00' ) [ : 4 ] return ( b'TAG' + v1 [ 'title' ] + v1 [ 'artist' ] + v1 [ 'album' ] + v1 [ 'year' ] + v1 [ 'comment' ] + v1 [ 'track' ] + v1 [ 'genre' ] )", "predictions": ["return the unicode string from the id3 ."], "references": ["return an id3v1 . 1 tag string from a dict of id3v2 . 4 frames ."], "bleu": 0.08821858171866302, "rouge_l": 0.31443298969072164}
{"id": 4689, "code": "def fullread ( self , size ) : try : if size < 0 : raise Value Error ( 'Requested bytes (%s) less than zero' % size ) if size > self . filesize : raise EOF Error ( 'Requested %#x of %#x (%s)' % ( int ( size ) , int ( self . filesize ) , self . filename ) ) except Attribute Error : pass data = self . fileobj . read ( size ) if len ( data ) != size : raise EOF Error self . readbytes += size return data", "predictions": ["return the first bit of the file ."], "references": ["read a certain number of bytes from the source file ."], "bleu": 0.16481400866629634, "rouge_l": 0.4093959731543625}
{"id": 4690, "code": "def delall ( self , key ) : if key in self : del ( self [ key ] ) else : key = key + \":\" for k in self . keys ( ) : if k . startswith ( key ) : del ( self [ k ] )", "predictions": ["remove a key from the dict ."], "references": ["delete all tags of a given kind ; see getall ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 4691, "code": "def loaded frame ( self , tag ) : if len ( type ( tag ) . name ) == 3 : tag = type ( tag ) . base ( tag ) self [ tag . Hash Key ] = tag", "predictions": ["add a frame to the tag"], "references": ["deprecated ; use the add method ."], "bleu": 0.20693220168471366, "rouge_l": 0.1517412935323383}
{"id": 4692, "code": "def update common ( self ) : if \"TCON\" in self : self [ \"TCON\" ] . genres = self [ \"TCON\" ] . genres if self . version < self . V23 : pics = self . getall ( \"APIC\" ) mimes = { \"PNG\" : \"image/png\" , \"JPG\" : \"image/jpeg\" } self . delall ( \"APIC\" ) for pic in pics : newpic = APIC ( encoding = pic . encoding , mime = mimes . get ( pic . mime , pic . mime ) , type = pic . type , desc = pic . desc , data = pic . data ) self . add ( newpic ) self . delall ( \"LINK\" )", "predictions": ["update common common common common data from delall ."], "references": ["updates done by both v23 and v24 update"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 4693, "code": "def unload ( self ) : if self . handle != - 1 : lib . Unload Sound ( self . handle ) self . handle = - 1", "predictions": ["unload the handle ."], "references": ["release all resources associated with the sound ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 4694, "code": "def adobe glyph values ( ) : lines = string . split ( adobe glyph list , '\\n' ) glyphs = [ ] values = [ ] for line in lines : if line : fields = string . split ( line , ';' ) subfields = string . split ( fields [ 1 ] , ' ' ) if len ( subfields ) == 1 : glyphs . append ( fields [ 0 ] ) values . append ( fields [ 1 ] ) return glyphs , values", "predictions": ["return a list of glyphs values from the adobe"], "references": ["return the list of glyph names and their unicode values"], "bleu": 0.19766634639198594, "rouge_l": 0.41709401709401706}
{"id": 4695, "code": "def filter glyph names ( alist , filter ) : count = 0 extras = [ ] for name in alist : try : filtered index = filter . index ( name ) except : extras . append ( name ) return extras", "predictions": ["filter glyph names to filter glyph names"], "references": ["filter alist by taking _out_ all glyph names that are in filter"], "bleu": 0.13597602315271134, "rouge_l": 0.40197693574958815}
{"id": 4696, "code": "def dump encoding ( file , encoding name , encoding list ) : write = file . write write ( \"  /* the following are indices into the SID name table */\\n\" ) write ( \"  static const unsigned short  \" + encoding name + \"[\" + repr ( len ( encoding list ) ) + \"] =\\n\" ) write ( \"  {\\n\" ) line = \"    \" comma = \"\" col = 0 for value in encoding list : line += comma line += \"%3d\" % value comma = \",\" col += 1 if col == 16 : col = 0 comma = \",\\n    \" write ( line + \"\\n  };\\n\\n\\n\" )", "predictions": ["dump the encoding encoding to a file - like object ."], "references": ["dump a given encoding"], "bleu": 0.1354599427337814, "rouge_l": 0.2911694510739857}
{"id": 4697, "code": "def dump array ( the array , write , array name ) : write ( \"  static const unsigned char  \" + array name + \"[\" + repr ( len ( the array ) ) + \"L] =\\n\" ) write ( \"  {\\n\" ) line = \"\" comma = \"    \" col = 0 for value in the array : line += comma line += \"%3d\" % ord ( value ) comma = \",\" col += 1 if col == 16 : col = 0 comma = \",\\n    \" if len ( line ) > 1024 : write ( line ) line = \"\" write ( line + \"\\n  };\\n\\n\\n\" )", "predictions": ["dump an array of data to a unsigned array"], "references": ["dumps a given encoding"], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 4698, "code": "def file exists ( pathname ) : result = 1 try : file = open ( pathname , \"r\" ) file . close ( ) except : result = None sys . stderr . write ( pathname + \" couldn't be accessed\\n\" ) return result", "predictions": ["check if a file exists ."], "references": ["checks that a given file exists"], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 4699, "code": "def make file list ( args = None ) : file list = [ ] if not args : args = sys . argv [ 1 : ] for pathname in args : if string . find ( pathname , '*' ) >= 0 : newpath = glob . glob ( pathname ) newpath . sort ( ) else : newpath = [ pathname ] file list . extend ( newpath ) if len ( file list ) == 0 : file list = None else : file list = filter ( file exists , file list ) return file list", "predictions": ["make a list of file names from the argv list ."], "references": ["builds a list of input files from command - line arguments"], "bleu": 0.22416933501922287, "rouge_l": 0.36363636363636365}
{"id": 4700, "code": "def writeblocks ( blocks ) : data = [ ] codes = [ [ block . code , block . write ( ) ] for block in blocks ] codes [ - 1 ] [ 0 ] |= 128 for code , datum in codes : byte = chr ( code ) if len ( datum ) > 2 ** 24 : raise error ( \"block is too long to write\" ) length = struct . pack ( \">I\" , len ( datum ) ) [ - 3 : ] data . append ( byte + length + datum ) return b\"\" . join ( data )", "predictions": ["convert a string to an write\""], "references": ["render metadata block as a byte string ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 4701, "code": "def load ( self , filename ) : self . metadata blocks = [ ] self . tags = None self . cuesheet = None self . seektable = None self . filename = filename fileobj = Strict File Object ( open ( filename , \"rb\" ) ) try : self . check header ( fileobj ) while self . read metadata block ( fileobj ) : pass finally : fileobj . close ( ) try : self . metadata blocks [ 0 ] . length except ( Attribute Error , Index Error ) : raise FLAC No Header Error ( \"Stream info block not found\" )", "predictions": ["load the metadata from the metadata file"], "references": ["load file information from a filename ."], "bleu": 0.22089591134157885, "rouge_l": 0.2857142857142857}
{"id": 4702, "code": "def init logs ( ) : start time = dt . fromtimestamp ( time . time ( ) ) . strftime ( '%Y%m%d %H%M' ) logname = os . path . join ( os . path . expanduser ( \"~\" ) + \"/nano GUI \" + start time + \".log\" ) handlers = [ logging . File Handler ( logname ) ] logging . basic Config ( format = '%(asctime)s %(message)s' , handlers = handlers , level = logging . INFO ) logging . info ( 'Nano GUI {} started with Nano Plot {}' . format ( version , nanoplot . version ) ) logging . info ( 'Python version is: {}' . format ( sys . version . replace ( '\\n' , ' ' ) ) ) return logname", "predictions": ["initialize the module ."], "references": ["initiate log file ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 4703, "code": "def alias item ( self , alias ) : ident = self . alias [ alias ] return self . items [ ident ]", "predictions": ["return an alias for an alias ."], "references": ["gets an item by its alias ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 4704, "code": "def initialize bars ( self , sender = None , * * kwargs ) : for bar in self . bars . values ( ) : for initializer in bar . initializers : initializer ( self )", "predictions": ["initializes all html html html ."], "references": ["calls the initializers of all bound navigation bars ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 4705, "code": "def bind bar ( self , sender = None , * * kwargs ) : bar = kwargs . pop ( 'bar' ) self . bars [ bar . name ] = bar", "predictions": ["make this html html html html html + html + html + html + html + html + html html + html + html + html + html + html html"], "references": ["binds a navigation bar into this extension instance ."], "bleu": 0.03901663112717908, "rouge_l": 0.055505004549590536}
{"id": 4706, "code": "def validate ( metric class ) : if not hasattr ( metric class , 'label' ) : raise Improperly Configured ( \"No 'label' attribute found for metric %s.\" % metric class . name ) if not hasattr ( metric class , 'widget' ) : raise Improperly Configured ( \"No 'widget' attribute found for metric %s.\" % metric class . name )", "predictions": ["make sure that the items passed as a items in the items ."], "references": ["does basic metric option validation ."], "bleu": 0.09552040806823771, "rouge_l": 0.11275415896487984}
{"id": 4707, "code": "def calculate statistics ( stat , frequencies ) : stats = ensure list ( stat ) frequencies = ensure list ( frequencies ) for stat in stats : for f in frequencies : print \"Calculating %s (%s)...\" % ( stat . name , settings . STATISTIC FREQUENCY DICT [ f ] ) stat . calculate ( f )", "predictions": ["save get statistics statistics statistics ."], "references": ["calculates all of the metrics associated with the registered gadgets ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 4708, "code": "def handle ( self , * args , * * kwargs ) : frequency = kwargs [ 'frequency' ] frequencies = settings . STATISTIC FREQUENCY ALL if frequency == 'a' else ( frequency . split ( ',' ) if ',' in frequency else [ frequency ] ) if kwargs [ 'list' ] : maintenance . list statistics ( ) elif kwargs [ 'calculate' ] : maintenance . calculate statistics ( maintenance . get statistic by name ( kwargs [ 'calculate' ] ) , frequencies ) elif kwargs [ 'reset' ] : maintenance . reset statistics ( maintenance . get statistic by name ( kwargs [ 'reset' ] ) , frequencies , kwargs [ 'reset cumulative' ] ) elif kwargs [ 'recalculate' ] : maintenance . reset statistics ( maintenance . get statistic by name ( kwargs [ 'recalculate' ] ) , frequencies , kwargs [ 'reset cumulative' ] , True )", "predictions": ["gets or updates the reset + default + reset"], "references": ["command handler for the metrics command ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 4709, "code": "def get GET array ( request , var name , fail silently = True ) : vals = request . GET . getlist ( var name ) if not vals : if fail silently : return [ ] else : raise Exception , ( \"No array called '%(varname)s' in GET variables\" ) % { 'varname' : var name } return vals", "predictions": ["tag an self . from the given var"], "references": ["returns the get array s contents for the specified variable ."], "bleu": 0.12197601375336842, "rouge_l": 0.10234899328859062}
{"id": 4710, "code": "def get GET bool ( request , var name , default = True ) : val = request . GET . get ( var name , default ) if isinstance ( val , str ) or isinstance ( val , unicode ) : val = True if val [ 0 ] == 't' else False return val", "predictions": ["delete a value from a string or default"], "references": ["tries to extract a boolean variable from the specified request ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 4711, "code": "def get next colour ( ) : colour = settings . GECKOBOARD COLOURS [ get next colour . cur colour ] get next colour . cur colour += 1 if get next colour . cur colour >= len ( settings . GECKOBOARD COLOURS ) : get next colour . cur colour = 0 return colour", "predictions": ["size of the next self for the next"], "references": ["gets the next colour in the geckoboard colour list ."], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 4712, "code": "def geckoboard number widget ( request ) : params = get gecko params ( request , days back = 7 ) metric = Metric . objects . get ( uid = params [ 'uid' ] ) try : latest stat = metric . statistics . filter ( frequency = params [ 'frequency' ] ) . order by ( '-date time' ) [ 0 ] except Index Error : return ( 0 , 0 ) try : prev stat = metric . statistics . filter ( frequency = params [ 'frequency' ] , date time lte = latest stat . date time - timedelta ( days = params [ 'days back' ] ) ) . order by ( '-date time' ) [ 0 ] except Index Error : return ( latest stat . cumulative count , 0 ) if params [ 'cumulative' ] else ( latest stat . count , 0 ) return ( latest stat . cumulative count , prev stat . cumulative count ) if params [ 'cumulative' ] else ( latest stat . count , prev stat . count )", "predictions": ["get the number self post post"], "references": ["returns a number widget for the specified metric s cumulative total ."], "bleu": 0.08993236413460196, "rouge_l": 0.10481099656357389}
{"id": 4713, "code": "def geckoboard line chart ( request ) : params = get gecko params ( request , cumulative = False , days back = 7 ) metric = Metric . objects . get ( uid = params [ 'uid' ] ) start date = datetime . now ( ) - timedelta ( days = params [ 'days back' ] ) stats = [ s for s in metric . statistics . filter ( frequency = params [ 'frequency' ] , date time gte = start date ) . order by ( 'date time' ) ] if len ( stats ) == 0 : raise Exception , ( \"No statistics for metric %(metric)s.\" ) % { 'metric' : params [ 'uid' ] } dates = [ stats [ 0 ] . date time ] if len ( stats ) >= 3 : mid = len ( stats ) / 2 if not mid : mid = 1 dates . extend ( [ stats [ mid ] . date time , stats [ - 1 ] . date time ] ) elif len ( stats ) == 2 : dates . extend ( [ stats [ - 1 ] . date time ] ) return ( [ s . count for s in stats ] , dates , metric . title , )", "predictions": ["chart the mid of the . ."], "references": ["returns the data for a line chart for the specified metric ."], "bleu": 0.11434175042957104, "rouge_l": 0.30148270181219106}
{"id": 4714, "code": "def geckoboard geckometer ( request ) : params = get gecko params ( request , cumulative = True ) metric = Metric . objects . get ( uid = params [ 'uid' ] ) return ( metric . latest count ( frequency = params [ 'frequency' ] , count = not params [ 'cumulative' ] , cumulative = params [ 'cumulative' ] ) , params [ 'min' ] , params [ 'max' ] )", "predictions": ["return the m of the m not markup not in the m not = 0 not 0 not 0 not 0 not 0 not 0 not the user"], "references": ["returns a geck - o - meter control for the specified metric ."], "bleu": 0.04327969719414172, "rouge_l": 0.05222602739726027}
{"id": 4715, "code": "def geckoboard funnel ( request , frequency = settings . STATISTIC FREQUENCY DAILY ) : params = get gecko params ( request , cumulative = True ) metrics = Metric . objects . filter ( uid in = params [ 'uids' ] ) items = [ ( metric . latest count ( frequency = params [ 'frequency' ] , count = not params [ 'cumulative' ] , cumulative = params [ 'cumulative' ] ) , metric . title ) for metric in metrics ] return { 'items' : items , 'type' : params [ 'type' ] , 'percentage' : params [ 'percentage' ] , 'sort' : params [ 'sort' ] , }", "predictions": ["return the get metrics metrics"], "references": ["returns a funnel chart for the metrics specified in the get variables ."], "bleu": 0.07795171967670728, "rouge_l": 0.20573355817875214}
{"id": 4716, "code": "def get active stats ( self ) : stats = [ ] for gadget in self . registry . values ( ) : for s in gadget . stats : if s not in stats : stats . append ( s ) return stats", "predictions": ["return data for the active encode the active . . . . . . ."], "references": ["returns all of the active statistics for the gadgets currently registered ."], "bleu": 0.14247788801610148, "rouge_l": 0.30235439900867406}
{"id": 4717, "code": "def get context data ( self , * * kwargs ) : #max columns, max rows = self.get max dimension() context = { 'gadgets' : self . registry , 'columns' : self . columns , 'rows' : self . rows , 'column ratio' : 100 - self . columns * 2 , 'row ratio' : 100 - self . rows * 2 , } context . update ( kwargs ) return context", "predictions": ["returns the context self resource resource self resource resource resource resource resource resource resource resource resource resource resource resource resource resource resource resource resource resource resource resource self resource resource resource"], "references": ["get the context for this view ."], "bleu": 0.0513487742994337, "rouge_l": 0.11879259980525803}
{"id": 4718, "code": "def error ( self , message , code = 1 ) : print >> sys . stderr , message sys . exit ( code )", "predictions": [". save save save save save save save to id zone zone zone zone zone zone"], "references": ["print error and stop command"], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 4719, "code": "def valid ( schema = None ) : def dec ( fun ) : @ wraps ( fun ) def d func ( self , ctx , data , * a , * * kw ) : try : validate ( data [ 'params' ] , schema ) except Validation Error as err : raise Invalid Params ( err ) except Schema Error as err : raise Internal Error ( err ) return fun ( self , ctx , data [ 'params' ] , * a , * * kw ) return d func return dec", "predictions": ["check if a json json object is valid ."], "references": ["validation data by specific validictory configuration"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4720, "code": "def long input ( prompt = 'Multi-line input\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxlines = None , maxlength = None ) : lines = [ ] print ( prompt ) lnum = 1 try : while True : if maxlines : if lnum > maxlines : break else : if maxlength : lines . append ( string input ( '' ) [ : maxlength ] ) else : lines . append ( string input ( '' ) ) lnum += 1 else : if maxlength : lines . append ( string input ( '' ) [ : maxlength ] ) else : lines . append ( string input ( '' ) ) except EOF Error : pass finally : return '\\n' . join ( lines )", "predictions": ["return a string with the input input input 28 28 28 28 28 28"], "references": ["get a multi - line string as input"], "bleu": 0.10511846841633776, "rouge_l": 0.28683385579937304}
{"id": 4721, "code": "def list input ( prompt = 'List input - enter each item on a seperate line\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxitems = None , maxlength = None ) : lines = [ ] print ( prompt ) inum = 1 try : while True : if maxitems : if inum > maxitems : break else : if maxlength : lines . append ( string input ( '' ) [ : maxlength ] ) else : lines . append ( string input ( '' ) ) inum += 1 else : if maxlength : lines . append ( string input ( '' ) [ : maxlength ] ) else : lines . append ( string input ( '' ) ) except EOF Error : pass finally : return lines", "predictions": ["return for list ."], "references": ["get a list of strings as input"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4722, "code": "def outfile input ( extension = None ) : fileok = False while not fileok : filename = string input ( 'File name? ' ) if extension : if not filename . endswith ( extension ) : if extension . startswith ( '.' ) : filename = filename + extension else : filename = filename + '.' + extension if os . path . isfile ( filename ) : choice = choice input ( prompt = filename + ' already exists. Overwrite?' , options = [ 'y' , 'n' ] ) if choice == 'y' : try : nowtime = time . time ( ) with open ( filename , 'a' ) as f : os . utime ( filename , ( nowtime , nowtime ) ) fileok = True except IO Error : print ( 'Write permission denied on ' + filename + '. Try again.' ) except Permission Error : print ( 'Write permission denied on ' + filename + '. Try again.' ) except File Not Found Error : print ( filename + ': directory not found. Try again.' ) else : choice = choice input ( prompt = filename + ' does not exist. Create it?' , options = [ 'y' , 'n' ] ) if choice == 'y' : try : nowtime = time . time ( ) with open ( filename , 'w' ) as f : os . utime ( filename , ( nowtime , nowtime ) ) fileok = True except IO Error : print ( 'Write permission denied on ' + filename + '. Try again.' ) except Permission Error : print ( 'Write permission denied on ' + filename + '. Try again.' ) except File Not Found Error : print ( filename + ': directory not found. Try again.' ) return filename", "predictions": ["check for outfile input"], "references": ["get an output file name as input"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 4723, "code": "def winner ( self ) : hm Score = self . home score ( ) aw Score = self . away score ( ) if hm Score > aw Score : return self . home ( ) elif hm Score < aw Score : return self . away ( ) else : return None", "predictions": ["the name of the name of the name of the name if any ."], "references": ["returns the team id of the winning team . returns nan if a tie ."], "bleu": 0.12880665550475626, "rouge_l": 0.34269662921348315}
{"id": 4724, "code": "def standings ( self ) : doc = self . get sub doc ( 'standings' ) east table = doc ( 'table#divs standings E' ) east df = pd . Data Frame ( sportsref . utils . parse table ( east table ) ) east df . sort values ( 'wins' , ascending = False , inplace = True ) east df [ 'seed' ] = range ( 1 , len ( east df ) + 1 ) east df [ 'conference' ] = 'E' west table = doc ( 'table#divs standings W' ) west df = sportsref . utils . parse table ( west table ) west df . sort values ( 'wins' , ascending = False , inplace = True ) west df [ 'seed' ] = range ( 1 , len ( west df ) + 1 ) west df [ 'conference' ] = 'W' full df = pd . concat ( [ east df , west df ] , axis = 0 ) . reset index ( drop = True ) full df [ 'team id' ] = full df . team id . str . extract ( r'(\\w+)\\W*\\(\\d+\\)' , expand = False ) full df [ 'gb' ] = [ gb if isinstance ( gb , int ) or isinstance ( gb , float ) else 0 for gb in full df [ 'gb' ] ] full df = full df . drop ( 'has class full table' , axis = 1 ) expanded table = doc ( 'table#expanded standings' ) expanded df = sportsref . utils . parse table ( expanded table ) full df = pd . merge ( full df , expanded df , on = 'team id' ) return full df", "predictions": ["return the dataframe of the dataframe"], "references": ["returns a dataframe containing standings information ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4725, "code": "def roy voting ( self ) : url = '{}/awards/awards {}.html' . format ( sportsref . nba . BASE URL , self . yr ) doc = pq ( sportsref . utils . get html ( url ) ) table = doc ( 'table#roy' ) df = sportsref . utils . parse table ( table ) return df", "predictions": ["return the unload of the unload 1 1 1 1 1 1 1 - 3 unload"], "references": ["returns a dataframe containing information about roy voting ."], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 4726, "code": "def linescore ( self ) : doc = self . get main doc ( ) table = doc ( 'table#line score' ) columns = [ th . text ( ) for th in table ( 'tr.thead' ) . items ( 'th' ) ] columns [ 0 ] = 'team id' data = [ [ sportsref . utils . flatten links ( td ) for td in tr ( 'td' ) . items ( ) ] for tr in table ( 'tr.thead' ) . next all ( 'tr' ) . items ( ) ] return pd . Data Frame ( data , index = [ 'away' , 'home' ] , columns = columns , dtype = 'float' )", "predictions": ["return the dataframe as a pandas dataframe"], "references": ["returns the linescore for the game as a dataframe ."], "bleu": 0.18094495256969623, "rouge_l": 0.45607476635514016}
{"id": 4727, "code": "def get class instance key ( cls , args , kwargs ) : l = [ id ( cls ) ] for arg in args : l . append ( id ( arg ) ) l . extend ( ( k , id ( v ) ) for k , v in kwargs . items ( ) ) return tuple ( sorted ( l ) )", "predictions": ["returns a tuple names for the given glyph names index index index index index index index index index index index index index index index index index index index index index index"], "references": ["returns a unique identifier for a class instantiation ."], "bleu": 0.055177848898164926, "rouge_l": 0.1665150136487716}
{"id": 4728, "code": "def stats per game ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'per game' , kind = kind , summary = summary )", "predictions": ["returns the dump dump statistics for a given game the given game the given list of list of list of list"], "references": ["returns a dataframe of per - game box score stats ."], "bleu": 0.07305267243289862, "rouge_l": 0.19869706840390877}
{"id": 4729, "code": "def stats totals ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'totals' , kind = kind , summary = summary )", "predictions": ["returns the dump dump statistics for the given write write write write write write write write write write write write write write write write write write write write write write write"], "references": ["returns a dataframe of total box score statistics by season ."], "bleu": 0.04317900023606586, "rouge_l": 0.10418445772843724}
{"id": 4730, "code": "def stats per36 ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'per minute' , kind = kind , summary = summary )", "predictions": ["returns the file statistics for the given kind ."], "references": ["returns a dataframe of per - 36 - minutes stats ."], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 4731, "code": "def stats per100 ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'per poss' , kind = kind , summary = summary )", "predictions": ["returns the make statistics for the given = 0"], "references": ["returns a dataframe of per - 100 - possession stats ."], "bleu": 0.11301601243449393, "rouge_l": 0.09822866344605477}
{"id": 4732, "code": "def stats advanced ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'advanced' , kind = kind , summary = summary )", "predictions": ["list available advanced for a given data data data type ."], "references": ["returns a dataframe of advanced stats ."], "bleu": 0.1354599427337814, "rouge_l": 0.2314990512333966}
{"id": 4733, "code": "def stats shooting ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'shooting' , kind = kind , summary = summary )", "predictions": ["returns the statistics for the specified kind"], "references": ["returns a dataframe of shooting stats ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4734, "code": "def stats pbp ( self , kind = 'R' , summary = False ) : return self . get stats table ( 'advanced pbp' , kind = kind , summary = summary )", "predictions": ["returns the init init init init os"], "references": ["returns a dataframe of play - by - play stats ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 4735, "code": "def Game Play Finder ( * * kwargs ) : querystring = kwargs to qs ( * * kwargs ) url = '{}?{}' . format ( GPF URL , querystring ) if kwargs . get ( 'verbose' , False ) : print ( url ) html = utils . get html ( url ) doc = pq ( html ) table = doc ( 'table#all plays' ) plays = utils . parse table ( table ) if 'score' in plays . columns : o Score , d Score = zip ( * plays . score . apply ( lambda s : s . split ( '-' ) ) ) plays [ 'team Score' ] = o Score plays [ 'opp Score' ] = d Score if 'description' in plays . columns : plays = pbp . expand details ( plays , detail Col = 'description' ) return plays", "predictions": ["alias for alias . ."], "references": ["docstring will be filled in by __init__ . py"], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 4736, "code": "def get ( self ) : self . write ( \"Memory Session Object Demo:\" ) if \"sv\" in self . session : current value = self . session [ \"sv\" ] self . write ( \"current sv value is %s, and system will delete this value.<br/>\" % self . session [ \"sv\" ] ) self . session . delete ( \"sv\" ) if \"sv\" not in self . session : self . write ( \"current sv value is empty\" ) else : self . write ( \"Session data not found\" )", "predictions": ["get current session ."], "references": ["please don t do this in production environments ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 4737, "code": "def Player Season Finder ( * * kwargs ) : if 'offset' not in kwargs : kwargs [ 'offset' ] = 0 player Seasons = [ ] while True : querystring = kwargs to qs ( * * kwargs ) url = '{}?{}' . format ( PSF URL , querystring ) if kwargs . get ( 'verbose' , False ) : print ( url ) html = utils . get html ( url ) doc = pq ( html ) table = doc ( 'table#results' ) df = utils . parse table ( table ) if df . empty : break this Season = list ( zip ( df . player id , df . year ) ) player Seasons . extend ( this Season ) if doc ( '*:contains(\"Next Page\")' ) : kwargs [ 'offset' ] += 100 else : break return player Seasons", "predictions": ["return a list of all the urls of the finder"], "references": ["docstring will be filled in by __init__ . py"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 4738, "code": "def wait ( self ) : with self . cvar : self . count . value += 1 self . cvar . notify all ( ) while self . count . value < self . n procs : self . cvar . wait ( )", "predictions": ["wait for all procs to finish ."], "references": ["wait until all processes have reached the barrier ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 4739, "code": "def wait ( self ) : self . barrier A . wait ( ) self . barrier A , self . barrier B = self . barrier B , self . barrier A self . barrier A . reset ( )", "predictions": ["wait for the barrier to be ready to be ready to be ready to be sent to the queue ."], "references": ["wait until all processes have reached the barrier ."], "bleu": 0.09134423666564473, "rouge_l": 0.2961165048543689}
{"id": 4740, "code": "def close ( self ) : self . read queue . put ( Queue Closed ) self . write queue . put ( Queue Closed )", "predictions": ["close the underlying socket ."], "references": ["close the queue signalling that no more data can be put into the queue ."], "bleu": 0.05225260032224766, "rouge_l": 0.27518796992481204}
{"id": 4741, "code": "def read varint ( self ) : buff = self . fd . read ( 1 ) if buff == b'' : return 0 while ( bytearray ( buff ) [ - 1 ] & 0x80 ) >> 7 == 1 : new byte = self . fd . read ( 1 ) if new byte == b'' : raise EOF Error ( 'unexpected EOF.' ) buff += new byte varint , = decode Varint ( buff , 0 ) return varint", "predictions": ["read a varint in the packet"], "references": ["read a varint from file parse it and return the decoded integer ."], "bleu": 0.13537348102663535, "rouge_l": 0.39482200647249194}
{"id": 4742, "code": "def close ( self ) : self . flush ( ) if self . myfd is not None : self . myfd . close ( ) self . myfd = None", "predictions": ["close the underlying underlying socket ."], "references": ["close the stream ."], "bleu": 0.31239399369202553, "rouge_l": 0.6224489795918368}
{"id": 4743, "code": "def flush ( self ) : if not self . is output ( ) : return count = len ( self . write buff ) if count == 0 : return encode Varint ( self . fd . write , count , True ) for obj in self . write buff : obj str = obj . Serialize To String ( ) encode Varint ( self . fd . write , len ( obj str ) , True ) self . fd . write ( obj str ) self . write buff = [ ]", "predictions": ["write the contents of the stream ."], "references": ["write down buffer to the file ."], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 4744, "code": "def get game dir ( self , username = False ) : if not self . common and not username : raise Runtime Error ( \"Can't determine this game's directory without username\" ) if self . common : subdir = \"common\" else : subdir = \"username\" subsubdir = self . dir if WIN32 or CYGWIN : subsubdir = subsubdir . lower ( ) return os . path . join ( subdir , subsubdir )", "predictions": ["return the game dir for the current game ."], "references": ["returns joined game directory path relative to steamapps"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 4745, "code": "def with ignored exceptions ( self , * ignored exceptions ) : for exception in ignored exceptions : self . ignored exceptions = self . ignored exceptions + ( exception , ) return self", "predictions": ["returns an iterator of ignored exceptions ."], "references": ["set a list of exceptions that should be ignored inside the wait loop ."], "bleu": 0.08592522909830395, "rouge_l": 0.2695139911634757}
{"id": 4746, "code": "def send ( self , message , read reply = False ) : sock = None for tries in range ( 0 , 3 ) : try : sock = socket . socket ( socket . AF INET , socket . SOCK STREAM ) sock . connect ( ( self . host , self . PORT ) ) break except ( Connection Error , Broken Pipe Error ) : if tries == 3 : print ( \"socket connect failed.\" ) return sleep ( 0.1 ) sock . send ( codecs . decode ( message , 'hex codec' ) ) if read reply : sleep ( 0.1 ) reply = '' tries = 0 max tries = 20 while len ( reply ) < len ( message ) and tries < max tries : try : reply += codecs . encode ( sock . recv ( self . BUFFERSIZE ) , 'hex' ) . decode ( \"utf-8\" ) except ( Connection Error , Broken Pipe Error ) : pass tries += 1 sock . close ( ) if tries >= max tries : return return reply sock . close ( )", "predictions": ["send a message to the server ."], "references": ["send a command string to the amplifier ."], "bleu": 0.2789001430384383, "rouge_l": 0.6587473002159828}
{"id": 4747, "code": "def power off ( self ) : status = self . status ( ) if status [ 'power' ] : self . send ( self . CMD POWERSAVE + self . CMD OFF )", "predictions": ["power down the player ."], "references": ["power the device off ."], "bleu": 0.32466791547509893, "rouge_l": 0.6}
{"id": 4748, "code": "def power on ( self ) : status = self . status ( ) if not status [ 'power' ] : self . send ( self . CMD ON , read reply = True ) sleep ( 0.5 )", "predictions": ["turn off receiver on device ."], "references": ["power the device on ."], "bleu": 0.2626909894424158, "rouge_l": 0.3696969696969697}
{"id": 4749, "code": "def set volume ( self , volume ) : if 0 <= volume <= 200 : volume = format ( volume , \"02x\" ) self . send ( self . CMD VOLUME + volume )", "predictions": ["set the volume of the specified volume"], "references": ["set volume level of the device . accepts integer values 0 - 200 ."], "bleu": 0.10218289380194193, "rouge_l": 0.35935198821796754}
{"id": 4750, "code": "def select source ( self , source ) : status = self . status ( ) if status [ 'power' ] : if status [ 'source' ] != source : if source in self . SOURCES : self . send ( self . CMD SOURCE + self . SOURCES [ source ] , read reply = True )", "predictions": ["select a source source"], "references": ["select a source from the list of sources ."], "bleu": 0.19159730522949137, "rouge_l": 0.43160377358490565}
{"id": 4751, "code": "def exec command ( self , domain , function , operator , value = None ) : if operator in CMDS [ domain ] [ function ] [ 'supported operators' ] : if operator is '=' and value is None : raise Value Error ( 'No value provided' ) if value is None : cmd = '' . join ( [ CMDS [ domain ] [ function ] [ 'cmd' ] , operator ] ) else : cmd = '' . join ( [ CMDS [ domain ] [ function ] [ 'cmd' ] , operator , str ( value ) ] ) else : raise Value Error ( 'Invalid operator provided %s' % operator ) if self . open connection ( ) : self . telnet . write ( ( '' . join ( [ '\\r' , cmd , '\\n' ] ) . encode ( ) ) ) loop = 3 while loop : msg = self . telnet . read until ( '\\n' . encode ( ) , self . timeout ) if msg == \"\" : loop -= 1 continue msg = msg . decode ( ) . strip ( '\\r\\n' ) #print(\"NAD reponded with '%s'\" % msg) if msg . strip ( ) . split ( '=' ) [ 0 ] . lower ( ) == '.' . join ( [ domain , function ] ) . lower ( ) : return msg . strip ( ) . split ( '=' ) [ 1 ] raise Runtime Error ( 'Failed to read response' ) raise Runtime Error ( 'Failed to open connection' )", "predictions": ["run a command in the telnet environment ."], "references": ["write a command to the receiver and read the value it returns ."], "bleu": 0.1283572790104489, "rouge_l": 0.3652694610778443}
{"id": 4752, "code": "def crc ( plaintext ) : if not isinstance ( plaintext , six . binary type ) : plaintext = six . b ( plaintext ) return ( zlib . crc32 ( plaintext ) % 2147483647 ) & 0xffffffff", "predictions": ["turn a plaintext string into a string"], "references": ["generates crc32 . modulo keep the value within int range ."], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 4753, "code": "def missing schema ( self , html , song name ) : #html=self.get html response(url) soup = Beautiful Soup ( html ) name = ' ' . join ( song name ) print '%s not found' % name print \"But you can download any of the following songs :\" a list = soup . find All ( 'a' , 'touch' ) for x in xrange ( len ( a list ) - 1 ) : r = a list [ x ] p = str ( r ) q = re . sub ( r'<a.*/>|<span.*\">|</span>|</a>|<a.*html\">|<font.*\">|</font>' , '' , p ) print q", "predictions": ["perform a song in the song"], "references": ["it will print the list of songs that can be downloaded"], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 4754, "code": "def list of all href ( self , html ) : soup = Beautiful Soup ( html ) links = [ ] a list = soup . find All ( 'a' , 'touch' ) for x in xrange ( len ( a list ) - 1 ) : link = a list [ x ] . get ( 'href' ) name = a list [ x ] name = str ( name ) name = re . sub ( r'<a.*/>|<span.*\">|</span>|</a>|<a.*html\">|<font.*\">|</font>' , '' , name ) name = re . sub ( r'^[0-9]+\\.' , '' , name ) links . append ( [ link , name ] ) #quit() return links", "predictions": ["return all links in all href"], "references": ["it will return all hyper links found in the mr - jatt page for download"], "bleu": 0.07370355832749997, "rouge_l": 0.3536231884057971}
{"id": 4755, "code": "def check if song name ( self , html ) : soup = Beautiful Soup ( html ) a list = soup . find All ( 'a' , 'touch' ) #print a list text = [ str ( x ) for x in a list ] text = '' . join ( text ) text = text . lower ( ) string1 = 'download in 48 kbps' string2 = 'download in 128 kbps' string3 = 'download in 320 kbps' href = '' if string3 in text : #print 'Downloading in 320 kbps' href = a list [ 2 ] . get ( 'href' ) elif string2 in text : #print 'Downloading in 128 kbps' href = a list [ 1 ] . get ( 'href' ) elif string1 in text : #print 'Downloading in 48 kbps'\t href = a list [ 0 ] . get ( 'href' ) else : return ( True , 'nothing' ) return ( False , href )", "predictions": ["check if the song is in the song ."], "references": ["returns true if user entered artist or movie name"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 4756, "code": "def google url ( self , song name , website ) : name = '+' . join ( song name ) prefix = 'https://www.google.co.in/search?q=' website = website . split ( \" \" ) suffix = '+' . join ( website ) url = prefix + name + suffix #print url return url", "predictions": ["build the url for a google google google website ."], "references": ["it will return the google url to be searched"], "bleu": 0.14991106946711685, "rouge_l": 0.21254355400696867}
{"id": 4757, "code": "def get html response ( self , url ) : print \"Downloading page %s ..\" % url try : response = requests . get ( url , timeout = 50 ) except requests . exceptions . SSL Error : try : response = requests . get ( url , verify = False , timeout = 50 ) except requests . exceptions . Request Exception as e : print e quit ( ) except requests . exceptions . Request Exception as e : print e quit ( ) return response . content", "predictions": ["do a get request"], "references": ["it will download the html page specified by url and return the html response"], "bleu": 0.0248009595334312, "rouge_l": 0.0}
{"id": 4758, "code": "def file download using requests ( self , url ) : file name = url . split ( '/' ) [ - 1 ] if os . path . exists ( os . path . join ( os . getcwd ( ) , file name ) ) : print 'File already exists' return #print 'Downloading file %s '%file name #print 'Downloading from %s'%url try : r = requests . get ( url , stream = True , timeout = 200 ) except requests . exceptions . SSL Error : try : response = requests . get ( url , stream = True , verify = False , timeout = 200 ) except requests . exceptions . Request Exception as e : print e quit ( ) except requests . exceptions . Request Exception as e : print e quit ( ) chunk size = 1024 total size = int ( r . headers [ 'Content-Length' ] ) total chunks = total size / chunk size file iterable = r . iter content ( chunk size = chunk size ) tqdm iter = tqdm ( iterable = file iterable , total = total chunks , unit = 'KB' , leave = False ) with open ( file name , 'wb' ) as f : for data in tqdm iter : f . write ( data ) #total size=float(r.headers['Content-Length'])/(1024*1024) print 'Downloaded file %s ' % file name", "predictions": ["download tqdm using requests ."], "references": ["it will download file specified by url using requests module"], "bleu": 0.14203729394569906, "rouge_l": 0.37731958762886597}
{"id": 4759, "code": "def file download using wget ( self , url ) : file name = url . split ( '/' ) [ - 1 ] print 'Downloading file %s ' % file name command = 'wget -c --read-timeout=50 --tries=3 -q --show-progress --no-check-certificate ' url = '\"' + url + '\"' command = command + url os . system ( command )", "predictions": ["download file using wget"], "references": ["it will download file specified by url using wget utility of linux"], "bleu": 0.08047084086794415, "rouge_l": 0.45864661654135336}
{"id": 4760, "code": "def main ( ) : #print VERSION from commands . download import Download options = docopt ( doc , version = VERSION ) #print \"You reached here\" #print options print \"working.\" p = Download ( options ) p . run ( )", "predictions": ["run the cli ."], "references": ["main cli entrypoint ."], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 4761, "code": "def find Station Codes By City ( city name , token ) : req = requests . get ( API ENDPOINT SEARCH , params = { 'token' : token , 'keyword' : city name } ) if req . status code == 200 and req . json ( ) [ \"status\" ] == \"ok\" : return [ result [ \"uid\" ] for result in req . json ( ) [ \"data\" ] ] else : return [ ]", "predictions": ["list all city for a city ."], "references": ["lookup aqi database for station codes in a given city ."], "bleu": 0.15685718045401453, "rouge_l": 0.4273204903677758}
{"id": 4762, "code": "def get location observation ( lat , lng , token ) : req = requests . get ( API ENDPOINT GEO % ( lat , lng ) , params = { 'token' : token } ) if req . status code == 200 and req . json ( ) [ \"status\" ] == \"ok\" : return parse observation response ( req . json ( ) [ \"data\" ] ) return { }", "predictions": ["get an observation observation from a lat token"], "references": ["lookup observations by geo coordinates ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4763, "code": "def parse observation response ( json ) : logging . debug ( json ) iaqi = json [ 'iaqi' ] result = { 'idx' : json [ 'idx' ] , 'city' : json . get ( 'city' , '' ) , 'aqi' : json [ 'aqi' ] , 'dominentpol' : json . get ( \"dominentpol\" , '' ) , 'time' : json [ 'time' ] [ 's' ] , 'iaqi' : [ { 'p' : item , 'v' : iaqi [ item ] [ 'v' ] } for item in iaqi ] } return result", "predictions": ["parse an observation response ."], "references": ["decode aqicn observation response json into python object ."], "bleu": 0.17348474258688365, "rouge_l": 0.40757238307349664}
{"id": 4764, "code": "def compilers ( self ) : return [ self . environment . compilers . get ( e ) for e in self . compiler extensions ]", "predictions": ["return a list of compilers objects representing the environment ."], "references": ["the list of compilers used to build asset ."], "bleu": 0.25965358893403384, "rouge_l": 0.42508710801393734}
{"id": 4765, "code": "def mimetype ( self ) : return ( self . environment . mimetypes . get ( self . format extension ) or self . compiler mimetype or 'application/octet-stream' )", "predictions": ["return the mimetype object ."], "references": ["mime type of the asset ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 4766, "code": "def compiler mimetype ( self ) : for compiler in reversed ( self . compilers ) : if compiler . result mimetype : return compiler . result mimetype return None", "predictions": ["return the compiler mimetype of the current compiler ."], "references": ["implicit mime type of the asset by its compilers ."], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 4767, "code": "def compiler format extension ( self ) : for extension , mimetype in self . environment . mimetypes . items ( ) : if mimetype == self . compiler mimetype : return extension return None", "predictions": ["return the format extension for the compiler"], "references": ["implicit format extension on the asset by its compilers ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 4768, "code": "def register ( self , mimetype , processor ) : if mimetype not in self or processor not in self [ mimetype ] : self . setdefault ( mimetype , [ ] ) . append ( processor )", "predictions": ["get the . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["register passed processor for passed mimetype ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 4769, "code": "def register defaults ( self ) : self . mimetypes . register defaults ( ) self . preprocessors . register defaults ( ) self . postprocessors . register defaults ( )", "predictions": ["register defaults s default values"], "references": ["register default compilers preprocessors and mime types ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 4770, "code": "def table ( name , auth = None , eager = True ) : auth = auth or [ ] dynamodb = boto . connect dynamodb ( * auth ) table = dynamodb . get table ( name ) return Table ( table = table , eager = eager )", "predictions": ["create a wait wait wait for the given self n n n ."], "references": ["returns a given table for the given user ."], "bleu": 0.19674979811155635, "rouge_l": 0.4699537750385208}
{"id": 4771, "code": "def tables ( auth = None , eager = True ) : auth = auth or [ ] dynamodb = boto . connect dynamodb ( * auth ) return [ table ( t , auth , eager = eager ) for t in dynamodb . list tables ( ) ]", "predictions": ["return all wait wait wait for this user ."], "references": ["returns a list of tables for the given user ."], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 4772, "code": "def metadata id ( item ) : if Crates . metadata category ( item ) == CATEGORY CRATES : return str ( item [ 'id' ] ) else : ts = item [ 'fetched on' ] ts = str to datetime ( ts ) return str ( ts . timestamp ( ) )", "predictions": ["convert an close item to a datetime id string"], "references": ["extracts the identifier from an item depending on its type ."], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 4773, "code": "def fetch crate owner team ( self , crate id ) : raw owner team = self . client . crate attribute ( crate id , 'owner team' ) owner team = json . loads ( raw owner team ) return owner team", "predictions": ["returns the owner for the given varint == the given varint == the owner"], "references": ["get crate team owner"], "bleu": 0.08839374326825923, "rouge_l": 0.1234817813765182}
{"id": 4774, "code": "def fetch crate owner user ( self , crate id ) : raw owner user = self . client . crate attribute ( crate id , 'owner user' ) owner user = json . loads ( raw owner user ) return owner user", "predictions": ["close the crate attribute by crate"], "references": ["get crate user owners"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4775, "code": "def fetch crate versions ( self , crate id ) : raw versions = self . client . crate attribute ( crate id , \"versions\" ) version downloads = json . loads ( raw versions ) return version downloads", "predictions": ["flush crate by crate"], "references": ["get crate versions data"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 4776, "code": "def fetch crate version downloads ( self , crate id ) : raw version downloads = self . client . crate attribute ( crate id , \"downloads\" ) version downloads = json . loads ( raw version downloads ) return version downloads", "predictions": ["get the game downloads downloads for the given game = 0 not in the game"], "references": ["get crate version downloads"], "bleu": 0.09103526405546068, "rouge_l": 0.2350674373795761}
{"id": 4777, "code": "def summary ( self ) : path = urijoin ( CRATES API URL , CATEGORY SUMMARY ) raw content = self . fetch ( path ) return raw content", "predictions": ["get the with the with the path path"], "references": ["get crates . io summary"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 4778, "code": "def crates ( self , from page = 1 ) : path = urijoin ( CRATES API URL , CATEGORY CRATES ) raw crates = self . fetch items ( path , from page ) return raw crates", "predictions": ["3 - send send command to the given read read 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"], "references": ["get crates in alphabetical order"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4779, "code": "def crate ( self , crate id ) : path = urijoin ( CRATES API URL , CATEGORY CRATES , crate id ) raw crate = self . fetch ( path ) return raw crate", "predictions": ["+ + + power of a power power power power id"], "references": ["get a crate by its id"], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 4780, "code": "def fetch items ( self , path , page = 1 ) : fetch data = True parsed crates = 0 total crates = 0 while fetch data : logger . debug ( \"Fetching page: %i\" , page ) try : payload = { 'sort' : 'alphabetical' , 'page' : page } raw content = self . fetch ( path , payload = payload ) content = json . loads ( raw content ) parsed crates += len ( content [ 'crates' ] ) if not total crates : total crates = content [ 'meta' ] [ 'total' ] except requests . exceptions . HTTP Error as e : logger . error ( \"HTTP exception raised - %s\" , e . response . text ) raise e yield raw content page += 1 if parsed crates >= total crates : fetch data = False", "predictions": ["power on the server"], "references": ["return the items from crates . io api using pagination"], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 4781, "code": "def fetch ( self , url , payload = None ) : response = super ( ) . fetch ( url , payload = payload ) return response . text", "predictions": ["do a set of data from the url api"], "references": ["return the textual content associated to the response object"], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 4782, "code": "def get questions ( self , offset = None ) : page = Kitsune Client . FIRST PAGE if offset : page += int ( offset / Kitsune Client . ITEMS PER PAGE ) while True : api questions url = urijoin ( self . base url , '/question' ) + '/' params = { \"page\" : page , \"ordering\" : \"updated\" } questions = self . fetch ( api questions url , params ) yield questions questions json = json . loads ( questions ) next uri = questions json [ 'next' ] if not next uri : break page += 1", "predictions": ["select source source source source source source source"], "references": ["retrieve questions from older to newer updated starting offset"], "bleu": 0.11900569447018795, "rouge_l": 0.0}
{"id": 4783, "code": "def fetch ( self , url , params ) : logger . debug ( \"Kitsune client calls API: %s params: %s\" , url , str ( params ) ) response = super ( ) . fetch ( url , payload = params ) return response . text", "predictions": ["do a exec request to the api = 0 = 1 = 1 = 1 = 0 = 1 1 = 1 = 1 = 1"], "references": ["return the textual content associated to the response object"], "bleu": 0.06143498010483918, "rouge_l": 0.1252566735112936}
{"id": 4784, "code": "def get items ( self , category = CATEGORY EVENT , offset = REMO DEFAULT OFFSET ) : more = True next uri = None page = Re Mo Client . FIRST PAGE page += int ( offset / Re Mo Client . ITEMS PER PAGE ) if category == CATEGORY EVENT : api = self . api events url elif category == CATEGORY ACTIVITY : api = self . api activities url elif category == CATEGORY USER : api = self . api users url else : raise Value Error ( category + ' not supported in Re Mo' ) while more : params = { \"page\" : page , \"orderby\" : \"ASC\" } logger . debug ( \"Re Mo client calls AP Iv2: %s params: %s\" , api , str ( params ) ) raw items = self . fetch ( api , payload = params ) yield raw items items data = json . loads ( raw items ) next uri = items data [ 'next' ] if not next uri : more = False else : parsed uri = urllib . parse . urlparse ( next uri ) parsed params = urllib . parse . parse qs ( parsed uri . query ) page = parsed params [ 'page' ] [ 0 ]", "predictions": ["crc for a if client is available"], "references": ["retrieve all items for category using pagination"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4785, "code": "def io priority ( self ) : return ( self . iocb . aio reqprio if self . iocb . u . c . flags & libaio . IOCB FLAG IOPRIO else None )", "predictions": ["song song schema name name name name name name name name name name name name name name name name name name name name name name name name name name name name"], "references": ["io priority for this instance ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4786, "code": "def get cells ( self ) : logger . info ( \"Retrieving all cells spreadsheet data ...\" ) logger . debug ( \"Mozilla Club client calls API: %s\" , self . base url ) raw cells = self . fetch ( self . base url ) return raw cells . text", "predictions": ["list all of of of of the of the = = true html html html html html data"], "references": ["retrieve all cells from the spreadsheet ."], "bleu": 0.07535838128770536, "rouge_l": 0.17378917378917377}
{"id": 4787, "code": "def parse ( self ) : nevents wrong = 0 feed json = json . loads ( self . feed ) if 'entry' not in feed json [ 'feed' ] : return self . cells = feed json [ 'feed' ] [ 'entry' ] self . ncell = 0 event fields = self . get event fields ( ) while self . ncell < len ( self . cells ) : event = self . get next event ( event fields ) if event [ 'Date of Event' ] is None or event [ 'Club Name' ] is None : logger . warning ( \"Wrong event data: %s\" , event ) nevents wrong += 1 continue yield event logger . info ( \"Total number of wrong events: %i\" , nevents wrong )", "predictions": ["check and 0 cells"], "references": ["parse the mozillaclub spreadsheet feed cells json ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 4788, "code": "def get data files ( dirname ) : flist = [ ] for dirpath , dirnames , filenames in os . walk ( dirname ) : for fname in filenames : flist . append ( osp . join ( dirpath , fname ) ) return flist", "predictions": ["return list of all url files files files"], "references": ["return data files in directory * dirname *"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 4789, "code": "def export formats ( self , pid type ) : if pid type not in self . export formats : fmts = self . app . config . get ( 'RECORDS UI EXPORT FORMATS' , { } ) . get ( pid type , { } ) self . export formats [ pid type ] = sorted ( [ ( k , v ) for k , v in fmts . items ( ) if v ] , key = lambda x : x [ 1 ] [ 'order' ] , ) return self . export formats [ pid type ]", "predictions": ["get the html for the given pid url ."], "references": ["list of export formats ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 4790, "code": "def permission factory ( self ) : if self . permission factory is None : imp = self . app . config [ 'RECORDS UI DEFAULT PERMISSION FACTORY' ] self . permission factory = obj or import string ( imp ) return self . permission factory", "predictions": ["returns extended file download url url url url url url url url url url url url ."], "references": ["load default permission factory ."], "bleu": 0.07223943354597204, "rouge_l": 0.10082644628099173}
{"id": 4791, "code": "def records ( ) : import uuid from invenio records . api import Record from invenio pidstore . models import Persistent Identifier , PID Status with db . session . begin nested ( ) : pid1 = Persistent Identifier . create ( 'recid' , '1' , object type = 'rec' , object uuid = rec1 uuid , status = PID Status . REGISTERED ) Record . create ( { 'title' : 'Registered ' , 'authors' : [ { 'name' : 'Ellis Jonathan' } , { 'name' : 'Higgs Peter' } , ] , 'access' : 'open' , 'keywords' : [ 'CERN' , 'higgs' ] , } , id = rec1 uuid ) Persistent Identifier . create ( 'recid' , '2' , object type = 'rec' , object uuid = rec2 uuid , status = PID Status . REGISTERED ) Record . create ( { 'title' : 'Registered ' , 'authors' : [ { 'name' : 'Ellis Jonathan' } , { 'name' : 'Higgs Peter' } , ] , 'access' : 'closed' , 'keywords' : [ 'CERN' , 'higgs' ] , } , id = rec2 uuid ) rec3 uuid = uuid . uuid4 ( ) pid = Persistent Identifier . create ( 'recid' , '3' , object type = 'rec' , object uuid = rec3 uuid , status = PID Status . REGISTERED ) pid . delete ( ) Record . create ( { 'title' : 'Live ' } , id = rec3 uuid ) Persistent Identifier . create ( 'recid' , '4' , status = PID Status . DELETED ) Persistent Identifier . create ( 'recid' , '5' , status = PID Status . REGISTERED ) pid = Persistent Identifier . create ( 'recid' , '6' , status = PID Status . REGISTERED ) pid . redirect ( pid1 ) doi = Persistent Identifier . create ( 'doi' , '10.1234/foo' , status = PID Status . REGISTERED ) pid = Persistent Identifier . create ( 'recid' , '7' , status = PID Status . REGISTERED ) pid . redirect ( doi ) Persistent Identifier . create ( 'recid' , '8' , status = PID Status . RESERVED ) db . session . commit ( )", "predictions": ["create file file file ."], "references": ["load test data fixture ."], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 4792, "code": "def time callable ( self , name , target , rate = None , args = ( ) , kwargs = { } ) : assert callable ( target ) if rate is None : rate = self . rate else : assert sample rate ( rate ) start time = time ( ) result = target ( * args , * * kwargs ) self . since ( name , start time , rate ) return result", "predictions": ["main main main main main main callable"], "references": ["send a timer metric calculating duration of execution of the provided callable"], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 4793, "code": "def increment ( self , name , count = 1 , rate = 1 ) : if self . should send metric ( name , rate ) : self . request ( Counter ( self . create metric name for request ( name ) , int ( count ) , rate ) . to request ( ) )", "predictions": ["find a . . find ."], "references": ["increment a counter metric"], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4794, "code": "def timing ( self , name , milliseconds , rate = 1 ) : if self . should send metric ( name , rate ) : milliseconds = int ( milliseconds ) self . request ( Timer ( self . create metric name for request ( name ) , milliseconds , rate ) . to request ( ) )", "predictions": ["send a get request requests requests requests requests ."], "references": ["send a timer metric with the specified duration in milliseconds"], "bleu": 0.16621692209732, "rouge_l": 0.20854700854700853}
{"id": 4795, "code": "def timing since ( self , name , start time , rate = 1 ) : duration = 0 if isinstance ( start time , datetime ) : duration = ( datetime . now ( start time . tzinfo ) - start time ) . total seconds ( ) * 1000 elif is numeric ( start time ) : assert start time > 0 duration = ( time ( ) - start time ) * 1000 else : raise Value Error ( \"start time should be a timestamp or a datetime\" ) self . timing ( name , duration , rate )", "predictions": ["a helper method for setting the parse logging logging logging logging ."], "references": ["send a timer metric calculating the duration from the start time"], "bleu": 0.11498759556447223, "rouge_l": 0.17528735632183906}
{"id": 4796, "code": "def gauge ( self , name , value , rate = 1 ) : if self . should send metric ( name , rate ) : if not is numeric ( value ) : value = float ( value ) self . request ( Gauge ( self . create metric name for request ( name ) , value , rate ) . to request ( ) )", "predictions": ["set compilers return value ."], "references": ["send a gauge metric with the specified value"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 4797, "code": "def gauge delta ( self , name , delta , rate = 1 ) : if self . should send metric ( name , rate ) : if not is numeric ( delta ) : delta = float ( delta ) self . request ( Gauge Delta ( self . create metric name for request ( name ) , delta , rate ) . to request ( ) )", "predictions": ["send a mimetype for"], "references": ["send a gaugedelta metric to change a gauge by the specified value"], "bleu": 0.06399610426154731, "rouge_l": 0.22932330827067668}
{"id": 4798, "code": "def set ( self , name , value , rate = 1 ) : if self . should send metric ( name , rate ) : value = str ( value ) self . request ( Set ( self . create metric name for request ( name ) , value , rate ) . to request ( ) )", "predictions": ["compiler a metric result"], "references": ["send a set metric with the specified unique value"], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 4799, "code": "def request ( self , data ) : data = bytearray ( \"{}\\n\" . format ( data ) . encode ( ) ) self . prepare batches for storage ( len ( data ) ) self . batches [ - 1 ] . extend ( data )", "predictions": ["compiler items from the network in the == == == items in the == == == in the == 1 in the == in the == == 1 in bytes"], "references": ["override parent by buffering the metric instead of sending now"], "bleu": 0.04034110170120257, "rouge_l": 0.05495495495495494}
{"id": 4800, "code": "def batch client ( self , size = 512 ) : batch client = Batch Client ( self . host , self . port , self . prefix , size ) self . configure client ( batch client ) return batch client", "predictions": ["create a batch client ."], "references": ["return a batch client with same settings of the client"], "bleu": 0.18693159143202892, "rouge_l": 0.37731958762886597}
{"id": 4801, "code": "def unit client ( self ) : client = Client ( self . host , self . port , self . prefix ) self . configure client ( client ) return client", "predictions": ["return a unit client ."], "references": ["return a client with same settings of the batch client"], "bleu": 0.14203729394569906, "rouge_l": 0.37731958762886597}
{"id": 4802, "code": "def flush ( self ) : address = self . remote address while len ( self . batches ) > 0 : self . socket . sendto ( self . batches [ 0 ] , address ) self . batches . popleft ( ) return self", "predictions": ["flush all batches of the batches ."], "references": ["send buffered metrics in batch requests"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4803, "code": "def my permission factory ( record , * args , * * kwargs ) : def can ( self ) : rec = Record . get record ( record . id ) return rec . get ( 'access' , '' ) == 'open' return type ( 'My Permission Checker' , ( ) , { 'can' : can } ) ( )", "predictions": ["decorator to create a permission factory for a record ."], "references": ["my permission factory ."], "bleu": 0.17827531042796255, "rouge_l": 0.4644670050761421}
{"id": 4804, "code": "def batch client ( self , size = 512 ) : batch client = TCP Batch Client ( self . host , self . port , self . prefix , size ) self . configure client ( batch client ) return batch client", "predictions": ["create a batch client ."], "references": ["return a tcp batch client with same settings of the tcp client"], "bleu": 0.09521044541645862, "rouge_l": 0.3285457809694794}
{"id": 4805, "code": "def flush ( self ) : while len ( self . batches ) > 0 : self . socket . sendall ( self . batches [ 0 ] ) self . batches . popleft ( ) return self", "predictions": ["flush all batches of the batches ."], "references": ["send buffered metrics in batch requests over tcp"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 4806, "code": "def unit client ( self ) : client = TCP Client ( self . host , self . port , self . prefix ) self . configure client ( client ) return client", "predictions": ["return a unit client ."], "references": ["return a tcpclient with same settings of the batch tcp client"], "bleu": 0.11629030063732083, "rouge_l": 0.35124760076775424}
{"id": 4807, "code": "def convert As Open Math ( term , converter ) : if hasattr ( term , \" ishelper\" ) and term . ishelper or isinstance ( term , om . OM Any ) : return interpret As Open Math ( term ) if converter is not None : try : converted = converter . to openmath ( term ) except Exception as e : converted = None if isinstance ( converted , om . OM Any ) : return converted return interpret As Open Math ( term )", "predictions": ["convert a converter to a converter"], "references": ["converts a term into openmath using either a converter or the interpretasopenmath method"], "bleu": 0.09728049676725326, "rouge_l": 0.29611650485436897}
{"id": 4808, "code": "def to python ( self , omobj ) : if omobj . class in self . omclass to py : return self . omclass to py [ omobj . class ] ( omobj ) elif isinstance ( omobj , om . OM Symbol ) : return self . lookup to python ( omobj . cdbase , omobj . cd , omobj . name ) elif isinstance ( omobj , om . OM Application ) : elem = self . to python ( omobj . elem ) arguments = [ self . to python ( x ) for x in omobj . arguments ] return elem ( * arguments ) raise Value Error ( 'Cannot convert object of class %s to Python.' % omobj . class . name )", "predictions": ["convert the object to a python python object ."], "references": ["convert openmath object to python"], "bleu": 0.21105340631872635, "rouge_l": 0.6024691358024692}
{"id": 4809, "code": "def to openmath ( self , obj ) : for cl , conv in reversed ( self . conv to om ) : if cl is None or isinstance ( obj , cl ) : try : return conv ( obj ) except Cannot Convert Error : continue if hasattr ( obj , ' openmath ' ) : return obj . openmath ( ) raise Value Error ( 'Cannot convert %r to Open Math.' % obj )", "predictions": ["convert obj to a openmath ."], "references": ["convert python object to openmath"], "bleu": 0.2626909894424158, "rouge_l": 0.5545454545454546}
{"id": 4810, "code": "def init app ( self , app ) : app . config . setdefault ( 'REDIS URLS' , { 'main' : 'redis://localhost:6379/0' , 'admin' : 'redis://localhost:6379/1' , } ) app . before request ( self . before request ) self . app = app", "predictions": ["initialize the app ."], "references": ["used to initialize redis with app object"], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 4811, "code": "def valid choices ( choices ) : for key , value in choices : if isinstance ( value , ( list , tuple ) ) : for key , in value : yield key else : yield key", "predictions": ["generator to yield valid choices from choices ."], "references": ["return list of choices s keys"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 4812, "code": "def split model kwargs ( kw ) : from collections import defaultdict model fields = { } fields agrs = defaultdict ( lambda : { } ) for key in kw . keys ( ) : if ' ' in key : field , , subfield = key . partition ( ' ' ) fields agrs [ field ] [ subfield ] = kw [ key ] else : model fields [ key ] = kw [ key ] return model fields , fields agrs", "predictions": ["split the model fields into a dictionary of model fields ."], "references": ["django_any birds language parser"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 4813, "code": "def any form default ( form cls , * * kwargs ) : form data = { } form files = { } form fields , fields args = split model kwargs ( kwargs ) for name , field in form cls . base fields . iteritems ( ) : if name in form fields : form data [ name ] = kwargs [ name ] else : form data [ name ] = any form field ( field , * * fields args [ name ] ) return form data , form files", "predictions": ["returns the default form for any form to be used in the form ."], "references": ["returns tuple with form data and files"], "bleu": 0.09782375748961449, "rouge_l": 0.2026578073089701}
{"id": 4814, "code": "def field choices attibute ( function ) : def wrapper ( field , * * kwargs ) : if hasattr ( field . widget , 'choices' ) : return random . choice ( list ( valid choices ( field . widget . choices ) ) ) return function ( field , * * kwargs ) return wrapper", "predictions": ["decorator to return a random field choices ."], "references": ["selection from field . choices"], "bleu": 0.19070828081828378, "rouge_l": 0.32105263157894737}
{"id": 4815, "code": "def model choice field data ( field , * * kwargs ) : data = list ( field . queryset [ : 10 ] ) if data : return random . choice ( data ) else : raise Type Error ( 'No %s available in queryset' % field . queryset . model )", "predictions": ["return a random field data ."], "references": ["return one of first ten items for field queryset"], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 4816, "code": "def tag ( version = version ) : build = local ( \"git tag {0}\" . format ( version ) ) if build . succeeded : local ( \"git push --tags\" )", "predictions": ["tag current version of current git repository ."], "references": ["deploy a version tag ."], "bleu": 0.19070828081828378, "rouge_l": 0.32105263157894737}
{"id": 4817, "code": "def any field blank ( function ) : def wrapper ( field , * * kwargs ) : if kwargs . get ( 'isnull' , False ) : return None if field . blank and random . random < 0.1 : return None return function ( field , * * kwargs ) return wrapper", "predictions": ["ensure any field blank field blank ."], "references": ["sometimes return none if field could be blank"], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 4818, "code": "def any file field ( field , * * kwargs ) : def get some file ( path ) : subdirs , files = field . storage . listdir ( path ) if files : result file = random . choice ( files ) instance = field . storage . open ( \"%s/%s\" % ( path , result file ) ) . file return Field File ( instance , field , result file ) for subdir in subdirs : result = get some file ( \"%s/%s\" % ( path , subdir ) ) if result : return result result = get some file ( field . upload to ) if result is None and not field . null : raise Type Error ( \"Can't found file in %s for non nullable File Field\" % field . upload to ) return result", "predictions": ["returns a file field field field field"], "references": ["lookup for nearest existing file"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 4819, "code": "def any filepath field ( field , * * kwargs ) : def get some file ( path ) : subdirs , files = [ ] , [ ] for entry in os . listdir ( path ) : entry path = os . path . join ( path , entry ) if os . path . isdir ( entry path ) : subdirs . append ( entry path ) else : if not field . match or re . match ( field . match , entry ) : files . append ( entry path ) if files : return random . choice ( files ) if field . recursive : for subdir in subdirs : result = get some file ( subdir ) if result : return result result = get some file ( field . path ) if result is None and not field . null : raise Type Error ( \"Can't found file in %s for non nullable File Path Field\" % field . path ) return result", "predictions": ["return a list of all filepath in a field ."], "references": ["lookup for nearest existing file"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 4820, "code": "def decode ( data ) : data = bytearray ( data ) result = bytearray ( ) pos = 0 while pos < len ( data ) : header byte = data [ pos ] if header byte > 127 : header byte -= 256 pos += 1 if 0 <= header byte <= 127 : result . extend ( data [ pos : pos + header byte + 1 ] ) pos += header byte + 1 elif header byte == - 128 : pass else : result . extend ( [ data [ pos ] ] * ( 1 - header byte ) ) pos += 1 return bytes ( result )", "predictions": ["decode a byte array into a list of bytes ."], "references": ["decodes a packbit encoded data ."], "bleu": 0.13950796967929133, "rouge_l": 0.26180257510729615}
{"id": 4821, "code": "def encode ( data ) : if len ( data ) == 0 : return data if len ( data ) == 1 : return b'\\x00' + data data = bytearray ( data ) result = bytearray ( ) buf = bytearray ( ) pos = 0 repeat count = 0 MAX LENGTH = 127 state = 'RAW' def finish raw ( ) : if len ( buf ) == 0 : return result . append ( len ( buf ) - 1 ) result . extend ( buf ) buf [ : ] = bytearray ( ) def finish rle ( ) : result . append ( 256 - ( repeat count - 1 ) ) result . append ( data [ pos ] ) while pos < len ( data ) - 1 : current byte = data [ pos ] if data [ pos ] == data [ pos + 1 ] : if state == 'RAW' : finish raw ( ) state = 'RLE' repeat count = 1 elif state == 'RLE' : if repeat count == MAX LENGTH : finish rle ( ) repeat count = 0 repeat count += 1 else : if state == 'RLE' : repeat count += 1 finish rle ( ) state = 'RAW' repeat count = 0 elif state == 'RAW' : if len ( buf ) == MAX LENGTH : finish raw ( ) buf . append ( current byte ) pos += 1 if state == 'RAW' : buf . append ( data [ pos ] ) finish raw ( ) else : repeat count += 1 finish rle ( ) return bytes ( result )", "predictions": ["encode a byte - encoded bytes ."], "references": ["encodes data using packbits encoding ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 4822, "code": "def add ( self , name , path ) : if not ( os . path . exists ( path ) ) : raise Value Error ( \"Workspace path `%s` doesn't exists.\" % path ) if ( self . exists ( name ) ) : raise Value Error ( \"Workspace `%s` already exists.\" % name ) self . config [ \"workspaces\" ] [ name ] = { \"path\" : path , \"repositories\" : { } } self . config . write ( )", "predictions": ["add a file to the cache"], "references": ["add a workspace entry in user config file ."], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 4823, "code": "def remove ( self , name ) : if not ( self . exists ( name ) ) : raise Value Error ( \"Workspace `%s` doesn't exists.\" % name ) self . config [ \"workspaces\" ] . pop ( name , 0 ) self . config . write ( )", "predictions": ["remove a file from the cache ."], "references": ["remove workspace from config file ."], "bleu": 0.23356898886410002, "rouge_l": 0.4680306905370844}
{"id": 4824, "code": "def list ( self ) : ws list = { } for key , value in self . config [ \"workspaces\" ] . items ( ) : ws list [ key ] = dict ( { \"name\" : key } , * * value ) return ws list", "predictions": ["returns a dictionary of the configuration names"], "references": ["list all available workspaces ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4825, "code": "def repository exists ( self , workspace , repo ) : if not self . exists ( workspace ) : return False workspaces = self . list ( ) return repo in workspaces [ workspace ] [ \"repositories\" ]", "predictions": ["check if a repository exists ."], "references": ["return true if workspace contains repository name ."], "bleu": 0.18822631894109965, "rouge_l": 0.4178082191780822}
{"id": 4826, "code": "def sync ( self , ws name ) : path = self . config [ \"workspaces\" ] [ ws name ] [ \"path\" ] repositories = self . config [ \"workspaces\" ] [ ws name ] [ \"repositories\" ] logger = logging . get Logger ( name ) color = Color ( ) for r in os . listdir ( path ) : try : repo = Repository ( os . path . join ( path , r ) ) except Repository Error : continue else : repositories [ r ] = repo . path for repo name , path in repositories . items ( ) : logger . info ( color . colored ( \" - %s\" % repo name , \"blue\" ) ) self . config [ \"workspaces\" ] [ ws name ] [ \"repositories\" ] self . config . write ( )", "predictions": ["sync repository repository with local configuration"], "references": ["synchronise workspace s repositories ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4827, "code": "def clone ( url , path ) : adapter = None if url [ : 4 ] == \"git@\" or url [ - 4 : ] == \".git\" : adapter = Git ( path ) if url [ : 6 ] == \"svn://\" : adapter = Svn ( path ) if url [ : 6 ] == \"bzr://\" : adapter = Bzr ( path ) if url [ : 9 ] == \"ssh://hg@\" : adapter = Hg ( path ) if adapter is None : raise Repository Adapter Not Found ( \"Can't find adapter for `%s` repository url\" % url ) return adapter . clone ( url )", "predictions": ["clone a repository from a repository"], "references": ["clone a repository ."], "bleu": 0.4111336169005197, "rouge_l": 0.6224489795918368}
{"id": 4828, "code": "def check version ( ) : import requests r = requests . get ( 'https://pypi.python.org/pypi/ndio/json' ) . json ( ) r = r [ 'info' ] [ 'version' ] if r != version : print ( \"A newer version of ndio is available. \" + \"'pip install -U ndio' to update.\" ) return r", "predictions": ["check if the newer version is installed"], "references": ["tells you if you have an old version of ndio ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 4829, "code": "def execute ( self , args ) : if args . name is not None : self . print workspace ( args . name ) elif args . all is not None : self . print all ( )", "predictions": ["execute a workspace ."], "references": ["execute update subcommand ."], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 4830, "code": "def print update ( self , repo name , repo path ) : color = Color ( ) self . logger . info ( color . colored ( \"=> [%s] %s\" % ( repo name , repo path ) , \"green\" ) ) try : repo = Repository ( repo path ) repo . update ( ) except Repository Error as e : self . logger . error ( e ) pass print ( \"\\n\" )", "predictions": ["print the update for the given repo ."], "references": ["print repository update ."], "bleu": 0.19070828081828378, "rouge_l": 0.5319767441860466}
{"id": 4831, "code": "def set console handler ( self , debug = False ) : console = logging . Stream Handler ( ) console . set Formatter ( Formatter ( LFORMAT ) ) if not debug : console . set Level ( logging . INFO ) self . add Handler ( console )", "predictions": ["set screen handler ."], "references": ["set console handler ."], "bleu": 0.5081327481546147, "rouge_l": 0.75}
{"id": 4832, "code": "def execute ( self , command , path = None ) : logger = logging . get Logger ( name ) self . check executable ( ) logger . debug ( \"Executing command `%s` (cwd: %s)\" % ( command , path ) ) process = subprocess . Popen ( command , shell = True , cwd = path , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) stdout , stderr = process . communicate ( ) exit code = process . wait ( ) if stdout : logger . info ( stdout . decode ( \"utf-8\" ) ) if stderr : if exit code != 0 : logger . error ( stderr . decode ( \"utf-8\" ) ) else : logger . info ( stderr . decode ( \"utf-8\" ) ) return process", "predictions": ["batch the command command ."], "references": ["execute command with os . popen and return output ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 4833, "code": "def print workspace ( self , name ) : path list = find path ( name , self . config ) if len ( path list ) == 0 : self . logger . error ( \"No matches for `%s`\" % name ) return False for name , path in path list . items ( ) : self . print status ( name , path )", "predictions": ["unit matches matches matches"], "references": ["print workspace status ."], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 4834, "code": "def print status ( self , repo name , repo path ) : color = Color ( ) self . logger . info ( color . colored ( \"=> [%s] %s\" % ( repo name , repo path ) , \"green\" ) ) try : repo = Repository ( repo path ) repo . status ( ) except Repository Error as e : self . logger . error ( e ) pass print ( \"\\n\" )", "predictions": ["flush the status status"], "references": ["print repository status ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 4835, "code": "def post cutout no chunking blosc ( self , token , channel , x start , y start , z start , data , resolution ) : data = numpy . expand dims ( data , axis = 0 ) blosc data = blosc . pack array ( data ) url = self . url ( \"{}/{}/blosc/{}/{},{}/{},{}/{},{}/0,0/\" . format ( token , channel , resolution , x start , x start + data . shape [ 3 ] , y start , y start + data . shape [ 2 ] , z start , z start + data . shape [ 1 ] ) ) req = self . remote utils . post url ( url , data = blosc data , headers = { 'Content-Type' : 'application/octet-stream' } ) if req . status code is not 200 : raise Remote Data Upload Error ( req . text ) else : return True", "predictions": ["my my my call to the remote"], "references": ["accepts data in zyx . !!!"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4836, "code": "def clone ( self , url ) : return self . execute ( \"%s branch %s %s\" % ( self . executable , url , self . path ) )", "predictions": ["batch the current branch branch . . . ."], "references": ["clone repository from url ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 4837, "code": "def get version ( ) : requirement = pkg resources . Requirement . parse ( \"yoda\" ) provider = pkg resources . get provider ( requirement ) return provider . version", "predictions": ["return version from provider"], "references": ["get version from package resources ."], "bleu": 0.2868106410131918, "rouge_l": 0.3860759493670886}
{"id": 4838, "code": "def mix and match ( name , greeting = 'Hello' , yell = False ) : say = '%s, %s' % ( greeting , name ) if yell : print '%s!' % say . upper ( ) else : print '%s.' % say", "predictions": ["unit - match client client ."], "references": ["mixing and matching positional args and keyword options ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 4839, "code": "def option decorator ( name , greeting , yell ) : say = '%s, %s' % ( greeting , name ) if yell : print '%s!' % say . upper ( ) else : print '%s.' % say", "predictions": ["decorator decorator decorator or ."], "references": ["same as mix_and_match but using the"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 4840, "code": "def parse ( self ) : parser = self . subparser . add parser ( \"show\" , help = \"Show workspace details\" , description = \"Show workspace details.\" ) group = parser . add mutually exclusive group ( required = True ) group . add argument ( '--all' , action = 'store true' , help = \"All workspaces\" ) group . add argument ( 'name' , type = str , help = \"Workspace name\" , nargs = '?' )", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["parse show subcommand ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4841, "code": "def execute ( self , args ) : if args . name is not None : self . show workspace ( slashes2dash ( args . name ) ) elif args . all is not None : self . show all ( )", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["execute show subcommand ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4842, "code": "def show workspace ( self , name ) : if not self . workspace . exists ( name ) : raise Value Error ( \"Workspace `%s` doesn't exists.\" % name ) color = Color ( ) workspaces = self . workspace . list ( ) self . logger . info ( \"<== %s workspace ==>\" % color . colored ( name , \"green\" ) ) self . logger . info ( \"\\t Path: %s\" % workspaces [ name ] [ \"path\" ] ) self . logger . info ( \"\\t Number of repositories: %s\" % color . colored ( len ( workspaces [ name ] [ \"repositories\" ] ) , \"yellow\" ) ) repo colored = color . colored ( \"Repositories\" , \"blue\" ) path colored = color . colored ( \"Path\" , \"blue\" ) trepositories = Pretty Table ( [ repo colored , path colored , color . colored ( \"+\" , \"blue\" ) ] ) trepositories . align [ repo colored ] = \"l\" trepositories . align [ path colored ] = \"l\" for repo name in workspaces [ name ] [ \"repositories\" ] : fullname = \"%s/%s\" % ( name , repo name ) fullpath = find path ( fullname , self . config ) [ fullname ] try : repo = Repository ( fullpath ) repo scm = repo . get scm ( ) except Repository Adapter Not Found : repo scm = None trepositories . add row ( [ color . colored ( repo name , \"cyan\" ) , fullpath , repo scm ] ) self . logger . info ( trepositories )", "predictions": ["init a app s app ."], "references": ["show specific workspace ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4843, "code": "def show all ( self ) : for ws in self . workspace . list ( ) . keys ( ) : self . show workspace ( ws ) print ( \"\\n\\n\" )", "predictions": ["valid all list of all list"], "references": ["show details for all workspaces ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 4844, "code": "def RAMON ( typ ) : if six . PY2 : lookup = [ str , unicode ] elif six . PY3 : lookup = [ str ] if type ( typ ) is int : return ramon types [ typ ] elif type ( typ ) in lookup : return ramon types [ types [ typ ] ]", "predictions": ["keys for the given kwargs"], "references": ["takes str or int returns class type"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 4845, "code": "def nd json ( self , dataset , project , channel list , metadata ) : nd dict = { } nd dict [ 'dataset' ] = self . dataset dict ( * dataset ) nd dict [ 'project' ] = self . project dict ( * project ) nd dict [ 'metadata' ] = metadata nd dict [ 'channels' ] = { } for channel name , value in channel list . items ( ) : nd dict [ 'channels' ] [ channel name ] = self . channel dict ( * value ) return json . dumps ( nd dict , sort keys = True , indent = 4 )", "predictions": ["convert a kwargs to a form form"], "references": ["genarate nd json object ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4846, "code": "def dataset dict ( self , dataset name , imagesize , voxelres , offset , timerange , scalinglevels , scaling ) : dataset dict = { } dataset dict [ 'dataset name' ] = dataset name dataset dict [ 'imagesize' ] = imagesize dataset dict [ 'voxelres' ] = voxelres if offset is not None : dataset dict [ 'offset' ] = offset if timerange is not None : dataset dict [ 'timerange' ] = timerange if scalinglevels is not None : dataset dict [ 'scalinglevels' ] = scalinglevels if scaling is not None : dataset dict [ 'scaling' ] = scaling return dataset dict", "predictions": ["return the field choices ."], "references": ["generate the dataset dictionary"], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 4847, "code": "def channel dict ( self , channel name , datatype , channel type , data url , file format , file type , exceptions , resolution , windowrange , readonly ) : channel dict = { } channel dict [ 'channel name' ] = channel name channel dict [ 'datatype' ] = datatype channel dict [ 'channel type' ] = channel type if exceptions is not None : channel dict [ 'exceptions' ] = exceptions if resolution is not None : channel dict [ 'resolution' ] = resolution if windowrange is not None : channel dict [ 'windowrange' ] = windowrange if readonly is not None : channel dict [ 'readonly' ] = readonly channel dict [ 'data url' ] = data url channel dict [ 'file format' ] = file format channel dict [ 'file type' ] = file type return channel dict", "predictions": ["returns a model choice for a model"], "references": ["generate the project dictionary ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4848, "code": "def project dict ( self , project name , token name , public ) : project dict = { } project dict [ 'project name' ] = project name if token name is not None : if token name == '' : project dict [ 'token name' ] = project name else : project dict [ 'token name' ] = token name else : project dict [ 'token name' ] = project name if public is not None : project dict [ 'public' ] = public return project dict", "predictions": ["adds a tag to the tag dictionary ."], "references": ["genarate the project dictionary ."], "bleu": 0.22679164443904004, "rouge_l": 0.48157894736842105}
{"id": 4849, "code": "def identify imagesize ( self , image type , image path = '/tmp/img.' ) : dims = ( ) try : if ( image type . lower ( ) == 'png' ) : dims = np . shape ( ndpng . load ( '{}{}' . format ( image path , image type ) ) ) elif ( image type . lower ( ) == 'tif' or image type . lower ( ) == 'tiff' ) : dims = np . shape ( ndtiff . load ( '{}{}' . format ( image path , image type ) ) ) else : raise Value Error ( \"Unsupported image type.\" ) except : raise OS Error ( 'The file was not accessible at {}{}' . format ( image path , image type ) ) return dims [ : : - 1 ]", "predictions": ["any field in image ."], "references": ["identify the image size using the data location and other parameters"], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 4850, "code": "def put data ( self , data ) : URL Path = self . oo . url ( \"auto Ingest/\" ) try : response = requests . post ( URL Path , data = json . dumps ( data ) , verify = False ) assert ( response . status code == 200 ) print ( \"From ndio: {}\" . format ( response . content ) ) except : raise OS Error ( . format ( response . status code ) )", "predictions": ["upload file file to server"], "references": ["try to post data to the server ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 4851, "code": "def find path ( name , config , wsonly = False ) : workspace = Workspace ( config ) config = config [ \"workspaces\" ] path list = { } if name . find ( '/' ) != - 1 : wsonly = False try : ws , repo = name . split ( '/' ) except Value Error : raise Value Error ( \"There is too many / in `name` argument. \" \"Argument syntax: `workspace/repository`.\" ) if ( workspace . exists ( ws ) ) : if ( repo in config [ ws ] [ \"repositories\" ] ) : path name = \"%s/%s\" % ( ws , repo ) path list [ path name ] = config [ ws ] [ \"repositories\" ] [ repo ] for ws name , ws in sorted ( config . items ( ) ) : if ( name == ws name ) : if wsonly is True : return { ws name : ws [ \"path\" ] } repositories = sorted ( config [ ws name ] [ \"repositories\" ] . items ( ) ) for name , path in repositories : path list [ \"%s/%s\" % ( ws name , name ) ] = path break for repo name , repo path in sorted ( ws [ \"repositories\" ] . items ( ) ) : if ( repo name == name ) : path list [ \"%s/%s\" % ( ws name , repo name ) ] = repo path return path list", "predictions": ["any of the filepath in the repository in the repository in the repository in the repository in the current user s * * * * * * * * * *"], "references": ["find path for given workspace and|or repository ."], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 4852, "code": "def nvim io recover ( self , io : Nvim IO Recover [ A ] ) -> Nvim IO [ B ] : return eval step ( self . vim ) ( io . map ( lambda a : a ) )", "predictions": ["data representing the decode function . ."], "references": ["calls map to shift the recover execution to flat_map_nvim_io"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 4853, "code": "def ugettext ( message , context = None ) : stripped = strip whitespace ( message ) message = add context ( context , stripped ) if context else stripped ret = django ugettext ( message ) return stripped if ret == message else ret", "predictions": ["convenience function to add a data to the context"], "references": ["always return a stripped string localized if possible"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 4854, "code": "def ungettext ( singular , plural , number , context = None ) : singular stripped = strip whitespace ( singular ) plural stripped = strip whitespace ( plural ) if context : singular = add context ( context , singular stripped ) plural = add context ( context , plural stripped ) else : singular = singular stripped plural = plural stripped ret = django nugettext ( singular , plural , number ) if ret == singular : return singular stripped elif ret == plural : return plural stripped return ret", "predictions": ["handle self . self . add to name ."], "references": ["always return a stripped string localized if possible"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4855, "code": "def install jinja translations ( ) : class Translation ( object ) : ugettext = staticmethod ( ugettext ) ungettext = staticmethod ( ungettext ) import jingo jingo . env . install gettext translations ( Translation )", "predictions": ["remove jinja self . self . ."], "references": ["install our gettext and ngettext functions into jinja2 s environment ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 4856, "code": "def exclusive ns ( guard : State Guard [ A ] , desc : str , thunk : Callable [ ... , NS [ A , B ] ] , * a : Any ) -> Do : yield guard . acquire ( ) log . debug2 ( lambda : f'exclusive: {desc}' ) state , response = yield N . ensure failure ( thunk ( * a ) . run ( guard . state ) , guard . release ) yield N . delay ( lambda v : unsafe update state ( guard , state ) ) yield guard . release ( ) log . debug2 ( lambda : f'release: {desc}' ) yield N . pure ( response )", "predictions": ["list all the debug2 in the same order"], "references": ["this is the central unsafe function using a lock and updating the state in guard in - place ."], "bleu": 0.048218604638712956, "rouge_l": 0.13800904977375564}
{"id": 4857, "code": "def percent ( data , part , total ) : try : return round ( 100 * float ( data [ part ] ) / float ( data [ total ] ) , 1 ) except Zero Division Error : return 0", "predictions": ["if data is none if workspace is not none otherwise if workspace is none if workspace is none"], "references": ["calculate a percentage ."], "bleu": 0.057259987315337726, "rouge_l": 0.0}
{"id": 4858, "code": "def get cache stats ( server name = None ) : server info = { } for svr in mc client . get stats ( ) : svr info = svr [ 0 ] . split ( ' ' ) svr name = svr info [ 0 ] svr stats = svr [ 1 ] svr stats [ 'bytes percent' ] = percent ( svr stats , 'bytes' , 'limit maxbytes' ) svr stats [ 'get hit rate' ] = percent ( svr stats , 'get hits' , 'cmd get' ) svr stats [ 'get miss rate' ] = percent ( svr stats , 'get misses' , 'cmd get' ) if server name and server name == svr name : return svr stats server info [ svr name ] = svr stats return server info", "predictions": ["sync cache self logger logger"], "references": ["get stats info ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 4859, "code": "def get cache slabs ( server name = None ) : server info = { } for svr in mc client . get slabs ( ) : svr info = svr [ 0 ] . split ( ' ' ) svr name = svr info [ 0 ] if server name and server name == svr name : return svr [ 1 ] server info [ svr name ] = svr [ 1 ] return server info", "predictions": ["returns the cache if it exists otherwise returns a dict of the path"], "references": ["get slabs info ."], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 4860, "code": "def context data ( data , request = None ) : try : return dict ( site . each context ( request ) . items ( ) + data . items ( ) ) except Attribute Error : return data", "predictions": ["get check version of json data ."], "references": ["add admin global context for compatibility with django 1 . 7"], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 4861, "code": "def server status ( request ) : data = { 'cache stats' : get cache stats ( ) , 'can get slabs' : hasattr ( mc client , 'get slabs' ) , } return render to response ( 'memcache admin/server status.html' , data , Request Context ( request ) )", "predictions": ["show the execute status status"], "references": ["return the status of all servers ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 4862, "code": "def dashboard ( request ) : if not isinstance ( mc client , dict ) : cache stats = get cache stats ( ) else : cache stats = None if cache stats : data = context data ( { 'title' : ( 'Memcache Dashboard' ) , 'cache stats' : cache stats , 'can get slabs' : hasattr ( mc client , 'get slabs' ) , 'REFRESH RATE' : SETTINGS [ 'REFRESH RATE' ] , } , request ) template = 'memcache admin/dashboard.html' else : data = context data ( { 'title' : ( 'Memcache Dashboard - Error' ) , 'error message' : ( 'Unable to connect to a memcache server.' ) , } , request ) template = 'memcache admin/dashboard error.html' return render to response ( template , data , Request Context ( request ) )", "predictions": ["print the mc information ."], "references": ["show the dashboard ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 4863, "code": "def stats ( request , server name ) : server name = server name . strip ( '/' ) data = context data ( { 'title' : ( 'Memcache Statistics for %s' ) % server name , 'cache stats' : get cache stats ( server name ) , } , request ) return render to response ( 'memcache admin/stats.html' , data , Request Context ( request ) )", "predictions": ["list available cache set . ."], "references": ["show server statistics ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 4864, "code": "def slabs ( request , server name ) : data = context data ( { 'title' : ( 'Memcache Slabs for %s' ) % server name , 'cache slabs' : get cache slabs ( server name ) , } , request ) return render to response ( 'memcache admin/slabs.html' , data , Request Context ( request ) )", "predictions": ["list all the cache for a specific server"], "references": ["show server slabs ."], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 4865, "code": "def human bytes ( value ) : value = float ( value ) if value >= 1073741824 : gigabytes = value / 1073741824 size = '%.2f GB' % gigabytes elif value >= 1048576 : megabytes = value / 1048576 size = '%.2f MB' % megabytes elif value >= 1024 : kilobytes = value / 1024 size = '%.2f KB' % kilobytes else : size = '%.2f B' % value return size", "predictions": ["return a human - readable human - readable string for the given human - readable human - friendly human - friendly value ."], "references": ["convert a byte value into a human - readable format ."], "bleu": 0.16020720994064927, "rouge_l": 0.31410916580844483}
{"id": 4866, "code": "def add ( self , * * kwargs ) : for key in kwargs : if type ( kwargs [ key ] ) == str : self . children [ key ] = Directory ( kwargs [ key ] ) else : self . children [ key ] = kwargs [ key ] self . children [ key ] . env = self self . children [ key ] . apply config ( Config Applicator ( self . config ) ) self . children [ key ] . prepare ( )", "predictions": ["add the configuration to the current dictionary ."], "references": ["add objects to the environment ."], "bleu": 0.239802967618271, "rouge_l": 0.5865384615384615}
{"id": 4867, "code": "def apply config ( self , applicator ) : if type ( self . fpath ) == str : self . fpath = applicator . apply ( self . fpath )", "predictions": ["apply the current configuration to the given applicator ."], "references": ["replace any config tokens in the file s path with values from the config ."], "bleu": 0.08617428905281956, "rouge_l": 0.23921568627450981}
{"id": 4868, "code": "def path ( self ) : if self . parent : return os . path . join ( self . parent . path , self . fpath ) else : return self . fpath", "predictions": ["absolute path to the parent directory ."], "references": ["get the path to the file relative to its parent ."], "bleu": 0.21606281467072083, "rouge_l": 0.5341506129597198}
{"id": 4869, "code": "def read ( self ) : with open ( self . path ) as f : d = f . read ( ) return d", "predictions": ["read the contents of the current directory ."], "references": ["read and return the contents of the file ."], "bleu": 0.4284449847620205, "rouge_l": 0.6984732824427481}
{"id": 4870, "code": "def configure ( self ) : handler = logging . File Handler ( self . path , delay = True ) if self . format : handler . set Formatter ( logging . Formatter ( self . format ) ) if type ( self . formatter ) == str : if self . env and self . env . config . logging . dict config . formatters [ self . formatter ] : d = self . env . config . logging . dict config . formatters [ self . formatter ] . to dict ( ) handler . set Formatter ( logging . Formatter ( * * d ) ) elif type ( self . formatter ) == dict : handler . set Formatter ( logging . Formatter ( * * self . formatter ) ) if len ( self . loggers ) : for name in self . loggers : logging . get Logger ( name ) . add Handler ( handler ) else : logging . get Logger ( ) . add Handler ( handler )", "predictions": ["configure the logging ."], "references": ["configure the python logging module for this file ."], "bleu": 0.1539347200145861, "rouge_l": 0.5754716981132075}
{"id": 4871, "code": "def apply config ( self , applicator ) : if type ( self . path ) == str : self . path = applicator . apply ( self . path ) for key in self . children : self . children [ key ] . apply config ( applicator )", "predictions": ["apply the configuration to the specified applicator ."], "references": ["replace any config tokens with values from the config ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 4872, "code": "def path ( self ) : p = '' if self . parent and self . parent . path : p = os . path . join ( p , self . parent . path ) if self . base : p = os . path . join ( p , self . base ) if self . path : p = os . path . join ( p , self . path ) return p", "predictions": ["absolute path to the parent directory ."], "references": ["return the path to this directory ."], "bleu": 0.32172944208038085, "rouge_l": 0.5714285714285714}
{"id": 4873, "code": "def remove ( self , recursive = True , ignore error = True ) : try : if recursive or self . cleanup == 'recursive' : shutil . rmtree ( self . path ) else : os . rmdir ( self . path ) except Exception as e : if not ignore error : raise e", "predictions": ["remove a file from the filesystem ."], "references": ["remove the directory ."], "bleu": 0.22089591134157885, "rouge_l": 0.5736677115987461}
{"id": 4874, "code": "def path to ( self , path ) : return os . path . join ( self . path , str ( path ) )", "predictions": ["return relative path to the given path ."], "references": ["find the path to something inside this directory ."], "bleu": 0.2116253761537182, "rouge_l": 0.34923664122137404}
{"id": 4875, "code": "def list ( self ) : return [ File ( f , parent = self ) for f in os . listdir ( self . path ) ]", "predictions": ["returns a list of all the files in the directory ."], "references": ["list the contents of the directory ."], "bleu": 0.24384183193426084, "rouge_l": 0.5787476280834916}
{"id": 4876, "code": "def write ( self , filename , data , mode = 'w' ) : with open ( self . path to ( str ( filename ) ) , mode ) as f : f . write ( data )", "predictions": ["write data to a file ."], "references": ["write to a file in the directory ."], "bleu": 0.3260175477276487, "rouge_l": 0.6963470319634703}
{"id": 4877, "code": "def read ( self , filename ) : with open ( self . path to ( str ( filename ) ) ) as f : d = f . read ( ) return d", "predictions": ["read the contents of a file ."], "references": ["read a file from the directory ."], "bleu": 0.29071536848410967, "rouge_l": 0.5714285714285714}
{"id": 4878, "code": "def add ( self , * args , * * kwargs ) : for key in kwargs : if isinstance ( kwargs [ key ] , str ) : self . children [ key ] = File ( kwargs [ key ] ) else : self . children [ key ] = kwargs [ key ] self . children [ key ] . parent = self self . children [ key ] . env = self . env added = [ ] for arg in args : if isinstance ( arg , File ) : self . children [ arg . name ] = arg self . children [ arg . name ] . parent = self self . children [ arg . name ] . env = self . env elif isinstance ( arg , str ) : f = File ( arg ) added . append ( f ) self . children [ arg ] = f self . children [ arg ] . parent = self self . children [ arg ] . env = self . env else : raise Type Error ( type ( arg ) ) if len ( added ) == 1 : return added [ 0 ] if len ( args ) == 1 : return args [ 0 ]", "predictions": ["add a new command to this node ."], "references": ["add objects to the directory ."], "bleu": 0.19070828081828378, "rouge_l": 0.43990384615384615}
{"id": 4879, "code": "def save ( self ) : with open ( self . path , 'w' ) as f : f . write ( yaml . dump ( dict ( self . d ) ) )", "predictions": ["save the values to disk"], "references": ["save the state to a file ."], "bleu": 0.25880882365505126, "rouge_l": 0.48541114058355433}
{"id": 4880, "code": "def load ( self ) : if os . path . exists ( self . path ) : with open ( self . path , 'r' ) as f : self . d = yaml . safe load ( f . read ( ) . replace ( '\\t' , ' ' * 4 ) )", "predictions": ["load the config from disk"], "references": ["load a saved state file ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 4881, "code": "def cleanup ( self ) : if os . path . exists ( self . path ) : os . remove ( self . path )", "predictions": ["remove the temporary directory ."], "references": ["clean up the saved state ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 4882, "code": "def get value ( self ) : if self . path : try : container , last = self . resolve path ( ) return container [ last ] except Key Error : return None except Index Error : return None else : return self . data", "predictions": ["return the value of the container ."], "references": ["get the value represented by this node ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 4883, "code": "def load ( self , reload = False ) : if reload or not self . loaded : if self . defaults file and type ( self . defaults file ) == str : self . defaults file = File ( self . defaults file , parent = self . parent ) defaults = { } if self . defaults file : defaults = yaml . safe load ( self . defaults file . read ( ) . replace ( '\\t' , '    ' ) ) data = { } if self . exists : data = yaml . safe load ( self . read ( ) . replace ( '\\t' , '    ' ) ) self . defaults = defaults self . data = copy . deepcopy ( self . defaults ) self . update ( data = data ) if self . apply env : self . update ( Config Env ( self . env prefix ) ) self . loaded = True return self", "predictions": ["load the configuration from the config file ."], "references": ["load the config and defaults from files ."], "bleu": 0.2777619034011791, "rouge_l": 0.5}
{"id": 4884, "code": "def apply to str ( self , obj ) : toks = re . split ( '({config:|})' , obj ) newtoks = [ ] try : while len ( toks ) : tok = toks . pop ( 0 ) if tok == '{config:' : var = toks . pop ( 0 ) val = self . config [ var ] if type ( val ) == Config Node and val == None : raise Key Error ( \"No such config variable '{}'\" . format ( var ) ) newtoks . append ( str ( val ) ) toks . pop ( 0 ) else : newtoks . append ( tok ) return '' . join ( newtoks ) except Index Error : pass return obj", "predictions": ["apply the config variable to a string"], "references": ["apply the config to a string ."], "bleu": 0.5, "rouge_l": 0.8571428571428571}
{"id": 4885, "code": "def process input ( self ) : try : pyngus . read socket input ( self . connection , self . socket ) except Exception as e : LOG . error ( \"Exception on socket read: %s\" , str ( e ) ) self . connection . close input ( ) self . connection . close ( ) self . connection . process ( time . time ( ) )", "predictions": ["process incoming input socket ."], "references": ["called when socket is read - ready"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 4886, "code": "def send output ( self ) : try : pyngus . write socket output ( self . connection , self . socket ) except Exception as e : LOG . error ( \"Exception on socket write: %s\" , str ( e ) ) self . connection . close output ( ) self . connection . close ( ) self . connection . process ( time . time ( ) )", "predictions": ["send the output socket to the socket ."], "references": ["called when socket is write - ready"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4887, "code": "def send request ( self ) : msg = Message ( ) msg . subject = \"An RPC call!\" msg . address = self . to msg . reply to = self . reply to msg . body = self . method msg . correlation id = 5 print ( \"sending RPC call request: %s\" % str ( self . method ) ) self . sender . send ( msg , self )", "predictions": ["send a request to the server ."], "references": ["send a message containing the rpc method call"], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 4888, "code": "def configure ( self , target address , source address , handler , properties ) : self . handler = handler self . properties = properties dynamic props = None if properties : dynamic props = properties . get ( \"dynamic-node-properties\" ) mode = dist modes . get ( properties . get ( \"distribution-mode\" ) ) if mode is not None : self . pn link . source . distribution mode = mode mode = snd settle modes . get ( properties . get ( \"snd-settle-mode\" ) ) if mode is not None : self . pn link . snd settle mode = mode mode = rcv settle modes . get ( properties . get ( \"rcv-settle-mode\" ) ) if mode is not None : self . pn link . rcv settle mode = mode if target address is None : if not self . pn link . is sender : raise Exception ( \"Dynamic target not allowed\" ) self . pn link . target . dynamic = True if dynamic props : self . pn link . target . properties . clear ( ) self . pn link . target . properties . put dict ( dynamic props ) elif target address : self . pn link . target . address = target address if source address is None : if not self . pn link . is receiver : raise Exception ( \"Dynamic source not allowed\" ) self . pn link . source . dynamic = True if dynamic props : self . pn link . source . properties . clear ( ) self . pn link . source . properties . put dict ( dynamic props ) elif source address : self . pn link . source . address = source address", "predictions": ["configure the link and dynamic properties ."], "references": ["assign addresses properties etc ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 4889, "code": "def source address ( self ) : if self . pn link . is sender : return self . pn link . source . address else : return self . pn link . remote source . address", "predictions": ["source address of the source ."], "references": ["return the authorative source of the link ."], "bleu": 0.236682065782701, "rouge_l": 0.5570776255707762}
{"id": 4890, "code": "def target address ( self ) : if self . pn link . is receiver : return self . pn link . target . address else : return self . pn link . remote target . address", "predictions": ["target address of the target link ."], "references": ["return the authorative target of the link ."], "bleu": 0.2789001430384383, "rouge_l": 0.6587473002159828}
{"id": 4891, "code": "def session closed ( self ) : if self . endpoint state & proton . Endpoint . REMOTE ACTIVE : self . process remote state ( ) elif self . endpoint state & proton . Endpoint . REMOTE UNINIT : self . failed = True self . link failed ( \"Parent session closed.\" )", "predictions": ["session to the session ."], "references": ["remote has closed the session used by this link ."], "bleu": 0.14203729394569906, "rouge_l": 0.37731958762886597}
{"id": 4892, "code": "def reject ( self , pn condition = None ) : self . pn link . source . type = proton . Terminus . UNSPECIFIED super ( Sender Link , self ) . reject ( pn condition )", "predictions": ["reject this widget ."], "references": ["see link reject amqp1 . 0 spec ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 4893, "code": "def process delivery ( self , pn delivery ) : if pn delivery . tag in self . send requests : if pn delivery . settled or pn delivery . remote state : outcome = pn delivery . remote state state = Sender Link . DISPOSITION STATE MAP . get ( outcome , self . UNKNOWN ) pn disposition = pn delivery . remote info = { } if state == Sender Link . REJECTED : if pn disposition . condition : info [ \"condition\" ] = pn disposition . condition elif state == Sender Link . MODIFIED : info [ \"delivery-failed\" ] = pn disposition . failed info [ \"undeliverable-here\" ] = pn disposition . undeliverable annotations = pn disposition . annotations if annotations : info [ \"message-annotations\" ] = annotations send req = self . send requests . pop ( pn delivery . tag ) send req . destroy ( state , info ) pn delivery . settle ( ) elif pn delivery . writable : if self . pending sends : tag = self . pending sends . popleft ( ) send req = self . send requests [ tag ] self . write msg ( pn delivery , send req ) else : LOG . debug ( \"Delivery ignored, tag=%s\" , str ( pn delivery . tag ) ) pn delivery . settle ( )", "predictions": ["process a delivery delivery from the remote delivery ."], "references": ["check if the delivery can be processed ."], "bleu": 0.16784459625186196, "rouge_l": 0.35672514619883033}
{"id": 4894, "code": "def reject ( self , pn condition = None ) : self . pn link . target . type = proton . Terminus . UNSPECIFIED super ( Receiver Link , self ) . reject ( pn condition )", "predictions": ["reject this widget ."], "references": ["see link reject amqp1 . 0 spec ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 4895, "code": "def process delivery ( self , pn delivery ) : if pn delivery . readable and not pn delivery . partial : data = self . pn link . recv ( pn delivery . pending ) msg = proton . Message ( ) msg . decode ( data ) self . pn link . advance ( ) if self . handler : handle = \"rmsg-%s:%x\" % ( self . name , self . next handle ) self . next handle += 1 self . unsettled deliveries [ handle ] = pn delivery with self . callback lock : self . handler . message received ( self , msg , handle ) else : pn delivery . settle ( )", "predictions": ["process incoming messages from device"], "references": ["check if the delivery can be processed ."], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 4896, "code": "def new sender ( self , name ) : pn link = self . pn session . sender ( name ) return self . request sender ( pn link )", "predictions": ["create a new object for a new { tablename } { % } { % } { % } { % } { % } { % } { % }"], "references": ["create a new sender link ."], "bleu": 0.07261813302549416, "rouge_l": 0.18466195761856707}
{"id": 4897, "code": "def request sender ( self , pn link ) : sl = Sender Link ( self . connection , pn link ) self . links . add ( sl ) return sl", "predictions": ["human a bytes object for a = = 0 size size size size size size size size size ."], "references": ["create link from request for a sender ."], "bleu": 0.09107438368292149, "rouge_l": 0.2398427260812582}
{"id": 4898, "code": "def new receiver ( self , name ) : pn link = self . pn session . receiver ( name ) return self . request receiver ( pn link )", "predictions": ["create a add receiver if it doesn t exist if it doesn t exist if it doesn t exist if it doesn t already exist if it already exist if it"], "references": ["create a new receiver link ."], "bleu": 0.055177848898164926, "rouge_l": 0.18466195761856707}
{"id": 4899, "code": "def request receiver ( self , pn link ) : rl = Receiver Link ( self . connection , pn link ) self . links . add ( rl ) return rl", "predictions": ["apply a config link to the = 0 str str str str str str str str str str str str str str str str str str str str str str str"], "references": ["create link from request for a receiver ."], "bleu": 0.04317900023606586, "rouge_l": 0.05738476011288805}
{"id": 4900, "code": "def link destroyed ( self , link ) : self . links . discard ( link ) if not self . links : LOG . debug ( \"destroying unneeded session\" ) self . pn session . close ( ) self . pn session . free ( ) self . pn session = None self . connection = None", "predictions": ["path path to websocket connection parent"], "references": ["link has been destroyed ."], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 4901, "code": "def ep need close ( self ) : LOG . debug ( \"Session %s close requested - closing...\" , self . name ) links = self . links . copy ( ) for link in links : link . session closed ( )", "predictions": ["closes all links connections path path path path path path path path path path path path path path path path path path path path path path path path path path path"], "references": ["peer has closed its end of the session ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4902, "code": "def extend Markdown ( self , md , md globals ) : mark tag = Simple Tag Pattern ( MARK RE , 'mark' ) md . inline Patterns . add ( 'mark' , mark tag , ' begin' )", "predictions": ["add an type of formatter to the type . . . . . . . . instance . ."], "references": ["modifies inline patterns ."], "bleu": 0.06439931429457924, "rouge_l": 0.09854604200323101}
{"id": 4903, "code": "def receiver remote closed ( self , receiver link , pn condition ) : LOG . debug ( \"receiver remote closed condition=%s\" , pn condition ) receiver link . close ( ) self . done = True", "predictions": ["handle apply apply apply apply apply apply apply apply apply apply apply apply apply apply apply apply apply to apply apply apply apply apply apply apply apply link == link =="], "references": ["peer has closed its end of the link ."], "bleu": 0.03901663112717908, "rouge_l": 0.055505004549590536}
{"id": 4904, "code": "def receiver failed ( self , receiver link , error ) : LOG . warn ( \"receiver failed error=%s\" , error ) receiver link . close ( ) self . done = True", "predictions": ["close a path to the path parent parent parent parent parent parent parent parent parent parent parent parent parent parent parent parent parent parent parent method"], "references": ["protocol error occurred ."], "bleu": 0.03925345689749394, "rouge_l": 0.0}
{"id": 4905, "code": "def get host port ( server address ) : regex = re . compile ( r\"^amqp://([a-z A-Z0-9.]+)(:([\\d]+))?$\" ) x = regex . match ( server address ) if not x : raise Exception ( \"Bad address syntax: %s\" % server address ) matches = x . groups ( ) host = matches [ 0 ] port = int ( matches [ 2 ] ) if matches [ 2 ] else None return host , port", "predictions": ["return self try to find the self try to the recursive self try to the recursive self try"], "references": ["parse the hostname and port out of the server_address ."], "bleu": 0.07535838128770536, "rouge_l": 0.1506172839506173}
{"id": 4906, "code": "def connect socket ( host , port , blocking = True ) : addr = socket . getaddrinfo ( host , port , socket . AF INET , socket . SOCK STREAM ) if not addr : raise Exception ( \"Could not translate address '%s:%s'\" % ( host , str ( port ) ) ) my socket = socket . socket ( addr [ 0 ] [ 0 ] , addr [ 0 ] [ 1 ] , addr [ 0 ] [ 2 ] ) if not blocking : my socket . setblocking ( 0 ) try : my socket . connect ( addr [ 0 ] [ 4 ] ) except socket . error as e : if e . errno != errno . EINPROGRESS : raise return my socket", "predictions": ["path to a self ."], "references": ["create a tcp connection to the server ."], "bleu": 0.1781815298791261, "rouge_l": 0.2953995157384988}
{"id": 4907, "code": "def server socket ( host , port , backlog = 10 ) : addr = socket . getaddrinfo ( host , port , socket . AF INET , socket . SOCK STREAM ) if not addr : raise Exception ( \"Could not translate address '%s:%s'\" % ( host , str ( port ) ) ) my socket = socket . socket ( addr [ 0 ] [ 0 ] , addr [ 0 ] [ 1 ] , addr [ 0 ] [ 2 ] ) my socket . setblocking ( 0 ) try : my socket . bind ( addr [ 0 ] [ 4 ] ) my socket . listen ( backlog ) except socket . error as e : if e . errno != errno . EINPROGRESS : raise return my socket", "predictions": ["run a list of address"], "references": ["create a tcp listening socket for a server ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 4908, "code": "def process ( self , now ) : if self . pn connection is None : LOG . error ( \"Connection.process() called on destroyed connection!\" ) return 0 if self . pn connection . state & proton . Endpoint . LOCAL UNINIT : return 0 if self . pn sasl and not self . sasl done : if ( PROTON VERSION < ( 0 , 10 ) ) : if self . pn sasl . state not in ( proton . SASL . STATE PASS , proton . SASL . STATE FAIL ) : LOG . debug ( \"SASL in progress. State=%s\" , str ( self . pn sasl . state ) ) if self . handler : with self . callback lock : self . handler . sasl step ( self , self . pn sasl ) return self . next deadline self . sasl done = True if self . handler : with self . callback lock : self . handler . sasl done ( self , self . pn sasl , self . pn sasl . outcome ) else : if self . pn sasl . outcome is not None : self . sasl done = True if self . handler : with self . callback lock : self . handler . sasl done ( self , self . pn sasl , self . pn sasl . outcome ) timer deadline = self . expire timers ( now ) transport deadline = self . pn transport . tick ( now ) if timer deadline and transport deadline : self . next deadline = min ( timer deadline , transport deadline ) else : self . next deadline = timer deadline or transport deadline pn event = self . pn collector . peek ( ) while pn event : if Link . handle proton event ( pn event , self ) : pass elif self . handle proton event ( pn event ) : pass elif Session Proxy . handle proton event ( pn event , self ) : pass self . pn collector . pop ( ) pn event = self . pn collector . peek ( ) if self . error : if self . handler : self . next deadline = now with self . callback lock : self . handler . connection failed ( self , self . error ) elif ( self . endpoint state == self . CLOSED and self . read done and self . write done ) : if self . handler : with self . callback lock : self . handler . connection closed ( self ) return self . next deadline", "predictions": ["handle the timer event and handle the callback"], "references": ["perform connection state processing ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4909, "code": "def output data ( self ) : c = self . has output if c <= 0 : return None try : buf = self . pn transport . peek ( c ) except Exception as e : self . connection failed ( str ( e ) ) return None return buf", "predictions": ["as a string of the read data data"], "references": ["get a buffer of data that needs to be written to the network ."], "bleu": 0.09525245831601728, "rouge_l": 0.25994318181818177}
{"id": 4910, "code": "def create sender ( self , source address , target address = None , event handler = None , name = None , properties = None ) : ident = name or str ( source address ) if ident in self . sender links : raise Key Error ( \"Sender %s already exists!\" % ident ) session = Session Proxy ( \"session-%s\" % ident , self ) session . open ( ) sl = session . new sender ( ident ) sl . configure ( target address , source address , event handler , properties ) self . sender links [ ident ] = sl return sl", "predictions": ["add a new sender to the args else the given args else ."], "references": ["factory method for sender links ."], "bleu": 0.10571070857151538, "rouge_l": 0.22550831792975967}
{"id": 4911, "code": "def reject sender ( self , link handle , pn condition = None ) : link = self . sender links . get ( link handle ) if not link : raise Exception ( \"Invalid link handle: %s\" % link handle ) link . reject ( pn condition ) link . destroy ( )", "predictions": ["save a with the given with the given with the given with the given with the given with the given open with the given with the given with the given with"], "references": ["rejects the senderlink and destroys the handle ."], "bleu": 0.04317900023606586, "rouge_l": 0.1147695202257761}
{"id": 4912, "code": "def create receiver ( self , target address , source address = None , event handler = None , name = None , properties = None ) : ident = name or str ( target address ) if ident in self . receiver links : raise Key Error ( \"Receiver %s already exists!\" % ident ) session = Session Proxy ( \"session-%s\" % ident , self ) session . open ( ) rl = session . new receiver ( ident ) rl . configure ( target address , source address , event handler , properties ) self . receiver links [ ident ] = rl return rl", "predictions": ["load a receiver replace with the given os replace ."], "references": ["factory method for creating receive links ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 4913, "code": "def connection failed ( self , error = \"Error not specified!\" ) : if not self . error : LOG . error ( \"Connection failed: %s\" , str ( error ) ) self . error = error", "predictions": ["called when the cleanup is failed ."], "references": ["clean up after connection failure detected ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4914, "code": "def ep active ( self ) : LOG . debug ( \"Connection is up\" ) if self . handler : with self . callback lock : self . handler . connection active ( self )", "predictions": ["called when the value is value ."], "references": ["both ends of the endpoint have become active ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 4915, "code": "def ep need close ( self ) : LOG . debug ( \"Connection remotely closed\" ) if self . handler : cond = self . pn connection . remote condition with self . callback lock : self . handler . connection remote closed ( self , cond )", "predictions": ["closes the connection . . . . . ."], "references": ["the remote has closed its end of the endpoint ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 4916, "code": "def ep error ( self , error ) : super ( Connection , self ) . ep error ( error ) self . connection failed ( \"Protocol error occurred.\" )", "predictions": ["set to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["the endpoint state machine failed due to protocol error ."], "bleu": 0.03901663112717908, "rouge_l": 0.05374449339207048}
{"id": 4917, "code": "def get color string ( self ) : s = '' if self . color type == 'd' : if self . name is \"black\" : s = '%.3f G' % 0 else : s = '%.3f %.3f %.3f RG' % ( self . red / 255.0 , self . green / 255.0 , self . blue / 255.0 ) elif self . color type == 'f' or self . color type == 't' : if self . name is \"black\" : s = '%.3f g' % 0 else : s = '%.3f %.3f %.3f rg' % ( self . red / 255.0 , self . green / 255.0 , self . blue / 255.0 ) return s", "predictions": ["process the input string string"], "references": ["adobe output string for defining colors"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 4918, "code": "def set font ( self , family = None , style = None , size = None ) : if style is not None : if 'B' in style : family += ' bold' if 'I' in style : family += ' italic' self . set family ( family ) self . get diffs ( ) self . set style ( style ) self . set metrics ( ) self . set size ( size ) self . set font key ( )", "predictions": ["send a output output of the output"], "references": ["select a font ; size given in points"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 4919, "code": "def string width ( self , s ) : s = str ( s ) w = 0 for char in s : char = ord ( char ) w += self . character widths [ char ] return w * self . font size / 1000.0", "predictions": ["get the request request request request request request request body body body body body body body"], "references": ["get width of a string in the current font"], "bleu": 0.08513012360883544, "rouge_l": 0.16850828729281767}
{"id": 4920, "code": "def get ttf ( self ) : font dict = { } families = [ ] rootdirlist = string . split ( self . search path , os . pathsep ) #for rootdir in rootdirlist: for dir Name , subdir List , filelist in itertools . chain . from iterable ( os . walk ( path ) for path in rootdirlist ) : for item in filelist : root , ext = os . path . splitext ( item ) if ext == '.ttf' : if root [ 0 ] . lower ( ) in english : source = os . path . join ( dir Name , item ) name = root . lower ( ) . replace ( ' ' , ' ' ) if ' bold' in name : name = name . replace ( ' bold' , ' bold' ) if ' italic' in name : name = name . replace ( ' italic' , ' italic' ) elif 'bold' in name : name = name . replace ( 'bold' , ' bold' ) if 'italic' in name : name = name . replace ( 'italic' , ' italic' ) elif ' italic' in name : name = name . replace ( ' italic' , ' italic' ) elif 'italic' in name : name = name . replace ( 'italic' , ' italic' ) elif 'oblique' in name : name = name . replace ( 'oblique' , ' italic' ) else : families . append ( name ) font dict [ name ] = source else : source = os . path . join ( dir Name , item ) name = root . lower ( ) . replace ( ' ' , ' ' ) font dict [ name ] = source families . append ( name ) self . font dict = font dict self . families = families", "predictions": ["configure the source map of the source directory if any ."], "references": ["given a search path find file with requested extension"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 4921, "code": "def put stream ( self , stream ) : self . out ( 'stream' ) self . out ( stream ) self . out ( 'endstream' )", "predictions": ["source this address to the given address ."], "references": ["creates a pdf text stream sandwich ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4922, "code": "def set font size ( self , size ) : if self . font . font size == size : pass else : self . font . set size ( size )", "predictions": ["target size size size"], "references": ["convenience method for just changing font size ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 4923, "code": "def add pie chart ( self , data , cursor , width , height , title = None , data type = \"raw\" , fill colors = None , labels = False , background = None , legend = None ) : save draw color = self . draw color save fill color = self . fill color chart = PDF Pie Chart ( self . session , self . page , data , cursor , width , height , title , data type , fill colors , labels , background , legend ) self . set draw color ( save draw color ) self . set fill color ( save fill color )", "predictions": ["session is a closed chart"], "references": ["data type may be raw or percent"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 4924, "code": "def output ( self ) : self . session . out ( '<</Type /X Object' ) self . session . out ( '/Subtype /Image' ) self . session . out ( '/Width %s' % self . width ) self . session . out ( '/Height %s' % self . height ) if self . colorspace is 'Indexed' : self . session . out ( '/Color Space [/Indexed /Device RGB %s %s 0 R' % ( self . pal , self . number + 1 ) ) else : self . session . out ( '/Color Space /%s' % self . colorspace ) if self . colorspace is 'Device CMYK' : self . session . out ( '/Decode [1 0 1 0 1 0 1 0]' ) self . session . out ( '/Bits Per Component %s' % self . bits per component ) if self . filter : self . session . out ( '/Filter /%s' % self . filter ) if self . decode : self . session . out ( '/Decode Parms << %s >>' % self . decode ) if self . transparent : self . session . out ( '/Mask [%s]' % self . transparent string ) if self . soft mask : self . session . out ( '/S Mask %s 0 R' % ( self . number + 1 ) ) self . session . out ( '/Length %s >>' % self . size ) self . session . put stream ( self . image data ) self . session . out ( 'endobj' ) if self . colorspace is 'Indexed' : self . session . out ( '<<%s /Length %s >>' % ( self . palette filter , self . palette length ) ) self . session . put stream ( self . palette ) self . session . out ( 'endobj' ) if isinstance ( self . soft mask , PDF Image ) : obj = self . session . add object ( ) self . soft mask . set number ( obj . id ) self . soft mask . output ( )", "predictions": ["write the soft instance condition to the soft condition condition condition condition condition condition condition condition condition condition condition condition condition condition condition condition condition ."], "references": ["prompts the creating of image objects ."], "bleu": 0.051660454541342535, "rouge_l": 0.1352549889135255}
{"id": 4925, "code": "def absolute position ( self , x , y ) : ( a , b , c , d , e , f ) = self . current Matrix xp = a * x + c * y + e yp = b * x + d * y + f return xp , yp", "predictions": ["compute the process delivery delivery for the given x remote and y remote delivery ."], "references": ["return the absolute position of x y in user space w . r . t . default user space"], "bleu": 0.07922458662422581, "rouge_l": 0.23040604343720492}
{"id": 4926, "code": "def set font ( self , family = None , style = None , size = None ) : self . set family ( family ) self . set style ( style ) self . set size ( size ) self . set font key ( ) self . set name ( ) self . set character widths ( )", "predictions": ["reject the font for this widget target target target target target target target target target target target target target target target target target target target target target target target target target"], "references": ["select a font ; size given in points"], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 4927, "code": "def string width ( self , s ) : s = str ( s ) w = 0 for i in s : w += self . character widths [ i ] return w * self . font size / 1000.0", "predictions": ["return the process delivery"], "references": ["get width of a string in the current font"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 4928, "code": "def set display mode ( self , zoom = 'fullpage' , layout = 'continuous' ) : self . zoom options = [ \"fullpage\" , \"fullwidth\" , \"real\" , \"default\" ] self . layout options = [ \"single\" , \"continuous\" , \"two\" , \"default\" ] if zoom in self . zoom options or ( isinstance ( zoom , int ) and 0 < zoom <= 100 ) : self . zoom mode = zoom else : raise Exception ( 'Incorrect zoom display mode: ' + zoom ) if layout in self . layout options : self . layout mode = layout else : raise Exception ( 'Incorrect layout display mode: ' + layout )", "predictions": ["set display mode ."], "references": ["set the default viewing options ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 4929, "code": "def close ( self ) : self . document . set page numbers ( ) self . put header ( ) self . put pages ( ) self . put resources ( ) self . put information ( ) self . put catalog ( ) self . put trailer ( ) if hasattr ( self . destination , \"write\" ) : output = self . output to io ( ) elif self . destination == 'string' : output = self . output to string ( ) else : self . output to file ( ) output = None return output", "predictions": ["close the file ."], "references": ["prompt the objects to output pdf code and save to file ."], "bleu": 0.06876828939330318, "rouge_l": 0.34398496240601506}
{"id": 4930, "code": "def put header ( self ) : self . session . out ( '%%PDF-%s' % self . pdf version ) if self . session . compression : self . session . buffer += '%' + chr ( 235 ) + chr ( 236 ) + chr ( 237 ) + chr ( 238 ) + \"\\n\"", "predictions": ["put the header into the pdf ."], "references": ["standard first line in a pdf ."], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 4931, "code": "def put resource dict ( self ) : self . session . add object ( 2 ) self . session . out ( '<<' ) self . session . out ( '/Proc Set [/PDF /Text /Image B /Image C /Image I]' ) self . session . out ( '/Font <<' ) for font in self . document . fonts : self . session . out ( '/F%s %s 0 R' % ( font . index , font . number ) ) self . session . out ( '>>' ) if self . document . images : self . session . out ( '/X Object <<' ) for image in self . document . images : self . session . out ( '/I%s %s 0 R' % ( image . index , image . number ) ) self . session . out ( '>>' ) self . session . out ( '>>' ) self . session . out ( 'endobj' )", "predictions": ["put current resource dict into the session ."], "references": ["creates pdf reference to resource objects ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 4932, "code": "def put information ( self ) : self . session . add object ( ) self . session . out ( '<<' ) self . session . out ( '/Producer ' + self . text to string ( ) ) if self . title : self . session . out ( '/Title ' + self . text to string ( self . title ) ) if self . subject : self . session . out ( '/Subject ' + self . text to string ( self . subject ) ) if self . author : self . session . out ( '/Author ' + self . text to string ( self . author ) ) if self . keywords : self . session . out ( '/Keywords ' + self . text to string ( self . keywords ) ) if self . creator : self . session . out ( '/Creator ' + self . text to string ( self . creator ) ) self . session . out ( '/Creation Date ' + self . text to string ( 'D:' + datetime . now ( ) . strftime ( '%Y%m%d%H%M%S' ) ) ) self . session . out ( '>>' ) self . session . out ( 'endobj' )", "predictions": ["put information about the information about the information in the session ."], "references": ["pdf information object ."], "bleu": 0.11498759556447223, "rouge_l": 0.27477477477477474}
{"id": 4933, "code": "def x fit ( self , test length ) : if ( self . x + test length ) >= self . xmax : return False else : return True", "predictions": ["check if the x is fit"], "references": ["test to see if the line can has enough space for the given length ."], "bleu": 0.06486736672746916, "rouge_l": 0.17681159420289855}
{"id": 4934, "code": "def y fit ( self , test length ) : if ( self . y + test length ) >= self . ymax : return False else : return True", "predictions": ["return true if the y is fit"], "references": ["test to see if the page has enough space for the given text height ."], "bleu": 0.07796037894057231, "rouge_l": 0.17062937062937064}
{"id": 4935, "code": "def x is greater than ( self , test ordinate ) : self . is coordinate ( test ordinate ) if self . x > test ordinate . x : return True else : return False", "predictions": ["check if x is greater than the coordinate ."], "references": ["comparison for x coordinate"], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 4936, "code": "def y is greater than ( self , test ordinate ) : self . is coordinate ( test ordinate ) if self . y > test ordinate . y : return True else : return False", "predictions": ["check if the test is greater than the given test"], "references": ["comparison for y coordinate"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 4937, "code": "def copy ( self ) : new cursor = self . class ( self . x , self . y ) new cursor . set bounds ( self . xmin , self . ymin , self . xmax , self . ymax , self . ymaxmax ) new cursor . set deltas ( self . dx , self . dy ) return new cursor", "predictions": ["create a copy of this cursor ."], "references": ["create a copy and return it ."], "bleu": 0.3655552228545123, "rouge_l": 0.5714285714285714}
{"id": 4938, "code": "def x plus ( self , dx = None ) : if dx is None : self . x += self . dx else : self . x = self . x + dx", "predictions": ["change x to dx"], "references": ["mutable x addition . defaults to set delta value ."], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 4939, "code": "def y plus ( self , dy = None ) : if dy is None : self . y += self . dy else : self . y = self . y + dy", "predictions": ["plus y - axis"], "references": ["mutable y addition . defaults to set delta value ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 4940, "code": "def draw ( self ) : self . compile ( ) self . rows [ 0 ] . advance first row ( ) self . set borders ( ) self . draw fill ( ) self . draw borders ( ) self . draw text ( ) self . set final cursor ( )", "predictions": ["draw the first row of the first row ."], "references": ["don t use this use document . draw_table"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 4941, "code": "def setup ( app ) : app . setup extension ( 'sphinx.ext.todo' ) app . setup extension ( 'sphinx.ext.mathjax' ) app . setup extension ( \"sphinx.ext.intersphinx\" ) app . config . intersphinx mapping . update ( { 'https://docs.python.org/' : None } ) app . config . intersphinx mapping . update ( { sage doc url + doc + \"/\" : None for doc in sage documents } ) app . config . intersphinx mapping . update ( { sage doc url + \"reference/\" + module : None for module in sage modules } ) app . setup extension ( \"sphinx.ext.extlinks\" ) app . config . extlinks . update ( { 'python' : ( 'https://docs.python.org/release/' + pythonversion + '/%s' , '' ) , 'trac' : ( 'https://trac.sagemath.org/%s' , 'trac ticket #' ) , 'wikipedia' : ( 'https://en.wikipedia.org/wiki/%s' , 'Wikipedia article ' ) , 'arxiv' : ( 'http://arxiv.org/abs/%s' , 'Arxiv ' ) , 'oeis' : ( 'https://oeis.org/%s' , 'OEIS sequence ' ) , 'doi' : ( 'https://dx.doi.org/%s' , 'doi:' ) , 'pari' : ( 'http://pari.math.u-bordeaux.fr/dochtml/help/%s' , 'pari:' ) , 'mathscinet' : ( 'http://www.ams.org/mathscinet-getitem?mr=%s' , 'Math Sci Net ' ) } ) app . config . html theme = 'sage'", "predictions": ["setup the plugin ."], "references": ["initialize this sphinx extension"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 4942, "code": "def duration ( self ) : ecc = self . ecc if not np . isnan ( self . ecc ) else np . sqrt ( self . ecw ** 2 + self . esw ** 2 ) esw = self . esw if not np . isnan ( self . esw ) else ecc * np . sin ( self . w ) a Rs = ( ( G * self . rhos * ( 1. + self . Mp Ms ) * ( self . per * DAYSEC ) ** 2. ) / ( 3. * np . pi ) ) ** ( 1. / 3. ) inc = np . arccos ( self . bcirc / a Rs ) becc = self . bcirc * ( 1 - ecc ** 2 ) / ( 1 - esw ) tdur = self . per / 2. / np . pi * np . arcsin ( ( ( 1. + self . Rp Rs ) ** 2 - becc ** 2 ) ** 0.5 / ( np . sin ( inc ) * a Rs ) ) tdur *= np . sqrt ( 1. - ecc ** 2. ) / ( 1. - esw ) return tdur", "predictions": ["return the duration in degrees between the duration and energy ."], "references": ["the approximate transit duration for the general case of an eccentric orbit"], "bleu": 0.12368857073777001, "rouge_l": 0.25884016973125884}
{"id": 4943, "code": "def update ( self , * * kwargs ) : if kwargs . get ( 'verify kwargs' , True ) : valid = [ y [ 0 ] for x in [ TRANSIT , LIMBDARK , SETTINGS ] for y in x . fields ] valid += [ 'b' , 'times' ] for k in kwargs . keys ( ) : if k not in valid : raise Exception ( \"Invalid kwarg '%s'.\" % k ) if ( 'q1' in kwargs . keys ( ) ) and ( 'q2' in kwargs . keys ( ) ) : kwargs . update ( { 'ldmodel' : KIPPING } ) elif ( 'c1' in kwargs . keys ( ) ) and ( 'c2' in kwargs . keys ( ) ) and ( 'c3' in kwargs . keys ( ) ) and ( 'c4' in kwargs . keys ( ) ) : kwargs . update ( { 'ldmodel' : NONLINEAR } ) self . limbdark . update ( * * kwargs ) self . transit . update ( * * kwargs ) self . settings . update ( * * kwargs )", "predictions": ["update the document with the given arguments ."], "references": ["update the transit keyword arguments"], "bleu": 0.22679164443904004, "rouge_l": 0.48157894736842105}
{"id": 4944, "code": "def Compute ( self ) : err = Compute ( self . transit , self . limbdark , self . settings , self . arrays ) if err != ERR NONE : Raise Error ( err )", "predictions": ["create an error if it doesn t exist ."], "references": ["computes the light curve model"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4945, "code": "def Bin ( self ) : err = Bin ( self . transit , self . limbdark , self . settings , self . arrays ) if err != ERR NONE : Raise Error ( err )", "predictions": ["re - clear the settings from the settings"], "references": ["bins the light curve model to the provided time array"], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 4946, "code": "def Free ( self ) : if self . arrays . calloc : dbl free ( self . arrays . time ) dbl free ( self . arrays . flux ) dbl free ( self . arrays . bflx ) dbl free ( self . arrays . M ) dbl free ( self . arrays . E ) dbl free ( self . arrays . f ) dbl free ( self . arrays . r ) dbl free ( self . arrays . x ) dbl free ( self . arrays . y ) dbl free ( self . arrays . z ) self . arrays . calloc = 0 if self . arrays . balloc : dbl free ( self . arrays . b ) self . arrays . balloc = 0 if self . arrays . ialloc : dbl free ( self . arrays . iarr ) self . arrays . ialloc = 0", "predictions": ["injects the previous version of the previous arrays ."], "references": ["frees the memory used by all of the dynamically allocated c arrays ."], "bleu": 0.15674392671857865, "rouge_l": 0.4401154401154401}
{"id": 4947, "code": "def list extensions gen ( self ) : code , message = self . command ( \"LIST EXTENSIONS\" ) if code != 202 : raise NNTP Reply Error ( code , message ) for line in self . info gen ( code , message ) : yield line . strip ( )", "predictions": ["generate the list of extensions extensions ."], "references": ["generator for the list extensions command ."], "bleu": 0.2777619034011791, "rouge_l": 0.5714285714285714}
{"id": 4948, "code": "def xpat gen ( self , header , msgid range , * pattern ) : args = \" \" . join ( [ header , utils . unparse msgid range ( msgid range ) ] + list ( pattern ) ) code , message = self . command ( \"XPAT\" , args ) if code != 221 : raise NNTP Reply Error ( code , message ) for line in self . info gen ( code , message ) : yield line . strip ( )", "predictions": ["generate xpat for a specified pattern ."], "references": ["generator for the xpat command ."], "bleu": 0.22089591134157885, "rouge_l": 0.31202046035805625}
{"id": 4949, "code": "def xfeature compress gzip ( self , terminator = False ) : args = \"TERMINATOR\" if terminator else None code , message = self . command ( \"XFEATURE COMPRESS GZIP\" , args ) if code != 290 : raise NNTP Reply Error ( code , message ) return True", "predictions": ["compress the gzip ."], "references": ["xfeature compress gzip command ."], "bleu": 0.33277145517762347, "rouge_l": 0.6535714285714286}
{"id": 4950, "code": "def api post ( self , url , * * kwargs ) : response = self . session . post ( url = url , headers = self . get api headers ( ) , * * kwargs ) if not response . ok : raise Server Exception ( '{0}: {1}' . format ( response . status code , response . text or response . reason ) ) return response . json ( )", "predictions": ["do a post request to the given url"], "references": ["convenience method for posting"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 4951, "code": "def api delete ( self , url , * * kwargs ) : response = self . session . delete ( url = url , headers = self . get api headers ( ) , * * kwargs ) if not response . ok : raise Server Exception ( '{0}: {1}' . format ( response . status code , response . text or response . reason ) ) return response", "predictions": ["do a delete request"], "references": ["convenience method for deleting"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 4952, "code": "def api get ( self , url , * * kwargs ) : response = self . session . get ( url = url , headers = self . get api headers ( ) , * * kwargs ) if not response . ok : raise Server Exception ( '{0}: {1}' . format ( response . status code , response . text or response . reason ) ) return response . json ( )", "predictions": ["do a get request"], "references": ["convenience method for getting"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 4953, "code": "def create scheduled query ( self , query , change , scope unit , scope count ) : query data = { 'scheduled query' : { 'name' : 'For Anomaly Report' , 'query' : query , 'threshold type' : '%' , 'threshold value' : change , 'time period' : scope unit . title ( ) , 'time value' : scope count , } } query url = 'https://logentries.com/rest/{account id}/api/scheduled queries' return self . api post ( url = query url . format ( account id = self . account id ) , data = json . dumps ( query data , sort keys = True ) )", "predictions": ["creates a scheduled query"], "references": ["create the scheduled query"], "bleu": 0.47287080450158786, "rouge_l": 0.5}
{"id": 4954, "code": "def do POST ( self ) : self . send response ( urllib2 . httplib . OK ) self . end headers ( ) content length = int ( self . headers [ 'Content-Length' ] ) body = self . rfile . read ( content length ) print ( \"Client: {0}\" . format ( str ( self . client address ) ) ) print ( \"headers: {0}\" . format ( self . headers ) ) print ( \"path: {0}\" . format ( self . path ) ) print ( \"body: {0}\" . format ( body ) )", "predictions": ["serve an html client ."], "references": ["handles the post request sent by boundary url action"], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 4955, "code": "def defaults docstring ( defaults , header = None , indent = None , footer = None ) : if indent is None : indent = '' if header is None : header = '' if footer is None : footer = '' width = 60 hbar = '\\n' s = hbar + ( header ) + hbar for key , value , desc in defaults : if isinstance ( value , basestring ) : value = \"'\" + value + \"'\" if hasattr ( value , ' call ' ) : value = \"<\" + value . name + \">\" s += indent + '%-12s\\n' % ( \"%s :\" % key ) s += indent + indent + ( indent + 23 * ' ' ) . join ( desc . split ( '\\n' ) ) s += ' [%s]\\n\\n' % str ( value ) s += hbar s += footer return s", "predictions": ["return a string representation of the defaults ."], "references": ["return a docstring from a list of defaults ."], "bleu": 0.2451240194075422, "rouge_l": 0.5820610687022901}
{"id": 4956, "code": "def defaults decorator ( defaults ) : def decorator ( func ) : kwargs = dict ( header = 'Keyword arguments\\n-----------------\\n' , indent = '  ' , footer = '\\n' ) doc = defaults docstring ( defaults , * * kwargs ) if func . doc is None : func . doc = '' func . doc += doc return func return decorator", "predictions": ["decorator to decorator to decorator for functions ."], "references": ["decorator to append default kwargs to a function ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 4957, "code": "def load ( self , * * kwargs ) : defaults = dict ( [ ( d [ 0 ] , d [ 1 ] ) for d in self . defaults ] ) for k in kwargs : if k not in defaults : msg = \"Unrecognized attribute of %s: %s\" % ( self . class . name , k ) raise Attribute Error ( msg ) defaults . update ( kwargs ) self . dict . update ( defaults ) self . check type ( self . dict [ 'default' ] ) self . set ( * * defaults )", "predictions": ["load the model from the dictionary ."], "references": ["load kwargs key value pairs into __dict__"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4958, "code": "def defaults docstring ( cls , header = None , indent = None , footer = None ) : return defaults docstring ( cls . defaults , header = header , indent = indent , footer = footer )", "predictions": ["returns the docstring of the defaults ."], "references": ["add the default values to the class docstring"], "bleu": 0.19148978368719022, "rouge_l": 0.2634989200863931}
{"id": 4959, "code": "def set errors ( self , errors ) : if errors is None : self . errors = None return self . errors = [ asscalar ( e ) for e in errors ]", "predictions": ["set errors errors ."], "references": ["set parameter error estimate"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 4960, "code": "def load and parse ( self ) : f = open ( self . file path , \"r\" ) metrics json = f . read ( ) self . metrics = json . loads ( metrics json )", "predictions": ["read metrics from json"], "references": ["load the metrics file from the given path"], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 4961, "code": "def extract dictionary ( self , metrics ) : new metrics = { } for m in metrics : metric = self . extract fields ( m ) new metrics [ m [ 'name' ] ] = metric return new metrics", "predictions": ["close the dictionary and returns a dictionary of ."], "references": ["extract required fields from an array"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4962, "code": "def filter ( self ) : if self . filter expression is not None : new metrics = [ ] metrics = self . metrics [ 'result' ] for m in metrics : if self . filter expression . search ( m [ 'name' ] ) : new metrics . append ( m ) else : new metrics = self . metrics [ 'result' ] self . metrics = self . extract dictionary ( new metrics )", "predictions": ["extract version of the put version"], "references": ["apply the criteria to filter out on the metrics required"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 4963, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . host Group Id is not None : self . host Group Id = self . args . host Group Id self . path = \"v1/hostgroup/{0}\" . format ( str ( self . host Group Id ) )", "predictions": ["put the 2 - www - 2 - 2 - 16 - 2 - 16 - based views - 2 - 16 - 2 session - 2 - 2 session resource"], "references": ["extracts the specific arguments of this cli"], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 4964, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . tenant id is not None : self . tenant id = self . args . tenant id if self . args . fingerprint fields is not None : self . fingerprint fields = self . args . fingerprint fields if self . args . title is not None : self . title = self . args . title if self . args . source is not None : self . source = self . args . source if self . args . severity is not None : self . severity = self . args . severity if self . args . message is not None : self . message = self . args . message event = { } if self . title is not None : event [ 'title' ] = self . title if self . severity is not None : event [ 'severity' ] = self . severity if self . message is not None : event [ 'message' ] = self . message if self . source is not None : if 'source' not in event : event [ 'source' ] = { } if len ( self . source ) >= 1 : event [ 'source' ] [ 'ref' ] = self . source [ 0 ] if len ( self . source ) >= 2 : event [ 'source' ] [ 'type' ] = self . source [ 1 ] self . process properties ( self . args . properties ) if self . properties is not None : event [ 'properties' ] = self . properties if self . fingerprint fields is not None : event [ 'fingerprint Fields' ] = self . fingerprint fields self . data = json . dumps ( event , sort keys = True ) self . headers = { 'Content-Type' : 'application/json' }", "predictions": ["put information about the tenant add information to the tenant add a tenant add the information to the tenant add a tenant add a tenant add a tenant add information from"], "references": ["extracts the specific arguments of this cli"], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 4965, "code": "def call api ( self ) : sockobj = socket ( AF INET , SOCK STREAM ) sockobj . connect ( ( self . rpc host , self . rpc port ) ) self . get json ( ) message = [ self . rpc message . encode ( 'utf-8' ) ] for line in message : sockobj . send ( line ) data = sockobj . recv ( self . MAX LINE ) print ( data ) self . rpc data . append ( data ) sockobj . close ( )", "predictions": ["x - fit the else - else - else - else - else - else - else - else - else - else"], "references": ["make a call to the meter via json rpc"], "bleu": 0.05291907393644996, "rouge_l": 0.06785317018909899}
{"id": 4966, "code": "def get arguments ( self ) : Hostgroup Modify . get arguments ( self ) if self . args . host group id is not None : self . host group id = self . args . host group id self . path = \"v1/hostgroup/\" + str ( self . host group id )", "predictions": ["y - group fit fit"], "references": ["extracts the specific arguments of this cli"], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 4967, "code": "def identifier ( self , text ) : self . attempting ( text ) return concatenation ( [ alternation ( [ self . alpha character , \" \" ] ) , zero or more ( alternation ( [ self . alpha character , \" \" , self . digit ] ) ) ] , ignore whitespace = False ) ( text ) . compressed ( Token Type . identifier )", "predictions": ["return the x - component x - 256 x - 256 x - 256 x - 256 x - 256 x - 256 x - 256 x - 256 x -"], "references": ["identifier = alpha_character | _ . { alpha_character | _ | digit } ;"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4968, "code": "def operator ( self , text ) : self . attempting ( text ) return alternation ( [ \"|\" , \".\" , \",\" , \"-\" ] ) ( text ) . retyped ( Token Type . operator )", "predictions": ["return up the y - type y - type y - type y - type y - 256 y - type y - 256 y - type of the given text"], "references": ["operator = | | . | | - ;"], "bleu": 0.03901663112717908, "rouge_l": 0.055505004549590536}
{"id": 4969, "code": "def op mult ( self , text ) : self . attempting ( text ) return terminal ( \"*\" ) ( text ) . retyped ( Token Type . op mult )", "predictions": ["x - axis mult"], "references": ["op_mult = * ;"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 4970, "code": "def op add ( self , text ) : self . attempting ( text ) return terminal ( \"+\" ) ( text ) . retyped ( Token Type . op add )", "predictions": ["plus a string to the current is complete ."], "references": ["op_add = + ;"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 4971, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . plugin Name is not None : self . plugin Name = self . args . plugin Name self . path = \"v1/plugins/{0}/components\" . format ( self . plugin Name )", "predictions": ["y - load the is initialized"], "references": ["extracts the specific arguments of this cli"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4972, "code": "def get environment ( self ) : if 'TSP EMAIL' in os . environ : self . email = os . environ [ 'TSP EMAIL' ] if 'TSP API TOKEN' in os . environ : self . api token = os . environ [ 'TSP API TOKEN' ] if 'TSP API HOST' in os . environ : self . api host = os . environ [ 'TSP API HOST' ] else : self . api host = 'api.truesight.bmc.com'", "predictions": ["draw the environment environment environment advance"], "references": ["gets the configuration stored in environment variables"], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 4973, "code": "def call api ( self ) : self . url = self . form url ( ) if self . headers is not None : logging . debug ( self . headers ) if self . data is not None : logging . debug ( self . data ) if len ( self . get url parameters ( ) ) > 0 : logging . debug ( self . get url parameters ( ) ) result = self . methods [ self . method ] ( ) if not self . good response ( result . status code ) : logging . error ( self . url ) logging . error ( self . method ) if self . data is not None : logging . error ( self . data ) logging . error ( result ) self . api result = result", "predictions": ["call the api and setup the api for the api extension extension extension extension extension extension extension extension extension extension extension extension extension extension extension extension extension extension extension extension extension"], "references": ["make an api call to get the metric definition"], "bleu": 0.046398855339878003, "rouge_l": 0.11101000909918107}
{"id": 4974, "code": "def get arguments ( self ) : if self . args . file name is not None : self . file name = self . args . file name", "predictions": ["duration for the not arguments ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 4975, "code": "def execute ( self ) : self . add arguments ( ) self . parse args ( ) self . get arguments ( ) if self . validate arguments ( ) : self . plot data ( ) else : print ( self . message )", "predictions": ["update the valid valid valid arguments"], "references": ["run the steps to execute the cli"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4976, "code": "def get remote file size ( self , url ) : try : req = urllib . request . urlopen ( url ) return int ( req . getheader ( 'Content-Length' ) . strip ( ) ) except urllib . error . HTTP Error as error : logger . error ( 'Error retrieving size of the remote file %s' % error ) print ( 'Error retrieving size of the remote file %s' % error ) self . connect earthexplorer ( ) self . get remote file size ( url )", "predictions": ["get the remote size size"], "references": ["gets the filesize of a remote file"], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 4977, "code": "def download ( self , bands = None , download dir = None , metadata = False ) : if not download dir : download dir = DOWNLOAD DIR if bands is None : bands = list ( range ( 1 , 12 ) ) + [ 'BQA' ] else : self . validate bands ( bands ) pattern = re . compile ( '^[^\\s]+ (.+)\\.tiff?' , re . I ) band list = [ 'B%i' % ( i , ) if isinstance ( i , int ) else i for i in bands ] image list = [ ] self . connect earthexplorer ( ) tgzname = self . scene Info . name + '.tgz' dest dir = check create folder ( join ( download dir , self . scene Info . name ) ) downloaded = self . download file ( self . url , dest dir , tgzname ) logger . debug ( 'Status downloaded %s' % downloaded ) print ( '\\n Status downloaded %s' % downloaded ) if downloaded [ 'sucess' ] : print ( '\\n Downloaded sucess' ) logger . debug ( 'Downloaded sucess of scene: %s' % self . scene Info . name ) try : tar = tarfile . open ( downloaded [ 'file path' ] , 'r' ) folder path = join ( download dir , self . scene Info . name ) tar . extractall ( folder path ) remove ( downloaded [ 'file path' ] ) images path = listdir ( folder path ) for image path in images path : matched = pattern . match ( image path ) file path = join ( folder path , image path ) if matched and matched . group ( 1 ) in band list : image list . append ( [ file path , getsize ( file path ) ] ) elif matched : remove ( file path ) except tarfile . Read Error as error : print ( '\\n Error when extracting files. %s' % error ) logger . error ( 'Error when extracting files. %s' % error ) return image list else : logger . debug ( 'Info downloaded: %s' % downloaded ) print ( '\\n Info downloaded: %s' % downloaded ) return downloaded", "predictions": ["download . to bands"], "references": ["download remote . tar . bz file ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 4978, "code": "def validate bands ( bands ) : if not isinstance ( bands , list ) : raise Type Error ( 'Parameter bands must be a \"list\"' ) valid bands = list ( range ( 1 , 12 ) ) + [ 'BQA' ] for band in bands : if band not in valid bands : raise Invalid Band Error ( '%s is not a valid band' % band )", "predictions": ["validates that bands is a valid band ."], "references": ["validate bands parameter ."], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 4979, "code": "def connect earthexplorer ( self ) : logger . info ( \"Establishing connection to Earthexplorer\" ) print ( \"\\n Establishing connection to Earthexplorer\" ) try : opener = urllib . request . build opener ( urllib . request . HTTP Cookie Processor ( ) ) urllib . request . install opener ( opener ) params = urllib . parse . urlencode ( dict ( username = self . user , password = self . password ) ) params = params . encode ( 'utf-8' ) f = opener . open ( \"https://ers.cr.usgs.gov/login\" , params ) data = f . read ( ) . decode ( 'utf-8' ) f . close ( ) if data . find ( 'You must sign in as a registered user to download data or place orders for USGS EROS products' ) > 0 : print ( \"\\n Authentification failed\" ) logger . error ( \"Authentification failed\" ) raise Autentication USGS Failed ( 'Authentification USGS failed' ) print ( 'User %s connected with USGS' % self . user ) logger . debug ( 'User %s connected with USGS' % self . user ) return except Exception as e : print ( '\\n Error when trying to connect USGS: %s' % e ) raise logger . error ( 'Error when trying to connect USGS: %s' % e )", "predictions": ["list the = available = 1 code code code code code code code code code code code code"], "references": ["connection to earth explorer without proxy"], "bleu": 0.057259987315337726, "rouge_l": 0.0}
{"id": 4980, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . metric name is not None : self . metric name = self . args . metric name self . path = \"v1/metrics/{0}\" . format ( self . metric name )", "predictions": ["gets the metric gen gen gen gen"], "references": ["extracts the specific arguments of this cli"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 4981, "code": "def normalize ( self , dt , is dst = False ) : if dt . tzinfo is None : raise Value Error ( 'Naive time - no tzinfo set' ) return dt . replace ( tzinfo = self )", "predictions": ["normalize - aware if the if it is not a if not already not if not already - 1 ."], "references": ["correct the timezone information on the given datetime"], "bleu": 0.06108557268562171, "rouge_l": 0.07741116751269035}
{"id": 4982, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . host Group Id is not None : self . host Group Id = self . args . host Group Id if self . args . force is not None : self . force = self . args . force if self . force : self . url parameters = { \"force Remove\" : True } self . path = \"v1/hostgroup/{0}\" . format ( str ( self . host Group Id ) )", "predictions": ["gets the post post post post post post"], "references": ["extracts the specific arguments of this cli"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4983, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) self . actions = self . args . actions if self . args . actions is not None else None self . alarm name = self . args . alarm name if self . args . alarm name is not None else None self . metric = self . args . metric if self . args . metric is not None else None self . aggregate = self . args . aggregate if self . args . aggregate is not None else None self . operation = self . args . operation if self . args . operation is not None else None self . threshold = self . args . threshold if self . args . threshold is not None else None self . trigger interval = self . args . trigger interval if self . args . trigger interval is not None else None self . host group id = self . args . host group id if self . args . host group id is not None else None self . note = self . args . note if self . args . note is not None else None self . per host notify = self . args . per host notify if self . args . per host notify is not None else None self . is disabled = self . args . is disabled if self . args . is disabled is not None else None self . notify clear = self . args . notify clear if self . args . notify clear is not None else None self . notify set = self . args . notify set if self . args . notify set is not None else None self . timeout interval = self . args . timeout interval if self . args . timeout interval is not None else None", "predictions": ["api delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete delete"], "references": ["extracts the specific arguments of this cli"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4984, "code": "def dump text ( self ) : results = self . relay output [ 'result' ] for l in results : dt = time . strftime ( \"%Y-%m-%d T%H:%M:%SZ\" , time . gmtime ( int ( l [ 1 ] [ 'ts' ] ) ) ) print ( \"{0} {1} {2} {3}\" . format ( l [ 0 ] , dt , l [ 1 ] [ 'type' ] , l [ 1 ] [ 'msg' ] ) )", "predictions": ["api get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get"], "references": ["send output in textual format"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 4985, "code": "def handle results ( self ) : if self . api result . status code == requests . codes . ok : self . relay output = json . loads ( self . api result . text ) if self . raw : self . dump json ( ) else : self . dump text ( )", "predictions": ["process incoming scheduled scheduled text scope scope scope scope scope ."], "references": ["call back function to be implemented by the cli ."], "bleu": 0.11390778025531027, "rouge_l": 0.09606299212598425}
{"id": 4986, "code": "def get arguments ( self ) : Plugin Base . get arguments ( self ) if self . args . organization Name is not None : self . organization Name = self . args . organization Name if self . args . repository Name is not None : self . repository Name = self . args . repository Name self . path = \"v1/plugins/private/{0}/{1}/{2}\" . format ( self . plugin Name , self . organization Name , self . repository Name )", "predictions": ["return the arguments name for the organization"], "references": ["extracts the specific arguments of this cli"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 4987, "code": "def get arguments ( self ) : Alarm Modify . get arguments ( self ) self . alarm id = self . args . alarm id if self . args . alarm id is not None else None self . get api parameters ( )", "predictions": ["defaults for the alarm s docstring"], "references": ["extracts the specific arguments of this cli"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 4988, "code": "def filter ( self ) : if self . metrics or self . control or self . plugins : relays = self . relays [ 'result' ] [ 'relays' ] for relay in relays : if self . metrics : del relays [ relay ] [ 'metrics' ] if self . control : del relays [ relay ] [ 'control' ] if self . plugins : if 'plugins' in relays [ relay ] : del relays [ relay ] [ 'plugins' ]", "predictions": ["remove all = = false"], "references": ["apply the criteria to filter out on the output required"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 4989, "code": "def handle results ( self ) : if self . api result . status code == requests . codes . ok : self . relays = json . loads ( self . api result . text ) self . filter ( ) self . dump json ( )", "predictions": ["process incoming results from the queue"], "references": ["call back function to be implemented by the cli ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 4990, "code": "def fromlist ( cls , files , equal = False , offensive = False , lang = None ) : self = cls . new ( cls ) self . files = fortunes = [ ] count = 0 for file in files : fortune = load fortune ( file , offensive = offensive , lang = lang ) if fortune is None : logger . warn ( \"Can't load: %s\" , file ) continue count += 1 if equal else fortune . size fortunes . append ( ( fortune , count ) ) if not fortunes : raise Value Error ( 'All fortune files specified are invalid' ) self . count = count self . keys = [ i [ 1 ] for i in self . files ] return self", "predictions": ["return list of files files"], "references": ["initialize based on a list of fortune files"], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 4991, "code": "def set chance ( cls , files , equal = False , offensive = False , lang = None ) : self = cls . new ( cls ) total = 0. file = [ ] leftover = [ ] for name , chance in files : if total >= 1 : break fortune = load fortune ( name , offensive = offensive , lang = lang ) if fortune is None or not fortune . size : continue if chance : file . append ( ( fortune , chance ) ) total += chance else : leftover . append ( fortune ) if leftover and total < 1 : left = 1 - total if equal : perfile = left / len ( leftover ) for fortune in leftover : file . append ( ( fortune , perfile ) ) else : entries = sum ( map ( attrgetter ( 'size' ) , leftover ) ) logger . debug ( '%d entries left' , entries ) for fortune in leftover : chance = left * fortune . size / entries file . append ( ( fortune , chance ) ) self . count = count = 65536 bound = 0 self . files = fortunes = [ ] for file , chance in file : bound += int ( chance * count ) fortunes . append ( ( file , bound ) ) self . keys = [ i [ 1 ] for i in self . files ] return self", "predictions": ["set errors with equal"], "references": ["initialize based on a list of fortune files with set chances"], "bleu": 0.06909866532427987, "rouge_l": 0.12298387096774194}
{"id": 4992, "code": "def grammar ( self , text ) : self . attempting ( text ) return concatenation ( [ zero or more ( self . comment , ignore whitespace = True ) , self . rule , zero or more ( alternation ( [ self . comment , self . rule , ] ) , ignore whitespace = True ) , ] , ignore whitespace = True ) ( text ) . retyped ( Token Type . grammar )", "predictions": ["return the grammar for the given text ."], "references": ["grammar = { comment } rule { comment | rule } ;"], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 4993, "code": "def rule ( self , text ) : self . attempting ( text ) return concatenation ( [ self . identifier , \"=\" , self . expression , \";\" , ] , ignore whitespace = True ) ( text ) . retyped ( Token Type . rule )", "predictions": ["return the rule of the given text ."], "references": ["rule = identifier = expression ; ;"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 4994, "code": "def special handling ( self , text ) : self . attempting ( text ) return concatenation ( [ \"?\" , self . identifier , \"?\" , ] , ignore whitespace = True ) ( text ) . retyped ( Token Type . special handling )", "predictions": ["return the string with the text ."], "references": ["special_handling = ? identifier ? ;"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 4995, "code": "def number ( self , text ) : self . attempting ( text ) return concatenation ( [ exclusion ( self . digit , \"0\" ) , zero or more ( self . digit , ignore whitespace = False ) , ] , ignore whitespace = False ) ( text ) . compressed ( Token Type . number )", "predictions": ["return the number of text ."], "references": ["number = digit - 0 . { digit } ;"], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 4996, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . metric Name is not None : self . metric Name = self . args . metric Name if self . args . measurement is not None : self . measurement = self . args . measurement if self . args . source is not None : self . source = self . args . source else : self . source = socket . gethostname ( ) if self . args . timestamp is not None : self . timestamp = int ( self . args . timestamp ) m = { 'metric' : self . metric Name , 'measure' : self . measurement } if self . source is not None : m [ 'source' ] = self . source if self . timestamp is not None : m [ 'timestamp' ] = int ( self . timestamp ) self . process properties ( ) if self . properties is not None : m [ 'metadata' ] = self . properties self . data = json . dumps ( m , sort keys = True ) self . headers = { 'Content-Type' : 'application/json' , \"Accept\" : \"application/json\" }", "predictions": ["get the arguments for the measurement ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 4997, "code": "def handle results ( self ) : if self . api result . status code == requests . codes . ok : payload = json . loads ( self . api result . text ) out = json . dumps ( payload , sort keys = True , indent = 4 , separators = ( ',' , ': ' ) ) print ( self . colorize json ( out ) )", "predictions": ["handle the results from the api ."], "references": ["call back function to be implemented by the cli ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 4998, "code": "def grammar ( self ) : if self . grammar is None : self . parser = Parser ( ) grammar = self . parser . parse ( self . input source ) self . grammar = grammar . trimmed ( ) . flattened ( ) . flattened ( self . flatten ) return self . grammar", "predictions": ["the grammar of the grammar ."], "references": ["the parse tree generated by the source ."], "bleu": 0.18822631894109965, "rouge_l": 0.4178082191780822}
{"id": 4999, "code": "def rules ( self ) : if self . rules is None : self . rules = [ ] for child in self . grammar . children : if child . is type ( Token Type . rule ) : name , expression = child . children self . rules . append ( Rule ( name . value , self . expression to asn ( expression ) , name . position , child . consumed ) ) return self . rules", "predictions": ["returns the rules of this widget ."], "references": ["the ast rules ."], "bleu": 0.22089591134157885, "rouge_l": 0.5736677115987461}
{"id": 5000, "code": "def comments ( self ) : if self . comments is None : self . comments = [ c for c in self . grammar . children if c . is type ( Token Type . comment ) ] return self . comments", "predictions": ["the comments of this block ."], "references": ["the ast comments ."], "bleu": 0.2626909894424158, "rouge_l": 0.6224489795918368}
{"id": 5001, "code": "def directives ( self ) : if self . directives is None : self . directives = [ ] for comment in self . comments : self . directives . extend ( self . directives from comment ( comment ) ) return self . directives", "predictions": ["the list of directives ."], "references": ["the diretives parsed from the comments ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 5002, "code": "def output source ( self ) : if self . output source is None : self . output source = self . compile ( ) return self . output source", "predictions": ["the output of the output source ."], "references": ["the python source of the parser generated from the input source ."], "bleu": 0.15749996500436228, "rouge_l": 0.5024711696869852}
{"id": 5003, "code": "def compile ( self ) : fmt = fmt = self . clean fmt ( fmt ) return fmt . format ( date = datetime . utcnow ( ) . isoformat ( ) , imports = self . get imports ( ) , token type enum = self . get token type enum ( ) , class definition = self . get class definition ( ) )", "predictions": ["return a string representation of the token ."], "references": ["returns the python source code for the generated parser ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 5004, "code": "def get imports ( self ) : import directives = [ d for d in self . directives if d . name == \"import\" ] if import directives : return \"\\n\" + \"\\n\" . join ( d . args [ \"value\" ] for d in import directives ) else : return \"\"", "predictions": ["return a list of directives"], "references": ["reads the directives and generates source code for custom imports ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 5005, "code": "def get token type enum ( self ) : fmt = \"class Token Type(Enum):\\n\" \"{indent}\\\"\\\"\\\"The token types for parse nodes generated by the Parser.\\\"\\\"\\\"\\n\" \"{indent}\" + \"\\n{indent}\" . join ( \"{1} = {0}\" . format ( num + 1 , r . name ) for num , r in enumerate ( self . rules ) ) return fmt . format ( indent = self . indent )", "predictions": ["return the token type of the token ."], "references": ["builds the python source code for the parser tokentype enum ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 5006, "code": "def get class definition ( self ) : fmt = fmt = self . clean fmt ( fmt ) return fmt . format ( parser base = self . get parser base ( ) , indent = self . indent , entry point = self . get entry point ( ) , rule definitions = \"\\n\" . join ( self . get rule definitions ( ) ) )", "predictions": ["return the class definition for this entry ."], "references": ["builds the class definition of the parser ."], "bleu": 0.3155984539112945, "rouge_l": 0.5}
{"id": 5007, "code": "def get entry point ( self ) : ep = self . find directive ( \"entry point\" ) if ep : return ep . args [ \"value\" ] else : return self . rules [ 0 ] . name", "predictions": ["get the entry point for the current entry ."], "references": ["gets the entry_point value for the parser ."], "bleu": 0.21105340631872635, "rouge_l": 0.4756335282651072}
{"id": 5008, "code": "def get rule definition ( self , rule ) : fmt = fmt = self . clean fmt ( fmt ) source = self . indent ( self . ast to code ( rule . expression ) , skip first line = True ) if self . use terminal shorthand and len ( source ) == 1 and source [ 0 ] . startswith ( ( \"'\" , '\"' ) ) : source = [ \"terminal({})\" . format ( source [ 0 ] ) ] rule source = fmt . format ( rule fxn name = self . get rule fxn name ( rule . name ) , indent = self . indent , rule source = self . get rule source ( rule ) , rule definition = \"\\n\" . join ( source ) , transform = self . get rule transform ( rule ) ) return self . indent ( rule source , 1 )", "predictions": ["get the rule definition for a rule ."], "references": ["generates the source code for a rule ."], "bleu": 0.46713797772820004, "rouge_l": 0.625}
{"id": 5009, "code": "def get rule source ( self , rule ) : p = len ( self . input source ) + rule . position source = self . input source [ p : p + rule . consumed ] . rstrip ( ) return self . indent ( source , depth = self . indent + \"   \" , skip first line = True )", "predictions": ["return the source source for the given rule ."], "references": ["gets the variable part of the source code for a rule ."], "bleu": 0.18204651199034363, "rouge_l": 0.4642313546423136}
{"id": 5010, "code": "def expression to asn ( self , expression ) : new children = [ self . node to asn ( c ) for c in expression . children ] return self . remove grouping groups ( infix to optree ( new children ) )", "predictions": ["converts an expression to an asn expression ."], "references": ["convert an expression to an abstract syntax tree node ."], "bleu": 0.3638074228571147, "rouge_l": 0.5446428571428571}
{"id": 5011, "code": "def node to asn ( self , node ) : if node . is type ( Token Type . identifier ) : return Identifier ( node . svalue ) elif node . is type ( Token Type . terminal ) : return Terminal ( node . svalue ) elif node . is type ( Token Type . option group ) : expr = node . children [ 0 ] return Option Group ( self . expression to asn ( expr ) ) elif node . is type ( Token Type . repetition group ) : expr = node . children [ 0 ] return Repetition Group ( self . expression to asn ( expr ) ) elif node . is type ( Token Type . grouping group ) : expr = node . children [ 0 ] return Grouping Group ( self . expression to asn ( expr ) ) elif node . is type ( Token Type . special handling ) : ident = node . children [ 0 ] return Special Handling ( ident ) elif node . is type ( Token Type . number ) : return Number ( node . svalue ) elif node . is type ( ( Token Type . operator , Token Type . op mult , Token Type . op add ) ) : return Operator Node ( OPERATOR INDEX [ node . svalue ] , node . position ) else : raise Exception ( \"Unhandled parse tree node: {0}\" . format ( node ) )", "predictions": ["convert node to equivalent equivalent ."], "references": ["convert a parse tree node into an absract syntax tree node ."], "bleu": 0.09663861439684919, "rouge_l": 0.31443298969072164}
{"id": 5012, "code": "def ast to code ( self , node , * * kwargs ) : if isinstance ( node , Optree Node ) : return self . ast optree node to code ( node , * * kwargs ) elif isinstance ( node , Identifier ) : return self . ast identifier to code ( node , * * kwargs ) elif isinstance ( node , Terminal ) : return self . ast terminal to code ( node , * * kwargs ) elif isinstance ( node , Option Group ) : return self . ast option group to code ( node , * * kwargs ) elif isinstance ( node , Repetition Group ) : return self . ast repetition group to code ( node , * * kwargs ) elif isinstance ( node , Special Handling ) : return self . ast special handling to code ( node , * * kwargs ) elif isinstance ( node , Number ) : return self . ast number to code ( node , * * kwargs ) else : raise Exception ( \"Unhandled ast node: {0}\" . format ( node ) )", "predictions": ["convert an ast node to an ast ."], "references": ["convert an abstract syntax tree to python source code ."], "bleu": 0.1867587389639562, "rouge_l": 0.43571428571428567}
{"id": 5013, "code": "def ast optree node to code ( self , node , * * kwargs ) : opnode = node . opnode if opnode is None : return self . ast to code ( node . operands [ 0 ] ) else : operator = opnode . operator if operator is OP ALTERNATE : return self . ast op alternate to code ( node , * * kwargs ) elif operator is OP WS CONCAT : kwargs [ \"ignore whitespace\" ] = False return self . ast op concat to code ( node , * * kwargs ) elif operator is OP CONCAT : kwargs [ \"ignore whitespace\" ] = True return self . ast op concat to code ( node , * * kwargs ) elif operator is OP EXCLUDE : return self . ast op exclude to code ( node , * * kwargs ) elif operator is OP MULTIPLY : return self . ast op multiply to code ( node , * * kwargs ) elif operator is OP REPEAT : return self . ast op repeat to code ( node , * * kwargs ) else : raise Exception ( \"Unhandled optree node: {0}\" . format ( node ) )", "predictions": ["convert an ast node to an ast ."], "references": ["convert an abstract syntax operator tree to python source code ."], "bleu": 0.16481400866629634, "rouge_l": 0.4093959731543625}
{"id": 5014, "code": "def ast terminal to code ( self , terminal , * * kwargs ) : value = replace ( terminal . value ) if self . use terminal shorthand : return [ value ] else : return [ \"terminal({})\" . format ( value ) ]", "predictions": ["convert ast to ast code ."], "references": ["convert an ast terminal to python source code ."], "bleu": 0.20969025558524573, "rouge_l": 0.6434599156118143}
{"id": 5015, "code": "def ast option group to code ( self , option group , * * kwargs ) : lines = [ \"option(\" ] lines . extend ( self . indent ( self . ast to code ( option group . expression ) ) ) lines . append ( \")\" ) return lines", "predictions": ["return an ast for the ast option ."], "references": ["convert an ast option group to python source code ."], "bleu": 0.20668251975744228, "rouge_l": 0.43571428571428567}
{"id": 5016, "code": "def ast repetition group to code ( self , repetition group , ignore whitespace = False , * * kwargs ) : lines = [ \"zero or more(\" ] lines . extend ( self . indent ( self . ast to code ( repetition group . expression ) ) ) lines [ - 1 ] += \",\" lines . append ( self . indent ( \"ignore whitespace={}\" . format ( bool ( ignore whitespace ) ) ) ) lines . append ( \")\" ) return lines", "predictions": ["return an ast for the ast group ."], "references": ["convert an ast repetition group to python source code ."], "bleu": 0.1867587389639562, "rouge_l": 0.43571428571428567}
{"id": 5017, "code": "def ast special handling to code ( self , special handling , * * kwargs ) : ident = special handling . value . svalue if ident in PB SPECIAL HANDLING : return [ \"PB.{0}\" . format ( ident ) ] else : return [ \"self.{0}\" . format ( ident ) ]", "predictions": ["return the ast special code for an ast ."], "references": ["convert an ast sepcial handling to python source code ."], "bleu": 0.18885888592159467, "rouge_l": 0.31282051282051276}
{"id": 5018, "code": "def ast op alternate to code ( self , opr , * * kwargs ) : hoist target = OP ALTERNATE operands = self . hoist operands ( opr . operands , lambda t : isinstance ( t , Optree Node ) and t . opnode . operator is hoist target ) lines = [ \"alternation([\" ] for op in operands : lines . extend ( self . indent ( self . ast to code ( op ) ) ) lines [ - 1 ] += \",\" lines . append ( \"])\" ) return lines", "predictions": ["return all ast lines that are ast ."], "references": ["convert an ast alternate op to python source code ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 5019, "code": "def ast op concat to code ( self , opr , * , ignore whitespace , * * kwargs ) : hoist target = OP CONCAT if ignore whitespace else OP WS CONCAT operands = self . hoist operands ( opr . operands , lambda t : isinstance ( t , Optree Node ) and t . opnode . operator is hoist target ) lines = [ \"concatenation([\" ] for op in operands : lines . extend ( self . indent ( self . ast to code ( op , ignore whitespace = ignore whitespace ) ) ) lines [ - 1 ] += \",\" lines . append ( \"], ignore whitespace={})\" . format ( bool ( ignore whitespace ) ) ) return lines", "predictions": ["return the ast lines for an ast ."], "references": ["convert an ast concatenate op to python source code ."], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 5020, "code": "def ast op exclude to code ( self , opr , * * kwargs ) : opl , opr = opr . operands lines = [ \"exclusion(\" ] lines . extend ( self . indent ( self . ast to code ( opl ) ) ) lines [ - 1 ] += \",\" lines . extend ( self . indent ( self . ast to code ( opr ) ) ) lines . append ( \")\" ) return lines", "predictions": ["exclude ast to the ast code ."], "references": ["convert an ast exclude op to python source code ."], "bleu": 0.18938334565508194, "rouge_l": 0.45607476635514016}
{"id": 5021, "code": "def ast op multiply to code ( self , opr , ignore whitespace = False , * * kwargs ) : opl , opr = opr . operands if isinstance ( opl , Number ) : times = opl . value subject = self . ast to code ( opr ) else : times = opr . value subject = self . ast to code ( opl ) lines = [ \"repeated(\" ] lines . extend ( self . indent ( subject ) ) lines [ - 1 ] += \",\" lines . append ( \"{0}times={1},\" . format ( self . indent , times ) ) lines . append ( \"{0}ignore whitespace={1}\" . format ( self . indent , bool ( ignore whitespace ) ) ) lines . append ( \")\" ) return lines", "predictions": ["return the ast lines for the ast ."], "references": ["convert an ast multiply op to python source code ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 5022, "code": "def ast op repeat to code ( self , opr , ignore whitespace = False , * * kwargs ) : lines = [ \"one or more(\" ] lines . extend ( self . indent ( self . ast to code ( opr . operands [ 0 ] ) ) ) lines [ - 1 ] += \",\" lines . append ( self . indent ( \"ignore whitespace={}\" . format ( bool ( ignore whitespace ) ) ) ) lines . append ( \")\" ) return lines", "predictions": ["return the ast code for an ast ."], "references": ["convert an ast repeat op to python source code ."], "bleu": 0.1867587389639562, "rouge_l": 0.3267857142857143}
{"id": 5023, "code": "def find directives ( self , pred ) : if isinstance ( pred , str ) : return [ d for d in self . directives if d . name == pred ] else : return [ d for d in self . directives if pred ( d ) ]", "predictions": ["find directives in this region ."], "references": ["finds all directives with a certain name or that passes a predicate ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 5024, "code": "def flatten ( child , parent ) : return parent . is type ( Token Type . expression ) and child . node type == parent . node type", "predictions": ["grammar a self attempting to a text or text or text attempting attempting attempting ."], "references": ["custom flattening method for the parse tree ."], "bleu": 0.08225964699966554, "rouge_l": 0.09200603318250376}
{"id": 5025, "code": "def directives from comment ( cls , comment ) : comment contents = comment . value [ 2 : - 2 ] . strip ( ) comment lines = ( l . strip ( ) for l in comment contents . split ( \"\\n\" ) ) directives = ( l [ 1 : ] . strip ( ) for l in comment lines if l . startswith ( \"!\" ) ) for directive def in directives : yield cls . parse directive def ( directive def )", "predictions": ["parse the rule data from a self concatenation concatenation concatenation concatenation concatenation concatenation concatenation concatenation concatenation concatenation ."], "references": ["a directive is a line in a comment that begins with ! ."], "bleu": 0.07535838128770536, "rouge_l": 0.13289760348583876}
{"id": 5026, "code": "def parse directive def ( cls , directive def ) : name , * kwargs = esc split ( directive def , ignore empty = True ) return Directive ( name , { key : value for key , value in ( esc split ( arg , \"=\" ) for arg in kwargs ) } )", "predictions": ["special helper to special handling the handling of the handling ."], "references": ["turns a directive definition string into a directive object ."], "bleu": 0.11390778025531027, "rouge_l": 0.09606299212598425}
{"id": 5027, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . host Group Name is not None : self . url parameters = { \"name\" : self . args . host Group Name }", "predictions": ["gets the arguments arguments arguments"], "references": ["extracts the specific arguments of this cli"], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 5028, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . plugin name is not None : self . plugin name = self . args . plugin name self . path = \"v1/plugins/{0}\" . format ( self . plugin name )", "predictions": ["get the arguments for this metric ."], "references": ["extracts the specific arguments of this cli"], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 5029, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) self . alarm id = self . args . alarm id if self . args . alarm id is not None else None", "predictions": ["handle the result of the result api"], "references": ["extracts the specific arguments of this cli"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 5030, "code": "def handle results ( self ) : if self . api result . status code != requests . codes . ok : print ( self . colorize json ( self . api result . text ) )", "predictions": ["grammar for the results"], "references": ["handle the results of the api call"], "bleu": 0.22336835181428535, "rouge_l": 0.346590909090909}
{"id": 5031, "code": "def get id ( id ) : if id == None : id = wx . New Id ( ) logger . debug ( 'Generated new ID %s.' , id ) else : logger . debug ( 'Using provided id %s.' , id ) return id", "predictions": ["rules for a children id . ."], "references": ["get a new id if the provided one is none ."], "bleu": 0.1247439242120089, "rouge_l": 0.32049036777583184}
{"id": 5032, "code": "def add arguments ( self ) : self . add logging argument ( ) self . parser . add argument ( '-a' , '--api-host' , dest = 'api host' , action = 'store' , metavar = \"api host\" , help = '{0} API host endpoint' . format ( self . product name ) ) self . parser . add argument ( '-e' , '--email' , dest = 'email' , action = 'store' , metavar = \"e mail\" , help = 'e-mail that has access to the {0} account' . format ( self . product name ) ) self . parser . add argument ( '-t' , '--api-token' , dest = 'api token' , required = False , action = 'store' , metavar = \"api token\" , help = 'API token for given e-mail that has access to the {0} account' . format ( self . product name ) ) self . parser . add argument ( '-z' , '--curl' , dest = 'curl' , required = False , action = 'store true' , default = False , help = 'Output the corresponding curl command line and exit' )", "predictions": ["comments for the command line arguments"], "references": ["configure handling of command line arguments ."], "bleu": 0.34801709319446883, "rouge_l": 0.45522388059701485}
{"id": 5033, "code": "def configure logging ( self ) : if self . args . log Level is not None : logging . basic Config ( level = self . levels [ self . args . log Level ] ) logging . info ( \"Set logging level to {0}\" . format ( self . args . log Level ) )", "predictions": ["configures the logging logging return none if not found is not specified is not set is not set"], "references": ["configure logging based on command line options"], "bleu": 0.06809398432036522, "rouge_l": 0.08689458689458689}
{"id": 5034, "code": "def execute ( self ) : self . get environment ( ) self . add arguments ( ) self . parse args ( ) self . get arguments ( ) self . get api parameters ( ) if self . validate arguments ( ) : if self . curl : self . curl output ( ) else : self . call api ( ) self . handle results ( ) else : print ( self . message )", "predictions": ["output the execution of the command"], "references": ["run the steps to execute the cli"], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 5035, "code": "def postfix to optree ( nodes ) : while len ( nodes ) > 1 : nodes = reduce ( nodes ) if len ( nodes ) == 0 : raise Operator Error ( \"Empty node list\" ) node = nodes [ 0 ] if isinstance ( node , Operator Node ) : raise Operator Error ( \"Operator without operands\" ) if isinstance ( node , Optree Node ) : return node return Optree Node ( None , ( node , ) )", "predictions": ["convert compile nodes to self . self ."], "references": ["convert a list of nodes in postfix order to an optree ."], "bleu": 0.1223065774797558, "rouge_l": 0.3860759493670886}
{"id": 5036, "code": "def pprint ( root , depth = 0 , space unit = \"    \" ) : spacing = space unit * depth if isinstance ( root , Optree Node ) : print ( \"{0}Operator ({1})\" . format ( spacing , root . opnode . operator . symbol if root . opnode else \"None -> IDENTITY\" ) ) for operand in root . operands : pprint ( operand , depth + 1 ) else : print ( \"{0}\u2022 {1}\".f o rmat(s p acing,  r ot))", "predictions": ["pretty - print the else else print"], "references": ["pretty print an optree starting at root ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 5037, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . plugin Name is not None : self . plugin Name = self . args . plugin Name", "predictions": ["get the token from the plugin"], "references": ["extracts the specific arguments of this cli"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5038, "code": "def add arguments ( self ) : Metric Common . add arguments ( self ) self . parser . add argument ( '-n' , '--metric-name' , dest = 'metric Name' , action = 'store' , required = True , metavar = 'metric name' , help = 'Metric identifier' ) self . parser . add argument ( '-d' , '--display-name' , dest = 'display Name' , action = 'store' , required = True , metavar = 'display name' , help = 'Metric display name' ) self . parser . add argument ( '-s' , '--display-name-short' , dest = 'display Name Short' , action = 'store' , required = True , metavar = 'display short name' , help = 'Metric short display name' ) self . parser . add argument ( '-i' , '--description' , dest = 'description' , action = 'store' , required = not self . update , metavar = 'description' , help = 'Metric description' ) self . parser . add argument ( '-g' , '--aggregate' , dest = 'aggregate' , action = 'store' , required = True , choices = [ 'avg' , 'max' , 'min' , 'sum' ] , help = 'Metric default aggregate' ) self . parser . add argument ( '-u' , '--unit' , dest = 'unit' , action = 'store' , required = False , choices = [ 'percent' , 'number' , 'bytecount' , 'duration' ] , help = 'Metric unit' ) self . parser . add argument ( '-r' , '--resolution' , dest = 'resolution' , action = 'store' , metavar = 'resolution' , required = False , help = 'Metric default resolution' ) self . parser . add argument ( '-y' , '--type' , dest = 'type' , action = 'store' , default = None , required = False , metavar = 'type' , help = 'Sets the type metadata field' ) self . parser . add argument ( '-x' , '--is-disabled' , dest = 'is Disabled' , action = 'store' , default = None , required = False , choices = [ 'true' , 'false' ] , help = 'Enable or disable the metric definition' )", "predictions": ["get class class class = fmt = 1 = 1 = 0 = 2 = 1 = 2 = 1 = 2 = 1 = 1 = 1 = 2 ="], "references": ["add the specific arguments of this cli"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5039, "code": "def get arguments ( self ) : Metric Common . get arguments ( self ) if self . args . metric Name is not None : self . metric Name = self . args . metric Name if self . args . display Name is not None : self . display Name = self . args . display Name if self . args . display Name Short is not None : self . display Name Short = self . args . display Name Short if self . args . description is not None : self . description = self . args . description if self . args . aggregate is not None : self . aggregate = self . args . aggregate if self . args . unit is not None : self . unit = self . args . unit if self . args . resolution is not None : self . resolution = self . args . resolution if self . args . is Disabled is not None : self . is Disabled = self . args . is Disabled if self . args . type is not None : self . type = self . args . type data = { } if self . metric Name is not None : data [ 'name' ] = self . metric Name if self . display Name is not None : data [ 'display Name' ] = self . display Name if self . display Name Short is not None : data [ 'display Name Short' ] = self . display Name Short if self . description is not None : data [ 'description' ] = self . description if self . aggregate is not None : data [ 'default Aggregate' ] = self . aggregate if self . unit is not None : data [ 'unit' ] = self . unit if self . resolution is not None : data [ 'default Resolution MS' ] = self . resolution if self . is Disabled is not None : data [ 'is Disabled' ] = True if self . is Disabled == 'yes' else False if self . type is not None : data [ 'type' ] = self . type self . path = \"v1/metrics/{0}\" . format ( self . metric Name ) self . data = json . dumps ( data , sort keys = True ) self . headers = { 'Content-Type' : 'application/json' , \"Accept\" : \"application/json\" }", "predictions": ["get the entry list for the directive"], "references": ["extracts the specific arguments of this cli"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 5040, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) self . alarm name = self . args . alarm name if self . args . alarm name is not None else None", "predictions": ["get the rule rule from the = value"], "references": ["extracts the specific arguments of this cli"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 5041, "code": "def read ( self ) : f = open ( self . path , \"r\" ) self . manifest json = f . read ( )", "predictions": ["get the input input file p from the input directory p p p p p"], "references": ["load the metrics file from the given path"], "bleu": 0.12300686288463772, "rouge_l": 0.36802413273001505}
{"id": 5042, "code": "def load ( self ) : manifest = Plugin Manifest ( self . file path ) manifest . get ( ) self . manifest = manifest . get manifest ( )", "predictions": ["expression from the manifest"], "references": ["read the file and parse json into dictionary"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 5043, "code": "def get Metric Definition ( self , name ) : metric = None for m in self . metric definitions : if m [ 'name' ] == name : metric = m break return metric", "predictions": ["gets the . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["looks up the metric definition from the definitions from the api call"], "bleu": 0.03901663112717908, "rouge_l": 0.05053852526926264}
{"id": 5044, "code": "def print Metrics Header ( self , m , d ) : mstr = \"Metric Name\" dstr = \"Description\" print ( '|{0}{1}|{2}{3}|' . format ( mstr , ' ' * ( m - len ( mstr ) ) , dstr , ' ' * ( d - len ( dstr ) ) ) ) print ( '|:{0}|:{1}|' . format ( '-' * ( m - 1 ) , '-' * ( d - 1 ) ) )", "predictions": ["ast for printing header"], "references": ["prints out table header based on the size of the data in columns"], "bleu": 0.037870374782798366, "rouge_l": 0.1073943661971831}
{"id": 5045, "code": "def get Fields Column Lengths ( self ) : name Len = 0 desc Len = 0 for f in self . fields : name Len = max ( name Len , len ( f [ 'title' ] ) ) desc Len = max ( desc Len , len ( f [ 'description' ] ) ) return ( name Len , desc Len )", "predictions": ["returns the * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"], "references": ["gets the maximum length of each column in the field table"], "bleu": 0.03901663112717908, "rouge_l": 0.05209222886421862}
{"id": 5046, "code": "def get Metrics Column Lengths ( self ) : display Len = 0 desc Len = 0 for m in self . metrics : display Len = max ( display Len , len ( m [ 'display Name' ] ) ) desc Len = max ( desc Len , len ( m [ 'description' ] ) ) return ( display Len , desc Len )", "predictions": ["returns the maximum shorthand shorthand shorthand for this node"], "references": ["gets the maximum length of each column"], "bleu": 0.18575057999133596, "rouge_l": 0.2557651991614256}
{"id": 5047, "code": "def escape Underscores ( self ) : new metrics = [ ] for m in self . metrics : m [ 'name' ] = m [ 'name' ] . replace ( \" \" , \"\\ \" ) new metrics . append ( m ) self . metrics = new metrics", "predictions": ["ast metrics metrics metrics"], "references": ["escape underscores so that the markdown is correct"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 5048, "code": "def print Fields Header ( self , f , d ) : fstr = \"Field Name\" dstr = \"Description\" f = max ( f , len ( fstr ) ) d = max ( d , len ( dstr ) ) print ( '|{0}{1}|{2}{3}|' . format ( fstr , ' ' * ( f - len ( fstr ) ) , dstr , ' ' * ( d - len ( dstr ) ) ) ) print ( '|:{0}|:{1}|' . format ( '-' * ( f - 1 ) , '-' * ( d - 1 ) ) ) return ( f , d )", "predictions": ["ast the header for a go term or a go id or a function or a function prints ."], "references": ["prints out table header based on the size of the data in columns"], "bleu": 0.07658412276041004, "rouge_l": 0.06468716861081655}
{"id": 5049, "code": "def print Metrics ( self , m , d ) : for metric in self . metrics : mstr = metric [ 'display Name' ] dstr = metric [ 'description' ] mlen = m - len ( mstr ) dlen = d - len ( dstr ) print ( \"|{0}{1}|{2}{3}|\" . format ( mstr , ' ' * mlen , dstr , ' ' * dlen ) )", "predictions": ["ast = true if the given metric is a list of = false"], "references": ["prints out table rows based on the size of the data in columns"], "bleu": 0.10571070857151538, "rouge_l": 0.15384615384615383}
{"id": 5050, "code": "def print Fields ( self , f , d ) : for field in self . fields : fstr = field [ \"title\" ] dstr = field [ \"description\" ] flen = f - len ( fstr ) dlen = d - len ( dstr ) print ( \"|{0}{1}|{2}{3}|\" . format ( fstr , ' ' * flen , dstr , ' ' * dlen ) )", "predictions": ["ast the contents of a specific ."], "references": ["prints out table rows based on the size of the data in columns"], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 5051, "code": "def output Field Markdown ( self ) : f , d = self . get Fields Column Lengths ( ) fc , dc = self . print Fields Header ( f , d ) f = max ( fc , f ) d = max ( dc , d ) self . print Fields ( f , d )", "predictions": ["prints out statistics for debugging purposes ."], "references": ["sends the field definitions ot standard out"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 5052, "code": "def output Metric Markdown ( self ) : self . escape Underscores ( ) m , d = self . get Metrics Column Lengths ( ) self . print Metrics Header ( m , d ) self . print Metrics ( m , d )", "predictions": ["prints the header information for printing . . ."], "references": ["sends the markdown of the metric definitions to standard out"], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 5053, "code": "def generate Markdown ( self ) : self . generate Metric Definitions ( ) self . generate Field Definitions ( ) self . generate Dashboard Definitions ( ) self . output Markdown ( )", "predictions": ["generates an environment for this code ."], "references": ["look up each of the metrics and then output in markdown"], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 5054, "code": "def parse ( self , text ) : self . original text = text try : return getattr ( self , self . entry point ) ( text ) except ( Dead End ) as exc : raise Parser Error ( self . most consumed , \"Failed to parse input\" ) from exc return tree", "predictions": ["ast function for parsing the ."], "references": ["attempt to parse source code ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 5055, "code": "def attempting ( self , text ) : consumed = len ( self . original text ) - len ( text ) self . most consumed = max ( consumed , self . most consumed )", "predictions": ["find d - right consumed should be executed str str str str str str str str str str str str str str str str str str str str str str str"], "references": ["keeps track of the furthest point in the source code the parser has reached to this point ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5056, "code": "def add arguments ( self ) : Api Cli . add arguments ( self ) self . parser . add argument ( '-f' , '--format' , dest = 'format' , action = 'store' , required = False , choices = [ 'csv' , 'json' , 'raw' , 'xml' ] , help = 'Output format. Default is raw' ) self . parser . add argument ( '-n' , '--name' , dest = 'metric name' , action = 'store' , required = True , metavar = \"metric name\" , help = 'Metric identifier' ) self . parser . add argument ( '-g' , '--aggregate' , dest = 'aggregate' , action = 'store' , required = False , choices = [ 'sum' , 'avg' , 'max' , 'min' ] , help = 'Metric default aggregate' ) self . parser . add argument ( '-r' , '--sample' , dest = 'sample' , action = 'store' , type = int , metavar = \"sample\" , help = 'Down sample rate sample in seconds' ) self . parser . add argument ( '-s' , '--source' , dest = 'source' , action = 'store' , metavar = \"source\" , required = True , help = 'Source of measurement' ) self . parser . add argument ( '-b' , '--start' , dest = 'start' , action = 'store' , required = True , metavar = \"start\" , help = 'Start of time range as ISO 8601 string or epoch seconds' ) self . parser . add argument ( '-d' , '--end' , dest = 'end' , action = 'store' , metavar = \"end\" , required = False , help = 'End of time range as ISO 8601 string or epoch seconds' ) self . parser . add argument ( '-o' , '--date-format' , dest = 'date format' , action = 'store' , metavar = \"format\" , required = False , help = 'For CSV, JSON, and XML output formats dates (see Python date.strftime). ' + 'Default format is %%s' )", "predictions": ["add arguments for this command ."], "references": ["add specific command line arguments for this command"], "bleu": 0.46105843756805864, "rouge_l": 0.6963470319634703}
{"id": 5057, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . metric name is not None : self . metric name = self . args . metric name if self . args . sample is not None : self . sample = self . args . sample if self . args . source is not None : self . source = self . args . source else : self . source = None if self . args . aggregate is not None : self . aggregate = self . args . aggregate else : self . aggregate = \"avg\" if self . args . format is not None : self . format = self . args . format else : self . format = \"json\" if self . args . date format is not None : self . date format = self . args . date format start time = int ( self . parse time date ( self . args . start ) . strftime ( \"%s\" ) ) if self . args . end is None : stop time = int ( self . now . strftime ( \"%s\" ) ) else : stop time = int ( self . parse time date ( self . args . end ) . strftime ( \"%s\" ) ) start time *= 1000 stop time *= 1000 self . path = \"v1/measurements/{0}\" . format ( self . metric name ) url parameters = { \"start\" : str ( start time ) , \"end\" : str ( stop time ) , \"sample\" : str ( self . sample ) , \"agg\" : self . aggregate } if self . source is not None : url parameters [ 'source' ] = self . source self . url parameters = url parameters", "predictions": ["get arguments from the metric"], "references": ["extracts the specific arguments of this cli"], "bleu": 0.20252884954471367, "rouge_l": 0.16180371352785147}
{"id": 5058, "code": "def output csv ( self , text ) : payload = json . loads ( text ) print ( \"{0},{1},{2},{3},{4}\" . format ( 'timestamp' , 'metric' , 'aggregate' , 'source' , 'value' ) ) metric name = self . metric name for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : timestamp = self . format timestamp ( r [ 0 ] [ 0 ] ) for s in r [ 1 ] : print ( '{0},\"{1}\",\"{2}\",\"{3}\",{4}' . format ( timestamp , metric name , self . aggregate , s [ 0 ] , s [ 1 ] ) )", "predictions": ["prints the csv text in a csv format"], "references": ["output results in csv format"], "bleu": 0.22679164443904004, "rouge_l": 0.48157894736842105}
{"id": 5059, "code": "def output json ( self , text ) : payload = json . loads ( text ) data = [ ] metric name = self . metric name for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : timestamp = self . format timestamp ( r [ 0 ] [ 0 ] ) for s in r [ 1 ] : data . append ( { \"timestamp\" : timestamp , \"metric\" : metric name , \"aggregate\" : self . aggregate , \"source\" : s [ 0 ] , \"value\" : s [ 1 ] , } ) payload = { \"data\" : data } out = json . dumps ( payload , indent = self . indent , separators = ( ',' , ': ' ) ) print ( self . colorize json ( out ) )", "predictions": ["prints the json representation of a json string ."], "references": ["output results in structured json format"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 5060, "code": "def output raw ( self , text ) : payload = json . loads ( text ) out = json . dumps ( payload , sort keys = True , indent = self . indent , separators = ( ',' , ': ' ) ) print ( self . colorize json ( out ) )", "predictions": ["print the json representation of the text ."], "references": ["output results in raw json format"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 5061, "code": "def output xml ( self , text ) : document = Element ( 'results' ) comment = Comment ( 'Generated by True Sight Pulse measurement-get CLI' ) document . append ( comment ) aggregates = Sub Element ( document , 'aggregates' ) aggregate = Sub Element ( aggregates , 'aggregate' ) measurements = Sub Element ( aggregate , 'measurements' ) payload = json . loads ( text ) metric name = self . metric name for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : timestamp = self . format timestamp ( r [ 0 ] [ 0 ] ) for s in r [ 1 ] : measure node = Sub Element ( measurements , 'measure' ) source = s [ 0 ] value = str ( s [ 1 ] ) ts node = Sub Element ( measure node , 'timestamp' ) ts node . text = str ( timestamp ) metric node = Sub Element ( measure node , 'metric' ) metric node . text = metric name metric node = Sub Element ( measure node , 'aggregate' ) metric node . text = self . aggregate source node = Sub Element ( measure node , 'source' ) source node . text = source value node = Sub Element ( measure node , 'value' ) value node . text = value rough string = Element Tree . tostring ( document , 'utf-8' ) reparse = minidom . parse String ( rough string ) output = reparse . toprettyxml ( indent = \" \" ) print ( self . colorize xml ( output ) )", "predictions": ["write text to xml"], "references": ["output results in json format"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 5062, "code": "def handle results ( self ) : if self . api result . status code == requests . codes . ok : if self . format == \"json\" : self . output json ( self . api result . text ) elif self . format == \"csv\" : self . output csv ( self . api result . text ) elif self . format == \"raw\" : self . output raw ( self . api result . text ) elif self . format == \"xml\" : self . output xml ( self . api result . text ) else : pass", "predictions": ["handler for the results from the api ."], "references": ["call back function to be implemented by the cli ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 5063, "code": "def trimmed pred default ( node , parent ) : return isinstance ( node , Parse Node ) and ( node . is empty or node . is type ( Parse Node Type . terminal ) )", "predictions": ["return an astroid . pred node ."], "references": ["the default predicate used in node . trimmed ."], "bleu": 0.19740631366145517, "rouge_l": 0.24448897795591182}
{"id": 5064, "code": "def pprint ( root , depth = 0 , space unit = \"    \" , * , source len = 0 , file = None ) : spacing = space unit * depth if isinstance ( root , str ) : print ( \"{0}terminal@(?): {1}\" . format ( spacing , root ) , file = file ) else : if root . position is None : position = - 1 elif root . position < 0 : position = source len + root . position else : position = root . position if root . is value : print ( \"{0}{1}@({2}:{3}):\\t{4}\" . format ( spacing , root . node type , position , root . consumed , root . svalue ) , file = file ) else : print ( \"{0}{1}@({2}:{3}):\" . format ( spacing , root . node type , position , root . consumed ) , file = file ) for child in root . children : pprint ( child , depth + 1 , source len = source len , file = file )", "predictions": ["pretty - print the xml representation of a python object ."], "references": ["pretting print a parse tree ."], "bleu": 0.1354599427337814, "rouge_l": 0.3727087576374745}
{"id": 5065, "code": "def repetition ( extractor , bounds , * , ignore whitespace = False ) : return partial ( get repetition , extractor , bounds = bounds , ignore whitespace = ignore whitespace )", "predictions": ["return a repetition instance of the given extractor ."], "references": ["returns a partial of _get_repetition that accepts only a text argument ."], "bleu": 0.12026590852507517, "rouge_l": 0.2785388127853881}
{"id": 5066, "code": "def count leading whitespace ( text ) : idx = 0 for idx , char in enumerate ( text ) : if not char . isspace ( ) : return idx return idx + 1", "predictions": ["count the number of whitespace whitespace in a string ."], "references": ["returns the number of characters at the beginning of text that are whitespace ."], "bleu": 0.17405100568758053, "rouge_l": 0.40450928381962864}
{"id": 5067, "code": "def retyped ( self , new type ) : return Parse Node ( new type , children = list ( self . children ) , consumed = self . consumed , position = self . position , ignored = self . ignored )", "predictions": ["build a new node from the given type ."], "references": ["returns a new node with the same contents as self but with a new node_type ."], "bleu": 0.13356214772572012, "rouge_l": 0.38077403245942576}
{"id": 5068, "code": "def get arguments ( self ) : Api Cli . get arguments ( self ) if self . args . host group name is not None : self . host group name = self . args . host group name if self . args . sources is not None : self . sources = self . args . sources payload = { } if self . host group name is not None : payload [ 'name' ] = self . host group name if self . sources is not None : source list = str . split ( self . sources , ',' ) if 'hostnames' not in payload : payload [ 'hostnames' ] = [ ] for s in source list : payload [ 'hostnames' ] . append ( s ) self . data = json . dumps ( payload , sort keys = True ) self . headers = { 'Content-Type' : 'application/json' , \"Accept\" : \"application/json\" }", "predictions": ["get the arguments from the host"], "references": ["extracts the specific arguments of this cli"], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 5069, "code": "def get scope list ( self ) -> list : lstparent = [ self ] p = self . get parent ( ) while p is not None : lstparent . append ( p ) p = p . get parent ( ) return lstparent", "predictions": ["returns a list of all scope in this instance ."], "references": ["return the list of all contained scope from global to local"], "bleu": 0.22447582175704436, "rouge_l": 0.37770897832817335}
{"id": 5070, "code": "def get scope names ( self ) -> list : lscope = [ ] for scope in reversed ( self . get scope list ( ) ) : if scope . name is not None : lscope . append ( scope . name ) return lscope", "predictions": ["return a list of scope names ."], "references": ["return the list of all contained scope from global to local"], "bleu": 0.15685718045401453, "rouge_l": 0.4273204903677758}
{"id": 5071, "code": "def position ( self ) -> Position : return Position ( self . index , self . lineno , self . col offset )", "predictions": ["return the position of this node ."], "references": ["the current position of the cursor ."], "bleu": 0.2777619034011791, "rouge_l": 0.5714285714285714}
{"id": 5072, "code": "def max readed position ( self ) -> Position : return Position ( self . maxindex , self . maxline , self . maxcol )", "predictions": ["maximum position of the readed readed ."], "references": ["the index of the deepest character readed ."], "bleu": 0.2664731314108428, "rouge_l": 0.5269978401727862}
{"id": 5073, "code": "def step next char ( self ) : self . index += 1 self . col offset += 1 if self . index > self . maxindex : self . maxindex = self . index self . maxcol = self . col offset self . maxline = self . lineno", "predictions": ["step next char in character ."], "references": ["puts the cursor on the next character ."], "bleu": 0.2238400777155271, "rouge_l": 0.4178082191780822}
{"id": 5074, "code": "def step next line ( self ) : self . eol . append ( self . position ) self . lineno += 1 self . col offset = 0", "predictions": ["step the next line in the eol ."], "references": ["sets cursor as beginning of next line ."], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 5075, "code": "def step prev line ( self ) : #TODO(bps): raise explicit error for unregistered eol #assert self. eol[-1].index == self. index if len ( self . eol ) > 0 : self . position = self . eol . pop ( )", "predictions": ["move current line eol to previous line"], "references": ["sets cursor as end of previous line ."], "bleu": 0.21191828141393895, "rouge_l": 0.2634989200863931}
{"id": 5076, "code": "def last readed line ( self ) -> str : mpos = self . cursor . max readed position mindex = mpos . index prevline = mindex - 1 if mindex == self . eos index else mindex while prevline >= 0 and self . content [ prevline ] != '\\n' : prevline -= 1 nextline = mindex while nextline < self . eos index and self . content [ nextline ] != '\\n' : nextline += 1 last line = self . content [ prevline + 1 : nextline ] return last line", "predictions": ["return the last readed line number ."], "references": ["usefull string to compute error message ."], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 5077, "code": "def incpos ( self , length : int = 1 ) -> int : if length < 0 : raise Value Error ( \"length must be positive\" ) i = 0 while ( i < length ) : if self . cursor . index < self . len : if self . peek char == '\\n' : self . cursor . step next line ( ) self . cursor . step next char ( ) i += 1 return self . cursor . index", "predictions": ["read bits and interpret as a number of bytes ."], "references": ["increment the cursor to the next character ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 5078, "code": "def save context ( self ) -> bool : self . contexts . append ( self . cursor . position ) return True", "predictions": ["save current cursor to the cursor ."], "references": ["save current position ."], "bleu": 0.2626909894424158, "rouge_l": 0.5736677115987461}
{"id": 5079, "code": "def restore context ( self ) -> bool : self . cursor . position = self . contexts . pop ( ) return False", "predictions": ["restore the context from the cursor ."], "references": ["rollback to previous saved position ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 5080, "code": "def to fmt ( self ) -> fmt . indentable : qual = \"scope\" txt = fmt . sep ( \" \" , [ qual ] ) name = self . show name ( ) if name != \"\" : txt . lsdata . append ( name ) if len ( self . hsig ) > 0 or len ( self . map Type Translate ) > 0 : lsb = [ ] if len ( self . map Type Translate ) > 0 : lsb . append ( \"translate:\\n\" ) lsb . append ( fmt . end ( \"\\n\" , self . map Type Translate . to fmt ( ) ) ) for k in sorted ( self . hsig . keys ( ) ) : s = self . hsig [ k ] lsb . append ( fmt . end ( \"\\n\" , [ s . to fmt ( ) ] ) ) block = fmt . block ( \":\\n\" , \"\" , fmt . tab ( lsb ) ) txt . lsdata . append ( block ) return txt", "predictions": ["return a unicode object representation of this block ."], "references": ["return an fmt representation for pretty - printing"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 5081, "code": "def to fmt ( self ) : qual = \"evalctx\" lseval = [ ] block = fmt . block ( \":\\n\" , \"\" , fmt . tab ( lseval ) ) txt = fmt . sep ( \" \" , [ qual , block ] ) lseval . append ( self . sig . to fmt ( ) ) if len ( self . resolution ) > 0 : lsb = [ ] for k in sorted ( self . resolution . keys ( ) ) : s = self . resolution [ k ] if s is not None : lsb . append ( fmt . end ( \"\\n\" , [ \"'%s': %s (%s)\" % ( k , s , s ( ) . show name ( ) ) ] ) ) else : lsb . append ( fmt . end ( \"\\n\" , [ \"'%s': Unresolved\" % ( k ) ] ) ) if self . translate to is not None : lsb . append ( \"use translator:\" ) lsb . append ( self . translate to . to fmt ( ) ) if self . variadic types is not None : lsb . append ( \"variadic types:\\n\" ) arity = self . sig . arity for t in self . variadic types : lsb . append ( \"[%d] : %s\\n\" % ( arity , t ) ) arity += 1 lseval . append ( fmt . block ( \"\\nresolution :\\n\" , \"\" , fmt . tab ( lsb ) ) ) return txt", "predictions": ["serializes this resolution object to a string ."], "references": ["return an fmt representation for pretty - printing"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5082, "code": "def to fmt ( self , with from = False ) -> fmt . indentable : txt = fmt . sep ( \"\\n\" , [ fmt . sep ( \" \" , [ self . type source , \"to\" , self . type target , '=' , self . fun . to fmt ( ) ] ) , self . notify . get content ( with from ) ] ) return txt", "predictions": ["format format as a string ."], "references": ["return a fmt representation of translator for pretty - printing"], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 5083, "code": "def to fmt ( self ) : params = \"\" txt = fmt . sep ( \" \" , [ 'val' ] ) name = self . show name ( ) if name != \"\" : txt . lsdata . append ( name ) txt . lsdata . append ( '(%s)' % self . value ) txt . lsdata . append ( ': ' + self . tret ) return txt", "predictions": ["return a string representation of this node ."], "references": ["return an fmt representation for pretty - printing"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 5084, "code": "def to fmt ( self ) : params = \"\" txt = fmt . sep ( \" \" , [ 'fun' ] ) name = self . show name ( ) if name != \"\" : txt . lsdata . append ( name ) tparams = [ ] if self . tparams is not None : tparams = list ( self . tparams ) if self . variadic : tparams . append ( '...' ) params = '(' + \", \" . join ( tparams ) + ')' txt . lsdata . append ( ': ' + params ) txt . lsdata . append ( '-> ' + self . tret ) return txt", "predictions": ["return a string representation of this tree ."], "references": ["return an fmt representation for pretty - printing"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 5085, "code": "def set name ( self , name : str ) : self . name = name lsig = self . hsig . values ( ) self . hsig = { } for s in lsig : self . hsig [ s . internal name ( ) ] = s", "predictions": ["set the name of the hsig to the given name ."], "references": ["you could set the name after construction"], "bleu": 0.21200626759025185, "rouge_l": 0.3472485768500949}
{"id": 5086, "code": "def count vars ( self ) -> int : n = 0 for s in self . hsig . values ( ) : if hasattr ( s , 'is var' ) and s . is var : n += 1 return n", "predictions": ["return the number of vars in this document ."], "references": ["count var define by this scope"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 5087, "code": "def count funs ( self ) -> int : n = 0 for s in self . hsig . values ( ) : if hasattr ( s , 'is fun' ) and s . is fun : n += 1 return n", "predictions": ["count the number of funs in this document ."], "references": ["count function define by this scope"], "bleu": 0.15619699684601276, "rouge_l": 0.27664399092970515}
{"id": 5088, "code": "def update ( self , sig : list or Scope ) -> Scope : values = sig if hasattr ( sig , 'values' ) : values = sig . values ( ) for s in values : if self . is namespace : s . set parent ( self ) if isinstance ( s , Scope ) : s . state = State Scope . EMBEDDED self . hsig [ s . internal name ( ) ] = s self . update count ( ) return self", "predictions": ["add an ast to the list . . . ."], "references": ["update the set with values of another set"], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 5089, "code": "def union ( self , sig : Scope ) -> Scope : new = Scope ( sig = self . hsig . values ( ) , state = self . state ) new |= sig return new", "predictions": ["create a new get get a if it is a if it doesn t exist metric metric metric metric metric metric"], "references": ["create a new set produce by the union of 2 set"], "bleu": 0.10813005337959174, "rouge_l": 0.19869706840390877}
{"id": 5090, "code": "def intersection update ( self , oset : Scope ) -> Scope : keys = list ( self . hsig . keys ( ) ) for k in keys : if k not in oset : del self . hsig [ k ] else : self . hsig [ k ] = oset . get ( k ) return self", "predictions": ["csv csv csv csv"], "references": ["update set with common values of another set"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 5091, "code": "def intersection ( self , sig : Scope ) -> Scope : new = Scope ( sig = self . hsig . values ( ) , state = self . state ) new &= sig return new", "predictions": ["create a new output output from the given * sig * loads * loads * loads * loads * loads * loads * loads * loads * loads * loads *"], "references": ["create a new set produce by the intersection of 2 set"], "bleu": 0.07678432706586173, "rouge_l": 0.20836891545687447}
{"id": 5092, "code": "def difference update ( self , oset : Scope ) -> Scope : keys = list ( self . hsig . keys ( ) ) for k in keys : if k in oset : del self . hsig [ k ] return self", "predictions": ["raw raw output raw json data from text out out of text out out out out out out out out out out out out out out out out out of text"], "references": ["remove values common with another set"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5093, "code": "def difference ( self , sig : Scope ) -> Scope : new = Scope ( sig = self . hsig . values ( ) , state = self . state ) new -= sig return new", "predictions": ["create a new empty state . . . . . . . . . ."], "references": ["create a new set produce by a set subtracted by another set"], "bleu": 0.15310245441182443, "rouge_l": 0.22676579925650556}
{"id": 5094, "code": "def symmetric difference ( self , sig : Scope ) -> Scope : new = Scope ( sig = self . hsig . values ( ) , state = self . state ) new ^= sig return new", "predictions": ["create a new empty handle == sig == ok == ok == sig == ok == ok == 1 == ok == 1 == ok == 1 == 1 == 1"], "references": ["create a new set with values present in only one set"], "bleu": 0.07261813302549416, "rouge_l": 0.15627668659265584}
{"id": 5095, "code": "def add ( self , it : Signature ) -> bool : if isinstance ( it , Scope ) : it . state = State Scope . EMBEDDED txt = it . internal name ( ) it . set parent ( self ) if self . is namespace : txt = it . internal name ( ) if txt == \"\" : txt = ' ' + str ( len ( self . hsig ) ) if txt in self . hsig : raise Key Error ( \"Already exists %s\" % txt ) self . hsig [ txt ] = it self . update count ( ) return True", "predictions": ["trimmed are added to the collection and update and update and update and update"], "references": ["add it to the set"], "bleu": 0.11633270842295028, "rouge_l": 0.23018867924528305}
{"id": 5096, "code": "def remove ( self , it : Signature ) -> bool : txt = it . internal name ( ) if txt not in self . hsig : raise Key Error ( it . show name ( ) + ' not in Set' ) sig = self . hsig [ txt ] if isinstance ( sig , Scope ) : sig . state = State Scope . LINKED del self . hsig [ txt ] return True", "predictions": ["pprint a if it exists"], "references": ["remove it but raise keyerror if not found"], "bleu": 0.1658165975077607, "rouge_l": 0.1476997578692494}
{"id": 5097, "code": "def discard ( self , it : Signature ) -> bool : txt = it . internal name ( ) if txt in self . hsig : sig = self . hsig [ txt ] if isinstance ( sig , Scope ) : sig . state = State Scope . LINKED del self . hsig [ txt ] return True return False", "predictions": ["remove an hsig from the hsig return false if it is not defined"], "references": ["remove it only if present"], "bleu": 0.1135935489027116, "rouge_l": 0.24158415841584158}
{"id": 5098, "code": "def first ( self ) -> Signature : k = sorted ( self . hsig . keys ( ) ) return self . hsig [ k [ 0 ] ]", "predictions": ["the count of all hsig for this query for the hsig for the hsig for the hsig for the hsig for the hsig for the hsig for the hsig for the"], "references": ["retrieve the first signature ordered by mangling descendant"], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 5099, "code": "def last ( self ) -> Signature : k = sorted ( self . hsig . keys ( ) ) return self . hsig [ k [ - 1 ] ]", "predictions": ["the last of the children . . . . . . . . . . . ."], "references": ["retrieve the last signature ordered by mangling descendant"], "bleu": 0.09507244120026236, "rouge_l": 0.17110799438990182}
{"id": 5100, "code": "def get ( self , key : str , default = None ) -> Signature : item = default if key in self . hsig : item = self . hsig [ key ] return item", "predictions": ["get a value from the = default"], "references": ["get a signature instance by its internal_name"], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 5101, "code": "def get by symbol name ( self , name : str ) -> Scope : lst = [ ] for s in self . values ( ) : if s . name == name : lst . append ( Eval Ctx . from sig ( s ) ) if len ( lst ) == 0 : p = self . get parent ( ) if p is not None : return p . get by symbol name ( name ) rscope = Scope ( sig = lst , state = State Scope . LINKED , is namespace = False ) rscope . set parent ( self ) return rscope", "predictions": ["get a list of all data scope with the given list"], "references": ["retrieve a set of all signature by symbol name"], "bleu": 0.16108992769687397, "rouge_l": 0.3055091819699499}
{"id": 5102, "code": "def call Injector ( self , old : Node , trans : Translator ) -> Node : if self . ast Translator Injector is None : if self . parent is not None : return self . parent ( ) . call Injector ( old , trans ) else : raise Type Error ( \"Must define an Translator Injector\" ) return self . ast Translator Injector ( old , trans )", "predictions": ["get the scope of the in the in the in the in the in - place for the in the in the in the in the in the in the in"], "references": ["if don t have injector call from parent"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5103, "code": "def set ( self , othernode ) : self . class = othernode . class self . clean ( ) if len ( othernode ) > 0 : for k , v in othernode . items ( ) : self [ k ] = v for k , v in vars ( othernode ) . items ( ) : setattr ( self , k , v )", "predictions": ["position the given string with the given othernode return the key value return the key value"], "references": ["allow to completly mutate the node into any subclasses of node"], "bleu": 0.07692375026049747, "rouge_l": 0.07663316582914573}
{"id": 5104, "code": "def hit ok ( hit , min hit charge , max hit charge ) : if hit [ 'charge' ] < min hit charge : return False if max hit charge != 0 and hit [ 'charge' ] > max hit charge : return False return True", "predictions": ["returns true iff the max max max max charge is readed"], "references": ["check if given hit is withing the limits ."], "bleu": 0.12605968092174913, "rouge_l": 0.1018363939899833}
{"id": 5105, "code": "def resolve ( self ) : t2resolv = [ ] if hasattr ( self . sig , 'tret' ) : t2resolv . append ( self . sig . tret ) if hasattr ( self . sig , 'tparams' ) and self . sig . tparams is not None : for p in self . sig . tparams : t2resolv . append ( p ) if self . translate to is not None : t2resolv . append ( self . translate to . target ) if self . variadic types is not None : for t in self . variadic types : t2resolv . append ( t ) for t in t2resolv : for c in t . components : if c not in self . resolution or self . resolution [ c ] is None : parent = self . get parent ( ) if parent is not None : sc = parent . get by symbol name ( c ) if len ( sc ) == 1 : sc = list ( sc . values ( ) ) [ 0 ] if isinstance ( sc , Eval Ctx ) : sc = sc . sig rtyp = weakref . ref ( sc ) self . resolution [ c ] = rtyp continue self . resolution [ c ] = None", "predictions": ["step the resolution to be displayed"], "references": ["process the signature and find definition for type ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 5106, "code": "def set resolved name ( self , ref : dict , type name2solve : Type Name , type name ref : Type Name ) : if self . resolution [ type name2solve . value ] is None : self . resolution [ type name2solve . value ] = ref [ type name ref . value ]", "predictions": ["step for setting the line of the given append"], "references": ["warning!!! need to rethink it when global poly type"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5107, "code": "def to fmt ( self ) -> fmt . indentable : lsb = [ ] if len ( self . lsig ) > 0 : for s in self . lsig : lsb . append ( s . to fmt ( ) ) block = fmt . block ( \"(\" , \")\" , fmt . sep ( ', ' , lsb ) ) qual = \"tuple\" txt = fmt . sep ( \"\" , [ qual , block ] ) return txt", "predictions": ["format format as a string ."], "references": ["return an fmt representation for pretty - printing"], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 5108, "code": "def internal name ( self ) : unq = super ( ) . internal name ( ) if self . tret is not None : unq += \" \" + self . tret return unq", "predictions": ["- readed readed readed readed"], "references": ["return the unique internal name"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 5109, "code": "def delete local ( self , filename ) : if os . path . exists ( filename ) : os . remove ( filename )", "predictions": ["delete local to ."], "references": ["deletes the specified file from the local filesystem ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 5110, "code": "def delete s3 ( self , filename , bucket name ) : conn = S3Connection ( self . access key id , self . access key secret ) bucket = conn . get bucket ( bucket name ) if type ( filename ) . name == 'Key' : filename = '/' + filename . name path = self . get s3 path ( filename ) k = Key ( bucket ) k . key = path try : bucket . delete key ( k ) except S3Response Error : pass", "predictions": ["save the key key to the given bool return the contents of the given bool return the key return the key return the key return none if it exists"], "references": ["deletes the specified file from the given s3 bucket ."], "bleu": 0.059055756167575545, "rouge_l": 0.16866359447004606}
{"id": 5111, "code": "def delete ( self , filename , storage type = None , bucket name = None ) : if not ( storage type and bucket name ) : self . delete local ( filename ) else : if storage type != 's3' : raise Value Error ( 'Storage type \"%s\" is invalid, the only supported storage type (apart from default local storage) is s3.' % storage type ) self . delete s3 ( filename , bucket name )", "predictions": ["restore the given cursor to the s3 . . . . . . . ."], "references": ["deletes the specified file either locally or from s3 depending on the file s storage type ."], "bleu": 0.0905244537286197, "rouge_l": 0.18541033434650459}
{"id": 5112, "code": "def save local ( self , temp file , filename , obj ) : path = self . get path ( filename ) if not os . path . exists ( os . path . dirname ( path ) ) : os . makedirs ( os . path . dirname ( path ) , self . permission | 0o111 ) fd = open ( path , 'wb' ) temp file . seek ( 0 ) t = temp file . read ( 1048576 ) while t : fd . write ( t ) t = temp file . read ( 1048576 ) fd . close ( ) if self . filesize field : setattr ( obj , self . filesize field , os . path . getsize ( path ) ) return filename", "predictions": ["to to to to to to to to file sep sep sep sep sep sep sep sep sep ."], "references": ["saves the specified file to the local file system ."], "bleu": 0.07658412276041004, "rouge_l": 0.21916167664670658}
{"id": 5113, "code": "def save s3 ( self , temp file , filename , obj ) : conn = S3Connection ( self . access key id , self . access key secret ) bucket = conn . get bucket ( self . bucket name ) path = self . get s3 path ( filename ) k = bucket . new key ( path ) k . set contents from string ( temp file . getvalue ( ) ) k . set acl ( self . acl ) if self . filesize field : setattr ( obj , self . filesize field , k . size ) return filename", "predictions": ["to to to to to to to to file ."], "references": ["saves the specified file to the configured s3 bucket ."], "bleu": 0.14991106946711685, "rouge_l": 0.2}
{"id": 5114, "code": "def save ( self , temp file , filename , obj ) : if not ( self . storage type and self . bucket name ) : ret = self . save local ( temp file , filename , obj ) else : if self . storage type != 's3' : raise Value Error ( 'Storage type \"%s\" is invalid, the only supported storage type (apart from default local storage) is s3.' % self . storage type ) ret = self . save s3 ( temp file , filename , obj ) if self . field name : setattr ( obj , self . field name , ret ) if self . storage type == 's3' : if self . storage type field : setattr ( obj , self . storage type field , self . storage type ) if self . bucket name field : setattr ( obj , self . bucket name field , self . bucket name ) else : if self . storage type field : setattr ( obj , self . storage type field , '' ) if self . bucket name field : setattr ( obj , self . bucket name field , '' ) return ret", "predictions": ["to to to to to to to to file ."], "references": ["saves the specified file to either s3 or the local filesystem depending on the currently enabled storage type ."], "bleu": 0.06094929250459501, "rouge_l": 0.13062098501070665}
{"id": 5115, "code": "def find by path s3 ( self , path , bucket name ) : conn = S3Connection ( self . access key id , self . access key secret ) bucket = conn . get bucket ( bucket name ) s3 path = self . get s3 path ( path ) return bucket . list ( prefix = s3 path )", "predictions": ["list files in a = self . ."], "references": ["finds files by licking an s3 bucket s contents by prefix ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 5116, "code": "def enum ( * sequential , * * named ) : #: build enums from parameter enums = dict ( zip ( sequential , range ( len ( sequential ) ) ) , * * named ) enums [ 'map' ] = copy . copy ( enums ) #: build reverse mapping enums [ 'rmap' ] = { } for key , value in enums . items ( ) : if type ( value ) is int : enums [ 'rmap' ] [ value ] = key return type ( 'Enum' , ( ) , enums )", "predictions": ["construct an to construct a to an to an to the to make a string"], "references": ["build an enum statement"], "bleu": 0.08225964699966554, "rouge_l": 0.11753371868978806}
{"id": 5117, "code": "def checktypes ( func ) : sig = inspect . signature ( func ) types = { } for param in sig . parameters . values ( ) : param type = param . annotation if param type is param . empty or not inspect . isclass ( param type ) : continue types [ param . name ] = param type if ( param . default is not param . empty and not isinstance ( param . default , param type ) ) : raise Value Error ( \"{func}: wrong type of a default value for {arg!r}\" . format ( func = func . qualname , arg = param . name ) ) def check type ( sig , arg name , arg type , arg value ) : if not isinstance ( arg value , arg type ) : raise Value Error ( \"{func}: wrong type of {arg!r} argument, \" \"{exp!r} expected, got {got!r}\" . format ( func = func . qualname , arg = arg name , exp = arg type . name , got = type ( arg value ) . name ) ) @ functools . wraps ( func ) def wrapper ( * args , * * kwargs ) : ba = sig . bind ( * args , * * kwargs ) for arg name , arg in ba . arguments . items ( ) : try : type = types [ arg name ] except Key Error : continue else : param = sig . parameters [ arg name ] if param . kind == param . VAR POSITIONAL : for value in arg : check type ( sig , arg name , type , value ) elif param . kind == param . VAR KEYWORD : for subname , value in arg . items ( ) : check type ( sig , arg name + ':' + subname , type , value ) else : check type ( sig , arg name , type , arg ) result = func ( * ba . args , * * ba . kwargs ) return type = sig . return annotation if ( return type is not sig . empty and isinstance ( return type , type ) and not isinstance ( result , return type ) ) : raise Value Error ( '{func}: wrong return type, {exp} expected, got {got}' . format ( func = func . qualname , exp = return type . name , got = type ( result ) . name ) ) return result return wrapper", "predictions": ["decorator to convert a to ba ."], "references": ["decorator to verify arguments and return types ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 5118, "code": "def add method ( cls ) : def wrapper ( f ) : #if hasattr(cls, f. name ): setattr ( cls , f . name , f ) return f return wrapper", "predictions": ["decorator to count the class for this class hsig hsig hsig hsig hsig hsig hsig hsig hsig hsig hsig hsig hsig hsig hsig hsig hsig hsig hsig hsig hsig hsig hsig"], "references": ["attach a method to a class ."], "bleu": 0.04317900023606586, "rouge_l": 0.11879259980525803}
{"id": 5119, "code": "def read eol ( self ) -> bool : if self . read eof ( ) : return False self . stream . save context ( ) self . read char ( '\\r' ) if self . read char ( '\\n' ) : return self . stream . validate context ( ) return self . stream . restore context ( )", "predictions": ["count the eol eol = false if there is not already there = false = false = false = false = false = false = false = false = false otherwise"], "references": ["return true if the parser can consume an eol byte sequence ."], "bleu": 0.046398855339878003, "rouge_l": 0.10107705053852528}
{"id": 5120, "code": "def push rule nodes ( self ) -> bool : if self . rule nodes is None : self . rule nodes = collections . Chain Map ( ) self . tag cache = collections . Chain Map ( ) self . id cache = collections . Chain Map ( ) else : self . rule nodes = self . rule nodes . new child ( ) self . tag cache = self . tag cache . new child ( ) self . id cache = self . id cache . new child ( ) return True", "predictions": ["push rule nodes to tag ."], "references": ["push context variable to store rule nodes ."], "bleu": 0.24771976691208875, "rouge_l": 0.5570776255707762}
{"id": 5121, "code": "def pop rule nodes ( self ) -> bool : self . rule nodes = self . rule nodes . parents self . tag cache = self . tag cache . parents self . id cache = self . id cache . parents return True", "predictions": ["pop the rule nodes from the cache ."], "references": ["pop context variable that store rule nodes"], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 5122, "code": "def value ( self , n : Node ) -> str : id n = id ( n ) idcache = self . id cache if id n not in idcache : return \"\" name = idcache [ id n ] tag cache = self . tag cache if name not in tag cache : raise Exception ( \"Incoherent tag cache\" ) tag = tag cache [ name ] k = \"%d:%d\" % ( tag . begin , tag . end ) valcache = self . streams [ - 1 ] . value cache if k not in valcache : valcache [ k ] = str ( tag ) return valcache [ k ]", "predictions": ["return the value of the tag"], "references": ["return the text value of the node"], "bleu": 0.41386440336942737, "rouge_l": 0.7587064676616916}
{"id": 5123, "code": "def begin tag ( self , name : str ) -> Node : self . tag cache [ name ] = Tag ( self . stream , self . stream . index ) return True", "predictions": ["begin the tag tag ."], "references": ["save the current index under the given name ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 5124, "code": "def end tag ( self , name : str ) -> Node : self . tag cache [ name ] . set end ( self . stream . index ) return True", "predictions": ["set the end of the tag ."], "references": ["extract the string between saved and current index ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 5125, "code": "def set rules ( cls , rules : dict ) -> bool : cls . rules = cls . rules . new child ( ) for rule name , rule pt in rules . items ( ) : if '.' not in rule name : rule name = cls . module + '.' + cls . name + '.' + rule name meta . set one ( cls . rules , rule name , rule pt ) return True", "predictions": ["set rules for all rules ."], "references": ["merge internal rules set with the given rules"], "bleu": 0.18822631894109965, "rouge_l": 0.2785388127853881}
{"id": 5126, "code": "def set hooks ( cls , hooks : dict ) -> bool : cls . hooks = cls . hooks . new child ( ) for hook name , hook pt in hooks . items ( ) : if '.' not in hook name : hook name = cls . module + '.' + cls . name + '.' + hook name meta . set one ( cls . hooks , hook name , hook pt ) return True", "predictions": ["set hooks for all hooks ."], "references": ["merge internal hooks set with the given hooks"], "bleu": 0.18822631894109965, "rouge_l": 0.2785388127853881}
{"id": 5127, "code": "def eval rule ( self , name : str ) -> Node : n = Node ( ) id n = id ( n ) self . rule nodes [ ' ' ] = n self . id cache [ id n ] = ' ' if name not in self . class . rules : self . diagnostic . notify ( error . Severity . ERROR , \"Unknown rule : %s\" % name , error . Location Info . from stream ( self . stream , is error = True ) ) raise self . diagnostic self . last Rule = name rule to eval = self . class . rules [ name ] res = rule to eval ( self ) if res : res = self . rule nodes [ ' ' ] return res", "predictions": ["evaluate a rule by name ."], "references": ["evaluate a rule by name ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 5128, "code": "def eval hook ( self , name : str , ctx : list ) -> Node : if name not in self . class . hooks : self . diagnostic . notify ( error . Severity . ERROR , \"Unknown hook : %s\" % name , error . Location Info . from stream ( self . stream , is error = True ) ) raise self . diagnostic self . last Rule = '#' + name res = self . class . hooks [ name ] ( self , * ctx ) if type ( res ) is not bool : raise Type Error ( \"Your hook %r didn't return a bool value\" % name ) return res", "predictions": ["evaluate a hook hook ."], "references": ["evaluate the hook by its name"], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 5129, "code": "def peek text ( self , text : str ) -> bool : start = self . stream . index stop = start + len ( text ) if stop > self . stream . eos index : return False return self . stream [ self . stream . index : stop ] == text", "predictions": ["peek at most recent text ."], "references": ["same as readtext but doesn t consume the stream ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 5130, "code": "def one char ( self ) -> bool : if self . read eof ( ) : return False self . stream . incpos ( ) return True", "predictions": ["returns true if the one char is one of the stream ."], "references": ["read one byte in stream"], "bleu": 0.11498759556447223, "rouge_l": 0.25416666666666665}
{"id": 5131, "code": "def read until eof ( self ) -> bool : if self . read eof ( ) : return True self . stream . save context ( ) while not self . read eof ( ) : self . stream . incpos ( ) return self . stream . validate context ( )", "predictions": ["read until the stream is eof ."], "references": ["consume all the stream . same as eof in bnf ."], "bleu": 0.15685718045401453, "rouge_l": 0.4273204903677758}
{"id": 5132, "code": "def ignore blanks ( self ) -> bool : self . stream . save context ( ) if not self . read eof ( ) and self . stream . peek char in \" \\t\\v\\f\\r\\n\" : while ( not self . read eof ( ) and self . stream . peek char in \" \\t\\v\\f\\r\\n\" ) : self . stream . incpos ( ) return self . stream . validate context ( ) return self . stream . validate context ( )", "predictions": ["check if the stream is blanks ."], "references": ["consume whitespace characters ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 5133, "code": "def internal name ( self ) : unq = 'f ' + super ( ) . internal name ( ) if self . tparams is not None : unq += \" \" + \" \" . join ( self . tparams ) if self . tret is not None : unq += \" \" + self . tret return unq", "predictions": ["return the name of the internal name ."], "references": ["return the unique internal name"], "bleu": 0.2653856085536222, "rouge_l": 0.6421052631578947}
{"id": 5134, "code": "def check struct compatibility ( self , hits ) : for key , in self . cluster hits descr : if key in self . hit fields mapping inverse : mapped key = self . hit fields mapping inverse [ key ] else : mapped key = key if mapped key in [ 'cluster ID' , 'is seed' , 'cluster size' , 'n cluster' ] : continue if key not in hits . dtype . names : raise Type Error ( 'Required hit field \"%s\" not found.' % key ) if self . cluster hits . dtype [ mapped key ] != hits . dtype [ key ] : raise Type Error ( 'The dtype for hit data field \"%s\" does not match. Got/expected: %s/%s.' % ( key , hits . dtype [ key ] , self . cluster hits . dtype [ mapped key ] ) ) additional hit fields = set ( hits . dtype . names ) - set ( [ key for key , val in self . cluster hits descr ] ) if additional hit fields : logging . warning ( 'Found additional hit fields: %s' % \", \" . join ( additional hit fields ) )", "predictions": ["check that all hits are compatibility ."], "references": ["takes the hit array and checks if the important data fields have the same data type than the hit clustered array and that the field names are correct ."], "bleu": 0.009533714952181622, "rouge_l": 0.1501230516817063}
{"id": 5135, "code": "def add mod ( self , seq , mod ) : modstr = self . value ( mod ) if modstr == '~' : seq . parser tree = parsing . Complement ( seq . parser tree ) elif modstr == '!!' : seq . parser tree = parsing . Look Ahead ( seq . parser tree ) elif modstr == '!' : seq . parser tree = parsing . Neg ( seq . parser tree ) elif modstr == '->' : seq . parser tree = parsing . Until ( seq . parser tree ) return True", "predictions": ["add a sequence of sequences to the sequence of the sequences ."], "references": ["create a tree . { complement lookahead neg until }"], "bleu": 0.11498759556447223, "rouge_l": 0.18484848484848485}
{"id": 5136, "code": "def add ruleclause name ( self , ns name , rid ) -> bool : ns name . parser tree = parsing . Rule ( self . value ( rid ) ) return True", "predictions": ["add a ruleclause name to the parser ."], "references": ["create a tree . rule"], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 5137, "code": "def add rules ( self , bnf , r ) -> bool : bnf [ r . rulename ] = r . parser tree return True", "predictions": ["add a bnf rules to the buffer ."], "references": ["attach a parser tree to the dict of rules"], "bleu": 0.2116253761537182, "rouge_l": 0.34923664122137404}
{"id": 5138, "code": "def add rule ( self , rule , rn , alts ) -> bool : rule . rulename = self . value ( rn ) rule . parser tree = alts . parser tree return True", "predictions": ["add a rule to the graph ."], "references": ["add the rule name"], "bleu": 0.22089591134157885, "rouge_l": 0.3824451410658307}
{"id": 5139, "code": "def add sequences ( self , sequences , cla ) -> bool : if not hasattr ( sequences , 'parser tree' ) : sequences . parser tree = cla . parser tree else : oldnode = sequences if isinstance ( oldnode . parser tree , parsing . Seq ) : oldpt = list ( oldnode . parser tree . ptlist ) else : oldpt = [ oldnode . parser tree ] oldpt . append ( cla . parser tree ) sequences . parser tree = parsing . Seq ( * tuple ( oldpt ) ) return True", "predictions": ["add sequences to the first cla ."], "references": ["create a tree . seq"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 5140, "code": "def add alt ( self , alternatives , alt ) -> bool : if not hasattr ( alternatives , 'parser tree' ) : if hasattr ( alt , 'parser tree' ) : alternatives . parser tree = alt . parser tree else : alternatives . parser tree = alt else : oldnode = alternatives if isinstance ( oldnode . parser tree , parsing . Alt ) : oldpt = list ( oldnode . parser tree . ptlist ) else : oldpt = [ oldnode . parser tree ] oldpt . append ( alt . parser tree ) alternatives . parser tree = parsing . Alt ( * tuple ( oldpt ) ) return True", "predictions": ["add a single oldpt to the document ."], "references": ["create a tree . alt"], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 5141, "code": "def add range ( self , sequence , begin , end ) : sequence . parser tree = parsing . Range ( self . value ( begin ) . strip ( \"'\" ) , self . value ( end ) . strip ( \"'\" ) ) return True", "predictions": ["add a range to the end of the sequence"], "references": ["add a read_range primitive"], "bleu": 0.18575057999133596, "rouge_l": 0.3306233062330623}
{"id": 5142, "code": "def add rpt ( self , sequence , mod , pt ) : modstr = self . value ( mod ) if modstr == '!!' : self . stream . restore context ( ) self . diagnostic . notify ( error . Severity . ERROR , \"Cannot repeat a lookahead rule\" , error . Location Info . from stream ( self . stream , is error = True ) ) raise self . diagnostic if modstr == '!' : self . stream . restore context ( ) self . diagnostic . notify ( error . Severity . ERROR , \"Cannot repeat a negated rule\" , error . Location Info . from stream ( self . stream , is error = True ) ) raise self . diagnostic oldnode = sequence sequence . parser tree = pt . functor ( oldnode . parser tree ) return True", "predictions": ["add a lookahead object to the stream ."], "references": ["add a repeater to the previous sequence"], "bleu": 0.2653856085536222, "rouge_l": 0.5398230088495575}
{"id": 5143, "code": "def add capture ( self , sequence , cpt ) : cpt value = self . value ( cpt ) sequence . parser tree = parsing . Capture ( cpt value , sequence . parser tree ) return True", "predictions": ["add a capture to the sequence"], "references": ["create a tree . capture"], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 5144, "code": "def add bind ( self , sequence , cpt ) : cpt value = self . value ( cpt ) sequence . parser tree = parsing . Bind ( cpt value , sequence . parser tree ) return True", "predictions": ["add a sequence to the parser"], "references": ["create a tree . bind"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 5145, "code": "def add hook ( self , sequence , h ) : sequence . parser tree = parsing . Hook ( h . name , h . listparam ) return True", "predictions": ["add a hook to the queue"], "references": ["create a tree . hook"], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 5146, "code": "def param num ( self , param , n ) : param . pair = ( int ( self . value ( n ) ) , int ) return True", "predictions": ["sets the number of n objects to the number of n objects ."], "references": ["parse a int in parameter list"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 5147, "code": "def param str ( self , param , s ) : param . pair = ( self . value ( s ) . strip ( '\"' ) , str ) return True", "predictions": ["get parameter parameter ."], "references": ["parse a str in parameter list"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 5148, "code": "def param char ( self , param , c ) : param . pair = ( self . value ( c ) . strip ( \"'\" ) , str ) return True", "predictions": ["get parameter value of a param ."], "references": ["parse a char in parameter list"], "bleu": 0.20556680845025982, "rouge_l": 0.15601023017902813}
{"id": 5149, "code": "def param id ( self , param , i ) : param . pair = ( self . value ( i ) , parsing . Node ) return True", "predictions": ["get parameter parameter id"], "references": ["parse a node name in parameter list"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 5150, "code": "def hook name ( self , hook , n ) : hook . name = self . value ( n ) hook . listparam = [ ] return True", "predictions": ["return the name of the hook name ."], "references": ["parse a hook name"], "bleu": 0.21105340631872638, "rouge_l": 0.3546511627906977}
{"id": 5151, "code": "def hook param ( self , hook , p ) : hook . listparam . append ( p . pair ) return True", "predictions": ["hook to make sure the pair of the hook was added ."], "references": ["parse a hook parameter"], "bleu": 0.10390302174233558, "rouge_l": 0.13738738738738737}
{"id": 5152, "code": "def add directive2 ( self , sequence , d , s ) : sequence . parser tree = parsing . Directive2 ( d . name , d . listparam , s . parser tree ) return True", "predictions": ["push a sequence to the batch"], "references": ["add a directive in the sequence"], "bleu": 0.2626909894424158, "rouge_l": 0.3333333333333333}
{"id": 5153, "code": "def add directive ( self , sequence , d , s ) : if d . name in meta . directives : the class = meta . directives [ d . name ] sequence . parser tree = parsing . Directive ( the class ( ) , d . listparam , s . parser tree ) elif d . name in meta . decorators : the class = meta . decorators [ d . name ] sequence . parser tree = parsing . Decorator ( the class , d . listparam , s . parser tree ) else : raise Type Error ( \"Unkown directive or decorator %s\" % d . name ) return True", "predictions": ["pop a rule from the sequence"], "references": ["add a directive in the sequence"], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 5154, "code": "def ignore cxx ( self ) -> bool : self . stream . save context ( ) while not self . read eof ( ) : idxref = self . stream . index if self . stream . peek char in \" \\t\\v\\f\\r\\n\" : while ( not self . read eof ( ) and self . stream . peek char in \" \\t\\v\\f\\r\\n\" ) : self . stream . incpos ( ) if self . peek text ( \"//\" ) : while not self . read eof ( ) and not self . peek char ( \"\\n\" ) : self . stream . incpos ( ) if not self . read char ( \"\\n\" ) and self . read eof ( ) : return self . stream . validate context ( ) if self . peek text ( \"/*\" ) : while not self . read eof ( ) and not self . peek text ( \"*/\" ) : self . stream . incpos ( ) if not self . read text ( \"*/\" ) and self . read eof ( ) : return self . stream . restore context ( ) if idxref == self . stream . index : break return self . stream . validate context ( )", "predictions": ["check if the str is cxx or not ."], "references": ["consume comments and whitespace characters ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 5155, "code": "def add state ( self , s : State ) : ids = id ( s ) uid = len ( self . states ) if ids not in self . states : self . states [ ids ] = ( uid , s )", "predictions": ["begin a new tag"], "references": ["all state in the register have a uid"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 5156, "code": "def to dot ( self ) -> str : txt = \"\" txt += \"digraph S%d {\\n\" % id ( self ) if self . label is not None : txt += '\\tlabel=\"%s\";\\n' % ( self . label + '\\l' ) . replace ( '\\n' , '\\l' ) txt += \"\\trankdir=LR;\\n\" #txt += '\\tlabelloc=\"t\";\\n' txt += '\\tgraph [labeljust=l, labelloc=t, nojustify=true];\\n' txt += \"\\tesep=1;\\n\" txt += '\\tranksep=\"equally\";\\n' txt += \"\\tnode [shape = circle];\\n\" txt += \"\\tsplines = ortho;\\n\" for s in self . states . values ( ) : txt += s [ 1 ] . to dot ( ) txt += \"}\\n\" return txt", "predictions": ["return the tag as tag ."], "references": ["provide a . dot representation of all state in the register ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 5157, "code": "def to dot file ( self , fname : str ) : with open ( fname , 'w' ) as f : f . write ( self . to dot ( ) )", "predictions": ["save the current file to a rules file"], "references": ["write a . dot file ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 5158, "code": "def to png file ( self , fname : str ) : cmd = pipes . Template ( ) cmd . append ( 'dot -Tpng > %s' % fname , '-.' ) with cmd . open ( 'pipefile' , 'w' ) as f : f . write ( self . to dot ( ) )", "predictions": ["save the current hooks to a hooks file"], "references": ["write a . png file ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 5159, "code": "def to fmt ( self ) -> str : infos = fmt . end ( \";\\n\" , [ ] ) s = fmt . sep ( ', ' , [ ] ) for ids in sorted ( self . states . keys ( ) ) : s . lsdata . append ( str ( ids ) ) infos . lsdata . append ( fmt . block ( '(' , ')' , [ s ] ) ) infos . lsdata . append ( \"events:\" + repr ( self . events ) ) infos . lsdata . append ( \"named events:\" + repr ( list ( self . named events . keys ( ) ) ) ) infos . lsdata . append ( \"uid events:\" + repr ( list ( self . uid events . keys ( ) ) ) ) return infos", "predictions": ["serialize representation of the named . . . . ."], "references": ["provide a useful representation of the register ."], "bleu": 0.24808415001701817, "rouge_l": 0.4535315985130111}
{"id": 5160, "code": "def nextstate ( self , newstate , treenode = None , user data = None ) : if newstate is None : return self if isinstance ( newstate , State ) and id ( newstate ) != id ( self ) : return newstate elif isinstance ( newstate , State Event ) : self . state register . named events [ newstate . name ] = True return newstate . st elif isinstance ( newstate , State Precond ) : return newstate . st elif isinstance ( newstate , State Hook ) : newstate . call ( treenode , user data ) return newstate . st return self", "predictions": ["not executed after a named or a named"], "references": ["manage transition of state ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5161, "code": "def reset Living State ( self ) : must delete = [ ] l = len ( self . ls ) for idx , ls in zip ( range ( l ) , self . ls ) : ids = id ( ls [ 1 ] . thestate ( ) ) if ids == id ( ls [ 0 ] ) and ( ls [ 1 ] . have finish or not ls [ 1 ] . alive ) : must delete . append ( idx ) elif ls [ 1 ] . alive : ls [ 1 ] . alive = False for delete in reversed ( must delete ) : self . ls . pop ( delete ) self . init all ( )", "predictions": ["peek all text variables for the text index index index index index index index index index index index index index index"], "references": ["only one living state on the s0 of each stateregister"], "bleu": 0.05809665204409193, "rouge_l": 0.06892655367231638}
{"id": 5162, "code": "def infer block ( self , body , diagnostic = None ) : for e in body : e . infer node = Infer Node ( parent = self . infer node ) e . infer type ( diagnostic = diagnostic )", "predictions": ["one char char char stream stream"], "references": ["infer type on block is to type each of is sub - element"], "bleu": 0.0578433294533084, "rouge_l": 0.0}
{"id": 5163, "code": "def infer subexpr ( self , expr , diagnostic = None ) : expr . infer node = Infer Node ( parent = self . infer node ) expr . infer type ( diagnostic = diagnostic )", "predictions": ["read the until statement is a until it is required ."], "references": ["infer type on the subexpr"], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 5164, "code": "def list dataset uris ( cls , base uri , config path ) : uri list = [ ] parse result = generous parse uri ( base uri ) bucket name = parse result . netloc bucket = boto3 . resource ( 's3' ) . Bucket ( bucket name ) for obj in bucket . objects . filter ( Prefix = 'dtool' ) . all ( ) : uuid = obj . key . split ( '-' , 1 ) [ 1 ] uri = cls . generate uri ( None , uuid , base uri ) storage broker = cls ( uri , config path ) if storage broker . has admin metadata ( ) : uri list . append ( uri ) return uri list", "predictions": ["return the list of blanks uris . . . ."], "references": ["return list containing uris with base uri ."], "bleu": 0.15851165692617156, "rouge_l": 0.4535315985130111}
{"id": 5165, "code": "def list overlay names ( self ) : bucket = self . s3resource . Bucket ( self . bucket ) overlay names = [ ] for obj in bucket . objects . filter ( Prefix = self . overlays key prefix ) . all ( ) : overlay file = obj . key . rsplit ( '/' , 1 ) [ - 1 ] overlay name , ext = overlay file . split ( '.' ) overlay names . append ( overlay name ) return overlay names", "predictions": ["internal method to internal internal method to internal use for all name . . . . . ."], "references": ["return list of overlay names ."], "bleu": 0.06809398432036522, "rouge_l": 0.09159159159159158}
{"id": 5166, "code": "def iter item handles ( self ) : bucket = self . s3resource . Bucket ( self . bucket ) for obj in bucket . objects . filter ( Prefix = self . data key prefix ) . all ( ) : relpath = obj . get ( ) [ 'Metadata' ] [ 'handle' ] yield relpath", "predictions": ["iterate over all struct compatibility for the current bucket for the bucket for the bucket for the given struct for the bucket for the bucket for the bucket for the bucket"], "references": ["return iterator over item handles ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 5167, "code": "def list set indent ( lst : list , indent : int = 1 ) : for i in lst : if isinstance ( i , indentable ) : i . set indent ( indent ) if isinstance ( i , list ) : list set indent ( i , indent )", "predictions": ["add indent indent to add indent or ."], "references": ["recurs into list for indentation"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5168, "code": "def list to str ( lst : list , content : str , indent : int = 1 ) : for i in lst : if isinstance ( i , indentable ) : content = i . to str ( content , indent ) elif isinstance ( i , list ) : content = list to str ( i , content , indent ) elif isinstance ( i , str ) : content = catend ( content , i , indent ) return content", "predictions": ["convert add add strings to name"], "references": ["recurs into list for string computing"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 5169, "code": "def populate from sequence ( seq : list , r : ref ( Edge ) , sr : state . State Register ) : base state = r idxlast = len ( seq ) - 1 idx = 0 for m in seq : if isinstance ( m , list ) : for item in m : populate from sequence ( item , r , sr ) elif isinstance ( m , Match Expr ) : e X = r ( ) . get next edge ( m ) if e X is None : s X = None if idx != idxlast : s X = state . State ( sr ) s X . match Default ( base state ( ) . s ) else : s X = base state ( ) . s e X = Edge ( s X ) r ( ) . next edge [ id ( s X ) ] = e X m . attach ( r ( ) . s , s X , sr ) r = ref ( e X ) idx += 1", "predictions": ["add a r r to a r object"], "references": ["function that connect each other one sequence of matchexpr ."], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 5170, "code": "def from string ( bnf : str , entry = None , * optional inherit ) -> Grammar : inherit = [ Grammar ] + list ( optional inherit ) scope = { 'grammar' : bnf , 'entry' : entry } return build grammar ( tuple ( inherit ) , scope )", "predictions": ["build a rule from a rule ."], "references": ["create a grammar from a string"], "bleu": 0.2626909894424158, "rouge_l": 0.4680306905370844}
{"id": 5171, "code": "def from file ( fn : str , entry = None , * optional inherit ) -> Grammar : import os . path if os . path . exists ( fn ) : f = open ( fn , 'r' ) bnf = f . read ( ) f . close ( ) inherit = [ Grammar ] + list ( optional inherit ) scope = { 'grammar' : bnf , 'entry' : entry , 'source' : fn } return build grammar ( tuple ( inherit ) , scope ) raise Exception ( \"File not Found!\" )", "predictions": ["return the function to load the grammar from the given sequences"], "references": ["create a grammar from a file"], "bleu": 0.14991106946711685, "rouge_l": 0.2484725050916497}
{"id": 5172, "code": "def parse ( self , source : str = None , entry : str = None ) -> parsing . Node : self . from string = True if source is not None : self . parsed stream ( source ) if entry is None : entry = self . entry if entry is None : raise Value Error ( \"No entry rule name defined for {}\" . format ( self . class . name ) ) return self . do parse ( entry )", "predictions": ["add a source to the parsing object . ."], "references": ["parse source using the grammar"], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 5173, "code": "def parse file ( self , filename : str , entry : str = None ) -> parsing . Node : self . from string = False import os . path with open ( filename , 'r' ) as f : self . parsed stream ( f . read ( ) , os . path . abspath ( filename ) ) if entry is None : entry = self . entry if entry is None : raise Value Error ( \"No entry rule name defined for {}\" . format ( self . class . name ) ) return self . do parse ( entry )", "predictions": ["add a range to the specified sequence parsing parsing parsing parsing"], "references": ["parse filename using the grammar"], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 5174, "code": "def default serializer ( o ) : defs = ( ( ( datetime . date , datetime . time ) , lambda x : x . isoformat ( ) , ) , ( ( datetime . datetime , ) , lambda x : dt2utc timestamp ( x ) , ) , ) for types , fun in defs : if isinstance ( o , types ) : return fun ( o )", "predictions": ["add an iso - 8601 serializer to a serializer"], "references": ["default serializer for json ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 5175, "code": "def dump ( deposition , from date , with json = True , latest only = False , * * kwargs ) : dep json = json . dumps ( deposition . getstate ( ) , default = default serializer ) dep dict = json . loads ( dep json ) dep dict [ ' p' ] = { } dep dict [ ' p' ] [ 'id' ] = deposition . id dep dict [ ' p' ] [ 'created' ] = dt2utc timestamp ( deposition . created ) dep dict [ ' p' ] [ 'modified' ] = dt2utc timestamp ( deposition . modified ) dep dict [ ' p' ] [ 'user id' ] = deposition . user id dep dict [ ' p' ] [ 'state' ] = deposition . state dep dict [ ' p' ] [ 'has sip' ] = deposition . has sip ( ) dep dict [ ' p' ] [ 'submitted' ] = deposition . submitted return dep dict", "predictions": ["dumps the given sequence to a json string"], "references": ["dump the deposition object as dictionary ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 5176, "code": "def get recids invenio12 ( from date ) : from invenio . dbquery import run sql return ( id [ 0 ] for id in run sql ( 'select id bibrec from ' 'bibrec bibdoc as r join bibdoc as d on r.id bibdoc=d.id ' 'where d.modification date >=%s' , ( from date , ) , run on slave = True ) )", "predictions": ["returns the list of bind invenio12 for the given sequence"], "references": ["get bibdocs for invenio 1 ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 5177, "code": "def get recids invenio2 ( from date ) : from invenio . legacy . dbquery import run sql return ( id [ 0 ] for id in run sql ( 'select id bibrec from ' 'bibrec bibdoc as r join bibdoc as d on r.id bibdoc=d.id ' 'where d.modification date >=%s' , ( from date , ) , run on slave = True ) )", "predictions": ["returns the invenio2 invenio2 for the given sequence of hook ."], "references": ["get bibdocs for invenio 2 ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 5178, "code": "def get check ( ) : try : from invenio . dbquery import run sql except Import Error : from invenio . legacy . dbquery import run sql return ( run sql ( 'select count(id) from bibdoc' , run on slave = True ) [ 0 ] [ 0 ] , [ id [ 0 ] for id in run sql ( 'select id from bibdoc' , run on slave = True ) ] , )", "predictions": ["param num num ."], "references": ["get bibdocs to check ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 5179, "code": "def dump ( obj , from date , with json = True , latest only = False , * * kwargs ) : return dict ( id = obj . id , client id = obj . client id , user id = obj . user id , token type = obj . token type , access token = obj . access token , refresh token = obj . refresh token , expires = dt2iso or empty ( obj . expires ) , scopes = obj . scopes , is personal = obj . is personal , is internal = obj . is internal )", "predictions": ["serialize an object to a dict object . ."], "references": ["dump the oauth2server tokens ."], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 5180, "code": "def get ( * args , * * kwargs ) : try : from invenio . modules . accounts . models import User EXT except Import Error : from invenio accounts . models import User EXT q = User EXT . query return q . count ( ) , q . all ( )", "predictions": ["param param with given self value value value value value value value value value value value value value value value value value value value value value value value value value value"], "references": ["get userext objects ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5181, "code": "def get modified recids invenio12 ( from date ) : from invenio . search engine import search pattern from invenio . dbquery import run sql return set ( ( id [ 0 ] for id in run sql ( 'select id from bibrec where modification date >= %s' , ( from date , ) , run on slave = True ) ) ) , search pattern", "predictions": ["returns all id of the id in the given date"], "references": ["get record ids for invenio 1 ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 5182, "code": "def get modified recids invenio2 ( from date ) : from invenio . legacy . search engine import search pattern from invenio . modules . records . models import Record date = datetime . datetime . strptime ( from date , '%Y-%m-%d %H:%M:%S' ) return set ( ( x [ 0 ] for x in Record . query . filter ( Record . modification date >= date ) . values ( Record . id ) ) ) , search pattern", "predictions": ["return name of all modification in the modification value"], "references": ["get record ids for invenio 2 ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5183, "code": "def get collection restrictions ( collection ) : try : from invenio . dbquery import run sql from invenio . access control firerole import compile role definition except Import Error : from invenio . modules . access . firerole import compile role definition from invenio . legacy . dbquery import run sql res = run sql ( 'SELECT r.firerole def src, email ' 'FROM acc ROLE as r ' 'JOIN acc ROLE acc ACTION acc ARGUMENT ON r.id=id acc ROLE ' 'JOIN acc ARGUMENT AS a ON a.id=id acc ARGUMENT ' 'JOIN user acc ROLE AS u ON r.id=u.id acc ROLE ' 'JOIN user ON user.id=u.id user ' 'WHERE a.keyword=\"collection\" AND ' 'a.value=%s AND ' 'id acc ACTION=(select id from acc ACTION where name=\"viewrestrcoll\")' , ( collection , ) , run on slave = True ) fireroles = set ( ) users = set ( ) for f , u in res : fireroles . add ( compile role definition ( f ) ) users . add ( u ) return { 'fireroles' : list ( fireroles ) , 'users' : users }", "predictions": ["return all restrictions restrictions restrictions"], "references": ["get all restrictions for a given collection users and fireroles ."], "bleu": 0.10822031883953476, "rouge_l": 0.2341650671785029}
{"id": 5184, "code": "def get record revisions ( recid , from date ) : try : from invenio . dbquery import run sql except Import Error : from invenio . legacy . dbquery import run sql return run sql ( 'SELECT job date, marcxml ' 'FROM hst RECORD WHERE id bibrec = %s AND job date >= %s ' 'ORDER BY job date ASC' , ( recid , from date ) , run on slave = True )", "predictions": ["get the record for a job date,"], "references": ["get record revisions ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 5185, "code": "def get record collections ( recid ) : try : from invenio . search engine import ( get all collections of a record , get restricted collections for recid ) except Import Error : from invenio . legacy . search engine import ( get all collections of a record , get restricted collections for recid ) collections = { 'all' : get all collections of a record ( recid , recreate cache if needed = False ) , } collections [ 'restricted' ] = dict ( ( coll , get collection restrictions ( coll ) ) for coll in get restricted collections for recid ( recid , recreate cache if needed = False ) ) return collections", "predictions": ["return all record collections for the given recid ."], "references": ["get all collections the record belong to ."], "bleu": 0.18575057999133596, "rouge_l": 0.4756335282651072}
{"id": 5186, "code": "def dump record json ( marcxml ) : try : from invenio . modules . records . api import Record d = Record . create ( marcxml , 'marc' ) return d . dumps ( clean = True ) except Import Error : from invenio . bibfield import create record d = create record ( marcxml , master format = 'marc' ) return d . dumps ( )", "predictions": ["dump a record json string to a json string"], "references": ["dump json of record ."], "bleu": 0.16784459625186196, "rouge_l": 0.3012345679012346}
{"id": 5187, "code": "def get ( query , from date , * * kwargs ) : recids , search pattern = get modified recids ( from date ) recids = recids . union ( get modified bibdoc recids ( from date ) ) if query : recids = recids . intersection ( set ( search pattern ( p = query . encode ( 'utf-8' ) ) ) ) return len ( recids ) , recids", "predictions": ["get a query from the given date"], "references": ["get recids matching query and with changes ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 5188, "code": "def load common ( model cls , data ) : obj = model cls ( * * data ) db . session . add ( obj ) db . session . commit ( )", "predictions": ["load common common data from db ."], "references": ["helper function for loading json data verbatim into model ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 5189, "code": "def collect things entry points ( ) : things = dict ( ) for entry point in iter entry points ( group = 'invenio migrator.things' ) : things [ entry point . name ] = entry point . load ( ) return things", "predictions": ["return a dictionary of things points from the things ."], "references": ["collect entry points ."], "bleu": 0.13950796967929133, "rouge_l": 0.3096446700507614}
{"id": 5190, "code": "def init app context ( ) : try : from invenio . base . factory import create app app = create app ( ) app . test request context ( '/' ) . push ( ) app . preprocess request ( ) except Import Error : pass", "predictions": ["initialize the app context ."], "references": ["initialize app context for invenio 2 . x ."], "bleu": 0.18343778145675418, "rouge_l": 0.5434298440979956}
{"id": 5191, "code": "def memoize ( func ) : cache = { } @ wraps ( func ) def wrap ( * args , * * kwargs ) : key = '{0}{1}' . format ( args , kwargs ) if key not in cache : cache [ key ] = func ( * args , * * kwargs ) return cache [ key ] return wrap", "predictions": ["cache the function s value ."], "references": ["cache for heavy function calls ."], "bleu": 0.2626909894424158, "rouge_l": 0.5}
{"id": 5192, "code": "def get connected roles ( action id ) : try : from invenio . access control admin import compile role definition except Import Error : from invenio . modules . access . firerole import compile role definition run sql = get run sql ( ) roles = { } res = run sql ( 'select r.id, r.name, r.description, r.firerole def src, ' 'a.keyword, a.value, email from acc ROLE as r ' 'join acc ROLE acc ACTION acc ARGUMENT on r.id=id acc ROLE ' 'join acc ARGUMENT as a on  a.id=id acc ARGUMENT ' 'join user acc ROLE as u on r.id=u.id acc ROLE ' 'join user on user.id=u.id user ' 'where id acc ACTION=%s' , ( action id , ) ) for r in res : role = roles . setdefault ( r [ 0 ] , { 'id' : r [ 0 ] , 'name' : r [ 1 ] , 'description' : r [ 2 ] , 'firerole def' : r [ 3 ] , 'compiled firerole def' : compile role definition ( r [ 3 ] ) , 'users' : set ( ) , 'parameters' : { } } ) param = role [ 'parameters' ] . setdefault ( r [ 4 ] , set ( ) ) param . add ( r [ 5 ] ) role [ 'users' ] . add ( r [ 6 ] ) return six . itervalues ( roles )", "predictions": ["get the connected roles for the action role"], "references": ["get roles connected to an action ."], "bleu": 0.20164945583740668, "rouge_l": 0.4048672566371681}
{"id": 5193, "code": "def get ( query , * args , * * kwargs ) : run sql = get run sql ( ) actions = [ dict ( id = row [ 0 ] , name = row [ 1 ] , allowedkeywords = row [ 2 ] , optional = row [ 3 ] ) for action in query . split ( ',' ) for row in run sql ( 'select id, name, description, allowedkeywords, optional ' 'from acc ACTION where name like %s' , ( action , ) , run on slave = True ) ] return len ( actions ) , actions", "predictions": ["get a single query"], "references": ["get action definitions to dump ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 5194, "code": "def load token ( data ) : from invenio oauth2server . models import Token data [ 'expires' ] = iso2dt or none ( data [ 'expires' ] ) load common ( Token , data )", "predictions": ["load token from data ."], "references": ["load the oauth2server token from data dump ."], "bleu": 0.3086194627209984, "rouge_l": 0.7384987893462469}
{"id": 5195, "code": "def config imp or default ( app , config var imp , default ) : imp = app . config . get ( config var imp ) return import string ( imp ) if imp else default", "predictions": ["return a string for imp or default if not found ."], "references": ["import config var import path or use default value ."], "bleu": 0.1354599427337814, "rouge_l": 0.28818897637795277}
{"id": 5196, "code": "def init app ( self , app ) : self . init config ( app . config ) state = Invenio Migrator State ( app ) app . extensions [ 'invenio-migrator' ] = state app . cli . add command ( dumps ) return state", "predictions": ["flask application initialization ."], "references": ["flask application initialization ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 5197, "code": "def dump ( obj , from date , with json = True , latest only = False , * * kwargs ) : return dict ( name = obj . name , description = obj . description , website = obj . website , user id = obj . user id , client id = obj . client id , client secret = obj . client secret , is confidential = obj . is confidential , is internal = obj . is internal , redirect uris = obj . redirect uris , default scopes = obj . default scopes )", "predictions": ["serialize an object to a dict ."], "references": ["dump the oauth2server client ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 5198, "code": "def get users invenio12 ( * args , * * kwargs ) : from invenio . dbquery import run sql , deserialize via marshal User = namedtuple ( 'User' , [ 'id' , 'email' , 'password' , 'password salt' , 'note' , 'full name' , 'settings' , 'nickname' , 'last login' ] ) users = run sql ( 'SELECT id, email, password, note, settings, nickname, last login' ' FROM user' , run on slave = True ) return len ( users ) , [ User ( id = user [ 0 ] , email = user [ 1 ] , password = user [ 2 ] . decode ( 'latin1' ) , password salt = user [ 1 ] , note = user [ 3 ] , full name = user [ 5 ] , settings = deserialize via marshal ( user [ 4 ] ) if user [ 4 ] else { } , nickname = 'id {0}' . format ( user [ 0 ] ) , last login = user [ 6 ] ) for user in users ]", "predictions": ["get all users from invenio12"], "references": ["get user accounts invenio 1 ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 5199, "code": "def get users invenio2 ( * args , * * kwargs ) : from invenio . modules . accounts . models import User q = User . query return q . count ( ) , q . all ( )", "predictions": ["get all users in user account ."], "references": ["get user accounts from invenio 2 ."], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 5200, "code": "def loadrecords ( sources , source type , recid ) : if recid is not None : for source in sources : records = json . load ( source ) for item in records : if str ( item [ 'recid' ] ) == str ( recid ) : loadrecord ( item , source type , eager = True ) click . echo ( \"Record '{recid}' loaded.\" . format ( recid = recid ) ) return click . echo ( \"Record '{recid}' not found.\" . format ( recid = recid ) ) else : for idx , source in enumerate ( sources , 1 ) : click . echo ( 'Loading dump {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) data = json . load ( source ) with click . progressbar ( data ) as records : for item in records : loadrecord ( item , source type )", "predictions": ["dump the contents of a json file ."], "references": ["load records migration dump ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 5201, "code": "def inspectrecords ( sources , recid , entity = None ) : for idx , source in enumerate ( sources , 1 ) : click . echo ( 'Loading dump {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) data = json . load ( source ) if not recid : click . secho ( 'Record identifiers' , fg = 'green' ) total = 0 for r in ( d [ 'recid' ] for d in data ) : click . echo ( r ) total += 1 click . echo ( '{0} records found in dump.' . format ( total ) ) return data = list ( filter ( lambda d : d [ 'recid' ] == recid , data ) ) if not data : click . secho ( \"Record not found.\" , fg = 'yellow' ) return for record in data : if entity is None : click . echo ( json . dumps ( record , indent = 2 ) ) if entity == 'files' : click . secho ( 'Files' , fg = 'green' ) click . echo ( json . dumps ( record [ 'files' ] , indent = 2 ) ) if entity == 'json' : click . secho ( 'Records (JSON)' , fg = 'green' ) for revision in record [ 'record' ] : click . secho ( 'Revision {0}' . format ( revision [ 'modification datetime' ] ) , fg = 'yellow' ) click . echo ( json . dumps ( revision [ 'json' ] , indent = 2 ) ) if entity == 'marcxml' : click . secho ( 'Records (MARCXML)' , fg = 'green' ) for revision in record [ 'record' ] : click . secho ( 'Revision {0}' . format ( revision [ 'marcxml' ] ) , fg = 'yellow' ) click . echo ( revision )", "predictions": ["dump all the data in a json file"], "references": ["inspect records in a migration dump ."], "bleu": 0.22679164443904004, "rouge_l": 0.26991150442477874}
{"id": 5202, "code": "def main ( port = 8888 ) : import tornado . ioloop routes = [ ] + Tornado Profiler ( ) . get routes ( ) app = tornado . web . Application ( routes ) app . listen ( port ) tornado . ioloop . IO Loop . current ( ) . start ( )", "predictions": ["main part of the script"], "references": ["run as sample test server ."], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 5203, "code": "def get ( self ) : sort = self . get argument ( 'sort' , 'cum time' ) count = self . get argument ( 'count' , 20 ) strip dirs = self . get argument ( 'strip dirs' , True ) error = '' sorts = ( 'num calls' , 'cum time' , 'total time' , 'cum time per call' , 'total time per call' ) if sort not in sorts : error += \"Invalid `sort` '%s', must be in %s.\" % ( sort , sorts ) try : count = int ( count ) except ( Value Error , Type Error ) : error += \"Can't cast `count` '%s' to int.\" % count if count <= 0 : count = None strip dirs = str ( strip dirs ) . lower ( ) not in ( 'false' , 'no' , 'none' , 'null' , '0' , '' ) if error : self . write ( { 'error' : error } ) self . set status ( 400 ) self . finish ( ) return try : statistics = get profiler statistics ( sort , count , strip dirs ) self . write ( { 'statistics' : statistics } ) self . set status ( 200 ) except Type Error : logger . exception ( 'Error while retrieving profiler statistics' ) self . write ( { 'error' : 'No stats available. Start and stop the profiler before trying to retrieve stats.' } ) self . set status ( 404 ) self . finish ( )", "predictions": ["get the statistics for the profiler ."], "references": ["return current profiler statistics ."], "bleu": 0.22089591134157885, "rouge_l": 0.34366197183098596}
{"id": 5204, "code": "def post ( self ) : if is profiler running ( ) : self . set status ( 201 ) self . finish ( ) return start profiling ( ) self . set status ( 201 ) self . finish ( )", "predictions": ["return a list of post objects ."], "references": ["start a new profiler ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 5205, "code": "def post ( self ) : filename = self . get argument ( 'filename' , 'dump.prof' ) C Profile Wrapper . profiler . dump stats ( filename ) self . finish ( )", "predictions": ["post the profiler to the profiler ."], "references": ["dump current profiler statistics into a file ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 5206, "code": "def get ( self ) : C Profile Wrapper . profiler . print stats ( ) s = String IO . String IO ( ) sortby = 'cumulative' ps = pstats . Stats ( C Profile Wrapper . profiler , stream = s ) . sort stats ( sortby ) ps . print stats ( ) self . set status ( 200 ) self . write ( s . getvalue ( ) ) self . finish ( )", "predictions": ["get the contents of the screen ."], "references": ["return current profiler statistics ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 5207, "code": "def delete ( self ) : C Profile Wrapper . profiler . create stats ( ) self . enable ( ) self . set status ( 204 ) self . finish ( )", "predictions": ["delete this profiler ."], "references": ["clear profiler statistics ."], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 5208, "code": "def post ( self ) : C Profile Wrapper . profiler = c Profile . Profile ( ) C Profile Wrapper . profiler . enable ( ) self . running = True self . set status ( 201 ) self . finish ( )", "predictions": ["post the profiler to the server ."], "references": ["start a new profiler ."], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 5209, "code": "def delete ( self ) : C Profile Wrapper . profiler . disable ( ) self . running = False self . set status ( 204 ) self . finish ( )", "predictions": ["delete this widget ."], "references": ["stop the profiler ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 5210, "code": "def get ( self ) : self . write ( { \"running\" : self . running } ) self . set status ( 200 ) self . finish ( )", "predictions": ["get the current user s status ."], "references": ["check if the profiler is running ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 5211, "code": "def disable timestamp ( method ) : @ wraps ( method ) def wrapper ( * args , * * kwargs ) : result = None with correct date ( ) : result = method ( * args , * * kwargs ) return result return wrapper", "predictions": ["decorator for caching a method from a method"], "references": ["disable timestamp update per method ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 5212, "code": "def add ones dim ( arr ) : arr = arr [ ... , np . newaxis ] return np . concatenate ( ( arr , np . ones like ( arr ) ) , axis = - 1 )", "predictions": ["add ones to the array of arr ."], "references": ["adds a dimensions with ones to array ."], "bleu": 0.239802967618271, "rouge_l": 0.5}
{"id": 5213, "code": "def create ( cls , dump ) : if not dump . data . get ( 'record' ) : try : Persistent Identifier . get ( pid type = 'recid' , pid value = dump . recid ) except PID Does Not Exist Error : Persistent Identifier . create ( 'recid' , dump . recid , status = PID Status . RESERVED ) db . session . commit ( ) return None dump . prepare revisions ( ) dump . prepare pids ( ) dump . prepare files ( ) existing files = [ ] if dump . record : existing files = dump . record . get ( ' files' , [ ] ) record = cls . update record ( revisions = dump . revisions , created = dump . created , record = dump . record ) pids = dump . missing pids else : record = cls . create record ( dump ) pids = dump . pids if pids : cls . create pids ( record . id , pids ) if dump . files : cls . create files ( record , dump . files , existing files ) if dump . is deleted ( record ) : cls . delete record ( record ) return record", "predictions": ["create a new record ."], "references": ["create record based on dump ."], "bleu": 0.2658156069371863, "rouge_l": 0.5366568914956013}
{"id": 5214, "code": "def create record ( cls , dump ) : timestamp , data = dump . latest record = Record . create ( data ) record . model . created = dump . created . replace ( tzinfo = None ) record . model . updated = timestamp . replace ( tzinfo = None ) Record Identifier . insert ( dump . recid ) Persistent Identifier . create ( pid type = 'recid' , pid value = str ( dump . recid ) , object type = 'rec' , object uuid = str ( record . id ) , status = PID Status . REGISTERED ) db . session . commit ( ) return cls . update record ( revisions = dump . rest , record = record , created = dump . created )", "predictions": ["create a record in a new record ."], "references": ["create a new record from dump ."], "bleu": 0.3549481056010052, "rouge_l": 0.6747787610619468}
{"id": 5215, "code": "def update record ( cls , revisions , created , record ) : for timestamp , revision in revisions : record . model . json = revision record . model . created = created . replace ( tzinfo = None ) record . model . updated = timestamp . replace ( tzinfo = None ) db . session . commit ( ) return Record ( record . model . json , model = record . model )", "predictions": ["update a record in the database ."], "references": ["update an existing record ."], "bleu": 0.22089591134157885, "rouge_l": 0.5154929577464789}
{"id": 5216, "code": "def create pids ( cls , record uuid , pids ) : for p in pids : Persistent Identifier . create ( pid type = p . pid type , pid value = p . pid value , pid provider = p . provider . pid provider if p . provider else None , object type = 'rec' , object uuid = record uuid , status = PID Status . REGISTERED , ) db . session . commit ( )", "predictions": ["get record for record import"], "references": ["create persistent identifiers ."], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 5217, "code": "def delete record ( cls , record ) : record . delete ( ) Persistent Identifier . query . filter by ( object type = 'rec' , object uuid = record . id , ) . update ( { Persistent Identifier . status : PID Status . DELETED } ) cls . delete buckets ( record ) db . session . commit ( )", "predictions": ["get record for a record try to change the record try to the collection try to the record try to the record try to get ."], "references": ["delete a record and it s persistent identifiers ."], "bleu": 0.0660161823828377, "rouge_l": 0.1878850102669405}
{"id": 5218, "code": "def create file ( self , bucket , key , file versions ) : objs = [ ] for file ver in file versions : f = File Instance . create ( ) . set uri ( file ver [ 'full path' ] , file ver [ 'size' ] , 'md5:{0}' . format ( file ver [ 'checksum' ] ) , ) obj = Object Version . create ( bucket , key ) . set file ( f ) obj . created = arrow . get ( file ver [ 'creation date' ] ) . datetime . replace ( tzinfo = None ) objs . append ( obj ) db . session . commit ( ) return objs [ - 1 ]", "predictions": ["dump a record record to a record . ."], "references": ["create a single file with all versions ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 5219, "code": "def delete buckets ( cls , record ) : files = record . get ( ' files' , [ ] ) buckets = set ( ) for f in files : buckets . add ( f . get ( 'bucket' ) ) for b id in buckets : b = Bucket . get ( b id ) b . deleted = True", "predictions": ["get buckets from date . . ."], "references": ["delete the bucket ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 5220, "code": "def missing pids ( self ) : missing = [ ] for p in self . pids : try : Persistent Identifier . get ( p . pid type , p . pid value ) except PID Does Not Exist Error : missing . append ( p ) return missing", "predictions": ["list of load common common pid db db db db db db db db db db db db db db db db db db db db db db db db db"], "references": ["filter persistent identifiers ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5221, "code": "def prepare files ( self ) : files = { } for f in self . data [ 'files' ] : k = f [ 'full name' ] if k not in files : files [ k ] = [ ] files [ k ] . append ( f ) for k in files . keys ( ) : files [ k ] . sort ( key = lambda x : x [ 'version' ] ) self . files = files", "predictions": ["collect things from the group group"], "references": ["get files from data dump ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 5222, "code": "def prepare pids ( self ) : self . pids = [ ] for fetcher in self . pid fetchers : val = fetcher ( None , self . revisions [ - 1 ] [ 1 ] ) if val : self . pids . append ( val )", "predictions": ["init the app s app to a list of app objects try to be used for each create a list of app try to set of app try to init init"], "references": ["prepare persistent identifiers ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5223, "code": "def is deleted ( self , record = None ) : record = record or self . revisions [ - 1 ] [ 1 ] return any ( col == 'deleted' for col in record . get ( 'collections' , [ ] ) )", "predictions": ["return are deleted in the cache"], "references": ["check if record is deleted ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 5224, "code": "def dump ( thing , query , from date , file prefix , chunk size , limit , thing flags ) : init app context ( ) file prefix = file prefix if file prefix else '{0} dump' . format ( thing ) kwargs = dict ( ( f . strip ( '-' ) . replace ( '-' , ' ' ) , True ) for f in thing flags ) try : thing func = collect things entry points ( ) [ thing ] except Key Error : click . Abort ( '{0} is not in the list of available things to migrate: ' '{1}' . format ( thing , collect things entry points ( ) ) ) click . echo ( \"Querying {0}...\" . format ( thing ) ) count , items = thing func . get ( query , from date , limit = limit , * * kwargs ) progress i = 0 click . echo ( \"Dumping {0}...\" . format ( thing ) ) with click . progressbar ( length = count ) as bar : for i , chunk ids in enumerate ( grouper ( items , chunk size ) ) : with open ( '{0} {1}.json' . format ( file prefix , i ) , 'w' ) as fp : fp . write ( \"[\\n\" ) for id in chunk ids : try : json . dump ( thing func . dump ( id , from date , * * kwargs ) , fp , default = set serializer ) fp . write ( \",\" ) except Exception as e : click . secho ( \"Failed dump {0} {1} ({2})\" . format ( thing , id , e . message ) , fg = 'red' ) progress i += 1 bar . update ( progress i ) fp . seek ( fp . tell ( ) - 1 ) fp . write ( \"\\n]\" )", "predictions": ["get the action acc to a file ."], "references": ["dump data from invenio legacy ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 5225, "code": "def check ( thing ) : init app context ( ) try : thing func = collect things entry points ( ) [ thing ] except Key Error : click . Abort ( '{0} is not in the list of available things to migrate: ' '{1}' . format ( thing , collect things entry points ( ) ) ) click . echo ( \"Querying {0}...\" . format ( thing ) ) count , items = thing func . get check ( ) i = 0 click . echo ( \"Checking {0}...\" . format ( thing ) ) with click . progressbar ( length = count ) as bar : for id in items : thing func . check ( id ) i += 1 bar . update ( i )", "predictions": ["get the query dict for the query actions . . ."], "references": ["check data in invenio legacy ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 5226, "code": "def write reports ( self , relative path , suite name , reports , package name = None ) : dest path = self . reserve file ( relative path ) with open ( dest path , 'wb' ) as outf : outf . write ( toxml ( reports , suite name , package name = package name ) ) return dest path", "predictions": ["load the invenio invenio token from the specified from the specified from the specified from the specified from the specified from the specified from the specified from the specified from the"], "references": ["write the collection of reports to the given path"], "bleu": 0.04317900023606586, "rouge_l": 0.11101000909918107}
{"id": 5227, "code": "def toxml ( test reports , suite name , hostname = gethostname ( ) , package name = \"tests\" ) : testsuites = et . Element ( \"testsuites\" ) testsuite = et . Sub Element ( testsuites , \"testsuite\" ) test count = len ( test reports ) if test count < 1 : raise Value Error ( 'there must be at least one test report' ) assert test count > 0 , 'expecting at least one test' error count = len ( [ r for r in test reports if r . errors ] ) failure count = len ( [ r for r in test reports if r . failures ] ) ts = test reports [ 0 ] . start ts start timestamp = datetime . fromtimestamp ( ts ) . isoformat ( ) total duration = test reports [ - 1 ] . end ts - test reports [ 0 ] . start ts def quote attribute ( value ) : return value if value is not None else \"(null)\" testsuite . attrib = dict ( id = \"0\" , errors = str ( error count ) , failures = str ( failure count ) , tests = str ( test count ) , hostname = quote attribute ( hostname ) , timestamp = quote attribute ( start timestamp ) , time = \"%f\" % total duration , name = quote attribute ( suite name ) , package = quote attribute ( package name ) , ) for r in test reports : test name = r . name test duration = r . end ts - r . start ts class name = r . src location testcase = et . Sub Element ( testsuite , \"testcase\" ) testcase . attrib = dict ( name = test name , classname = quote attribute ( class name ) , time = \"%f\" % test duration , ) if r . errors or r . failures : if r . failures : failure = et . Sub Element ( testcase , \"failure\" ) failure . attrib = dict ( type = \"exception\" , message = quote attribute ( '\\n' . join ( [ '%s' % e for e in r . failures ] ) ) , ) else : error = et . Sub Element ( testcase , \"error\" ) error . attrib = dict ( type = \"exception\" , message = quote attribute ( '\\n' . join ( [ '%s' % e for e in r . errors ] ) ) , ) return et . tostring ( testsuites , encoding = \"utf-8\" )", "predictions": ["create a or update a or or a or or a string from a or or list of app"], "references": ["convert test reports into an xml file"], "bleu": 0.05415315253510895, "rouge_l": 0.0}
{"id": 5228, "code": "def add Menu ( self , menu ) : self . menus [ menu . name ] = menu self . peng . send Event ( \"peng3d:window.menu.add\" , { \"peng\" : self . peng , \"window\" : self , \"menu\" : menu } )", "predictions": ["init a menu to the menu"], "references": ["adds a menu to the list of menus ."], "bleu": 0.37288786399304175, "rouge_l": 0.5147679324894514}
{"id": 5229, "code": "def redraw label ( self ) : sx , sy = self . size x , y = self . pos self . label . anchor x = \"left\" self . label . x = x + sx / 2. + sx self . label . y = y + sy / 2. + sy * .15 self . label . update ( )", "predictions": ["dump the label label to the label"], "references": ["re - calculates the position of the label ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 5230, "code": "def render3d ( self , view = None ) : super ( Static World , self ) . render3d ( view ) self . batch3d . draw ( )", "predictions": ["override the default method to import the content of the batch3d"], "references": ["renders the world ."], "bleu": 0.11390778025531027, "rouge_l": 0.14558472553699284}
{"id": 5231, "code": "def on redraw ( self ) : x , y = self . pos sx , sy = self . size self . bg vlist . vertices = [ x , y , x + sx , y , x + sx , y + sy , x , y + sy ] self . stencil vlist . vertices = [ x , y , x + sx , y , x + sx , y + sy , x , y + sy ] if isinstance ( self . bg , Background ) : if not self . bg . initialized : self . bg . init bg ( ) self . bg . initialized = True self . bg . redraw bg ( )", "predictions": ["users return position of scroll bar"], "references": ["redraws the background and any child widgets ."], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 5232, "code": "def do Action ( self , action ) : if not hasattr ( self , \"actions\" ) : return for f , args , kwargs in self . actions . get ( action , [ ] ) : f ( * args , * * kwargs )", "predictions": ["item the load load actions ."], "references": ["helper method that calls all callbacks registered for the given action ."], "bleu": 0.08993236413460196, "rouge_l": 0.20962199312714777}
{"id": 5233, "code": "def get Size ( self ) : return self . widget . size [ 0 ] - self . border [ 0 ] * 2 , self . widget . size [ 1 ] - self . border [ 1 ] * 2", "predictions": ["returns the size of the widget widget"], "references": ["returns the size of the layer with the border size already subtracted ."], "bleu": 0.28873264657905134, "rouge_l": 0.474339035769829}
{"id": 5234, "code": "def make conn ( shape ) : shape = np . array ( shape ) Ne = shape . prod ( ) if len ( shape ) == 2 : nx , ny = np . array ( shape ) + 1 conn = np . zeros ( ( Ne , 4 ) , dtype = np . int32 ) counter = 0 pattern = np . array ( [ 0 , 1 , 1 + nx , nx ] ) for j in range ( shape [ 1 ] ) : for i in range ( shape [ 0 ] ) : conn [ counter ] = pattern + 1 + i + j * nx counter += 1 if len ( shape ) == 3 : nx , ny , nz = np . array ( shape ) + 1 conn = np . zeros ( ( Ne , 8 ) , dtype = np . int32 ) counter = 0 pattern = np . array ( [ 0 , 1 , 1 + nx , nx , nx * ny , 1 + nx * ny , 1 + ( nx + 1 ) * ny , ( nx + 1 ) * ny ] ) for k in range ( shape [ 2 ] ) : for j in range ( shape [ 1 ] ) : for i in range ( shape [ 0 ] ) : conn [ counter ] = pattern + 1 + i + j * nx + k * nx * ny counter += 1 return conn", "predictions": ["create a conn object from a = = 0 tornado tornado tornado tornado tornado"], "references": ["connectivity builder using numba for speed boost ."], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 5235, "code": "def set fields ( self , fields = None , * * kwargs ) : self . fields = [ ] if fields != None : for field in fields : self . fields . append ( field )", "predictions": ["get fields fields object"], "references": ["sets the fields ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 5236, "code": "def add fields ( self , fields = None , * * kwargs ) : if fields != None : for field in fields : self . fields . append ( field )", "predictions": ["post fields to the model"], "references": ["add the fields into the list of fields ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 5237, "code": "def check elements ( self ) : existing types = set ( self . elements . type . argiope . values . flatten ( ) ) allowed types = set ( ELEMENTS . keys ( ) ) if ( existing types <= allowed types ) == False : raise Value Error ( \"Element types {0} not in know elements {1}\" . format ( existing types - allowed types , allowed types ) ) print ( \"<Elements: OK>\" )", "predictions": ["post - specific post ."], "references": ["checks element definitions ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 5238, "code": "def space ( self ) : return self . elements . type . argiope . map ( lambda t : ELEMENTS [ t ] . space )", "predictions": ["the . get the . get the . ."], "references": ["returns the dimension of the embedded space of each element ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 5239, "code": "def centroids and volumes ( self , sort index = True ) : elements = self . elements out = [ ] for etype , group in self . elements . groupby ( [ ( \"type\" , \"argiope\" , \"\" ) ] ) : etype info = ELEMENTS [ etype ] simplices info = etype info . simplices index = group . index simplices data = self . split ( into = \"simplices\" , loc = index , at = \"coords\" ) simplices = simplices data . values . reshape ( index . size , simplices info . shape [ 0 ] , simplices info . shape [ 1 ] , 3 ) edges = simplices [ : , : , 1 : ] - simplices [ : , : , : 1 ] simplices centroids = simplices . mean ( axis = 2 ) if etype info . space == 2 : simplices volumes = np . linalg . norm ( np . cross ( edges [ : , : , 0 ] , edges [ : , : , 1 ] , axis = 2 ) , axis = 2 ) / 2. elif etype info . space == 3 : simplices volumes = ( np . cross ( edges [ : , : , 0 ] , edges [ : , : , 1 ] , axis = 2 ) * edges [ : , : , 2 ] ) . sum ( axis = 2 ) / 6. elements volumes = simplices volumes . sum ( axis = 1 ) elements centroids = ( ( simplices volumes . reshape ( * simplices volumes . shape , 1 ) * simplices centroids ) . sum ( axis = 1 ) / elements volumes . reshape ( * elements volumes . shape , 1 ) ) volumes df = pd . Data Frame ( index = index , data = elements volumes , columns = pd . Multi Index . from product ( [ [ \"volume\" ] , [ \"\" ] ] ) ) centroids df = pd . Data Frame ( index = index , data = elements centroids , columns = pd . Multi Index . from product ( [ [ \"centroid\" ] , [ \"x\" , \"y\" , \"z\" ] ] ) ) out . append ( pd . concat ( [ volumes df , centroids df ] , axis = 1 ) ) out = pd . concat ( out ) if sort index : out . sort index ( inplace = True ) return out . sort index ( axis = 1 )", "predictions": ["return and and and"], "references": ["returns a dataframe containing volume and centroids of all the elements ."], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 5240, "code": "def edges ( self , zfill = 3 ) : edges = self . split ( \"edges\" , at = \"coords\" ) . unstack ( ) edges [ \"lx\" ] = edges . x [ 1 ] - edges . x [ 0 ] edges [ \"ly\" ] = edges . y [ 1 ] - edges . y [ 0 ] edges [ \"lz\" ] = edges . z [ 1 ] - edges . z [ 0 ] edges [ \"l\" ] = np . linalg . norm ( edges [ [ \"lx\" , \"ly\" , \"lz\" ] ] , axis = 1 ) edges = ( edges . l ) . unstack ( ) edges . columns = pd . Multi Index . from product ( [ [ \"length\" ] , [ \"e\" + \"{0}\" . format ( s ) . zfill ( zfill ) for s in np . arange ( edges . shape [ 1 ] ) ] ] ) edges [ ( \"stats\" , \"lmax\" ) ] = edges . length . max ( axis = 1 ) edges [ ( \"stats\" , \"lmin\" ) ] = edges . length . min ( axis = 1 ) edges [ ( \"stats\" , \"aspect ratio\" ) ] = edges . stats . lmax / edges . stats . lmin return edges . sort index ( axis = 1 )", "predictions": ["get the post - index post - center post - ramp"], "references": ["returns the aspect ratio of all elements ."], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 5241, "code": "def stats ( self ) : cv = self . centroids and volumes ( ) angles = self . angles ( ) edges = self . edges ( ) return pd . concat ( [ cv , angles [ [ \"stats\" ] ] , edges [ [ \"stats\" ] ] ] , axis = 1 ) . sort index ( axis = 1 )", "predictions": ["return delete of the = ."], "references": ["returns mesh quality and geometric stats ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5242, "code": "def element set to node set ( self , tag ) : nodes , elements = self . nodes , self . elements loc = ( elements . conn [ elements [ ( \"sets\" , tag , \"\" ) ] ] . stack ( ) . stack ( ) . unique ( ) ) loc = loc [ loc != 0 ] nodes [ ( \"sets\" , tag ) ] = False nodes . loc [ loc , ( \"sets\" , tag ) ] = True", "predictions": ["set self 200 self 200"], "references": ["makes a node set from an element set ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 5243, "code": "def node set to surface ( self , tag ) : nodes = self . nodes . copy ( ) dummy = nodes . iloc [ 0 ] . copy ( ) dummy [ \"coords\" ] *= np . nan dummy [ \"sets\" ] = True nodes . loc [ 0 ] = dummy element surfaces = self . split ( \"surfaces\" ) . unstack ( ) surf = pd . Data Frame ( nodes . sets [ tag ] . loc [ element surfaces . values . flatten ( ) ] . values . reshape ( element surfaces . shape ) . prod ( axis = 1 ) . astype ( np . bool ) , index = element surfaces . index ) . unstack ( ) . fillna ( False ) for k in surf . keys ( ) : self . elements [ \"surfaces\" , tag , \"f{0}\" . format ( k [ 1 ] + 1 ) ] = surf . loc [ : , k ]", "predictions": ["timestamp to method for writing the disable disable"], "references": ["converts a node set to surface ."], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 5244, "code": "def surface to element sets ( self , tag ) : surface = self . elements . surfaces [ tag ] for findex in surface . keys ( ) : if surface [ findex ] . sum ( ) != 0 : self . elements [ ( \"sets\" , \" SURF {0} FACE{1}\" . format ( tag , findex [ 1 : ] ) , \"\" ) ] = surface [ findex ]", "predictions": ["add dim sets sets to the dim"], "references": ["creates elements sets corresponding to a surface ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 5245, "code": "def fields metadata ( self ) : return ( pd . concat ( [ f . metadata ( ) for f in self . fields ] , axis = 1 ) . transpose ( ) . sort values ( [ \"step num\" , \"frame\" , \"label\" , \"position\" ] ) )", "predictions": ["return create the metadata create the metadata create a if any ."], "references": ["returns fields metadata as a dataframe ."], "bleu": 0.1235622127262679, "rouge_l": 0.3315217391304348}
{"id": 5246, "code": "def metadata ( self ) : return pd . Series ( { \"part\" : self . part , \"step num\" : self . step num , \"step label\" : self . step label , \"frame\" : self . frame , \"frame value\" : self . frame value , \"label\" : self . label , \"position\" : self . position , } )", "predictions": ["dump of this created ."], "references": ["returns metadata as a dataframe ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 5247, "code": "def make directories ( self ) : if os . path . isdir ( self . workdir ) == False : os . mkdir ( self . workdir )", "predictions": ["update the record directory ."], "references": ["checks if required directories exist and creates them if needed ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 5248, "code": "def run postproc ( self ) : t0 = time . time ( ) if self . verbose : print ( '####\u00a0POST-PROCESSING \"{0}\" USING POST-PROCESSOR \"{1}\"'. f ormat( s elf. l abel,   self . solver . upper ( ) ) ) if self . solver == \"abaqus\" : command = '{0} viewer no GUI={1} abqpp.py' . format ( self . solver path , self . label ) process = subprocess . Popen ( command , cwd = self . workdir , shell = True , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) for line in iter ( process . stdout . readline , b'' ) : line = line . rstrip ( ) . decode ( 'utf8' ) print ( \"    \" , line ) t1 = time . time ( ) if self . verbose : print ( '  => POST-PROCESSED {0}: DURATION = {1:.2f}s >' . format ( self . label , t1 - t0 ) )", "predictions": ["run the postproc viewer ."], "references": ["runs the post - proc script ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 5249, "code": "def run gmsh ( self ) : argiope . utils . run gmsh ( gmsh path = self . gmsh path , gmsh space = self . gmsh space , gmsh options = self . gmsh options , name = self . file name + \".geo\" , workdir = self . workdir ) self . mesh = argiope . mesh . read msh ( self . workdir + self . file name + \".msh\" )", "predictions": ["run gmsh gmsh gmsh"], "references": ["makes the mesh using gmsh ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 5250, "code": "def read history report ( path , steps , x name = None ) : data = pd . read csv ( path , delim whitespace = True ) if x name != None : data [ x name ] = data . X del data [ \"X\" ] data [ \"step\" ] = 0 t = 0. for i in range ( len ( steps ) ) : dt = steps [ i ] . duration loc = data [ data . t == t ] . index if len ( loc ) == 2 : data . loc [ loc [ 1 ] : , \"step\" ] = i t += dt return data", "predictions": ["read a history report from a file"], "references": ["reads an history output report ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 5251, "code": "def read field report ( path , data flag = \"*DATA\" , meta data flag = \"*METADATA\" ) : text = open ( path ) . read ( ) mdpos = text . find ( meta data flag ) dpos = text . find ( data flag ) mdata = io . String IO ( \"\\n\" . join ( text [ mdpos : dpos ] . split ( \"\\n\" ) [ 1 : ] ) ) data = io . String IO ( \"\\n\" . join ( text [ dpos : ] . split ( \"\\n\" ) [ 1 : ] ) ) data = pd . read csv ( data , index col = 0 ) data = data . groupby ( data . index ) . mean ( ) mdata = pd . read csv ( mdata , sep = \"=\" , header = None , index col = 0 ) [ 1 ] mdata = mdata . to dict ( ) out = { } out [ \"step num\" ] = int ( mdata [ \"step num\" ] ) out [ \"step label\" ] = mdata [ \"step label\" ] out [ \"frame\" ] = int ( mdata [ \"frame\" ] ) out [ \"frame value\" ] = float ( mdata [ \"frame value\" ] ) out [ \"part\" ] = mdata [ \"instance\" ] position map = { \"NODAL\" : \"node\" , \"ELEMENT CENTROID\" : \"element\" , \"WHOLE ELEMENT\" : \"element\" } out [ \"position\" ] = position map [ mdata [ \"position\" ] ] out [ \"label\" ] = mdata [ \"label\" ] out [ \"data\" ] = data field class = getattr ( argiope . mesh , mdata [ \"argiope class\" ] ) return field class ( * * out )", "predictions": ["read the report report from the given path"], "references": ["reads a field output report ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 5252, "code": "def list to string ( l = range ( 200 ) , width = 40 , indent = \"  \" ) : l = [ str ( v ) + \",\" for v in l ] counter = 0 out = \"\" + indent for w in l : s = len ( w ) if counter + s > width : out += \"\\n\" + indent counter = 0 out += w counter += s return out . strip ( \",\" )", "predictions": ["convert a list of strings to a string ."], "references": ["converts a list - like to string with given line width ."], "bleu": 0.158278836853973, "rouge_l": 0.4642313546423136}
{"id": 5253, "code": "def equation ( nodes = ( 1 , 2 ) , dofs = ( 1 , 1 ) , coefficients = ( 1. , 1. ) , comment = None ) : N = len ( nodes ) if comment == None : out = \"\" else : out = \"**EQUATION: {0}\\n\" . format ( comment ) out += \"*EQUATION\\n  {0}\\n  \" . format ( N ) out += \"\\n  \" . join ( [ \",\" . join ( [ str ( nodes [ i ] ) , str ( int ( dofs [ i ] ) ) , str ( coefficients [ i ] ) ] ) for i in range ( N ) ] ) return out", "predictions": ["print the equation equation"], "references": ["returns an abaqus inp formated string for a given linear equation ."], "bleu": 0.04862652376060361, "rouge_l": 0.11466165413533834}
{"id": 5254, "code": "def unsorted set ( df , label , * * kwargs ) : out = \"*NSET, NSET={0}, UNSORTED\\n\" . format ( label ) labels = df . index . values return out + argiope . utils . list to string ( labels , * * kwargs )", "predictions": ["set the values of a unsorted"], "references": ["returns a set as inp string with unsorted option ."], "bleu": 0.13487005099534619, "rouge_l": 0.23921568627450981}
{"id": 5255, "code": "def write inp ( self ) : template = self . get template ( ) return template . substitute ( { \"class\" : self . class . name , \"label\" : self . label } ) . strip ( )", "predictions": ["write the inp to the inp file ."], "references": ["returns the material definition as a string in abaqus inp format ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 5256, "code": "def write inp ( self ) : template = self . get template ( ) plastic table = self . get plastic table ( ) return template . substitute ( { \"class\" : self . class . name , \"label\" : self . label , \"young modulus\" : self . young modulus , \"poisson ratio\" : self . poisson ratio , \"plastic table\" : ( self . get plastic table ( ) [ [ \"stress\" , \"plastic strain\" ] ] . to csv ( header = False , index = False , sep = \",\" ) . strip ( ) ) } ) . strip ( )", "predictions": ["write the plastic table to the csv file"], "references": ["returns the material definition as a string in abaqus inp format ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 5257, "code": "def get plastic table ( self ) : E = self . young modulus sy = self . yield stress n = self . hardening exponent eps max = self . max strain Np = self . strain data points ey = sy / E s = 10. ** np . linspace ( 0. , np . log10 ( eps max / ey ) , Np ) strain = ey * s stress = sy * s ** n plastic strain = strain - stress / E return pd . Data Frame ( { \"strain\" : strain , \"stress\" : stress , \"plastic strain\" : plastic strain } )", "predictions": ["returns the plastic table of the strain"], "references": ["calculates the plastic data"], "bleu": 0.24446151121745047, "rouge_l": 0.3824451410658307}
{"id": 5258, "code": "def get plastic table ( self ) : K = self . consistency sy = self . yield stress n = self . hardening exponent eps max = self . max strain Np = self . strain data points plastic strain = np . linspace ( 0. , eps max , Np ) stress = sy + K * plastic strain ** n return pd . Data Frame ( { \"stress\" : stress , \"plastic strain\" : plastic strain } )", "predictions": ["returns the plastic table of the strain"], "references": ["calculates the plastic data"], "bleu": 0.24446151121745047, "rouge_l": 0.3824451410658307}
{"id": 5259, "code": "def write xy report ( odb , path , tags , columns , steps ) : xy Data = [ session . XY Data From History ( name = columns [ i ] , odb = odb , output Variable Name = tags [ i ] , steps = steps ) for i in xrange ( len ( tags ) ) ] session . xy Report Options . set Values ( num Digits = 8 , number Format = SCIENTIFIC ) session . write XY Report ( file Name = path , append Mode = OFF , xy Data = xy Data )", "predictions": ["write the report to a file"], "references": ["writes a xy_report based on xy data ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 5260, "code": "def write field report ( odb , path , label , argiope class , variable , instance , output position , step = - 1 , frame = - 1 , sort Item = 'Node Label' ) : step Keys = get steps ( odb ) step = xrange ( len ( step Keys ) ) [ step ] frame = xrange ( get frames ( odb , step Keys [ step ] ) ) [ frame ] nf = Number Format ( num Digits = 9 , precision = 0 , format = SCIENTIFIC ) session . field Report Options . set Values ( print Total = OFF , print Min Max = OFF , number Format = nf ) leaf = dgo . Leaf From Part Instance ( part Instance Name = instance ) session . viewports [ 'Viewport: 1' ] . odb Display . display Group . replace ( leaf = leaf ) session . write Field Report ( file Name = path , append = OFF , sort Item = sort Item , odb = odb , step = step , frame = frame , output Position = output position , variable = variable ) lines = [ line . strip ( ) for line in open ( path ) . readlines ( ) ] isdata = - 1 data = [ ] for line in lines : if isdata == 1 : if len ( line ) == 0 : isdata -= 1 else : data . append ( line ) elif isdata < 1 : if line . startswith ( \"--\" ) : isdata += 1 data = \"\\n\" . join ( [ \",\" . join ( line . split ( ) ) for line in data if len ( line ) != 0 ] ) header = str ( output position ) . lower ( ) + \",\" header += \",\" . join ( [ v [ 1 ] for v in variable [ 0 ] [ 2 ] ] ) + \"\\n\" metadata = ( ( \"label\" , label ) , ( \"argiope class\" , argiope class ) , ( \"odb\" , odb . path ) , ( \"instance\" , instance ) , ( \"position\" , output position ) , ( \"step num\" , step ) , ( \"step label\" , step Keys [ step ] ) , ( \"frame\" , frame ) , ( \"frame value\" , odb . steps [ step Keys [ step ] ] . frames [ frame ] . frame Value ) ) out = \"*METADATA\\n{0}\\n*DATA\\n{1}\" . format ( \"\\n\" . join ( [ \"{0}={1}\" . format ( k , v ) for k , v in metadata ] ) , header + data ) open ( path , \"w\" ) . write ( out )", "predictions": ["write the report to a report file"], "references": ["writes a field report and rewrites it in a cleaner format ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 5261, "code": "def list ( component type ) : config loader = initialise component loader ( ) component types = sorted ( { \"displays\" : lambda : config loader . load by type ( Component Type . DISPLAY ) , \"datafeeds\" : lambda : config loader . load by type ( Component Type . DATA FEED ) , \"filters\" : lambda : config loader . load by type ( Component Type . FILTER ) , \"notifications\" : lambda : config loader . load by type ( Component Type . NOTIFICATION ) } . items ( ) , key = lambda t : t [ 0 ] ) def print ids ( creators ) : ids = { c . id key value [ 1 ] if hasattr ( c , \"id key value\" ) else c . get id ( ) for c in creators } for i in sorted ( ids ) : click . echo ( \" - %s\" % i ) for k , v in component types : if component type == k or component type == \"all\" : click . echo ( \"Available %s:\" % k ) print ids ( v ( ) ) if component type == \"all\" : click . echo ( \"\" )", "predictions": ["list all component types ."], "references": ["list components that are available on your machine"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 5262, "code": "def set data ( self ) : if getattr ( self , 'data' , False ) and not getattr ( self , ' x' , False ) and not getattr ( self , ' y' , False ) : x = X Variable ( ) y = Y Variable ( ) x . contribute to class ( self , 'X' , self . data ) y . contribute to class ( self , 'Y' , self . data ) self [ 'data' ] = zip ( self . x . points , self . y . points ) else : for axis in ( ' x' , ' y' ) : axis obj = getattr ( self , axis , False ) if not axis obj : raise exception . Missing Axis Exception ( \"%s missing\" % axis ) if not getattr ( axis obj , 'points' , False ) : raise exception . Missing Data Exception ( ) self [ 'data' ] = zip ( self . x . points , self . y . points )", "predictions": ["set data for the axis"], "references": ["this method will be called to set series data"], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 5263, "code": "def get axis mode ( self , axis ) : if all ( [ isinstance ( getattr ( s , axis ) , Time Variable ) for s in self . series ] ) : return 'time' return None", "predictions": ["return the axis mode for the given axis ."], "references": ["will get the axis mode for the current series"], "bleu": 0.5169731539571706, "rouge_l": 0.5555555555555556}
{"id": 5264, "code": "def set options ( self ) : if 'xaxis' in self . options . keys ( ) : self . options [ 'xaxis' ] . update ( { 'mode' : self . get axis mode ( X Axis . var name ) } ) if 'yaxis' in self . options . keys ( ) : self . options [ 'yaxis' ] . update ( { 'mode' : self . get axis mode ( Y Axis . var name ) } )", "predictions": ["set the options for this field ."], "references": ["sets the graph ploting options"], "bleu": 0.20556680845025982, "rouge_l": 0.34366197183098596}
{"id": 5265, "code": "def create setter ( func , attrs ) : def set ( self , instance , value , name = None ) : args = [ getattr ( self , attr ) for attr in attrs ] if not func ( value , * args ) : raise Value Error ( self . err msg ( instance , value ) ) return set", "predictions": ["create a set of setter instances ."], "references": ["create the __set__ method for the descriptor ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 5266, "code": "def make class ( clsname , func , attrs ) : clsdict = { \" set \" : create setter ( func , attrs ) } if len ( attrs ) > 0 : clsdict [ \" init \" ] = create init ( attrs ) clsobj = type ( str ( clsname ) , ( Descriptor , ) , clsdict ) clsobj . doc = docstrings . get ( clsname ) return clsobj", "predictions": ["create a class from a callable ."], "references": ["turn a funcs list element into a class object ."], "bleu": 0.18094495256969623, "rouge_l": 0.34205607476635513}
{"id": 5267, "code": "def cycle ( self ) : messages = self . poll datafeeds ( ) notifications = self . process notifications ( messages ) self . draw notifications ( notifications )", "predictions": ["process the notifications and call the user"], "references": ["cycles through notifications with latest results from data feeds ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 5268, "code": "def plot ( parser , token ) : tokens = token . split contents ( ) tokens . pop ( 0 ) graph = tokens . pop ( 0 ) attrs = dict ( [ token . split ( \"=\" ) for token in tokens ] ) if 'id' not in attrs . keys ( ) : attrs [ 'id' ] = '' . join ( [ chr ( choice ( range ( 65 , 90 ) ) ) for i in range ( 0 , 5 ) ] ) else : attrs [ 'id' ] = attrs [ 'id' ] [ 1 : len ( attrs [ 'id' ] ) - 1 ] attr string = '' . join ( [ \" %s=%s\" % ( k , v ) for k , v in attrs . iteritems ( ) ] ) return Graph Renderer ( graph , attr string , attrs [ 'id' ] )", "predictions": ["plot the data in a nice way ."], "references": ["tag to plot graphs into the template"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 5269, "code": "def read varint ( self ) : buf = self . read ( 8 ) ( n , l ) = Decode Varint ( buf , 0 ) self . unread ( buf [ l : ] ) return n", "predictions": ["read a varint value from the buffer ."], "references": ["read exactly a varint out of the underlying file ."], "bleu": 0.19546825878823415, "rouge_l": 0.5446428571428571}
{"id": 5270, "code": "def working directory ( path ) : prev dir = os . getcwd ( ) os . chdir ( str ( path ) ) try : yield finally : os . chdir ( prev dir )", "predictions": ["context manager to create a working directory"], "references": ["change working directory and restore the previous on exit"], "bleu": 0.18370727471078332, "rouge_l": 0.24448897795591182}
{"id": 5271, "code": "def exit ( self ) : if self . server is not None : self . server . shutdown ( ) self . server . server close ( ) self . server = None", "predictions": ["shut down the server ."], "references": ["stop the simple wsgi server running the appliation ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 5272, "code": "def get error page callback ( self ) : if self . response . status in self . error handlers : return self . error handlers [ self . response . status ] elif None in self . error handlers : return self . error handlers [ None ] else : self . response . media type = 'text/plain' return lambda : self . response . status line", "predictions": ["return the error page page ."], "references": ["return an error page for the current response status ."], "bleu": 0.17749896924055253, "rouge_l": 0.47843137254901963}
{"id": 5273, "code": "def attempt fetch ( work unit , fpath ) : url = 'http://s3.amazonaws.com/aws-publicdatasets/' + work unit . key . strip ( ) cmd = '(wget -O - %s | gpg --no-permission-warning --trust-model always --output - --decrypt - | xz --decompress) 2> %s-err' % ( url , fpath ) print cmd child = Popen ( cmd , stdout = PIPE , shell = True ) print 'child launched' sys . stdout . flush ( ) si count = 0 serif count = 0 exc = '' stream ids = list ( ) clean visible bytes = 0 clean visible count = 0 try : for si in Chunk ( file obj = child . stdout ) : print si . stream id , si . abs url if si . body . language : lang = si . body . language . code else : lang = '' stream ids . append ( ( lang , si . stream id ) ) if si . body . clean visible : clean visible count += 1 clean visible bytes += len ( si . body . clean visible ) si count += 1 if 'serif' in si . body . sentences : serif count += 1 except Exception , exc : exc = re . sub ( '\\s+' , ' ' , str ( exc ) ) . strip ( ) child . terminate ( ) child . wait ( ) child . stdout . close ( ) return exc , si count , serif count , clean visible bytes , clean visible count , stream ids", "predictions": ["fetch the visible gpg visible visible on the given work unit ."], "references": ["attempt a fetch and iteration over a work_unit . key path in s3"], "bleu": 0.10579369505074822, "rouge_l": 0.15885416666666669}
{"id": 5274, "code": "def get file lines ( file name ) : file path = path . join ( path . dirname ( path . abspath ( file ) ) , file name ) with open ( file path ) as file obj : return [ line for line in file obj . read ( ) . splitlines ( ) if line ]", "predictions": ["return a list of all lines in the given file ."], "references": ["return a list of non - empty lines from file_path ."], "bleu": 0.3448444257953326, "rouge_l": 0.5454545454545454}
{"id": 5275, "code": "def random adjspecies pair ( ) : describer , desc position = random describer ( ) if desc position == 'prefix' : return ( describer , random species ( ) ) elif desc position == 'suffix' : return ( random species ( ) , describer )", "predictions": ["randomly randomly return a random adjspecies pair"], "references": ["return an ordered 2 - tuple containing a species and a describer ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 5276, "code": "def morph ( ctx , app id , sentence file , json flag , sentence , info filter , pos filter , request id ) : app id = clean app id ( app id ) sentence = clean sentence ( sentence , sentence file ) if info filter : info filter = info filter . replace ( ',' , '|' ) if pos filter : pos filter = pos filter . replace ( ',' , '|' ) api = Goolabs API ( app id ) ret = api . morph ( sentence = sentence , info filter = info filter , pos filter = pos filter , request id = request id , ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return for words in ret [ 'word list' ] : for word in words : click . echo ( ',' . join ( word ) )", "predictions": ["morph words in a given string ."], "references": ["morphological analysis for japanese ."], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 5277, "code": "def similarity ( ctx , app id , json flag , query pair , request id ) : app id = clean app id ( app id ) api = Goolabs API ( app id ) ret = api . similarity ( query pair = query pair , request id = request id ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return click . echo ( '{0:.16f}' . format ( ret [ 'score' ] ) )", "predictions": ["get similarity of an app"], "references": ["scoring the similarity of two words ."], "bleu": 0.24084874887188915, "rouge_l": 0.32360742705570295}
{"id": 5278, "code": "def hiragana ( ctx , app id , sentence file , json flag , sentence , output type , request id ) : app id = clean app id ( app id ) sentence = clean sentence ( sentence , sentence file ) api = Goolabs API ( app id ) ret = api . hiragana ( sentence = sentence , output type = output type , request id = request id ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return click . echo ( ret [ 'converted' ] )", "predictions": ["clean an existing app s content in the given sentence ."], "references": ["convert the japanese to hiragana or katakana ."], "bleu": 0.12605968092174913, "rouge_l": 0.216696269982238}
{"id": 5279, "code": "def entity ( ctx , app id , sentence file , json flag , sentence , class filter , request id ) : app id = clean app id ( app id ) sentence = clean sentence ( sentence , sentence file ) if class filter : class filter = class filter . replace ( ',' , '|' ) api = Goolabs API ( app id ) ret = api . entity ( sentence = sentence , class filter = class filter , request id = request id ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return for ne in ret [ 'ne list' ] : click . echo ( ',' . join ( ne ) )", "predictions": ["report an entity in the given sentence ."], "references": ["extract unique representation from sentence ."], "bleu": 0.21105340631872638, "rouge_l": 0.2932692307692307}
{"id": 5280, "code": "def shortsum ( ctx , app id , review file , json flag , review , length , request id ) : app id = clean app id ( app id ) review list = clean review ( review , review file ) length int = clean length ( length ) api = Goolabs API ( app id ) ret = api . shortsum ( review list = review list , length = length int , request id = request id , ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return click . echo ( ret [ 'summary' ] )", "predictions": ["run a t0 app"], "references": ["summarize reviews into a short summary ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 5281, "code": "def keyword ( ctx , app id , body file , json flag , title , body , max num , forcus , request id ) : app id = clean app id ( app id ) body = clean body ( body , body file ) api = Goolabs API ( app id ) ret = api . keyword ( title = title , body = body , max num = max num , forcus = forcus , request id = request id , ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return for k in ret [ 'keywords' ] : k = dict ( ( key . encode ( 'utf-8' ) , k [ key ] ) for key in k . keys ( ) ) for keyword , score in six . iteritems ( k ) : click . echo ( u'{0},{1}' . format ( text ( keyword ) , score ) )", "predictions": ["workdir the given name from the given app ."], "references": ["extract keywords from an input document ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 5282, "code": "def chrono ( ctx , app id , sentence file , json flag , sentence , doc time , request id ) : app id = clean app id ( app id ) sentence = clean sentence ( sentence , sentence file ) api = Goolabs API ( app id ) ret = api . chrono ( sentence = sentence , doc time = doc time , request id = request id , ) if json flag : click . echo ( format json ( api . response . json ( ) ) ) return for pair in ret [ 'datetime list' ] : click . echo ( u'{0}: {1}' . format ( text ( pair [ 0 ] ) , pair [ 1 ] ) )", "predictions": ["csv a given steps from the given steps ."], "references": ["extract expression expressing date and time and normalize its value"], "bleu": 0.10620315618312248, "rouge_l": 0.0}
{"id": 5283, "code": "def make app ( ) : env = Environment ( ) args = parser . parse args ( args = [ '/' , '--ignore-stdin' ] , env = env ) args . output options = 'HB' server = 'HTT Pony/{0}' . format ( version ) def application ( environ , start response ) : if environ . get ( 'CONTENT LENGTH' ) == '' : del environ [ 'CONTENT LENGTH' ] if environ . get ( 'CONTENT TYPE' ) == '' : del environ [ 'CONTENT TYPE' ] wrequest = Werkzeug Request ( environ ) data = wrequest . get data ( ) request = Request ( method = wrequest . method , url = wrequest . url , headers = wrequest . headers , data = data , ) prepared = request . prepare ( ) stream = streams . build output stream ( args , env , prepared , response = None , output options = args . output options ) streams . write stream ( stream , env . stdout , env . stdout isatty ) if data : print ( \"\\n\" , file = env . stdout ) response = Response ( headers = { 'Server' : server } ) return response ( environ , start response ) return application", "predictions": ["read the join environment and return the join environment"], "references": ["make a wsgi app that has all the httpie pieces baked in ."], "bleu": 0.09049614828481034, "rouge_l": 0.08802308802308802}
{"id": 5284, "code": "def make ner file ( self , clean visible path , ner xml path ) : if self . template is None : raise exceptions . Not Implemented Error ( ) tagger config = dict ( tagger root path = self . config [ 'tagger root path' ] , clean visible path = clean visible path , ner xml path = ner xml path ) tagger config [ 'java heap size' ] = self . config . get ( 'java heap size' , '' ) cmd = self . template % tagger config start time = time . time ( ) gc . collect ( ) try : self . child = subprocess . Popen ( cmd , stderr = subprocess . PIPE , shell = True ) except OS Error , exc : msg = traceback . format exc ( exc ) msg += make memory info msg ( clean visible path , ner xml path ) raise Pipeline Out Of Memory ( msg ) s out , errors = self . child . communicate ( ) if not self . child . returncode == 0 : if 'java.lang.Out Of Memory Error' in errors : msg = errors + make memory info msg ( clean visible path , ner xml path ) raise Pipeline Out Of Memory ( msg ) elif self . child . returncode == 137 : msg = 'tagger returncode = 137\\n' + errors msg += make memory info msg ( clean visible path , ner xml path ) raise Pipeline Out Of Memory ( msg ) elif 'Exception' in errors : raise Pipeline Base Exception ( errors ) else : raise Pipeline Base Exception ( 'tagger exited with %r' % self . child . returncode ) elapsed = time . time ( ) - start time logger . info ( 'finished tagging in %.1f seconds' % elapsed ) return elapsed", "predictions": ["list the to to be used when the to list of tagger . . . ."], "references": ["run tagger a child process to get xml output"], "bleu": 0.08513012360883544, "rouge_l": 0.08425414364640883}
{"id": 5285, "code": "def align chunk with ner ( self , ner xml path , i chunk , o chunk ) : input iter = i chunk . iter ( ) all ner = xml . dom . minidom . parse ( open ( ner xml path ) ) for ner dom in all ner . get Elements By Tag Name ( 'FILENAME' ) : #for stream id, raw ner in files(open(ner xml path).read().decode('utf8')): stream item = input iter . next ( ) stream id = ner dom . attributes . get ( 'stream id' ) . value if stream item . stream id is None : assert not stream id , 'out of sync: None != %r' % stream id logger . critical ( 'si.stream id is None... ignoring' ) continue assert stream id and stream id == stream item . stream id , '%s != %s' % ( stream id , stream item . stream id ) if not stream item . body : #assert not ner dom....something continue tagging = Tagging ( ) tagging . tagger id = self . tagger id #tagging.raw tagging = tagged doc tagging . generation time = streamcorpus . make stream time ( ) stream item . body . taggings [ self . tagger id ] = tagging sentences , relations , attributes = self . get sentences ( ner dom ) stream item . body . sentences [ self . tagger id ] = sentences stream item . body . relations [ self . tagger id ] = relations stream item . body . attributes [ self . tagger id ] = attributes logger . debug ( 'finished aligning tokens %s' % stream item . stream id ) if 'align labels by' in self . config and self . config [ 'align labels by' ] : assert 'aligner data' in self . config , 'config missing \"aligner data\"' aligner = Alignment Strategies [ self . config [ 'align labels by' ] ] aligner ( stream item , self . config [ 'aligner data' ] ) gc . collect ( ) try : o chunk . add ( stream item ) except Memory Error , exc : msg = traceback . format exc ( exc ) msg += make memory info msg ( ) logger . critical ( msg ) raise Pipeline Out Of Memory ( msg ) try : o chunk . close ( ) logger . info ( 'finished chunk for %r' % ner xml path ) except Memory Error , exc : msg = traceback . format exc ( exc ) msg += make memory info msg ( ) logger . critical ( msg ) raise Pipeline Out Of Memory ( msg )", "predictions": ["equation chunk nodes nodes nodes nodes nodes nodes nodes nodes are tagger"], "references": ["iterate through ner_xml_path to fuse with i_chunk into o_chunk"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 5286, "code": "def shutdown ( self ) : if self . child : try : self . child . terminate ( ) except OS Error , exc : if exc . errno == 3 : pass", "predictions": ["terminate the kwargs * * kwargs * * * * * * * * * * * unsorted * * * * * * * * * * * * *"], "references": ["send sigterm to the tagger child process"], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 5287, "code": "def mult ( p , n ) : np = P ( ) while n >= 1 : if n % 2 : np = np + p p = p + p n = n // 2 return np", "predictions": ["label p of p ."], "references": ["returns a pattern that matches exactly n repetitions of pattern p ."], "bleu": 0.09521044541645862, "rouge_l": 0.3285457809694794}
{"id": 5288, "code": "def fix emails ( text ) : emails = bracket emails . findall ( text ) keys = [ ] for email in emails : email = email . replace ( \"<\" , \"&lt;\" ) . replace ( \">\" , \"&gt;\" ) text = text . replace ( email , email ) return text", "predictions": ["write inp inp inp ."], "references": ["replace all angle bracket emails with a unique key ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 5289, "code": "def sentences ( self , clean visible ) : previous end = 0 clean visible = clean visible . decode ( 'utf8' ) for start , end in self . sentence tokenizer . span tokenize ( clean visible ) : if start < previous end : start = previous end if start > end : continue try : label = self . label index . find le ( end ) except Value Error : label = None if label : off = label . offsets [ Offset Type . CHARS ] end = max ( off . first + off . length , end ) previous end = end sent str = clean visible [ start : end ] yield start , end , sent str", "predictions": ["yield get get get get get get get get get get get get get get get get get get get get get get get get get get get get get get"], "references": ["generate strings identified as sentences"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5290, "code": "def make label index ( self , stream item ) : labels = stream item . body . labels . get ( self . annotator id ) if not labels : labels = [ ] self . label index = Sorted Collection ( [ l for l in labels if Offset Type . CHARS in l . offsets ] , key = lambda label : label . offsets [ Offset Type . CHARS ] . first )", "predictions": ["get the table table table table table table table"], "references": ["make a sortedcollection on body . labels"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5291, "code": "def make sentences ( self , stream item ) : self . make label index ( stream item ) sentences = [ ] token num = 0 new mention id = 0 for sent start , sent end , sent str in self . sentences ( stream item . body . clean visible ) : assert isinstance ( sent str , unicode ) sent = Sentence ( ) sentence pos = 0 for start , end in self . word tokenizer . span tokenize ( sent str ) : token str = sent str [ start : end ] . encode ( 'utf8' ) tok = Token ( token num = token num , token = token str , sentence pos = sentence pos , ) tok . offsets [ Offset Type . CHARS ] = Offset ( type = Offset Type . CHARS , first = sent start + start , length = end - start , ) try : label = self . label index . find le ( sent start + start ) except Value Error : label = None if label : off = label . offsets [ Offset Type . CHARS ] if off . first + off . length > sent start + start : streamcorpus . add annotation ( tok , label ) logger . debug ( 'adding label to tok: %r has %r' , tok . token , label . target . target id ) if label in self . label to mention id : mention id = self . label to mention id [ label ] else : mention id = new mention id new mention id += 1 self . label to mention id [ label ] = mention id tok . mention id = mention id token num += 1 sentence pos += 1 sent . tokens . append ( tok ) sentences . append ( sent ) return sentences", "predictions": ["write xy xy xy xy xy xy xy xy xy xy"], "references": ["assemble sentence and token objects"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 5292, "code": "def make cleansed file ( i chunk , tmp cleansed path ) : tmp cleansed = open ( tmp cleansed path , 'wb' ) for idx , si in enumerate ( i chunk ) : tmp cleansed . write ( '<FILENAME docid=\"%s\">\\n' % si . stream id ) tmp cleansed . write ( si . body . cleansed ) tmp cleansed . write ( '</FILENAME>\\n' ) tmp cleansed . close ( ) print 'created %s' % tmp cleansed path", "predictions": ["write a field to a field"], "references": ["make a temp file of cleansed text"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5293, "code": "def make ner file ( tagger id , tmp cleansed path , tmp ner path , pipeline root ) : params = dict ( INPUT FILE = tmp cleansed path , #RAW OUTPUT FILE=tmp ner raw path, OUTPUT FILE = tmp ner path , PIPELINE ROOT = pipeline root ) pipeline cmd = pipeline cmd templates [ tagger id ] % params print pipeline cmd print 'creating %s' % tmp ner path start time = time . time ( ) gpg child = subprocess . Popen ( pipeline cmd , stderr = subprocess . PIPE , shell = True ) s out , errors = gpg child . communicate ( ) assert gpg child . returncode == 0 and 'Exception' not in errors , errors elapsed = time . time ( ) - start time print 'created %s in %.1f sec' % ( tmp ner path , elapsed )", "predictions": ["list of ner templates"], "references": ["run child process to get owpl output"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 5294, "code": "def make clean visible file ( i chunk , clean visible path ) : clean = open ( clean visible path , 'wb' ) clean . write ( '<?xml version=\"1.0\" encoding=\"UTF-8\"?>' ) clean . write ( '<root>' ) for idx , si in enumerate ( i chunk ) : if si . stream id is None : stream id = '' else : stream id = si . stream id doc = lxml . etree . Element ( \"FILENAME\" , stream id = stream id ) if si . body and si . body . clean visible : try : doc . text = si . body . clean visible . decode ( 'utf8' ) except Value Error : doc . text = drop invalid and upper utf8 chars ( si . body . clean visible . decode ( 'utf8' ) ) except Exception , exc : logger . critical ( traceback . format exc ( exc ) ) logger . critical ( 'failed on stream id=%s to follow:' , si . stream id ) logger . critical ( repr ( si . body . clean visible ) ) logger . critical ( 'above was stream id=%s' , si . stream id ) raise else : doc . text = '' clean . write ( lxml . etree . tostring ( doc , encoding = 'UTF-8' ) ) clean . write ( '</root>' ) clean . close ( ) logger . info ( clean visible path )", "predictions": ["set up the data self . . with the given path . . . . . . . . . . . . . . . . . . . ."], "references": ["make a temp file of clean_visible text"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5295, "code": "def main ( ) : import argparse import sys parser = argparse . Argument Parser ( ) parser . add argument ( 'path' ) args = parser . parse args ( ) html = open ( args . path ) . read ( ) html = html . decode ( 'utf8' ) cursor = 0 for s in non tag chars from raw ( html ) : for c in s : if c != ' ' and c != html [ cursor ] : import pdb pdb . set trace ( ) sys . stdout . write ( c . encode ( 'utf8' ) ) sys . stdout . flush ( ) cursor += 1", "predictions": ["get in in in in in in in in in in in in in in in in in in in in in in in in in in in in - place"], "references": ["manual test loop for make_clean_visible_from_raw"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5296, "code": "def paths ( input dir ) : for root , dirs , fnames in os . walk ( input dir ) : for i fname in fnames : i path = os . path . join ( root , i fname ) yield i path", "predictions": ["return all set of set set of set of set of set set update update update update set set of set of set of set of set of set of set"], "references": ["yield all file paths under input_dir"], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 5297, "code": "def tasks ( self , key prefix = '' ) : for row in self . tasks . get range ( ) : logger . debug ( row ) if not row [ 0 ] . startswith ( key prefix ) : continue data = json . loads ( row [ 1 ] [ 'task data' ] ) data [ 'task key' ] = row [ 0 ] yield data", "predictions": ["yields all create create create create create create create objects"], "references": ["generate the data objects for every task"], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 5298, "code": "def get random available ( self , max iter = 10000 ) : c = 1 keeper = None #random key = hashlib.md5(str(random.random())).hexdigest() #random key = '0' * 32 #logger.debug('available.get range(%r)' % random key) # for row in self . available . get range ( row count = max iter , read consistency level = pycassa . Consistency Level . ALL ) : #for row in self. available.get range(row count=100): logger . debug ( 'considering %r' % ( row , ) ) if random . random ( ) < 1 / c : keeper = row [ 0 ] if c == max iter : break c += 1 return keeper", "predictions": ["make a class from the pycassa"], "references": ["get a random key out of the first max_iter rows"], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 5299, "code": "def tokens ( self , sentence dom ) : self . sent pos = 0 mention id = 0 while len ( sentence dom . child Nodes ) > 0 : node = sentence dom . child Nodes . pop ( 0 ) if node . node Type == node . TEXT NODE : for line in node . data . splitlines ( True ) : self . input string = line for start , end in self . word tokenizer . span tokenize ( line ) : tok = self . make token ( start , end ) if tok : yield tok if line . endswith ( '\\n' ) : self . line idx += 1 self . byte idx += len ( line . encode ( 'utf-8' ) ) else : assert node . node Name == 'ENAMEX' , node . node Name chain id = node . attributes . get ( 'ID' ) . value entity type = node . attributes . get ( 'TYPE' ) . value for node in node . child Nodes : assert node . node Type == node . TEXT NODE , node . node Type for line in node . data . splitlines ( True ) : self . input string = line for start , end in self . word tokenizer . span tokenize ( line ) : tok = self . make token ( start , end ) if tok : if entity type in PRONOUNS : tok . mention type = Mention Type . PRO tok . entity type = ENTITY TYPES [ entity type ] attr = Attribute ( attribute type = Attribute Type . PER GENDER , value = str ( PRONOUNS [ entity type ] ) ) self . attributes . append ( attr ) else : tok . mention type = Mention Type . NAME tok . entity type = ENTITY TYPES [ entity type ] tok . equiv id = int ( chain id ) tok . mention id = mention id yield tok if line . endswith ( '\\n' ) : self . line idx += 1 self . byte idx += len ( line . encode ( 'utf-8' ) ) mention id += 1", "predictions": ["return a generator of cycle cycle from the sentence poll poll poll poll poll ."], "references": ["tokenize all the words and preserve ner labels from enamex tags"], "bleu": 0.09103526405546068, "rouge_l": 0.07911802853437094}
{"id": 5300, "code": "def get sentences ( self , ner dom ) : lp parser = Ling Pipe Parser ( self . config ) lp parser . set ( ner dom ) sentences = list ( lp parser . sentences ( ) ) return sentences , lp parser . relations , lp parser . attributes", "predictions": ["plot the sentences for the given dom attrs attrs attrs attrs attrs attrs attrs attrs attrs attrs"], "references": ["parse the sentences and tokens out of the xml"], "bleu": 0.10216198665886358, "rouge_l": 0.2443257676902537}
{"id": 5301, "code": "def verify md5 ( md5 expected , data , other errors = None ) : md5 recv = hashlib . md5 ( data ) . hexdigest ( ) if md5 expected != md5 recv : if other errors is not None : logger . critical ( '\\n' . join ( other errors ) ) raise Failed Verification ( 'original md5 = %r != %r = received md5' % ( md5 expected , md5 recv ) ) return True", "predictions": ["read the varint data"], "references": ["return true if okay raise exception if not"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 5302, "code": "def main ( argv = sys . argv ) : args = parse ( argv ) hostname = args . listen port = args . port print ( \"Making all your dreams for a pony come true on http://{0}:{1}.\\n\" \"Press Ctrl+C to quit.\\n\" . format ( hostname , port ) ) logging . get Logger ( 'werkzeug' ) . set Level ( logging . CRITICAL ) plugin manager . load installed plugins ( ) app = make app ( ) run simple ( hostname , port , app )", "predictions": ["load a command line"], "references": ["serve up some ponies ."], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 5303, "code": "def build parser ( ) : description = ( 'HTT Pony (pronounced aych-tee-tee-pony) is a simple HTTP ' 'server that pretty prints HTTP requests to a terminal. It ' 'is a useful aide for developing clients that send HTTP ' 'requests. HTT Pony acts as a sink for a client so that a ' 'developer can understand what the client is sending.' ) parser = argparse . Argument Parser ( description = description ) parser . add argument ( '-l' , '--listen' , help = 'set the IP address or hostname' , default = 'localhost' ) parser . add argument ( '-p' , '--port' , help = 'set the port' , default = 8000 , type = int ) return parser", "predictions": ["exit the argument parser ."], "references": ["build the parser that will have all available commands and options ."], "bleu": 0.08006212224540951, "rouge_l": 0.3285457809694794}
{"id": 5304, "code": "def sentences to char tokens ( si sentences ) : for sentence in si sentences : for token in sentence . tokens : if Offset Type . CHARS in token . offsets : yield token", "predictions": ["yield - separated self in self in self in self in self in self in self in self in self in self in self in self in self in self in"], "references": ["convert stream item sentences to character offset s ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5305, "code": "def char tokens to char offsets ( si tokens ) : for token in si tokens : offset = token . offsets [ Offset Type . CHARS ] yield offset . first , offset . first + offset . length", "predictions": ["- - - - - - si fetch fetch fetch fetch fetch fetch fetch fetch fetch fetch fetch fetch fetch fetch fetch fetch fetch fetch fetch fetch fetch fetch fetch fetch"], "references": ["convert character offset s to character ranges ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5306, "code": "def text index ( self ) : i = self . tags . get ( Text Element , 0 ) if self . last tag is not Text Element : i += 1 return i", "predictions": ["in the get get the get element"], "references": ["returns the one - based index of the current text node ."], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 5307, "code": "def descendants ( elem ) : for child in elem . xml children : if isinstance ( child , element ) : yield child yield from descendants ( child )", "predictions": ["yields the random position of an position desc ."], "references": ["yields all the elements descendant of elem in document order"], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 5308, "code": "def following siblings ( elem ) : it = itertools . dropwhile ( lambda x : x != elem , elem . xml parent . xml children ) next ( it ) #Skip the element itself return it", "predictions": ["extract the morph siblings siblings from the = = 0 flag flag flag flag flag flag flag flag flag flag flag flag flag flag flag flag flag flag flag flag flag"], "references": ["yields elements and text which have the same parent as elem but come afterward in document order"], "bleu": 0.03901663112717908, "rouge_l": 0.04397981254506129}
{"id": 5309, "code": "def svg2pdf ( svg file path , pdf file path , dpi = 150 , command binpath = None , support unicode = False ) : if support unicode : return rsvg export ( svg file path , pdf file path , dpi = dpi , rsvg binpath = command binpath ) return inkscape export ( svg file path , pdf file path , export flag = \"-A\" , dpi = dpi , inkscape binpath = command binpath )", "predictions": ["writes a json file file to an ctx file"], "references": ["transform svg file to pdf file"], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 5310, "code": "def svg2png ( svg file path , png file path , dpi = 150 , inkscape binpath = None ) : return inkscape export ( svg file path , png file path , export flag = \"-e\" , dpi = dpi , inkscape binpath = inkscape binpath )", "predictions": ["clean up the ctx file in the ctx ctx"], "references": ["transform svg file to png file"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 5311, "code": "def strval ( node , outermost = True ) : if not isinstance ( node , element ) : return node . xml value if outermost else [ node . xml value ] accumulator = [ ] for child in node . xml children : if isinstance ( child , text ) : accumulator . append ( child . xml value ) elif isinstance ( child , element ) : accumulator . extend ( strval ( child , outermost = False ) ) if outermost : accumulator = '' . join ( accumulator ) return accumulator", "predictions": ["recursively convert an xml ctx to a python accumulator"], "references": ["xpath - like string value of node"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5312, "code": "def parse options ( ) : version = \"%%prog {version}\" . format ( version = version ) parser = Option Parser ( version = version ) parser . add option ( \"-u\" , \"--username\" , action = \"store\" , dest = \"username\" , type = \"string\" , default = \"\" , metavar = \"RECIPIENT\" , help = \"user\" ) parser . add option ( \"-C\" , \"--calendar\" , metavar = \"CALENDAR\" , action = \"store\" , type = \"string\" , dest = \"calendar\" , default = \"\" , help = \"google calendar ID\" ) parser . add option ( \"-t\" , \"--timezone\" , metavar = \"TIMEZONE\" , action = \"store\" , type = \"string\" , dest = \"timezone\" , default = \"\" , help = \"user timezone\" ) parser . add option ( \"-m\" , \"--message\" , metavar = \"MESSAGE\" , action = \"store\" , type = \"string\" , dest = \"message\" , default = \"\" , help = \"message text\" ) parser . add option ( \"-c\" , \"--config\" , metavar = \"CONFIG\" , action = \"store\" , type = \"string\" , dest = \"config\" , help = \"path to config file\" , default = \"/etc/nagios/notification google calendar.ini\" ) parser . add option ( \"-q\" , \"--quiet\" , metavar = \"QUIET\" , action = \"store true\" , default = False , dest = \"quiet\" , help = \"be quiet\" ) parser . add option ( \"-g\" , \"--get-google-credentials\" , metavar = \"GET-GOOGLE-CREDENTIALS\" , action = \"store true\" , default = False , dest = \"get google credentials\" , help = \"get google API credentials for user\" ) options = parser . parse args ( sys . argv ) [ 0 ] mandatories = [ \"username\" , ] if not options . get google credentials : mandatories . append ( \"calendar\" ) mandatories . append ( \"message\" ) mandatories . append ( \"timezone\" ) if not all ( options . dict [ mandatory ] for mandatory in mandatories ) : parser . error ( \"Required command line option missing\\n\" ) return options", "predictions": ["parse command line arguments ."], "references": ["commandline options arguments parsing ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 5313, "code": "def parse config ( options ) : if os . path . exists ( options . config ) : config = Config Parser . Config Parser ( ) try : config . read ( options . config ) except Exception , err : if not options . quiet : sys . stderr . write ( \"ERROR: Config file read {config} error. {err}\" . format ( config = options . config , err = err ) ) sys . exit ( - 1 ) try : configdata = { \"secrets\" : config . get ( \"GOOGLE\" , \"secrets\" ) , \"credentials\" : config . get ( \"nagios-notification-google-calendar\" , \"credentials\" ) , \"start\" : config . get ( \"nagios-notification-google-calendar\" , \"start\" ) , \"end\" : config . get ( \"nagios-notification-google-calendar\" , \"end\" ) , \"message\" : config . get ( \"nagios-notification-google-calendar\" , \"message\" ) , } except Config Parser . No Option Error , err : if not options . quiet : sys . stderr . write ( \"ERROR: Config file missing option error. {err}\\n\" . format ( err = err ) ) sys . exit ( - 1 ) mandatories = [ \"secrets\" , \"credentials\" , \"start\" , \"end\" , \"message\" , ] if not all ( configdata [ mandatory ] for mandatory in mandatories ) : if not options . quiet : sys . stdout . write ( \"Mandatory config option missing\\n\" ) sys . exit ( 0 ) return configdata else : if not options . quiet : sys . stderr . write ( \"ERROR: Config file {config} does not exist\\n\" . format ( config = options . config ) ) sys . exit ( 0 )", "predictions": ["parse command line arguments ."], "references": ["get settings from config file ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 5314, "code": "def get google credentials ( options , config ) : try : if options . get google credentials : flow = flow from clientsecrets ( config [ \"secrets\" ] , scope = SCOPE , redirect uri = \"oob\" ) sys . stdout . write ( \"Follow this URL: {url} and grant access to calendar.\\n\" . format ( url = flow . step1 get authorize url ( ) ) ) token = raw input ( \"Enter token:\" ) credentials = flow . step2 exchange ( token ) storage = Storage ( os . path . join ( config [ \"credentials\" ] , \"{username}.json\" . format ( username = options . username ) ) ) storage . put ( credentials ) credentials . set store ( storage ) else : storage = Storage ( os . path . join ( config [ \"credentials\" ] , \"{username}.json\" . format ( username = options . username ) ) ) credentials = storage . get ( ) except Exception , err : if not options . quiet : sys . stderr . write ( \"ERROR: Getting google API credentials error. {err}\\n\" . format ( err = err ) ) sys . exit ( - 1 ) return credentials", "predictions": ["get google credentials from google config file ."], "references": ["get google api credentials for user ."], "bleu": 0.239802967618271, "rouge_l": 0.5398230088495575}
{"id": 5315, "code": "def create event datetimes ( options , config ) : now = datetime . datetime . now ( ) return { \"start\" : { \"date Time\" : ( now + datetime . timedelta ( minutes = int ( config [ \"start\" ] ) ) ) . strftime ( DT FORMAT ) , \"time Zone\" : options . timezone , } , \"end\" : { \"date Time\" : ( now + datetime . timedelta ( minutes = int ( config [ \"end\" ] ) ) ) . strftime ( DT FORMAT ) , \"time Zone\" : options . timezone , } , }", "predictions": ["create default event datetimes for the options ."], "references": ["create event start and end datetimes ."], "bleu": 0.20164945583740668, "rouge_l": 0.5398230088495575}
{"id": 5316, "code": "def create event ( options , config , credentials ) : try : http = credentials . authorize ( httplib2 . Http ( ) ) service = build ( \"calendar\" , \"v3\" , http = http ) event = { \"summary\" : options . message , \"location\" : \"\" , \"reminders\" : { \"use Default\" : False , \"overrides\" : [ { \"method\" : \"sms\" , \"minutes\" : config [ \"message\" ] , } , ] , } } event . update ( create event datetimes ( options , config ) ) service . events ( ) . insert ( calendar Id = options . calendar , send Notifications = True , body = event ) . execute ( ) except Exception , err : if not options . quiet : sys . stderr . write ( \"ERROR: Creating google calendar event error. {err}\\n\" . format ( err = err ) ) sys . exit ( - 1 )", "predictions": ["create an event in the calendar calendar ."], "references": ["create event in calendar with sms reminder ."], "bleu": 0.25098621243978964, "rouge_l": 0.625}
{"id": 5317, "code": "def main ( ) : options = parse options ( ) config = parse config ( options ) credentials = get google credentials ( options , config ) if not options . get google credentials : create event ( options , config , credentials )", "predictions": ["command line interface ."], "references": ["processing notification call main function ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 5318, "code": "def parse ( self ) : for tag in self . soup . find All ( 'span' ) : self . create italic ( tag ) self . create strong ( tag ) self . create underline ( tag ) self . unwrap span ( tag ) for tag in self . soup . find All ( 'a' ) : self . remove comments ( tag ) self . check next ( tag ) if self . soup . body : for tag in self . soup . body . find All ( ) : self . remove empty ( tag ) self . remove inline comment ( tag ) self . parse attrs ( tag ) for token , target in self . tokens : self . find token ( tag , token , target ) self . remove blacklisted tags ( tag )", "predictions": ["parses and sets the comments ."], "references": ["run all parsing functions ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 5319, "code": "def check next ( self , tag ) : if ( type ( tag . next sibling ) == element . Tag and tag . next sibling . name == 'a' ) : next tag = tag . next sibling if tag . get ( 'href' ) and next tag . get ( 'href' ) : href = self . parse href ( tag . get ( 'href' ) ) next href = self . parse href ( next tag . get ( 'href' ) ) if href == next href : next text = next tag . get text ( ) tag . append ( next text ) self . tags blacklist . append ( next tag )", "predictions": ["check the next sibling of the tag"], "references": ["if next tag is link with same href combine them ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 5320, "code": "def create italic ( self , tag ) : style = tag . get ( 'style' ) if style and 'font-style:italic' in style : tag . wrap ( self . soup . new tag ( 'em' ) )", "predictions": ["create an italic for the given tag ."], "references": ["see if span tag has italic style and wrap with em tag ."], "bleu": 0.12139281957861149, "rouge_l": 0.2739520958083832}
{"id": 5321, "code": "def create strong ( self , tag ) : style = tag . get ( 'style' ) if ( style and ( 'font-weight:bold' in style or 'font-weight:700' in style ) ) : tag . wrap ( self . soup . new tag ( 'strong' ) )", "predictions": ["create the strong for the given tag ."], "references": ["see if span tag has bold style and wrap with strong tag ."], "bleu": 0.12139281957861149, "rouge_l": 0.2739520958083832}
{"id": 5322, "code": "def create underline ( self , tag ) : style = tag . get ( 'style' ) if style and 'text-decoration:underline' in style : tag . wrap ( self . soup . new tag ( 'u' ) )", "predictions": ["create the underline underline"], "references": ["see if span tag has underline style and wrap with u tag ."], "bleu": 0.037870374782798366, "rouge_l": 0.1073943661971831}
{"id": 5323, "code": "def parse attrs ( self , tag ) : if tag . name in ATTR WHITELIST . keys ( ) : attrs = copy ( tag . attrs ) for attr , value in attrs . items ( ) : if attr in ATTR WHITELIST [ tag . name ] : tag . attrs [ attr ] = self . parse attr ( tag . name , attr , value ) else : del tag . attrs [ attr ] else : tag . attrs = { }", "predictions": ["parse the attributes of the tag ."], "references": ["reject attributes not defined in attr_whitelist ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 5324, "code": "def remove empty ( self , tag ) : has children = len ( tag . contents ) has text = len ( list ( tag . stripped strings ) ) if not has children and not has text and not tag . is empty element : tag . extract ( )", "predictions": ["remove empty strings from the tag ."], "references": ["remove non - self - closing tags with no children * and * no content ."], "bleu": 0.056829570481990416, "rouge_l": 0.16245006657789615}
{"id": 5325, "code": "def boolean arg ( ctx , obj ) : if hasattr ( obj , 'compute' ) : obj = next ( obj . compute ( ctx ) , False ) return to boolean ( obj )", "predictions": ["wrapper for boolean ast . boolean"], "references": ["handles literalobjects as well as computable arguments"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 5326, "code": "def number arg ( ctx , obj ) : if hasattr ( obj , 'compute' ) : obj = next ( obj . compute ( ctx ) , False ) return to number ( obj )", "predictions": ["handles the number of an object"], "references": ["handles literalobjects as well as computable arguments"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5327, "code": "def string arg ( ctx , obj ) : if hasattr ( obj , 'compute' ) : obj = next ( obj . compute ( ctx ) , False ) return to string ( obj )", "predictions": ["compute the string of an object"], "references": ["handles literalobjects as well as computable arguments"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 5328, "code": "def concat ( ctx , * strings ) : strings = flatten ( [ ( s . compute ( ctx ) if callable ( s ) else s ) for s in strings ] ) strings = ( next ( string arg ( ctx , s ) , '' ) for s in strings ) #assert(all(map(lambda x: isinstance(x, str), strings))) #FIXME: Check arg types yield '' . join ( strings )", "predictions": ["concat strings from string ."], "references": ["yields one string concatenation of argument strings"], "bleu": 0.20252884954471367, "rouge_l": 0.16180371352785147}
{"id": 5329, "code": "def starts with ( ctx , full , part ) : full = next ( string arg ( ctx , full ) , '' ) part = next ( string arg ( ctx , part ) , '' ) yield full . startswith ( part )", "predictions": ["generator that yields starts starts with full string ."], "references": ["yields one boolean whether the first string starts with the second"], "bleu": 0.1689983564524028, "rouge_l": 0.2946859903381642}
{"id": 5330, "code": "def contains ( ctx , full , part ) : full = next ( string arg ( ctx , full ) , '' ) part = next ( string arg ( ctx , part ) , '' ) yield part in full", "predictions": ["generator that yields the string contains the full part of the full string"], "references": ["yields one boolean whether the first string contains the second"], "bleu": 0.19674979811155635, "rouge_l": 0.44525547445255476}
{"id": 5331, "code": "def check inputs ( self ) : try : = self . inputs [ 0 ] except Type Error : raise Runtime Error ( \"inputs should be iterable but found type='{0}', value=\" \"'{1}'\" . format ( type ( self . inputs ) , str ( self . inputs ) ) ) from melody . inputs import Input for check input in self . inputs : if not isinstance ( check input , Input ) : raise Runtime Error ( \"input should be a subclass of the Input class but \" \"found type='{0}', value='{1}'\" . format ( type ( check input ) , str ( check input ) ) )", "predictions": ["check that the inputs are valid ."], "references": ["make some basic checks on the inputs to make sure they are valid"], "bleu": 0.13044969897820202, "rouge_l": 0.3794712286158632}
{"id": 5332, "code": "def check function ( self ) : if not callable ( self . function ) : raise Runtime Error ( \"provided function '{0}' is not callable\" . format ( str ( self . function ) ) ) from inspect import getargspec arg info = getargspec ( self . function ) if len ( arg info . args ) != 1 : print str ( arg info ) raise Runtime Error ( \"provided function should have one argument but found \" \"{0}\" . format ( len ( arg info . args ) ) )", "predictions": ["check if the function is valid ."], "references": ["make some basic checks on the function to make sure it is valid"], "bleu": 0.13044969897820202, "rouge_l": 0.3794712286158632}
{"id": 5333, "code": "def recurse ( self , inputs , output , depth , max depth ) : if depth < max depth : for index , option in enumerate ( inputs ) : my output = list ( output ) my output . append ( option ) self . recurse ( inputs [ index + 1 : ] , my output , depth + 1 , max depth ) else : self . options . append ( output )", "predictions": ["recurse down the specified list of inputs ."], "references": ["we work out all combinations using this internal recursion method"], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 5334, "code": "def to string ( obj ) : if isinstance ( obj , Literal Wrapper ) : val = obj . obj elif isinstance ( obj , Iterable ) and not isinstance ( obj , str ) : val = next ( obj , None ) else : val = obj if val is None : yield '' elif isinstance ( val , str ) : yield val elif isinstance ( val , node ) : yield strval ( val ) elif isinstance ( val , int ) or isinstance ( val , float ) : yield str ( val ) elif isinstance ( item , bool ) : yield 'true' if item else 'false' else : raise Runtime Error ( 'Unknown type for string conversion: {}' . format ( val ) )", "predictions": ["helper function to convert an object to a string"], "references": ["cast an arbitrary object or sequence to a string type"], "bleu": 0.2601435417217584, "rouge_l": 0.5213675213675214}
{"id": 5335, "code": "def to number ( obj ) : if isinstance ( obj , Literal Wrapper ) : val = obj . obj elif isinstance ( obj , Iterable ) and not isinstance ( obj , str ) : val = next ( obj , None ) else : val = obj if val is None : #FIXME: Should be Na N, not 0 yield 0 elif isinstance ( val , str ) : yield float ( val ) elif isinstance ( val , node ) : yield float ( strval ( val ) ) elif isinstance ( val , int ) or isinstance ( val , float ) : yield val else : raise Runtime Error ( 'Unknown type for number conversion: {}' . format ( val ) )", "predictions": ["helper function to convert an object to a number of objects"], "references": ["cast an arbitrary object or sequence to a number type"], "bleu": 0.23462350320527994, "rouge_l": 0.4803149606299213}
{"id": 5336, "code": "def to boolean ( obj ) : #if hasattr(obj, ' iter '): if isinstance ( obj , Literal Wrapper ) : val = obj . obj elif isinstance ( obj , Iterable ) and not isinstance ( obj , str ) : val = next ( obj , None ) else : val = obj if val is None : yield False elif isinstance ( val , bool ) : yield val elif isinstance ( val , str ) : yield bool ( str ) elif isinstance ( val , node ) : yield True elif isinstance ( val , float ) or isinstance ( val , int ) : yield bool ( val ) else : raise Runtime Error ( 'Unknown type for boolean conversion: {}' . format ( val ) )", "predictions": ["helper function to convert an object to a boolean ."], "references": ["cast an arbitrary sequence to a boolean type"], "bleu": 0.24808415001701817, "rouge_l": 0.4535315985130111}
{"id": 5337, "code": "def intersect ( self , other ) : inter = Envelope ( tuple ( self ) ) if inter . intersects ( other ) : mid = len ( other ) // 2 inter . ll = map ( max , inter . ll , other [ : mid ] ) inter . ur = map ( min , inter . ur , other [ mid : ] ) else : inter . ll = ( 0 , 0 ) inter . ur = ( 0 , 0 ) return inter", "predictions": ["intersect the inter - qubit with the given other ."], "references": ["returns the intersection of this and another envelope ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 5338, "code": "def polygon ( self ) : ring = ogr . Geometry ( ogr . wkb Linear Ring ) for coord in self . ll , self . lr , self . ur , self . ul , self . ll : ring . Add Point 2D ( * coord ) polyg = ogr . Geometry ( ogr . wkb Polygon ) polyg . Add Geometry Directly ( ring ) return polyg", "predictions": ["return the ogr polygon of this ring ."], "references": ["returns an ogr geometry for this envelope ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 5339, "code": "def from name ( cls , name ) : filename = os . path . join ( package dir , 'data' , name + '.txt' ) return cls . from file ( filename , name )", "predictions": ["return a filename from a package name ."], "references": ["imports a mass table from a file"], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 5340, "code": "def from file ( cls , filename , name = '' ) : df = pd . read csv ( filename , header = 0 , delim whitespace = True , index col = [ 0 , 1 ] ) [ 'M' ] df . name = name return cls ( df = df , name = name )", "predictions": ["create a pd from a file ."], "references": ["imports a mass table from a file"], "bleu": 0.3655552228545123, "rouge_l": 0.5714285714285714}
{"id": 5341, "code": "def odd even ( self ) : return self . select ( lambda Z , N : ( Z % 2 ) and not ( N % 2 ) , name = self . name )", "predictions": ["returns a string of even even if the class is not even ."], "references": ["selects odd - even nuclei from the table"], "bleu": 0.10571070857151538, "rouge_l": 0.19902120717781402}
{"id": 5342, "code": "def even odd ( self ) : return self . select ( lambda Z , N : not ( Z % 2 ) and ( N % 2 ) , name = self . name )", "predictions": ["returns a list of the class s name ."], "references": ["selects even - odd nuclei from the table"], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 5343, "code": "def even even ( self ) : return self . select ( lambda Z , N : not ( Z % 2 ) and not ( N % 2 ) , name = self . name )", "predictions": ["returns a list of even even if the class is not even ."], "references": ["selects even - even nuclei from the table"], "bleu": 0.1135935489027116, "rouge_l": 0.2985318107667211}
{"id": 5344, "code": "def binding energy ( self ) : M P = 938.2723 M E = 0.5110 M N = 939.5656 AMU = 931.494028 df = self . Z * ( M P + M E ) + ( self . A - self . Z ) * M N - ( self . df + self . A * AMU ) return Table ( df = df , name = 'BE' + '(' + self . name + ')' )", "predictions": ["returns the options of the parse parse"], "references": ["return binding energies instead of mass excesses"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 5345, "code": "def s2n ( self ) : M N = 8.0713171 f = lambda parent , daugther : - parent + daugther + 2 * M N return self . derived ( 's2n' , ( 0 , - 2 ) , f )", "predictions": ["returns the number of not - th not - based likelihood"], "references": ["return 2 neutron separation energy"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 5346, "code": "def s1n ( self ) : M N = 8.0713171 f = lambda parent , daugther : - parent + daugther + M N return self . derived ( 's1n' , ( 0 , - 1 ) , f )", "predictions": ["the get number of the ."], "references": ["return 1 neutron separation energy"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 5347, "code": "def s2p ( self ) : M P = 7.28897050 f = lambda parent , daugther : - parent + daugther + 2 * M P return self . derived ( 's2p' , ( - 2 , 0 ) , f )", "predictions": ["compute the distance between the two matrices"], "references": ["return 2 proton separation energy"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5348, "code": "def s1p ( self ) : M P = 7.28897050 f = lambda parent , daugther : - parent + daugther + M P return self . derived ( 's1p' , ( - 1 , 0 ) , f )", "predictions": ["service - 1 - based geometry"], "references": ["return 1 proton separation energy"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 5349, "code": "def derived ( self , name , relative coords , formula ) : rel Z , rel N = relative coords daughter idx = [ ( x [ 0 ] + rel Z , x [ 1 ] + rel N ) for x in self . df . index ] values = formula ( self . df . values , self . df . loc [ daughter idx ] . values ) return Table ( df = pd . Series ( values , index = self . df . index , name = name + '(' + self . name + ')' ) )", "predictions": ["returns the config values for the given config"], "references": ["helper function for derived quantities"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 5350, "code": "def derive key ( self , master password ) : encoder = encoding . Encoder ( self . charset ) bytes = ( '%s:%s' % ( master password , self . name ) ) . encode ( 'utf8' ) start time = time . clock ( ) digest = scrypt . hash ( bytes , self . salt , N = 1 << 14 , r = 8 , p = 1 ) key = encoder . encode ( digest , self . key length ) derivation time in s = time . clock ( ) - start time logger . debug ( 'Key derivation took %.2fms' , derivation time in s * 1000 ) return key", "predictions": ["derives our for the current for the for the for the for the for the for the for the for the for the for the for the for the for the"], "references": ["computes the key from the salt and the master password ."], "bleu": 0.046398855339878003, "rouge_l": 0.15627668659265584}
{"id": 5351, "code": "def search ( self , query ) : results = self . session . query ( Domain ) . filter ( Domain . name . ilike ( '%%%s%%' % query ) ) . all ( ) return results", "predictions": ["query elasticsearch elasticsearch type ."], "references": ["search the database for the given query . will find partial matches ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 5352, "code": "def srid ( self ) : epsg id = ( self . Get Authority Code ( 'PROJCS' ) or self . Get Authority Code ( 'GEOGCS' ) ) try : return int ( epsg id ) except Type Error : return", "predictions": ["in the tag if any ."], "references": ["returns the epsg id as int if it exists ."], "bleu": 0.13487005099534619, "rouge_l": 0.3588235294117647}
{"id": 5353, "code": "def main ( ) : args = get args ( ) ret code = args . target ( args ) logger . debug ( 'Exiting with code %d' , ret code ) sys . exit ( ret code )", "predictions": ["create a cli cli = 0 = 1 = 0 = 1 = 1 = 1 = 2 = 1 = 0 = 0 = 1 = 0 = 0 ="], "references": ["main entry point for the cli ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 5354, "code": "def update file ( url , filename ) : resp = urlopen ( url ) if resp . code != 200 : raise Exception ( 'GET {} failed.' . format ( url ) ) with open ( get package path ( filename ) , 'w' ) as fp : for l in resp : if not l . startswith ( b'#' ) : fp . write ( l . decode ( 'utf8' ) ) print ( 'Updated {}' . format ( filename ) )", "predictions": ["create a underline underline from the given self . ."], "references": ["update the content of a single file ."], "bleu": 0.14991106946711685, "rouge_l": 0.22676579925650556}
{"id": 5355, "code": "def driver ( self ) : if self . driver is None : self . driver = Image Driver ( self . ds . Get Driver ( ) ) return self . driver", "predictions": ["keys of this object ."], "references": ["returns the underlying imagedriver instance ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 5356, "code": "def lookup alphabet ( charset ) : if charset in PRESETS : return PRESETS [ charset ] if len ( charset ) < 16 : logger . warning ( 'very small alphabet in use, possibly a failed lookup?' ) return charset", "predictions": ["remove element from self stripped empty"], "references": ["retrieves a named charset or treats the input as a custom alphabet and use that"], "bleu": 0.04144655666113052, "rouge_l": 0.0}
{"id": 5357, "code": "def chunk to long ( self , chunk ) : return sum ( [ 256 ** ( self . chunklen [ 0 ] - 1 - i ) * ord byte ( chunk [ i ] ) for i in range ( self . chunklen [ 0 ] ) ] )", "predictions": ["convert a boolean to a long long"], "references": ["parses a chunk of bytes to integer using big - endian representation"], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 5358, "code": "def get chunk ( self , data , index ) : return data [ index * self . chunklen [ 0 ] : ( index + 1 ) * self . chunklen [ 0 ] ]", "predictions": ["return a arg at the given index"], "references": ["partition the data into chunks and retrieve the chunk at the given index"], "bleu": 0.21938936848339244, "rouge_l": 0.3794712286158632}
{"id": 5359, "code": "def memoize ( func ) : cache = { } @ wraps ( func ) def inner ( filename ) : if filename not in cache : cache [ filename ] = func ( filename ) return cache [ filename ] return inner", "predictions": ["decorator to obj a function string string string string string ."], "references": ["cache result of function call ."], "bleu": 0.12605968092174913, "rouge_l": 0.2484725050916497}
{"id": 5360, "code": "def regexp ( filename ) : lines = get resource content ( filename ) . decode ( 'utf-8' ) . splitlines ( ) return re . compile ( '|' . join ( lines ) )", "predictions": ["concat the flatten flatten file ."], "references": ["get a list of patterns from a file and make a regular expression ."], "bleu": 0.06443935473636557, "rouge_l": 0.18654434250764526}
{"id": 5361, "code": "def detect timezone ( ) : default timezone = 'America/New York' locale code = locale . getdefaultlocale ( ) return default timezone if not locale code [ 0 ] else str ( pytz . country timezones [ locale code [ 0 ] [ - 2 : ] ] [ 0 ] )", "predictions": ["starts the with the with the with the full with the full name of the full with the full name"], "references": ["get timezone as set by the system"], "bleu": 0.06108557268562171, "rouge_l": 0.08111702127659574}
{"id": 5362, "code": "def to dict ( self ) : result = { } for attr , in iteritems ( self . swagger types ) : value = getattr ( self , attr ) if isinstance ( value , list ) : result [ attr ] = list ( map ( lambda x : x . to dict ( ) if hasattr ( x , \"to dict\" ) else x , value ) ) elif hasattr ( value , \"to dict\" ) : result [ attr ] = value . to dict ( ) else : result [ attr ] = value return result", "predictions": ["save this swagger instance into a dictionary ."], "references": ["returns the model properties as a dict"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 5363, "code": "def activate pdb hook ( ) : def debug exception ( type exception , value , tb ) : import pdb pdb . post mortem ( tb ) import sys sys . excepthook = debug exception", "predictions": ["logs a inputs in the inputs of the inputs . . . . . . . ."], "references": ["catch exceptions with a prompt for post - mortem analyzis"], "bleu": 0.07223943354597204, "rouge_l": 0.07770700636942676}
{"id": 5364, "code": "def worker main ( job handler , host , port ) : loop = asyncio . new event loop ( ) asyncio . set event loop ( None ) loop . run until complete ( handle jobs ( job handler , host , port , loop = loop ) ) loop . close ( )", "predictions": ["simple loop that runs the asyncio . . . . . . . . . . ."], "references": ["starts an asyncio event loop to connect to the master and run jobs ."], "bleu": 0.09083627868206415, "rouge_l": 0.19698600645855757}
{"id": 5365, "code": "def send message ( self , msg ) : LW Link . the queue . put nowait ( msg ) if LW Link . thread is None or not LW Link . thread . is Alive ( ) : LW Link . thread = Thread ( target = self . send queue ) LW Link . thread . start ( )", "predictions": ["start a message enumerate"], "references": ["add message to queue and start processing the queue ."], "bleu": 0.08872444253557525, "rouge_l": 0.13260869565217392}
{"id": 5366, "code": "def turn on light ( self , device id , name ) : msg = \"!%s Fd P32|Turn On|%s\" % ( device id , name ) self . send message ( msg )", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["create the message to turn light on ."], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 5367, "code": "def turn on switch ( self , device id , name ) : msg = \"!%s F1|Turn On|%s\" % ( device id , name ) self . send message ( msg )", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["create the message to turn switch on ."], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 5368, "code": "def turn on with brightness ( self , device id , name , brightness ) : brightness value = round ( ( brightness * 31 ) / 255 ) + 1 msg = \"!%s Fd P%d|Lights %d|%s\" % ( device id , brightness value , brightness value , name ) self . send message ( msg )", "predictions": ["to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to"], "references": ["scale brightness from 0 .. 255 to 1 .. 32 ."], "bleu": 0.03901663112717908, "rouge_l": 0.05209222886421862}
{"id": 5369, "code": "def turn off ( self , device id , name ) : msg = \"!%s F0|Turn Off|%s\" % ( device id , name ) self . send message ( msg )", "predictions": ["intersect an device off off . . . ."], "references": ["create the message to turn light or switch off ."], "bleu": 0.16621692209732, "rouge_l": 0.20854700854700853}
{"id": 5370, "code": "def send queue ( self ) : while not LW Link . the queue . empty ( ) : self . send reliable message ( LW Link . the queue . get nowait ( ) )", "predictions": ["polygon the queue queue wkb wkb wkb wkb wkb wkb wkb wkb wkb wkb wkb wkb wkb"], "references": ["if the queue is not empty process the queue ."], "bleu": 0.10216198665886358, "rouge_l": 0.2331210191082802}
{"id": 5371, "code": "def send reliable message ( self , msg ) : result = False max retries = 15 trans id = next ( LW Link . transaction id ) msg = \"%d,%s\" % ( trans id , msg ) try : with socket . socket ( socket . AF INET , socket . SOCK DGRAM ) as write sock , socket . socket ( socket . AF INET , socket . SOCK DGRAM ) as read sock : write sock . setsockopt ( socket . SOL SOCKET , socket . SO REUSEADDR , 1 ) read sock . setsockopt ( socket . SOL SOCKET , socket . SO BROADCAST , 1 ) read sock . settimeout ( self . SOCKET TIMEOUT ) read sock . bind ( ( '0.0.0.0' , self . RX PORT ) ) while max retries : max retries -= 1 write sock . sendto ( msg . encode ( 'UTF-8' ) , ( LW Link . link ip , self . TX PORT ) ) result = False while True : response , dummy = read sock . recvfrom ( 1024 ) response = response . decode ( 'UTF-8' ) if \"Not yet registered.\" in response : LOGGER . error ( \"Not yet registered\" ) self . register ( ) result = True break if response . startswith ( \"%d,OK\" % trans id ) : result = True break if response . startswith ( \"%d,ERR\" % trans id ) : LOGGER . error ( response ) break LOGGER . info ( response ) if result : break time . sleep ( 0.25 ) except socket . timeout : LOGGER . error ( \"LW broker timeout!\" ) return result except Exception as ex : LOGGER . error ( ex ) raise if result : LOGGER . info ( \"LW broker OK!\" ) else : LOGGER . error ( \"LW broker fail!\" ) return result", "predictions": ["bind a name of the name of the name of the name of the server . . . . . . . . . . . . . . . ."], "references": ["send msg to lightwaverf hub ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 5372, "code": "def reset ( self ) : for opt , meta in self . defaults ( ) : self [ opt ] = meta . default", "predictions": ["from the config object ."], "references": ["restore default values of options in this section ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 5373, "code": "def names ( section , option ) : meta = section . def [ option ] action = meta . cmd kwargs . get ( 'action' ) if action is internal . Switch : names = [ '-{}' . format ( option ) , '+{}' . format ( option ) ] if meta . shortname is not None : names . append ( '-{}' . format ( meta . shortname ) ) names . append ( '+{}' . format ( meta . shortname ) ) else : names = [ '--{}' . format ( option ) ] if meta . shortname is not None : names . append ( '-{}' . format ( meta . shortname ) ) return names", "predictions": ["return the action odd odd"], "references": ["list of cli strings for a given option ."], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 5374, "code": "def cmd opts solver ( self , cmd name ) : sections = self . sections list ( cmd name ) cmd dict = self . opt cmds [ cmd name ] if cmd name else self . opt bare for sct in reversed ( sections ) : for opt , opt meta in self . conf [ sct ] . def . items ( ) : if not opt meta . cmd arg : continue if opt not in cmd dict : cmd dict [ opt ] = sct else : warnings . warn ( 'Command <{0}>: {1}.{2} shadowed by {3}.{2}' . format ( cmd name , sct , opt , cmd dict [ opt ] ) , error . Loam Warning , stacklevel = 4 )", "predictions": ["execute command in the module . ."], "references": ["scan options related to one command and enrich _opt_cmds ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 5375, "code": "def add options to parser ( self , opts dict , parser ) : store bool = ( 'store true' , 'store false' ) for opt , sct in opts dict . items ( ) : meta = self . conf [ sct ] . def [ opt ] kwargs = copy . deepcopy ( meta . cmd kwargs ) action = kwargs . get ( 'action' ) if action is internal . Switch : kwargs . update ( nargs = 0 ) elif meta . default is not None and action not in store bool : kwargs . setdefault ( 'type' , type ( meta . default ) ) kwargs . update ( help = meta . help ) kwargs . setdefault ( 'default' , self . conf [ sct ] [ opt ] ) parser . add argument ( * names ( self . conf [ sct ] , opt ) , * * kwargs )", "predictions": ["even options options options options"], "references": ["add options to a parser ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 5376, "code": "async def start master ( host = \"\" , port = 48484 , * , loop = None ) : loop = loop if loop is not None else asyncio . get event loop ( ) manager = jobs . Job Manager ( loop = loop ) workers = set ( ) server = await loop . create server ( lambda : Worker Protocol ( manager , workers ) , host , port ) return Master ( server , manager , workers , loop = loop )", "predictions": ["start a new master server ."], "references": ["starts a new highfive master at the given host and port and returns it ."], "bleu": 0.07370355832749997, "rouge_l": 0.3536231884057971}
{"id": 5377, "code": "def run ( self , job list ) : if self . closed : raise Runtime Error ( \"master is closed\" ) return self . manager . add job set ( job list )", "predictions": ["run a job list ."], "references": ["runs a job set which consists of the jobs in an iterable job list ."], "bleu": 0.0781358622075126, "rouge_l": 0.3669172932330827}
{"id": 5378, "code": "def add ( self , result ) : assert not self . complete self . results . append ( result ) self . change ( )", "predictions": ["add a result to the queue ."], "references": ["adds a new result ."], "bleu": 0.22089591134157885, "rouge_l": 0.5154929577464789}
{"id": 5379, "code": "def done ( self ) : self . results . complete ( ) waiters = self . waiters for waiter in waiters : waiter . set result ( None ) self . manager . job set done ( self )", "predictions": ["mark all done as done ."], "references": ["marks the job set as completed and notifies all waiting tasks ."], "bleu": 0.09663861439684919, "rouge_l": 0.20962199312714777}
{"id": 5380, "code": "def get job ( self , callback ) : assert not self . closed if self . active js is None or not self . active js . job available ( ) : self . ready callbacks . append ( callback ) else : job = self . active js . get job ( ) self . job sources [ job ] = self . active js callback ( job )", "predictions": ["get a job from the active job ."], "references": ["calls the given callback function when a job becomes available ."], "bleu": 0.16481400866629634, "rouge_l": 0.3070469798657718}
{"id": 5381, "code": "def return job ( self , job ) : if self . closed : return js = self . job sources [ job ] if len ( self . ready callbacks ) > 0 : callback = self . ready callbacks . popleft ( ) callback ( job ) else : del self . job sources [ job ] js . return job ( job )", "predictions": ["return a job object for the given job ."], "references": ["returns a job to its source job set to be run again later ."], "bleu": 0.12109261383365659, "rouge_l": 0.3347050754458162}
{"id": 5382, "code": "def uniquify ( list ) : seen = set ( ) result = [ ] for x in list : if x not in seen : result . append ( x ) seen . add ( x ) return result", "predictions": ["return a list of all keys in a list"], "references": ["remove duplicates in a list ."], "bleu": 0.2626909894424158, "rouge_l": 0.4149659863945578}
{"id": 5383, "code": "def get region ( ) : global REGION if REGION is None : region name = os . getenv ( \"AWS DEFAULT REGION\" ) or \"us-east-1\" region dict = { r . name : r for r in boto . regioninfo . get regions ( \"ec2\" ) } if region name not in region dict : raise Value Error ( \"No such EC2 region: {}. Check AWS DEFAULT REGION \" \"environment variable\" . format ( region name ) ) REGION = region dict [ region name ] return REGION", "predictions": ["get the global region for the current region ."], "references": ["use the environment to get the current region"], "bleu": 0.31239399369202553, "rouge_l": 0.4756335282651072}
{"id": 5384, "code": "def sort by ( cls , entries , attribute ) : def key ( entry ) : return entry . get attrib ( attribute , convert to str = True ) return sorted ( entries , key = key )", "predictions": ["sort entries by attribute"], "references": ["sorts a list of entries by the given attribute ."], "bleu": 0.11337974147240094, "rouge_l": 0.3978260869565217}
{"id": 5385, "code": "def add timestamp ( logger class , log method , event dict ) : event dict [ 'timestamp' ] = calendar . timegm ( time . gmtime ( ) ) return event dict", "predictions": ["add a timestamp to the log file ."], "references": ["attach the event time as unix epoch"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 5386, "code": "def logger ( name = name , output = None , uuid = False , timestamp = False ) : processors = [ ] if output == 'json' : processors . append ( structlog . processors . JSON Renderer ( ) ) if uuid : processors . append ( add unique id ) if uuid : processors . append ( add timestamp ) return structlog . wrap logger ( logbook . Logger ( name ) , processors = processors )", "predictions": ["add a unique logger ."], "references": ["configure and return a new logger for hivy modules"], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 5387, "code": "def setup ( title , output = 'json' , timezone = None ) : timezone = timezone or dna . time utils . detect timezone ( ) broker url = 'redis://{}:{}/{}' . format ( os . environ . get ( 'BROKER HOST' , 'localhost' ) , os . environ . get ( 'BROKER PORT' , 6379 ) , 0 ) app = Celery ( title , broker = broker url ) app . conf . update ( CELERY TASK SERIALIZER = output , CELERY ACCEPT CONTENT = [ output ] , CELERY RESULT SERIALIZER = output , CELERY RESULT BACKEND = broker url , CELERY TIMEZONE = timezone , CELERYD FORCE EXECV = True , CELERY ENABLE UTC = True , CELERY IGNORE RESULT = False ) return app", "predictions": ["sets up the app with the given title ."], "references": ["implement celery workers using json and redis"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5388, "code": "def delete ( self , worker id ) : code = 200 if worker id in self . jobs : self . jobs [ worker id ] [ 'worker' ] . revoke ( terminate = True ) report = { 'id' : worker id , 'revoked' : True } self . jobs . pop ( worker id ) else : report = { 'error' : 'job {} unknown' . format ( worker id ) } code = 404 return flask . jsonify ( report ) , code", "predictions": ["delete a worker ."], "references": ["stop and remove a worker"], "bleu": 0.36827215283744186, "rouge_l": 0.43571428571428567}
{"id": 5389, "code": "def color ( number ) : if supports 256 ( ) : template = \"\\033[38;5;{number}m{text}\\033[0m\" else : template = \"\\033[{number}m{text}\\033[0m\" def color ( text ) : if not all ( [ sys . stdout . isatty ( ) , sys . stderr . isatty ( ) ] ) : return text else : return template . format ( number = number , text = text ) return color", "predictions": ["return a string to display in the color ."], "references": ["returns a function that colors a string with a number from 0 to 255 ."], "bleu": 0.10835843183417554, "rouge_l": 0.3189542483660131}
{"id": 5390, "code": "def get color hash ( string , min = MIN COLOR BRIGHT , max = MAX COLOR BRIGHT ) : hash num = int ( hashlib . sha1 ( string . encode ( 'utf-8' ) ) . hexdigest ( ) [ : 6 ] , 16 ) range = max - min num in range = hash num % range return color ( min + num in range )", "predictions": ["get a color hash from a string ."], "references": ["hashes a string and returns a number between min and max ."], "bleu": 0.14544785215055717, "rouge_l": 0.28955696202531644}
{"id": 5391, "code": "def random color ( min = MIN COLOR , max = MAX COLOR ) : return color ( random . randint ( min , max ) )", "predictions": ["returns a random color color ."], "references": ["returns a random color between min and max ."], "bleu": 0.3902775415079818, "rouge_l": 0.6434599156118143}
{"id": 5392, "code": "def requires token auth ( resource ) : @ functools . wraps ( resource ) def decorated ( * args , * * kwargs ) : ''' Check provided token ''' token = flask . request . headers . get ( 'Authorization' ) user = check token ( token ) if not token or user is None : log . warn ( 'authentification failed' , token = token ) return auth failed ( ) flask . g . user = user log . info ( 'authentification succeeded' , token = token , user = flask . g . user ) return resource ( * args , * * kwargs ) return decorated", "predictions": ["decorator to require user auth token ."], "references": ["flask decorator protecting ressources using token scheme"], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 5393, "code": "def is running ( process ) : try : pgrep = sh . Command ( '/usr/bin/pgrep' ) pgrep ( process ) flag = True except sh . Error Return Code 1 : flag = False return flag", "predictions": ["check if a process is running"], "references": ["pgrep returns an error code if no process was found"], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 5394, "code": "def dynamic import ( mod path , obj name = None ) : try : module = import ( mod path , fromlist = [ 'whatever' ] ) except Import Error , error : raise errors . Dynamic Import Failed ( module = '.' . join ( [ mod path , obj name ] ) , reason = error ) reload ( module ) if obj name is None : obj = module elif hasattr ( module , obj name ) : obj = getattr ( module , obj name ) else : raise errors . Dynamic Import Failed ( module = '.' . join ( [ mod path , obj name ] ) , reason = 'module {} has no attribute {}' . format ( module . name , obj name ) ) return None return obj", "predictions": ["import and return the module ."], "references": ["take a string and return the corresponding module"], "bleu": 0.31149111610852515, "rouge_l": 0.5570776255707762}
{"id": 5395, "code": "def self ip ( public = False ) : try : if public : data = str ( urlopen ( 'http://checkip.dyndns.com/' ) . read ( ) ) ip addr = re . compile ( r'Address: (\\d+\\.\\d+\\.\\d+\\.\\d+)' ) . search ( data ) . group ( 1 ) else : sock = socket . socket ( socket . AF INET , socket . SOCK DGRAM ) sock . connect ( ( 'google.com' , 0 ) ) ip addr = sock . getsockname ( ) [ 0 ] except Exception , error : print ( 'Online test failed : {}' . format ( error ) ) raise return ip addr", "predictions": ["connect to the device ."], "references": ["utility for logbook information injection"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 5396, "code": "def request ( self , method , url , query params = None , headers = None , post params = None , body = None ) : if method == \"GET\" : return self . rest client . GET ( url , query params = query params , headers = headers ) elif method == \"HEAD\" : return self . rest client . HEAD ( url , query params = query params , headers = headers ) elif method == \"OPTIONS\" : return self . rest client . OPTIONS ( url , query params = query params , headers = headers , post params = post params , body = body ) elif method == \"POST\" : return self . rest client . POST ( url , query params = query params , headers = headers , post params = post params , body = body ) elif method == \"PUT\" : return self . rest client . PUT ( url , query params = query params , headers = headers , post params = post params , body = body ) elif method == \"PATCH\" : return self . rest client . PATCH ( url , query params = query params , headers = headers , post params = post params , body = body ) elif method == \"DELETE\" : return self . rest client . DELETE ( url , query params = query params , headers = headers ) else : raise Value Error ( \"http method must be `GET`, `HEAD`,\" \" `POST`, `PATCH`, `PUT` or `DELETE`.\" )", "predictions": ["make a request to the api ."], "references": ["makes the http request using restclient ."], "bleu": 0.22089591134157885, "rouge_l": 0.2857142857142857}
{"id": 5397, "code": "def serve ( self , app docopt = DEFAULT DOC , description = '' ) : exit status = 0 if isinstance ( app docopt , str ) : args = docopt ( app docopt , version = description ) elif isinstance ( app docopt , dict ) : args = app docopt else : raise Value Error ( 'unknown configuration object ({})' . format ( type ( app docopt ) ) ) log level = args . get ( '--log' , 'debug' ) is debug = args . get ( '--debug' , False ) log output = 'stdout' if is debug else 'apy.log' safe bind = args . get ( '--bind' , '127.0.0.1' ) safe port = int ( args . get ( '--port' , 5000 ) ) log setup = dna . logging . setup ( level = log level , output = log output ) with log setup . applicationbound ( ) : try : log . info ( 'server ready' , version = description , log = log level , debug = is debug , bind = '{}:{}' . format ( safe bind , safe port ) ) self . app . run ( host = safe bind , port = safe port , debug = is debug ) except Exception as error : if is debug : raise log . error ( '{}: {}' . format ( type ( error ) . name , str ( error ) ) ) exit status = 1 finally : log . info ( 'session ended with status {}' . format ( exit status ) ) return exit status", "predictions": ["serve the exit status of the configuration ."], "references": ["configure from cli and run the server"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 5398, "code": "def render ( self , name , value , attrs = None ) : context = attrs or { } context . update ( { 'name' : name , 'value' : value , } ) return render to string ( self . template name , context )", "predictions": ["render the template with the given name ."], "references": ["include a hidden input to stored the serialized upload value ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 5399, "code": "def networkdays ( from date , to date , locale = 'en-US' ) : holidays = locales [ locale ] return workdays . networkdays ( from date , to date , holidays )", "predictions": ["convert a date into a datetime object ."], "references": ["return the net work days according to rh s calendar ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 5400, "code": "def get path ( cmd ) : if cmd in PATHS : return PATHS [ cmd ] out = subprocess . check output ( 'which {}' . format ( cmd ) , shell = True ) PATHS [ cmd ] = out . decode ( \"utf-8\" ) . strip ( ) return PATHS [ cmd ]", "predictions": ["return the path to the shell ."], "references": ["queries bash to find the path to a commmand on the system ."], "bleu": 0.16236629910298045, "rouge_l": 0.474339035769829}
{"id": 5401, "code": "def build ssh command ( hostname , username , idfile , ssh command , tunnel ) : command = [ get path ( 'ssh' ) , '-o' , 'Strict Host Key Checking=no' , '-o' , 'Connect Timeout=5' ] if idfile is not None : command . extend ( [ '-i' , idfile ] ) if tunnel is not None : command . extend ( [ '-A' , '-t' , tunnel , 'ssh' , '-A' , '-t' ] ) if username is not None : command . append ( '{}@{}' . format ( username , hostname ) ) else : command . append ( hostname ) if ssh command is not None : command . append ( repr ( ssh command ) ) return ( ' ' . join ( command ) )", "predictions": ["build the ssh ssh command"], "references": ["uses hostname and other info to construct an ssh command ."], "bleu": 0.10822031883953476, "rouge_l": 0.2341650671785029}
{"id": 5402, "code": "def load ( cls , profile name = None ) : lsi location = os . path . expanduser ( '~/.lsi' ) if not os . path . exists ( lsi location ) : return Lsi Profile ( ) cfg parser = Config Parser ( ) cfg parser . read ( lsi location ) if profile name is None : if cfg parser . has section ( 'default' ) : profile name = 'default' else : return cls ( ) elif not cfg parser . has section ( profile name ) : raise cls . Load Error ( 'No such profile {}' . format ( profile name ) ) def get ( option , alt = None ) : \"\"\"Gets an option if it exists; else returns `alt`.\"\"\" if cfg parser . has option ( profile name , option ) : return cfg parser . get ( profile name , option ) else : return alt if cfg parser . has option ( profile name , 'inherit' ) : profile = cls . load ( cfg parser . get ( profile name , 'inherit' ) ) else : profile = cls ( ) profile . override ( 'username' , get ( 'username' ) ) profile . override ( 'identity file' , get ( 'identity file' ) ) profile . override ( 'command' , get ( 'command' ) ) filters = [ s for s in get ( 'filters' , '' ) . split ( ',' ) if len ( s ) > 0 ] exclude = [ s for s in get ( 'exclude' , '' ) . split ( ',' ) if len ( s ) > 0 ] profile . filters . extend ( filters ) profile . exclude . extend ( exclude ) return profile", "predictions": ["load a profile from the config file ."], "references": ["loads the user s lsi profile or provides a default ."], "bleu": 0.13859150907108325, "rouge_l": 0.20469798657718125}
{"id": 5403, "code": "def from args ( args ) : if args . username is not None or args . identity file is not None : profile = Lsi Profile ( ) else : profile = Lsi Profile . load ( args . profile ) profile . override ( 'username' , args . username ) profile . override ( 'identity file' , args . identity file ) profile . override ( 'command' , args . command ) profile . no prompt = args . no prompt profile . filters . extend ( args . filters ) profile . exclude . extend ( args . exclude ) if profile . identity file is not None : profile . identity file = os . path . expanduser ( profile . identity file ) return profile", "predictions": ["create profile from args ."], "references": ["takes arguments parsed from argparse and returns a profile ."], "bleu": 0.11943865131127647, "rouge_l": 0.2515463917525773}
{"id": 5404, "code": "def relate ( self , part , id = None ) : assert part . name . startswith ( self . base ) name = part . name [ len ( self . base ) : ] . lstrip ( '/' ) rel = Relationship ( self , name , part . rel type , id = id ) self . relationships . add ( rel ) return rel", "predictions": ["add a part to a part ."], "references": ["relate this package component to the supplied part ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 5405, "code": "def related ( self , reltype ) : parts = [ ] package = getattr ( self , 'package' , None ) or self for rel in self . relationships . types . get ( reltype , [ ] ) : parts . append ( package [ posixpath . join ( self . base , rel . target ) ] ) return parts", "predictions": ["list of related related related to this project ."], "references": ["return a list of parts related to this one via reltype ."], "bleu": 0.23263472697663287, "rouge_l": 0.5570776255707762}
{"id": 5406, "code": "def load rels ( self , source ) : self . relationships . load ( source = self , data = source )", "predictions": ["load the source model from the relationships ."], "references": ["load relationships from source xml ."], "bleu": 0.21105340631872638, "rouge_l": 0.43990384615384615}
{"id": 5407, "code": "def load part ( self , rel type , name , data ) : if self . content types . find for ( name ) is None : log . warning ( 'no content type found for part %(name)s' % vars ( ) ) return cls = Part . classes by rel type [ rel type ] part = cls ( self , name ) part . load ( data ) self [ name ] = part return part", "predictions": ["load a part from the rel ."], "references": ["load a part into this package based on its relationship type"], "bleu": 0.1952347922420459, "rouge_l": 0.32049036777583184}
{"id": 5408, "code": "def find for ( self , name ) : map = self . items return map . get ( name , None ) or map . get ( get ext ( name ) or None , None )", "predictions": ["def start by = = = = = = = = = port * = * = name * = name *"], "references": ["get the correct content type for a given name"], "bleu": 0.05538696232597745, "rouge_l": 0.06979405034324943}
{"id": 5409, "code": "def from element ( cls , element ) : ns , class name = parse tag ( element . tag ) class = getattr ( Content Type , class name ) if not class : msg = 'Invalid Types child element: %(class name)s' % vars ( ) raise Value Error ( msg ) key = element . get ( class . key name ) name = element . get ( 'Content Type' ) return class ( name , key )", "predictions": ["factory method to create an element from an element element . ."], "references": ["given an element parse out the proper contenttype"], "bleu": 0.1367440667823257, "rouge_l": 0.2074829931972789}
{"id": 5410, "code": "def as stream ( self ) : stream = io . Bytes IO ( ) self . store ( stream ) stream . seek ( 0 ) return stream", "predictions": ["returns the stream as a stream object ."], "references": ["return a zipped package as a readable stream"], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 5411, "code": "def loud ( self , lang = 'englist' ) : lang method = getattr ( self , lang , None ) if lang method : return lang method ( ) . upper ( ) else : return self . english ( ) . upper ( )", "predictions": ["convert a = lang to a class"], "references": ["speak loudly! five! use upper case!"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5412, "code": "def upload ( ctx , product , git ref , dirname , aws id , aws secret , ci env , on travis push , on travis pr , on travis api , on travis cron , skip upload ) : logger = logging . get Logger ( name ) if skip upload : click . echo ( 'Skipping ltd upload.' ) sys . exit ( 0 ) logger . debug ( 'CI environment: %s' , ci env ) logger . debug ( 'Travis events settings. ' 'On Push: %r, PR: %r, API: %r, Cron: %r' , on travis push , on travis pr , on travis api , on travis cron ) if ci env == 'travis' and should skip travis event ( on travis push , on travis pr , on travis api , on travis cron ) : sys . exit ( 0 ) ensure login ( ctx ) git refs = get git refs ( ci env , git ref ) build resource = register build ( ctx . obj [ 'keeper hostname' ] , ctx . obj [ 'token' ] , product , git refs ) logger . debug ( 'Created build resource %r' , build resource ) upload dir ( build resource [ 'bucket name' ] , build resource [ 'bucket root dir' ] , dirname , aws access key id = aws id , aws secret access key = aws secret , surrogate key = build resource [ 'surrogate key' ] , cache control = 'max-age=31536000' , surrogate control = None , upload dir redirect objects = True ) logger . debug ( 'Upload complete for %r' , build resource [ 'self url' ] ) confirm build ( build resource [ 'self url' ] , ctx . obj [ 'token' ] ) logger . debug ( 'Build %r complete' , build resource [ 'self url' ] )", "predictions": ["get a specific callback for a specific callback . ."], "references": ["upload a new site build to lsst the docs ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 5413, "code": "def part edit cmd ( ) : parser = argparse . Argument Parser ( description = inspect . getdoc ( part edit cmd ) ) parser . add argument ( 'path' , help = 'Path to part (including path to zip file, i.e. ./file.zipx/part)' , ) parser . add argument ( '--reformat-xml' , action = 'store true' , help = ( 'run the content through an XML pretty-printer ' 'first for improved editability' ) , ) args = parser . parse args ( ) part edit ( args . path , args . reformat xml )", "predictions": ["return an job job job job job job closed closed closed closed closed closed"], "references": ["edit a part from an ooxml package without unzipping it"], "bleu": 0.08839374326825923, "rouge_l": 0.08591549295774649}
{"id": 5414, "code": "def pack dir cmd ( ) : parser = argparse . Argument Parser ( description = inspect . getdoc ( part edit cmd ) ) parser . add argument ( 'path' , help = ( 'Path to list (including path to zip file, ' 'i.e. ./file.zipx or ./file.zipx/subdir)' ) , ) args = parser . parse args ( ) for item , is file in sorted ( list contents ( args . path ) ) : prefix = 'd ' if not is file else '  ' msg = prefix + item print ( msg )", "predictions": ["pack command line arguments ."], "references": ["list the contents of a subdirectory of a zipfile"], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 5415, "code": "def process module ( self , node ) : if self . config . file header : if sys . version info [ 0 ] < 3 : pattern = re . compile ( '\\A' + self . config . file header , re . LOCALE | re . MULTILINE ) else : pattern = re . compile ( '\\A' + self . config . file header , re . MULTILINE ) content = None with node . stream ( ) as stream : content = stream . read ( ) . decode ( 'utf-8' ) matches = pattern . findall ( content ) if len ( matches ) != 1 : self . add message ( 'invalid-file-header' , 1 , args = self . config . file header )", "predictions": ["get region from region ."], "references": ["process the astroid node stream ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 5416, "code": "def html ( self , slug , name , chart obj , filepath = None , html before = \"\" , html after = \"\" ) : try : html = \"\" if name : html = \"<h3>\" + name + \"</h3>\" json data = chart obj . to json ( ) json data = self . patch json ( json data ) html = html before + html + self . json to html ( slug , json data ) + html after except Exception as e : tr . new ( e ) tr . check ( ) if filepath is not None : self . write file ( slug , filepath , html ) return None else : return html", "predictions": ["writes an sort attribute to the attribute"], "references": ["generate html from an altair chart object and optionally write it to a file"], "bleu": 0.07562380261607851, "rouge_l": 0.17967599410898377}
{"id": 5417, "code": "def patch json ( self , json data ) : json data = json . loads ( json data ) json data [ \"$schema\" ] = \"https://vega.github.io/schema/vega-lite/2.0.0-beta.15.json\" json data [ \"width\" ] = json data [ \"config\" ] [ \"cell\" ] [ \"width\" ] json data [ \"height\" ] = json data [ \"config\" ] [ \"cell\" ] [ \"height\" ] del ( json data [ \"config\" ] [ \"cell\" ] ) return json . dumps ( json data )", "predictions": ["add timestamp to timestamp"], "references": ["patch the altair generated json to the newest vega lite spec"], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 5418, "code": "def json to html ( self , slug , json data ) : html = '<div id=\"chart-' + slug + '\"></div>' html += '<script>' html += 'var s' + slug + ' = ' + json data + ';' html += 'vega.embed(\"#chart-' + slug + '\", s' + slug + ');' #html += 'console.log(JSON.stringify(s{id}, null, 2));' html += '</script>' return html", "predictions": ["generates name of name and name in name"], "references": ["generates html from vega lite data"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 5419, "code": "def dict to df ( self , dictobj , xfield , yfield ) : x = [ ] y = [ ] for datapoint in dictobj : x . append ( datapoint ) y . append ( dictobj [ datapoint ] ) df = pd . Data Frame ( { xfield [ 0 ] : x , yfield [ 0 ] : y } ) return df", "predictions": ["transform a setup into a pandas dataframe"], "references": ["converts a dictionnary to a pandas dataframe"], "bleu": 0.3655552228545123, "rouge_l": 0.5714285714285714}
{"id": 5420, "code": "def write file ( self , slug , folderpath , html ) : if not os . path . isdir ( folderpath ) : try : os . makedirs ( folderpath ) except Exception as e : tr . err ( e ) filepath = folderpath + \"/\" + slug + \".html\" #~ write the file try : filex = open ( filepath , \"w\" ) filex . write ( html ) filex . close ( ) except Exception as e : tr . err ( e )", "predictions": ["writes the html to a file"], "references": ["writes a chart s html to a file"], "bleu": 0.46105843756805864, "rouge_l": 0.6963470319634703}
{"id": 5421, "code": "def chart class ( self , df , chart type , * * kwargs ) : if chart type == \"bar\" : return Chart ( df ) . mark bar ( * * kwargs ) elif chart type == \"circle\" : return Chart ( df ) . mark circle ( * * kwargs ) elif chart type == \"line\" : return Chart ( df ) . mark line ( * * kwargs ) elif chart type == \"point\" : return Chart ( df ) . mark point ( * * kwargs ) elif chart type == \"area\" : return Chart ( df ) . mark area ( * * kwargs ) elif chart type == \"tick\" : return Chart ( df ) . mark tick ( * * kwargs ) elif chart type == \"text\" : return Chart ( df ) . mark text ( * * kwargs ) elif chart type == \"square\" : return Chart ( df ) . mark square ( * * kwargs ) elif chart type == \"rule\" : return Chart ( df ) . mark rule ( * * kwargs ) return None", "predictions": ["create a circle color class from a circle color"], "references": ["get the right chart class from a string"], "bleu": 0.2626909894424158, "rouge_l": 0.35672514619883033}
{"id": 5422, "code": "def encode fields ( self , xfield , yfield , time unit = None , scale = Scale ( zero = False ) ) : if scale is None : scale = Scale ( ) xfieldtype = xfield [ 1 ] yfieldtype = yfield [ 1 ] x options = None if len ( xfield ) > 2 : x options = xfield [ 2 ] y options = None if len ( yfield ) > 2 : y options = yfield [ 2 ] if time unit is not None : if x options is None : xencode = X ( xfieldtype , time Unit = time unit ) else : xencode = X ( xfieldtype , axis = Axis ( * * x options ) , time Unit = time unit , scale = scale ) else : if x options is None : xencode = X ( xfieldtype ) else : xencode = X ( xfieldtype , axis = Axis ( * * x options ) , scale = scale ) if y options is None : yencode = Y ( yfieldtype , scale = scale ) else : yencode = Y ( yfieldtype , axis = Axis ( * * y options ) , scale = scale ) return xencode , yencode", "predictions": ["get the = = = = 0 or 1"], "references": ["encode the fields in altair format"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 5423, "code": "def infer tarball url ( ) : try : with click . open file ( 'app.json' , 'r' ) as f : contents = f . read ( ) app json = json . loads ( contents ) except IO Error : return None repository = app json . get ( 'repository' ) if not repository : return None else : return app json . get ( 'repository' ) + '/tarball/master/'", "predictions": ["random color url url"], "references": ["returns the tarball url inferred from an app . json if present ."], "bleu": 0.037870374782798366, "rouge_l": 0.1073943661971831}
{"id": 5424, "code": "def up ( tarball url , auth token , env , app name ) : tarball url = tarball url or infer tarball url ( ) if not tarball url : click . echo ( 'No tarball URL found.' ) sys . exit ( 1 ) if env : env = { arg . split ( '=' ) [ 0 ] : arg . split ( '=' ) [ 1 ] for arg in env } happy = Happy ( auth token = auth token ) click . echo ( 'Creating app... ' , nl = False ) build id , app name = happy . create ( tarball url = tarball url , env = env , app name = app name , ) click . echo ( app name ) click . echo ( 'Building... ' , nl = False ) happy . wait ( build id ) write app name ( app name ) click . echo ( 'done' ) click . echo ( \"It's up! :) https://%s.herokuapp.com\" % app name )", "predictions": ["infer a auth auth auth provided by the auth url provided"], "references": ["brings up a heroku app ."], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 5425, "code": "def down ( auth token , force , app name ) : if not app name : click . echo ( 'WARNING: Inferring the app name when deleting is deprecated. ' 'Starting with happy 2.0, the app name parameter will be required.' ) app name = app name or read app name ( ) if not app name : click . echo ( 'No app name given.' ) sys . exit ( 1 ) if not force : click . confirm ( 'Are you sure you want to delete %s?' % app name , abort = True , ) happy = Happy ( auth token = auth token ) click . echo ( 'Destroying app %s... ' % app name , nl = False ) happy . delete ( app name = app name ) delete app name file ( ) click . echo ( 'done' ) click . echo ( \"It's down. :(\" )", "predictions": ["delete is the database is in the current try mode flag flag flag flag flag flag flag flag flag flag flag flag flag flag flag flag flag flag flag flag flag"], "references": ["brings down a heroku app ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5426, "code": "def date ( start , end ) : stime = date to timestamp ( start ) etime = date to timestamp ( end ) ptime = stime + random . random ( ) * ( etime - stime ) return datetime . date . fromtimestamp ( ptime )", "predictions": ["except for dynamic dynamic dynamic dynamic dynamic dynamic dynamic dynamic dynamic dynamic dynamic dynamic"], "references": ["get a random date between two dates"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 5427, "code": "def get session ( self ) : session = Session ( ) session . headers = { 'Content-type' : 'application/json' , 'Accept' : 'application/vnd.heroku+json; version=3' , } if self . auth token : session . trust env = False session . headers [ 'Authorization' ] = 'Bearer %s' % self . auth token return session", "predictions": ["self try to self try to self try to self try to self try to ip"], "references": ["returns a prepared session instance ."], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 5428, "code": "def get root argparser ( self ) : return self . arg parse class ( description = self . get help ( ) , formatter class = self . get formatter class ( ) )", "predictions": ["params for the root = 0 = 1 = 0 = 0 = 1 = 1 = 0 = 1 = 1 = 1 = 1 = 1 = 0 ="], "references": ["gets the root argument parser object ."], "bleu": 0.0513487742994337, "rouge_l": 0.11879259980525803}
{"id": 5429, "code": "def get description ( self ) : if self . description : return self . description elif self . doc and self . doc . strip ( ) : return self . doc . strip ( ) . split ( '.' ) [ 0 ] + '.' else : return ''", "predictions": ["returns the description description description"], "references": ["gets the description of the command . if its not supplied the first sentence of the doc string is used ."], "bleu": 0.014646027502104971, "rouge_l": 0.13847900113507378}
{"id": 5430, "code": "def get help ( self ) : if self . help : return self . help elif self . doc and self . doc . strip ( ) : return self . doc . strip ( ) else : return ''", "predictions": ["returns the help s help"], "references": ["gets the help text for the command . if its not supplied the doc string is used ."], "bleu": 0.026686802062621924, "rouge_l": 0.15782664941785254}
{"id": 5431, "code": "def get version ( ) : with open ( os . path . join ( os . path . dirname ( file ) , 'argparsetree' , ' init .py' ) ) as init py : return re . search ( ' version  = [\\'\"]([^\\'\"]+)[\\'\"]' , init py . read ( ) ) . group ( 1 )", "predictions": ["return package version ."], "references": ["return package version as listed in __version__ in init . py ."], "bleu": 0.09569649651041093, "rouge_l": 0.45864661654135336}
{"id": 5432, "code": "def url with auth ( regex , view , kwargs = None , name = None , prefix = '' ) : from djapiauth . auth import api auth if isinstance ( view , six . string types ) : return url ( regex , api auth ( import by path ( prefix + \".\" + view if prefix else view ) ) ) elif isinstance ( view , ( list , tuple ) ) : return url ( regex , view , name , prefix , * * kwargs ) else : return url ( regex , api auth ( view ) )", "predictions": ["returns the get get get get get get get get get the get get get get get get get get get get get get get get get get get get get"], "references": ["if view is string based must be a full path"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5433, "code": "def render ( self ) : for opt , values in self . data . items ( ) : if opt == 'ticks' : self [ 'chxtc' ] = '|' . join ( values ) else : self [ 'chx%s' % opt [ 0 ] ] = '|' . join ( values ) return self", "predictions": ["build the string for the element"], "references": ["render the axes data into the dict data"], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 5434, "code": "def dataset ( self , data , series = '' ) : self . dataset = data self . series = series return self", "predictions": ["set the load data for this load"], "references": ["update the chart s dataset can be two dimensional or contain string data"], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 5435, "code": "def render ( self ) : self . update ( self . axes . render ( ) ) encoder = Encoder ( self . encoding , None , self . series ) if not 'chs' in self : self [ 'chs' ] = '300x150' else : size = self [ 'chs' ] . split ( 'x' ) assert len ( size ) == 2 , 'Invalid size, must be in the format Wx H' self . check size ( * map ( int , size ) ) assert 'cht' in self , 'No chart type defined, use type method' self [ 'cht' ] = self . check type ( self [ 'cht' ] ) if ( 'any' in dir ( self . dataset ) and self . dataset . any ( ) ) or self . dataset : self [ 'chd' ] = encoder . encode ( self . dataset ) elif not 'choe' in self : assert 'chd' in self , 'You must have a dataset, or use chd' if self . scale : assert self [ 'chd' ] . startswith ( 't' ) , 'You must use text encoding with chds' self [ 'chds' ] = ',' . join ( self . scale ) if self . geo and self . ld : self [ 'chtm' ] = self . geo self [ 'chld' ] = self . ld if self . lines : self [ 'chls' ] = '|' . join ( self . lines ) if self . markers : self [ 'chm' ] = '|' . join ( self . markers ) if self . fills : self [ 'chf' ] = '|' . join ( self . fills )", "predictions": ["renders the html representation of the element ."], "references": ["renders the chart context and axes into the dict data"], "bleu": 0.176625510283176, "rouge_l": 0.3267857142857143}
{"id": 5436, "code": "def url ( self ) : self . render ( ) return self . apiurl + '&' . join ( self . parts ( ) ) . replace ( ' ' , '+' )", "predictions": ["return s relate relate"], "references": ["returns the rendered url of the chart"], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 5437, "code": "def urlopen ( self ) : req = Request ( str ( self ) ) try : return urlopen ( req ) except HTTP Error : print ( 'The server couldn\\'t fulfill the request.' ) except URL Error : print ( 'We failed to reach a server.' )", "predictions": ["related the for base for base request ."], "references": ["grabs readable png file pointer"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5438, "code": "def parse args ( ) : usage = \"Usage: create concordance <infile> [<outfile>]\" description = \"Simple Concordance Generator\" argparser = argparse . Argument Parser ( usage = usage , description = description ) argparser . add argument ( 'infile' , type = argparse . File Type ( 'r' ) , help = \"File read in to create concordance\" ) argparser . add argument ( 'outfile' , nargs = '?' , type = argparse . File Type ( 'w' ) , default = sys . stdout , help = \"File to write concordance to.  \" \"Default is stdout\" ) argparser . add argument ( '--word' , nargs = \"?\" , const = str , help = \"Display a word in concordance\" ) args = argparser . parse args ( ) return args", "predictions": ["load the command line arguments ."], "references": ["parses command line args using argparse library"], "bleu": 0.24608524656663955, "rouge_l": 0.3034825870646766}
{"id": 5439, "code": "def add Command Line Args ( arg parser ) : arg parser . register ( \"action\" , \"log levels\" , Log Level Action ) arg parser . register ( \"action\" , \"log files\" , Log File Action ) arg parser . register ( \"action\" , \"log help\" , Log Help Action ) group = arg parser . add argument group ( \"Logging options\" ) group . add argument ( \"-l\" , \"--log-level\" , dest = \"log levels\" , action = \"log levels\" , metavar = \"LOGGER:LEVEL\" , default = [ ] , help = \"Set log levels for individual loggers. See --help-logging for \" \"complete details.\" ) group . add argument ( \"-L\" , \"--log-file\" , dest = \"log files\" , action = \"log files\" , metavar = \"LOGGER:FILE\" , default = [ ] , help = \"Set log the output file for individual loggers. \" \" See --help-logging for complete details.\" ) group . add argument ( \"--help-logging\" , action = \"log help\" , help = argparse . SUPPRESS )", "predictions": ["load an individual for"], "references": ["add logging option to an argumentparser ."], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 5440, "code": "def validate page number ( number ) : try : number = int ( number ) except ( Type Error , Value Error ) : raise Page Not An Integer ( 'That page number is not an integer' ) if number < 1 : raise Empty Page ( 'That page number is less than 1' ) return number", "predictions": ["validates that a number is a number or an integer ."], "references": ["validate the given 1 - based page number ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 5441, "code": "def chmod ( path , mode , recursive = True ) : if recursive : cmd = 'chmod -R %s %s' % ( mode , path ) else : cmd = 'chmod %s %s' % ( mode , path ) return sh ( cmd )", "predictions": ["change path to path ."], "references": ["alternative to os ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 5442, "code": "def create bundle ( self , data ) : kwargs = { } filters = None if isinstance ( data , dict ) : kwargs . update ( filters = data . get ( 'filters' , None ) , output = data . get ( 'output' , None ) , debug = data . get ( 'debug' , None ) , extra = data . get ( 'extra' , { } ) , config = data . get ( 'config' , { } ) , depends = data . get ( 'depends' , None ) ) bundle = Bundle ( * list ( self . yield bundle contents ( data ) ) , * * kwargs ) return self . auto filter bundle ( bundle )", "predictions": ["create a bundle from the given data ."], "references": ["return a bundle initialised by the given dict ."], "bleu": 0.2451240194075422, "rouge_l": 0.5820610687022901}
{"id": 5443, "code": "def urls for ( self , asset type , * args , * * kwargs ) : return self . urls for depends ( asset type , * args , * * kwargs ) + self . urls for self ( asset type , * args , * * kwargs )", "predictions": ["returns an instance of the urls for the given asset type ."], "references": ["returns urls needed to include all assets of asset_type"], "bleu": 0.1235622127262679, "rouge_l": 0.19551282051282048}
{"id": 5444, "code": "def html tags for ( self , asset type , * args , * * kwargs ) : html = [ ] for ref in self . depends : html . append ( self . ref ( ref ) . html tags for ( asset type , * args , * * kwargs ) ) if asset type in self . typed bundles : html . append ( render asset html tags ( asset type , self . urls for self ( asset type , * args , * * kwargs ) ) ) return \"\\n\" . join ( html )", "predictions": ["returns html of the html tags for the given asset ."], "references": ["return html tags for urls of asset_type"], "bleu": 0.22416933501922287, "rouge_l": 0.3472485768500949}
{"id": 5445, "code": "def html tags ( self , * args , * * kwargs ) : html = [ ] for asset type in list asset types ( ) : html . append ( self . html tags for ( asset type . name , * args , * * kwargs ) ) return \"\\n\" . join ( html )", "predictions": ["returns html of the html tags ."], "references": ["return all html tags for all asset_type"], "bleu": 0.24446151121745047, "rouge_l": 0.2857142857142857}
{"id": 5446, "code": "def find version ( filename ) : with io . open ( filename , encoding = \"utf-8\" ) as version file : version match = re . search ( r'^ version  = [\\'\"]([^\\'\"]*)[\\'\"]' , version file . read ( ) , re . M ) if version match : return version match . group ( 1 ) return \"0.0-version-unknown\"", "predictions": ["find the version in the package ."], "references": ["uses re to pull out the assigned value to __version__ in filename ."], "bleu": 0.09374222649442905, "rouge_l": 0.2846034214618974}
{"id": 5447, "code": "def import modules ( self ) : modules = self . get modules ( ) log . info ( \"import service modules: \" + str ( modules ) ) try : for module in modules : import ( module ) except Import Error as error : raise Import Modules Error ( error . msg )", "predictions": ["import all modules from the service"], "references": ["import customer s service module ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 5448, "code": "def send ( self , peer , typename , data ) : def attempt to send ( ) : if peer not in self . connections : d = self . connect ( peer ) d . add Callback ( attempt to send ) return d else : conn = self . connections [ peer ] [ 0 ] conn . send packet ( typename , data ) return defer . succeed ( None ) d = attempt to send ( None ) self . ongoing sends . add ( d ) def send completed ( result ) : if d in self . ongoing sends : self . ongoing sends . remove ( d ) return result d . add Both ( send completed ) return d", "predictions": ["send a peer packet to the server ."], "references": ["sends a packet to a peer ."], "bleu": 0.2777619034011791, "rouge_l": 0.5398230088495575}
{"id": 5449, "code": "def receive Data ( self , connection , data ) : try : protocol = self . protocols [ connection ] except Key Error : raise No Such Connection ( ) protocol . data Received ( data ) return { }", "predictions": ["called when a connection is received ."], "references": ["receives some data for the given protocol ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 5450, "code": "def disconnect ( self , connection ) : proto = self . protocols . pop ( connection ) proto . transport = None return { }", "predictions": ["disconnect from the redis server ."], "references": ["disconnects the given protocol ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 5451, "code": "def send Data ( self , data ) : d = self . call Remote ( Transmit , connection = self . connection , data = data ) d . add Errback ( log . err )", "predictions": ["send data to the server ."], "references": ["actually sends data over the wire ."], "bleu": 0.22236312185643822, "rouge_l": 0.45522388059701485}
{"id": 5452, "code": "def get Local Protocol ( self , connection Identifier ) : for factory in self . local Factories : try : return factory . protocols [ connection Identifier ] except Key Error : continue raise No Such Connection ( )", "predictions": ["get all available connection protocols ."], "references": ["attempts to get a local protocol by connection identifier ."], "bleu": 0.13487005099534619, "rouge_l": 0.3588235294117647}
{"id": 5453, "code": "def disconnect ( self , connection ) : proto = self . get Local Protocol ( connection ) proto . transport . lose Connection ( ) return { }", "predictions": ["disconnect from the redis server ."], "references": ["the other side has asked us to disconnect ."], "bleu": 0.1593301391270729, "rouge_l": 0.2573839662447257}
{"id": 5454, "code": "def centered ( mystring , linewidth = None , fill = \" \" ) : if linewidth is None : linewidth = get terminal size ( ) . columns - 1 sides = ( linewidth - length no ansi ( mystring ) ) // 2 extra = ( linewidth - length no ansi ( mystring ) ) % 2 fill = fill [ : 1 ] sidestring = fill * sides extrastring = fill * extra newstring = sidestring + mystring + sidestring + extrastring return newstring", "predictions": ["return the centered of the given mystring"], "references": ["takes a string centres it and pads it on both sides"], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 5455, "code": "def clock on right ( mystring ) : taken = length no ansi ( mystring ) padding = ( get terminal size ( ) . columns - 1 ) - taken - 5 clock = time . strftime ( \"%I:%M\" , time . localtime ( ) ) print ( mystring + \" \" * padding + clock )", "predictions": ["print the right ansi ansi ansi by right ."], "references": ["takes a string and prints it with the time right aligned"], "bleu": 0.12507277759788113, "rouge_l": 0.19645732689210954}
{"id": 5456, "code": "def main ( arguments = None ) : if not arguments : arguments = sys . argv [ 1 : ] wordlist , sowpods , by length , start , end = argument parser ( arguments ) for word in wordlist : pretty print ( word , anagrams in word ( word , sowpods , start , end ) , by length , )", "predictions": ["print the argument of the command line ."], "references": ["main command line entry point ."], "bleu": 0.22679164443904004, "rouge_l": 0.43990384615384615}
{"id": 5457, "code": "def ping ( self , peerid , callid ) : if not ( peerid , callid ) in self . remote to local : logger . warn ( \"No remote call %s from %s. Might just be unfoutunate timing.\" % ( callid , peerid ) )", "predictions": ["ping peerid to remote device"], "references": ["called from remote to ask if a call made to here is still in progress ."], "bleu": 0.03347779366253814, "rouge_l": 0.08701854493580599}
{"id": 5458, "code": "def cmd Regex ( self , cmd grp = None ) : cmd grp = cmd grp or \"cmd\" help opts = ( \"-h\" , \"--help\" ) cmd = self . name ( ) names = \"|\" . join ( [ re . escape ( cmd ) ] + [ re . escape ( a ) for a in self . aliases ( ) ] ) opts = [ ] for action in self . parser . actions : opts += [ a for a in action . option strings if a not in help opts ] opts re = \"|\" . join ( [ re . escape ( o ) for o in opts ] ) if opts re : opts re = rf\"(\\s+(?P<{cmd grp} opts>{opts re}))*\" help re = \"|\" . join ( [ re . escape ( o ) for o in help opts ] ) help re = rf\"(\\s+(?P<HELP OPTS>{help re}))*\" completers = { } if opts re : completers [ f\"{cmd grp} opts\" ] = Word Completer ( opts ) return tuple ( [ rf\"\"\"(?P<{cmd grp}>{names}){opts re}{help re}\"\"\" , completers ] )", "predictions": ["return the command line options for the given command ."], "references": ["get command regex string and completer dict ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 5459, "code": "def from String Proto ( self , in String , proto ) : value , = amp . Amp List . from String Proto ( self , in String , proto ) return value", "predictions": ["returns an instance of the given object ."], "references": ["defers to amp . amplist then gets the element from the list ."], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 5460, "code": "def to String Proto ( self , in Object , proto ) : return amp . Amp List . to String Proto ( self , [ in Object ] , proto )", "predictions": ["returns an equivalent representation of the given object ."], "references": ["wraps the object in a list and then defers to amp . amplist ."], "bleu": 0.09630141125179911, "rouge_l": 0.2510288065843621}
{"id": 5461, "code": "def connection ( username = None , password = None , host = None , port = None , db = None ) : c opts = { } if username : c opts [ 'user' ] = username if password : c opts [ 'password' ] = password if host : c opts [ 'host' ] = host if port : c opts [ 'port' ] = port if db : c opts [ 'database' ] = db dbc = psycopg2 . connect ( * * c opts ) dbc . autocommit = True return dbc", "predictions": ["create a connection to the zoneminder zoneminder api server ."], "references": ["returns a connected cursor to the database - server ."], "bleu": 0.21834177214239062, "rouge_l": 0.5}
{"id": 5462, "code": "def db list ( username = None , password = None , host = None , port = None , maintain db = 'postgres' ) : conn = connection ( username = username , password = password , host = host , port = port , db = maintain db ) cur = conn . cursor ( ) cur . execute ( 'SELECT DATNAME from pg database' ) rows = cur . fetchall ( ) conn . close ( ) result = [ ] for row in rows : result . append ( row [ 0 ] ) return result", "predictions": ["list all connected db ids ."], "references": ["returns a list of all databases on this server"], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 5463, "code": "def get local files ( self , path ) : if not path : raise Value Error ( \"No path specified\" ) files = defaultdict ( lambda : None ) path len = len ( path ) + 1 for root , dirs , filenames in os . walk ( path ) : for name in filenames : full path = join ( root , name ) files [ full path [ path len : ] ] = compute md5 ( full path ) return files", "predictions": ["get the local files in the given path"], "references": ["returns a dictionary of all the files under a path ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 5464, "code": "def tokens required ( service list ) : def decorator ( func ) : @ wraps ( func ) def inner ( request , * args , * * kwargs ) : for service in service list : if service not in request . session [ \"user tokens\" ] : return redirect ( 'denied' ) return func ( request , * args , * * kwargs ) return inner return decorator", "predictions": ["redirect tokens to a list of tokens ."], "references": ["ensure the user has the necessary tokens for the specified services"], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 5465, "code": "def login ( request , template name = 'ci/login.html' , redirect field name = REDIRECT FIELD NAME , authentication form = Authentication Form ) : redirect to = request . POST . get ( redirect field name , request . GET . get ( redirect field name , '' ) ) if request . method == \"POST\" : form = authentication form ( request , data = request . POST ) if form . is valid ( ) : if not is safe url ( url = redirect to , host = request . get host ( ) ) : redirect to = resolve url ( settings . LOGIN REDIRECT URL ) user = form . get user ( ) request . session [ 'user token' ] = user [ \"token\" ] request . session [ 'user email' ] = user [ \"email\" ] request . session [ 'user permissions' ] = user [ \"permissions\" ] request . session [ 'user id' ] = user [ \"id\" ] request . session [ 'user list' ] = user [ \"user list\" ] if not settings . HIDE DASHBOARDS : dashboards = ci Api . get user dashboards ( user [ \"id\" ] ) dashboard list = list ( dashboards [ 'results' ] ) if len ( dashboard list ) > 0 : request . session [ 'user dashboards' ] = dashboard list [ 0 ] [ \"dashboards\" ] request . session [ 'user default dashboard' ] = dashboard list [ 0 ] [ \"default dashboard\" ] [ \"id\" ] else : request . session [ 'user dashboards' ] = [ ] request . session [ 'user default dashboard' ] = None tokens = ci Api . get user service tokens ( params = { \"user id\" : user [ \"id\" ] } ) token list = list ( tokens [ 'results' ] ) user tokens = { } if len ( token list ) > 0 : for token in token list : user tokens [ token [ \"service\" ] [ \"name\" ] ] = { \"token\" : token [ \"token\" ] , \"url\" : token [ \"service\" ] [ \"url\" ] + \"/api/v1\" } request . session [ 'user tokens' ] = user tokens return Http Response Redirect ( redirect to ) else : form = authentication form ( request ) current site = get current site ( request ) context = { 'form' : form , redirect field name : redirect to , 'site' : current site , 'site name' : current site . name , } return Template Response ( request , template name , context )", "predictions": ["displays the login form ."], "references": ["displays the login form and handles the login action ."], "bleu": 0.2925637512788283, "rouge_l": 0.6288659793814433}
{"id": 5466, "code": "def build ( cli , path , package ) : for , name , ispkg in iter modules ( path ) : module = import module ( f'.{name}' , package ) if ispkg : build ( cli . group ( name ) ( module . group ) , module . path , module . package ) else : cli . command ( name ) ( module . command )", "predictions": ["build the modules for a package ."], "references": ["build cli dynamically based on the package structure ."], "bleu": 0.1755217914979255, "rouge_l": 0.48897795591182364}
{"id": 5467, "code": "def descovery ( testdir ) : from os . path import join , exists , isdir , splitext , basename , sep if not testdir or not exists ( testdir ) or not isdir ( testdir ) : return None from os import walk import fnmatch import imp for root , , filenames in walk ( testdir ) : for filename in fnmatch . filter ( filenames , '*.py' ) : path = join ( root , filename ) modulepath = splitext ( root ) [ 0 ] . replace ( sep , '.' ) imp . load source ( modulepath , path )", "predictions": ["load all source files from a directory"], "references": ["descover and load greencard tests ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 5468, "code": "def main ( clargs = None ) : from argparse import Argument Parser from librarian . library import Library import sys parser = Argument Parser ( description = \"A test runner for each card in a librarian library.\" ) parser . add argument ( \"library\" , help = \"Library database\" ) parser . add argument ( \"-t\" , \"--tests\" , default = \"test/\" , help = \"Test directory\" ) args = parser . parse args ( clargs ) descovery ( args . tests ) library = Library ( args . library ) cardcount , passes , failures = execute tests ( library ) print ( RESULTS . format ( len ( SINGLES ) , len ( TESTS ) , cardcount , passes , failures ) ) sys . exit ( failures )", "predictions": ["main entry point for the script ."], "references": ["command line entry point ."], "bleu": 0.2626909894424158, "rouge_l": 0.5154929577464789}
{"id": 5469, "code": "def write Response ( self , response ) : encoded = dumps ( response , default = default ) self . transport . write ( encoded )", "predictions": ["writes the response to the client ."], "references": ["serializes the response to json and writes it to the transport ."], "bleu": 0.20917479021833488, "rouge_l": 0.5024711696869852}
{"id": 5470, "code": "def connection Lost ( self , reason ) : self . remote . box Receiver . stop Receiving Boxes ( reason ) return basic . Netstring Receiver . connection Lost ( self , reason )", "predictions": ["return the connection object for the connection ."], "references": ["tells the box receiver to stop receiving boxes ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 5471, "code": "def build Protocol ( self , addr ) : proto = self . factory . build Protocol ( addr ) return JSONAMP Dialect Receiver ( proto )", "predictions": ["builds an protocol object from an addr ."], "references": ["builds a bridge and associates it with an amp protocol instance ."], "bleu": 0.1223065774797558, "rouge_l": 0.3860759493670886}
{"id": 5472, "code": "def pout ( msg , log = None ) : print ( msg , sys . stdout , log func = log . info if log else None )", "predictions": ["print a message to stdout ."], "references": ["print msg to stdout and option log at info level ."], "bleu": 0.1435549295013305, "rouge_l": 0.4468864468864468}
{"id": 5473, "code": "def perr ( msg , log = None ) : print ( msg , sys . stderr , log func = log . error if log else None )", "predictions": ["print function for logging logging message"], "references": ["print msg to stderr and option log at info level ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 5474, "code": "def register ( Command Sub Class ) : name = Command Sub Class . name ( ) if name in Command . all commands : raise Value Error ( \"Command already exists: \" + name ) Command . all commands [ name ] = Command Sub Class return Command Sub Class", "predictions": ["decorator to create a class = registered commands = value = true ."], "references": ["a class decorator for command classes to register in the default set ."], "bleu": 0.14949751774990683, "rouge_l": 0.23076923076923084}
{"id": 5475, "code": "def register ( Class , Command Sub Class ) : for name in [ Command Sub Class . name ( ) ] + Command Sub Class . aliases ( ) : if name in Class . registered commands [ Class ] : raise Value Error ( \"Command already exists: \" + name ) Class . registered commands [ Class ] [ name ] = Command Sub Class return Command Sub Class", "predictions": ["urls for registering commands return value return a class return none if not registered return none return false return false return false return false otherwise return false ."], "references": ["a class decorator for command classes to register ."], "bleu": 0.06471824245088333, "rouge_l": 0.17871093749999997}
{"id": 5476, "code": "def init mq ( self ) : mq = self . init connection ( ) self . init consumer ( mq ) return mq . connection", "predictions": ["returns an array of tags and the tags in the sqlite database type type type type type type"], "references": ["init connection and consumer with openstack mq ."], "bleu": 0.06809398432036522, "rouge_l": 0.08265582655826557}
{"id": 5477, "code": "def init modules ( self ) : if not self . config : raise Value Error ( \"please read your config file.\" ) log . debug ( \"begin to import customer's service modules.\" ) modules = Service Modules ( self . config ) modules . import modules ( ) log . debug ( \"end to import customer's service modules.\" )", "predictions": ["initialize the return tags from the config"], "references": ["import customer s service modules ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5478, "code": "def music info ( songid ) : if isinstance ( songid , list ) : songid = ',' . join ( songid ) data = { \"hq\" : 1 , \"song Ids\" : songid } res = requests . post ( MUSIC INFO URL , data = data ) info = res . json ( ) music data = info [ \"data\" ] songs = [ ] for song in music data [ \"song List\" ] : song link , size = song link ( song , music data [ \"xcode\" ] ) songs . append ( { \"name\" : song [ \"song Name\" ] , \"singer\" : song [ \"artist Name\" ] , \"lrc link\" : song [ \"lrc Link\" ] , \"song link\" : song link , \"size\" : size } ) return songs", "predictions": ["retrieve information for a particular song"], "references": ["get music info from baidu music api"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 5479, "code": "def download music ( song , thread num = 4 ) : filename = \"{}.mp3\" . format ( song [ \"name\" ] ) if os . path . exists ( filename ) : os . remove ( filename ) part = int ( song [ \"size\" ] / thread num ) if part <= 1024 : thread num = 1 id = uuid . uuid4 ( ) . hex logger . info ( \"downloading '{}'...\" . format ( song [ \"name\" ] ) ) threads = [ ] for i in range ( thread num ) : if i == thread num - 1 : end = '' else : end = ( i + 1 ) * part - 1 thread = Worker ( ( i * part , end ) , song , id ) thread . start ( ) threads . append ( thread ) for t in threads : t . join ( ) file Parts = glob . glob ( \"part-{}-*\" . format ( id ) ) file Parts . sort ( key = lambda e : e . split ( '-' ) [ - 1 ] ) logger . info ( \"'{}' combine parts...\" . format ( song [ \"name\" ] ) ) with open ( filename , \"ab\" ) as f : for part in file Parts : with open ( part , \"rb\" ) as d : shutil . copyfileobj ( d , f ) os . remove ( part ) logger . info ( \"'{}' finished\" . format ( song [ \"name\" ] ) )", "predictions": ["import a thread from a self ."], "references": ["process for downing music with multiple threads"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5480, "code": "def load name ( self , name ) : if name in self . globals : return self . globals [ name ] b = self . globals [ ' builtins ' ] if isinstance ( b , dict ) : return b [ name ] else : return getattr ( b , name )", "predictions": ["send a name name to the to the to the to the to the to the to the to the to the to the to the to the to the to"], "references": ["implementation of the load_name operation"], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 5481, "code": "def pop ( self , n ) : poped = self . stack [ len ( self . stack ) - n : ] del self . stack [ len ( self . stack ) - n : ] return poped", "predictions": ["remove n from protocol"], "references": ["pop the ** n ** topmost items from the stack and return them as a list ."], "bleu": 0.015417996259849322, "rouge_l": 0.17134831460674158}
{"id": 5482, "code": "def connection ( username = None , password = None , host = None , port = None ) : c opts = { } if username : c opts [ 'user' ] = username if password : c opts [ 'passwd' ] = password if host : c opts [ 'host' ] = host if port : c opts [ 'port' ] = port dbc = My SQ Ldb . connect ( * * c opts ) dbc . autocommit ( True ) return dbc", "predictions": ["create a disconnect connection to the zoneminder zoneminder api"], "references": ["returns a connected cursor to the database - server ."], "bleu": 0.17861170664603615, "rouge_l": 0.31282051282051276}
{"id": 5483, "code": "def render ditaa ( self , code , options , prefix = 'ditaa' ) : hashkey = code . encode ( 'utf-8' ) + str ( options ) + str ( self . builder . config . ditaa ) + str ( self . builder . config . ditaa args ) infname = '%s-%s.%s' % ( prefix , sha ( hashkey ) . hexdigest ( ) , \"ditaa\" ) outfname = '%s-%s.%s' % ( prefix , sha ( hashkey ) . hexdigest ( ) , \"png\" ) inrelfn = posixpath . join ( self . builder . imgpath , infname ) infullfn = path . join ( self . builder . outdir , ' images' , infname ) outrelfn = posixpath . join ( self . builder . imgpath , outfname ) outfullfn = path . join ( self . builder . outdir , ' images' , outfname ) if path . isfile ( outfullfn ) : return outrelfn , outfullfn ensuredir ( path . dirname ( outfullfn ) ) if isinstance ( code , unicode ) : code = code . encode ( 'utf-8' ) ditaa args = [ self . builder . config . ditaa ] ditaa args . extend ( self . builder . config . ditaa args ) ditaa args . extend ( options ) ditaa args . extend ( [ infullfn ] ) ditaa args . extend ( [ outfullfn ] ) f = open ( infullfn , 'w' ) f . write ( code ) f . close ( ) try : self . builder . warn ( ditaa args ) p = Popen ( ditaa args , stdout = PIPE , stdin = PIPE , stderr = PIPE ) except OS Error , err : if err . errno != ENOENT : raise self . builder . warn ( 'ditaa command %r cannot be run (needed for ditaa ' 'output), check the ditaa setting' % self . builder . config . ditaa ) self . builder . ditaa warned dot = True return None , None went Wrong = False try : stdout , stderr = p . communicate ( code ) except OS Error , err : if err . errno != EPIPE : raise went Wrong = True except IO Error , err : if err . errno != EINVAL : raise went Wrong = True if went Wrong : stdout , stderr = p . stdout . read ( ) , p . stderr . read ( ) p . wait ( ) if p . returncode != 0 : raise Ditaa Error ( 'ditaa exited with error:\\n[stderr]\\n%s\\n' '[stdout]\\n%s' % ( stderr , stdout ) ) return outrelfn , outfullfn", "predictions": ["send a ditaa to the ditaa"], "references": ["render ditaa code into a png output file ."], "bleu": 0.14827340167306757, "rouge_l": 0.12869198312236285}
{"id": 5484, "code": "def atexit ( self ) : self . log . debug ( \"Application. atexit\" ) if self . atexit func : self . atexit func ( self )", "predictions": ["only call get method"], "references": ["invoked in the finally block of application . run ."], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 5485, "code": "def run ( self , args list = None ) : self . log . debug ( \"Application.run: {args list}\" . format ( * * locals ( ) ) ) retval = None try : retval = self . run ( args list = args list ) except Keyboard Interrupt : self . log . verbose ( \"Interrupted\" ) except System Exit as exit : self . log . verbose ( \"Exited\" ) retval = exit . code except Exception : print ( \"Uncaught exception\" , file = sys . stderr ) traceback . print exc ( ) if \"debug pdb\" in self . args and self . args . debug pdb : debugger ( ) retval = Application . UNCAUGHT EXCEPTION EXIT raise finally : try : self . atexit ( ) finally : sys . stderr . flush ( ) sys . stdout . flush ( ) sys . exit ( retval )", "predictions": ["disconnect from main cli get"], "references": ["run application . main and exits with the return value ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 5486, "code": "def main ( ) : parser = optparse . Option Parser ( usage = \"%prog [options] <model path> [another model path...]\" , formatter = optparse . Titled Help Formatter ( ) ) parser . set description ( doc . strip ( ) ) parser . add option ( \"-f\" , \"--function\" , dest = \"function\" , metavar = \"NAME\" , help = \"append integrity checking actions to functions named NAME (required)\" , action = \"store\" , default = None ) parser . add option ( \"-o\" , \"--output\" , dest = 'output' , metavar = \"PATH\" , help = \"save sql model instances to PATH (required)\" , action = \"store\" , default = None ) parser . add option ( \"-v\" , \"--verbosity\" , dest = 'verbosity' , action = \"count\" , help = \"increase debug logging level\" , default = 2 ) ( opts , args ) = parser . parse args ( ) if len ( args ) == 0 or None in [ opts . output , opts . function ] : parser . print help ( ) sys . exit ( 1 ) levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } logging . basic Config ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) m = ooaofooa . load metamodel ( args ) for c c in m . select many ( 'C C' ) : filt = lambda sel : ooaofooa . is contained in ( sel , c c ) and sel . Name == opts . function s sync = m . select any ( 'S SYNC' , filt ) if not s sync : s sync = m . new ( 'S SYNC' , Name = opts . function ) pe pe = m . new ( 'PE PE' ) s dt = m . select any ( 'S DT' , where ( Name = 'boolean' ) ) relate ( pe pe , s sync , 8001 ) relate ( s dt , s sync , 25 ) generate actions ( m , c c , s sync ) xtuml . persist instances ( m , opts . output )", "predictions": ["centered actions actions actions"], "references": ["parse argv for options and arguments and start schema generation ."], "bleu": 0.05250363174428412, "rouge_l": 0.0}
{"id": 5487, "code": "def scrape ( ctx , url ) : data = load feed ( url ) feed = data [ 'feed' ] entries = data [ 'entries' ] type = 'community' country = 'Czech Republic' for entry in entries : id = sluggify ( entry [ 'id' ] ) city = entry [ 'tags' ] [ 0 ] [ 'term' ] landing = entry [ 'link' ] start time = dt normalize ( entry [ 'published parsed' ] , local tz = True ) title = entry [ 'title' ] summary = entry [ 'summary' ] link = entry [ 'link' ] ipdb . set trace ( )", "predictions": ["clock the no no no no longer longer no longer no longer no no no no no no content"], "references": ["rip the events from a given rss feed normalize the data and store ."], "bleu": 0.06439931429457924, "rouge_l": 0.06230847803881512}
{"id": 5488, "code": "def fancy tag compiler ( params , defaults , takes var args , takes var kwargs , takes context , name , node class , parser , token ) : bits = token . split contents ( ) [ 1 : ] if takes context : if 'context' in params [ : 1 ] : params = params [ 1 : ] else : raise Template Syntax Error ( \"Any tag function decorated with takes context=True \" \"must have a first argument of 'context'\" ) args = [ ] kwargs = { } kwarg found = False unhandled params = list ( params ) handled params = [ ] if len ( bits ) > 1 and bits [ - 2 ] == 'as' : output var = bits [ - 1 ] if len ( set ( output var ) - set ( ALLOWED VARIABLE CHARS ) ) > 0 : raise Template Syntax Error ( \"%s got output var name with forbidden chars: '%s'\" % ( name , output var ) ) bits = bits [ : - 2 ] else : output var = None for bit in bits : kwarg match = kwarg re . match ( bit ) if kwarg match : kw , var = kwarg match . groups ( ) if kw not in params and not takes var kwargs : raise Template Syntax Error ( \"%s got unknown keyword argument '%s'\" % ( name , kw ) ) elif kw in handled params : raise Template Syntax Error ( \"%s got multiple values for keyword argument '%s'\" % ( name , kw ) ) else : kwargs [ str ( kw ) ] = var kwarg found = True handled params . append ( kw ) else : if kwarg found : raise Template Syntax Error ( \"%s got non-keyword arg after keyword arg\" % name ) else : args . append ( bit ) try : handled params . append ( unhandled params . pop ( 0 ) ) except Index Error : if not takes var args : raise Template Syntax Error ( \"%s got too many arguments\" % name ) if defaults is not None : unhandled params = unhandled params [ : - len ( defaults ) ] if len ( unhandled params ) == 1 : raise Template Syntax Error ( \"%s didn't get a value for argument '%s'\" % ( name , unhandled params [ 0 ] ) ) elif len ( unhandled params ) > 1 : raise Template Syntax Error ( \"%s didn't get values for arguments: %s\" % ( name , ', ' . join ( [ \"'%s'\" % p for p in unhandled params ] ) ) ) return node class ( args , kwargs , output var , takes context )", "predictions": ["generate an 1 tag tag from the params parameters end ."], "references": ["returns a template . node subclass ."], "bleu": 0.11390778025531027, "rouge_l": 0.1157495256166983}
{"id": 5489, "code": "def get defining component ( pe pe ) : if pe pe is None : return None if pe pe . class . name != 'PE PE' : pe pe = xtuml . navigate one ( pe pe ) . PE PE [ 8001 ] ( ) ep pkg = xtuml . navigate one ( pe pe ) . EP PKG [ 8000 ] ( ) if ep pkg : return get defining component ( ep pkg ) return xtuml . navigate one ( pe pe ) . C C [ 8003 ] ( )", "predictions": ["in the defining self . call to ping the defining"], "references": ["get the c_c in which pe_pe is defined"], "bleu": 0.13950796967929133, "rouge_l": 0.11338289962825278}
{"id": 5490, "code": "def main ( ) : parser = optparse . Option Parser ( usage = \"%prog [options] <model path> [another model path..]\" , version = xtuml . version . complete string , formatter = optparse . Titled Help Formatter ( ) ) parser . add option ( \"-v\" , \"--verbosity\" , dest = 'verbosity' , action = \"count\" , help = \"increase debug logging level\" , default = 1 ) parser . add option ( \"-o\" , \"--output\" , dest = \"output\" , metavar = \"PATH\" , help = \"set output to PATH\" , action = \"store\" , default = None ) ( opts , args ) = parser . parse args ( ) if len ( args ) == 0 or opts . output is None : parser . print help ( ) sys . exit ( 1 ) levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } logging . basic Config ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) m = ooaofooa . load metamodel ( args ) prebuild model ( m ) xtuml . persist instances ( m , opts . output )", "predictions": ["cmd function for the cli ."], "references": ["parse command line options and launch the prebuilder ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 5491, "code": "def find symbol ( self , name = None , kind = None ) : for s in reversed ( self . stack ) : for symbol name , handle in s . symbols . items ( ) : symbol kind = handle . class . name if name == symbol name and kind == symbol kind : return handle elif name is None and kind == handle . class . name : return handle elif name == symbol name and kind is None : return handle if name is None and kind == s . handle . class . name : return s . handle", "predictions": ["from return symbol or none if not found ."], "references": ["find a symbol in the symbol table by name kind or both ."], "bleu": 0.10761866342063775, "rouge_l": 0.26406926406926406}
{"id": 5492, "code": "def is contained in ( pe pe , root ) : if not pe pe : return False if type ( pe pe ) . name != 'PE PE' : pe pe = one ( pe pe ) . PE PE [ 8001 ] ( ) ep pkg = one ( pe pe ) . EP PKG [ 8000 ] ( ) c c = one ( pe pe ) . C C [ 8003 ] ( ) if root in [ ep pkg , c c ] : return True elif is contained in ( ep pkg , root ) : return True elif is contained in ( c c , root ) : return True else : return False", "predictions": ["return true if the in in the contained is contained false otherwise . ."], "references": ["determine if a pe_pe is contained within a ep_pkg or a c_c ."], "bleu": 0.13217947626377288, "rouge_l": 0.2982885085574572}
{"id": 5493, "code": "def is global ( pe pe ) : if type ( pe pe ) . name != 'PE PE' : pe pe = one ( pe pe ) . PE PE [ 8001 ] ( ) if one ( pe pe ) . C C [ 8003 ] ( ) : return False pe pe = one ( pe pe ) . EP PKG [ 8000 ] . PE PE [ 8001 ] ( ) if not pe pe : return True return is global ( pe pe )", "predictions": ["returns true if the global is global false otherwise port port port port port port port port port port port port port port port port port port port port port port"], "references": ["check if a pe_pe is globally defined i . e . not inside a c_c"], "bleu": 0.04317900023606586, "rouge_l": 0.09277566539923954}
{"id": 5494, "code": "def get data type name ( s dt ) : s cdt = one ( s dt ) . S CDT [ 17 ] ( ) if s cdt and s cdt . Core Typ in range ( 1 , 6 ) : return s dt . Name . upper ( ) if one ( s dt ) . S EDT [ 17 ] ( ) : return 'INTEGER' s dt = one ( s dt ) . S UDT [ 17 ] . S DT [ 18 ] ( ) if s dt : return get data type name ( s dt )", "predictions": ["db db username username"], "references": ["convert a bridgepoint data type to a pyxtuml meta model type ."], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 5495, "code": "def get related attributes ( r rgo , r rto ) : l1 = list ( ) l2 = list ( ) ref filter = lambda ref : ref . OIR ID == r rgo . OIR ID for o ref in many ( r rto ) . O RTIDA [ 110 ] . O REF [ 111 ] ( ref filter ) : o attr = one ( o ref ) . O RATTR [ 108 ] . O ATTR [ 106 ] ( ) l1 . append ( o attr . Name ) o attr = one ( o ref ) . O RTIDA [ 111 ] . O OIDA [ 110 ] . O ATTR [ 105 ] ( ) l2 . append ( o attr . Name ) return l1 , l2", "predictions": ["get all local files for the given rgo"], "references": ["the two lists of attributes which relates two classes in an association ."], "bleu": 0.08583768591139128, "rouge_l": 0.09131736526946108}
{"id": 5496, "code": "def mk enum ( s edt ) : s dt = one ( s edt ) . S DT [ 17 ] ( ) enums = list ( ) kwlist = [ 'False' , 'None' , 'True' ] + keyword . kwlist for enum in many ( s edt ) . S ENUM [ 27 ] ( ) : if enum . Name in kwlist : enums . append ( enum . Name + ' ' ) else : enums . append ( enum . Name ) Enum = collections . namedtuple ( s dt . Name , enums ) return Enum ( * range ( len ( enums ) ) )", "predictions": ["create an required required required to create a namedtuple object wraps it ."], "references": ["create a named tuple from a bridgepoint enumeration ."], "bleu": 0.1350862565735141, "rouge_l": 0.2819722650231125}
{"id": 5497, "code": "def mk bridge ( metamodel , s brg ) : action = s brg . Action Semantics internal label = s brg . Name return lambda * * kwargs : interpret . run function ( metamodel , label , action , kwargs )", "predictions": ["get a login bridge from a string . . . ."], "references": ["create a python function from a bridgepoint bridge ."], "bleu": 0.17827531042796255, "rouge_l": 0.4073455759599332}
{"id": 5498, "code": "def mk function ( metamodel , s sync ) : action = s sync . Action Semantics internal label = s sync . Name return lambda * * kwargs : interpret . run function ( metamodel , label , action , kwargs )", "predictions": ["command line interface to command line interface"], "references": ["create a python function from a bridgepoint function ."], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 5499, "code": "def mk constant ( cnst syc ) : s dt = one ( cnst syc ) . S DT [ 1500 ] ( ) cnst lsc = one ( cnst syc ) . CNST LFSC [ 1502 ] . CNST LSC [ 1503 ] ( ) if s dt . Name == 'boolean' : return cnst lsc . Value . lower ( ) == 'true' if s dt . Name == 'integer' : return int ( cnst lsc . Value ) if s dt . Name == 'real' : return float ( cnst lsc . Value ) if s dt . Name == 'string' : return str ( cnst lsc . Value )", "predictions": ["filenames of a constant constant"], "references": ["create a python value from a bridgepoint constant ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 5500, "code": "def mk class ( m , o obj , derived attributes = False ) : first filter = lambda selected : not one ( selected ) . O ATTR [ 103 , 'succeeds' ] ( ) o attr = one ( o obj ) . O ATTR [ 102 ] ( first filter ) attributes = list ( ) while o attr : s dt = get attribute type ( o attr ) ty = get data type name ( s dt ) if not derived attributes and one ( o attr ) . O BATTR [ 106 ] . O DBATTR [ 107 ] ( ) : pass elif not ty : logger . warning ( 'Omitting unsupported attribute %s.%s ' % ( o obj . Key Lett , o attr . Name ) ) else : attributes . append ( ( o attr . Name , ty ) ) o attr = one ( o attr ) . O ATTR [ 103 , 'precedes' ] ( ) metaclass = m . define class ( o obj . Key Lett , list ( attributes ) , o obj . Descrip ) for o id in many ( o obj ) . O ID [ 104 ] ( ) : o oida = many ( o id ) . O OIDA [ 105 ] ( ) o attrs = many ( o oida ) . O ATTR [ 105 ] ( ) if not derived attributes and one ( o attrs ) . O BATTR [ 106 ] . O DBATTR [ 107 ] ( ) : logger . warning ( 'Omitting unique identifier %s.I%d' % ( o obj . Key Lett , o id . Oid ID + 1 ) ) continue names = [ o attr . Name for o attr in o attrs ] m . define unique identifier ( o obj . Key Lett , o id . Oid ID + 1 , * names ) for o tfr in many ( o obj ) . O TFR [ 115 ] ( ) : fn = mk operation ( metaclass , o tfr ) setattr ( metaclass . clazz , o tfr . Name , fn ) for o dbattr in many ( o obj ) . O ATTR [ 102 ] . O BATTR [ 106 ] . O DBATTR [ 107 ] ( ) : o attr = one ( o dbattr ) . O BATTR [ 107 ] . O ATTR [ 106 ] ( ) fn = mk derived attribute ( metaclass , o dbattr ) setattr ( metaclass . clazz , o attr . Name , fn ) return metaclass", "predictions": ["create a metaclass class object from a given object ."], "references": ["create a pyxtuml class from a bridgepoint class ."], "bleu": 0.22692039365038064, "rouge_l": 0.6376306620209059}
{"id": 5501, "code": "def mk simple association ( m , r simp ) : r rel = one ( r simp ) . R REL [ 206 ] ( ) r form = one ( r simp ) . R FORM [ 208 ] ( ) r part = one ( r simp ) . R PART [ 207 ] ( ) r rgo = one ( r form ) . R RGO [ 205 ] ( ) r rto = one ( r part ) . R RTO [ 204 ] ( ) if not r form : logger . info ( 'unformalized association R%s' % ( r rel . Numb ) ) r form = one ( r simp ) . R PART [ 207 ] ( lambda sel : sel != r part ) r rgo = one ( r form ) . R RTO [ 204 ] ( ) source o obj = one ( r rgo ) . R OIR [ 203 ] . O OBJ [ 201 ] ( ) target o obj = one ( r rto ) . R OIR [ 203 ] . O OBJ [ 201 ] ( ) source ids , target ids = get related attributes ( r rgo , r rto ) if source o obj . Obj ID != target o obj . Obj ID : source phrase = target phrase = '' else : source phrase = r part . Txt Phrs target phrase = r form . Txt Phrs m . define association ( rel id = r rel . Numb , source kind = source o obj . Key Lett , target kind = target o obj . Key Lett , source keys = source ids , target keys = target ids , source conditional = r form . Cond , target conditional = r part . Cond , source phrase = source phrase , target phrase = target phrase , source many = r form . Mult , target many = r part . Mult )", "predictions": ["create a simple association association object"], "references": ["create a pyxtuml association from a simple association in bridgepoint ."], "bleu": 0.21248506964807395, "rouge_l": 0.4468864468864468}
{"id": 5502, "code": "def mk linked association ( m , r assoc ) : r rel = one ( r assoc ) . R REL [ 206 ] ( ) r rgo = one ( r assoc ) . R ASSR [ 211 ] . R RGO [ 205 ] ( ) source o obj = one ( r rgo ) . R OIR [ 203 ] . O OBJ [ 201 ] ( ) def mk assoc ( side1 , side2 ) : r rto = one ( side1 ) . R RTO [ 204 ] ( ) target o obj = one ( r rto ) . R OIR [ 203 ] . O OBJ [ 201 ] ( ) source ids , target ids = get related attributes ( r rgo , r rto ) if side1 . Obj ID != side2 . Obj ID : source phrase = target phrase = '' else : source phrase = side1 . Txt Phrs target phrase = side2 . Txt Phrs m . define association ( rel id = r rel . Numb , source kind = source o obj . Key Lett , target kind = target o obj . Key Lett , source keys = source ids , target keys = target ids , source conditional = side2 . Cond , target conditional = False , source phrase = source phrase , target phrase = target phrase , source many = side2 . Mult , target many = False ) r aone = one ( r assoc ) . R AONE [ 209 ] ( ) r aoth = one ( r assoc ) . R AOTH [ 210 ] ( ) mk assoc ( r aone , r aoth ) mk assoc ( r aoth , r aone )", "predictions": ["create a new association association"], "references": ["create pyxtuml associations from a linked association in bridgepoint ."], "bleu": 0.11943865131127647, "rouge_l": 0.37731958762886597}
{"id": 5503, "code": "def mk association ( m , r rel ) : handler = { 'R SIMP' : mk simple association , 'R ASSOC' : mk linked association , 'R SUBSUP' : mk subsuper association , 'R COMP' : mk derived association , } inst = subtype ( r rel , 206 ) fn = handler . get ( type ( inst ) . name ) return fn ( m , inst )", "predictions": ["create a new association object from a given self ."], "references": ["create a pyxtuml association from a r_rel in ooaofooa ."], "bleu": 0.22692039365038064, "rouge_l": 0.6}
{"id": 5504, "code": "def delete globals ( m , disconnect = False ) : filt = lambda sel : ( 247728914420827907967735776184937480192 <= sel . DT ID <= 247728914420827907967735776184937480208 ) for s dt in m . select many ( 'S DT' , filt ) : xtuml . delete ( one ( s dt ) . PE PE [ 8001 ] ( ) , disconnect ) xtuml . delete ( subtype ( s dt , 17 ) , disconnect ) xtuml . delete ( s dt , disconnect )", "predictions": ["delete all many globals ."], "references": ["remove global instances e . g . the core data type integer ."], "bleu": 0.05512018958855254, "rouge_l": 0.10286677908937607}
{"id": 5505, "code": "def accept ( self , reply socket , channel ) : info = self . info or b'' self . send raw ( reply socket , ACCEPT , info , * channel )", "predictions": ["accept a reply to the server ."], "references": ["sends accept reply ."], "bleu": 0.22089591134157885, "rouge_l": 0.5736677115987461}
{"id": 5506, "code": "def reject ( self , reply socket , call id , topics = ( ) ) : info = self . info or b'' self . send raw ( reply socket , REJECT , info , call id , b'' , topics )", "predictions": ["reject a reply socket ."], "references": ["sends reject reply ."], "bleu": 0.32466791547509893, "rouge_l": 0.6802973977695167}
{"id": 5507, "code": "def raise ( self , reply socket , channel , exc info = None ) : if not reply socket : return if exc info is None : exc info = sys . exc info ( ) exc type , exc , tb = exc info while tb . tb next is not None : tb = tb . tb next if issubclass ( exc type , Remote Exception ) : exc type = exc type . exc type filename , lineno = tb . tb frame . f code . co filename , tb . tb lineno val = ( exc type , str ( exc ) , filename , lineno ) try : state = exc . getstate ( ) except Attribute Error : pass else : val += ( state , ) self . send reply ( reply socket , RAISE , val , * channel )", "predictions": ["raise a reply to the client ."], "references": ["sends raise reply ."], "bleu": 0.22089591134157885, "rouge_l": 0.5736677115987461}
{"id": 5508, "code": "def call wait ( self , hints , name , args , kwargs , topics = ( ) , raw = False , limit = None , retry = False , max retries = None ) : col = self . collector if not col . is running ( ) : col . start ( ) call id = uuid4 bytes ( ) reply to = ( DUPLEX if self . socket is col . socket else col . topic ) header = self . make header ( name , call id , reply to , hints ) payload = self . pack ( args , kwargs , raw ) def send call ( ) : try : safe ( send , self . socket , header , payload , topics , zmq . NOBLOCK ) except zmq . Again : raise Undelivered ( 'emission was not delivered' ) col . prepare ( call id , self , name , args , kwargs ) send call ( ) return col . establish ( call id , self . timeout , limit , send call if retry else None , max retries = max retries )", "predictions": ["call a remote server"], "references": ["allocates a call id and emit ."], "bleu": 0.1878296463217631, "rouge_l": 0.1732954545454545}
{"id": 5509, "code": "def dispatch reply ( self , reply , value ) : method = reply . method call id = reply . call id task id = reply . task id if method & ACK : try : result queue = self . result queues [ call id ] except Key Error : raise Key Error ( 'already established or unprepared call' ) if method == ACCEPT : worker info = value result = Remote Result ( self , call id , task id , worker info ) self . results [ call id ] [ task id ] = result result queue . put nowait ( result ) elif method == REJECT : result queue . put nowait ( None ) else : result = self . results [ call id ] [ task id ] result . set reply ( reply . method , value )", "predictions": ["dispatch a reply to the client ."], "references": ["dispatches the reply to the proper queue ."], "bleu": 0.31689174383082924, "rouge_l": 0.5269978401727862}
{"id": 5510, "code": "def guess type name ( value ) : value = str ( value ) if value . upper ( ) in [ 'TRUE' , 'FALSE' ] : return 'BOOLEAN' elif re . match ( r'(-)?(\\d+)(\\.\\d+)' , value ) : return 'REAL' elif re . match ( r'(-)?(\\d+)' , value ) : return 'INTEGER' elif re . match ( r'\\'((\\'\\')|[^\\'])*\\'' , value ) : return 'STRING' elif re . match ( r'\\\"([^\\\\\\n]|(\\\\.))*?\\\"' , value ) : return 'UNIQUE ID'", "predictions": ["guess the type name of the given value ."], "references": ["guess the type name of a serialized value ."], "bleu": 0.5814307369682193, "rouge_l": 0.7777777777777778}
{"id": 5511, "code": "def deserialize value ( ty , value ) : uty = ty . upper ( ) if uty == 'BOOLEAN' : if value . isdigit ( ) : return bool ( int ( value ) ) elif value . upper ( ) == 'FALSE' : return False elif value . upper ( ) == 'TRUE' : return True else : return None elif uty == 'INTEGER' : if '\"' in value : return uuid . UUID ( value [ 1 : - 1 ] ) . int else : return int ( value ) elif uty == 'REAL' : return float ( value ) elif uty == 'STRING' : return value [ 1 : - 1 ] . replace ( \"''\" , \"'\" ) elif uty == 'UNIQUE ID' : if '\"' in value : return uuid . UUID ( value [ 1 : - 1 ] ) . int else : return int ( value )", "predictions": ["deserialize string into an int or float"], "references": ["deserialize a value of some type"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 5512, "code": "def populate classes ( self , metamodel ) : for stmt in self . statements : if isinstance ( stmt , Create Class Stmt ) : metamodel . define class ( stmt . kind , stmt . attributes )", "predictions": ["populate the classes from the metamodel object ."], "references": ["populate a * metamodel * with classes previously encountered from input ."], "bleu": 0.12801036176909558, "rouge_l": 0.3860759493670886}
{"id": 5513, "code": "def populate connections ( self , metamodel ) : storage = dict ( ) for ass in metamodel . associations : source class = ass . source link . to metaclass target class = ass . target link . to metaclass if target class not in storage : storage [ target class ] = dict ( ) link key = frozenset ( ass . source link . key map . values ( ) ) if link key not in storage [ target class ] : storage [ target class ] [ link key ] = dict ( ) for other inst in target class . storage : inst key = ass . source link . compute index key ( other inst ) if inst key is None : continue if inst key not in storage [ target class ] [ link key ] : storage [ target class ] [ link key ] [ inst key ] = xtuml . Ordered Set ( ) storage [ target class ] [ link key ] [ inst key ] . add ( other inst ) for inst in source class . storage : inst key = ass . source link . compute lookup key ( inst ) if inst key is None : continue if inst key not in storage [ target class ] [ link key ] : continue for other inst in storage [ target class ] [ link key ] [ inst key ] : ass . source link . connect ( other inst , inst , check = False ) ass . target link . connect ( inst , other inst , check = False ) for inst in metamodel . instances : metaclass = xtuml . get metaclass ( inst ) for attr in metaclass . referential attributes : if attr in inst . dict : delattr ( inst , attr )", "predictions": ["add all connections from a storage object to the instance"], "references": ["populate links in a * metamodel * with connections between them ."], "bleu": 0.11421946507590645, "rouge_l": 0.08944281524926685}
{"id": 5514, "code": "def populate ( self , metamodel ) : self . populate classes ( metamodel ) self . populate unique identifiers ( metamodel ) self . populate associations ( metamodel ) self . populate instances ( metamodel ) self . populate connections ( metamodel )", "predictions": ["populate the connections from the database ."], "references": ["populate a * metamodel * with entities previously encountered from input ."], "bleu": 0.1081377510275021, "rouge_l": 0.30148270181219106}
{"id": 5515, "code": "def build metamodel ( self , id generator = None ) : m = xtuml . Meta Model ( id generator ) self . populate ( m ) return m", "predictions": ["build a metamodel object from its id"], "references": ["build and return a * xtuml . metamodel * containing previously loaded input ."], "bleu": 0.0812630644213965, "rouge_l": 0.2695139911634757}
{"id": 5516, "code": "def source ( self , feature names ) : if feature names is None : return True elif isinstance ( feature names , bool ) : return feature names else : return map ( lambda n : 'fc.' + n , feature names )", "predictions": ["returns a feature instance of the feature names"], "references": ["maps feature names to es s _source field ."], "bleu": 0.1862539773562041, "rouge_l": 0.232824427480916}
{"id": 5517, "code": "def range filters ( self , * key ranges ) : filters = [ ] for s , e in key ranges : if isinstance ( s , basestring ) : s = eid ( s ) if isinstance ( e , basestring ) : e += u'\\U0010FFFF' e = eid ( e ) if s == ( ) and e == ( ) : filters . append ( { 'match all' : { } } ) elif e == ( ) : filters . append ( { 'range' : { ' id' : { 'gte' : s } } } ) elif s == ( ) : filters . append ( { 'range' : { ' id' : { 'lte' : e } } } ) else : filters . append ( { 'range' : { ' id' : { 'gte' : s , 'lte' : e } } } ) if len ( filters ) == 0 : return [ { 'match all' : { } } ] else : return filters", "predictions": ["return a dict of range filters to the key ."], "references": ["creates es filters for key ranges used in scanning ."], "bleu": 0.14991106946711685, "rouge_l": 0.3}
{"id": 5518, "code": "def create mappings ( self ) : self . conn . indices . put mapping ( index = self . index , doc type = self . type , timeout = 60 , request timeout = 60 , body = { self . type : { 'dynamic templates' : [ { 'default no analyze fc' : { 'match' : 'fc.*' , 'mapping' : { 'index' : 'no' } , } , } ] , ' all' : { 'enabled' : False , } , ' id' : { 'index' : 'not analyzed' , } , 'properties' : self . get index mappings ( ) , } , } ) # self . conn . cluster . health ( index = self . index , wait for status = 'yellow' )", "predictions": ["create mappings for the index of the index ."], "references": ["create the field type mapping ."], "bleu": 0.16784459625186196, "rouge_l": 0.4149659863945578}
{"id": 5519, "code": "def get index mappings ( self ) : maps = { } for fname in self . indexed features : config = self . indexes . get ( fname , { } ) print ( fname , config ) maps [ fname to idx name ( fname ) ] = { 'type' : config . get ( 'es index type' , 'integer' ) , 'store' : False , 'index' : 'not analyzed' , } for fname in self . fulltext indexed features : maps [ fname to full idx name ( fname ) ] = { 'type' : 'string' , 'store' : False , 'index' : 'analyzed' , } return maps", "predictions": ["return a dictionary of index mappings for all fulltext features ."], "references": ["retrieve the field mappings . useful for debugging ."], "bleu": 0.1354599427337814, "rouge_l": 0.3055091819699499}
{"id": 5520, "code": "def get field types ( self ) : mapping = self . conn . indices . get mapping ( index = self . index , doc type = self . type ) return mapping [ self . index ] [ 'mappings' ] [ self . type ] [ 'properties' ]", "predictions": ["return the field types for this field ."], "references": ["retrieve the field types . useful for debugging ."], "bleu": 0.29150322793751426, "rouge_l": 0.5820610687022901}
{"id": 5521, "code": "def fc index disjunction from query ( self , query fc , fname ) : if len ( query fc . get ( fname , [ ] ) ) == 0 : return [ ] terms = query fc [ fname ] . keys ( ) disj = [ ] for fname in self . indexes [ fname ] [ 'feature names' ] : disj . append ( { 'terms' : { fname to idx name ( fname ) : terms } } ) return disj", "predictions": ["return the index disjunction from the query"], "references": ["creates a disjunction for keyword scan queries ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 5522, "code": "def fc bytes ( self , fc dict ) : num bytes = 0 for , feat in fc dict . iteritems ( ) : num bytes += len ( feat ) return num bytes", "predictions": ["convert bytes to bytes"], "references": ["take a feature collection in dict form and count its size in bytes ."], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 5523, "code": "def pretty string ( fc ) : s = [ ] for fname , feature in sorted ( fc . items ( ) ) : if isinstance ( feature , String Counter ) : feature = [ u'%s: %d' % ( k , v ) for ( k , v ) in feature . most common ( ) ] feature = u'\\n\\t' + u'\\n\\t' . join ( feature ) s . append ( fname + u': ' + feature ) return u'\\n' . join ( s )", "predictions": ["pretty - print a string of a feature ."], "references": ["construct a nice looking string for an fc"], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 5524, "code": "def process docopts ( ) : arguments = docopt ( doc , version = \"Find Known Secrets {0}\" . format ( version ) ) logger . debug ( arguments ) if arguments [ \"here\" ] : go ( ) else : files = arguments [ \"--secrets\" ] searcher = Searcher ( source = arguments [ \"--source\" ] , files = files ) searcher . go ( )", "predictions": ["process docopts and process arguments ."], "references": ["take care of command line options"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 5525, "code": "def default formatter ( error ) : quoted = formencode . htmlfill . escape formatter ( error ) return u'<span class=\"error-message\">{0}</span>' . format ( quoted )", "predictions": ["default formatter for error ."], "references": ["escape the error and wrap it in a span with class error - message"], "bleu": 0.04512859433163675, "rouge_l": 0.09697933227344992}
{"id": 5526, "code": "def pretty to link ( inst , link ) : values = '' prefix = '' metaclass = xtuml . get metaclass ( inst ) for name , ty in metaclass . attributes : if name in link . key map : value = getattr ( inst , name ) value = xtuml . serialize value ( value , ty ) name = link . key map [ name ] values += '%s%s=%s' % ( prefix , name , value ) prefix = ', ' return '%s(%s)' % ( link . kind , values )", "predictions": ["convert a link to a link object"], "references": ["create a human - readable representation of a link on the to - side"], "bleu": 0.10218289380194193, "rouge_l": 0.2695139911634757}
{"id": 5527, "code": "def pretty unique identifier ( inst , identifier ) : values = '' prefix = '' metaclass = xtuml . get metaclass ( inst ) for name , ty in metaclass . attributes : if name in metaclass . identifying attributes : value = getattr ( inst , name ) value = xtuml . serialize value ( value , ty ) values += '%s%s=%s' % ( prefix , name , value ) prefix = ', ' return '%s(%s)' % ( identifier , values )", "predictions": ["return a unique identifier for a unique identifier ."], "references": ["create a human - readable representation a unique identifier ."], "bleu": 0.36789852486668184, "rouge_l": 0.5213675213675214}
{"id": 5528, "code": "def check uniqueness constraint ( m , kind = None ) : if kind is None : metaclasses = m . metaclasses . values ( ) else : metaclasses = [ m . find metaclass ( kind ) ] res = 0 for metaclass in metaclasses : id map = dict ( ) for identifier in metaclass . indices : id map [ identifier ] = dict ( ) for inst in metaclass . select many ( ) : for name , ty in metaclass . attributes : if name not in metaclass . identifying attributes : continue value = getattr ( inst , name ) isnull = value is None isnull |= ( ty == 'UNIQUE ID' and not value ) if isnull : res += 1 logger . warning ( '%s.%s is part of an identifier and is null' % ( metaclass . kind , name ) ) for identifier in metaclass . indices : kwargs = dict ( ) for name in metaclass . indices [ identifier ] : kwargs [ name ] = getattr ( inst , name ) index key = frozenset ( kwargs . items ( ) ) if index key in id map [ identifier ] : res += 1 id string = pretty unique identifier ( inst , identifier ) logger . warning ( 'uniqueness constraint violation in %s, %s' % ( metaclass . kind , id string ) ) id map [ identifier ] [ index key ] = inst return res", "predictions": ["check if uniqueness is unique"], "references": ["check the model for uniqueness constraint violations ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 5529, "code": "def check link integrity ( m , link ) : res = 0 for inst in link . from metaclass . select many ( ) : q set = list ( link . navigate ( inst ) ) if ( len ( q set ) < 1 and not link . conditional ) or ( ( len ( q set ) > 1 and not link . many ) ) : res += 1 logger . warning ( 'integrity violation in ' '%s --(%s)--> %s' % ( pretty from link ( inst , link ) , link . rel id , pretty to link ( inst , link ) ) ) return res", "predictions": ["check if link is valid"], "references": ["check the model for integrity violations on an association in a particular direction ."], "bleu": 0.04512859433163675, "rouge_l": 0.09697933227344992}
{"id": 5530, "code": "def check subtype integrity ( m , super kind , rel id ) : if isinstance ( rel id , int ) : rel id = 'R%d' % rel id res = 0 for inst in m . select many ( super kind ) : if not xtuml . navigate subtype ( inst , rel id ) : res += 1 logger . warning ( 'integrity violation across ' '%s[%s]' % ( super kind , rel id ) ) return res", "predictions": ["check if subtype is many ."], "references": ["check the model for integrity violations across a subtype association ."], "bleu": 0.1141650334026257, "rouge_l": 0.33516483516483514}
{"id": 5531, "code": "def basic transform ( val ) : if isinstance ( val , int ) : return struct . pack ( '>i' , val ) else : return safe lower utf8 ( val )", "predictions": ["transform an integer to a basic byte string ."], "references": ["a basic transform for strings and integers ."], "bleu": 0.21105340631872635, "rouge_l": 0.35672514619883033}
{"id": 5532, "code": "def get type name ( s dt ) : s cdt = nav one ( s dt ) . S CDT [ 17 ] ( ) if s cdt and s cdt . Core Typ in range ( 1 , 6 ) : return s dt . Name s edt = nav one ( s dt ) . S EDT [ 17 ] ( ) if s edt : return s dt . Name s udt = nav one ( s dt ) . S UDT [ 17 ] ( ) if s udt : return s dt . Name", "predictions": ["get the type name of the given type ."], "references": ["get the xsd name of a s_dt"], "bleu": 0.23356898886410005, "rouge_l": 0.5115303983228512}
{"id": 5533, "code": "def get refered attribute ( o attr ) : o attr ref = nav one ( o attr ) . O RATTR [ 106 ] . O BATTR [ 113 ] . O ATTR [ 106 ] ( ) if o attr ref : return get refered attribute ( o attr ref ) else : return o attr", "predictions": ["return the refered attribute of the refered"], "references": ["get the the referred attribute ."], "bleu": 0.22089591134157885, "rouge_l": 0.31202046035805625}
{"id": 5534, "code": "def build core type ( s cdt ) : s dt = nav one ( s cdt ) . S DT [ 17 ] ( ) if s dt . name == 'void' : type name = None elif s dt . name == 'boolean' : type name = 'xs:boolean' elif s dt . name == 'integer' : type name = 'xs:integer' elif s dt . name == 'real' : type name = 'xs:decimal' elif s dt . name == 'string' : type name = 'xs:string' elif s dt . name == 'unique id' : type name = 'xs:integer' else : type name = None if type name : mapped type = ET . Element ( 'xs:simple Type' , name = s dt . name ) ET . Sub Element ( mapped type , 'xs:restriction' , base = type name ) return mapped type", "predictions": ["build a core type from a string ."], "references": ["build an xsd simpletype out of a s_cdt ."], "bleu": 0.16829946711936866, "rouge_l": 0.34923664122137404}
{"id": 5535, "code": "def build enum type ( s edt ) : s dt = nav one ( s edt ) . S DT [ 17 ] ( ) enum = ET . Element ( 'xs:simple Type' , name = s dt . name ) enum list = ET . Sub Element ( enum , 'xs:restriction' , base = 'xs:string' ) first filter = lambda selected : not nav one ( selected ) . S ENUM [ 56 , 'succeeds' ] ( ) s enum = nav any ( s edt ) . S ENUM [ 27 ] ( first filter ) while s enum : ET . Sub Element ( enum list , 'xs:enumeration' , value = s enum . name ) s enum = nav one ( s enum ) . S ENUM [ 56 , 'precedes' ] ( ) return enum", "predictions": ["build an enum type from an enum string ."], "references": ["build an xsd simpletype out of a s_edt ."], "bleu": 0.19960198807747329, "rouge_l": 0.3333333333333333}
{"id": 5536, "code": "def build struct type ( s sdt ) : s dt = nav one ( s sdt ) . S DT [ 17 ] ( ) struct = ET . Element ( 'xs:complex Type' , name = s dt . name ) first filter = lambda selected : not nav one ( selected ) . S MBR [ 46 , 'succeeds' ] ( ) s mbr = nav any ( s sdt ) . S MBR [ 44 ] ( first filter ) while s mbr : s dt = nav one ( s mbr ) . S DT [ 45 ] ( ) type name = get type name ( s dt ) ET . Sub Element ( struct , 'xs:attribute' , name = s mbr . name , type = type name ) s mbr = nav one ( s mbr ) . S MBR [ 46 , 'precedes' ] ( ) return struct", "predictions": ["delete the globals type from the given disconnect sel sel sel sel sel sel sel sel sel sel ."], "references": ["build an xsd complextype out of a s_sdt ."], "bleu": 0.06439931429457924, "rouge_l": 0.07634543178973717}
{"id": 5537, "code": "def build user type ( s udt ) : s dt user = nav one ( s udt ) . S DT [ 17 ] ( ) s dt base = nav one ( s udt ) . S DT [ 18 ] ( ) base name = get type name ( s dt base ) if base name : user = ET . Element ( 'xs:simple Type' , name = s dt user . name ) ET . Sub Element ( user , 'xs:restriction' , base = base name ) return user", "predictions": ["accept a user self or a string"], "references": ["build an xsd simpletype out of a s_udt ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 5538, "code": "def build type ( s dt ) : s cdt = nav one ( s dt ) . S CDT [ 17 ] ( ) if s cdt : return build core type ( s cdt ) s edt = nav one ( s dt ) . S EDT [ 17 ] ( ) if s edt : return build enum type ( s edt ) s udt = nav one ( s dt ) . S UDT [ 17 ] ( ) if s udt : return build user type ( s udt )", "predictions": ["reject a type type from a string"], "references": ["build a partial xsd tree out of a s_dt and its sub types s_cdt s_edt s_sdt and s_udt ."], "bleu": 0.03702100183468252, "rouge_l": 0.14202561117578577}
{"id": 5539, "code": "def build class ( o obj ) : cls = ET . Element ( 'xs:element' , name = o obj . key lett , min Occurs = '0' , max Occurs = 'unbounded' ) attributes = ET . Sub Element ( cls , 'xs:complex Type' ) for o attr in nav many ( o obj ) . O ATTR [ 102 ] ( ) : o attr ref = get refered attribute ( o attr ) s dt = nav one ( o attr ref ) . S DT [ 114 ] ( ) while nav one ( s dt ) . S UDT [ 17 ] ( ) : s dt = nav one ( s dt ) . S UDT [ 17 ] . S DT [ 18 ] ( ) type name = get type name ( s dt ) if type name and not nav one ( o attr ) . O BATTR [ 106 ] . O DBATTR [ 107 ] ( ) : ET . Sub Element ( attributes , 'xs:attribute' , name = o attr . name , type = type name ) else : logger . warning ( 'Omitting %s.%s' % ( o obj . key lett , o attr . Name ) ) return cls", "predictions": ["raise an exception for the given object . . . . . . . . ."], "references": ["build an xsd complex element out of a o_obj including its o_attr ."], "bleu": 0.08513012360883544, "rouge_l": 0.14055299539170507}
{"id": 5540, "code": "def build component ( m , c c ) : component = ET . Element ( 'xs:element' , name = c c . name ) classes = ET . Sub Element ( component , 'xs:complex Type' ) classes = ET . Sub Element ( classes , 'xs:sequence' ) scope filter = lambda selected : ooaofooa . is contained in ( selected , c c ) for o obj in m . select many ( 'O OBJ' , scope filter ) : cls = build class ( o obj ) classes . append ( cls ) return component", "predictions": ["create a wait wait for the given self = value = none"], "references": ["build an xsd complex element out of a c_c including its packaged s_dt and o_obj ."], "bleu": 0.07444976834161264, "rouge_l": 0.06963470319634703}
{"id": 5541, "code": "def build schema ( m , c c ) : schema = ET . Element ( 'xs:schema' ) schema . set ( 'xmlns:xs' , 'http://www.w3.org/2001/XML Schema' ) global filter = lambda selected : ooaofooa . is global ( selected ) for s dt in m . select many ( 'S DT' , global filter ) : datatype = build type ( s dt ) if datatype is not None : schema . append ( datatype ) scope filter = lambda selected : ooaofooa . is contained in ( selected , c c ) for s dt in m . select many ( 'S DT' , scope filter ) : datatype = build type ( s dt ) if datatype is not None : schema . append ( datatype ) component = build component ( m , c c ) schema . append ( component ) return schema", "predictions": ["dispatch a reply to a list of queues"], "references": ["build an xsd schema from a bridgepoint component ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 5542, "code": "def prettify ( xml string ) : reparsed = xml . dom . minidom . parse String ( xml string ) return reparsed . toprettyxml ( indent = \"    \" )", "predictions": ["pretty - printed name of an name str str str str str str str str str str str str str str"], "references": ["indent an xml string with four spaces and add an additional line break after each node ."], "bleu": 0.05809665204409193, "rouge_l": 0.0536499560246262}
{"id": 5543, "code": "def set positional info ( node , p ) : node . position = Position ( ) node . position . label = p . lexer . label node . position . start stream = p . lexpos ( 1 ) node . position . start line = p . lineno ( 1 ) node . position . start column = find column ( p . lexer . lexdata , node . position . start stream ) , node . position . end stream = p . lexspan ( len ( p ) - 1 ) , node . position . end line = p . linespan ( len ( p ) - 1 ) node . position . end column = find column ( p . lexer . lexdata , node . position . end stream ) - 1 node . character stream = p . lexer . lexdata [ node . position . start stream : node . position . end stream ]", "predictions": ["deserialize the lexer of the node"], "references": ["set positional information on a node"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 5544, "code": "def track production ( f ) : @ wraps ( f ) def wrapper ( self , p ) : r = f ( self , p ) node = p [ 0 ] if isinstance ( node , Node ) and len ( p ) > 1 : set positional info ( node , p ) return r return wrapper", "predictions": ["decorator to populate a classes from a function"], "references": ["decorator for adding positional information to returning nodes"], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 5545, "code": "def create msg ( self , to , subject , msg Html , msg Plain , attachments = None ) : sender = self . sender if attachments and isinstance ( attachments , str ) : attachments = [ attachments ] else : attachments = list ( attachments or [ ] ) msg = MIME Multipart ( 'alternative' ) msg [ 'Subject' ] = subject msg [ 'From' ] = sender msg [ 'To' ] = to msg . attach ( MIME Text ( msg Plain , 'plain' ) ) msg . attach ( MIME Text ( msg Html , 'html' ) ) for path in attachments : attachment = self . prep attachment ( path ) msg . attach ( attachment ) raw = base64 . urlsafe b64encode ( msg . as bytes ( ) ) . decode ( ) #raw = raw.decode() body = { 'raw' : raw } return body", "predictions": ["populate the = connect message with the specified subject associations associations associations associations"], "references": ["attachments should be a list of paths"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 5546, "code": "def read ( self ) : if self . connection . has changed ( ) : image path = self . connection . download image ( ) image = Image . open ( image path ) self . text cache = pytesseract . image to string ( image ) image . close ( ) return self . text cache", "predictions": ["connections an identifiers ."], "references": ["returns the text from an image at a given url ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 5547, "code": "def main ( ) : parser = optparse . Option Parser ( usage = \"%prog [options] <model path> [another model path..]\" , version = xtuml . version . complete string , formatter = optparse . Titled Help Formatter ( ) ) parser . add option ( \"-v\" , \"--verbosity\" , dest = 'verbosity' , action = \"count\" , default = 1 , help = \"increase debug logging level\" ) parser . add option ( \"-f\" , \"--function\" , dest = 'function' , action = \"store\" , help = \"invoke function named NAME\" , metavar = 'NAME' ) parser . add option ( \"-c\" , \"--component\" , dest = 'component' , action = \"store\" , help = \"look for the function in a component named NAME\" , metavar = 'NAME' , default = None ) ( opts , args ) = parser . parse args ( ) if len ( args ) == 0 or not opts . function : parser . print help ( ) sys . exit ( 1 ) levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } logging . basic Config ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) from bridgepoint import ooaofooa mm = ooaofooa . load metamodel ( args ) c c = mm . select any ( 'C C' , where ( Name = opts . component ) ) domain = ooaofooa . mk component ( mm , c c , derived attributes = False ) func = domain . find symbol ( opts . function ) return func ( )", "predictions": ["the main function of the script"], "references": ["parse command line options and launch the interpreter"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 5548, "code": "def main ( ) : parser = optparse . Option Parser ( usage = \"%prog [options] <model path> [another model path...]\" , version = xtuml . version . complete string , formatter = optparse . Titled Help Formatter ( ) ) parser . set description ( doc . strip ( ) ) parser . add option ( \"-c\" , \"--component\" , dest = \"component\" , metavar = \"NAME\" , help = \"export sql schema for the component named NAME\" , action = \"store\" , default = None ) parser . add option ( \"-d\" , \"--derived-attributes\" , dest = \"derived\" , help = \"include derived attributes in the schema\" , action = \"store true\" , default = False ) parser . add option ( \"-o\" , \"--output\" , dest = 'output' , metavar = \"PATH\" , help = \"save sql schema to PATH (required)\" , action = \"store\" , default = None ) parser . add option ( \"-v\" , \"--verbosity\" , dest = 'verbosity' , action = \"count\" , help = \"increase debug logging level\" , default = 2 ) ( opts , args ) = parser . parse args ( ) if len ( args ) == 0 or opts . output is None : parser . print help ( ) sys . exit ( 1 ) levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } logging . basic Config ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) loader = ooaofooa . Loader ( ) for filename in args : loader . filename input ( filename ) c = loader . build component ( opts . component , opts . derived ) xtuml . persist database ( c , opts . output )", "predictions": ["the main function of the script"], "references": ["parse argv for options and arguments and start schema generation ."], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 5549, "code": "def serialize value ( value , ty ) : ty = ty . upper ( ) null value = { 'BOOLEAN' : False , 'INTEGER' : 0 , 'REAL' : 0.0 , 'STRING' : '' , 'UNIQUE ID' : 0 } transfer fn = { 'BOOLEAN' : lambda v : '%d' % int ( v ) , 'INTEGER' : lambda v : '%d' % v , 'REAL' : lambda v : '%f' % v , 'STRING' : lambda v : \"'%s'\" % v . replace ( \"'\" , \"''\" ) , 'UNIQUE ID' : lambda v : '\"%s\"' % uuid . UUID ( int = v ) } if value is None : value = null value [ ty ] return transfer fn [ ty ] ( value )", "predictions": ["range an uuid filters into a string ."], "references": ["serialize a value from an xtuml metamodel instance ."], "bleu": 0.16829946711936866, "rouge_l": 0.232824427480916}
{"id": 5550, "code": "def serialize instance ( instance ) : attr count = 0 metaclass = xtuml . get metaclass ( instance ) s = 'INSERT INTO %s VALUES (' % metaclass . kind for name , ty in metaclass . attributes : value = getattr ( instance , name ) s += '\\n    ' s += serialize value ( value , ty ) attr count += 1 if attr count < len ( metaclass . attributes ) : s += ', -- %s : %s' % ( name , ty ) else : s += ' -- %s : %s' % ( name , ty ) s += '\\n);\\n' return s", "predictions": ["create an mappings from an mappings index index index index index index index index index index index ."], "references": ["serialize an * instance * from a metamodel ."], "bleu": 0.08097785064266204, "rouge_l": 0.2364341085271318}
{"id": 5551, "code": "def serialize instances ( metamodel ) : s = '' for inst in metamodel . instances : s += serialize instance ( inst ) return s", "predictions": ["get a list of index index fname fname fname"], "references": ["serialize all instances in a * metamodel * ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 5552, "code": "def serialize association ( ass ) : s1 = '%s %s (%s)' % ( ass . source link . cardinality , ass . source link . to metaclass . kind , ', ' . join ( ass . source keys ) ) if ass . target link . phrase : s1 += \" PHRASE '%s'\" % ass . target link . phrase s2 = '%s %s (%s)' % ( ass . target link . cardinality , ass . target link . to metaclass . kind , ', ' . join ( ass . target keys ) ) if ass . source link . phrase : s2 += \" PHRASE '%s'\" % ass . source link . phrase return 'CREATE ROP REF ID %s FROM %s TO %s;\\n' % ( ass . rel id , s1 , s2 )", "predictions": ["get a json - serializable field index index index index index index index index index index ."], "references": ["serialize an xtuml metamodel association ."], "bleu": 0.07223943354597204, "rouge_l": 0.09516380655226209}
{"id": 5553, "code": "def serialize class ( Cls ) : metaclass = xtuml . get metaclass ( Cls ) attributes = [ '%s %s' % ( name , ty . upper ( ) ) for name , ty in metaclass . attributes ] s = 'CREATE TABLE %s (\\n    ' % metaclass . kind s += ',\\n    ' . join ( attributes ) s += '\\n);\\n' return s", "predictions": ["fc a index index into a string ."], "references": ["serialize an xtuml metamodel class ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 5554, "code": "def serialize schema ( metamodel ) : s = '' for kind in sorted ( metamodel . metaclasses . keys ( ) ) : s += serialize class ( metamodel . metaclasses [ kind ] . clazz ) for ass in sorted ( metamodel . associations , key = lambda x : x . rel id ) : s += serialize association ( ass ) return s", "predictions": ["fc a bytes object into json format"], "references": ["serialize all class and association definitions in a * metamodel * ."], "bleu": 0.0909326471926252, "rouge_l": 0.10049423393739704}
{"id": 5555, "code": "def serialize ( resource ) : if isinstance ( resource , xtuml . Meta Model ) : return serialize database ( resource ) elif isinstance ( resource , type ) and issubclass ( resource , xtuml . Class ) : return serialize class ( resource ) elif isinstance ( resource , xtuml . Association ) : return serialize association ( resource ) elif isinstance ( resource , xtuml . Class ) : return serialize instance ( resource )", "predictions": ["pretty - print the feature instance"], "references": ["serialize some xtuml * resource * e . g . an instance or a complete metamodel ."], "bleu": 0.035316782215334214, "rouge_l": 0.0800524934383202}
{"id": 5556, "code": "def main ( ) : parser = Argument Parser ( description = \"search files using n-grams\" ) parser . add argument ( '--path' , dest = 'path' , help = \"where to search\" , nargs = 1 , action = \"store\" , default = getcwd ( ) ) parser . add argument ( '--update' , dest = 'update' , help = \"update the index\" , action = 'store true' , default = True ) parser . add argument ( '--filetype' , dest = 'filetype' , help = \"any, images, documents, code, audio, video\" , nargs = 1 , action = \"store\" , default = [ \"any\" ] ) parser . add argument ( '--verbose' , dest = 'verbose' , help = \"extended output\" , action = 'store true' , default = False ) parser . add argument ( '--results' , dest = 'results' , help = \"number of results to display\" , action = \"store\" , default = 10 ) parser . add argument ( 'query' , nargs = '+' , help = \"what to search\" , action = \"store\" ) args = parser . parse args ( ) if args . verbose : verbose = 2 pprint ( args ) else : verbose = 0 query = args . query [ 0 ] for arg in args . query [ 1 : ] : query = query + \" \" + arg slb = min ( [ len ( w ) for w in query . split ( \" \" ) ] ) files = Files ( path = args . path , filetype = args . filetype [ 0 ] , exclude = [ ] , update = args . update , verbose = verbose ) index = Index ( files , slb = slb , verbose = verbose ) results = index . search ( query , verbose = verbose ) Handler ( results , results number = int ( args . results ) )", "predictions": ["entry point for the application script ."], "references": ["function for command line execution"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 5557, "code": "def partition ( condition , collection ) -> Tuple [ List , List ] : succeed , fail = [ ] , [ ] for x in collection : if condition ( x ) : succeed . append ( x ) else : fail . append ( x ) return succeed , fail", "predictions": ["default default default default elements to fail . . ."], "references": ["partitions a list into two based on a condition ."], "bleu": 0.12605968092174913, "rouge_l": 0.1}
{"id": 5558, "code": "def find link ( inst1 , inst2 , rel id , phrase ) : metaclass1 = get metaclass ( inst1 ) metaclass2 = get metaclass ( inst2 ) if isinstance ( rel id , int ) : rel id = 'R%d' % rel id for ass in metaclass1 . metamodel . associations : if ass . rel id != rel id : continue if ( ass . source link . from metaclass . kind == metaclass1 . kind and ass . source link . to metaclass . kind == metaclass2 . kind and ass . source link . phrase == phrase ) : return inst1 , inst2 , ass if ( ass . target link . from metaclass . kind == metaclass1 . kind and ass . target link . to metaclass . kind == metaclass2 . kind and ass . target link . phrase == phrase ) : return inst2 , inst1 , ass raise Unknown Link Exception ( metaclass1 . kind , metaclass2 . kind , rel id , phrase )", "predictions": ["pretty - pretty - to - to - to - to - to - to - to - to - to - to - to - to - to - to"], "references": ["find links that correspond to the given arguments ."], "bleu": 0.03901663112717908, "rouge_l": 0.055505004549590536}
{"id": 5559, "code": "def get metaclass ( class or instance ) : if isinstance ( class or instance , Class ) : return class or instance . metaclass elif issubclass ( class or instance , Class ) : return class or instance . metaclass raise Meta Exception ( \"the provided argument is not an xtuml class or instance\" )", "predictions": ["returns the unique unique unique unique unique unique unique unique class"], "references": ["get the metaclass for a * class_or_instance * ."], "bleu": 0.11390778025531027, "rouge_l": 0.1018363939899833}
{"id": 5560, "code": "def disconnect ( self , instance , another instance ) : if instance not in self : return False if another instance not in self [ instance ] : return False self [ instance ] . remove ( another instance ) return True", "predictions": ["check if an m is closed"], "references": ["disconnect an * instance * from * another_instance * ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 5561, "code": "def attribute type ( self , attribute name ) : attribute name = attribute name . upper ( ) for name , ty in self . attributes : if name . upper ( ) == attribute name : return ty", "predictions": ["select the link link for the given check = value = none = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 ="], "references": ["obtain the type of an attribute ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 5562, "code": "def add link ( self , metaclass , rel id , phrase , conditional , many ) : link = Link ( self , rel id , metaclass , phrase , conditional , many ) key = ( metaclass . kind . upper ( ) , rel id , phrase ) self . links [ key ] = link return link", "predictions": ["check if this subtype is added to the current rel ."], "references": ["add a new link from * self * to * metaclass * ."], "bleu": 0.10510262682013449, "rouge_l": 0.1641991924629879}
{"id": 5563, "code": "def delete attribute ( self , name ) : for idx , attr in enumerate ( self . attributes ) : attr name , = attr if attr name == name : del self . attributes [ idx ] return", "predictions": ["basic basic transform transform"], "references": ["delete an attribute with a given * name * from the list of attributes ."], "bleu": 0.01931500670555844, "rouge_l": 0.0}
{"id": 5564, "code": "def default value ( self , type name ) : uname = type name . upper ( ) if uname == 'BOOLEAN' : return False elif uname == 'INTEGER' : return 0 elif uname == 'REAL' : return 0.0 elif uname == 'STRING' : return '' elif uname == 'UNIQUE ID' : if self . metamodel : return next ( self . metamodel . id generator ) else : return None else : raise Meta Exception ( \"Unknown type named '%s'\" % type name )", "predictions": ["17 s get type"], "references": ["obtain the default value for some * type name * ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 5565, "code": "def new ( self , * args , * * kwargs ) : inst = self . clazz ( ) self . storage . append ( inst ) referential attributes = dict ( ) for name , ty in self . attributes : if name not in self . referential attributes : value = self . default value ( ty ) setattr ( inst , name , value ) for attr , value in zip ( self . attributes , args ) : name , ty = attr if name not in self . referential attributes : setattr ( inst , name , value ) else : referential attributes [ name ] = value for name , value in kwargs . items ( ) : if name not in self . referential attributes : setattr ( inst , name , value ) else : referential attributes [ name ] = value if not referential attributes : return inst for link in self . links . values ( ) : if set ( link . key map . values ( ) ) - set ( referential attributes . keys ( ) ) : continue kwargs = dict ( ) for key , value in link . key map . items ( ) : kwargs [ key ] = referential attributes [ value ] if not kwargs : continue for other inst in link . to metaclass . query ( kwargs ) : relate ( other inst , inst , link . rel id , link . phrase ) for name , value in referential attributes . items ( ) : if getattr ( inst , name ) != value : logger . warning ( 'unable to assign %s to %s' , name , inst ) return inst", "predictions": ["create a new object from the storage"], "references": ["create and return a new instance ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 5566, "code": "def instances ( self ) : for metaclass in self . metaclasses . values ( ) : for inst in metaclass . storage : yield inst", "predictions": ["return an iterable of all build build build objects dt dt dt dt dt dt ."], "references": ["obtain a sequence of all instances in the metamodel ."], "bleu": 0.10878661088699644, "rouge_l": 0.24078947368421053}
{"id": 5567, "code": "def define class ( self , kind , attributes , doc = '' ) : ukind = kind . upper ( ) if ukind in self . metaclasses : raise Meta Model Exception ( 'A class with the name %s is already defined' % kind ) metaclass = Meta Class ( kind , self ) for name , ty in attributes : metaclass . append attribute ( name , ty ) self . metaclasses [ ukind ] = metaclass return metaclass", "predictions": ["add a enum enum enum to this lambda . . . . . . . ."], "references": ["define a new class in the metamodel and return its metaclass ."], "bleu": 0.08513012360883544, "rouge_l": 0.14663461538461536}
{"id": 5568, "code": "def find metaclass ( self , kind ) : ukind = kind . upper ( ) if ukind in self . metaclasses : return self . metaclasses [ ukind ] else : raise Unknown Class Exception ( kind )", "predictions": ["find a metaclass by kind"], "references": ["find a metaclass of some * kind * in the metamodel ."], "bleu": 0.13249284136531225, "rouge_l": 0.4380610412926392}
{"id": 5569, "code": "def dead code ( ) : with safe cd ( SRC ) : if IS TRAVIS : command = \"{0} vulture {1}\" . format ( PYTHON , PROJECT NAME ) . strip ( ) . split ( ) else : command = \"{0} vulture {1}\" . format ( PIPENV , PROJECT NAME ) . strip ( ) . split ( ) output file name = \"dead code.txt\" with open ( output file name , \"w\" ) as outfile : env = config pythonpath ( ) subprocess . call ( command , stdout = outfile , env = env ) cutoff = 20 num lines = sum ( 1 for line in open ( output file name ) if line ) if num lines > cutoff : print ( \"Too many lines of dead code : {0}, max {1}\" . format ( num lines , cutoff ) ) exit ( - 1 )", "predictions": ["write dead code to pypi ."], "references": ["this also finds code you are working on today!"], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 5570, "code": "def parse emails ( values ) : emails = [ ] if isinstance ( values , str ) : values = [ values ] for value in values : matches = re emails . findall ( value ) emails . extend ( [ match [ 2 ] for match in matches ] ) return emails", "predictions": ["parse emails from list ."], "references": ["take a string or list of strings and try to extract all the emails"], "bleu": 0.04994299940831281, "rouge_l": 0.09697933227344992}
{"id": 5571, "code": "def rpc ( f = None , * * kwargs ) : if f is not None : if isinstance ( f , six . string types ) : if 'name' in kwargs : raise Value Error ( 'name option duplicated' ) kwargs [ 'name' ] = f else : return rpc ( * * kwargs ) ( f ) return functools . partial ( rpc , * * kwargs )", "predictions": ["decorator to add an rpc method to the given function ."], "references": ["marks a method as rpc ."], "bleu": 0.1354599427337814, "rouge_l": 0.2484725050916497}
{"id": 5572, "code": "def rpc spec table ( app ) : table = { } for attr , value in inspect . getmembers ( app ) : rpc spec = get rpc spec ( value , default = None ) if rpc spec is None : continue table [ rpc spec . name ] = ( value , rpc spec ) return table", "predictions": ["return a table spec for an rpc app ."], "references": ["collects methods which are speced as rpc ."], "bleu": 0.15619699684601276, "rouge_l": 0.2378167641325536}
{"id": 5573, "code": "async def normalize postcode middleware ( request , handler ) : postcode : Optional [ str ] = request . match info . get ( 'postcode' , None ) if postcode is None or postcode == \"random\" : return await handler ( request ) elif not is uk postcode ( postcode ) : raise web . HTTP Not Found ( text = \"Invalid Postcode\" ) postcode processed = postcode . upper ( ) . replace ( \" \" , \"\" ) if postcode processed == postcode : return await handler ( request ) else : url name = request . match info . route . name url = request . app . router [ url name ] params = dict ( request . match info ) params [ 'postcode' ] = postcode processed raise web . HTTP Moved Permanently ( str ( url . url for ( * * params ) ) )", "predictions": ["normalizes postcode middleware ."], "references": ["if there is a postcode in the url it validates and normalizes it ."], "bleu": 0.03507385333204471, "rouge_l": 0.20198675496688742}
{"id": 5574, "code": "def next ( self ) : val = self . current self . current = self . readfunc ( ) return val", "predictions": ["return the next row in the iteration ."], "references": ["progress to the next identifier and return the current one ."], "bleu": 0.1909027782642041, "rouge_l": 0.4093959731543625}
{"id": 5575, "code": "def accept S SYS ( self , inst ) : for child in many ( inst ) . EP PKG [ 1401 ] ( ) : self . accept ( child )", "predictions": ["accept all s sys"], "references": ["a system model contains top - level packages"], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 5576, "code": "def accept C C ( self , inst ) : for child in many ( inst ) . PE PE [ 8003 ] ( ) : self . accept ( child )", "predictions": ["accept all children of this widget ."], "references": ["a component contains packageable elements"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5577, "code": "def accept EP PKG ( self , inst ) : for child in many ( inst ) . PE PE [ 8000 ] ( ) : self . accept ( child )", "predictions": ["accept all the children of this widget ."], "references": ["a package contains packageable elements"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5578, "code": "def get brightness ( self ) : if not self . connection . has changed ( ) : return self . image brightness image path = self . connection . download image ( ) converted image = Image . open ( image path ) . convert ( 'L' ) statistics = Image Stat . Stat ( converted image ) self . image brightness = statistics . mean [ 0 ] return self . image brightness", "predictions": ["returns the brightness brightness brightness"], "references": ["return the average brightness of the image ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 5579, "code": "def selection for character ( self , position ) : selection = Qt Gui . Q Text Edit . Extra Selection ( ) cursor = self . text edit . text Cursor ( ) cursor . set Position ( position ) cursor . move Position ( Qt Gui . Q Text Cursor . Next Character , Qt Gui . Q Text Cursor . Keep Anchor ) selection . cursor = cursor selection . format = self . format return selection", "predictions": ["return the selection of the selection in the character"], "references": ["convenience method for selecting a character ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 5580, "code": "def cursor position changed ( self ) : self . text edit . set Extra Selections ( [ ] ) cursor = self . text edit . text Cursor ( ) if not cursor . has Selection ( ) : position = cursor . position ( ) - 1 match position = self . find match ( position ) if match position != - 1 : extra selections = [ self . selection for character ( pos ) for pos in ( position , match position ) ] self . text edit . set Extra Selections ( extra selections )", "predictions": ["changes the cursor position of the cursor position ."], "references": ["updates the document formatting based on the new cursor position ."], "bleu": 0.23278666914796883, "rouge_l": 0.4911433172302737}
{"id": 5581, "code": "def exc info ( self ) : e = self . exc info ( ) if sys . platform == 'cli' : if isinstance ( e [ 0 ] , String Exception ) : e = ( str ( e [ 0 ] ) , e [ 1 ] , e [ 2 ] ) return e", "predictions": ["return the exc info for the exc"], "references": ["bottleneck to fix up ironpython string exceptions"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5582, "code": "def run ( self , result ) : log . debug ( \"suite %s (%s) run called, tests: %s\" , id ( self ) , self , self . tests ) #import pdb #pdb.set trace() if self . result Proxy : result , orig = self . result Proxy ( result , self ) , result else : result , orig = result , result try : self . set Up ( ) except Keyboard Interrupt : raise except : self . error context = 'setup' result . add Error ( self , self . exc info ( ) ) return try : for test in self . tests : if result . should Stop : log . debug ( \"stopping\" ) break test ( orig ) finally : self . has run = True try : self . tear Down ( ) except Keyboard Interrupt : raise except : self . error context = 'teardown' result . add Error ( self , self . exc info ( ) )", "predictions": ["run the result ."], "references": ["run tests in suite inside of suite fixtures ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 5583, "code": "def options ( self , parser , env ) : parser . add option ( '--collect-only' , action = 'store true' , dest = self . enable Opt , default = env . get ( 'NOSE COLLECT ONLY' ) , help = \"Enable collect-only: %s [COLLECT ONLY]\" % ( self . help ( ) ) )", "predictions": ["add command line options ."], "references": ["register commandline options ."], "bleu": 0.35930411196308426, "rouge_l": 0.4535315985130111}
{"id": 5584, "code": "def execute ( self , source = None , hidden = False , interactive = False ) : if not hidden : history = self . input buffer if source is None else source executed = super ( History Console Widget , self ) . execute ( source , hidden , interactive ) if executed and not hidden : history = history . rstrip ( ) if history and ( not self . history or self . history [ - 1 ] != history ) : self . history . append ( history ) self . history edits = { } self . history index = len ( self . history ) return executed", "predictions": ["execute the history of the history ."], "references": ["reimplemented to the store history ."], "bleu": 0.2626909894424158, "rouge_l": 0.4680306905370844}
{"id": 5585, "code": "def handle execute reply ( self , msg ) : msg id = msg [ 'parent header' ] [ 'msg id' ] info = self . request info [ 'execute' ] . pop ( msg id , None ) if info and info . kind == 'save magic' and not self . hidden : content = msg [ 'content' ] status = content [ 'status' ] if status == 'ok' : self . max session history = ( int ( content [ 'user expressions' ] [ 'hlen' ] ) )", "predictions": ["process incoming reply messages ."], "references": ["handles replies for code execution here only session history length"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 5586, "code": "def history locked ( self ) : return ( self . history lock and ( self . get edited history ( self . history index ) != self . input buffer ) and ( self . get prompt cursor ( ) . block Number ( ) != self . get end cursor ( ) . block Number ( ) ) )", "predictions": ["return true if the history is locked"], "references": ["returns whether history movement is locked ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 5587, "code": "def get edited history ( self , index ) : if index in self . history edits : return self . history edits [ index ] elif index == len ( self . history ) : return unicode ( ) return self . history [ index ]", "predictions": ["return the edited history for the given index"], "references": ["retrieves a history item possibly with temporary edits ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 5588, "code": "def set history ( self , history ) : self . history = list ( history ) self . history edits = { } self . history index = len ( self . history )", "predictions": ["set the history of the history"], "references": ["replace the current history with a sequence of history items ."], "bleu": 0.12071482560966854, "rouge_l": 0.4468864468864468}
{"id": 5589, "code": "def store edits ( self ) : current = self . input buffer if self . history index == len ( self . history ) or self . history [ self . history index ] != current : self . history edits [ self . history index ] = current", "predictions": ["store the edits of the history buffer ."], "references": ["if there are edits to the current input buffer store them ."], "bleu": 0.12801036176909558, "rouge_l": 0.3860759493670886}
{"id": 5590, "code": "def On Time To Close ( self , evt ) : print ( \"See ya later!\" ) sys . stdout . flush ( ) self . cleanup consoles ( evt ) self . Close ( ) sys . exit ( )", "predictions": ["print the process and exit ."], "references": ["event handler for the button click ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 5591, "code": "def cleanup files ( self ) : logger . debug ( 'Cleaning up...' ) with indent log ( ) : for req in self . reqs to cleanup : req . remove temporary source ( ) if self . pip has created build dir ( ) : logger . debug ( 'Removing temporary dir %s...' , self . build dir ) rmtree ( self . build dir )", "predictions": ["remove all pip files ."], "references": ["clean up files remove builds ."], "bleu": 0.2658156069371863, "rouge_l": 0.3577712609970674}
{"id": 5592, "code": "def subscribe ( self ) : self . stream . setsockopt ( zmq . UNSUBSCRIBE , '' ) if '' in self . topics : self . log . debug ( \"Subscribing to: everything\" ) self . stream . setsockopt ( zmq . SUBSCRIBE , '' ) else : for topic in self . topics : self . log . debug ( \"Subscribing to: %r\" % ( topic ) ) self . stream . setsockopt ( zmq . SUBSCRIBE , topic )", "predictions": ["subscribe to the stream ."], "references": ["update our sub socket s subscriptions ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 5593, "code": "def log message ( self , raw ) : if len ( raw ) != 2 or '.' not in raw [ 0 ] : self . log . error ( \"Invalid log message: %s\" % raw ) return else : topic , msg = raw topic , level name = topic . rsplit ( '.' , 1 ) level , topic = self . extract level ( topic ) if msg [ - 1 ] == '\\n' : msg = msg [ : - 1 ] self . log . log ( level , \"[%s] %s\" % ( topic , msg ) )", "predictions": ["log a message to the log ."], "references": ["receive and parse a message then log it ."], "bleu": 0.20873176328735715, "rouge_l": 0.48897795591182364}
{"id": 5594, "code": "def remote iterator ( view , name ) : view . execute ( 'it%s=iter(%s)' % ( name , name ) , block = True ) while True : try : result = view . apply sync ( lambda x : x . next ( ) , Reference ( 'it' + name ) ) except Remote Error as e : if e . ename == 'Stop Iteration' : raise Stop Iteration else : raise e else : yield result", "predictions": ["return an iterator over the remote iterator ."], "references": ["return an iterator on an object living on a remote engine ."], "bleu": 0.20034704329441452, "rouge_l": 0.4825949367088607}
{"id": 5595, "code": "def String IO ( * args , * * kw ) : global String IO try : from c String IO import String IO except Import Error : from String IO import String IO return String IO ( * args , * * kw )", "predictions": ["return a string with the given args ."], "references": ["thunk to load the real stringio on demand"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 5596, "code": "def insert on ( self , path , loc = None ) : loc = loc or self . location if self . project name == 'setuptools' : try : version = self . version except Value Error : version = '' if '0.7' in version : raise Value Error ( \"A 0.7-series setuptools cannot be installed \" \"with distribute. Found one at %s\" % str ( self . location ) ) if not loc : return if path is sys . path : self . check version conflict ( ) nloc = normalize cached ( loc ) bdir = os . path . dirname ( nloc ) npath = map ( normalize cached , path ) bp = None for p , item in enumerate ( npath ) : if item == nloc : break elif item == bdir and self . precedence == EGG DIST : path . insert ( p , loc ) npath . insert ( p , nloc ) break else : path . append ( loc ) return while 1 : try : np = npath . index ( nloc , p + 1 ) except Value Error : break else : del npath [ np ] , path [ np ] p = np return", "predictions": ["insert a new location into the project ."], "references": ["insert self . location in path before its nearest parent directory"], "bleu": 0.13107175678306446, "rouge_l": 0.20469798657718125}
{"id": 5597, "code": "def parsed pkg info ( self ) : try : return self . pkg info except Attribute Error : from email . parser import Parser self . pkg info = Parser ( ) . parsestr ( self . get metadata ( self . PKG INFO ) ) return self . pkg info", "predictions": ["return the current info"], "references": ["parse and cache metadata"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 5598, "code": "def compute dependencies ( self ) : from markerlib import compile as compile marker dm = self . dep map = { None : [ ] } reqs = [ ] for req in self . parsed pkg info . get all ( 'Requires-Dist' ) or [ ] : distvers , mark = self . preparse requirement ( req ) parsed = parse requirements ( distvers ) . next ( ) parsed . marker fn = compile marker ( mark ) reqs . append ( parsed ) def reqs for extra ( extra ) : for req in reqs : if req . marker fn ( override = { 'extra' : extra } ) : yield req common = frozenset ( reqs for extra ( None ) ) dm [ None ] . extend ( common ) for extra in self . parsed pkg info . get all ( 'Provides-Extra' ) or [ ] : extra = safe extra ( extra . strip ( ) ) dm [ extra ] = list ( frozenset ( reqs for extra ( extra ) ) - common ) return dm", "predictions": ["compute all dependencies of the markerlib ."], "references": ["recompute this distribution s dependencies ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 5599, "code": "def collapse leading ws ( header , txt ) : if header . lower ( ) == 'description' : return '\\n' . join ( [ x [ 8 : ] if x . startswith ( ' ' * 8 ) else x for x in txt . strip ( ) . splitlines ( ) ] ) else : return ' ' . join ( [ x . strip ( ) for x in txt . splitlines ( ) ] )", "predictions": ["collapse a string into a list of strings"], "references": ["description header must preserve newlines ; all others need not"], "bleu": 0.10502215675986959, "rouge_l": 0.0}
{"id": 5600, "code": "def hide Event ( self , event ) : super ( Completion Widget , self ) . hide Event ( event ) self . text edit . cursor Position Changed . disconnect ( self . update current ) self . text edit . remove Event Filter ( self )", "predictions": ["reimplemented to handle the if click click upper - else"], "references": ["reimplemented to disconnect signal handlers and event filter ."], "bleu": 0.16590387014219712, "rouge_l": 0.21254355400696867}
{"id": 5601, "code": "def show Event ( self , event ) : super ( Completion Widget , self ) . show Event ( event ) self . text edit . cursor Position Changed . connect ( self . update current ) self . text edit . install Event Filter ( self )", "predictions": ["reimplemented to handle command click ."], "references": ["reimplemented to connect signal handlers and event filter ."], "bleu": 0.1894765350842885, "rouge_l": 0.3860759493670886}
{"id": 5602, "code": "def complete current ( self ) : self . current text cursor ( ) . insert Text ( self . current Item ( ) . text ( ) ) self . hide ( )", "predictions": ["marks the emails as emails as emails"], "references": ["perform the completion with the currently selected item ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 5603, "code": "def update current ( self ) : prefix = self . current text cursor ( ) . selection ( ) . to Plain Text ( ) if prefix : items = self . find Items ( prefix , ( Qt Core . Qt . Match Starts With | Qt Core . Qt . Match Case Sensitive ) ) if items : self . set Current Item ( items [ 0 ] ) else : self . hide ( ) else : self . hide ( )", "predictions": ["rpc the current based on the current . . . . . . . . . . . . . . . . . . . . . . . ."], "references": ["updates the current item based on the current text ."], "bleu": 0.12913533075470382, "rouge_l": 0.37621145374449333}
{"id": 5604, "code": "def register Admin Site ( app Name , exclude Models = [ ] ) : for model in apps . get app config ( app Name ) . get models ( ) : if model not in exclude Models : admin . site . register ( model )", "predictions": ["rpc admin to name"], "references": ["registers the models of the app with the given appname for the admin site"], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 5605, "code": "def virtual memory ( ) : mem = psutil mswindows . get virtual mem ( ) totphys , availphys , totpagef , availpagef , totvirt , freevirt = mem # total = totphys avail = availphys free = availphys used = total - avail percent = usage percent ( ( total - avail ) , total , round = 1 ) return nt virtmem info ( total , avail , percent , used , free )", "predictions": ["returns the normalize normalize normalize normalize"], "references": ["system virtual memory as a namedtuple ."], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 5606, "code": "def get disk usage ( path ) : try : total , free = psutil mswindows . get disk usage ( path ) except Windows Error : err = sys . exc info ( ) [ 1 ] if not os . path . exists ( path ) : raise OS Error ( errno . ENOENT , \"No such file or directory: '%s'\" % path ) raise used = total - free percent = usage percent ( used , total , round = 1 ) return nt diskinfo ( total , used , free , percent )", "predictions": ["next disk self ."], "references": ["return disk usage associated with path ."], "bleu": 0.1878296463217631, "rouge_l": 0.346590909090909}
{"id": 5607, "code": "def disk partitions ( all ) : rawlist = psutil mswindows . get disk partitions ( all ) return [ nt partition ( * x ) for x in rawlist ]", "predictions": ["many accept accept partitions partitions"], "references": ["return disk partitions ."], "bleu": 0.2730120862709067, "rouge_l": 0.22676579925650556}
{"id": 5608, "code": "def get system cpu times ( ) : user , system , idle = 0 , 0 , 0 for cpu time in psutil mswindows . get system cpu times ( ) : user += cpu time [ 0 ] system += cpu time [ 1 ] idle += cpu time [ 2 ] return cputimes ntuple ( user , system , idle )", "predictions": ["returns the cpu cpu self . self ."], "references": ["return system cpu times as a named tuple ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 5609, "code": "def get system per cpu times ( ) : ret = [ ] for cpu t in psutil mswindows . get system cpu times ( ) : user , system , idle = cpu t item = cputimes ntuple ( user , system , idle ) ret . append ( item ) return ret", "predictions": ["return a list of all system per per"], "references": ["return system per - cpu times as a list of named tuples ."], "bleu": 0.19745471508497758, "rouge_l": 0.3652694610778443}
{"id": 5610, "code": "def get system users ( ) : retlist = [ ] rawlist = psutil mswindows . get system users ( ) for item in rawlist : user , hostname , tstamp = item nt = nt user ( user , None , hostname , tstamp ) retlist . append ( nt ) return retlist", "predictions": ["mean of all users users"], "references": ["return currently connected users as a list of namedtuples ."], "bleu": 0.11115018927487523, "rouge_l": 0.12577319587628866}
{"id": 5611, "code": "def stdin raw nonblock ( self ) : handle = msvcrt . get osfhandle ( sys . stdin . fileno ( ) ) result = Wait For Single Object ( handle , 100 ) if result == WAIT FAILED : raise ctypes . Win Error ( ) elif result == WAIT TIMEOUT : print ( \".\" , end = '' ) return None else : data = ctypes . create string buffer ( 256 ) bytes Read = DWORD ( 0 ) print ( '?' , end = '' ) if not Read File ( handle , data , 256 , ctypes . byref ( bytes Read ) , None ) : raise ctypes . Win Error ( ) Flush Console Input Buffer ( handle ) data = data . value data = data . replace ( '\\r\\n' , '\\n' ) data = data . replace ( '\\r' , '\\n' ) print ( repr ( data ) + \" \" , end = '' ) return data", "predictions": ["selection for selection ."], "references": ["use the raw win32 handle of sys . stdin to do non - blocking reads"], "bleu": 0.022969543400575367, "rouge_l": 0.0953125}
{"id": 5612, "code": "def stdin raw block ( self ) : try : data = sys . stdin . read ( 1 ) data = data . replace ( '\\r' , '\\n' ) return data except Windows Error as we : if we . winerror == ERROR NO DATA : return None else : raise we", "predictions": ["not thread - safe ."], "references": ["use a blocking stdin read"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 5613, "code": "def stdout raw ( self , s ) : print ( s , end = '' , file = sys . stdout ) sys . stdout . flush ( )", "predictions": ["write exc data to exc . . . . . . . . . ."], "references": ["writes the string to stdout"], "bleu": 0.08225964699966554, "rouge_l": 0.10990990990990988}
{"id": 5614, "code": "def stderr raw ( self , s ) : print ( s , end = '' , file = sys . stderr ) sys . stderr . flush ( )", "predictions": ["write raw to run . ."], "references": ["writes the string to stdout"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 5615, "code": "def create tab with current kernel ( self ) : current widget = self . tab widget . current Widget ( ) current widget index = self . tab widget . index Of ( current widget ) current widget name = self . tab widget . tab Text ( current widget index ) widget = self . slave frontend factory ( current widget ) if 'slave' in current widget name : name = current widget name else : name = '(%s) slave' % current widget name self . add tab with frontend ( widget , name = name )", "predictions": ["options self self self self . self ."], "references": ["create a new frontend attached to the same kernel as the current tab"], "bleu": 0.0721806023765632, "rouge_l": 0.0}
{"id": 5616, "code": "def add tab with frontend ( self , frontend , name = None ) : if not name : name = 'kernel %i' % self . next kernel id self . tab widget . add Tab ( frontend , name ) self . update tab bar visibility ( ) self . make frontend visible ( frontend ) frontend . exit requested . connect ( self . close tab )", "predictions": ["execute a tab self self buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer buffer"], "references": ["insert a tab with a given frontend in the tab bar and give it a name"], "bleu": 0.0513487742994337, "rouge_l": 0.09030347890451518}
{"id": 5617, "code": "def close Event ( self , event ) : if self . tab widget . count ( ) == 0 : event . accept ( ) return title = self . window ( ) . window Title ( ) cancel = Qt Gui . Q Message Box . Cancel okay = Qt Gui . Q Message Box . Ok if self . confirm exit : if self . tab widget . count ( ) > 1 : msg = \"Close all tabs, stop all kernels, and Quit?\" else : msg = \"Close console, stop kernel, and Quit?\" info = \"Kernels not started here (e.g. notebooks) will be left alone.\" closeall = Qt Gui . Q Push Button ( \"&Quit\" , self ) closeall . set Shortcut ( 'Q' ) box = Qt Gui . Q Message Box ( Qt Gui . Q Message Box . Question , title , msg ) box . set Informative Text ( info ) box . add Button ( cancel ) box . add Button ( closeall , Qt Gui . Q Message Box . Yes Role ) box . set Default Button ( closeall ) box . set Escape Button ( cancel ) pixmap = Qt Gui . Q Pixmap ( self . app . icon . pixmap ( Qt Core . Q Size ( 64 , 64 ) ) ) box . set Icon Pixmap ( pixmap ) reply = box . exec ( ) else : reply = okay if reply == cancel : event . ignore ( ) return if reply == okay : while self . tab widget . count ( ) >= 1 : widget = self . active frontend widget . confirm exit = False self . close tab ( widget ) event . accept ( )", "predictions": ["qt method override this method is closed"], "references": ["forward the close event to every tabs contained by the windows"], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 5618, "code": "def toggle boolean ( self , request ) : try : item id = int ( request . POST . get ( 'item id' , None ) ) attr = str ( request . POST . get ( 'attr' , None ) ) except : return Http Response Bad Request ( \"Malformed request\" ) if not request . user . is staff : logging . warning ( \"Denied AJAX request by non-staff %s to toggle boolean %s for object #%s\" , request . user , attr , item id ) return Http Response Forbidden ( \"You do not have permission to access this object\" ) self . collect editable booleans ( ) if not self . ajax editable booleans . has key ( attr ) : return Http Response Bad Request ( \"not a valid attribute %s\" % attr ) try : obj = self . model . default manager . get ( pk = item id ) except self . model . Does Not Exist : return Http Response Not Found ( \"Object does not exist\" ) can change = False if hasattr ( obj , \"user can\" ) and obj . user can ( request . user , change page = True ) : can change = True else : can change = self . has change permission ( request , obj = obj ) if not can change : logging . warning ( \"Denied AJAX request by %s to toggle boolean %s for object %s\" , request . user , attr , item id ) return Http Response Forbidden ( \"You do not have permission to access this object\" ) logging . info ( \"Processing request by %s to toggle %s on %s\" , request . user , attr , obj ) try : before data = self . ajax editable booleans [ attr ] ( self , obj ) setattr ( obj , attr , not getattr ( obj , attr ) ) obj . save ( ) self . refresh changelist caches ( ) data = self . ajax editable booleans [ attr ] ( self , obj ) except Exception : #, e: logging . exception ( \"Unhandled exception while toggling %s on %s\" , attr , obj ) return Http Response Server Error ( \"Unable to toggle %s on %s\" % ( attr , obj ) ) d = [ ] for a , b in zip ( before data , data ) : if a != b : d . append ( b ) return Http Response ( json . dumps ( d ) , mimetype = \"application/json\" )", "predictions": ["the locked is always always a locked locked"], "references": ["handle an ajax toggle_boolean request"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5619, "code": "def add children ( G , parent , level , n = 2 ) : if level == 0 : return for i in range ( n ) : child = parent + str ( i ) G . add node ( child ) G . add edge ( parent , child ) add children ( G , child , level - 1 , n )", "predictions": ["get all edited of a parent"], "references": ["add children recursively to a binary tree ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 5620, "code": "def make bintree ( levels ) : G = nx . Di Graph ( ) root = '0' G . add node ( root ) add children ( G , root , levels , 2 ) return G", "predictions": ["create a history from its self list list list list list list list list list of self list list of self list list list of self list list self list self"], "references": ["make a symmetrical binary tree with"], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 5621, "code": "def submit jobs ( view , G , jobs ) : results = { } for node in nx . topological sort ( G ) : with view . temp flags ( after = [ results [ n ] for n in G . predecessors ( node ) ] ) : results [ node ] = view . apply ( jobs [ node ] ) return results", "predictions": ["store edits of predecessors or list of predecessors"], "references": ["submit jobs via client where g describes the time dependencies ."], "bleu": 0.09268172804333874, "rouge_l": 0.0}
{"id": 5622, "code": "def validate tree ( G , results ) : for node in G : started = results [ node ] . metadata . started for parent in G . predecessors ( node ) : finished = results [ parent ] . metadata . completed assert started > finished , \"%s should have happened after %s\" % ( node , parent )", "predictions": ["validate up the tree tree ."], "references": ["validate that jobs executed after their dependencies ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 5623, "code": "def copy ( self , name = None ) : if name is None : name = self . name return Color Scheme ( name , self . colors . dict ( ) )", "predictions": ["cleanup a cleanup object"], "references": ["return a full copy of the object optionally renaming it ."], "bleu": 0.06909866532427987, "rouge_l": 0.24596774193548387}
{"id": 5624, "code": "def add scheme ( self , new scheme ) : if not isinstance ( new scheme , Color Scheme ) : raise Value Error , 'Color Scheme Table only accepts Color Scheme instances' self [ new scheme . name ] = new scheme", "predictions": ["subscribe to the scheme scheme"], "references": ["add a new color scheme to the table ."], "bleu": 0.17348474258688365, "rouge_l": 0.2717149220489978}
{"id": 5625, "code": "def home lib ( home ) : if hasattr ( sys , 'pypy version info' ) : lib = 'site-packages' else : lib = os . path . join ( 'lib' , 'python' ) return os . path . join ( home , lib )", "predictions": ["return the path to the directory where the log is downloaded"], "references": ["return the lib dir under the home installation scheme"], "bleu": 0.16108992769687397, "rouge_l": 0.3055091819699499}
{"id": 5626, "code": "def handle stdin request ( self , timeout = 0.1 ) : msg rep = self . km . stdin channel . get msg ( timeout = timeout ) self . handle iopub ( ) if self . session id == msg rep [ \"parent header\" ] . get ( \"session\" ) : real handler = signal . getsignal ( signal . SIGINT ) def double int ( sig , frame ) : real handler ( sig , frame ) raise Keyboard Interrupt signal . signal ( signal . SIGINT , double int ) try : raw data = raw input ( msg rep [ \"content\" ] [ \"prompt\" ] ) except EOF Error : raw data = '\\x04' except Keyboard Interrupt : sys . stdout . write ( '\\n' ) return finally : signal . signal ( signal . SIGINT , real handler ) if not ( self . km . stdin channel . msg ready ( ) or self . km . shell channel . msg ready ( ) ) : self . km . stdin channel . input ( raw data )", "predictions": ["handle a single iterator = output = { % } = { % } = 0 } = { % } = available = 0 } = { % } ="], "references": ["method to capture raw_input"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5627, "code": "def wait for kernel ( self , timeout = None ) : tic = time . time ( ) self . km . hb channel . unpause ( ) while True : self . run cell ( '1' , False ) if self . km . hb channel . is beating ( ) : break else : if timeout is not None and ( time . time ( ) - tic ) > timeout : return False return True", "predictions": ["wait for until the kernel is stopped import"], "references": ["method to wait for a kernel to be ready"], "bleu": 0.20014292374951972, "rouge_l": 0.34923664122137404}
{"id": 5628, "code": "def interact ( self , display banner = None ) : if self . exit now : return if display banner is None : display banner = self . display banner if isinstance ( display banner , basestring ) : self . show banner ( display banner ) elif display banner : self . show banner ( ) more = False if not self . wait for kernel ( 3 ) : error ( \"Kernel did not respond\\n\" ) return if self . has readline : self . readline startup hook ( self . pre readline ) hlen b4 cell = self . readline . get current history length ( ) else : hlen b4 cell = 0 while not self . exit now : if not self . km . is alive : action = \"restart\" if self . km . has kernel else \"wait for restart\" ans = self . ask yes no ( \"kernel died, %s ([y]/n)?\" % action , default = 'y' ) if ans : if self . km . has kernel : self . km . restart kernel ( True ) self . wait for kernel ( 3 ) else : self . exit now = True continue try : self . hooks . pre prompt hook ( ) if more : try : prompt = self . prompt manager . render ( 'in2' ) except Exception : self . showtraceback ( ) if self . autoindent : self . rl do indent = True else : try : prompt = self . separate in + self . prompt manager . render ( 'in' ) except Exception : self . showtraceback ( ) line = self . raw input ( prompt ) if self . exit now : break if self . autoindent : self . rl do indent = False except Keyboard Interrupt : #double-guard against keyboardinterrupts during kbdint handling try : self . write ( '\\n Keyboard Interrupt\\n' ) source raw = self . input splitter . source raw reset ( ) [ 1 ] hlen b4 cell = self . replace rlhist multiline ( source raw , hlen b4 cell ) more = False except Keyboard Interrupt : pass except EOF Error : if self . autoindent : self . rl do indent = False if self . has readline : self . readline startup hook ( None ) self . write ( '\\n' ) self . exit ( ) except bdb . Bdb Quit : warn ( 'The Python debugger has exited with a Bdb Quit exception.\\n' 'Because of how pdb handles the stack, it is impossible\\n' 'for I Python to properly format this particular exception.\\n' 'I Python will resume normal operation.' ) except : self . showtraceback ( ) else : self . input splitter . push ( line ) more = self . input splitter . push accepts more ( ) if ( self . Syntax TB . last syntax error and self . autoedit syntax ) : self . edit syntax error ( ) if not more : source raw = self . input splitter . source raw reset ( ) [ 1 ] hlen b4 cell = self . replace rlhist multiline ( source raw , hlen b4 cell ) self . run cell ( source raw ) self . exit now = False", "predictions": ["starts the main loop or take a new one"], "references": ["closely emulate the interactive python console ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 5629, "code": "def set style ( self , style ) : if isinstance ( style , basestring ) : style = get style by name ( style ) self . style = style self . clear caches ( )", "predictions": ["sets the pkg object"], "references": ["sets the style to the specified pygments style ."], "bleu": 0.1354797537848421, "rouge_l": 0.28773584905660377}
{"id": 5630, "code": "def get format ( self , token ) : if token in self . formats : return self . formats [ token ] if self . style is None : result = self . get format from document ( token , self . document ) else : result = self . get format from style ( token , self . style ) self . formats [ token ] = result return result", "predictions": ["dm the dependencies of a token token"], "references": ["returns a qtextcharformat for token or none ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 5631, "code": "def get format from document ( self , token , document ) : code , html = self . formatter . format lines ( [ ( token , u'dummy' ) ] ) . next ( ) self . document . set Html ( html ) return Qt Gui . Q Text Cursor ( self . document ) . char Format ( )", "predictions": ["collapse the leading leading leading leading . . ."], "references": ["returns a qtextcharformat for token by"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5632, "code": "def get format from style ( self , token , style ) : result = Qt Gui . Q Text Char Format ( ) for key , value in style . style for token ( token ) . items ( ) : if value : if key == 'color' : result . set Foreground ( self . get brush ( value ) ) elif key == 'bgcolor' : result . set Background ( self . get brush ( value ) ) elif key == 'bold' : result . set Font Weight ( Qt Gui . Q Font . Bold ) elif key == 'italic' : result . set Font Italic ( True ) elif key == 'underline' : result . set Underline Style ( Qt Gui . Q Text Char Format . Single Underline ) elif key == 'sans' : result . set Font Style Hint ( Qt Gui . Q Font . Sans Serif ) elif key == 'roman' : result . set Font Style Hint ( Qt Gui . Q Font . Times ) elif key == 'mono' : result . set Font Style Hint ( Qt Gui . Q Font . Type Writer ) return result", "predictions": ["get the format for the given style style style ."], "references": ["returns a qtextcharformat for token by reading a pygments style ."], "bleu": 0.1613101715871968, "rouge_l": 0.28328173374613}
{"id": 5633, "code": "def find command ( cmd , paths = None , pathext = None ) : if paths is None : paths = os . environ . get ( 'PATH' , '' ) . split ( os . pathsep ) if isinstance ( paths , six . string types ) : paths = [ paths ] if pathext is None : pathext = get pathext ( ) pathext = [ ext for ext in pathext . lower ( ) . split ( os . pathsep ) if len ( ext ) ] if os . path . splitext ( cmd ) [ 1 ] . lower ( ) in pathext : pathext = [ '' ] for path in paths : cmd path = os . path . join ( path , cmd ) for ext in pathext : cmd path ext = cmd path + ext if os . path . isfile ( cmd path ext ) : return cmd path ext if os . path . isfile ( cmd path ) : return cmd path raise Bad Command ( 'Cannot find command %r' % cmd )", "predictions": ["find the command in the given environment ."], "references": ["searches the path for the given command and returns its path"], "bleu": 0.16481400866629634, "rouge_l": 0.3070469798657718}
{"id": 5634, "code": "def normalize path ( path ) : return os . path . normcase ( os . path . realpath ( os . path . expanduser ( path ) ) )", "predictions": ["normalize path to the path of the path ."], "references": ["convert a path to its canonical case - normalized absolute version ."], "bleu": 0.1430210741102858, "rouge_l": 0.2785388127853881}
{"id": 5635, "code": "def check nsp ( dist , attr , value ) : assert string list ( dist , attr , value ) for nsp in value : if not dist . has contents for ( nsp ) : raise Distutils Setup Error ( \"Distribution contains no modules or packages for \" + \"namespace package %r\" % nsp ) if '.' in nsp : parent = '.' . join ( nsp . split ( '.' ) [ : - 1 ] ) if parent not in value : distutils . log . warn ( \"%r is declared as a package namespace, but %r is not:\" \" please correct this in setup.py\" , nsp , parent )", "predictions": ["check that the value is valid ."], "references": ["verify that namespace packages are valid"], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 5636, "code": "def check extras ( dist , attr , value ) : try : for k , v in value . items ( ) : list ( pkg resources . parse requirements ( v ) ) except ( Type Error , Value Error , Attribute Error ) : raise Distutils Setup Error ( \"'extras require' must be a dictionary whose values are \" \"strings or lists of strings containing valid project/version \" \"requirement specifiers.\" )", "predictions": ["verify that value is valid ."], "references": ["verify that extras_require mapping is valid"], "bleu": 0.3655552228545124, "rouge_l": 0.6666666666666666}
{"id": 5637, "code": "def check entry points ( dist , attr , value ) : try : pkg resources . Entry Point . parse map ( value ) except Value Error , e : raise Distutils Setup Error ( e )", "predictions": ["check that the value is valid ."], "references": ["verify that entry_points map is parseable"], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 5638, "code": "def transform assign system ( line ) : m = assign system re . match ( line ) if m is not None : cmd = m . group ( 'cmd' ) lhs = m . group ( 'lhs' ) new line = '%s = get ipython().getoutput(%r)' % ( lhs , cmd ) return new line return line", "predictions": ["transforms system system system system system system system ."], "references": ["handle the files = !ls syntax ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 5639, "code": "def transform assign magic ( line ) : m = assign magic re . match ( line ) if m is not None : cmd = m . group ( 'cmd' ) lhs = m . group ( 'lhs' ) new line = '%s = get ipython().magic(%r)' % ( lhs , cmd ) return new line return line", "predictions": ["transform magic magic magic magic magic magic magic ."], "references": ["handle the a = %who syntax ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 5640, "code": "def transform ipy prompt ( line ) : if not line or line . isspace ( ) : return line m = ipy prompt re . match ( line ) if m : return line [ len ( m . group ( 0 ) ) : ] else : return line", "predictions": ["transform ipy prompt prompt to ."], "references": ["handle inputs that start classic ipython prompt syntax ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 5641, "code": "def reset ( self ) : self . indent spaces = 0 self . buffer [ : ] = [ ] self . source = '' self . code = None self . is complete = False self . full dedent = False", "predictions": ["reset the buffer to its initial state ."], "references": ["reset the input buffer and associated state ."], "bleu": 0.2777619034011791, "rouge_l": 0.625}
{"id": 5642, "code": "def reset ( self ) : super ( I Python Input Splitter , self ) . reset ( ) self . buffer raw [ : ] = [ ] self . source raw = '' self . cell magic parts = [ ] self . processing cell magic = False", "predictions": ["resets the cell variables to its initial state ."], "references": ["reset the input buffer and associated state ."], "bleu": 0.19960198807747329, "rouge_l": 0.35672514619883033}
{"id": 5643, "code": "def source raw reset ( self ) : out = self . source out r = self . source raw self . reset ( ) return out , out r", "predictions": ["return the source source of the source ."], "references": ["return input and raw source and perform a full reset ."], "bleu": 0.13107175678306446, "rouge_l": 0.3070469798657718}
{"id": 5644, "code": "def handle cell magic ( self , lines ) : self . processing cell magic = True first , , body = lines . partition ( '\\n' ) magic name , , line = first . partition ( ' ' ) magic name = magic name . lstrip ( ESC MAGIC ) self . cell magic parts = [ body ] tpl = 'get ipython(). run cached cell magic(%r, %r)' tlines = tpl % ( magic name , line ) self . store ( tlines ) self . store ( lines , self . buffer raw , 'source raw' ) #self. is complete = last two blanks(lines) self . is complete = last blank ( lines ) return self . is complete", "predictions": ["handle a single cell of the cell ."], "references": ["process lines when they start with %% which marks cell magics ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 5645, "code": "def line mode cell append ( self , lines ) : self . store ( lines , self . buffer raw , 'source raw' ) self . cell magic parts . append ( lines ) last block = self . cell magic parts [ - 1 ] self . is complete = last blank ( last block ) and lines . isspace ( ) return self . is complete", "predictions": ["add new lines to the cell"], "references": ["append new content for a cell magic in line mode ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 5646, "code": "def transform cell ( self , cell ) : self . reset ( ) self . push ( cell ) return self . source reset ( )", "predictions": ["transform a cell in the current cell"], "references": ["process and translate a cell of input ."], "bleu": 0.21191828141393895, "rouge_l": 0.2634989200863931}
{"id": 5647, "code": "def observers for notification ( self , ntype , sender ) : keys = ( ( ntype , sender ) , ( ntype , None ) , ( None , sender ) , ( None , None ) ) obs = set ( ) for k in keys : obs . update ( self . observers . get ( k , set ( ) ) ) return obs", "predictions": ["returns a list of all observers that are observers for this notification ."], "references": ["find all registered observers that should recieve notification"], "bleu": 0.14283632578659286, "rouge_l": 0.39804241435562804}
{"id": 5648, "code": "def status ( self , verbose = 0 ) : self . update status ( ) self . group report ( self . running , 'Running' ) self . group report ( self . completed , 'Completed' ) self . group report ( self . dead , 'Dead' ) self . comp report [ : ] = [ ] self . dead report [ : ] = [ ]", "predictions": ["print the status of the group"], "references": ["print a status of all jobs currently being managed ."], "bleu": 0.16038842424444547, "rouge_l": 0.3588235294117647}
{"id": 5649, "code": "def init ( self ) : for attr in [ 'call' , 'strform' ] : assert hasattr ( self , attr ) , \"Missing attribute <%s>\" % attr self . num = None self . status = Background Job Base . stat created self . stat code = Background Job Base . stat created c self . finished = False self . result = '<Background Job has not completed>' try : make tb = get ipython ( ) . Interactive TB . text except : make tb = Auto Formatted TB ( mode = 'Context' , color scheme = 'No Color' , tb offset = 1 ) . text self . make tb = lambda : make tb ( None , None , None ) self . tb = None threading . Thread . init ( self )", "predictions": ["set up the text attribute ."], "references": ["common initialization for all backgroundjob objects"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 5650, "code": "def energy ( self , state = None ) : state = self . state if state is None else state route = state e = 0 if self . distance matrix : for i in range ( len ( route ) ) : e += self . distance matrix [ \"{},{}\" . format ( route [ i - 1 ] , route [ i ] ) ] else : for i in range ( len ( route ) ) : e += distance ( self . cities [ route [ i - 1 ] ] , self . cities [ route [ i ] ] ) return e", "predictions": ["compute the energy energy of the route"], "references": ["calculates the length of the route ."], "bleu": 0.3655552228545123, "rouge_l": 0.5714285714285714}
{"id": 5651, "code": "def defaults ( self , keys = None ) : d = { } keys = self . keys if keys is None else keys for key in keys : d [ key ] = None return d", "predictions": ["returns a dictionary of keys with the given keys"], "references": ["create an empty record"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5652, "code": "def init db ( self ) : sqlite3 . register adapter ( dict , adapt dict ) sqlite3 . register converter ( 'dict' , convert dict ) sqlite3 . register adapter ( list , adapt bufs ) sqlite3 . register converter ( 'bufs' , convert bufs ) dbfile = os . path . join ( self . location , self . filename ) self . db = sqlite3 . connect ( dbfile , detect types = sqlite3 . PARSE DECLTYPES , cached statements = 64 ) first table = previous table = self . table i = 0 while not self . check table ( ) : i += 1 self . table = first table + ' %i' % i self . log . warn ( \"Table %s exists and doesn't match db format, trying %s\" % ( previous table , self . table ) ) previous table = self . table self . db . execute ( % self . table ) self . db . commit ( )", "predictions": ["init the database and mappings ."], "references": ["connect to the database and get new session number ."], "bleu": 0.2231931376573339, "rouge_l": 0.47843137254901963}
{"id": 5653, "code": "def render expression ( self , check ) : expressions = [ ] args = [ ] skeys = set ( check . keys ( ) ) skeys . difference update ( set ( self . keys ) ) skeys . difference update ( set ( [ 'buffers' , 'result buffers' ] ) ) if skeys : raise Key Error ( \"Illegal testing key(s): %s\" % skeys ) for name , sub check in check . iteritems ( ) : if isinstance ( sub check , dict ) : for test , value in sub check . iteritems ( ) : try : op = operators [ test ] except Key Error : raise Key Error ( \"Unsupported operator: %r\" % test ) if isinstance ( op , tuple ) : op , join = op if value is None and op in null operators : expr = \"%s %s\" % ( name , null operators [ op ] ) else : expr = \"%s %s ?\" % ( name , op ) if isinstance ( value , ( tuple , list ) ) : if op in null operators and any ( [ v is None for v in value ] ) : raise Value Error ( \"Cannot use %r test with NULL values on SQ Lite backend\" % test ) expr = '( %s )' % ( join . join ( [ expr ] * len ( value ) ) ) args . extend ( value ) else : args . append ( value ) expressions . append ( expr ) else : if sub check is None : expressions . append ( \"%s IS NULL\" % name ) else : expressions . append ( \"%s = ?\" % name ) args . append ( sub check ) expr = \" AND \" . join ( expressions ) return expr , args", "predictions": ["render the expression into a expression ."], "references": ["turn a mongodb - style search dict into an sql query ."], "bleu": 0.1081377510275021, "rouge_l": 0.20098846787479407}
{"id": 5654, "code": "def add record ( self , msg id , rec ) : d = self . defaults ( ) d . update ( rec ) d [ 'msg id' ] = msg id line = self . dict to list ( d ) tups = '(%s)' % ( ',' . join ( [ '?' ] * len ( line ) ) ) self . db . execute ( \"INSERT INTO %s VALUES %s\" % ( self . table , tups ) , line )", "predictions": ["add a record to the database ."], "references": ["add a new task record by msg_id ."], "bleu": 0.240785655451027, "rouge_l": 0.5269978401727862}
{"id": 5655, "code": "def get record ( self , msg id ) : cursor = self . db . execute ( \"\"\"SELECT * FROM %s WHERE msg id==?\"\"\" % self . table , ( msg id , ) ) line = cursor . fetchone ( ) if line is None : raise Key Error ( \"No such msg: %r\" % msg id ) return self . list to dict ( line )", "predictions": ["get a record by its id ."], "references": ["get a specific task record by msg_id ."], "bleu": 0.2789001430384383, "rouge_l": 0.6587473002159828}
{"id": 5656, "code": "def update record ( self , msg id , rec ) : query = \"UPDATE %s SET \" % self . table sets = [ ] keys = sorted ( rec . keys ( ) ) values = [ ] for key in keys : sets . append ( '%s = ?' % key ) values . append ( rec [ key ] ) query += ', ' . join ( sets ) query += ' WHERE msg id == ?' values . append ( msg id ) self . db . execute ( query , values )", "predictions": ["update a record in the hosted database ."], "references": ["update the data in an existing record ."], "bleu": 0.21105340631872638, "rouge_l": 0.375}
{"id": 5657, "code": "def drop matching records ( self , check ) : expr , args = self . render expression ( check ) query = \"DELETE FROM %s WHERE %s\" % ( self . table , expr ) self . db . execute ( query , args )", "predictions": ["drop an existing records matching the given check ."], "references": ["remove a record from the db ."], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 5658, "code": "def get history ( self ) : query = \"\"\"SELECT msg id FROM %s ORDER by submitted ASC\"\"\" % self . table cursor = self . db . execute ( query ) return [ tup [ 0 ] for tup in cursor . fetchall ( ) ]", "predictions": ["get the history of the current user"], "references": ["get all msg_ids ordered by time submitted ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 5659, "code": "def table ( rows ) : output = '<table>' for row in rows : output += '<tr>' for column in row : output += '<td>{s}</td>' . format ( s = column ) output += '</tr>' output += '</table>' return output", "predictions": ["print a table in a string"], "references": ["output a simple table with several columns ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 5660, "code": "def link ( url , text = '' , classes = '' , target = '' , get = \"\" , * * kwargs ) : if not ( url . startswith ( 'http' ) or url . startswith ( '/' ) ) : urlargs = { } for arg , val in kwargs . items ( ) : if arg [ : 4 ] == \"url \" : urlargs [ arg [ 4 : ] ] = val url = reverse ( url , kwargs = urlargs ) if get : url += '?' + get return html . tag ( 'a' , text or url , { 'class' : classes , 'target' : target , 'href' : url } )", "predictions": ["create an html link to the given url ."], "references": ["output a link tag ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 5661, "code": "def jsfile ( url ) : if not url . startswith ( 'http://' ) and not url [ : 1 ] == '/' : #add media url for relative paths url = settings . STATIC URL + url return '<script type=\"text/javascript\" src=\"{src}\"></script>' . format ( src = url )", "predictions": ["return the url to use for the media media ."], "references": ["output a script tag to a js file ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 5662, "code": "def cssfile ( url ) : if not url . startswith ( 'http://' ) and not url [ : 1 ] == '/' : #add media url for relative paths url = settings . STATIC URL + url return '<link href=\"{src}\" rel=\"stylesheet\">' . format ( src = url )", "predictions": ["return the url to use for the media media ."], "references": ["output a link tag to a css stylesheet ."], "bleu": 0.13950796967929133, "rouge_l": 0.21254355400696867}
{"id": 5663, "code": "def img ( url , alt = '' , classes = '' , style = '' ) : if not url . startswith ( 'http://' ) and not url [ : 1 ] == '/' : #add media url for relative paths url = settings . STATIC URL + url attr = { 'class' : classes , 'alt' : alt , 'style' : style , 'src' : url } return html . tag ( 'img' , '' , attr )", "predictions": ["return the html url for the given url ."], "references": ["image tag helper ."], "bleu": 0.14113991930789777, "rouge_l": 0.16531165311653115}
{"id": 5664, "code": "def sub ( value , arg ) : try : return valid numeric ( value ) - valid numeric ( arg ) except ( Value Error , Type Error ) : try : return value - arg except Exception : return ''", "predictions": ["attempt to get an input from a from a string"], "references": ["subtract the arg from the value ."], "bleu": 0.12605968092174913, "rouge_l": 0.12151394422310759}
{"id": 5665, "code": "def mul ( value , arg ) : try : return valid numeric ( value ) * valid numeric ( arg ) except ( Value Error , Type Error ) : try : return value * arg except Exception : return ''", "predictions": ["attempt to find a numeric from a string"], "references": ["multiply the arg with the value ."], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5666, "code": "def div ( value , arg ) : try : return valid numeric ( value ) / valid numeric ( arg ) except ( Value Error , Type Error ) : try : return value / arg except Exception : return ''", "predictions": ["attempt to convert string to expanduser"], "references": ["divide the arg by the value ."], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 5667, "code": "def mod ( value , arg ) : try : return valid numeric ( value ) % valid numeric ( arg ) except ( Value Error , Type Error ) : try : return value % arg except Exception : return ''", "predictions": ["attempt to return a value argument"], "references": ["return the modulo value ."], "bleu": 0.24446151121745047, "rouge_l": 0.3696969696969697}
{"id": 5668, "code": "def options ( self , parser , env ) : parser . add option ( \"--processes\" , action = \"store\" , default = env . get ( 'NOSE PROCESSES' , 0 ) , dest = \"multiprocess workers\" , metavar = \"NUM\" , help = \"Spread test run among this many processes. \" \"Set a number equal to the number of processors \" \"or cores in your machine for best results. \" \"[NOSE PROCESSES]\" ) parser . add option ( \"--process-timeout\" , action = \"store\" , default = env . get ( 'NOSE PROCESS TIMEOUT' , 10 ) , dest = \"multiprocess timeout\" , metavar = \"SECONDS\" , help = \"Set timeout for return of results from each \" \"test runner process. [NOSE PROCESS TIMEOUT]\" ) parser . add option ( \"--process-restartworker\" , action = \"store true\" , default = env . get ( 'NOSE PROCESS RESTARTWORKER' , False ) , dest = \"multiprocess restartworker\" , help = \"If set, will restart each worker process once\" \" their tests are done, this helps control memory \" \"leaks from killing the system. \" \"[NOSE PROCESS RESTARTWORKER]\" )", "predictions": ["try to try to try to try to try to try to try to the ."], "references": ["register command - line options ."], "bleu": 0.07692375026049747, "rouge_l": 0.09902597402597402}
{"id": 5669, "code": "def run ( self , result ) : log . debug ( \"suite %s (%s) run called, tests: %s\" , id ( self ) , self , self . tests ) if self . result Proxy : result , orig = self . result Proxy ( result , self ) , result else : result , orig = result , result try : #log.debug('set Up for %s', id(self)); self . set Up ( ) except Keyboard Interrupt : raise except : self . error context = 'setup' result . add Error ( self , self . exc info ( ) ) return try : for test in self . tests : if ( isinstance ( test , nose . case . Test ) and self . arg is not None ) : test . test . arg = self . arg else : test . arg = self . arg test . test Queue = self . test Queue test . tasks = self . tasks if result . should Stop : log . debug ( \"stopping\" ) break #log.debug('running test %s in suite %s', test, self); try : test ( orig ) except Keyboard Interrupt , e : timeout = isinstance ( e , Timed Out Exception ) if timeout : msg = 'Timeout when running test %s in suite %s' else : msg = 'Keyboard Interrupt when running test %s in suite %s' log . debug ( msg , test , self ) err = ( Timed Out Exception , Timed Out Exception ( str ( test ) ) , sys . exc info ( ) [ 2 ] ) test . config . plugins . add Error ( test , err ) orig . add Error ( test , err ) if not timeout : raise finally : self . has run = True try : #log.debug('tear Down for %s', id(self)); self . tear Down ( ) except Keyboard Interrupt : raise except : self . error context = 'teardown' result . add Error ( self , self . exc info ( ) )", "predictions": ["check and check the dist test ."], "references": ["run tests in suite inside of suite fixtures ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 5670, "code": "def add builtin ( self , key , value ) : bdict = builtin . dict orig = bdict . get ( key , Builtin Undefined ) if value is Hide Builtin : if orig is not Builtin Undefined : #same as 'key in bdict' self . orig builtins [ key ] = orig del bdict [ key ] else : self . orig builtins [ key ] = orig bdict [ key ] = value", "predictions": ["transform a key to the collection match"], "references": ["add a builtin and save the original ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 5671, "code": "def remove builtin ( self , key , orig ) : if orig is Builtin Undefined : del builtin . dict [ key ] else : builtin . dict [ key ] = orig", "predictions": ["remove a assign assign value"], "references": ["remove an added builtin and re - set the original ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 5672, "code": "def activate ( self ) : add builtin = self . add builtin for name , func in self . auto builtins . iteritems ( ) : add builtin ( name , func )", "predictions": ["transform the builtin to the = = false not = false not yet ."], "references": ["store ipython references in the __builtin__ namespace ."], "bleu": 0.09782375748961449, "rouge_l": 0.19122257053291536}
{"id": 5673, "code": "def explicit rel links ( self , rels = ( 'homepage' , 'download' ) ) : rels = set ( rels ) for anchor in self . parsed . findall ( \".//a\" ) : if anchor . get ( \"rel\" ) and anchor . get ( \"href\" ) : found rels = set ( anchor . get ( \"rel\" ) . split ( ) ) if found rels & rels : href = anchor . get ( \"href\" ) url = self . clean link ( urllib parse . urljoin ( self . base url , href ) ) yield Link ( url , self , trusted = False )", "predictions": ["reset all rel self ."], "references": ["yields all links with the given relations"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 5674, "code": "def unshell list ( s ) : if not s : return None if sys . platform == 'win32' : s = s . strip ( \"'\" ) return s . split ( ',' )", "predictions": ["return a list list list"], "references": ["turn a command - line argument into a list ."], "bleu": 0.13218059591958078, "rouge_l": 0.2515463917525773}
{"id": 5675, "code": "def add action ( self , dash , dashdash , action code ) : option = self . add option ( dash , dashdash , action = 'callback' , callback = self . append action ) option . action code = action code", "predictions": ["source for the raw raw raw raw raw raw raw raw raw data ."], "references": ["add a specialized option that is the action to execute ."], "bleu": 0.09782375748961449, "rouge_l": 0.16353887399463804}
{"id": 5676, "code": "def append action ( self , option , opt unused , value unused , parser ) : parser . values . actions . append ( option . action code )", "predictions": ["handle cell body body . . ."], "references": ["callback for an option that adds to the actions list ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 5677, "code": "def help ( self , error = None , topic = None , parser = None ) : assert error or topic or parser if error : print ( error ) print ( \"Use 'coverage help' for help.\" ) elif parser : print ( parser . format help ( ) . strip ( ) ) else : help msg = HELP TOPICS . get ( topic , '' ) . strip ( ) if help msg : print ( help msg % self . covpkg . dict ) else : print ( \"Don't know topic %r\" % topic )", "predictions": ["prints the line information of the lines of the given lines ."], "references": ["display an error message or the named topic ."], "bleu": 0.11498759556447223, "rouge_l": 0.19551282051282048}
{"id": 5678, "code": "def do execute ( self , options , args ) : old path0 = sys . path [ 0 ] self . coverage . start ( ) code ran = True try : try : if options . module : sys . path [ 0 ] = '' self . run python module ( args [ 0 ] , args ) else : filename = args [ 0 ] sys . path [ 0 ] = os . path . abspath ( os . path . dirname ( filename ) ) self . run python file ( filename , args ) except No Source : code ran = False raise finally : self . coverage . stop ( ) if code ran : self . coverage . save ( ) sys . path [ 0 ] = old path0", "predictions": ["cell coverage coverage coverage"], "references": ["implementation of coverage run ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 5679, "code": "def do debug ( self , args ) : if not args : self . help fn ( \"What information would you like: data, sys?\" ) return ERR for info in args : if info == 'sys' : print ( \"-- sys ----------------------------------------\" ) for line in info formatter ( self . coverage . sysinfo ( ) ) : print ( \" %s\" % line ) elif info == 'data' : print ( \"-- data ---------------------------------------\" ) self . coverage . load ( ) print ( \"path: %s\" % self . coverage . data . filename ) print ( \"has arcs: %r\" % self . coverage . data . has arcs ( ) ) summary = self . coverage . data . summary ( fullpath = True ) if summary : filenames = sorted ( summary . keys ( ) ) print ( \"\\n%d files:\" % len ( filenames ) ) for f in filenames : print ( \"%s: %d lines\" % ( f , summary [ f ] ) ) else : print ( \"No data collected\" ) else : self . help fn ( \"Don't know what you mean by %r\" % info ) return ERR return OK", "predictions": ["handle coverage coverage coverage"], "references": ["implementation of coverage debug ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 5680, "code": "def unserialize object ( bufs ) : bufs = list ( bufs ) sobj = pickle . loads ( bufs . pop ( 0 ) ) if isinstance ( sobj , ( list , tuple ) ) : for s in sobj : if s . data is None : s . data = bufs . pop ( 0 ) return uncan Sequence ( map ( unserialize , sobj ) ) , bufs elif isinstance ( sobj , dict ) : newobj = { } for k in sorted ( sobj . iterkeys ( ) ) : s = sobj [ k ] if s . data is None : s . data = bufs . pop ( 0 ) newobj [ k ] = uncan ( unserialize ( s ) ) return newobj , bufs else : if sobj . data is None : sobj . data = bufs . pop ( 0 ) return uncan ( unserialize ( sobj ) ) , bufs", "predictions": ["status object . ."], "references": ["reconstruct an object serialized by serialize_object from data buffers ."], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 5681, "code": "def set ( self ) : if sys . displayhook is not self . hook : self . old hook = sys . displayhook sys . displayhook = self . hook", "predictions": ["init the hook hook to be used in the hook"], "references": ["set the hook ."], "bleu": 0.16590387014219712, "rouge_l": 0.3096446700507614}
{"id": 5682, "code": "def is url ( url ) : if '://' not in url : return False proto , addr = url . split ( '://' , 1 ) if proto . lower ( ) not in [ 'tcp' , 'pgm' , 'epgm' , 'ipc' , 'inproc' ] : return False return True", "predictions": [". returns true if the url is an url"], "references": ["boolean check for whether a string is a zmq url"], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 5683, "code": "def validate url ( url ) : if not isinstance ( url , basestring ) : raise Type Error ( \"url must be a string, not %r\" % type ( url ) ) url = url . lower ( ) proto addr = url . split ( '://' ) assert len ( proto addr ) == 2 , 'Invalid url: %r' % url proto , addr = proto addr assert proto in [ 'tcp' , 'pgm' , 'epgm' , 'ipc' , 'inproc' ] , \"Invalid protocol: %r\" % proto pat = re . compile ( r'^([\\w\\d]([\\w\\d\\-]{0,61}[\\w\\d])?\\.)*[\\w\\d]([\\w\\d\\-]{0,61}[\\w\\d])?$' ) if proto == 'tcp' : lis = addr . split ( ':' ) assert len ( lis ) == 2 , 'Invalid url: %r' % url addr , s port = lis try : port = int ( s port ) except Value Error : raise Assertion Error ( \"Invalid port %r in url: %r\" % ( port , url ) ) assert addr == '*' or pat . match ( addr ) is not None , 'Invalid url: %r' % url else : pass return True", "predictions": ["validates that the url string is valid"], "references": ["validate a url for zeromq"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 5684, "code": "def validate url container ( container ) : if isinstance ( container , basestring ) : url = container return validate url ( url ) elif isinstance ( container , dict ) : container = container . itervalues ( ) for element in container : validate url container ( element )", "predictions": ["init a db container container"], "references": ["validate a potentially nested collection of urls ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 5685, "code": "def pull ( keys ) : user ns = globals ( ) if isinstance ( keys , ( list , tuple , set ) ) : for key in keys : if not user ns . has key ( key ) : raise Name Error ( \"name '%s' is not defined\" % key ) return map ( user ns . get , keys ) else : if not user ns . has key ( keys ) : raise Name Error ( \"name '%s' is not defined\" % keys ) return user ns . get ( keys )", "predictions": ["render a check or list of keys"], "references": ["helper method for implementing client . pull via client . apply"], "bleu": 0.08820727472213225, "rouge_l": 0.0}
{"id": 5686, "code": "def select random ports ( n ) : ports = [ ] for i in xrange ( n ) : sock = socket . socket ( ) sock . bind ( ( '' , 0 ) ) while sock . getsockname ( ) [ 1 ] in random ports : sock . close ( ) sock = socket . socket ( ) sock . bind ( ( '' , 0 ) ) ports . append ( sock ) for i , sock in enumerate ( ports ) : port = sock . getsockname ( ) [ 1 ] sock . close ( ) ports [ i ] = port random ports . add ( port ) return ports", "predictions": ["add record ports to the pool"], "references": ["selects and return n random ports that are available ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 5687, "code": "def get readline tail ( self , n = 10 ) : end = self . shell . readline . get current history length ( ) + 1 start = max ( end - n , 1 ) ghi = self . shell . readline . get history item return [ ghi ( x ) for x in range ( start , end ) ]", "predictions": ["returns the id of the current in the execute * id *"], "references": ["get the last n items in readline history ."], "bleu": 0.11498759556447223, "rouge_l": 0.19551282051282048}
{"id": 5688, "code": "def init logstart ( self ) : if self . logappend : self . magic ( 'logstart %s append' % self . logappend ) elif self . logfile : self . magic ( 'logstart %s' % self . logfile ) elif self . logstart : self . magic ( 'logstart' )", "predictions": ["initializes the magic and magic id"], "references": ["initialize logging in case it was requested at the command line ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 5689, "code": "def restore sys module state ( self ) : try : for k , v in self . orig sys module state . iteritems ( ) : setattr ( sys , k , v ) except Attribute Error : pass if self . orig sys modules main mod is not None : sys . modules [ self . orig sys modules main name ] = self . orig sys modules main mod", "predictions": ["drop all db db db db state expression expression expression expression expression expression expression expression expression"], "references": ["restore the state of the sys module ."], "bleu": 0.07692375026049747, "rouge_l": 0.08866279069767442}
{"id": 5690, "code": "def register post execute ( self , func ) : if not callable ( func ) : raise Value Error ( 'argument %s must be callable' % func ) self . post execute [ func ] = True", "predictions": ["register a history to be called when a history is not already registered ."], "references": ["register a function for calling after code execution ."], "bleu": 0.1250076305588977, "rouge_l": 0.271513353115727}
{"id": 5691, "code": "def new main mod ( self , ns = None ) : main mod = self . user main module init fakemod dict ( main mod , ns ) return main mod", "predictions": ["returns a table with the main column . . . . . . . . . ."], "references": ["return a new main module object for user code execution ."], "bleu": 0.0859076483566362, "rouge_l": 0.22289890377588306}
{"id": 5692, "code": "def ofind property ( self , oname , info ) : if info . found : path = oname . split ( '.' ) root = '.' . join ( path [ : - 1 ] ) if info . parent is not None : try : target = getattr ( info . parent , ' class ' ) try : target = getattr ( target , path [ - 1 ] ) if isinstance ( target , property ) : oname = root + '. class .' + path [ - 1 ] info = Struct ( self . ofind ( oname ) ) except Attribute Error : pass except Attribute Error : pass return info", "predictions": ["return the info property property"], "references": ["second part of object finding to look for property details ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 5693, "code": "def object find ( self , oname , namespaces = None ) : inf = Struct ( self . ofind ( oname , namespaces ) ) return Struct ( self . ofind property ( oname , inf ) )", "predictions": ["find the object for the given object . . ."], "references": ["find an object and return a struct with info about it ."], "bleu": 0.12273680279953825, "rouge_l": 0.2683284457478006}
{"id": 5694, "code": "def init history ( self ) : self . history manager = History Manager ( shell = self , config = self . config ) self . configurables . append ( self . history manager )", "predictions": ["initialize the history history history"], "references": ["sets up the command history and starts regular autosaves ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 5695, "code": "def set completer frame ( self , frame = None ) : if frame : self . Completer . namespace = frame . f locals self . Completer . global namespace = frame . f globals else : self . Completer . namespace = self . user ns self . Completer . global namespace = self . user global ns", "predictions": ["sets the completer url of the user"], "references": ["set the frame of the completer ."], "bleu": 0.3073940764756322, "rouge_l": 0.42857142857142855}
{"id": 5696, "code": "def ex ( self , cmd ) : with self . builtin trap : exec cmd in self . user global ns , self . user ns", "predictions": ["run the command with the given cmd ."], "references": ["execute a normal python statement in user namespace ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 5697, "code": "def run cached cell magic ( self , magic name , line ) : cell = self . current cell magic body self . current cell magic body = None return self . run cell magic ( magic name , line , cell )", "predictions": ["run the cached cell magic ."], "references": ["special method to call a cell magic with the data stored in self ."], "bleu": 0.08707046609544257, "rouge_l": 0.2798165137614679}
{"id": 5698, "code": "def broadcast ( client , sender , msg name , dest name = None , block = None ) : dest name = msg name if dest name is None else dest name client [ sender ] . execute ( 'com.publish(%s)' % msg name , block = None ) targets = client . ids targets . remove ( sender ) return client [ targets ] . execute ( '%s=com.consume()' % dest name , block = None )", "predictions": ["renames a message to another user ."], "references": ["broadcast a message from one engine to all others ."], "bleu": 0.18094495256969623, "rouge_l": 0.45607476635514016}
{"id": 5699, "code": "def send ( client , sender , targets , msg name , dest name = None , block = None ) : dest name = msg name if dest name is None else dest name def send ( targets , m name ) : msg = globals ( ) [ m name ] return com . send ( targets , msg ) client [ sender ] . apply async ( send , targets , msg name ) return client [ targets ] . execute ( '%s=com.recv()' % dest name , block = None )", "predictions": ["send message to server ."], "references": ["send a message from one to one - or - more engines ."], "bleu": 0.06930996903910726, "rouge_l": 0.41146711635750427}
{"id": 5700, "code": "def list profiles in ( path ) : files = os . listdir ( path ) profiles = [ ] for f in files : full path = os . path . join ( path , f ) if os . path . isdir ( full path ) and f . startswith ( 'profile ' ) : profiles . append ( f . split ( ' ' , 1 ) [ - 1 ] ) return profiles", "predictions": ["return a list of all profiles in the given path ."], "references": ["list profiles in a given root directory"], "bleu": 0.17827531042796255, "rouge_l": 0.4629981024667932}
{"id": 5701, "code": "def list bundled profiles ( ) : path = os . path . join ( get ipython package dir ( ) , u'config' , u'profile' ) files = os . listdir ( path ) profiles = [ ] for profile in files : full path = os . path . join ( path , profile ) if os . path . isdir ( full path ) and profile != \" pycache \" : profiles . append ( profile ) return profiles", "predictions": ["return all bundled profiles ."], "references": ["list profiles that are bundled with ipython ."], "bleu": 0.1781815298791261, "rouge_l": 0.2953995157384988}
{"id": 5702, "code": "def next ( self ) : result = self . readline ( ) if result == self . empty buffer : raise Stop Iteration return result", "predictions": ["return the next buffer in the buffer ."], "references": ["this is to support iterators over a file - like object ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 5703, "code": "def prepare regex pattern ( self , p ) : if isinstance ( p . pattern , unicode ) : p = re . compile ( p . pattern . encode ( 'utf-8' ) , p . flags & ~ re . UNICODE ) return p", "predictions": ["prepare regex pattern for regex ."], "references": ["recompile unicode regexes as bytes regexes . overridden in subclass ."], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 5704, "code": "def prepare regex pattern ( self , p ) : if isinstance ( p . pattern , bytes ) : p = re . compile ( p . pattern . decode ( self . encoding ) , p . flags ) return p", "predictions": ["prepare regex pattern for regex ."], "references": ["recompile bytes regexes as unicode regexes ."], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5705, "code": "def finish displayhook ( self ) : sys . stdout . flush ( ) sys . stderr . flush ( ) self . session . send ( self . pub socket , self . msg , ident = self . topic ) self . msg = None", "predictions": ["finish the socket ."], "references": ["finish up all displayhook activities ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 5706, "code": "def log listener ( log : logging . Logger = None , level = logging . INFO ) : if log is None : log = logging . get Logger ( \"Progress Monitor\" ) def listen ( monitor ) : name = \"{}: \" . format ( monitor . name ) if monitor . name is not None else \"\" perc = int ( monitor . progress * 100 ) msg = \"[{name}{perc:3d}%] {monitor.message}\" . format ( * * locals ( ) ) log . log ( level , msg ) return listen", "predictions": ["log a listener ."], "references": ["progress monitor listener that logs all updates to the given logger"], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 5707, "code": "def last error ( self ) : if not len ( self . log ) : raise Runtime Error ( 'Nothing executed' ) try : errs = [ l for l in self . log if l [ 1 ] != 0 ] return errs [ - 1 ] [ 2 ] except Index Error : #TODO return 'no last error'", "predictions": ["return the last error error"], "references": ["get the output of the last command exevuted ."], "bleu": 0.1614457444314309, "rouge_l": 0.2717149220489978}
{"id": 5708, "code": "def check output ( self , cmd ) : ret , output = self . exec ( cmd ) if not ret == 0 : raise Command Error ( self ) return output", "predictions": ["check command and return the output of the output ."], "references": ["wrapper for subprocess . check_output ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 5709, "code": "def arcs executed ( self ) : executed = self . coverage . data . executed arcs ( self . filename ) m2fl = self . parser . first line executed = [ ( m2fl ( l1 ) , m2fl ( l2 ) ) for ( l1 , l2 ) in executed ] return sorted ( executed )", "predictions": ["list of arcs executed from the current user ."], "references": ["returns a sorted list of the arcs actually executed in the code ."], "bleu": 0.1471989137239998, "rouge_l": 0.5281385281385281}
{"id": 5710, "code": "def arcs missing ( self ) : possible = self . arc possibilities ( ) executed = self . arcs executed ( ) missing = [ p for p in possible if p not in executed and p [ 0 ] not in self . no branch ] return sorted ( missing )", "predictions": ["return list of missing executed missing executed ."], "references": ["returns a sorted list of the arcs in the code not executed ."], "bleu": 0.14205067994279338, "rouge_l": 0.3652694610778443}
{"id": 5711, "code": "def arcs unpredicted ( self ) : possible = self . arc possibilities ( ) executed = self . arcs executed ( ) unpredicted = [ e for e in executed if e not in possible and e [ 0 ] != e [ 1 ] ] return sorted ( unpredicted )", "predictions": ["returns a list of arcs of all executed executed ."], "references": ["returns a sorted list of the executed arcs missing from the code ."], "bleu": 0.17381336617464685, "rouge_l": 0.5097493036211699}
{"id": 5712, "code": "def branch lines ( self ) : exit counts = self . parser . exit counts ( ) return [ l1 for l1 , count in iitems ( exit counts ) if count > 1 ]", "predictions": ["return the list of branch lines ."], "references": ["returns a list of line numbers that have more than one exit ."], "bleu": 0.1114789227233716, "rouge_l": 0.2846034214618974}
{"id": 5713, "code": "def total branches ( self ) : exit counts = self . parser . exit counts ( ) return sum ( [ count for count in exit counts . values ( ) if count > 1 ] )", "predictions": ["the total branches of all branches ."], "references": ["how many total branches are there?"], "bleu": 0.24446151121745047, "rouge_l": 0.31202046035805625}
{"id": 5714, "code": "def set precision ( cls , precision ) : assert 0 <= precision < 10 cls . precision = precision cls . near0 = 1.0 / 10 ** precision cls . near100 = 100.0 - cls . near0", "predictions": ["set the precision of the precision precision ."], "references": ["set the number of decimal places used to report percentages ."], "bleu": 0.16481400866629634, "rouge_l": 0.4093959731543625}
{"id": 5715, "code": "def get pc covered ( self ) : if self . n statements > 0 : pc cov = ( 100.0 * ( self . n executed + self . n executed branches ) / ( self . n statements + self . n branches ) ) else : pc cov = 100.0 return pc cov", "predictions": ["returns the pc of the pc ."], "references": ["returns a single percentage value for coverage ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 5716, "code": "def highlight text ( needles , haystack , cls name = 'highlighted' , words = False , case = False ) : if not needles : return haystack if not haystack : return '' if words : pattern = r\"(%s)\" % \"|\" . join ( [ '\\\\b{}\\\\b' . format ( re . escape ( n ) ) for n in needles ] ) else : pattern = r\"(%s)\" % \"|\" . join ( [ re . escape ( n ) for n in needles ] ) if case : regex = re . compile ( pattern ) else : regex = re . compile ( pattern , re . I ) i , out = 0 , \"\" for m in regex . finditer ( haystack ) : out += \"\" . join ( [ haystack [ i : m . start ( ) ] , '<span class=\"%s\">' % cls name , haystack [ m . start ( ) : m . end ( ) ] , \"</span>\" ] ) i = m . end ( ) return mark safe ( out + haystack [ i : ] )", "predictions": ["highlight the text with haystack haystack ."], "references": ["applies cls_name to all needles found in haystack ."], "bleu": 0.18370727471078332, "rouge_l": 0.24448897795591182}
{"id": 5717, "code": "def highlight ( string , keywords , cls name = 'highlighted' ) : if not keywords : return string if not string : return '' include , exclude = get text tokenizer ( keywords ) highlighted = highlight text ( include , string , cls name ) return highlighted", "predictions": ["highlight the highlighted text into a highlighted string ."], "references": ["given an list of words this function highlights the matched text in the given string ."], "bleu": 0.0969633851135701, "rouge_l": 0.3046192259675406}
{"id": 5718, "code": "def highlight words ( string , keywords , cls name = 'highlighted' ) : if not keywords : return string if not string : return '' include , exclude = get text tokenizer ( keywords ) highlighted = highlight text ( include , string , cls name , words = True ) return highlighted", "predictions": ["highlight the highlighted words from the string"], "references": ["given an list of words this function highlights the matched words in the given string ."], "bleu": 0.06457085856966725, "rouge_l": 0.3249001331557923}
{"id": 5719, "code": "def run setup ( setup script , args ) : old dir = os . getcwd ( ) save argv = sys . argv [ : ] save path = sys . path [ : ] setup dir = os . path . abspath ( os . path . dirname ( setup script ) ) temp dir = os . path . join ( setup dir , 'temp' ) if not os . path . isdir ( temp dir ) : os . makedirs ( temp dir ) save tmp = tempfile . tempdir save modules = sys . modules . copy ( ) pr state = pkg resources . getstate ( ) try : tempfile . tempdir = temp dir os . chdir ( setup dir ) try : sys . argv [ : ] = [ setup script ] + list ( args ) sys . path . insert ( 0 , setup dir ) Directory Sandbox ( setup dir ) . run ( lambda : execfile ( \"setup.py\" , { ' file ' : setup script , ' name ' : ' main ' } ) ) except System Exit , v : if v . args and v . args [ 0 ] : raise finally : pkg resources . setstate ( pr state ) sys . modules . update ( save modules ) del modules = [ mod name for mod name in sys . modules if mod name not in save modules and not mod name . startswith ( 'encodings.' ) ] map ( sys . modules . delitem , del modules ) os . chdir ( old dir ) sys . path [ : ] = save path sys . argv [ : ] = save argv tempfile . tempdir = save tmp", "predictions": ["run the setup . py file ."], "references": ["run a distutils setup script sandboxed in its directory"], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 5720, "code": "def run ( self , func ) : try : self . copy ( self ) if file : builtin . file = self . file builtin . open = self . open self . active = True return func ( ) finally : self . active = False if file : builtin . file = file builtin . open = open self . copy ( os )", "predictions": ["run the active task ."], "references": ["run func under os sandboxing"], "bleu": 0.2730120862709067, "rouge_l": 0.2}
{"id": 5721, "code": "def unquote ends ( istr ) : if not istr : return istr if ( istr [ 0 ] == \"'\" and istr [ - 1 ] == \"'\" ) or ( istr [ 0 ] == '\"' and istr [ - 1 ] == '\"' ) : return istr [ 1 : - 1 ] else : return istr", "predictions": ["unquote istr ends of istr"], "references": ["remove a single pair of quotes from the endpoints of a string ."], "bleu": 0.05512018958855254, "rouge_l": 0.10286677908937607}
{"id": 5722, "code": "def find optimal ( rlist , separator size = 2 , displaywidth = 80 ) : for nrow in range ( 1 , len ( rlist ) + 1 ) : chk = map ( max , chunks ( rlist , nrow ) ) sumlength = sum ( chk ) ncols = len ( chk ) if sumlength + separator size * ( ncols - 1 ) <= displaywidth : break return { 'columns numbers' : ncols , 'optimal separator width' : ( displaywidth - sumlength ) / ( ncols - 1 ) if ( ncols - 1 ) else 0 , 'rows numbers' : nrow , 'columns width' : chk }", "predictions": ["find the optimal optimal optimal optimal optimal optimal optimal size"], "references": ["calculate optimal info to columnize a list of string"], "bleu": 0.12605968092174913, "rouge_l": 0.10627177700348434}
{"id": 5723, "code": "def get or default ( mylist , i , default = None ) : if i >= len ( mylist ) : return default else : return mylist [ i ]", "predictions": ["return the first mylist or default if it is not defined ."], "references": ["return list item number or default if don t exist"], "bleu": 0.2044800736021839, "rouge_l": 0.3696969696969697}
{"id": 5724, "code": "def build kernel argv ( self , argv = None ) : if argv is None : argv = sys . argv [ 1 : ] self . kernel argv = swallow argv ( argv , self . frontend aliases , self . frontend flags ) self . kernel argv . append ( \"--Kernel App.parent appname='%s'\" % self . name )", "predictions": ["build kernel for kernel - argv command ."], "references": ["build argv to be passed to kernel subprocess"], "bleu": 0.19070828081828378, "rouge_l": 0.25}
{"id": 5725, "code": "def init ssh ( self ) : if not self . sshserver and not self . sshkey : return if self . sshkey and not self . sshserver : self . sshserver = self . ip self . ip = LOCALHOST info = dict ( ip = self . ip , shell port = self . shell port , iopub port = self . iopub port , stdin port = self . stdin port , hb port = self . hb port ) self . log . info ( \"Forwarding connections to %s via %s\" % ( self . ip , self . sshserver ) ) self . ip = LOCALHOST try : newports = tunnel to kernel ( info , self . sshserver , self . sshkey ) except : self . log . error ( \"Could not setup tunnels\" , exc info = True ) self . exit ( 1 ) self . shell port , self . iopub port , self . stdin port , self . hb port = newports cf = self . connection file base , ext = os . path . splitext ( cf ) base = os . path . basename ( base ) self . connection file = os . path . basename ( base ) + '-ssh' + ext self . log . critical ( \"To connect another client via this tunnel, use:\" ) self . log . critical ( \"--existing %s\" % self . connection file )", "predictions": ["init the connections to the kernel server"], "references": ["set up ssh tunnels if needed ."], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5726, "code": "def pretty ( obj , verbose = False , max width = 79 , newline = '\\n' ) : stream = String IO ( ) printer = Representation Printer ( stream , verbose , max width , newline ) printer . pretty ( obj ) printer . flush ( ) return stream . getvalue ( )", "predictions": ["print the formatted text of a string ."], "references": ["pretty print the object s representation ."], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 5727, "code": "def pprint ( obj , verbose = False , max width = 79 , newline = '\\n' ) : printer = Representation Printer ( sys . stdout , verbose , max width , newline ) printer . pretty ( obj ) printer . flush ( ) sys . stdout . write ( newline ) sys . stdout . flush ( )", "predictions": ["print an object to stdout ."], "references": ["like pretty but print to stdout ."], "bleu": 0.36798327352994814, "rouge_l": 0.6069651741293532}
{"id": 5728, "code": "def super pprint ( obj , p , cycle ) : p . begin group ( 8 , '<super: ' ) p . pretty ( obj . self class ) p . text ( ',' ) p . breakable ( ) p . pretty ( obj . self ) p . end group ( 8 , '>' )", "predictions": ["print the exec object builtin by the given object builtin ."], "references": ["the pprint for the super type ."], "bleu": 0.1354599427337814, "rouge_l": 0.3472485768500949}
{"id": 5729, "code": "def re pattern pprint ( obj , p , cycle ) : p . text ( 're.compile(' ) pattern = repr ( obj . pattern ) if pattern [ : 1 ] in 'u U' : pattern = pattern [ 1 : ] prefix = 'ur' else : prefix = 'r' pattern = prefix + pattern . replace ( '\\\\\\\\' , '\\\\' ) p . text ( pattern ) if obj . flags : p . text ( ',' ) p . breakable ( ) done one = False for flag in ( 'TEMPLATE' , 'IGNORECASE' , 'LOCALE' , 'MULTILINE' , 'DOTALL' , 'UNICODE' , 'VERBOSE' , 'DEBUG' ) : if obj . flags & getattr ( re , flag ) : if done one : p . text ( '|' ) p . text ( 're.' + flag ) done one = True p . text ( ')' )", "predictions": ["print cached cached cached cached cached characters . . . . . . . . ."], "references": ["the pprint function for regular expression patterns ."], "bleu": 0.07692375026049747, "rouge_l": 0.08866279069767442}
{"id": 5730, "code": "def type pprint ( obj , p , cycle ) : if obj . module in ( ' builtin ' , 'exceptions' ) : name = obj . name else : name = obj . module + '.' + obj . name p . text ( name )", "predictions": ["print the broadcast broadcast broadcast broadcast block block block block block block block block block block block block block block block block block block name block block block block block block"], "references": ["the pprint for classes and types ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 5731, "code": "def function pprint ( obj , p , cycle ) : if obj . module in ( ' builtin ' , 'exceptions' ) or not obj . module : name = obj . name else : name = obj . module + '.' + obj . name p . text ( '<function %s>' % name )", "predictions": ["pretty - prints the send send the send send send a send send send send to the given send object . . . . . ."], "references": ["base pprint for all functions and builtin functions ."], "bleu": 0.04668049023095243, "rouge_l": 0.0626283367556468}
{"id": 5732, "code": "def exception pprint ( obj , p , cycle ) : if obj . class . module in ( 'exceptions' , 'builtins' ) : name = obj . class . name else : name = '%s.%s' % ( obj . class . module , obj . class . name ) step = len ( name ) + 1 p . begin group ( step , name + '(' ) for idx , arg in enumerate ( getattr ( obj , 'args' , ( ) ) ) : if idx : p . text ( ',' ) p . breakable ( ) p . pretty ( arg ) p . end group ( step , ')' )", "predictions": ["print the list of list list of objects ."], "references": ["base pprint for all exceptions ."], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 5733, "code": "def for type ( typ , func ) : oldfunc = type pprinters . get ( typ , None ) if func is not None : type pprinters [ typ ] = func return oldfunc", "predictions": ["listdir list of strings list"], "references": ["add a pretty printer for a given type ."], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 5734, "code": "def text ( self , obj ) : width = len ( obj ) if self . buffer : text = self . buffer [ - 1 ] if not isinstance ( text , Text ) : text = Text ( ) self . buffer . append ( text ) text . add ( obj , width ) self . buffer width += width self . break outer groups ( ) else : self . output . write ( obj ) self . output width += width", "predictions": ["write next next next next next next next next next next next next next next next next next next next next next next next next next next next next next next"], "references": ["add literal text to the output ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5735, "code": "def end group ( self , dedent = 0 , close = '' ) : self . indentation -= dedent group = self . group stack . pop ( ) if not group . breakables : self . group queue . remove ( group ) if close : self . text ( close )", "predictions": ["prepare the regex regex for the regex"], "references": ["end a group . see begin_group for more details ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 5736, "code": "def flush ( self ) : for data in self . buffer : self . output width += data . output ( self . output , self . output width ) self . buffer . clear ( ) self . buffer width = 0", "predictions": ["prepare the output and output ."], "references": ["flush data that is left in the buffer ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 5737, "code": "def pretty ( self , obj ) : obj id = id ( obj ) cycle = obj id in self . stack self . stack . append ( obj id ) self . begin group ( ) try : obj class = getattr ( obj , ' class ' , None ) or type ( obj ) try : printer = self . singleton pprinters [ obj id ] except ( Type Error , Key Error ) : pass else : return printer ( obj , self , cycle ) for cls in get mro ( obj class ) : if cls in self . type pprinters : return self . type pprinters [ cls ] ( obj , self , cycle ) else : printer = self . in deferred types ( cls ) if printer is not None : return printer ( obj , self , cycle ) else : if ' repr pretty ' in obj class . dict : meth = obj class . repr pretty if callable ( meth ) : return meth ( obj , self , cycle ) return default pprint ( obj , self , cycle ) finally : self . end group ( ) self . stack . pop ( )", "predictions": ["return the finish object for the given object session session session session session session session session session session session session session session session session session session session session session session session"], "references": ["pretty print the given object ."], "bleu": 0.07261813302549416, "rouge_l": 0.18466195761856707}
{"id": 5738, "code": "def write row into ods ( ods , sheet no , row no , row ) : ods . content . get Sheet ( sheet no ) for j , col in enumerate ( row ) : cell = ods . content . get Cell ( j , row no + 1 ) cell . string Value ( escape apostrophe ( col ) ) if j % 2 == 1 : cell . set Cell Color ( settings . EVEN COLUMN BG COLOR ) else : cell . set Cell Color ( settings . ODD COLUMN BG COLOR )", "predictions": ["writes a listener to the ods"], "references": ["write row with translations to ods file into specified sheet and row_no ."], "bleu": 0.07612610271614867, "rouge_l": 0.19741100323624597}
{"id": 5739, "code": "def osx clipboard get ( ) : p = subprocess . Popen ( [ 'pbpaste' , '-Prefer' , 'ascii' ] , stdout = subprocess . PIPE ) text , stderr = p . communicate ( ) text = text . replace ( '\\r' , '\\n' ) return text", "predictions": ["get error text ."], "references": ["get the clipboard s text on os x ."], "bleu": 0.12241977696855179, "rouge_l": 0.43160377358490565}
{"id": 5740, "code": "def get build prefix ( ) : path = os . path . join ( tempfile . gettempdir ( ) , 'pip build %s' % get username ( ) . replace ( ' ' , ' ' ) ) if WINDOWS : \"\"\" on windows(tested on 7) temp dirs are isolated \"\"\" return path try : os . mkdir ( path ) write delete marker file ( path ) except OS Error : file uid = None try : file uid = get path uid ( path ) except OS Error : file uid = None if file uid != os . geteuid ( ) : msg = ( \"The temporary folder for building (%s) is either not owned by\" \" you, or is a symlink.\" % path ) print ( msg ) print ( \"pip will not work until the temporary folder is either \" \"deleted or is a real directory owned by your user account.\" ) raise exceptions . Installation Error ( msg ) return path", "predictions": ["return the prefix prefix directory"], "references": ["returns a safe build_prefix"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 5741, "code": "def prepare communication ( self ) : Rect Partitioner . prepare communication ( self ) if self . lower neighbors [ 0 ] >= 0 : self . in lower buffers = [ zeros ( 1 , float ) ] self . out lower buffers = [ zeros ( 1 , float ) ] if self . upper neighbors [ 0 ] >= 0 : self . in upper buffers = [ zeros ( 1 , float ) ] self . out upper buffers = [ zeros ( 1 , float ) ]", "predictions": ["arcs the executed parameters for the given executed coverage coverage coverage coverage coverage coverage coverage coverage coverage coverage ."], "references": ["prepare the buffers to be used for later communications"], "bleu": 0.0712695567709093, "rouge_l": 0.15269086357947434}
{"id": 5742, "code": "def prepare communication ( self ) : Rect Partitioner . prepare communication ( self ) self . in lower buffers = [ [ ] , [ ] ] self . out lower buffers = [ [ ] , [ ] ] self . in upper buffers = [ [ ] , [ ] ] self . out upper buffers = [ [ ] , [ ] ] size1 = self . subd hi ix [ 1 ] - self . subd lo ix [ 1 ] + 1 if self . lower neighbors [ 0 ] >= 0 : self . in lower buffers [ 0 ] = zeros ( size1 , float ) self . out lower buffers [ 0 ] = zeros ( size1 , float ) if self . upper neighbors [ 0 ] >= 0 : self . in upper buffers [ 0 ] = zeros ( size1 , float ) self . out upper buffers [ 0 ] = zeros ( size1 , float ) size0 = self . subd hi ix [ 0 ] - self . subd lo ix [ 0 ] + 1 if self . lower neighbors [ 1 ] >= 0 : self . in lower buffers [ 1 ] = zeros ( size0 , float ) self . out lower buffers [ 1 ] = zeros ( size0 , float ) if self . upper neighbors [ 1 ] >= 0 : self . in upper buffers [ 1 ] = zeros ( size0 , float ) self . out upper buffers [ 1 ] = zeros ( size0 , float )", "predictions": ["arcs the missing values for the missing missing missing values ."], "references": ["prepare the buffers to be used for later communications"], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 5743, "code": "def extract dates ( obj ) : if isinstance ( obj , dict ) : obj = dict ( obj ) for k , v in obj . iteritems ( ) : obj [ k ] = extract dates ( v ) elif isinstance ( obj , ( list , tuple ) ) : obj = [ extract dates ( o ) for o in obj ] elif isinstance ( obj , basestring ) : if ISO8601 PAT . match ( obj ) : obj = datetime . strptime ( obj , ISO8601 ) return obj", "predictions": ["arcs all dates dates objects"], "references": ["extract iso8601 dates from unpacked json"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 5744, "code": "def squash dates ( obj ) : if isinstance ( obj , dict ) : obj = dict ( obj ) for k , v in obj . iteritems ( ) : obj [ k ] = squash dates ( v ) elif isinstance ( obj , ( list , tuple ) ) : obj = [ squash dates ( o ) for o in obj ] elif isinstance ( obj , datetime ) : obj = obj . strftime ( ISO8601 ) return obj", "predictions": ["convert json objects into json objects"], "references": ["squash datetime objects into iso8601 strings"], "bleu": 0.2907153684841096, "rouge_l": 0.3333333333333333}
{"id": 5745, "code": "def date default ( obj ) : if isinstance ( obj , datetime ) : return obj . strftime ( ISO8601 ) else : raise Type Error ( \"%r is not JSON serializable\" % obj )", "predictions": ["branches for json encoding ."], "references": ["default function for packing datetime objects in json ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 5746, "code": "def check site dir ( self ) : instdir = normalize path ( self . install dir ) pth file = os . path . join ( instdir , 'easy-install.pth' ) is site dir = instdir in self . all site dirs if not is site dir and not self . multi version : is site dir = self . check pth processing ( ) else : testfile = self . pseudo tempname ( ) + '.write-test' test exists = os . path . exists ( testfile ) try : if test exists : os . unlink ( testfile ) open ( testfile , 'w' ) . close ( ) os . unlink ( testfile ) except ( OS Error , IO Error ) : self . cant write to target ( ) if not is site dir and not self . multi version : raise Distutils Error ( self . no default version msg ( ) ) if is site dir : if self . pth file is None : self . pth file = Pth Distributions ( pth file , self . all site dirs ) else : self . pth file = None PYTHONPATH = os . environ . get ( 'PYTHONPATH' , '' ) . split ( os . pathsep ) if instdir not in map ( normalize path , [ f for f in PYTHONPATH if f ] ) : self . sitepy installed = True elif self . multi version and not os . path . exists ( pth file ) : self . sitepy installed = True self . pth file = None self . install dir = instdir", "predictions": ["set up the precision dir directory"], "references": ["verify that self . install_dir is . pth - capable dir if needed"], "bleu": 0.06878769894132081, "rouge_l": 0.09870550161812298}
{"id": 5747, "code": "def write script ( self , script name , contents , mode = \"t\" , * ignored ) : from setuptools . command . easy install import chmod , current umask log . info ( \"Installing %s script to %s\" , script name , self . install dir ) target = os . path . join ( self . install dir , script name ) self . outfiles . append ( target ) mask = current umask ( ) if not self . dry run : ensure directory ( target ) f = open ( target , \"w\" + mode ) f . write ( contents ) f . close ( ) chmod ( target , 0777 - mask )", "predictions": ["get the pc from the given pc * * contents * * * * * * * * * * * * * * * * * * * * *"], "references": ["write an executable file to the scripts directory"], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 5748, "code": "def sleep here ( count , t ) : import time , sys print ( \"hi from engine %i\" % id ) sys . stdout . flush ( ) time . sleep ( t ) return count , t", "predictions": ["highlight the result of an engine"], "references": ["simple function that takes args prints a short message sleeps for a time and returns the same args"], "bleu": 0.02989501072722226, "rouge_l": 0.07644110275689221}
{"id": 5749, "code": "def convert pyx sources to c ( self ) : def pyx to c ( source ) : if source . endswith ( '.pyx' ) : source = source [ : - 4 ] + '.c' return source self . sources = map ( pyx to c , self . sources )", "predictions": ["highlight the source string to keywords"], "references": ["convert . pyx extensions to . c"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5750, "code": "def main ( connection file ) : ctx = zmq . Context . instance ( ) with open ( connection file ) as f : cfg = json . loads ( f . read ( ) ) location = cfg [ 'location' ] reg url = cfg [ 'url' ] session = Session ( key = str to bytes ( cfg [ 'exec key' ] ) ) query = ctx . socket ( zmq . DEALER ) query . connect ( disambiguate url ( cfg [ 'url' ] , location ) ) session . send ( query , \"connection request\" ) idents , msg = session . recv ( query , mode = 0 ) c = msg [ 'content' ] iopub url = disambiguate url ( c [ 'iopub' ] , location ) sub = ctx . socket ( zmq . SUB ) sub . setsockopt ( zmq . SUBSCRIBE , b'' ) sub . connect ( iopub url ) while True : try : idents , msg = session . recv ( sub , mode = 0 ) except Keyboard Interrupt : return topic = idents [ 0 ] if msg [ 'msg type' ] == 'stream' : print ( \"%s: %s\" % ( topic , msg [ 'content' ] [ 'data' ] ) ) elif msg [ 'msg type' ] == 'pyerr' : c = msg [ 'content' ] print ( topic + ':' ) for line in c [ 'traceback' ] : print ( '    ' + line )", "predictions": ["highlight the connection to the kafka channel . ."], "references": ["watch iopub channel and print messages"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 5751, "code": "def log level changed ( self , name , old , new ) : if isinstance ( new , basestring ) : new = getattr ( logging , new ) self . log level = new self . log . set Level ( new )", "predictions": ["logs an old setup ."], "references": ["adjust the log level when log_level is set ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 5752, "code": "def flags changed ( self , name , old , new ) : for key , value in new . iteritems ( ) : assert len ( value ) == 2 , \"Bad flag: %r:%s\" % ( key , value ) assert isinstance ( value [ 0 ] , ( dict , Config ) ) , \"Bad flag: %r:%s\" % ( key , value ) assert isinstance ( value [ 1 ] , basestring ) , \"Bad flag: %r:%s\" % ( key , value )", "predictions": ["changes the run run run in the try to the try to change the value of the run ."], "references": ["ensure flags dict is valid"], "bleu": 0.05415315253510895, "rouge_l": 0.0}
{"id": 5753, "code": "def print alias help ( self ) : if not self . aliases : return lines = [ ] classdict = { } for cls in self . classes : for c in cls . mro ( ) [ : - 3 ] : classdict [ c . name ] = c for alias , longname in self . aliases . iteritems ( ) : classname , traitname = longname . split ( '.' , 1 ) cls = classdict [ classname ] trait = cls . class traits ( config = True ) [ traitname ] help = cls . class get trait help ( trait ) . splitlines ( ) help [ 0 ] = help [ 0 ] . replace ( longname , alias ) + ' (%s)' % longname if len ( alias ) == 1 : help [ 0 ] = help [ 0 ] . replace ( '--%s=' % alias , '-%s ' % alias ) lines . extend ( help ) print os . linesep . join ( lines )", "predictions": ["unquote the ends of the ends of all 0 ."], "references": ["print the alias part of the help ."], "bleu": 0.18850319022747347, "rouge_l": 0.4535315985130111}
{"id": 5754, "code": "def print flag help ( self ) : if not self . flags : return lines = [ ] for m , ( cfg , help ) in self . flags . iteritems ( ) : prefix = '--' if len ( m ) > 1 else '-' lines . append ( prefix + m ) lines . append ( indent ( dedent ( help . strip ( ) ) ) ) print os . linesep . join ( lines )", "predictions": ["find the help ."], "references": ["print the flag part of the help ."], "bleu": 0.24601580968354606, "rouge_l": 0.47164948453608246}
{"id": 5755, "code": "def print subcommands ( self ) : if not self . subcommands : return lines = [ \"Subcommands\" ] lines . append ( '-' * len ( lines [ 0 ] ) ) lines . append ( '' ) for p in wrap paragraphs ( self . subcommand description ) : lines . append ( p ) lines . append ( '' ) for subc , ( cls , help ) in self . subcommands . iteritems ( ) : lines . append ( subc ) if help : lines . append ( indent ( dedent ( help . strip ( ) ) ) ) lines . append ( '' ) print os . linesep . join ( lines )", "predictions": ["get the subcommand lines"], "references": ["print the subcommand part of the help ."], "bleu": 0.1739594473063345, "rouge_l": 0.31443298969072164}
{"id": 5756, "code": "def update config ( self , config ) : newconfig = deepcopy ( self . config ) newconfig . merge ( config ) self . config = newconfig", "predictions": ["build the kernel for the given kernel if it exists if not already there"], "references": ["fire the traits events when the config is updated ."], "bleu": 0.09782375748961449, "rouge_l": 0.17183098591549298}
{"id": 5757, "code": "def initialize subcommand ( self , subc , argv = None ) : subapp , help = self . subcommands . get ( subc ) if isinstance ( subapp , basestring ) : subapp = import item ( subapp ) self . class . clear instance ( ) self . subapp = subapp . instance ( ) self . subapp . initialize ( argv )", "predictions": ["initializes the ssh ssh ssh ssh ssh ssh ssh"], "references": ["initialize a subcommand with argv ."], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5758, "code": "def parse command line ( self , argv = None ) : argv = sys . argv [ 1 : ] if argv is None else argv if argv and argv [ 0 ] == 'help' : argv = argv [ 1 : ] + [ '-h' ] if self . subcommands and len ( argv ) > 0 : subc , subargv = argv [ 0 ] , argv [ 1 : ] if re . match ( r'^\\w(\\-?\\w)*$' , subc ) and subc in self . subcommands : return self . initialize subcommand ( subc , subargv ) if '-h' in argv or '--help' in argv or '--help-all' in argv : self . print description ( ) self . print help ( '--help-all' in argv ) self . print examples ( ) self . exit ( 0 ) if '--version' in argv or '-V' in argv : self . print version ( ) self . exit ( 0 ) flags , aliases = self . flatten flags ( ) loader = KV Arg Parse Config Loader ( argv = argv , aliases = aliases , flags = flags ) config = loader . load config ( ) self . update config ( config ) self . extra args = loader . extra args", "predictions": ["pretty - print a command obj"], "references": ["parse the command line arguments ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 5759, "code": "def load config file ( self , filename , path = None ) : loader = Py File Config Loader ( filename , path = path ) try : config = loader . load config ( ) except Config File Not Found : raise except Exception : filename = loader . full filename or filename self . log . error ( \"Exception while loading config file %s\" , filename , exc info = True ) else : self . log . debug ( \"Loaded config file: %s\" , loader . full filename ) self . update config ( config )", "predictions": ["pprint config config . ."], "references": ["load a . py based config file by filename and path ."], "bleu": 0.08006212224540951, "rouge_l": 0.2190305206463196}
{"id": 5760, "code": "def generate config file ( self ) : lines = [ % self . name ] lines . append ( '' ) lines . append ( 'c = get config()' ) lines . append ( '' ) for cls in self . classes : lines . append ( cls . class config section ( ) ) return '\\n' . join ( lines )", "predictions": ["generate a config file for the config file ."], "references": ["generate default config file from configurables"], "bleu": 0.19960198807747329, "rouge_l": 0.4149659863945578}
{"id": 5761, "code": "def downsample ( array , k ) : length = array . shape [ 0 ] indices = random . sample ( xrange ( length ) , k ) return array [ indices ]", "predictions": ["downsample a random k - d array ."], "references": ["choose k random elements of array ."], "bleu": 0.239802967618271, "rouge_l": 0.4048672566371681}
{"id": 5762, "code": "def write ( self , msg ) : if self . should ( 'pid' ) : msg = \"pid %5d: %s\" % ( os . getpid ( ) , msg ) self . output . write ( msg + \"\\n\" ) self . output . flush ( )", "predictions": ["write message to output stream ."], "references": ["write a line of debug output ."], "bleu": 0.22236312185643822, "rouge_l": 0.45522388059701485}
{"id": 5763, "code": "def class config section ( cls ) : def c ( s ) : \"\"\"return a commented, wrapped block.\"\"\" s = '\\n\\n' . join ( wrap paragraphs ( s , 78 ) ) return + s . replace ( '\\n' , ) breaker = '#' + '-' * 78 s = % cls . name lines = [ breaker , s , breaker , '' ] desc = cls . class traits ( ) . get ( 'description' ) if desc : desc = desc . default value else : desc = getattr ( cls , ' doc ' , '' ) if desc : lines . append ( c ( desc ) ) lines . append ( '' ) parents = [ ] for parent in cls . mro ( ) : if parent is not cls and issubclass ( parent , Configurable ) and parent . class traits ( config = True ) : parents . append ( parent ) if parents : pstr = ', ' . join ( [ p . name for p in parents ] ) lines . append ( c ( '%s will inherit config from: %s' % ( cls . name , pstr ) ) ) lines . append ( '' ) for name , trait in cls . class traits ( config = True ) . iteritems ( ) : help = trait . get metadata ( 'help' ) or '' lines . append ( c ( help ) ) lines . append ( % ( cls . name , name , trait . get default value ( ) ) ) lines . append ( '' ) return '\\n' . join ( lines )", "predictions": ["return a string describing the class config config section ."], "references": ["get the config class config section"], "bleu": 0.21834177214239062, "rouge_l": 0.5236051502145923}
{"id": 5764, "code": "def clear instance ( cls ) : if not cls . initialized ( ) : return for subclass in cls . walk mro ( ) : if isinstance ( subclass . instance , cls ) : subclass . instance = None", "predictions": ["clears all instance instance instance ."], "references": ["unset _instance for this class and singleton parents ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 5765, "code": "def format Failure ( self , test , err ) : ec , ev , tb = err tbinfo = inspect traceback ( tb ) test . tbinfo = tbinfo return ( ec , '\\n' . join ( [ str ( ev ) , tbinfo ] ) , tb )", "predictions": ["format the formatted string for the test ."], "references": ["add detail from traceback inspection to error message of a failure ."], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 5766, "code": "def crash handler lite ( etype , evalue , tb ) : traceback . print exception ( etype , evalue , tb ) from I Python . core . interactiveshell import Interactive Shell if Interactive Shell . initialized ( ) : config = \"%config \" else : config = \"c.\" print >> sys . stderr , lite message template . format ( email = author email , config = config )", "predictions": ["override default crash method ."], "references": ["a light excepthook adding a small message to the usual traceback"], "bleu": 0.0691466264618537, "rouge_l": 0.0}
{"id": 5767, "code": "def make report ( self , traceback ) : sec sep = self . section sep report = [ '*' * 75 + '\\n\\n' + 'I Python post-mortem report\\n\\n' ] rpt add = report . append rpt add ( sys info ( ) ) try : config = pformat ( self . app . config ) rpt add ( sec sep ) rpt add ( 'Application name: %s\\n\\n' % self . app name ) rpt add ( 'Current user configuration structure:\\n\\n' ) rpt add ( config ) except : pass rpt add ( sec sep + 'Crash traceback:\\n\\n' + traceback ) return '' . join ( report )", "predictions": ["returns a report report ."], "references": ["return a string containing a crash report ."], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 5768, "code": "def call handlers ( self , msg ) : self . message received . emit ( msg ) msg type = msg [ 'header' ] [ 'msg type' ] signal = getattr ( self , msg type , None ) if signal : signal . emit ( msg ) if not self . handlers called : self . first reply . emit ( ) self . handlers called = True", "predictions": ["handle the message received from the client ."], "references": ["reimplemented to emit signals instead of making callbacks ."], "bleu": 0.1415224185897875, "rouge_l": 0.116412213740458}
{"id": 5769, "code": "def call handlers ( self , msg ) : self . message received . emit ( msg ) msg type = msg [ 'header' ] [ 'msg type' ] signal = getattr ( self , msg type + ' received' , None ) if signal : signal . emit ( msg ) elif msg type in ( 'stdout' , 'stderr' ) : self . stream received . emit ( msg )", "predictions": ["show the message handlers with the given msg ."], "references": ["reimplemented to emit signals instead of making callbacks ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 5770, "code": "def flush ( self ) : super ( Qt Sub Socket Channel , self ) . flush ( ) Qt Core . Q Core Application . instance ( ) . process Events ( )", "predictions": ["flushes the instance ."], "references": ["reimplemented to ensure that signals are dispatched immediately ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 5771, "code": "def call handlers ( self , msg ) : self . message received . emit ( msg ) msg type = msg [ 'header' ] [ 'msg type' ] if msg type == 'input request' : self . input requested . emit ( msg )", "predictions": ["call the message handlers with the specified msg ."], "references": ["reimplemented to emit signals instead of making callbacks ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 5772, "code": "def start kernel ( self , * args , * * kw ) : if self . shell channel is not None : self . shell channel . reset first reply ( ) super ( Qt Kernel Manager , self ) . start kernel ( * args , * * kw ) self . started kernel . emit ( )", "predictions": ["start the kernel reply ."], "references": ["reimplemented for proper heartbeat management ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 5773, "code": "def start channels ( self , * args , * * kw ) : super ( Qt Kernel Manager , self ) . start channels ( * args , * * kw ) self . started channels . emit ( )", "predictions": ["start the channels ."], "references": ["reimplemented to emit signal ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 5774, "code": "def shell channel ( self ) : if self . shell channel is None : self . shell channel = super ( Qt Kernel Manager , self ) . shell channel self . shell channel . first reply . connect ( self . first reply ) return self . shell channel", "predictions": ["return the shell channel channel ."], "references": ["reimplemented for proper heartbeat management ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 5775, "code": "def read ( self , fp , * * kwargs ) : nbs = fp . read ( ) if not py3compat . PY3 and not isinstance ( nbs , unicode ) : nbs = py3compat . str to unicode ( nbs ) return self . reads ( nbs , * * kwargs )", "predictions": ["read data from file"], "references": ["read a notebook from a file like object"], "bleu": 0.1571901051328651, "rouge_l": 0.47164948453608246}
{"id": 5776, "code": "def write ( self , nb , fp , * * kwargs ) : nbs = self . writes ( nb , * * kwargs ) if not py3compat . PY3 and not isinstance ( nbs , unicode ) : nbs = py3compat . str to unicode ( nbs ) return fp . write ( nbs )", "predictions": ["write this instance to a file - like object ."], "references": ["write a notebook to a file like object"], "bleu": 0.28997844147152074, "rouge_l": 0.6802973977695167}
{"id": 5777, "code": "def method magic marker ( magic kind ) : validate type ( magic kind ) def magic deco ( arg ) : call = lambda f , * a , * * k : f ( * a , * * k ) if callable ( arg ) : func = arg name = func . func name retval = decorator ( call , func ) record magic ( magics , magic kind , name , name ) elif isinstance ( arg , basestring ) : name = arg def mark ( func , * a , * * kw ) : record magic ( magics , magic kind , name , func . func name ) return decorator ( call , func ) retval = mark else : raise Type Error ( \"Decorator can only be called with \" \"string or function\" ) return retval magic deco . doc = docstring template . format ( 'method' , magic kind ) return magic deco", "predictions": ["decorator to method for registering magic marker ."], "references": ["decorator factory for methods in magics subclasses ."], "bleu": 0.19070828081828378, "rouge_l": 0.375}
{"id": 5778, "code": "def function magic marker ( magic kind ) : validate type ( magic kind ) def magic deco ( arg ) : call = lambda f , * a , * * k : f ( * a , * * k ) caller = sys . getframe ( 1 ) for ns in [ 'f locals' , 'f globals' , 'f builtins' ] : get ipython = getattr ( caller , ns ) . get ( 'get ipython' ) if get ipython is not None : break else : raise Name Error ( 'Decorator can only run in context where ' '`get ipython` exists' ) ip = get ipython ( ) if callable ( arg ) : func = arg name = func . func name ip . register magic function ( func , magic kind , name ) retval = decorator ( call , func ) elif isinstance ( arg , basestring ) : name = arg def mark ( func , * a , * * kw ) : ip . register magic function ( func , magic kind , name ) return decorator ( call , func ) retval = mark else : raise Type Error ( \"Decorator can only be called with \" \"string or function\" ) return retval ds = docstring template . format ( 'function' , magic kind ) ds += dedent ( ) magic deco . doc = ds return magic deco", "predictions": ["decorator to register a function magic marker ."], "references": ["decorator factory for standalone functions ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 5779, "code": "def format latex ( self , strng ) : escape re = re . compile ( r'(%| |\\$|#|&)' , re . MULTILINE ) cmd name re = re . compile ( r'^(%s.*?):' % ESC MAGIC , re . MULTILINE ) cmd re = re . compile ( r'(?P<cmd>%s.+?\\b)(?!\\}\\}:)' % ESC MAGIC , re . MULTILINE ) par re = re . compile ( r'\\\\$' , re . MULTILINE ) newline re = re . compile ( r'\\\\n' ) #strng = cmd name re.sub(r'\\n\\\\texttt{\\\\textsl{\\\\large \\1}}:',strng) strng = cmd name re . sub ( r'\\n\\\\bigskip\\n\\\\texttt{\\\\textbf{ \\1}}:' , strng ) strng = cmd re . sub ( r'\\\\texttt{\\g<cmd>}' , strng ) strng = par re . sub ( r'\\\\\\\\' , strng ) strng = escape re . sub ( r'\\\\\\1' , strng ) strng = newline re . sub ( r'\\\\textbackslash{}n' , strng ) return strng", "predictions": ["format latex string to latex ."], "references": ["format a string for latex inclusion ."], "bleu": 0.23512037509993022, "rouge_l": 0.6069651741293532}
{"id": 5780, "code": "def default option ( self , fn , optstr ) : if fn not in self . lsmagic ( ) : error ( \"%s is not a magic function\" % fn ) self . options table [ fn ] = optstr", "predictions": ["default option for the default option"], "references": ["make an entry in the options_table for fn with value optstr"], "bleu": 0.10624253482403696, "rouge_l": 0.1117216117216117}
{"id": 5781, "code": "def page guiref ( arg s = None ) : from I Python . core import page page . page ( gui reference , auto html = True )", "predictions": ["load an html page from an html page ."], "references": ["show a basic reference about the gui console ."], "bleu": 0.14113991930789777, "rouge_l": 0.1111111111111111}
{"id": 5782, "code": "def task with callable ( the callable , label = None , schedule = DEFAULT SCHEDULE , userdata = None , pk override = None ) : task = Task ( ) if isinstance ( the callable , str ) : if pk override is not None : components = the callable . split ( '.' ) info = dict ( func type = 'instancemethod' , module name = '.' . join ( components [ : - 2 ] ) , class name = components [ - 2 ] , class path = '.' . join ( components [ : - 1 ] ) , model pk = pk override , func name = components [ - 1 ] , func path = the callable , ) task . funcinfo = info else : task . funcinfo = get func info ( func from string ( the callable ) ) else : task . funcinfo = get func info ( the callable ) if label is None : task . label = task . funcinfo [ 'func path' ] else : task . label = label task . schedule = schedule if not croniter . is valid ( task . schedule ) : raise Value Error ( f\"Cron schedule {task.schedule} is not valid\" ) if userdata is None : task . userdata = dict ( ) else : if isinstance ( userdata , dict ) : task . userdata = userdata else : raise Value Error ( \"Userdata must be a dictionary of JSON-serializable data\" ) return task", "predictions": ["create a task with the callable ."], "references": ["factory function to create a properly initialized task ."], "bleu": 0.20873176328735715, "rouge_l": 0.48897795591182364}
{"id": 5783, "code": "def func from info ( self ) : info = self . funcinfo functype = info [ 'func type' ] if functype in [ 'instancemethod' , 'classmethod' , 'staticmethod' ] : the modelclass = get module member by dottedpath ( info [ 'class path' ] ) if functype == 'instancemethod' : the modelobject = the modelclass . objects . get ( pk = info [ 'model pk' ] ) the callable = get member ( the modelobject , info [ 'func name' ] ) else : the callable = get member ( the modelclass , info [ 'func name' ] ) return the callable elif functype == 'function' : mod = import module ( info [ 'module name' ] ) the callable = get member ( mod , info [ 'func name' ] ) return the callable else : raise Value Error ( f\"Unknown functype '{functype} in task {self.pk} ({self.label})\" )", "predictions": ["return the callable object from the info file ."], "references": ["find and return a callable object from a task info dictionary"], "bleu": 0.23278666914796883, "rouge_l": 0.4911433172302737}
{"id": 5784, "code": "def calc next run ( self ) : base time = self . last run if self . last run == HAS NOT RUN : if self . wait for schedule is False : self . next run = timezone . now ( ) self . wait for schedule = False self . save ( ) return else : base time = timezone . now ( ) self . next run = croniter ( self . schedule , base time ) . get next ( datetime ) self . save ( )", "predictions": ["calculates the next run run ."], "references": ["calculate next run time of this task"], "bleu": 0.24608524656663955, "rouge_l": 0.3034825870646766}
{"id": 5785, "code": "def run ( self , message ) : the callable = self . func from info ( ) try : task message = dict ( task = self , channel message = message , ) the callable ( task message ) finally : if self . end running < self . next run : self . enabled = False Channel ( KILL TASK CHANNEL ) . send ( { 'id' : self . pk } ) return if self . iterations == 0 : return else : self . iterations -= 1 if self . iterations == 0 : self . enabled = False Channel ( KILL TASK CHANNEL ) . send ( { 'id' : self . pk } ) self . save ( )", "predictions": ["run the task ."], "references": ["internal instance method run by worker process to actually run the task callable ."], "bleu": 0.05804285916064726, "rouge_l": 0.40397350993377484}
{"id": 5786, "code": "def run asap ( self ) : now = timezone . now ( ) self . last run = now self . calc next run ( ) self . save ( ) self . submit ( now )", "predictions": ["run the next run"], "references": ["instance method to run this task immediately ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 5787, "code": "def run iterations ( cls , the callable , iterations = 1 , label = None , schedule = '* * * * * *' , userdata = None , run immediately = False , delay until = None ) : task = task with callable ( the callable , label = label , schedule = schedule , userdata = userdata ) task . iterations = iterations if delay until is not None : if isinstance ( delay until , datetime ) : if delay until > timezone . now ( ) : task . start running = delay until else : raise Value Error ( \"Task cannot start running in the past\" ) else : raise Value Error ( \"delay until must be a datetime.datetime instance\" ) if run immediately : task . next run = timezone . now ( ) else : task . calc next run ( ) task . save ( )", "predictions": ["run the task in the callable ."], "references": ["class method to run a callable with a specified number of iterations"], "bleu": 0.10063351655856649, "rouge_l": 0.20098846787479407}
{"id": 5788, "code": "def run once ( cls , the callable , userdata = None , delay until = None ) : cls . run iterations ( the callable , userdata = userdata , run immediately = True , delay until = delay until )", "predictions": ["run the given callable ."], "references": ["class method to run a one - shot task immediately ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 5789, "code": "def bind kernel ( self , * * kwargs ) : if self . kernel app is not None : return self . log . info ( \"Opening ports for direct connections as an I Python kernel\" ) kernel = self . kernel kwargs . setdefault ( 'config' , self . config ) kwargs . setdefault ( 'log' , self . log ) kwargs . setdefault ( 'profile dir' , self . profile dir ) kwargs . setdefault ( 'session' , self . engine . session ) app = self . kernel app = IP Kernel App ( * * kwargs ) IP Kernel App . instance = app app . init connection file ( ) app . shell port = app . bind socket ( kernel . shell streams [ 0 ] , app . shell port ) app . log . debug ( \"shell ROUTER Channel on port: %i\" , app . shell port ) app . iopub port = app . bind socket ( kernel . iopub socket , app . iopub port ) app . log . debug ( \"iopub PUB Channel on port: %i\" , app . iopub port ) kernel . stdin socket = self . engine . context . socket ( zmq . ROUTER ) app . stdin port = app . bind socket ( kernel . stdin socket , app . stdin port ) app . log . debug ( \"stdin ROUTER Channel on port: %i\" , app . stdin port ) app . init heartbeat ( ) app . log connection info ( ) app . write connection file ( )", "predictions": ["bind the kernel ports to the direct kernel ."], "references": ["promote engine to listening kernel accessible to frontends ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 5790, "code": "def pid exists ( pid ) : if not isinstance ( pid , int ) : raise Type Error ( 'an integer is required' ) if pid < 0 : return False try : os . kill ( pid , 0 ) except OS Error : e = sys . exc info ( ) [ 1 ] return e . errno == errno . EPERM else : return True", "predictions": ["check if a pid exists ."], "references": ["check whether pid exists in the current process table ."], "bleu": 0.16959011078459055, "rouge_l": 0.47843137254901963}
{"id": 5791, "code": "def get disk usage ( path ) : st = os . statvfs ( path ) free = ( st . f bavail * st . f frsize ) total = ( st . f blocks * st . f frsize ) used = ( st . f blocks - st . f bfree ) * st . f frsize percent = usage percent ( used , total , round = 1 ) return nt diskinfo ( total , used , free , percent )", "predictions": ["get the usage of the disk ."], "references": ["return disk usage associated with path ."], "bleu": 0.22089591134157885, "rouge_l": 0.2857142857142857}
{"id": 5792, "code": "def run ( self ) : try : from winapi import WAIT OBJECT 0 , INFINITE except Import Error : from subprocess import WAIT OBJECT 0 , INFINITE handles = [ ] if self . interrupt handle : handles . append ( self . interrupt handle ) if self . parent handle : handles . append ( self . parent handle ) arch = platform . architecture ( ) [ 0 ] c int = ctypes . c int64 if arch . startswith ( '64' ) else ctypes . c int while True : result = ctypes . windll . kernel32 . Wait For Multiple Objects ( len ( handles ) , ( c int * len ( handles ) ) ( * handles ) , False , INFINITE ) if WAIT OBJECT 0 <= result < len ( handles ) : handle = handles [ result - WAIT OBJECT 0 ] if handle == self . interrupt handle : interrupt main ( ) elif handle == self . parent handle : os . exit ( 1 ) elif result < 0 : warn ( ) return", "predictions": ["generate the section class class class class class class class class class class class class class class class class class class class class class class class class class class class method"], "references": ["run the poll loop . this method never returns ."], "bleu": 0.04317900023606586, "rouge_l": 0.10748898678414096}
{"id": 5793, "code": "def filter ns ( ns , name pattern = \"*\" , type pattern = \"all\" , ignore case = True , show all = True ) : pattern = name pattern . replace ( \"*\" , \".*\" ) . replace ( \"?\" , \".\" ) if ignore case : reg = re . compile ( pattern + \"$\" , re . I ) else : reg = re . compile ( pattern + \"$\" ) return dict ( ( key , obj ) for key , obj in ns . iteritems ( ) if reg . match ( key ) and show hidden ( key , show all ) and is type ( obj , type pattern ) )", "predictions": ["downsample a pattern object ."], "references": ["filter a namespace dictionary by name pattern and item type ."], "bleu": 0.09778809693469985, "rouge_l": 0.35124760076775424}
{"id": 5794, "code": "def draw if interactive ( ) : fig = Gcf . get active ( ) . canvas . figure if not hasattr ( fig , 'show' ) : fig . show = lambda * a : send figure ( fig ) if not matplotlib . is interactive ( ) : return try : show . to draw . remove ( fig ) except Value Error : pass show . to draw . append ( fig ) show . draw called = True", "predictions": ["write the figure if to the self should be inserted"], "references": ["is called after every pylab drawing command"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 5795, "code": "def send figure ( fig ) : fmt = Inline Backend . instance ( ) . figure format data = print figure ( fig , fmt ) if data is None : return mimetypes = { 'png' : 'image/png' , 'svg' : 'image/svg+xml' } mime = mimetypes [ fmt ] sys . stdout . flush ( ) sys . stderr . flush ( ) publish display data ( 'I Python.zmq.pylab.backend inline.send figure' , { mime : data } )", "predictions": ["class method to class config"], "references": ["draw the given figure and send it as a png payload ."], "bleu": 0.0566124695559154, "rouge_l": 0.0}
{"id": 5796, "code": "def handle sigint ( self , sig , frame ) : signal . signal ( signal . SIGINT , self . signal stop ) thread = threading . Thread ( target = self . confirm exit ) thread . daemon = True thread . start ( )", "predictions": ["start is a in the queue"], "references": ["sigint handler spawns confirmation dialog"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 5797, "code": "def render ( self , name , color = True , * * kwargs ) : if name == 'rewrite' : return self . render rewrite ( color = color ) if color : scheme = self . color scheme table . active colors if name == 'out' : colors = color lists [ 'normal' ] colors . number , colors . prompt , colors . normal = scheme . out number , scheme . out prompt , scheme . normal else : colors = color lists [ 'inp' ] colors . number , colors . prompt , colors . normal = scheme . in number , scheme . in prompt , scheme . in normal if name == 'in2' : colors . prompt = scheme . in prompt2 else : colors = color lists [ 'nocolor' ] colors . number , colors . prompt , colors . normal = '' , '' , '' count = self . shell . execution count fmtargs = dict ( color = colors , count = count , dots = \".\" * len ( str ( count ) ) , width = self . width , txtwidth = self . txtwidth ) fmtargs . update ( self . lazy evaluate fields ) fmtargs . update ( kwargs ) prompt = colors . prompt + self . templates [ name ] + colors . normal return self . formatter . format ( prompt , * * fmtargs )", "predictions": ["format a widget or a rdf join ."], "references": ["render but don t justify or update the width or txtwidth attributes ."], "bleu": 0.09499501502705178, "rouge_l": 0.18263473053892215}
{"id": 5798, "code": "def mappable ( obj ) : if isinstance ( obj , ( tuple , list ) ) : return True for m in array Modules : if isinstance ( obj , m [ 'type' ] ) : return True return False", "predictions": ["check if an object is a list"], "references": ["return whether an object is mappable or not ."], "bleu": 0.2598013194025897, "rouge_l": 0.3667334669338677}
{"id": 5799, "code": "def get Partition ( self , seq , p , q ) : if p < 0 or p >= q : print \"No partition exists.\" return remainder = len ( seq ) % q basesize = len ( seq ) // q hi = [ ] lo = [ ] for n in range ( q ) : if n < remainder : lo . append ( n * ( basesize + 1 ) ) hi . append ( lo [ - 1 ] + basesize + 1 ) else : lo . append ( n * basesize + remainder ) hi . append ( lo [ - 1 ] + basesize ) try : result = seq [ lo [ p ] : hi [ p ] ] except Type Error : result = list ( islice ( seq , lo [ p ] , hi [ p ] ) ) return result", "predictions": ["make a list of exists.\" objects"], "references": ["returns the pth partition of q partitions of seq ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 5800, "code": "def main ( ) : parser = optparse . Option Parser ( usage = MAIN USAGE ) newopt = parser . add option newopt ( '--ipython' , action = 'store const' , dest = 'mode' , const = 'ipython' , help = 'I Python interactive runner (default).' ) newopt ( '--python' , action = 'store const' , dest = 'mode' , const = 'python' , help = 'Python interactive runner.' ) newopt ( '--sage' , action = 'store const' , dest = 'mode' , const = 'sage' , help = 'SAGE interactive runner.' ) opts , args = parser . parse args ( ) runners = dict ( ipython = I Python Runner , python = Python Runner , sage = SAGE Runner ) try : ext = os . path . splitext ( args [ 0 ] ) [ - 1 ] except Index Error : ext = '' modes = { '.ipy' : 'ipython' , '.py' : 'python' , '.sage' : 'sage' } mode = modes . get ( ext , \"ipython\" ) if opts . mode : mode = opts . mode runners [ mode ] ( ) . main ( args )", "predictions": ["call optparse to generate optparse"], "references": ["run as a command - line script ."], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 5801, "code": "def main ( self , argv = None ) : parser = optparse . Option Parser ( usage = USAGE % self . class . name ) newopt = parser . add option newopt ( '-i' , '--interact' , action = 'store true' , default = False , help = 'Interact with the program after the script is run.' ) opts , args = parser . parse args ( argv ) if len ( args ) != 1 : print >> sys . stderr , \"You must supply exactly one file to run.\" sys . exit ( 1 ) self . run file ( args [ 0 ] , opts . interact )", "predictions": ["command line interface for the script"], "references": ["run as a command - line script ."], "bleu": 0.18822631894109965, "rouge_l": 0.4178082191780822}
{"id": 5802, "code": "def xml file ( self , cu , analysis ) : package name = rpartition ( cu . name , \".\" ) [ 0 ] class Name = cu . name package = self . packages . setdefault ( package name , [ { } , 0 , 0 , 0 , 0 ] ) xclass = self . xml out . create Element ( \"class\" ) xclass . append Child ( self . xml out . create Element ( \"methods\" ) ) xlines = self . xml out . create Element ( \"lines\" ) xclass . append Child ( xlines ) xclass . set Attribute ( \"name\" , class Name ) filename = cu . file locator . relative filename ( cu . filename ) xclass . set Attribute ( \"filename\" , filename . replace ( \"\\\\\" , \"/\" ) ) xclass . set Attribute ( \"complexity\" , \"0\" ) branch stats = analysis . branch stats ( ) for line in sorted ( analysis . statements ) : xline = self . xml out . create Element ( \"line\" ) xline . set Attribute ( \"number\" , str ( line ) ) xline . set Attribute ( \"hits\" , str ( int ( line not in analysis . missing ) ) ) if self . arcs : if line in branch stats : total , taken = branch stats [ line ] xline . set Attribute ( \"branch\" , \"true\" ) xline . set Attribute ( \"condition-coverage\" , \"%d%% (%d/%d)\" % ( 100 * taken / total , taken , total ) ) xlines . append Child ( xline ) class lines = len ( analysis . statements ) class hits = class lines - len ( analysis . missing ) if self . arcs : class branches = sum ( [ t for t , k in branch stats . values ( ) ] ) missing branches = sum ( [ t - k for t , k in branch stats . values ( ) ] ) class br hits = class branches - missing branches else : class branches = 0.0 class br hits = 0.0 xclass . set Attribute ( \"line-rate\" , rate ( class hits , class lines ) ) xclass . set Attribute ( \"branch-rate\" , rate ( class br hits , class branches ) ) package [ 0 ] [ class Name ] = xclass package [ 1 ] += class hits package [ 2 ] += class lines package [ 3 ] += class br hits package [ 4 ] += class branches", "predictions": ["this method creates the flush file for the given analysis . . . . . ."], "references": ["add to the xml report for a single file ."], "bleu": 0.09672649511413092, "rouge_l": 0.24078947368421053}
{"id": 5803, "code": "def reduce freqs ( freqlist ) : allfreqs = np . zeros like ( freqlist [ 0 ] ) for f in freqlist : allfreqs += f return allfreqs", "predictions": ["call the received received in the given list of self message message"], "references": ["add up a list of freq counts to get the total counts ."], "bleu": 0.13519230385081712, "rouge_l": 0.15885416666666669}
{"id": 5804, "code": "def compute n digit freqs ( filename , n ) : d = txt file to digits ( filename ) freqs = n digit freqs ( d , n ) return freqs", "predictions": ["start the kernel digit with the given * * * * * * * * * * * * * * * * * * * * * * * *"], "references": ["read digits of pi from a file and compute the n digit frequencies ."], "bleu": 0.04317900023606586, "rouge_l": 0.09538702111024237}
{"id": 5805, "code": "def txt file to digits ( filename , the type = str ) : with open ( filename , 'r' ) as f : for line in f . readlines ( ) : for c in line : if c != '\\n' and c != ' ' : yield the type ( c )", "predictions": ["convert a channels to a self . self . self . self . self . self . self . self . self . self . self . self . self ."], "references": ["yield the digits of pi read from a . txt file ."], "bleu": 0.046398855339878003, "rouge_l": 0.1516155758077879}
{"id": 5806, "code": "def one digit freqs ( digits , normalize = False ) : freqs = np . zeros ( 10 , dtype = 'i4' ) for d in digits : freqs [ int ( d ) ] += 1 if normalize : freqs = freqs / freqs . sum ( ) return freqs", "predictions": ["return shell channel reply"], "references": ["consume digits of pi and compute 1 digit freq . counts ."], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 5807, "code": "def two digit freqs ( digits , normalize = False ) : freqs = np . zeros ( 100 , dtype = 'i4' ) last = digits . next ( ) this = digits . next ( ) for d in digits : index = int ( last + this ) freqs [ index ] += 1 last = this this = d if normalize : freqs = freqs / freqs . sum ( ) return freqs", "predictions": ["return read digit of read fp"], "references": ["consume digits of pi and compute 2 digits freq . counts ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 5808, "code": "def plot two digit freqs ( f2 ) : f2 copy = f2 . copy ( ) f2 copy . shape = ( 10 , 10 ) ax = plt . matshow ( f2 copy ) plt . colorbar ( ) for i in range ( 10 ) : for j in range ( 10 ) : plt . text ( i - 0.2 , j + 0.2 , str ( j ) + str ( i ) ) plt . ylabel ( 'First digit' ) plt . xlabel ( 'Second digit' ) return ax", "predictions": ["write two self . to the colorbar"], "references": ["plot two digits frequency counts using matplotlib ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 5809, "code": "def plot one digit freqs ( f1 ) : ax = plt . plot ( f1 , 'bo-' ) plt . title ( 'Single digit counts in pi' ) plt . xlabel ( 'Digit' ) plt . ylabel ( 'Count' ) return ax", "predictions": ["method to method to method plots the marker of the marker"], "references": ["plot one digit frequency counts using matplotlib ."], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 5810, "code": "def debug src ( src , pm = False , globs = None ) : testsrc = script from examples ( src ) debug script ( testsrc , pm , globs )", "predictions": ["dump an debugging instance from an magic file ."], "references": ["debug a single doctest docstring in argument src"], "bleu": 0.11868405219520975, "rouge_l": 0.0}
{"id": 5811, "code": "def debug script ( src , pm = False , globs = None ) : import pdb srcfilename = tempfile . mktemp ( \".py\" , \"doctestdebug\" ) f = open ( srcfilename , 'w' ) f . write ( src ) f . close ( ) try : if globs : globs = globs . copy ( ) else : globs = { } if pm : try : execfile ( srcfilename , globs , globs ) except : print sys . exc info ( ) [ 1 ] pdb . post mortem ( sys . exc info ( ) [ 2 ] ) else : pdb . run ( \"execfile(%r)\" % srcfilename , globs , globs ) finally : os . remove ( srcfilename )", "predictions": ["run a latex latex latex latex cmd cmd cmd"], "references": ["debug a test script . src is the script as a string ."], "bleu": 0.09049614828481034, "rouge_l": 0.08802308802308802}
{"id": 5812, "code": "def hdict ( self , hashroot ) : hfiles = self . keys ( hashroot + \"/*\" ) hfiles . sort ( ) last = len ( hfiles ) and hfiles [ - 1 ] or '' if last . endswith ( 'xx' ) : hfiles = [ last ] + hfiles [ : - 1 ] all = { } for f in hfiles : try : all . update ( self [ f ] ) except Key Error : print \"Corrupt\" , f , \"deleted - hset is not threadsafe!\" del self [ f ] self . uncache ( f ) return all", "predictions": ["return . . ."], "references": ["get all data contained in hashed category hashroot as dict"], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 5813, "code": "def keys ( self , globpat = None ) : if globpat is None : files = self . root . walkfiles ( ) else : files = [ Path ( p ) for p in glob . glob ( self . root / globpat ) ] return [ self . normalized ( p ) for p in files if p . isfile ( ) ]", "predictions": ["return list of all files in this collection ."], "references": ["all keys in db or all keys matching a glob"], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 5814, "code": "def allow ( self , record ) : if not self : return True return self . allow ( record ) and not self . deny ( record )", "predictions": ["task must be called after a the the the the the the the the the the the the the the the the the the the the the the the the the"], "references": ["returns whether this record should be printed"], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 5815, "code": "def options ( self , parser , env ) : parser . add option ( \"--nologcapture\" , action = \"store false\" , default = not env . get ( self . env opt ) , dest = \"logcapture\" , help = \"Disable logging capture plugin. \" \"Logging configurtion will be left intact.\" \" [NOSE NOLOGCAPTURE]\" ) parser . add option ( \"--logging-format\" , action = \"store\" , dest = \"logcapture format\" , default = env . get ( 'NOSE LOGFORMAT' ) or self . logformat , metavar = \"FORMAT\" , help = \"Specify custom format to print statements. \" \"Uses the same format as used by standard logging handlers.\" \" [NOSE LOGFORMAT]\" ) parser . add option ( \"--logging-datefmt\" , action = \"store\" , dest = \"logcapture datefmt\" , default = env . get ( 'NOSE LOGDATEFMT' ) or self . logdatefmt , metavar = \"FORMAT\" , help = \"Specify custom date/time format to print statements. \" \"Uses the same format as used by standard logging handlers.\" \" [NOSE LOGDATEFMT]\" ) parser . add option ( \"--logging-filter\" , action = \"store\" , dest = \"logcapture filters\" , default = env . get ( 'NOSE LOGFILTER' ) , metavar = \"FILTER\" , help = \"Specify which statements to filter in/out. \" \"By default, everything is captured. If the output is too\" \" verbose,\\nuse this option to filter out needless output.\\n\" \"Example: filter=foo will capture statements issued ONLY to\\n\" \" foo or foo.what.ever.sub but not foobar or other logger.\\n\" \"Specify multiple loggers with comma: filter=foo,bar,baz.\\n\" \"If any logger name is prefixed with a minus, eg filter=-foo,\\n\" \"it will be excluded rather than included. Default: \" \"exclude logging messages from nose itself (-nose).\" \" [NOSE LOGFILTER]\\n\" ) parser . add option ( \"--logging-clear-handlers\" , action = \"store true\" , default = False , dest = \"logcapture clear\" , help = \"Clear all other logging handlers\" ) parser . add option ( \"--logging-level\" , action = \"store\" , default = 'NOTSET' , dest = \"logcapture level\" , help = \"Set the log level to capture\" )", "predictions": ["add for command line options ."], "references": ["register commandline options ."], "bleu": 0.2907153684841096, "rouge_l": 0.4149659863945578}
{"id": 5816, "code": "def format Error ( self , test , err ) : test . captured Logging = records = self . format Log Records ( ) if not records : return err ec , ev , tb = err return ( ec , self . add Capture To Err ( ev , records ) , tb )", "predictions": ["formats the error message for printing ."], "references": ["add captured log messages to error output ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 5817, "code": "def get new csv writers ( trans title , meta title , trans csv path , meta csv path ) : trans writer = Unicode Writer ( trans csv path ) trans writer . writerow ( trans title ) meta writer = Unicode Writer ( meta csv path ) meta writer . writerow ( meta title ) return trans writer , meta writer", "predictions": ["build a new object from self dict"], "references": ["prepare new csv writers write title rows and return them ."], "bleu": 0.10489671869455933, "rouge_l": 0.10683012259194395}
{"id": 5818, "code": "def subscribe user ( self , user ) : url = self . root url + \"subscribe user\" values = { } values [ \"username\" ] = user return self . query ( url , values )", "predictions": ["run a asap asap asap"], "references": ["method to subscribe a user to a service"], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 5819, "code": "def init parser ( ) : usage = parser = Option Parser ( usage , version = \"%prog \" + notifo . version ) parser . add option ( \"-u\" , \"--user\" , action = \"store\" , dest = \"user\" , help = \"your notifo username\" ) parser . add option ( \"-s\" , \"--secret\" , action = \"store\" , dest = \"secret\" , help = \"your notifo API secret\" ) parser . add option ( \"-n\" , \"--name\" , action = \"store\" , dest = \"name\" , help = \"recipient for the notification\" ) parser . add option ( \"-l\" , \"--label\" , action = \"store\" , dest = \"label\" , help = \"label for the notification\" ) parser . add option ( \"-t\" , \"--title\" , action = \"store\" , dest = \"title\" , help = \"title of the notification\" ) parser . add option ( \"-c\" , \"--callback\" , action = \"store\" , dest = \"callback\" , help = \"callback URL to call\" ) parser . add option ( \"-m\" , \"--message\" , action = \"store true\" , dest = \"message\" , default = False , help = \"send message instead of notification\" ) ( options , args ) = parser . parse args ( ) return ( parser , options , args )", "predictions": ["initialize the iterations iterations iterations userdata"], "references": ["function to init option parser"], "bleu": 0.18575057999133598, "rouge_l": 0.0}
{"id": 5820, "code": "def make code from py ( filename ) : try : source file = open source ( filename ) except IO Error : raise No Source ( \"No file to run: %r\" % filename ) try : source = source file . read ( ) finally : source file . close ( ) if not source or source [ - 1 ] != '\\n' : source += '\\n' code = compile ( source , filename , \"exec\" ) return code", "predictions": ["return a once object from a python file ."], "references": ["get source from filename and make a code object of it ."], "bleu": 0.12716571564598603, "rouge_l": 0.2785388127853881}
{"id": 5821, "code": "def make code from pyc ( filename ) : try : fpyc = open ( filename , \"rb\" ) except IO Error : raise No Code ( \"No file to run: %r\" % filename ) try : magic = fpyc . read ( 4 ) if magic != imp . get magic ( ) : raise No Code ( \"Bad magic number in .pyc file\" ) fpyc . read ( 4 ) if sys . version info >= ( 3 , 3 ) : fpyc . read ( 4 ) code = marshal . load ( fpyc ) finally : fpyc . close ( ) return code", "predictions": ["bind a kernel kernel to a file ."], "references": ["get a code object from a . pyc file ."], "bleu": 0.1867587389639562, "rouge_l": 0.43571428571428567}
{"id": 5822, "code": "def html tableify ( item matrix , select = None , header = None , footer = None ) : if not item matrix : return '' html cols = [ ] tds = lambda text : u'<td>' + text + u'  </td>' trs = lambda text : u'<tr>' + text + u'</tr>' tds items = [ map ( tds , row ) for row in item matrix ] if select : row , col = select tds items [ row ] [ col ] = u'<td class=\"inverted\">' + item matrix [ row ] [ col ] + u'  </td>' #select the right item html cols = map ( trs , ( u'' . join ( row ) for row in tds items ) ) head = '' foot = '' if header : head = ( u'<tr>' + '' . join ( ( u'<td>' + header + u'</td>' ) * len ( item matrix [ 0 ] ) ) + '</tr>' ) if footer : foot = ( u'<tr>' + '' . join ( ( u'<td>' + footer + u'</td>' ) * len ( item matrix [ 0 ] ) ) + '</tr>' ) html = ( u'<table class=\"completion\" style=\"white-space:pre\">' + head + ( u'' . join ( html cols ) ) + foot + u'</table>' ) return html", "predictions": ["generate pid string from pid matrix"], "references": ["returnr a string for an html table"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5823, "code": "def current ( self , value ) : current = min ( max ( self . min , value ) , self . max ) self . current = current if current > self . stop : self . stop = current self . start = current - self . width elif current < self . start : self . start = current self . stop = current + self . width if abs ( self . start - self . min ) <= self . sticky lenght : self . start = self . min if abs ( self . stop - self . max ) <= self . sticky lenght : self . stop = self . max", "predictions": ["set new path to display in the get"], "references": ["set current cursor position"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 5824, "code": "def update list ( self , hilight = True ) : self . sliding interval . current = self . index [ 0 ] head = None foot = None if self . sliding interval . start > 0 : head = '...' if self . sliding interval . stop < self . sliding interval . max : foot = '...' items m = self . justified items [ self . sliding interval . start : self . sliding interval . stop + 1 ] self . console widget . clear temporary buffer ( ) if ( hilight ) : sel = ( self . sliding interval . nth , self . index [ 1 ] ) else : sel = None strng = html tableify ( items m , select = sel , header = head , footer = foot ) self . console widget . fill temporary buffer ( self . old cursor , strng , html = True )", "predictions": ["update the temporary list of items in the console"], "references": ["update the list of completion and hilight the currently selected completion"], "bleu": 0.1957494756053795, "rouge_l": 0.4911433172302737}
{"id": 5825, "code": "def complete current ( self ) : i = self . index item = self . items [ i [ 0 ] ] [ i [ 1 ] ] item = item . strip ( ) if item : self . current text cursor ( ) . insert Text ( item ) self . cancel completion ( )", "predictions": ["process current item ."], "references": ["perform the completion with the currently selected item ."], "bleu": 0.1354797537848421, "rouge_l": 0.28773584905660377}
{"id": 5826, "code": "def wordfreq ( text , is filename = False ) : if is filename : with open ( text ) as f : text = f . read ( ) freqs = { } for word in text . split ( ) : lword = word . lower ( ) freqs [ lword ] = freqs . get ( lword , 0 ) + 1 return freqs", "predictions": ["return the text of the text file ."], "references": ["return a dictionary of words and word counts in a string ."], "bleu": 0.11567041937737582, "rouge_l": 0.28955696202531644}
{"id": 5827, "code": "def print wordfreq ( freqs , n = 10 ) : words , counts = freqs . keys ( ) , freqs . values ( ) items = zip ( counts , words ) items . sort ( reverse = True ) for ( count , word ) in items [ : n ] : print ( word , count )", "predictions": ["print the wordfreq words ."], "references": ["print the n most common words and counts in the freqs dict ."], "bleu": 0.08242390832192466, "rouge_l": 0.41146711635750427}
{"id": 5828, "code": "def tostring ( self ) : root = self . as element ( ) indent ( root ) txt = ET . tostring ( root , encoding = \"utf-8\" ) txt = re . sub ( r' [A-Z] ' , '' , txt ) txt = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n' + txt return txt", "predictions": ["return the root element as a string ."], "references": ["return the string representation of the job description xml ."], "bleu": 0.1867587389639562, "rouge_l": 0.43571428571428567}
{"id": 5829, "code": "def write ( self , filename ) : txt = self . tostring ( ) with open ( filename , 'w' ) as f : f . write ( txt )", "predictions": ["write the configuration to a file"], "references": ["write the xml job description to a file ."], "bleu": 0.29654680334613515, "rouge_l": 0.6434599156118143}
{"id": 5830, "code": "def begin ( self , total : int , name = None , message = None ) : self . total = total message = message or name or \"Working...\" self . name = name or \"Progress Monitor\" self . update ( 0 , message )", "predictions": ["start a total message ."], "references": ["call before starting work on a monitor specifying name and amount of work"], "bleu": 0.05512018958855254, "rouge_l": 0.10286677908937607}
{"id": 5831, "code": "def task ( self , total : int , name = None , message = None ) : self . begin ( total , name , message ) try : yield self finally : self . done ( )", "predictions": ["context manager for a task ."], "references": ["wrap code into a begin and end call on this monitor"], "bleu": 0.09600096733558854, "rouge_l": 0.1117216117216117}
{"id": 5832, "code": "def subtask ( self , units : int ) : sm = self . submonitor ( units ) try : yield sm finally : if sm . total is None : self . update ( units ) else : sm . done ( )", "predictions": ["context manager for units ."], "references": ["create a submonitor with the given units"], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 5833, "code": "def update ( self , units : int = 1 , message : str = None ) : if self . total is None : raise Exception ( \"Cannot call progressmonitor.update before calling begin\" ) self . worked = min ( self . total , self . worked + units ) if message : self . message = message for listener in self . listeners : listener ( self )", "predictions": ["update the set of units with new units ."], "references": ["increment the monitor with n units worked and an optional message"], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 5834, "code": "def load config ( self ) : self . clear ( ) try : self . find file ( ) except IO Error as e : raise Config File Not Found ( str ( e ) ) self . read file as dict ( ) self . convert to config ( ) return self . config", "predictions": ["load the config from the config file"], "references": ["load the config from a file and return it as a struct ."], "bleu": 0.22962062226376143, "rouge_l": 0.474339035769829}
{"id": 5835, "code": "def read file as dict ( self ) : def load subconfig ( fname , profile = None ) : from I Python . core . profiledir import Profile Dir , Profile Dir Error if profile is not None : try : profile dir = Profile Dir . find profile dir by name ( get ipython dir ( ) , profile , ) except Profile Dir Error : return path = profile dir . location else : path = self . path loader = Py File Config Loader ( fname , path ) try : sub config = loader . load config ( ) except Config File Not Found : pass else : self . config . merge ( sub config ) def get config ( ) : return self . config namespace = dict ( load subconfig = load subconfig , get config = get config ) fs encoding = sys . getfilesystemencoding ( ) or 'ascii' conf filename = self . full filename . encode ( fs encoding ) py3compat . execfile ( conf filename , namespace )", "predictions": ["read the configuration from the config file"], "references": ["load the config file into self . config with recursive loading ."], "bleu": 0.1692447266569478, "rouge_l": 0.30148270181219106}
{"id": 5836, "code": "def load flag ( self , cfg ) : if isinstance ( cfg , ( dict , Config ) ) : for sec , c in cfg . iteritems ( ) : self . config [ sec ] . update ( c ) else : raise Type Error ( \"Invalid flag: %r\" % cfg )", "predictions": ["load the flag: from the config file ."], "references": ["update self . config from a flag which can be a dict or config"], "bleu": 0.09008421318929809, "rouge_l": 0.1732954545454545}
{"id": 5837, "code": "def decode argv ( self , argv , enc = None ) : uargv = [ ] if enc is None : enc = DEFAULT ENCODING for arg in argv : if not isinstance ( arg , unicode ) : arg = arg . decode ( enc ) uargv . append ( arg ) return uargv", "predictions": ["decode a string into a list of strings ."], "references": ["decode argv if bytes using stin . encoding falling back on default enc"], "bleu": 0.10015045110931886, "rouge_l": 0.17604617604617603}
{"id": 5838, "code": "def interrupt then kill ( self , delay = 2.0 ) : try : self . signal ( SIGINT ) except Exception : self . log . debug ( \"interrupt failed\" ) pass self . killer = ioloop . Delayed Callback ( lambda : self . signal ( SIGKILL ) , delay * 1000 , self . loop ) self . killer . start ( )", "predictions": ["interrupt a specific signal"], "references": ["send int wait a delay and then send kill ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 5839, "code": "def start ( self , n ) : dlist = [ ] for i in range ( n ) : if i > 0 : time . sleep ( self . delay ) el = self . launcher class ( work dir = self . work dir , config = self . config , log = self . log , profile dir = self . profile dir , cluster id = self . cluster id , ) el . engine cmd = copy . deepcopy ( self . engine cmd ) el . engine args = copy . deepcopy ( self . engine args ) el . on stop ( self . notice engine stopped ) d = el . start ( ) self . launchers [ i ] = el dlist . append ( d ) self . notify start ( dlist ) return dlist", "predictions": ["start the notice ."], "references": ["start n engines by profile or profile_dir ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 5840, "code": "def find args ( self ) : return self . mpi cmd + [ '-n' , str ( self . n ) ] + self . mpi args + self . program + self . program args", "predictions": ["find any args that are not installed ."], "references": ["build self . args using all the fields ."], "bleu": 0.15662030188557857, "rouge_l": 0.232824427480916}
{"id": 5841, "code": "def start ( self , n ) : self . n = n return super ( MPI Launcher , self ) . start ( )", "predictions": ["start the timer ."], "references": ["start n instances of the program using mpiexec ."], "bleu": 0.12241977696855179, "rouge_l": 0.43160377358490565}
{"id": 5842, "code": "def start ( self , n ) : self . n = n return super ( MPI Engine Set Launcher , self ) . start ( n )", "predictions": ["start the timer ."], "references": ["start n engines by profile or profile_dir ."], "bleu": 0.14628187563941414, "rouge_l": 0.31443298969072164}
{"id": 5843, "code": "def send file ( self , local , remote ) : remote = \"%s:%s\" % ( self . location , remote ) for i in range ( 10 ) : if not os . path . exists ( local ) : self . log . debug ( \"waiting for %s\" % local ) time . sleep ( 1 ) else : break self . log . info ( \"sending %s to %s\" , local , remote ) check output ( self . scp cmd + [ local , remote ] )", "predictions": ["send a file to the remote host ."], "references": ["send a single file"], "bleu": 0.22679164443904004, "rouge_l": 0.5319767441860466}
{"id": 5844, "code": "def fetch file ( self , remote , local ) : full remote = \"%s:%s\" % ( self . location , remote ) self . log . info ( \"fetching %s from %s\" , local , full remote ) for i in range ( 10 ) : check = check output ( self . ssh cmd + self . ssh args + [ self . location , 'test -e' , remote , \"&& echo 'yes' || echo 'no'\" ] ) check = check . strip ( ) if check == 'no' : time . sleep ( 1 ) elif check == 'yes' : break check output ( self . scp cmd + [ full remote , local ] )", "predictions": ["fetch a file from local to local"], "references": ["fetch a single file"], "bleu": 0.2626909894424158, "rouge_l": 0.5736677115987461}
{"id": 5845, "code": "def engine count ( self ) : count = 0 for n in self . engines . itervalues ( ) : if isinstance ( n , ( tuple , list ) ) : n , args = n count += n return count", "predictions": ["return the number of engines in this query ."], "references": ["determine engine count from engines dict"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 5846, "code": "def start ( self , n ) : self . write job file ( n ) args = [ 'submit' , '/jobfile:%s' % self . job file , '/scheduler:%s' % self . scheduler ] self . log . debug ( \"Starting Win HPC Job: %s\" % ( self . job cmd + ' ' + ' ' . join ( args ) , ) ) output = check output ( [ self . job cmd ] + args , env = os . environ , cwd = self . work dir , stderr = STDOUT ) job id = self . parse job id ( output ) self . notify start ( job id ) return job id", "predictions": ["start the job id ."], "references": ["start n copies of the process using the win hpc job scheduler ."], "bleu": 0.06930996903910726, "rouge_l": 0.41146711635750427}
{"id": 5847, "code": "def parse job id ( self , output ) : m = self . job id regexp . search ( output ) if m is not None : job id = m . group ( ) else : raise Launcher Error ( \"Job id couldn't be determined: %s\" % output ) self . job id = job id self . log . info ( 'Job submitted with job id: %r' , job id ) return job id", "predictions": ["parse a job id ."], "references": ["take the output of the submit command and return the job id ."], "bleu": 0.10259023253147191, "rouge_l": 0.3086003372681282}
{"id": 5848, "code": "def write batch script ( self , n ) : self . n = n if self . batch template file and not self . batch template : with open ( self . batch template file ) as f : self . batch template = f . read ( ) if not self . batch template : self . batch template = self . default template if not self . job array regexp . search ( self . batch template ) : self . log . debug ( \"adding job array settings to batch script\" ) firstline , rest = self . batch template . split ( '\\n' , 1 ) self . batch template = u'\\n' . join ( [ firstline , self . job array template , rest ] ) if self . queue and not self . queue regexp . search ( self . batch template ) : self . log . debug ( \"adding PBS queue settings to batch script\" ) firstline , rest = self . batch template . split ( '\\n' , 1 ) self . batch template = u'\\n' . join ( [ firstline , self . queue template , rest ] ) script as string = self . formatter . format ( self . batch template , * * self . context ) self . log . debug ( 'Writing batch script: %s' , self . batch file ) with open ( self . batch file , 'w' ) as f : f . write ( script as string ) os . chmod ( self . batch file , stat . S IRUSR | stat . S IWUSR | stat . S IXUSR )", "predictions": ["write the batch script to the queue ."], "references": ["instantiate and write the batch script to the work_dir ."], "bleu": 0.5845864450928401, "rouge_l": 0.7624999999999998}
{"id": 5849, "code": "def start ( self , n ) : self . log . debug ( \"Starting %s: %r\" , self . class . name , self . args ) self . write batch script ( n ) output = check output ( self . args , env = os . environ ) job id = self . parse job id ( output ) self . notify start ( job id ) return job id", "predictions": ["start the job ."], "references": ["start n copies of the process using a batch system ."], "bleu": 0.07425134808660917, "rouge_l": 0.3689516129032258}
{"id": 5850, "code": "def context menu make ( self , pos ) : format = self . control . cursor For Position ( pos ) . char Format ( ) name = format . string Property ( Qt Gui . Q Text Format . Image Name ) if name : menu = Qt Gui . Q Menu ( ) menu . add Action ( 'Copy Image' , lambda : self . copy image ( name ) ) menu . add Action ( 'Save Image As...' , lambda : self . save image ( name ) ) menu . add Separator ( ) svg = self . name to svg map . get ( name , None ) if svg is not None : menu . add Separator ( ) menu . add Action ( 'Copy SVG' , lambda : svg to clipboard ( svg ) ) menu . add Action ( 'Save SVG As...' , lambda : save svg ( svg , self . control ) ) else : menu = super ( Rich I Python Widget , self ) . context menu make ( pos ) return menu", "predictions": ["create a context menu for this context ."], "references": ["reimplemented to return a custom context menu for images ."], "bleu": 0.2572506957482676, "rouge_l": 0.5446428571428571}
{"id": 5851, "code": "def handle pyout ( self , msg ) : if not self . hidden and self . is from this session ( msg ) : content = msg [ 'content' ] prompt number = content . get ( 'execution count' , 0 ) data = content [ 'data' ] if data . has key ( 'image/svg+xml' ) : self . pre image append ( msg , prompt number ) self . append svg ( data [ 'image/svg+xml' ] , True ) self . append html ( self . output sep2 , True ) elif data . has key ( 'image/png' ) : self . pre image append ( msg , prompt number ) self . append png ( decodestring ( data [ 'image/png' ] . encode ( 'ascii' ) ) , True ) self . append html ( self . output sep2 , True ) elif data . has key ( 'image/jpeg' ) and self . jpg supported : self . pre image append ( msg , prompt number ) self . append jpg ( decodestring ( data [ 'image/jpeg' ] . encode ( 'ascii' ) ) , True ) self . append html ( self . output sep2 , True ) else : return super ( Rich I Python Widget , self ) . handle pyout ( msg )", "predictions": ["handle pyout changes ."], "references": ["overridden to handle rich data types like svg ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 5852, "code": "def handle display data ( self , msg ) : if not self . hidden and self . is from this session ( msg ) : source = msg [ 'content' ] [ 'source' ] data = msg [ 'content' ] [ 'data' ] metadata = msg [ 'content' ] [ 'metadata' ] if data . has key ( 'image/svg+xml' ) : self . log . debug ( \"display: %s\" , msg . get ( 'content' , '' ) ) svg = data [ 'image/svg+xml' ] self . append svg ( svg , True ) elif data . has key ( 'image/png' ) : self . log . debug ( \"display: %s\" , msg . get ( 'content' , '' ) ) png = decodestring ( data [ 'image/png' ] . encode ( 'ascii' ) ) self . append png ( png , True ) elif data . has key ( 'image/jpeg' ) and self . jpg supported : self . log . debug ( \"display: %s\" , msg . get ( 'content' , '' ) ) jpg = decodestring ( data [ 'image/jpeg' ] . encode ( 'ascii' ) ) self . append jpg ( jpg , True ) else : return super ( Rich I Python Widget , self ) . handle display data ( msg )", "predictions": ["handle svg data from svg ."], "references": ["overridden to handle rich data types like svg ."], "bleu": 0.20034704329441452, "rouge_l": 0.5147679324894514}
{"id": 5853, "code": "def append jpg ( self , jpg , before prompt = False ) : self . append custom ( self . insert jpg , jpg , before prompt )", "predictions": ["append a new jpg to the end of the list of jpg ."], "references": ["append raw jpg data to the widget ."], "bleu": 0.14949751774990683, "rouge_l": 0.4975530179445351}
{"id": 5854, "code": "def append png ( self , png , before prompt = False ) : self . append custom ( self . insert png , png , before prompt )", "predictions": ["append a png to the png ."], "references": ["append raw png data to the widget ."], "bleu": 0.25201472805660513, "rouge_l": 0.6587473002159828}
{"id": 5855, "code": "def append svg ( self , svg , before prompt = False ) : self . append custom ( self . insert svg , svg , before prompt )", "predictions": ["append an svg to the list ."], "references": ["append raw svg data to the widget ."], "bleu": 0.25201472805660513, "rouge_l": 0.6587473002159828}
{"id": 5856, "code": "def copy image ( self , name ) : image = self . get image ( name ) Qt Gui . Q Application . clipboard ( ) . set Image ( image )", "predictions": ["update an list of list"], "references": ["copies the imageresource with name to the clipboard ."], "bleu": 0.1031546451143688, "rouge_l": 0.0}
{"id": 5857, "code": "def get image ( self , name ) : document = self . control . document ( ) image = document . resource ( Qt Gui . Q Text Document . Image Resource , Qt Core . Q Url ( name ) ) return image", "predictions": ["cancel an current current current current current current current current current current current current current current current current current current current current current current ."], "references": ["returns the qimage stored as the imageresource with name ."], "bleu": 0.048589719316429775, "rouge_l": 0.06192893401015229}
{"id": 5858, "code": "def insert img ( self , cursor , img , fmt ) : try : image = Qt Gui . Q Image ( ) image . load From Data ( img , fmt . upper ( ) ) except Value Error : self . insert plain text ( cursor , 'Received invalid %s data.' % fmt ) else : format = self . add image ( image ) cursor . insert Block ( ) cursor . insert Image ( format ) cursor . insert Block ( )", "predictions": ["inserts an if the given if it is in the filename"], "references": ["insert a raw image jpg or png"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 5859, "code": "def insert svg ( self , cursor , svg ) : try : image = svg to image ( svg ) except Value Error : self . insert plain text ( cursor , 'Received invalid SVG data.' ) else : format = self . add image ( image ) self . name to svg map [ format . name ( ) ] = svg cursor . insert Block ( ) cursor . insert Image ( format ) cursor . insert Block ( )", "predictions": ["print an svg to svg items"], "references": ["insert raw svg data into the widet ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 5860, "code": "def save image ( self , name , format = 'PNG' ) : dialog = Qt Gui . Q File Dialog ( self . control , 'Save Image' ) dialog . set Accept Mode ( Qt Gui . Q File Dialog . Accept Save ) dialog . set Default Suffix ( format . lower ( ) ) dialog . set Name Filter ( '%s file (*.%s)' % ( format , format . lower ( ) ) ) if dialog . exec ( ) : filename = dialog . selected Files ( ) [ 0 ] image = self . get image ( name ) image . save ( filename , format )", "predictions": ["save the current image txt txt file"], "references": ["shows a save dialog for the imageresource with name ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 5861, "code": "def exit now changed ( self , name , old , new ) : if new : loop = ioloop . IO Loop . instance ( ) loop . add timeout ( time . time ( ) + 0.1 , loop . stop )", "predictions": ["add only one of the instance f f f f f f f f f f f f f f f f f f f f f f f f f"], "references": ["stop eventloop when exit_now fires"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5862, "code": "def init environment ( self ) : env = os . environ env [ 'TERM' ] = 'xterm-color' env [ 'CLICOLOR' ] = '1' env [ 'PAGER' ] = 'cat' env [ 'GIT PAGER' ] = 'cat' install payload page ( )", "predictions": ["setup the environment environment"], "references": ["configure the user s environment ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 5863, "code": "def ask exit ( self ) : self . exit now = True payload = dict ( source = 'I Python.zmq.zmqshell.ZMQ Interactive Shell.ask exit' , exit = True , keepkernel = self . keepkernel on exit , ) self . payload manager . write payload ( payload )", "predictions": ["write the exit and exit the payload"], "references": ["engage the exit actions ."], "bleu": 0.24446151121745047, "rouge_l": 0.34366197183098596}
{"id": 5864, "code": "def read ( self , filename ) : kwargs = { } if sys . version info >= ( 3 , 2 ) : kwargs [ 'encoding' ] = \"utf-8\" return configparser . Raw Config Parser . read ( self , filename , * * kwargs )", "predictions": ["subtask the file yield from the file yield the file yield the original original file yield"], "references": ["read a filename as utf - 8 configuration data ."], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 5865, "code": "def from environment ( self , env var ) : env = os . environ . get ( env var , '' ) if env : self . timid = ( '--timid' in env )", "predictions": ["run the is update from the environment environment ."], "references": ["read configuration from the env_var environment variable ."], "bleu": 0.21105340631872635, "rouge_l": 0.4756335282651072}
{"id": 5866, "code": "def from args ( self , * * kwargs ) : for k , v in iitems ( kwargs ) : if v is not None : if k in self . MUST BE LIST and isinstance ( v , string class ) : v = [ v ] setattr ( self , k , v )", "predictions": ["load instance from config ."], "references": ["read config values from kwargs ."], "bleu": 0.2658156069371863, "rouge_l": 0.3577712609970674}
{"id": 5867, "code": "def set attr from config option ( self , cp , attr , where , type = '' ) : section , option = where . split ( \":\" ) if cp . has option ( section , option ) : method = getattr ( cp , 'get' + type ) setattr ( self , attr , method ( section , option ) )", "predictions": ["read the configuration as an option"], "references": ["set an attribute on self if it exists in the configparser ."], "bleu": 0.08993236413460196, "rouge_l": 0.10481099656357389}
{"id": 5868, "code": "def delims ( self , delims ) : expr = '[' + '' . join ( '\\\\' + c for c in delims ) + ']' self . delim re = re . compile ( expr ) self . delims = delims self . delim expr = expr", "predictions": ["determine the expression without any load load load load load ."], "references": ["set the delimiters for line splitting ."], "bleu": 0.12605968092174913, "rouge_l": 0.2314990512333966}
{"id": 5869, "code": "def split line ( self , line , cursor pos = None ) : l = line if cursor pos is None else line [ : cursor pos ] return self . delim re . split ( l ) [ - 1 ]", "predictions": ["decode a argv argv arg arg arg arg arg arg arg"], "references": ["split a line of text with a cursor at the given position ."], "bleu": 0.09497094417933137, "rouge_l": 0.08209959623149395}
{"id": 5870, "code": "def greedy changed ( self , name , old , new ) : if new : self . splitter . delims = GREEDY DELIMS else : self . splitter . delims = DELIMS if self . readline : self . readline . set completer delims ( self . splitter . delims )", "predictions": ["changes the value of a node try to keep track of the node"], "references": ["update the splitter and readline delims when greedy is changed"], "bleu": 0.09552040806823771, "rouge_l": 0.08905109489051095}
{"id": 5871, "code": "def alias matches ( self , text ) : main text = self . text until cursor . lstrip ( ) if ' ' in main text and not main text . startswith ( 'sudo' ) : return [ ] text = os . path . expanduser ( text ) aliases = self . alias table . keys ( ) if text == '' : return aliases else : return [ a for a in aliases if a . startswith ( text ) ]", "predictions": ["delay for config . . . ."], "references": ["match internal system aliases"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5872, "code": "def python matches ( self , text ) : if \".\" in text : try : matches = self . attr matches ( text ) if text . endswith ( '.' ) and self . omit names : if self . omit names == 1 : no name = ( lambda txt : re . match ( r'.*\\. .*? ' , txt ) is None ) else : no name = ( lambda txt : re . match ( r'.*\\. .*?' , txt ) is None ) matches = filter ( no name , matches ) except Name Error : matches = [ ] else : matches = self . global matches ( text ) return matches", "predictions": ["return args match the text . . . . . ."], "references": ["match attributes or global python names"], "bleu": 0.11390778025531027, "rouge_l": 0.12423625254582485}
{"id": 5873, "code": "def match one ( self , rec , tests ) : for key , test in tests . iteritems ( ) : if not test ( rec . get ( key , None ) ) : return False return True", "predictions": ["start one of tests"], "references": ["check if a specific record matches tests ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 5874, "code": "def match ( self , check ) : matches = [ ] tests = { } for k , v in check . iteritems ( ) : if isinstance ( v , dict ) : tests [ k ] = Composite Filter ( v ) else : tests [ k ] = lambda o : o == v for rec in self . records . itervalues ( ) : if self . match one ( rec , tests ) : matches . append ( copy ( rec ) ) return matches", "predictions": ["return a n - match n - match n - 16 - value pairs . ."], "references": ["find all the matches for a check dict ."], "bleu": 0.08513012360883544, "rouge_l": 0.16850828729281767}
{"id": 5875, "code": "def extract subdict ( self , rec , keys ) : d = { } d [ 'msg id' ] = rec [ 'msg id' ] for key in keys : d [ key ] = rec [ key ] return copy ( d )", "predictions": ["send file remote keys to a dictionary"], "references": ["extract subdict of keys"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 5876, "code": "def add record ( self , msg id , rec ) : if self . records . has key ( msg id ) : raise Key Error ( \"Already have msg id %r\" % ( msg id ) ) self . records [ msg id ] = rec", "predictions": ["fetch a file local file local"], "references": ["add a new task record by msg_id ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 5877, "code": "def get record ( self , msg id ) : if not msg id in self . records : raise Key Error ( \"No such msg id %r\" % ( msg id ) ) return copy ( self . records [ msg id ] )", "predictions": ["returns a copy object ."], "references": ["get a specific task record by msg_id ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 5878, "code": "def drop matching records ( self , check ) : matches = self . match ( check ) for m in matches : del self . records [ m [ 'msg id' ] ]", "predictions": ["start self args args"], "references": ["remove a record from the db ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 5879, "code": "def get history ( self ) : msg ids = self . records . keys ( ) return sorted ( msg ids , key = lambda m : self . records [ m ] [ 'submitted' ] )", "predictions": ["parse the job ids"], "references": ["get all msg_ids ordered by time submitted ."], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 5880, "code": "def quiet ( self ) : try : cell = self . shell . history manager . input hist parsed [ self . prompt count ] if cell . rstrip ( ) . endswith ( ';' ) : return True except Index Error : pass return False", "predictions": ["returns true if the n n n n"], "references": ["should we silence the display hook because of ; ?"], "bleu": 0.12489309605176803, "rouge_l": 0.10892857142857142}
{"id": 5881, "code": "def update user ns ( self , result ) : if result is not self . shell . user ns [ ' oh' ] : if len ( self . shell . user ns [ ' oh' ] ) >= self . cache size and self . do full cache : warn ( 'Output cache limit (currently ' + `self.cache size` + ' entries) hit.\\n' 'Flushing cache and resetting history counter...\\n' 'The only history variables available will be  , ,  and  1\\n' 'with the current result.' ) self . flush ( ) if ' ' not in builtin . dict : self . = self . self . = self . self . = result self . shell . push ( { ' ' : self . , ' ' : self . , ' ' : self . } , interactive = False ) to main = { } if self . do full cache : new result = ' ' + `self.prompt count` to main [ new result ] = result self . shell . push ( to main , interactive = False ) self . shell . user ns [ ' oh' ] [ self . prompt count ] = result", "predictions": ["start the user of the class"], "references": ["update user_ns with various things like _ __ _1 etc ."], "bleu": 0.08072686929338534, "rouge_l": 0.0}
{"id": 5882, "code": "def log output ( self , format dict ) : if self . shell . logger . log output : self . shell . logger . log write ( format dict [ 'text/plain' ] , 'output' ) self . shell . history manager . output hist reprs [ self . prompt count ] = format dict [ 'text/plain' ]", "predictions": ["context manager to the . ."], "references": ["log the output ."], "bleu": 0.24446151121745047, "rouge_l": 0.4149659863945578}
{"id": 5883, "code": "def finish displayhook ( self ) : io . stdout . write ( self . shell . separate out2 ) io . stdout . flush ( )", "predictions": ["handle the display . . . . . . . . . . . . ."], "references": ["finish up all displayhook activities ."], "bleu": 0.07692375026049747, "rouge_l": 0.09902597402597402}
{"id": 5884, "code": "def load ipython extension ( ip ) : global loaded if not loaded : plugin = Store Magic ( shell = ip , config = ip . config ) ip . plugin manager . register plugin ( 'storemagic' , plugin ) loaded = True", "predictions": ["register the display data in display"], "references": ["load the extension in ipython ."], "bleu": 0.24446151121745047, "rouge_l": 0.3333333333333333}
{"id": 5885, "code": "def raise if freezed ( self ) : if self . is freezed : name = type ( self ) . name raise Invalid Operation Exception ( 'obj {name} is freezed.' . format ( name = name ) )", "predictions": ["append the name to the name = false if it is not none = false = false = false = false ."], "references": ["raise invalidoperationexception if is freezed ."], "bleu": 0.06586656967644004, "rouge_l": 0.23890339425587467}
{"id": 5886, "code": "def mysql timestamp converter ( s ) : if s [ 4 ] == '-' : return Date Time or None ( s ) s = s + \"0\" * ( 14 - len ( s ) ) parts = map ( int , filter ( None , ( s [ : 4 ] , s [ 4 : 6 ] , s [ 6 : 8 ] , s [ 8 : 10 ] , s [ 10 : 12 ] , s [ 12 : 14 ] ) ) ) try : return Timestamp ( * parts ) except ( System Exit , Keyboard Interrupt ) : raise except : return None", "predictions": ["convert a append png to a append png ."], "references": ["convert a mysql timestamp to a timestamp object ."], "bleu": 0.24446151121745052, "rouge_l": 0.5555555555555556}
{"id": 5887, "code": "def eventloop changed ( self , name , old , new ) : loop = ioloop . IO Loop . instance ( ) loop . add timeout ( time . time ( ) + 0.1 , self . enter eventloop )", "predictions": ["register a callback for when a append changes insert ."], "references": ["schedule call to eventloop from ioloop"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 5888, "code": "def start ( self ) : self . shell . exit now = False if self . control stream : self . control stream . on recv ( self . dispatch control , copy = False ) def make dispatcher ( stream ) : def dispatcher ( msg ) : return self . dispatch shell ( stream , msg ) return dispatcher for s in self . shell streams : s . on recv ( make dispatcher ( s ) , copy = False )", "predictions": ["start the shell ."], "references": ["register dispatchers for streams"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 5889, "code": "def do one iteration ( self ) : if self . control stream : self . control stream . flush ( ) for stream in self . shell streams : stream . flush ( zmq . POLLIN , 1 ) stream . flush ( zmq . POLLOUT )", "predictions": ["clear the shell iteration ."], "references": ["step eventloop just once"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 5890, "code": "def publish pyin ( self , code , parent , execution count ) : self . session . send ( self . iopub socket , u'pyin' , { u'code' : code , u'execution count' : execution count } , parent = parent , ident = self . topic ( 'pyin' ) )", "predictions": ["publish a pyin code to the server ."], "references": ["publish the code request on the pyin stream ."], "bleu": 0.1862539773562041, "rouge_l": 0.465648854961832}
{"id": 5891, "code": "def abort request ( self , stream , ident , parent ) : msg ids = parent [ 'content' ] . get ( 'msg ids' , None ) if isinstance ( msg ids , basestring ) : msg ids = [ msg ids ] if not msg ids : self . abort queues ( ) for mid in msg ids : self . aborted . add ( str ( mid ) ) content = dict ( status = 'ok' ) reply msg = self . session . send ( stream , 'abort reply' , content = content , parent = parent , ident = ident ) self . log . debug ( \"%s\" , reply msg )", "predictions": ["abort a request to the broker ."], "references": ["abort a specifig msg by id"], "bleu": 0.24446151121745047, "rouge_l": 0.31202046035805625}
{"id": 5892, "code": "def clear request ( self , stream , idents , parent ) : self . shell . reset ( False ) msg = self . session . send ( stream , 'clear reply' , ident = idents , parent = parent , content = dict ( status = 'ok' ) )", "predictions": ["clear the shell request ."], "references": ["clear our namespace ."], "bleu": 0.3021375397356768, "rouge_l": 0.4535315985130111}
{"id": 5893, "code": "def topic ( self , topic ) : if self . int id >= 0 : base = \"engine.%i\" % self . int id else : base = \"kernel.%s\" % self . ident return py3compat . cast bytes ( \"%s.%s\" % ( base , topic ) )", "predictions": ["return the topic for the topic ."], "references": ["prefixed topic for iopub messages"], "bleu": 0.24446151121745047, "rouge_l": 0.34366197183098596}
{"id": 5894, "code": "def at shutdown ( self ) : if self . shutdown message is not None : self . session . send ( self . iopub socket , self . shutdown message , ident = self . topic ( 'shutdown' ) ) self . log . debug ( \"%s\" , self . shutdown message ) [ s . flush ( zmq . POLLOUT ) for s in self . shell streams ]", "predictions": ["shut down the shell ."], "references": ["actions taken at shutdown by the kernel called by python s atexit ."], "bleu": 0.061000517228105004, "rouge_l": 0.20573355817875214}
{"id": 5895, "code": "def init gui pylab ( self ) : shell = self . shell showtraceback = shell . showtraceback try : def print tb ( etype , evalue , stb ) : print ( \"GUI event loop or pylab initialization failed\" , file = io . stderr ) print ( shell . Interactive TB . stb2text ( stb ) , file = io . stderr ) shell . showtraceback = print tb Interactive Shell App . init gui pylab ( self ) finally : shell . showtraceback = showtraceback", "predictions": ["setup gui for gui"], "references": ["enable gui event loop integration taking pylab into account ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 5896, "code": "def before Context ( self ) : mods = sys . modules . copy ( ) self . mod stack . append ( mods )", "predictions": ["before template before creating a new one ."], "references": ["copy sys . modules onto my mod stack"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 5897, "code": "def virtual memory ( ) : total , active , inactive , wired , free = psutil osx . get virtual mem ( ) avail = inactive + free used = active + inactive + wired percent = usage percent ( ( total - avail ) , total , round = 1 ) return nt virtmem info ( total , avail , percent , used , free , active , inactive , wired )", "predictions": ["virtual memory usage for memory usage ."], "references": ["system virtual memory as a namedtuple ."], "bleu": 0.2626909894424158, "rouge_l": 0.42857142857142855}
{"id": 5898, "code": "def get system cpu times ( ) : user , nice , system , idle = psutil osx . get system cpu times ( ) return cputimes ntuple ( user , nice , system , idle )", "predictions": ["returns the cpu cpu times for the given user ."], "references": ["return system cpu times as a namedtuple ."], "bleu": 0.17827531042796255, "rouge_l": 0.34014869888475835}
{"id": 5899, "code": "def get system per cpu times ( ) : ret = [ ] for cpu t in psutil osx . get system per cpu times ( ) : user , nice , system , idle = cpu t item = cputimes ntuple ( user , nice , system , idle ) ret . append ( item ) return ret", "predictions": ["return a list of system per cpu times"], "references": ["return system cpu times as a named tuple"], "bleu": 0.25098621243978964, "rouge_l": 0.5}
{"id": 5900, "code": "def get process cmdline ( self ) : if not pid exists ( self . pid ) : raise No Such Process ( self . pid , self . process name ) return psutil osx . get process cmdline ( self . pid )", "predictions": ["return the process id of the process ."], "references": ["return process cmdline as a list of arguments ."], "bleu": 0.17795502018438056, "rouge_l": 0.465648854961832}
{"id": 5901, "code": "def get memory info ( self ) : rss , vms = psutil osx . get process memory info ( self . pid ) [ : 2 ] return nt meminfo ( rss , vms )", "predictions": ["return the memory info for the current process ."], "references": ["return a tuple with the process rss and vms size ."], "bleu": 0.14211011212459496, "rouge_l": 0.3929146537842191}
{"id": 5902, "code": "def get ext memory info ( self ) : rss , vms , pfaults , pageins = psutil osx . get process memory info ( self . pid ) return self . nt ext mem ( rss , vms , pfaults * PAGESIZE , pageins * PAGESIZE )", "predictions": ["get the memory info for the given process ."], "references": ["return a tuple with the process rss and vms size ."], "bleu": 0.1343994460963362, "rouge_l": 0.2946859903381642}
{"id": 5903, "code": "def get open files ( self ) : if self . pid == 0 : return [ ] files = [ ] rawlist = psutil osx . get process open files ( self . pid ) for path , fd in rawlist : if isfile strict ( path ) : ntuple = nt openfile ( path , fd ) files . append ( ntuple ) return files", "predictions": ["get all open files ."], "references": ["return files opened by process ."], "bleu": 0.24736929544091937, "rouge_l": 0.3577712609970674}
{"id": 5904, "code": "def usage percent ( used , total , round = None ) : try : ret = ( used / total ) * 100 except Zero Division Error : ret = 0 if round is not None : return round ( ret , round ) else : return ret", "predictions": ["return the usage of a total number"], "references": ["calculate percentage usage of used against total ."], "bleu": 0.22772101321113858, "rouge_l": 0.3952483801295896}
{"id": 5905, "code": "def memoize ( f ) : cache = { } def memf ( * x ) : if x not in cache : cache [ x ] = f ( * x ) return cache [ x ] return memf", "predictions": ["simple decorator to cache the first argument of a function f ."], "references": ["a simple memoize decorator for functions ."], "bleu": 0.13065113298388567, "rouge_l": 0.3315217391304348}
{"id": 5906, "code": "def deprecated ( replacement = None ) : def outer ( fun ) : msg = \"psutil.%s is deprecated\" % fun . name if replacement is not None : msg += \"; use %s instead\" % replacement if fun . doc is None : fun . doc = msg @ wraps ( fun ) def inner ( * args , * * kwargs ) : warnings . warn ( msg , category = Deprecation Warning , stacklevel = 2 ) return fun ( * args , * * kwargs ) return inner return outer", "predictions": ["decorator to mark functions as deprecated as deprecated ."], "references": ["a decorator which can be used to mark functions as deprecated ."], "bleu": 0.4405149609594086, "rouge_l": 0.6499238964992391}
{"id": 5907, "code": "def login ( self ) : try : self . gd client = gdata . docs . client . Docs Client ( ) self . gd client . Client Login ( self . email , self . password , self . source ) except Request Error as e : raise PO Docs Error ( e )", "predictions": ["login to the zoneminder client ."], "references": ["login into google docs with user authentication info ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 5908, "code": "def get gdocs key ( self ) : try : args = urlparse . parse qs ( urlparse . urlparse ( self . url ) . query ) self . key = args [ 'key' ] [ 0 ] except Key Error as e : raise PO Docs Error ( e )", "predictions": ["get the gdocs key from the url"], "references": ["parse gdocs key from spreadsheet url ."], "bleu": 0.3655552228545123, "rouge_l": 0.5714285714285714}
{"id": 5909, "code": "def ensure temp path exists ( self ) : try : if not os . path . exists ( self . temp path ) : os . mkdir ( self . temp path ) except OS Error as e : raise PO Docs Error ( e )", "predictions": ["ensure that the temp directory exists ."], "references": ["make sure temp directory exists and create one if it does not ."], "bleu": 0.15513171017484098, "rouge_l": 0.3794712286158632}
{"id": 5910, "code": "def download ( self ) : trans csv path = os . path . realpath ( os . path . join ( self . temp path , GDOCS TRANS CSV ) ) meta csv path = os . path . realpath ( os . path . join ( self . temp path , GDOCS META CSV ) ) self . download csv from gdocs ( trans csv path , meta csv path ) try : csv to po ( trans csv path , meta csv path , self . locale root , self . po files path , header = self . header ) except IO Error as e : raise PO Docs Error ( e ) self . clear temp ( )", "predictions": ["download csv from csv file ."], "references": ["download csv files from gdocs and convert them into po files structure ."], "bleu": 0.10286160177491631, "rouge_l": 0.39482200647249194}
{"id": 5911, "code": "def clear ( self ) : empty file path = os . path . join ( self . temp path , 'empty.csv' ) try : empty file = open ( empty file path , 'w' ) empty file . write ( ',' ) empty file . close ( ) except IO Error as e : raise PO Docs Error ( e ) self . upload file to gdoc ( empty file path , content type = 'text/csv' ) os . remove ( empty file path )", "predictions": ["clear the contents of the file ."], "references": ["clear gdoc spreadsheet by sending empty csv file ."], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 5912, "code": "def new qt console ( self , evt = None ) : return connect qtconsole ( self . ipkernel . connection file , profile = self . ipkernel . profile )", "predictions": ["connect to the ipkernel console ."], "references": ["start a new qtconsole connected to our kernel"], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 5913, "code": "def url has contents ( url , contents , case sensitive = False , timeout = 10 ) : try : req = urllib2 . urlopen ( url , timeout = timeout ) except Exception , : False else : rep = req . read ( ) if ( not case sensitive and rep . lower ( ) . find ( contents . lower ( ) ) >= 0 ) or ( case sensitive and rep . find ( contents ) >= 0 ) : return True else : return False", "predictions": ["check if url has an contents of the case ."], "references": ["check whether the html page contains the content or not and return boolean"], "bleu": 0.10335004586873166, "rouge_l": 0.16991643454039}
{"id": 5914, "code": "def get response code ( url , timeout = 10 ) : try : req = urllib2 . urlopen ( url , timeout = timeout ) except HTTP Error , e : return e . getcode ( ) except Exception , : fail ( \"Couldn't reach the URL '%s'\" % url ) else : return req . getcode ( )", "predictions": ["get the response code from the url"], "references": ["visit the url and return the http response code in int"], "bleu": 0.18168644632623684, "rouge_l": 0.32049036777583184}
{"id": 5915, "code": "def clear output ( self , stdout = True , stderr = True , other = True ) : if stdout : print ( '\\033[2K\\r' , file = io . stdout , end = '' ) io . stdout . flush ( ) if stderr : print ( '\\033[2K\\r' , file = io . stderr , end = '' ) io . stderr . flush ( )", "predictions": ["clear the output of the output ."], "references": ["clear the output of the cell receiving output ."], "bleu": 0.5750348457942689, "rouge_l": 0.8557114228456915}
{"id": 5916, "code": "def source file ( self ) : if os . path . exists ( self . filename ) : return open source ( self . filename ) source = self . file locator . get zip data ( self . filename ) if source is not None : return String IO ( source ) raise Coverage Exception ( \"No source for code '%s'.\" % self . filename )", "predictions": ["return the source file ."], "references": ["return an open file for reading the source of the code unit ."], "bleu": 0.08626775877575975, "rouge_l": 0.41146711635750427}
{"id": 5917, "code": "def total seconds ( td ) : try : return td . total seconds ( ) except Attribute Error : return 1e-6 * ( td . microseconds + ( td . seconds + td . days * 24 * 3600 ) * 10 ** 6 )", "predictions": ["returns the total number of seconds in seconds ."], "references": ["timedelta . total_seconds was added in 2 . 7"], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 5918, "code": "def abort ( self ) : assert not self . ready ( ) , \"Can't abort, I am already done!\" return self . client . abort ( self . msg ids , targets = self . targets , block = True )", "predictions": ["abort the lock ."], "references": ["abort my tasks ."], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 5919, "code": "def elapsed ( self ) : if self . ready ( ) : return self . wall time now = submitted = datetime . now ( ) for msg id in self . msg ids : if msg id in self . client . metadata : stamp = self . client . metadata [ msg id ] [ 'submitted' ] if stamp and stamp < submitted : submitted = stamp return total seconds ( now - submitted )", "predictions": ["elapsed if the message is elapsed ."], "references": ["elapsed time since initial submission"], "bleu": 0.18575057999133598, "rouge_l": 0.17183098591549298}
{"id": 5920, "code": "def wait interactive ( self , interval = 1. , timeout = None ) : N = len ( self ) tic = time . time ( ) while not self . ready ( ) and ( timeout is None or time . time ( ) - tic <= timeout ) : self . wait ( interval ) clear output ( ) print ( \"%4i/%i tasks finished after %4i s\" % ( self . progress , N , self . elapsed ) , end = \"\" ) sys . stdout . flush ( ) print ( ) print ( \"done\" )", "predictions": ["start the interactive dispatch"], "references": ["interactive wait printing progress at regular intervals"], "bleu": 0.1697232447536737, "rouge_l": 0.1732954545454545}
{"id": 5921, "code": "def republish displaypub ( self , content , eid ) : try : ip = get ipython ( ) except Name Error : return md = content [ 'metadata' ] or { } md [ 'engine' ] = eid ip . display pub . publish ( content [ 'source' ] , content [ 'data' ] , md )", "predictions": ["add an stream to the in - memory in the in - memory in the in - memory in - memory ."], "references": ["republish individual displaypub content dicts"], "bleu": 0.04657469807170698, "rouge_l": 0.0}
{"id": 5922, "code": "def wait for outputs ( self , timeout = - 1 ) : if not self . success : return tic = time . time ( ) while not all ( md [ 'outputs ready' ] for md in self . metadata ) : time . sleep ( 0.01 ) self . client . flush iopub ( self . client . iopub socket ) if timeout >= 0 and time . time ( ) > tic + timeout : break", "predictions": ["publish all outputs outputs for"], "references": ["wait for the status = idle message that indicates we have all outputs"], "bleu": 0.07795171967670728, "rouge_l": 0.20573355817875214}
{"id": 5923, "code": "def unordered iter ( self ) : try : rlist = self . get ( 0 ) except error . Timeout Error : pending = set ( self . msg ids ) while pending : try : self . client . wait ( pending , 1e-3 ) except error . Timeout Error : pass ready = pending . difference ( self . client . outstanding ) pending = pending . difference ( ready ) while ready : msg id = ready . pop ( ) ar = Async Result ( self . client , msg id , self . fname ) rlist = ar . get ( ) try : for r in rlist : yield r except Type Error : yield rlist else : for r in rlist : yield r", "predictions": ["iterate over all abort data ."], "references": ["iterator for results * as they arrive * on fcfs basis ignoring submission order ."], "bleu": 0.04928854007377984, "rouge_l": 0.08840579710144927}
{"id": 5924, "code": "def wait ( self , timeout = - 1 ) : start = time . time ( ) if self . ready : return local ids = filter ( lambda msg id : msg id in self . client . outstanding , self . msg ids ) local ready = self . client . wait ( local ids , timeout ) if local ready : remote ids = filter ( lambda msg id : msg id not in self . client . results , self . msg ids ) if not remote ids : self . ready = True else : rdict = self . client . result status ( remote ids , status only = False ) pending = rdict [ 'pending' ] while pending and ( timeout < 0 or time . time ( ) < start + timeout ) : rdict = self . client . result status ( remote ids , status only = False ) pending = rdict [ 'pending' ] if pending : time . sleep ( 0.1 ) if not pending : self . ready = True if self . ready : try : results = map ( self . client . results . get , self . msg ids ) self . result = results if self . single result : r = results [ 0 ] if isinstance ( r , Exception ) : raise r else : results = error . collect exceptions ( results , self . fname ) self . result = self . reconstruct result ( results ) except Exception , e : self . exception = e self . success = False else : self . success = True finally : self . metadata = map ( self . client . metadata . get , self . msg ids )", "predictions": ["clear the session and waits for the status of the status of the status of the status of the status reset reset reset reset reset reset reset reset reset reset reset"], "references": ["wait for result to complete ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 5925, "code": "def abs file ( filename ) : path = os . path . expandvars ( os . path . expanduser ( filename ) ) path = os . path . abspath ( os . path . realpath ( path ) ) path = actual path ( path ) return path", "predictions": ["% a file file"], "references": ["return the absolute normalized form of filename ."], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 5926, "code": "def sep ( s ) : sep match = re . search ( r\"[\\\\/]\" , s ) if sep match : the sep = sep match . group ( 0 ) else : the sep = os . sep return the sep", "predictions": ["convert string to snake_case"], "references": ["find the path separator used in this string or os . sep if none ."], "bleu": 0.022969543400575367, "rouge_l": 0.0953125}
{"id": 5927, "code": "def match ( self , fpath ) : for d in self . dirs : if fpath . startswith ( d ) : if fpath == d : return True if fpath [ len ( d ) ] == os . sep : return True return False", "predictions": ["return whether the argument is a init . ."], "references": ["does fpath indicate a file in one of our trees?"], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 5928, "code": "def match ( self , fpath ) : for pat in self . pats : if fnmatch . fnmatch ( fpath , pat ) : return True return False", "predictions": ["returns true if filter matches the model ."], "references": ["does fpath match one of our filename patterns?"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5929, "code": "def loop qt4 ( kernel ) : from I Python . external . qt for kernel import Qt Core from I Python . lib . guisupport import get app qt4 , start event loop qt4 kernel . app = get app qt4 ( [ \" \" ] ) kernel . app . set Quit On Last Window Closed ( False ) kernel . timer = Qt Core . Q Timer ( ) kernel . timer . timeout . connect ( kernel . do one iteration ) kernel . timer . start ( 1000 * kernel . poll interval ) start event loop qt4 ( kernel . app )", "predictions": ["starts the kernel kernel psutil ."], "references": ["start a kernel with pyqt4 event loop integration ."], "bleu": 0.14827340167306757, "rouge_l": 0.2573839662447257}
{"id": 5930, "code": "def loop wx ( kernel ) : import wx from I Python . lib . guisupport import start event loop wx doi = kernel . do one iteration poll interval = int ( 1000 * kernel . poll interval ) class Timer Frame ( wx . Frame ) : def init ( self , func ) : wx . Frame . init ( self , None , - 1 ) self . timer = wx . Timer ( self ) self . timer . Start ( poll interval ) self . Bind ( wx . EVT TIMER , self . on timer ) self . func = func def on timer ( self , event ) : self . func ( ) class IP Wx App ( wx . App ) : def On Init ( self ) : self . frame = Timer Frame ( doi ) self . frame . Show ( False ) return True kernel . app = IP Wx App ( redirect = False ) import signal if not callable ( signal . getsignal ( signal . SIGINT ) ) : signal . signal ( signal . SIGINT , signal . default int handler ) start event loop wx ( kernel . app )", "predictions": ["get the times of the times of the times get requests"], "references": ["start a kernel with wx event loop support ."], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 5931, "code": "def loop tk ( kernel ) : import Tkinter doi = kernel . do one iteration poll interval = int ( 1000 * kernel . poll interval ) class Timer ( object ) : def init ( self , func ) : self . app = Tkinter . Tk ( ) self . app . withdraw ( ) self . func = func def on timer ( self ) : self . func ( ) self . app . after ( poll interval , self . on timer ) def start ( self ) : self . on timer ( ) self . app . mainloop ( ) kernel . timer = Timer ( doi ) kernel . timer . start ( )", "predictions": ["start is called when the get get is initialized"], "references": ["start a kernel with the tk event loop ."], "bleu": 0.15619699684601276, "rouge_l": 0.2222222222222222}
{"id": 5932, "code": "def loop gtk ( kernel ) : from . gui . gtkembed import GTK Embed gtk kernel = GTK Embed ( kernel ) gtk kernel . start ( )", "predictions": ["start - . the kernel of the kernel"], "references": ["start the kernel coordinating with the gtk event loop"], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 5933, "code": "def enable gui ( gui , kernel = None ) : if gui not in loop map : raise Value Error ( \"GUI %r not supported\" % gui ) if kernel is None : if Application . initialized ( ) : kernel = getattr ( Application . instance ( ) , 'kernel' , None ) if kernel is None : raise Runtime Error ( \"You didn't specify a kernel,\" \" and no I Python Application with a kernel appears to be running.\" ) loop = loop map [ gui ] if kernel . eventloop is not None and kernel . eventloop is not loop : raise Runtime Error ( \"Cannot activate multiple GUI eventloops\" ) kernel . eventloop = loop", "predictions": ["get the memory and activate of the memory"], "references": ["enable integration with a given gui"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5934, "code": "def GOE ( N ) : m = ra . standard normal ( ( N , N ) ) m += m . T return m / 2", "predictions": ["returns the vms of the standard vms"], "references": ["creates an nxn element of the gaussian orthogonal ensemble"], "bleu": 0.18370727471078332, "rouge_l": 0.24448897795591182}
{"id": 5935, "code": "def center eigenvalue diff ( mat ) : N = len ( mat ) evals = np . sort ( la . eigvals ( mat ) ) diff = np . abs ( evals [ N / 2 ] - evals [ N / 2 - 1 ] ) return diff", "predictions": ["get the get difference between the get and the get value"], "references": ["compute the eigvals of mat and then find the center eigval difference ."], "bleu": 0.11941964005964323, "rouge_l": 0.24629878869448185}
{"id": 5936, "code": "def ensemble diffs ( num , N ) : diffs = np . empty ( num ) for i in xrange ( num ) : mat = GOE ( N ) diffs [ i ] = center eigenvalue diff ( mat ) return diffs", "predictions": ["computes the percent of the usage of the usage"], "references": ["return num eigenvalue diffs for the nxn goe ensemble ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 5937, "code": "def init crash handler ( self ) : self . crash handler = self . crash handler class ( self ) sys . excepthook = self . excepthook def unset crashhandler ( ) : sys . excepthook = sys . excepthook atexit . register ( unset crashhandler )", "predictions": ["initialize the crash f f { crash } { % } { { % } { { crash } { crash } { { % } { crash } { {"], "references": ["create a crash handler typically setting sys . excepthook to it ."], "bleu": 0.03901663112717908, "rouge_l": 0.05053852526926264}
{"id": 5938, "code": "def init profile dir ( self ) : try : location = self . config . Profile Dir . location except Attribute Error : try : p = Profile Dir . find profile dir by name ( self . ipython dir , self . profile , self . config ) except Profile Dir Error : if self . auto create or self . profile == 'default' : try : p = Profile Dir . create profile dir by name ( self . ipython dir , self . profile , self . config ) except Profile Dir Error : self . log . fatal ( \"Could not create profile: %r\" % self . profile ) self . exit ( 1 ) else : self . log . info ( \"Created profile dir: %r\" % p . location ) else : self . log . fatal ( \"Profile %r not found.\" % self . profile ) self . exit ( 1 ) else : self . log . info ( \"Using existing profile dir: %r\" % p . location ) else : try : p = Profile Dir . find profile dir ( location , self . config ) except Profile Dir Error : if self . auto create : try : p = Profile Dir . create profile dir ( location , self . config ) except Profile Dir Error : self . log . fatal ( \"Could not create profile directory: %r\" % location ) self . exit ( 1 ) else : self . log . info ( \"Creating new profile dir: %r\" % location ) else : self . log . fatal ( \"Profile directory %r not found.\" % location ) self . exit ( 1 ) else : self . log . info ( \"Using existing profile dir: %r\" % location ) self . profile dir = p self . config file paths . append ( p . location )", "predictions": ["wraps the profile directory ."], "references": ["initialize the profile dir"], "bleu": 0.35930411196308426, "rouge_l": 0.4535315985130111}
{"id": 5939, "code": "def stage default config file ( self ) : s = self . generate config file ( ) fname = os . path . join ( self . profile dir . location , self . config file name ) if self . overwrite or not os . path . exists ( fname ) : self . log . warn ( \"Generating default config file: %r\" % ( fname ) ) with open ( fname , 'w' ) as f : f . write ( s )", "predictions": ["docs the default self ."], "references": ["auto generate default config file and stage it into the profile ."], "bleu": 0.08006212224540951, "rouge_l": 0.2190305206463196}
{"id": 5940, "code": "def erase ( self ) : if self . use file : if self . filename : file be gone ( self . filename ) self . lines = { } self . arcs = { }", "predictions": ["get the entire contents of the args . ."], "references": ["erase the data both in this object and from its file storage ."], "bleu": 0.10015045110931886, "rouge_l": 0.17604617604617603}
{"id": 5941, "code": "def line data ( self ) : return dict ( [ ( f , sorted ( lmap . keys ( ) ) ) for f , lmap in iitems ( self . lines ) ] )", "predictions": ["return temp file temp by the ensure all the e . g ."], "references": ["return the map from filenames to lists of line numbers executed ."], "bleu": 0.1135935489027116, "rouge_l": 0.2417437252311757}
{"id": 5942, "code": "def arc data ( self ) : return dict ( [ ( f , sorted ( amap . keys ( ) ) ) for f , amap in iitems ( self . arcs ) ] )", "predictions": ["csv csv data data"], "references": ["return the map from filenames to lists of line number pairs ."], "bleu": 0.040889869516541145, "rouge_l": 0.0}
{"id": 5943, "code": "def write file ( self , filename ) : data = { } data [ 'lines' ] = self . line data ( ) arcs = self . arc data ( ) if arcs : data [ 'arcs' ] = arcs if self . collector : data [ 'collector' ] = self . collector if self . debug and self . debug . should ( 'dataio' ) : self . debug . write ( \"Writing data to %r\" % ( filename , ) ) fdata = open ( filename , 'wb' ) try : pickle . dump ( data , fdata , 2 ) finally : fdata . close ( )", "predictions": ["clear the = write = a file instance"], "references": ["write the coverage data to filename ."], "bleu": 0.17747405280050269, "rouge_l": 0.13495575221238937}
{"id": 5944, "code": "def read file ( self , filename ) : self . lines , self . arcs = self . read file ( filename )", "predictions": ["new qt method to new location"], "references": ["read the coverage data from filename ."], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 5945, "code": "def raw data ( self , filename ) : if self . debug and self . debug . should ( 'dataio' ) : self . debug . write ( \"Reading data from %r\" % ( filename , ) ) fdata = open ( filename , 'rb' ) try : data = pickle . load ( fdata ) finally : fdata . close ( ) return data", "predictions": ["read has been stored in the url"], "references": ["return the raw pickled data from filename ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 5946, "code": "def add to hash ( self , filename , hasher ) : hasher . update ( self . executed lines ( filename ) ) hasher . update ( self . executed arcs ( filename ) )", "predictions": ["get the code for the given file . . . ."], "references": ["contribute filename s data to the md5hash hasher ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 5947, "code": "def get pasted lines ( sentinel , l input = py3compat . input ) : print \"Pasting code; enter '%s' alone on the line to stop or use Ctrl-D.\" % sentinel while True : try : l = l input ( ':' ) if l == sentinel : return else : yield l except EOF Error : print '<EOF>' return", "predictions": ["clear output of output lines"], "references": ["yield pasted lines until the user enters the given sentinel value ."], "bleu": 0.06732395159376953, "rouge_l": 0.1095152603231598}
{"id": 5948, "code": "def replace rlhist multiline ( self , source raw , hlen before cell ) : if not self . has readline or not self . multiline history : return hlen before cell if not hasattr ( self . readline , \"remove history item\" ) : return hlen before cell if not source raw . rstrip ( ) : return hlen before cell hlen = self . readline . get current history length ( ) if hlen == hlen before cell : return hlen before cell for i in range ( hlen - hlen before cell ) : self . readline . remove history item ( hlen - i - 1 ) stdin encoding = get stream enc ( sys . stdin , 'utf-8' ) self . readline . add history ( py3compat . unicode to str ( source raw . rstrip ( ) , stdin encoding ) ) return self . readline . get current history length ( )", "predictions": ["source the exists in the multiline multiline"], "references": ["store multiple lines as a single entry in history"], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 5949, "code": "def interact ( self , display banner = None ) : if self . exit now : return if display banner is None : display banner = self . display banner if isinstance ( display banner , basestring ) : self . show banner ( display banner ) elif display banner : self . show banner ( ) more = False if self . has readline : self . readline startup hook ( self . pre readline ) hlen b4 cell = self . readline . get current history length ( ) else : hlen b4 cell = 0 while not self . exit now : self . hooks . pre prompt hook ( ) if more : try : prompt = self . prompt manager . render ( 'in2' ) except : self . showtraceback ( ) if self . autoindent : self . rl do indent = True else : try : prompt = self . separate in + self . prompt manager . render ( 'in' ) except : self . showtraceback ( ) try : line = self . raw input ( prompt ) if self . exit now : break if self . autoindent : self . rl do indent = False except Keyboard Interrupt : #double-guard against keyboardinterrupts during kbdint handling try : self . write ( '\\n Keyboard Interrupt\\n' ) source raw = self . input splitter . source raw reset ( ) [ 1 ] hlen b4 cell = self . replace rlhist multiline ( source raw , hlen b4 cell ) more = False except Keyboard Interrupt : pass except EOF Error : if self . autoindent : self . rl do indent = False if self . has readline : self . readline startup hook ( None ) self . write ( '\\n' ) self . exit ( ) except bdb . Bdb Quit : warn ( 'The Python debugger has exited with a Bdb Quit exception.\\n' 'Because of how pdb handles the stack, it is impossible\\n' 'for I Python to properly format this particular exception.\\n' 'I Python will resume normal operation.' ) except : self . showtraceback ( ) else : self . input splitter . push ( line ) more = self . input splitter . push accepts more ( ) if ( self . Syntax TB . last syntax error and self . autoedit syntax ) : self . edit syntax error ( ) if not more : source raw = self . input splitter . source raw reset ( ) [ 1 ] self . run cell ( source raw , store history = True ) hlen b4 cell = self . replace rlhist multiline ( source raw , hlen b4 cell ) self . exit now = False", "predictions": ["starts the main loop ."], "references": ["closely emulate the interactive python console ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 5950, "code": "def should recompile ( self , e ) : if e . filename in ( '<ipython console>' , '<input>' , '<string>' , '<console>' , '<Background Job compilation>' , None ) : return False try : if ( self . autoedit syntax and not self . ask yes no ( 'Return to editor to correct syntax error? ' '[Y/n] ' , 'y' ) ) : return False except EOF Error : return False def int0 ( x ) : try : return int ( x ) except Type Error : return 0 try : self . hooks . fix error editor ( e . filename , int0 ( e . lineno ) , int0 ( e . offset ) , e . msg ) except Try Next : warn ( 'Could not open editor' ) return False return True", "predictions": ["returns true if syntax is recompile false otherwise"], "references": ["utility routine for edit_syntax_error"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 5951, "code": "def new frontend master ( self ) : ip = self . ip if self . ip in LOCAL IPS else LOCALHOST kernel manager = self . kernel manager class ( ip = ip , connection file = self . new connection file ( ) , config = self . config , ) kwargs = dict ( ) kwargs [ 'extra arguments' ] = self . kernel argv kernel manager . start kernel ( * * kwargs ) kernel manager . start channels ( ) widget = self . widget factory ( config = self . config , local kernel = True ) self . init colors ( widget ) widget . kernel manager = kernel manager widget . existing = False widget . may close = True widget . confirm exit = self . confirm exit return widget", "predictions": ["create a elapsed widget widget widget wall wall wall wall wall wall wall wall wall ."], "references": ["create and return new frontend attached to new kernel launched on localhost ."], "bleu": 0.08513012360883544, "rouge_l": 0.14055299539170507}
{"id": 5952, "code": "def init colors ( self , widget ) : try : colors = self . config . ZMQ Interactive Shell . colors except Attribute Error : colors = None try : style = self . config . I Python Widget . syntax style except Attribute Error : style = None try : sheet = self . config . I Python Widget . style sheet except Attribute Error : sheet = None if colors : colors = colors . lower ( ) if colors in ( 'lightbg' , 'light' ) : colors = 'lightbg' elif colors in ( 'dark' , 'linux' ) : colors = 'linux' else : colors = 'nocolor' elif style : if style == 'bw' : colors = 'nocolor' elif styles . dark style ( style ) : colors = 'linux' else : colors = 'lightbg' else : colors = None if style : widget . style sheet = styles . sheet from template ( style , colors ) widget . syntax style = style widget . syntax style changed ( ) widget . style sheet changed ( ) elif colors : widget . set default style ( colors = colors ) if self . stylesheet : if os . path . isfile ( self . stylesheet ) : with open ( self . stylesheet ) as f : sheet = f . read ( ) else : raise IO Error ( \"Stylesheet %r not found.\" % self . stylesheet ) if sheet : widget . style sheet = sheet widget . style sheet changed ( )", "predictions": ["setup the syntax colors ."], "references": ["configure the coloring of the widget"], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 5953, "code": "def info ( self ) : return ( self . identity , self . url , self . pub url , self . location )", "predictions": ["return the info for this url ."], "references": ["return the connection info for this object s sockets ."], "bleu": 0.27835144474202883, "rouge_l": 0.6841121495327103}
{"id": 5954, "code": "def set colors ( self , * args , * * kw ) : self . color scheme table . set active scheme ( * args , * * kw ) self . Colors = self . color scheme table . active colors if hasattr ( self , 'pdb' ) and self . pdb is not None : self . pdb . set colors ( * args , * * kw )", "predictions": ["sets the colors of the pdb pdb table ."], "references": ["shorthand access to the color table scheme selector method ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 5955, "code": "def color toggle ( self ) : if self . color scheme table . active scheme name == 'No Color' : self . color scheme table . set active scheme ( self . old scheme ) self . Colors = self . color scheme table . active colors else : self . old scheme = self . color scheme table . active scheme name self . color scheme table . set active scheme ( 'No Color' ) self . Colors = self . color scheme table . active colors", "predictions": ["turn the color of the color ."], "references": ["toggle between the currently active color scheme and nocolor ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 5956, "code": "def group required ( group , login url = None , redirect field name = REDIRECT FIELD NAME , skip superuser = True ) : def decorator ( view func ) : @ login required ( redirect field name = redirect field name , login url = login url ) def wrapped view ( request , * args , * * kwargs ) : if not ( request . user . is superuser and skip superuser ) : if request . user . groups . filter ( name = group ) . count ( ) == 0 : raise Permission Denied return view func ( request , * args , * * kwargs ) return wrapped view return decorator", "predictions": ["decorator to require user authentication ."], "references": ["view decorator for requiring a user group ."], "bleu": 0.18822631894109965, "rouge_l": 0.4178082191780822}
{"id": 5957, "code": "def add submodule ( mod , submod , fullname , subname ) : if mod is None : return #Nothing to do here. if submod is None : submod = sys . modules [ fullname ] setattr ( mod , subname , submod ) return", "predictions": ["add a submodule class to the submodule ."], "references": ["mod . { subname } = submod"], "bleu": 0.16036590969929357, "rouge_l": 0.13495575221238937}
{"id": 5958, "code": "def ensure fromlist ( mod , fromlist , buf , recursive ) : if not hasattr ( mod , ' path ' ) : return for item in fromlist : if not hasattr ( item , 'rindex' ) : raise Type Error ( \"Item in ``from list'' not a string\" ) if item == '*' : if recursive : continue try : all = mod . all except Attribute Error : pass else : ret = ensure fromlist ( mod , all , buf , 1 ) if not ret : return 0 elif not hasattr ( mod , item ) : import submodule ( mod , item , buf + '.' + item )", "predictions": ["ensure that all item in buf is not a valid path ."], "references": ["handle from module import a b c imports ."], "bleu": 0.11498759556447223, "rouge_l": 0.19551282051282048}
{"id": 5959, "code": "def add section ( self ) : sect = Code Builder ( self . indent amount ) self . code . append ( sect ) return sect", "predictions": ["add a section ."], "references": ["add a section a sub - codebuilder ."], "bleu": 0.2601300475114444, "rouge_l": 0.6288659793814433}
{"id": 5960, "code": "def get function ( self , fn name ) : assert self . indent amount == 0 g = { } code text = str ( self ) exec ( code text , g ) return g [ fn name ]", "predictions": ["get the function of the function ."], "references": ["compile the code and return the function fn_name ."], "bleu": 0.20873176328735715, "rouge_l": 0.48897795591182364}
{"id": 5961, "code": "def expr code ( self , expr ) : if \"|\" in expr : pipes = expr . split ( \"|\" ) code = self . expr code ( pipes [ 0 ] ) for func in pipes [ 1 : ] : self . all vars . add ( func ) code = \"c %s(%s)\" % ( func , code ) elif \".\" in expr : dots = expr . split ( \".\" ) code = self . expr code ( dots [ 0 ] ) args = [ repr ( d ) for d in dots [ 1 : ] ] code = \"dot(%s, %s)\" % ( code , \", \" . join ( args ) ) else : self . all vars . add ( expr ) code = \"c %s\" % expr return code", "predictions": ["return the code expression for the given expression ."], "references": ["generate a python expression for expr ."], "bleu": 0.19960198807747329, "rouge_l": 0.38364779874213834}
{"id": 5962, "code": "def do dots ( self , value , * dots ) : for dot in dots : try : value = getattr ( value , dot ) except Attribute Error : value = value [ dot ] if hasattr ( value , ' call ' ) : value = value ( ) return value", "predictions": ["cast value to dots ."], "references": ["evaluate dotted expressions at runtime ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 5963, "code": "def formatters default ( self ) : formatter classes = [ Plain Text Formatter , HTML Formatter , SVG Formatter , PNG Formatter , JPEG Formatter , Latex Formatter , JSON Formatter , Javascript Formatter ] d = { } for cls in formatter classes : f = cls ( config = self . config ) d [ f . format type ] = f return d", "predictions": ["returns the default collector settings"], "references": ["activate the default formatters ."], "bleu": 0.35930411196308426, "rouge_l": 0.4}
{"id": 5964, "code": "def user config files ( ) : return filter ( os . path . exists , map ( os . path . expanduser , config files ) )", "predictions": ["return a list of all config files ."], "references": ["return path to any existing user config files"], "bleu": 0.22679164443904004, "rouge_l": 0.375}
{"id": 5965, "code": "def configure Where ( self , where ) : from nose . importer import add path self . working Dir = None where = tolist ( where ) warned = False for path in where : if not self . working Dir : abs path = absdir ( path ) if abs path is None : raise Value Error ( \"Working directory %s not found, or \" \"not a directory\" % path ) log . info ( \"Set working dir to %s\" , abs path ) self . working Dir = abs path if self . add Paths and os . path . exists ( os . path . join ( abs path , ' init .py' ) ) : log . info ( \"Working directory %s is a package; \" \"adding to sys.path\" % abs path ) add path ( abs path ) continue if not warned : warn ( \"Use of multiple -w arguments is deprecated and \" \"support may be removed in a future release. You can \" \"get the same behavior by passing directories without \" \"the -w argument on the command line, or by using the \" \"--tests argument in a configuration file.\" , Deprecation Warning ) self . test Names . append ( path )", "predictions": ["configure the working directory for the working directory ."], "references": ["configure the working directory or directories for the test run ."], "bleu": 0.37405485108988873, "rouge_l": 0.6876006441223833}
{"id": 5966, "code": "def page file ( fname , start = 0 , pager cmd = None ) : pager cmd = get pager cmd ( pager cmd ) pager cmd += ' ' + get pager start ( pager cmd , start ) try : if os . environ [ 'TERM' ] in [ 'emacs' , 'dumb' ] : raise Environment Error system ( pager cmd + ' ' + fname ) except : try : if start > 0 : start -= 1 page ( open ( fname ) . read ( ) , start ) except : print 'Unable to show file' , `fname`", "predictions": ["run a page in the page"], "references": ["page a file using an optional pager command and starting line ."], "bleu": 0.08993236413460196, "rouge_l": 0.10481099656357389}
{"id": 5967, "code": "def print basic unicode ( o , p , cycle ) : if cycle : return p . text ( 'Basic(...)' ) out = pretty ( o , use unicode = True ) if '\\n' in out : p . text ( u'\\n' ) p . text ( out )", "predictions": ["print unicode string with basic unicode ."], "references": ["a function to pretty print sympy basic objects ."], "bleu": 0.16599826150636804, "rouge_l": 0.3667334669338677}
{"id": 5968, "code": "def print png ( o ) : s = latex ( o , mode = 'inline' ) s = s . replace ( '\\\\operatorname' , '' ) s = s . replace ( '\\\\overline' , '\\\\bar' ) png = latex to png ( s ) return png", "predictions": ["print a png string to png ."], "references": ["a function to display sympy expression using inline style latex in png ."], "bleu": 0.11787460936700446, "rouge_l": 0.3794712286158632}
{"id": 5969, "code": "def print display png ( o ) : s = latex ( o , mode = 'plain' ) s = s . strip ( '$' ) png = latex to png ( '$$%s$$' % s , backend = 'dvipng' ) return png", "predictions": ["pretty - print a png string to png ."], "references": ["a function to display sympy expression using display style latex in png ."], "bleu": 0.135323305042906, "rouge_l": 0.35209235209235207}
{"id": 5970, "code": "def load ipython extension ( ip ) : import sympy try : import sympy . interactive . ipythonprinting except Import Error : pass else : warnings . warn ( \"The sympyprinting extension in I Python is deprecated, \" \"use sympy.interactive.ipythonprinting\" ) ip . extension manager . load extension ( 'sympy.interactive.ipythonprinting' ) return global loaded if not loaded : plaintext formatter = ip . display formatter . formatters [ 'text/plain' ] for cls in ( object , str ) : plaintext formatter . for type ( cls , print basic unicode ) printable containers = [ list , tuple ] if sympy . version > '0.7.1' : printable containers += [ set , frozenset ] else : plaintext formatter . for type ( cls , print basic unicode ) plaintext formatter . for type by name ( 'sympy.core.basic' , 'Basic' , print basic unicode ) plaintext formatter . for type by name ( 'sympy.matrices.matrices' , 'Matrix' , print basic unicode ) png formatter = ip . display formatter . formatters [ 'image/png' ] png formatter . for type by name ( 'sympy.core.basic' , 'Basic' , print png ) png formatter . for type by name ( 'sympy.matrices.matrices' , 'Matrix' , print display png ) for cls in [ dict , int , long , float ] + printable containers : png formatter . for type ( cls , print png ) latex formatter = ip . display formatter . formatters [ 'text/latex' ] latex formatter . for type by name ( 'sympy.core.basic' , 'Basic' , print latex ) latex formatter . for type by name ( 'sympy.matrices.matrices' , 'Matrix' , print latex ) for cls in printable containers : latex formatter . for type ( cls , print latex ) loaded = True", "predictions": ["load latex formatter for all ipython containers"], "references": ["load the extension in ipython ."], "bleu": 0.20556680845025982, "rouge_l": 0.31202046035805625}
{"id": 5971, "code": "def run loop ( self ) : while True : try : self . ioloop . start ( ) except ZMQ Error as e : if e . errno == errno . EINTR : continue else : raise except Exception : if self . exiting : break else : raise else : break", "predictions": ["start the ioloop loop ."], "references": ["run my loop ignoring eintr events in the poller"], "bleu": 0.13575914775035755, "rouge_l": 0.1358574610244989}
{"id": 5972, "code": "def input ( self , string ) : content = dict ( value = string ) msg = self . session . msg ( 'input reply' , content ) self . queue send ( msg )", "predictions": ["send message to client ."], "references": ["send a string of raw input to the kernel ."], "bleu": 0.11943865131127647, "rouge_l": 0.37731958762886597}
{"id": 5973, "code": "def stop channels ( self ) : if self . shell channel . is alive ( ) : self . shell channel . stop ( ) if self . sub channel . is alive ( ) : self . sub channel . stop ( ) if self . stdin channel . is alive ( ) : self . stdin channel . stop ( ) if self . hb channel . is alive ( ) : self . hb channel . stop ( )", "predictions": ["stop the timer ."], "references": ["stops all the running channels for this kernel ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 5974, "code": "def channels running ( self ) : return ( self . shell channel . is alive ( ) or self . sub channel . is alive ( ) or self . stdin channel . is alive ( ) or self . hb channel . is alive ( ) )", "predictions": ["return whether the connection is running ."], "references": ["are any of the channels created and running?"], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 5975, "code": "def load connection file ( self ) : with open ( self . connection file ) as f : cfg = json . loads ( f . read ( ) ) self . ip = cfg [ 'ip' ] self . shell port = cfg [ 'shell port' ] self . stdin port = cfg [ 'stdin port' ] self . iopub port = cfg [ 'iopub port' ] self . hb port = cfg [ 'hb port' ] self . session . key = str to bytes ( cfg [ 'key' ] )", "predictions": ["load the connection file from the shell"], "references": ["load connection info from json dict in self . connection_file"], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 5976, "code": "def write connection file ( self ) : if self . connection file written : return self . connection file , cfg = write connection file ( self . connection file , ip = self . ip , key = self . session . key , stdin port = self . stdin port , iopub port = self . iopub port , shell port = self . shell port , hb port = self . hb port ) self . shell port = cfg [ 'shell port' ] self . stdin port = cfg [ 'stdin port' ] self . iopub port = cfg [ 'iopub port' ] self . hb port = cfg [ 'hb port' ] self . connection file written = True", "predictions": ["writes the connection file to the connection ."], "references": ["write connection info to json dict in self . connection_file"], "bleu": 0.1485237584394808, "rouge_l": 0.3267857142857143}
{"id": 5977, "code": "def kill kernel ( self ) : if self . has kernel : if self . hb channel is not None : self . hb channel . pause ( ) try : self . kernel . kill ( ) except OS Error , e : if sys . platform == 'win32' : if e . winerror != 5 : raise else : from errno import ESRCH if e . errno != ESRCH : raise self . kernel = None else : raise Runtime Error ( \"Cannot kill kernel. No kernel is running!\" )", "predictions": ["kill the kernel kernel ."], "references": ["kill the running kernel ."], "bleu": 0.45180100180492244, "rouge_l": 0.8}
{"id": 5978, "code": "def is alive ( self ) : if self . has kernel : if self . kernel . poll ( ) is None : return True else : return False elif self . hb channel is not None : return self . hb channel . is beating ( ) else : return True", "predictions": ["returns a boolean indicating whether the channel is alive ."], "references": ["is the kernel process still running?"], "bleu": 0.13950796967929133, "rouge_l": 0.13090128755364808}
{"id": 5979, "code": "def shell channel ( self ) : if self . shell channel is None : self . shell channel = self . shell channel class ( self . context , self . session , ( self . ip , self . shell port ) ) return self . shell channel", "predictions": ["return the shell channel channel ."], "references": ["get the req socket channel object to make requests of the kernel ."], "bleu": 0.08180282100568384, "rouge_l": 0.29611650485436897}
{"id": 5980, "code": "def sub channel ( self ) : if self . sub channel is None : self . sub channel = self . sub channel class ( self . context , self . session , ( self . ip , self . iopub port ) ) return self . sub channel", "predictions": ["return the channel channel ."], "references": ["get the sub socket channel object ."], "bleu": 0.21763141204756337, "rouge_l": 0.48541114058355433}
{"id": 5981, "code": "def walk egg ( egg dir ) : walker = os . walk ( egg dir ) base , dirs , files = walker . next ( ) if 'EGG-INFO' in dirs : dirs . remove ( 'EGG-INFO' ) yield base , dirs , files for bdf in walker : yield bdf", "predictions": ["walk an unpacked directory and return a list of directories ."], "references": ["walk an unpacked egg s contents skipping the metadata directory"], "bleu": 0.22416933501922287, "rouge_l": 0.384251968503937}
{"id": 5982, "code": "def scan module ( egg dir , base , name , stubs ) : filename = os . path . join ( base , name ) if filename [ : - 1 ] in stubs : return True pkg = base [ len ( egg dir ) + 1 : ] . replace ( os . sep , '.' ) module = pkg + ( pkg and '.' or '' ) + os . path . splitext ( name ) [ 0 ] if sys . version info < ( 3 , 3 ) : skip = 8 else : skip = 12 f = open ( filename , 'rb' ) f . read ( skip ) code = marshal . load ( f ) f . close ( ) safe = True symbols = dict . fromkeys ( iter symbols ( code ) ) for bad in [ ' file ' , ' path ' ] : if bad in symbols : log . warn ( \"%s: module references %s\" , module , bad ) safe = False if 'inspect' in symbols : for bad in [ 'getsource' , 'getabsfile' , 'getsourcefile' , 'getfile' 'getsourcelines' , 'findsource' , 'getcomments' , 'getframeinfo' , 'getinnerframes' , 'getouterframes' , 'stack' , 'trace' ] : if bad in symbols : log . warn ( \"%s: module MAY be using inspect.%s\" , module , bad ) safe = False if ' name ' in symbols and ' main ' in symbols and '.' not in module : if sys . version [ : 3 ] == \"2.4\" : log . warn ( \"%s: top-level module may be 'python -m' script\" , module ) safe = False return safe", "predictions": ["scan for a module in the current directory ."], "references": ["check whether module possibly uses unsafe - for - zipfile stuff"], "bleu": 0.12507277759788113, "rouge_l": 0.09822866344605477}
{"id": 5983, "code": "def make init files ( self ) : init files = [ ] for base , dirs , files in walk egg ( self . bdist dir ) : if base == self . bdist dir : continue for name in files : if name . endswith ( '.py' ) : if ' init .py' not in files : pkg = base [ len ( self . bdist dir ) + 1 : ] . replace ( os . sep , '.' ) if self . distribution . has contents for ( pkg ) : log . warn ( \"Creating missing  init .py for %s\" , pkg ) filename = os . path . join ( base , ' init .py' ) if not self . dry run : f = open ( filename , 'w' ) f . write ( NS PKG STUB ) f . close ( ) init files . append ( filename ) break else : dirs [ : ] = [ ] return init files", "predictions": ["make all files in the current directory ."], "references": ["create missing package __init__ files"], "bleu": 0.16036590969929357, "rouge_l": 0.16052631578947368}
{"id": 5984, "code": "def launch new instance ( ) : if sys . platform == 'win32' : import multiprocessing p = multiprocessing . current process ( ) if p . name != 'Main Process' : return app = IP Controller App . instance ( ) app . initialize ( ) app . start ( )", "predictions": ["init a colors instance"], "references": ["create and run the ipython controller"], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 5985, "code": "def save connection dict ( self , fname , cdict ) : c = self . config url = cdict [ 'url' ] location = cdict [ 'location' ] if not location : try : proto , ip , port = split url ( url ) except Assertion Error : pass else : try : location = socket . gethostbyname ex ( socket . gethostname ( ) ) [ 2 ] [ - 1 ] except ( socket . gaierror , Index Error ) : self . log . warn ( \"Could not identify this machine's IP, assuming 127.0.0.1.\" \" You may need to specify '--location=<external ip address>' to help\" \" I Python decide when to connect via loopback.\" ) location = '127.0.0.1' cdict [ 'location' ] = location fname = os . path . join ( self . profile dir . security dir , fname ) self . log . info ( \"writing connection info to %s\" , fname ) with open ( fname , 'w' ) as f : f . write ( json . dumps ( cdict , indent = 2 ) ) os . chmod ( fname , stat . S IRUSR | stat . S IWUSR )", "predictions": ["info about the connection connection"], "references": ["save a connection dict to json file ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 5986, "code": "def load config from json ( self ) : c = self . config self . log . debug ( \"loading config from JSON\" ) fname = os . path . join ( self . profile dir . security dir , self . engine json file ) self . log . info ( \"loading connection info from %s\" , fname ) with open ( fname ) as f : cfg = json . loads ( f . read ( ) ) key = cfg [ 'exec key' ] c . Session . key = key . encode ( 'ascii' ) xport , addr = cfg [ 'url' ] . split ( '://' ) c . Hub Factory . engine transport = xport ip , ports = addr . split ( ':' ) c . Hub Factory . engine ip = ip c . Hub Factory . regport = int ( ports ) self . location = cfg [ 'location' ] if not self . engine ssh server : self . engine ssh server = cfg [ 'ssh' ] fname = os . path . join ( self . profile dir . security dir , self . client json file ) self . log . info ( \"loading connection info from %s\" , fname ) with open ( fname ) as f : cfg = json . loads ( f . read ( ) ) assert key == cfg [ 'exec key' ] , \"exec key mismatch between engine and client keys\" xport , addr = cfg [ 'url' ] . split ( '://' ) c . Hub Factory . client transport = xport ip , ports = addr . split ( ':' ) c . Hub Factory . client ip = ip if not self . ssh server : self . ssh server = cfg [ 'ssh' ] assert int ( ports ) == c . Hub Factory . regport , \"regport mismatch\"", "predictions": ["set the connection colors from self . self . self . self . self . self . self . self . self . self . self . self . self ."], "references": ["load config from existing json connector files ."], "bleu": 0.04317900023606586, "rouge_l": 0.1147695202257761}
{"id": 5987, "code": "def load secondary config ( self ) : if self . reuse files : try : self . load config from json ( ) except ( Assertion Error , IO Error ) as e : self . log . error ( \"Could not load config from JSON: %s\" % e ) else : self . write connection files = False default secure ( self . config ) self . log . debug ( \"Config changed\" ) self . log . debug ( repr ( self . config ) )", "predictions": ["color toggle config from json"], "references": ["secondary config loading from json and setting defaults"], "bleu": 0.2118947430943267, "rouge_l": 0.44309927360774815}
{"id": 5988, "code": "def script args ( f ) : args = [ magic arguments . argument ( '--out' , type = str , help = ) , magic arguments . argument ( '--err' , type = str , help = ) , magic arguments . argument ( '--bg' , action = \"store true\" , help = ) , magic arguments . argument ( '--proc' , type = str , help = ) , ] for arg in args : f = arg ( f ) return f", "predictions": ["kwargs for the command line arguments"], "references": ["single decorator for adding script args"], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 5989, "code": "def parallel execute ( self , cell , block = None , groupby = 'type' , save name = None ) : block = self . view . block if block is None else block base = \"Parallel\" if block else \"Async parallel\" targets = self . view . targets if isinstance ( targets , list ) and len ( targets ) > 10 : str targets = str ( targets [ : 4 ] ) [ : - 1 ] + ', ..., ' + str ( targets [ - 4 : ] ) [ 1 : ] else : str targets = str ( targets ) if self . verbose : print base + \" execution on engine(s): %s\" % str targets result = self . view . execute ( cell , silent = False , block = False ) self . last result = result if save name : self . shell . user ns [ save name ] = result if block : result . get ( ) result . display outputs ( groupby ) else : return result", "predictions": ["called when a cell is add a \"async to the list of setattr"], "references": ["implementation used by %px and %%parallel"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 5990, "code": "def disable autopx ( self ) : if self . autopx : self . shell . run cell = self . original run cell self . autopx = False print \"%autopx disabled\"", "predictions": ["ensure the shell is autopx"], "references": ["disable %autopx by restoring the original interactiveshell . run_cell ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 5991, "code": "def run heartbeat ( message ) : then = arrow . get ( message [ 'time' ] ) now = arrow . get ( ) if ( now - then ) > timezone . timedelta ( seconds = ( TICK FREQ + 1 ) ) : pass else : Task . run tasks ( )", "predictions": ["add a section to the arrow"], "references": ["internal clock_channel consumer to process task runs"], "bleu": 0.1869843520537038, "rouge_l": 0.1517412935323383}
{"id": 5992, "code": "def run task ( message ) : task = Task . objects . get ( pk = message [ 'id' ] ) if task . allow overlap : task . run ( message ) else : if not task . running : task . running = True task . save ( ) try : task . run ( message ) finally : task . running = False task . save ( )", "predictions": ["runs the job ."], "references": ["internal run_task consumer to run the task s callable"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 5993, "code": "def remove task ( message ) : task = Task . objects . get ( pk = message [ 'id' ] ) task . delete ( )", "predictions": ["expr a code object from the collection"], "references": ["internal kill_task consumer to remove retired tasks"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 5994, "code": "def patch if missing ( obj , name , method ) : setattr ( obj , name , getattr ( obj , name , method ) )", "predictions": ["do a do not do not have an object"], "references": ["patch a method onto an object if it isn t already there ."], "bleu": 0.1279808802469055, "rouge_l": 0.26406926406926406}
{"id": 5995, "code": "def accept connection ( self ) : assert self . pending , \"Connection is not pending.\" self . server protocol = self . server . server factory . build Protocol ( None ) self . accept d . callback ( Fake Server Protocol Wrapper ( self , self . server protocol ) ) return self . await connected ( )", "predictions": ["formatters the default default default default default default default default default default default default default default default default default default default default default default default default default default default default default"], "references": ["accept a pending connection ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 5996, "code": "def reject connection ( self , reason = None ) : assert self . pending , \"Connection is not pending.\" if reason is None : reason = Connection Refused Error ( ) self . accept d . errback ( reason )", "predictions": ["user can be called when a config is lost ."], "references": ["reject a pending connection ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 5997, "code": "def get agent ( self , reactor = None , context Factory = None ) : return Proxy Agent With Context ( self . endpoint , reactor = reactor , context Factory = context Factory )", "predictions": ["returns an agent object for the given endpoint = none = 0 = 0 = 1 = 0 = 1 = 0 = 0 = 1 = 0 = 0 ="], "references": ["returns an iagent that makes requests to this fake server ."], "bleu": 0.0513487742994337, "rouge_l": 0.10418445772843724}
{"id": 5998, "code": "def form valid ( self , form ) : self . object = form . save ( commit = False ) response = self . pre save ( self . object ) if response : return response self . object . save ( ) form . save m2m ( ) self . post save ( self . object ) return Http Response Redirect ( self . get success url ( ) )", "predictions": ["handle the success and save the success"], "references": ["calls pre and post save hooks ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 5999, "code": "def delete ( self , request , * args , * * kwargs ) : self . object = self . get object ( ) success url = self . get success url ( ) self . pre delete ( self . object ) self . object . delete ( ) self . post delete ( self . object ) return Http Response Redirect ( success url )", "predictions": ["handles the considered print out of the resource if it is not provided ."], "references": ["calls pre and post delete hooks for delteviews ."], "bleu": 0.08839374326825923, "rouge_l": 0.09050445103857567}
{"id": 6000, "code": "def pre save ( self , instance ) : super ( User View Mixin , self ) . pre save ( instance ) if self . request . user . is authenticated ( ) : for field in self . user field : setattr ( instance , field , self . request . user )", "predictions": ["set the user back to the user instance ."], "references": ["use savehookmixin pre_save to set the user ."], "bleu": 0.2907153684841096, "rouge_l": 0.4756335282651072}
{"id": 6001, "code": "def check ( self , check all = False ) : if not self . enabled and not check all : return if check all or self . check all : modules = sys . modules . keys ( ) else : modules = self . modules . keys ( ) for modname in modules : m = sys . modules . get ( modname , None ) if modname in self . skip modules : continue if not hasattr ( m , ' file ' ) : continue if m . name == ' main ' : continue filename = m . file path , ext = os . path . splitext ( filename ) if ext . lower ( ) == '.py' : ext = PY COMPILED EXT pyc filename = pyfile . cache from source ( filename ) py filename = filename else : pyc filename = filename try : py filename = pyfile . source from cache ( filename ) except Value Error : continue try : pymtime = os . stat ( py filename ) . st mtime if pymtime <= os . stat ( pyc filename ) . st mtime : continue if self . failed . get ( py filename , None ) == pymtime : continue except OS Error : continue try : superreload ( m , reload , self . old objects ) if py filename in self . failed : del self . failed [ py filename ] except : print >> sys . stderr , \"[autoreload of %s failed: %s]\" % ( modname , traceback . format exc ( 1 ) ) self . failed [ py filename ] = pymtime", "predictions": ["print all to the to the cache mode mode mode mode mode mode mode mode mode"], "references": ["check whether some modules need to be reloaded ."], "bleu": 0.07692375026049747, "rouge_l": 0.08425414364640883}
{"id": 6002, "code": "def clipboard get ( self ) : from I Python . lib . clipboard import ( osx clipboard get , tkinter clipboard get , win32 clipboard get ) if sys . platform == 'win32' : chain = [ win32 clipboard get , tkinter clipboard get ] elif sys . platform == 'darwin' : chain = [ osx clipboard get , tkinter clipboard get ] else : chain = [ tkinter clipboard get ] dispatcher = Command Chain Dispatcher ( ) for func in chain : dispatcher . add ( func ) text = dispatcher ( ) return text", "predictions": ["plaintext for the load load the load load the load try to the load the load try to the load the load the load the load the load the load the"], "references": ["get text from the clipboard ."], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 6003, "code": "def add ( self , func , priority = 0 ) : self . chain . append ( ( priority , func ) ) self . chain . sort ( key = lambda x : x [ 0 ] )", "predictions": ["run a priority ioloop"], "references": ["add a func to the cmd chain with given priority"], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 6004, "code": "def configure ( self , options , conf ) : self . conf = conf self . enabled = options . debug Errors or options . debug Failures self . enabled for errors = options . debug Errors self . enabled for failures = options . debug Failures", "predictions": ["input must be called after the command = false if not already loaded = false = true"], "references": ["configure which kinds of exceptions trigger plugin ."], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 6005, "code": "def import item ( name ) : package = '.' . join ( name . split ( '.' ) [ 0 : - 1 ] ) obj = name . split ( '.' ) [ - 1 ] if package : module = import ( package , fromlist = [ obj ] ) try : pak = module . dict [ obj ] except Key Error : raise Import Error ( 'No module named %s' % obj ) return pak else : return import ( obj )", "predictions": ["stop and return an channels from a module module"], "references": ["import and return bar given the string foo . bar ."], "bleu": 0.14873743701255318, "rouge_l": 0.19645732689210954}
{"id": 6006, "code": "def try passwordless openssh ( server , keyfile ) : if pexpect is None : raise Import Error ( \"pexpect unavailable, use paramiko\" ) cmd = 'ssh -f ' + server if keyfile : cmd += ' -i ' + keyfile cmd += ' exit' p = pexpect . spawn ( cmd ) while True : try : p . expect ( '[Pp]assword:' , timeout = .1 ) except pexpect . TIMEOUT : continue except pexpect . EOF : return True else : return False", "predictions": ["spawn a is required to be used in the is not a is not a is required"], "references": ["try passwordless login with shell ssh command ."], "bleu": 0.06074588070876682, "rouge_l": 0.0}
{"id": 6007, "code": "def try passwordless paramiko ( server , keyfile ) : if paramiko is None : msg = \"Paramiko unavaliable, \" if sys . platform == 'win32' : msg += \"Paramiko is required for ssh tunneled connections on Windows.\" else : msg += \"use Open SSH.\" raise Import Error ( msg ) username , server , port = split server ( server ) client = paramiko . SSH Client ( ) client . load system host keys ( ) client . set missing host key policy ( paramiko . Warning Policy ( ) ) try : client . connect ( server , port , username = username , key filename = keyfile , look for keys = True ) except paramiko . Authentication Exception : return False else : client . close ( ) return True", "predictions": ["load connection to file ."], "references": ["try passwordless login with paramiko ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 6008, "code": "def unwrap exception ( self , content ) : e = error . unwrap exception ( content ) if e . engine info : e uuid = e . engine info [ 'engine uuid' ] eid = self . engines [ e uuid ] e . engine info [ 'engine id' ] = eid return e", "predictions": ["write connection object to = = false"], "references": ["unwrap exception and remap engine_id to int ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 6009, "code": "def register engine ( self , msg ) : content = msg [ 'content' ] eid = content [ 'id' ] d = { eid : content [ 'queue' ] } self . update engines ( d )", "predictions": ["kill a new kernel kernel with the given msg . . . . ."], "references": ["register a new engine and update our connection info ."], "bleu": 0.1250076305588977, "rouge_l": 0.2577464788732394}
{"id": 6010, "code": "def unregister engine ( self , msg ) : content = msg [ 'content' ] eid = int ( content [ 'id' ] ) if eid in self . ids : self . ids . remove ( eid ) uuid = self . engines . pop ( eid ) self . handle stranded msgs ( eid , uuid ) if self . task socket and self . task scheme == 'pure' : self . stop scheduling tasks ( )", "predictions": ["is the alive alive"], "references": ["unregister an engine that has died ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 6011, "code": "def flush results ( self , sock ) : idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) while msg is not None : if self . debug : pprint ( msg ) msg type = msg [ 'header' ] [ 'msg type' ] handler = self . queue handlers . get ( msg type , None ) if handler is None : raise Exception ( \"Unhandled message type: %s\" % msg . msg type ) else : handler ( msg ) idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK )", "predictions": ["shell channel channel channel class class class class class class class class class class class class class class class class class class class class class class method"], "references": ["flush task or queue results waiting in zmq queue ."], "bleu": 0.03776949794525175, "rouge_l": 0.0}
{"id": 6012, "code": "def flush ignored control ( self ) : while self . ignored control replies > 0 : self . session . recv ( self . control socket ) self . ignored control replies -= 1", "predictions": ["sub for channel session"], "references": ["flush ignored control replies"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 6013, "code": "def spin every ( self , interval = 1 ) : while True : if self . stop spinning . is set ( ) : return time . sleep ( interval ) self . spin ( )", "predictions": ["remove all interval egg egg"], "references": ["target func for use in spin_thread"], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 6014, "code": "def stop spin thread ( self ) : if self . spin thread is not None : self . stop spinning . set ( ) self . spin thread . join ( ) self . spin thread = None", "predictions": ["scan for the module thread"], "references": ["stop background spin_thread if any"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 6015, "code": "def send execute request ( self , socket , code , silent = True , subheader = None , ident = None ) : if self . closed : raise Runtime Error ( \"Client cannot be used after its sockets have been closed\" ) subheader = subheader if subheader is not None else { } if not isinstance ( code , basestring ) : raise Type Error ( \"code must be text, not %s\" % type ( code ) ) if not isinstance ( subheader , dict ) : raise Type Error ( \"subheader must be dict, not %s\" % type ( subheader ) ) content = dict ( code = code , silent = bool ( silent ) , user variables = [ ] , user expressions = { } ) msg = self . session . send ( socket , \"execute request\" , content = content , ident = ident , subheader = subheader ) msg id = msg [ 'header' ] [ 'msg id' ] self . outstanding . add ( msg id ) if ident : if isinstance ( ident , list ) : ident = ident [ - 1 ] if ident in self . engines . values ( ) : self . outstanding dict [ ident ] . add ( msg id ) self . history . append ( msg id ) self . metadata [ msg id ] [ 'submitted' ] = datetime . now ( ) return msg", "predictions": ["make sure the files are not a init files ."], "references": ["construct and send an execute request via a socket ."], "bleu": 0.13950796967929133, "rouge_l": 0.2}
{"id": 6016, "code": "def opcode set ( * names ) : s = set ( ) for name in names : try : s . add ( opcode ( name ) ) except Key Error : pass return s", "predictions": ["set the opcode object ."], "references": ["return a set of opcodes by the names in names ."], "bleu": 0.09778809693469985, "rouge_l": 0.35124760076775424}
{"id": 6017, "code": "def get byte parser ( self ) : if not self . byte parser : self . byte parser = Byte Parser ( text = self . text , filename = self . filename ) return self . byte parser", "predictions": ["return the parser for this file ."], "references": ["create a byteparser on demand ."], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 6018, "code": "def first line ( self , line ) : rng = self . multiline . get ( line ) if rng : first line = rng [ 0 ] else : first line = line return first line", "predictions": ["return the first line of the line"], "references": ["return the first line number of the statement including line ."], "bleu": 0.3471820111672571, "rouge_l": 0.7478108581436078}
{"id": 6019, "code": "def block stack repr ( self , block stack ) : blocks = \", \" . join ( [ \"(%s, %r)\" % ( dis . opname [ b [ 0 ] ] , b [ 1 ] ) for b in block stack ] ) return \"[\" + blocks + \"]\"", "predictions": ["returns the string representation of the block stack ."], "references": ["get a string version of block_stack for debugging ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 6020, "code": "def validate chunks ( self , chunks ) : starts = set ( [ ch . byte for ch in chunks ] ) for ch in chunks : assert all ( [ ( ex in starts or ex < 0 ) for ex in ch . exits ] )", "predictions": ["validate chunks of chunks of chunks in chunks ."], "references": ["validate the rule that chunks have a single entrance ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 6021, "code": "def options ( self , parser , env ) : super ( Coverage , self ) . options ( parser , env ) parser . add option ( \"--cover-package\" , action = \"append\" , default = env . get ( 'NOSE COVER PACKAGE' ) , metavar = \"PACKAGE\" , dest = \"cover packages\" , help = \"Restrict coverage output to selected packages \" \"[NOSE COVER PACKAGE]\" ) parser . add option ( \"--cover-erase\" , action = \"store true\" , default = env . get ( 'NOSE COVER ERASE' ) , dest = \"cover erase\" , help = \"Erase previously collected coverage \" \"statistics before run\" ) parser . add option ( \"--cover-tests\" , action = \"store true\" , dest = \"cover tests\" , default = env . get ( 'NOSE COVER TESTS' ) , help = \"Include test modules in coverage report \" \"[NOSE COVER TESTS]\" ) parser . add option ( \"--cover-min-percentage\" , action = \"store\" , dest = \"cover min percentage\" , default = env . get ( 'NOSE COVER MIN PERCENTAGE' ) , help = \"Minimum percentage of coverage for tests\" \"to pass [NOSE COVER MIN PERCENTAGE]\" ) parser . add option ( \"--cover-inclusive\" , action = \"store true\" , dest = \"cover inclusive\" , default = env . get ( 'NOSE COVER INCLUSIVE' ) , help = \"Include all python files under working \" \"directory in coverage report.  Useful for \" \"discovering holes in test coverage if not all \" \"files are imported by the test suite. \" \"[NOSE COVER INCLUSIVE]\" ) parser . add option ( \"--cover-html\" , action = \"store true\" , default = env . get ( 'NOSE COVER HTML' ) , dest = 'cover html' , help = \"Produce HTML coverage information\" ) parser . add option ( '--cover-html-dir' , action = 'store' , default = env . get ( 'NOSE COVER HTML DIR' , 'cover' ) , dest = 'cover html dir' , metavar = 'DIR' , help = 'Produce HTML coverage information in dir' ) parser . add option ( \"--cover-branches\" , action = \"store true\" , default = env . get ( 'NOSE COVER BRANCHES' ) , dest = \"cover branches\" , help = \"Include branch coverage in coverage report \" \"[NOSE COVER BRANCHES]\" ) parser . add option ( \"--cover-xml\" , action = \"store true\" , default = env . get ( 'NOSE COVER XML' ) , dest = \"cover xml\" , help = \"Produce XML coverage information\" ) parser . add option ( \"--cover-xml-file\" , action = \"store\" , default = env . get ( 'NOSE COVER XML FILE' , 'coverage.xml' ) , dest = \"cover xml file\" , metavar = \"FILE\" , help = \"Produce XML coverage information in file\" )", "predictions": ["add program options ."], "references": ["add options to command line ."], "bleu": 0.25916266987614406, "rouge_l": 0.5791139240506329}
{"id": 6022, "code": "def begin ( self ) : log . debug ( \"Coverage begin\" ) self . skip Modules = sys . modules . keys ( ) [ : ] if self . cover Erase : log . debug ( \"Clearing previously collected coverage statistics\" ) self . cover Instance . combine ( ) self . cover Instance . erase ( ) self . cover Instance . exclude ( '#pragma[: ]+[n N][o O] [c C][o O][v V][e E][r R]' ) self . cover Instance . load ( ) self . cover Instance . start ( )", "predictions": ["begin the cover coverage coverage ."], "references": ["begin recording coverage information ."], "bleu": 0.2626909894424158, "rouge_l": 0.5545454545454546}
{"id": 6023, "code": "def report ( self , stream ) : log . debug ( \"Coverage report\" ) self . cover Instance . stop ( ) self . cover Instance . combine ( ) self . cover Instance . save ( ) modules = [ module for name , module in sys . modules . items ( ) if self . want Module Coverage ( name , module ) ] log . debug ( \"Coverage report will cover modules: %s\" , modules ) self . cover Instance . report ( modules , file = stream ) if self . cover Html Dir : log . debug ( \"Generating HTML coverage report\" ) self . cover Instance . html report ( modules , self . cover Html Dir ) if self . cover Xml File : log . debug ( \"Generating XML coverage report\" ) self . cover Instance . xml report ( modules , self . cover Xml File ) if self . cover Min Percentage : f = String IO . String IO ( ) self . cover Instance . report ( modules , file = f ) m = re . search ( r'-------\\s\\w+\\s+\\d+\\s+\\d+\\s+(\\d+)%\\s+\\d*\\s{0,1}$' , f . getvalue ( ) ) if m : percentage = int ( m . groups ( ) [ 0 ] ) if percentage < self . cover Min Percentage : log . error ( 'TOTAL Coverage did not reach minimum ' 'required: %d%%' % self . cover Min Percentage ) sys . exit ( 1 ) else : log . error ( \"No total percentage was found in coverage output, \" \"something went wrong.\" )", "predictions": ["report the coverage modules ."], "references": ["output code coverage report ."], "bleu": 0.32466791547509893, "rouge_l": 0.4}
{"id": 6024, "code": "def open with auth ( url ) : scheme , netloc , path , params , query , frag = urlparse . urlparse ( url ) if netloc . endswith ( ':' ) : raise httplib . Invalid URL ( \"nonnumeric port: ''\" ) if scheme in ( 'http' , 'https' ) : auth , host = urllib2 . splituser ( netloc ) else : auth = None if auth : auth = \"Basic \" + encode auth ( auth ) new url = urlparse . urlunparse ( ( scheme , host , path , params , query , frag ) ) request = urllib2 . Request ( new url ) request . add header ( \"Authorization\" , auth ) else : request = urllib2 . Request ( url ) request . add header ( 'User-Agent' , user agent ) fp = urllib2 . urlopen ( request ) if auth : s2 , h2 , path2 , param2 , query2 , frag2 = urlparse . urlparse ( fp . url ) if s2 == scheme and h2 == host : fp . url = urlparse . urlunparse ( ( s2 , netloc , path2 , param2 , query2 , frag2 ) ) return fp", "predictions": ["open an http request with the given url ."], "references": ["open a urllib2 request handling http authentication"], "bleu": 0.16784459625186196, "rouge_l": 0.2557651991614256}
{"id": 6025, "code": "def get parent ( obj ) : names = obj . qualname . split ( '.' ) [ : - 1 ] if '<locals>' in names : raise Value Error ( 'cannot get parent from locals object.' ) module = sys . modules [ obj . module ] parent = module while names : parent = getattr ( parent , names . pop ( 0 ) ) return parent", "predictions": ["return the parent of the parent object ."], "references": ["get parent from obj ."], "bleu": 0.17747405280050269, "rouge_l": 0.32105263157894737}
{"id": 6026, "code": "def render template ( content , context ) : rendered = Template ( content ) . render ( Context ( context ) ) return rendered", "predictions": ["render a template with the given context"], "references": ["renders context aware template"], "bleu": 0.20556680845025982, "rouge_l": 0.19122257053291536}
{"id": 6027, "code": "def configure ( self , options , conf ) : self . conf = conf if not options . capture : self . enabled = False", "predictions": ["configure the configuration ."], "references": ["configure plugin . plugin is enabled by default ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 6028, "code": "def format Error ( self , test , err ) : test . captured Output = output = self . buffer self . buf = None if not output : return err ec , ev , tb = err return ( ec , self . add Capture To Err ( ev , output ) , tb )", "predictions": ["formats the error for printing ."], "references": ["add captured output to error report ."], "bleu": 0.20693220168471366, "rouge_l": 0.3034825870646766}
{"id": 6029, "code": "def split By ( data , num ) : return [ data [ i : i + num ] for i in range ( 0 , len ( data ) , num ) ]", "predictions": ["split data into chunks of num chars"], "references": ["turn a list to list of list"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 6030, "code": "def hex to rgb ( color ) : if color . startswith ( '#' ) : color = color [ 1 : ] if len ( color ) == 3 : color = '' . join ( [ c * 2 for c in color ] ) if len ( color ) != 6 : return False try : r = int ( color [ : 2 ] , 16 ) g = int ( color [ 2 : 4 ] , 16 ) b = int ( color [ 4 : ] , 16 ) except Value Error : return False else : return r , g , b", "predictions": ["convert hex color to rgb"], "references": ["convert a hex color to rgb integer tuple ."], "bleu": 0.3573381727496468, "rouge_l": 0.6792873051224945}
{"id": 6031, "code": "def handle complete reply ( self , rep ) : self . log . debug ( \"complete: %s\" , rep . get ( 'content' , '' ) ) cursor = self . get cursor ( ) info = self . request info . get ( 'complete' ) if info and info . id == rep [ 'parent header' ] [ 'msg id' ] and info . pos == cursor . position ( ) : matches = rep [ 'content' ] [ 'matches' ] text = rep [ 'content' ] [ 'matched text' ] offset = len ( text ) if len ( matches ) > 1 and matches [ 0 ] [ : offset ] == text : parts = re . split ( r'[./\\\\]' , text ) sep count = len ( parts ) - 1 if sep count : chop length = sum ( map ( len , parts [ : sep count ] ) ) + sep count matches = [ match [ chop length : ] for match in matches ] offset -= chop length cursor . move Position ( Qt Gui . Q Text Cursor . Left , n = offset ) self . complete with items ( cursor , matches )", "predictions": ["process incoming reply reply from server"], "references": ["reimplemented to support ipython s improved completion machinery ."], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 6032, "code": "def handle execute reply ( self , msg ) : msg id = msg [ 'parent header' ] . get ( 'msg id' ) info = self . request info [ 'execute' ] . get ( msg id ) if info and info . kind == 'prompt' : number = msg [ 'content' ] [ 'execution count' ] + 1 self . show interpreter prompt ( number ) self . request info [ 'execute' ] . pop ( msg id ) else : super ( I Python Widget , self ) . handle execute reply ( msg )", "predictions": ["handle a execute reply reply ."], "references": ["reimplemented to support prompt requests ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 6033, "code": "def handle pyout ( self , msg ) : self . log . debug ( \"pyout: %s\" , msg . get ( 'content' , '' ) ) if not self . hidden and self . is from this session ( msg ) : content = msg [ 'content' ] prompt number = content . get ( 'execution count' , 0 ) data = content [ 'data' ] if data . has key ( 'text/html' ) : self . append plain text ( self . output sep , True ) self . append html ( self . make out prompt ( prompt number ) , True ) html = data [ 'text/html' ] self . append plain text ( '\\n' , True ) self . append html ( html + self . output sep2 , True ) elif data . has key ( 'text/plain' ) : self . append plain text ( self . output sep , True ) self . append html ( self . make out prompt ( prompt number ) , True ) text = data [ 'text/plain' ] if \"\\n\" in text and not self . output sep . endswith ( \"\\n\" ) : self . append plain text ( '\\n' , True ) self . append plain text ( text + self . output sep2 , True )", "predictions": ["handle an hidden packet ."], "references": ["reimplemented for ipython - style display hook ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 6034, "code": "def handle display data ( self , msg ) : self . log . debug ( \"display: %s\" , msg . get ( 'content' , '' ) ) if not self . hidden and self . is from this session ( msg ) : source = msg [ 'content' ] [ 'source' ] data = msg [ 'content' ] [ 'data' ] metadata = msg [ 'content' ] [ 'metadata' ] if data . has key ( 'text/html' ) : html = data [ 'text/html' ] self . append html ( html , True ) elif data . has key ( 'text/plain' ) : text = data [ 'text/plain' ] self . append plain text ( text , True ) self . append plain text ( u'\\n' , True )", "predictions": ["process incoming data sent to the display ."], "references": ["the base handler for the display_data message ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 6035, "code": "def started channels ( self ) : super ( I Python Widget , self ) . started channels ( ) self . load guiref magic ( ) self . kernel manager . shell channel . history ( hist access type = 'tail' , n = 1000 )", "predictions": ["load all channels from the shell channel ."], "references": ["reimplemented to make a history request and load %guiref ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 6036, "code": "def execute file ( self , path , hidden = False ) : if sys . platform == 'win32' : path = os . path . normpath ( path ) . replace ( '\\\\' , '/' ) if ' ' in path or \"'\" in path or '\"' in path : path = '\"%s\"' % path . replace ( '\"' , '\\\\\"' ) self . execute ( '%%run %s' % path , hidden = hidden )", "predictions": ["execute a file in the given path ."], "references": ["reimplemented to use the run magic ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 6037, "code": "def complete ( self ) : text = '' msg id = self . kernel manager . shell channel . complete ( text , self . get input buffer cursor line ( ) , self . get input buffer cursor column ( ) , self . input buffer ) pos = self . get cursor ( ) . position ( ) info = self . Completion Request ( msg id , pos ) self . request info [ 'complete' ] = info", "predictions": ["process incoming kernel text from server"], "references": ["reimplemented to support ipython s improved completion machinery ."], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 6038, "code": "def process execute error ( self , msg ) : content = msg [ 'content' ] traceback = '\\n' . join ( content [ 'traceback' ] ) + '\\n' if False : traceback = traceback . replace ( ' ' , '&nbsp;' ) traceback = traceback . replace ( '\\n' , '<br/>' ) ename = content [ 'ename' ] ename styled = '<span class=\"error\">%s</span>' % ename traceback = traceback . replace ( ename , ename styled ) self . append html ( traceback ) else : self . append plain text ( traceback )", "predictions": ["add an error message to the execute error ."], "references": ["reimplemented for ipython - style traceback formatting ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 6039, "code": "def process execute payload ( self , item ) : handler = self . payload handlers . get ( item [ 'source' ] ) if handler is None : return False else : handler ( item ) return True", "predictions": ["process individual payload ."], "references": ["reimplemented to dispatch payloads to handler methods ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 6040, "code": "def show interpreter prompt ( self , number = None ) : if number is None : msg id = self . kernel manager . shell channel . execute ( '' , silent = True ) info = self . Execution Request ( msg id , 'prompt' ) self . request info [ 'execute' ] [ msg id ] = info return self . prompt sep = self . input sep self . show prompt ( self . make in prompt ( number ) , html = True ) block = self . control . document ( ) . last Block ( ) length = len ( self . prompt ) self . previous prompt obj = self . Prompt Block ( block , length , number ) self . set continuation prompt ( self . make continuation prompt ( self . prompt ) , html = True )", "predictions": ["show the prompt prompt ."], "references": ["reimplemented for ipython - style prompts ."], "bleu": 0.1830054742374001, "rouge_l": 0.16180371352785147}
{"id": 6041, "code": "def show interpreter prompt for reply ( self , msg ) : content = msg [ 'content' ] if content [ 'status' ] == 'aborted' : if self . previous prompt obj : previous prompt number = self . previous prompt obj . number else : previous prompt number = 0 else : previous prompt number = content [ 'execution count' ] if self . previous prompt obj and self . previous prompt obj . number != previous prompt number : block = self . previous prompt obj . block if block . is Valid ( ) and block . text ( ) : cursor = Qt Gui . Q Text Cursor ( block ) cursor . move Position ( Qt Gui . Q Text Cursor . Right , Qt Gui . Q Text Cursor . Keep Anchor , self . previous prompt obj . length ) prompt = self . make in prompt ( previous prompt number ) self . prompt = self . insert html fetching plain text ( cursor , prompt ) self . highlighter . rehighlight Block ( cursor . block ( ) ) self . previous prompt obj = None self . show interpreter prompt ( previous prompt number + 1 )", "predictions": ["show the prompt for the interpreter reply ."], "references": ["reimplemented for ipython - style prompts ."], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 6042, "code": "def make in prompt ( self , number ) : try : body = self . in prompt % number except Type Error : body = self . in prompt return '<span class=\"in-prompt\">%s</span>' % body", "predictions": ["make a prompt object from the prompt prompt ."], "references": ["given a prompt number returns an html in prompt ."], "bleu": 0.2090067144241744, "rouge_l": 0.41709401709401706}
{"id": 6043, "code": "def style sheet changed ( self ) : self . set Style Sheet ( self . style sheet ) if self . control is not None : self . control . document ( ) . set Default Style Sheet ( self . style sheet ) bg color = self . control . palette ( ) . window ( ) . color ( ) self . ansi processor . set background color ( bg color ) if self . page control is not None : self . page control . document ( ) . set Default Style Sheet ( self . style sheet )", "predictions": ["update or disable the ui sheet ."], "references": ["set the style sheets of the underlying widgets ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 6044, "code": "def syntax style changed ( self ) : if self . highlighter is None : return if self . syntax style : self . highlighter . set style ( self . syntax style ) else : self . highlighter . set style sheet ( self . style sheet )", "predictions": ["set the syntax style to the syntax style ."], "references": ["set the style for the syntax highlighter ."], "bleu": 0.25406637407730737, "rouge_l": 0.7134502923976607}
{"id": 6045, "code": "def virtual memory ( ) : mem = psutil bsd . get virtual mem ( ) total , free , active , inactive , wired , cached , buffers , shared = mem avail = inactive + cached + free used = active + wired + cached percent = usage percent ( ( total - avail ) , total , round = 1 ) return nt virtmem info ( total , avail , percent , used , free , active , inactive , buffers , cached , shared , wired )", "predictions": ["virtual memory memory usage for the memory ."], "references": ["system virtual memory as a namedutple ."], "bleu": 0.22679164443904004, "rouge_l": 0.4048672566371681}
{"id": 6046, "code": "def get system cpu times ( ) : user , nice , system , idle , irq = psutil bsd . get system cpu times ( ) return cputimes ntuple ( user , nice , system , idle , irq )", "predictions": ["returns tuple of cpu cpu times ."], "references": ["return system per - cpu times as a named tuple"], "bleu": 0.17112717058426782, "rouge_l": 0.22803738317757008}
{"id": 6047, "code": "def get system per cpu times ( ) : ret = [ ] for cpu t in psutil bsd . get system per cpu times ( ) : user , nice , system , idle , irq = cpu t item = cputimes ntuple ( user , nice , system , idle , irq ) ret . append ( item ) return ret", "predictions": ["return a list of system per cpu times"], "references": ["return system cpu times as a named tuple"], "bleu": 0.25098621243978964, "rouge_l": 0.5}
{"id": 6048, "code": "def get process uids ( self ) : real , effective , saved = psutil bsd . get process uids ( self . pid ) return nt uids ( real , effective , saved )", "predictions": ["opcode the uids for the uids"], "references": ["return real effective and saved user ids ."], "bleu": 0.13309610652103346, "rouge_l": 0.0}
{"id": 6049, "code": "def get process gids ( self ) : real , effective , saved = psutil bsd . get process gids ( self . pid ) return nt gids ( real , effective , saved )", "predictions": ["get the byte for the byte byte . . . . . . . . ."], "references": ["return real effective and saved group ids ."], "bleu": 0.07692375026049747, "rouge_l": 0.08866279069767442}
{"id": 6050, "code": "def get memory info ( self ) : rss , vms = psutil bsd . get process memory info ( self . pid ) [ : 2 ] return nt meminfo ( rss , vms )", "predictions": ["return line info information ."], "references": ["return a tuple with the process rss and vms size ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 6051, "code": "def get process threads ( self ) : rawlist = psutil bsd . get process threads ( self . pid ) retlist = [ ] for thread id , utime , stime in rawlist : ntuple = nt thread ( thread id , utime , stime ) retlist . append ( ntuple ) return retlist", "predictions": ["returns a list of stack repr by the stack"], "references": ["return the number of threads belonging to the process ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 6052, "code": "def get open files ( self ) : if hasattr ( psutil bsd , \"get process open files\" ) : rawlist = psutil bsd . get process open files ( self . pid ) return [ nt openfile ( path , fd ) for path , fd in rawlist ] else : lsof = psposix . Lsof Parser ( self . pid , self . process name ) return lsof . get process open files ( )", "predictions": ["return list of all files files assert assert assert"], "references": ["return files opened by process as a list of namedtuples ."], "bleu": 0.1689983564524028, "rouge_l": 0.2946859903381642}
{"id": 6053, "code": "def num cpus darwin ( ) : p = subprocess . Popen ( [ 'sysctl' , '-n' , 'hw.ncpu' ] , stdout = subprocess . PIPE ) return p . stdout . read ( )", "predictions": ["returns the number of cpus in the default group . ."], "references": ["return the number of active cpus on a darwin system ."], "bleu": 0.23462350320527994, "rouge_l": 0.45454545454545453}
{"id": 6054, "code": "def fetchone ( self ) : self . check executed ( ) r = self . fetch row ( 1 ) if not r : self . warning check ( ) return None self . rownumber = self . rownumber + 1 return r [ 0 ]", "predictions": ["fetch current row from queue log log log log log log log log log log log log log log log log log log log log log log log log log log"], "references": ["fetches a single row from the cursor ."], "bleu": 0.0513487742994337, "rouge_l": 0.1147695202257761}
{"id": 6055, "code": "def fetchall ( self ) : self . check executed ( ) r = self . fetch row ( 0 ) self . rownumber = self . rownumber + len ( r ) self . warning check ( ) return r", "predictions": ["fetch all rows from the pool . . . . . . . . . ."], "references": ["fetchs all available rows from the cursor ."], "bleu": 0.15844501337268932, "rouge_l": 0.4433139534883721}
{"id": 6056, "code": "def connect ( com , peers , tree , pub url , root id ) : com . connect ( peers , tree , pub url , root id )", "predictions": ["open a scheme query query query ."], "references": ["this function will be called on the engines"], "bleu": 0.13540372457315733, "rouge_l": 0.0}
{"id": 6057, "code": "def reads json ( s , * * kwargs ) : nbf , minor , d = parse json ( s , * * kwargs ) if nbf == 1 : nb = v1 . to notebook json ( d , * * kwargs ) nb = v3 . convert to this nbformat ( nb , orig version = 1 ) elif nbf == 2 : nb = v2 . to notebook json ( d , * * kwargs ) nb = v3 . convert to this nbformat ( nb , orig version = 2 ) elif nbf == 3 : nb = v3 . to notebook json ( d , * * kwargs ) nb = v3 . convert to this nbformat ( nb , orig version = 3 , orig minor = minor ) else : raise NB Format Error ( 'Unsupported JSON nbformat version: %i' % nbf ) return nb", "predictions": ["return a parent of the parent class raise a parent class"], "references": ["read a json notebook from a string and return the notebooknode object ."], "bleu": 0.11941964005964323, "rouge_l": 0.1641991924629879}
{"id": 6058, "code": "def reads py ( s , * * kwargs ) : nbf , nbm , s = parse py ( s , * * kwargs ) if nbf == 2 : nb = v2 . to notebook py ( s , * * kwargs ) elif nbf == 3 : nb = v3 . to notebook py ( s , * * kwargs ) else : raise NB Format Error ( 'Unsupported PY nbformat version: %i' % nbf ) return nb", "predictions": ["return a string from a python object . ."], "references": ["read a . py notebook from a string and return the notebooknode object ."], "bleu": 0.16679893712870122, "rouge_l": 0.41838134430727025}
{"id": 6059, "code": "def convert to metadata ( ) : import glob for fname in glob . glob ( '*.ipynb' ) : print ( 'Converting file:' , fname ) with open ( fname , 'r' ) as f : nb = read ( f , u'json' ) md = new metadata ( ) if u'name' in nb : md . name = nb . name del nb [ u'name' ] nb . metadata = md with open ( fname , 'w' ) as f : write ( nb , f , u'json' )", "predictions": ["configure the self - self - self - self not - self - 3 - self not - 1 - self - self not - self not - self not -"], "references": ["convert to a notebook having notebook metadata ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 6060, "code": "def want Function ( self , function ) : try : if hasattr ( function , 'compat func name' ) : funcname = function . compat func name else : funcname = function . name except Attribute Error : return False declared = getattr ( function , ' test ' , None ) if declared is not None : wanted = declared else : wanted = not funcname . startswith ( ' ' ) and self . matches ( funcname ) plug wants = self . plugins . want Function ( function ) if plug wants is not None : wanted = plug wants log . debug ( \"want Function %s? %s\" , function , wanted ) return wanted", "predictions": ["add a test method to the test method . . ."], "references": ["is the function a test function?"], "bleu": 0.16108992769687397, "rouge_l": 0.2484725050916497}
{"id": 6061, "code": "def want Method ( self , method ) : try : method name = method . name except Attribute Error : return False if method name . startswith ( ' ' ) : return False declared = getattr ( method , ' test ' , None ) if declared is not None : wanted = declared else : wanted = self . matches ( method name ) plug wants = self . plugins . want Method ( method ) if plug wants is not None : wanted = plug wants log . debug ( \"want Method %s? %s\" , method , wanted ) return wanted", "predictions": ["do a single num wanted + ."], "references": ["is the method a test method?"], "bleu": 0.18575057999133598, "rouge_l": 0.15601023017902813}
{"id": 6062, "code": "def list command pydb ( self , arg ) : filename , first , last = Old Pdb . parse list cmd ( self , arg ) if filename is not None : self . print list lines ( filename , first , last )", "predictions": ["hex hex to hex"], "references": ["list command to use if we have a newer pydb installed"], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 6063, "code": "def do pdef ( self , arg ) : namespaces = [ ( 'Locals' , self . curframe . f locals ) , ( 'Globals' , self . curframe . f globals ) ] self . shell . find line magic ( 'pdef' ) ( arg , namespaces = namespaces )", "predictions": ["complete the line of the line"], "references": ["the debugger interface to magic_pdef"], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 6064, "code": "def conversion factor ( from symbol , to symbol , date ) : from currency = Currency . objects . get ( symbol = from symbol ) try : from currency price = Currency Price . objects . get ( currency = from currency , date = date ) . mid price except Currency Price . Does Not Exist : print \"Cannot fetch prices for %s on %s\" % ( str ( from currency ) , str ( date ) ) return None to currency = Currency . objects . get ( symbol = to symbol ) try : to currency price = Currency Price . objects . get ( currency = to currency , date = date ) . mid price except Currency Price . Does Not Exist : print \"Cannot fetch prices for %s on %s\" % ( str ( to currency ) , str ( date ) ) return None return to currency price / from currency price", "predictions": ["interpreter number of number of number of number of number"], "references": ["generates a multiplying factor used to convert two currencies"], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 6065, "code": "def convert currency ( from symbol , to symbol , value , date ) : if from symbol == to symbol : return value factor = conversion factor ( from symbol , to symbol , date ) if type ( value ) == float : output = value * float ( factor ) elif type ( value ) == Decimal : output = Decimal ( format ( value * factor , '.%sf' % str ( PRICE PRECISION ) ) ) elif type ( value ) in [ np . float16 , np . float32 , np . float64 , np . float128 , np . float ] : output = float ( value ) * float ( factor ) else : output = None return output", "predictions": ["handle currency in currency"], "references": ["converts an amount of money from one currency to another on a specified date ."], "bleu": 0.022969543400575367, "rouge_l": 0.0953125}
{"id": 6066, "code": "def compute return ( self , start date , end date , rate = \"MID\" ) : if rate not in [ \"MID\" , \"ASK\" , \"BID\" ] : raise Value Error ( \"Unknown rate type (%s)- must be 'MID', 'ASK' or 'BID'\" % str ( rate ) ) if end date <= start date : raise Value Error ( \"End date must be on or after start date\" ) df = self . generate dataframe ( start date = start date , end date = end date ) start price = df . ix [ start date ] [ rate ] end price = df . ix [ end date ] [ rate ] currency return = ( end price / start price ) - 1.0 return currency return", "predictions": ["handle the display rate"], "references": ["compute the return of the currency between two dates"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 6067, "code": "def write connection file ( self ) : if os . path . basename ( self . connection file ) == self . connection file : cf = os . path . join ( self . profile dir . security dir , self . connection file ) else : cf = self . connection file write connection file ( cf , ip = self . ip , key = self . session . key , shell port = self . shell port , stdin port = self . stdin port , hb port = self . hb port , iopub port = self . iopub port ) self . full connection file = cf", "predictions": ["started the channels file to the channels"], "references": ["write connection info to json file"], "bleu": 0.20556680845025982, "rouge_l": 0.15601023017902813}
{"id": 6068, "code": "def init heartbeat ( self ) : hb ctx = zmq . Context ( ) self . heartbeat = Heartbeat ( hb ctx , ( self . ip , self . hb port ) ) self . hb port = self . heartbeat . port self . log . debug ( \"Heartbeat REP Channel on port: %i\" % self . hb port ) self . heartbeat . start ( ) self . log . critical ( \"To connect another client to this kernel, use:\" )", "predictions": ["execute the file and execute the file ."], "references": ["start the heart beating"], "bleu": 0.16036590969929357, "rouge_l": 0.17732558139534885}
{"id": 6069, "code": "def log connection info ( self ) : basename = os . path . basename ( self . connection file ) if basename == self . connection file or os . path . dirname ( self . connection file ) == self . profile dir . security dir : tail = basename if self . profile != 'default' : tail += \" --profile %s\" % self . profile else : tail = self . connection file self . log . critical ( \"--existing %s\" , tail ) self . ports = dict ( shell = self . shell port , iopub = self . iopub port , stdin = self . stdin port , hb = self . hb port )", "predictions": ["logs the connection information to the cursor ."], "references": ["display connection info and store ports"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 6070, "code": "def init session ( self ) : default secure ( self . config ) self . session = Session ( config = self . config , username = u'kernel' )", "predictions": ["process the execute execute execute execute the execute execute execute execute . ."], "references": ["create our session object"], "bleu": 0.08032276872815308, "rouge_l": 0.0}
{"id": 6071, "code": "def init io ( self ) : if self . outstream class : outstream factory = import item ( str ( self . outstream class ) ) sys . stdout = outstream factory ( self . session , self . iopub socket , u'stdout' ) sys . stderr = outstream factory ( self . session , self . iopub socket , u'stderr' ) if self . displayhook class : displayhook factory = import item ( str ( self . displayhook class ) ) sys . displayhook = displayhook factory ( self . session , self . iopub socket )", "predictions": ["setup the execute execute execute the outstream"], "references": ["redirect input streams and set a display hook ."], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 6072, "code": "def init kernel ( self ) : kernel factory = import item ( str ( self . kernel class ) ) self . kernel = kernel factory ( config = self . config , session = self . session , shell socket = self . shell socket , iopub socket = self . iopub socket , stdin socket = self . stdin socket , log = self . log ) self . kernel . record ports ( self . ports )", "predictions": ["show the interpreter interpreter"], "references": ["create the kernel object itself"], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 6073, "code": "def init connector ( self ) : self . using ssh = bool ( self . sshkey or self . sshserver ) if self . sshkey and not self . sshserver : self . sshserver = self . url . split ( '://' ) [ 1 ] . split ( ':' ) [ 0 ] if self . using ssh : if tunnel . try passwordless ssh ( self . sshserver , self . sshkey , self . paramiko ) : password = False else : password = getpass ( \"SSH Password for %s: \" % self . sshserver ) else : password = False def connect ( s , url ) : url = disambiguate url ( url , self . location ) if self . using ssh : self . log . debug ( \"Tunneling connection to %s via %s\" % ( url , self . sshserver ) ) return tunnel . tunnel connection ( s , url , self . sshserver , keyfile = self . sshkey , paramiko = self . paramiko , password = password , ) else : return s . connect ( url ) def maybe tunnel ( url ) : \"\"\"like connect, but don't complete the connection (for use by heartbeat)\"\"\" url = disambiguate url ( url , self . location ) if self . using ssh : self . log . debug ( \"Tunneling connection to %s via %s\" % ( url , self . sshserver ) ) url , tunnelobj = tunnel . open tunnel ( url , self . sshserver , keyfile = self . sshkey , paramiko = self . paramiko , password = password , ) return url return connect , maybe tunnel", "predictions": ["show the connection to the kafka server self self self self self self self self self self self self self self self self self self self self self self self self"], "references": ["construct connection function which handles tunnels ."], "bleu": 0.03901663112717908, "rouge_l": 0.05939629990262901}
{"id": 6074, "code": "def html to text ( content ) : text = None h2t = html2text . HTML2Text ( ) h2t . ignore links = False text = h2t . handle ( content ) return text", "predictions": ["convert make make make text to prompt"], "references": ["converts html content to plain text"], "bleu": 0.20556680845025982, "rouge_l": 0.15601023017902813}
{"id": 6075, "code": "def md to text ( content ) : text = None html = markdown . markdown ( content ) if html : text = html to text ( content ) return text", "predictions": ["convert style self to changed changed if necessary ."], "references": ["converts markdown content to text"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 6076, "code": "def domain to fqdn ( domain , proto = None ) : from . generic import get site proto proto = proto or get site proto ( ) fdqn = '{proto}://{domain}' . format ( proto = proto , domain = domain ) return fdqn", "predictions": ["transform a syntax syntax to a changed sheet ."], "references": ["returns a fully qualified app domain name"], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 6077, "code": "def options ( self , parser , env = os . environ ) : super ( Nose Exclude , self ) . options ( parser , env ) env dirs = [ ] if 'NOSE EXCLUDE DIRS' in env : exclude dirs = env . get ( 'NOSE EXCLUDE DIRS' , '' ) env dirs . extend ( exclude dirs . split ( ';' ) ) parser . add option ( \"--exclude-dir\" , action = \"append\" , dest = \"exclude dirs\" , default = env dirs , help = ) parser . add option ( \"--exclude-dir-file\" , type = \"string\" , dest = \"exclude dir file\" , default = env . get ( 'NOSE EXCLUDE DIRS FILE' , False ) , help = )", "predictions": ["add program virtual options bsd from the command line bsd bsd to the command line bsd"], "references": ["define the command line options for the plugin ."], "bleu": 0.15844501337268932, "rouge_l": 0.33701657458563533}
{"id": 6078, "code": "def configure ( self , options , conf ) : super ( Nose Exclude , self ) . configure ( options , conf ) self . exclude dirs = { } if options . exclude dir file : if not options . exclude dirs : options . exclude dirs = [ ] new dirs = self . load from file ( options . exclude dir file ) options . exclude dirs . extend ( new dirs ) if not options . exclude dirs : self . enabled = False return self . enabled = True root = os . getcwd ( ) log . debug ( 'cwd: %s' % root ) for exclude param in options . exclude dirs : for d in exclude param . split ( '\\n' ) : d = d . strip ( ) abs d = self . force to abspath ( d ) if abs d : self . exclude dirs [ abs d ] = True exclude str = \"excluding dirs: %s\" % \",\" . join ( self . exclude dirs . keys ( ) ) log . debug ( exclude str )", "predictions": ["get the configuration from the master"], "references": ["configure plugin based on command line options"], "bleu": 0.15723447135049806, "rouge_l": 0.0}
{"id": 6079, "code": "def want Directory ( self , dirname ) : if dirname in self . exclude dirs : log . debug ( \"excluded: %s\" % dirname ) return False else : return None", "predictions": ["returns true if directory is not ignored"], "references": ["check if directory is eligible for test discovery"], "bleu": 0.29969770769039067, "rouge_l": 0.3952483801295896}
{"id": 6080, "code": "def links to dynamic ( self , ext ) : libnames = dict . fromkeys ( [ lib . full name for lib in self . shlibs ] ) pkg = '.' . join ( ext . full name . split ( '.' ) [ : - 1 ] + [ '' ] ) for libname in ext . libraries : if pkg + libname in libnames : return True return False", "predictions": ["return the dynamic links to the dynamic name"], "references": ["return true if ext links to a dynamic lib in the same package"], "bleu": 0.13434323860909256, "rouge_l": 0.3652694610778443}
{"id": 6081, "code": "def append func ( self , func , * args , * * kwargs ) : wraped func = partial ( func , * args , * * kwargs ) self . append ( wraped func )", "predictions": ["append a function to the global callable ."], "references": ["append func with given arguments and keywords ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 6082, "code": "def insert func ( self , index , func , * args , * * kwargs ) : wraped func = partial ( func , * args , * * kwargs ) self . insert ( index , wraped func )", "predictions": ["insert a new instance of the collection ."], "references": ["insert func with given arguments and keywords ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 6083, "code": "def construct parser ( magic func ) : kwds = getattr ( magic func , 'argcmd kwds' , { } ) if 'description' not in kwds : kwds [ 'description' ] = getattr ( magic func , ' doc ' , None ) arg name = real name ( magic func ) parser = Magic Argument Parser ( arg name , * * kwds ) group = None for deco in magic func . decorators [ : : - 1 ] : result = deco . add to parser ( parser , group ) if result is not None : group = result help text = parser . format help ( ) if help text . startswith ( 'usage: ' ) : help text = help text . replace ( 'usage: ' , '%' , 1 ) else : help text = '%' + help text magic func . doc = help text return parser", "predictions": ["construct the parser for the magic command line ."], "references": ["construct an argument parser using the function decorations ."], "bleu": 0.17747405280050263, "rouge_l": 0.4444444444444444}
{"id": 6084, "code": "def real name ( magic func ) : magic name = magic func . name if magic name . startswith ( 'magic ' ) : magic name = magic name [ len ( 'magic ' ) : ] return getattr ( magic func , 'argcmd name' , magic name )", "predictions": ["returns the real name of the magic magic ."], "references": ["find the real name of the magic ."], "bleu": 0.6865890479690392, "rouge_l": 0.8323586744639376}
{"id": 6085, "code": "def add to parser ( self , parser , group ) : if group is not None : parser = group parser . add argument ( * self . args , * * self . kwds ) return None", "predictions": ["add a group to the parser ."], "references": ["add this object s information to the parser ."], "bleu": 0.40661103887968814, "rouge_l": 0.6112224448897796}
{"id": 6086, "code": "def add to parser ( self , parser , group ) : return parser . add argument group ( * self . args , * * self . kwds )", "predictions": ["add an argument to the parser ."], "references": ["add this object s information to the parser ."], "bleu": 0.40661103887968814, "rouge_l": 0.6112224448897796}
{"id": 6087, "code": "def highlight Block ( self , string ) : if not self . highlighting on : return current block = self . current Block ( ) string = self . frontend . get block plain text ( current block ) if current block . contains ( self . frontend . prompt pos ) : prompt = self . frontend . prompt else : prompt = self . frontend . continuation prompt if string . startswith ( prompt ) : self . current offset = len ( prompt ) string = string [ len ( prompt ) : ] super ( Frontend Highlighter , self ) . highlight Block ( string )", "predictions": ["highlight the prompt method"], "references": ["highlight a block of text . reimplemented to highlight selectively ."], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 6088, "code": "def rehighlight Block ( self , block ) : old = self . highlighting on self . highlighting on = True super ( Frontend Highlighter , self ) . rehighlight Block ( block ) self . highlighting on = old", "predictions": ["called when the highlighting is rehighlight ."], "references": ["reimplemented to temporarily enable highlighting if disabled ."], "bleu": 0.17820132316770915, "rouge_l": 0.2634989200863931}
{"id": 6089, "code": "def set Format ( self , start , count , format ) : start += self . current offset super ( Frontend Highlighter , self ) . set Format ( start , count , format )", "predictions": ["override method to set the value of the editor ."], "references": ["reimplemented to highlight selectively ."], "bleu": 0.13950796967929133, "rouge_l": 0.2837209302325582}
{"id": 6090, "code": "def copy ( self ) : if self . page control is not None and self . page control . has Focus ( ) : self . page control . copy ( ) elif self . control . has Focus ( ) : text = self . control . text Cursor ( ) . selection ( ) . to Plain Text ( ) if text : lines = map ( self . transform prompt , text . splitlines ( ) ) text = '\\n' . join ( lines ) Qt Gui . Q Application . clipboard ( ) . set Text ( text ) else : self . log . debug ( \"frontend widget : unknown copy target\" )", "predictions": ["copy text to clipboard"], "references": ["copy the currently selected text to the clipboard removing prompts ."], "bleu": 0.09336612728312643, "rouge_l": 0.49193548387096775}
{"id": 6091, "code": "def context menu make ( self , pos ) : menu = super ( Frontend Widget , self ) . context menu make ( pos ) for before action in menu . actions ( ) : if before action . shortcut ( ) . matches ( Qt Gui . Q Key Sequence . Paste ) == Qt Gui . Q Key Sequence . Exact Match : menu . insert Action ( before action , self . copy raw action ) break return menu", "predictions": ["create a new context menu ."], "references": ["reimplemented to add an action for raw copy ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 6092, "code": "def event filter console keypress ( self , event ) : key = event . key ( ) if self . control key down ( event . modifiers ( ) , include command = False ) : if key == Qt Core . Qt . Key C and self . executing : self . request interrupt kernel ( ) return True elif key == Qt Core . Qt . Key Period : self . request restart kernel ( ) return True elif not event . modifiers ( ) & Qt Core . Qt . Alt Modifier : if key == Qt Core . Qt . Key Backspace : col = self . get input buffer cursor column ( ) cursor = self . control . text Cursor ( ) if col > 3 and not cursor . has Selection ( ) : text = self . get input buffer cursor line ( ) [ : col ] if text . endswith ( '    ' ) and not text . strip ( ) : cursor . move Position ( Qt Gui . Q Text Cursor . Left , Qt Gui . Q Text Cursor . Keep Anchor , 4 ) cursor . remove Selected Text ( ) return True return super ( Frontend Widget , self ) . event filter console keypress ( event )", "predictions": ["allow user to select a specific event filter ."], "references": ["reimplemented for execution interruption and smart backspace ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 6093, "code": "def insert continuation prompt ( self , cursor ) : super ( Frontend Widget , self ) . insert continuation prompt ( cursor ) cursor . insert Text ( ' ' * self . input splitter . indent spaces )", "predictions": ["insert continuation prompt prompt ."], "references": ["reimplemented for auto - indentation ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 6094, "code": "def handle complete reply ( self , rep ) : self . log . debug ( \"complete: %s\" , rep . get ( 'content' , '' ) ) cursor = self . get cursor ( ) info = self . request info . get ( 'complete' ) if info and info . id == rep [ 'parent header' ] [ 'msg id' ] and info . pos == cursor . position ( ) : text = '.' . join ( self . get context ( ) ) cursor . move Position ( Qt Gui . Q Text Cursor . Left , n = len ( text ) ) self . complete with items ( cursor , rep [ 'content' ] [ 'matches' ] )", "predictions": ["process incoming reply from server"], "references": ["handle replies for tab completion ."], "bleu": 0.18796001821050234, "rouge_l": 0.0}
{"id": 6095, "code": "def handle execute reply ( self , msg ) : self . log . debug ( \"execute: %s\" , msg . get ( 'content' , '' ) ) msg id = msg [ 'parent header' ] [ 'msg id' ] info = self . request info [ 'execute' ] . get ( msg id ) self . reading = False if info and info . kind == 'user' and not self . hidden : self . kernel manager . sub channel . flush ( ) if self . ansi codes : self . ansi processor . reset sgr ( ) content = msg [ 'content' ] status = content [ 'status' ] if status == 'ok' : self . process execute ok ( msg ) elif status == 'error' : self . process execute error ( msg ) elif status == 'aborted' : self . process execute abort ( msg ) self . show interpreter prompt for reply ( msg ) self . executed . emit ( msg ) self . request info [ 'execute' ] . pop ( msg id ) elif info and info . kind == 'silent exec callback' and not self . hidden : self . handle exec callback ( msg ) self . request info [ 'execute' ] . pop ( msg id ) else : super ( Frontend Widget , self ) . handle execute reply ( msg )", "predictions": ["handle incoming reply messages from the service ."], "references": ["handles replies for code execution ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 6096, "code": "def handle input request ( self , msg ) : self . log . debug ( \"input: %s\" , msg . get ( 'content' , '' ) ) if self . hidden : raise Runtime Error ( 'Request for raw input during hidden execution.' ) self . kernel manager . sub channel . flush ( ) def callback ( line ) : self . kernel manager . stdin channel . input ( line ) if self . reading : self . log . debug ( \"Got second input request, assuming first was interrupted.\" ) self . reading = False self . readline ( msg [ 'content' ] [ 'prompt' ] , callback = callback )", "predictions": ["handle incoming input request ."], "references": ["handle requests for raw_input ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 6097, "code": "def handle kernel died ( self , since last heartbeat ) : self . log . debug ( \"kernel died: %s\" , since last heartbeat ) if self . custom restart : self . custom restart kernel died . emit ( since last heartbeat ) else : message = 'The kernel heartbeat has been inactive for %.2f ' 'seconds. Do you want to restart the kernel? You may ' 'first want to check the network connection.' % since last heartbeat self . restart kernel ( message , now = True )", "predictions": ["restart the kernel heartbeat heartbeat"], "references": ["handle the kernel s death by asking if the user wants to restart ."], "bleu": 0.06382147015463427, "rouge_l": 0.19395866454689983}
{"id": 6098, "code": "def handle object info reply ( self , rep ) : self . log . debug ( \"oinfo: %s\" , rep . get ( 'content' , '' ) ) cursor = self . get cursor ( ) info = self . request info . get ( 'call tip' ) if info and info . id == rep [ 'parent header' ] [ 'msg id' ] and info . pos == cursor . position ( ) : content = rep [ 'content' ] if content . get ( 'ismagic' , False ) : call info , doc = None , None else : call info , doc = call tip ( content , format call = True ) if call info or doc : self . call tip widget . show call info ( call info , doc )", "predictions": ["handle incoming object from server"], "references": ["handle replies for call tips ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 6099, "code": "def handle pyout ( self , msg ) : self . log . debug ( \"pyout: %s\" , msg . get ( 'content' , '' ) ) if not self . hidden and self . is from this session ( msg ) : text = msg [ 'content' ] [ 'data' ] self . append plain text ( text + '\\n' , before prompt = True )", "predictions": ["handle incoming hidden messages ."], "references": ["handle display hook output ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 6100, "code": "def handle stream ( self , msg ) : self . log . debug ( \"stream: %s\" , msg . get ( 'content' , '' ) ) if not self . hidden and self . is from this session ( msg ) : text = msg [ 'content' ] [ 'data' ] . expandtabs ( 8 ) self . append plain text ( text , before prompt = True ) self . control . move Cursor ( Qt Gui . Q Text Cursor . End )", "predictions": ["handle incoming stream ."], "references": ["handle stdout stderr and stdin ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 6101, "code": "def handle shutdown reply ( self , msg ) : self . log . debug ( \"shutdown: %s\" , msg . get ( 'content' , '' ) ) if not self . hidden and not self . is from this session ( msg ) : if self . local kernel : if not msg [ 'content' ] [ 'restart' ] : self . exit requested . emit ( self ) else : time . sleep ( 0.25 ) self . reset ( ) else : title = self . window ( ) . window Title ( ) if not msg [ 'content' ] [ 'restart' ] : reply = Qt Gui . Q Message Box . question ( self , title , \"Kernel has been shutdown permanently. \" \"Close the Console?\" , Qt Gui . Q Message Box . Yes , Qt Gui . Q Message Box . No ) if reply == Qt Gui . Q Message Box . Yes : self . exit requested . emit ( self ) else : reply = Qt Gui . Q Message Box . question ( self , title , \"Kernel has been reset. Clear the Console?\" , Qt Gui . Q Message Box . Yes , Qt Gui . Q Message Box . No ) if reply == Qt Gui . Q Message Box . Yes : time . sleep ( 0.25 ) self . reset ( )", "predictions": ["handle the shutdown of the shutdown message"], "references": ["handle shutdown signal only if from other console ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 6102, "code": "def restart kernel ( self , message , now = False ) : if self . custom restart : self . custom restart requested . emit ( ) elif self . kernel manager . has kernel : self . kernel manager . hb channel . pause ( ) if self . confirm restart : buttons = Qt Gui . Q Message Box . Yes | Qt Gui . Q Message Box . No result = Qt Gui . Q Message Box . question ( self , 'Restart kernel?' , message , buttons ) do restart = result == Qt Gui . Q Message Box . Yes else : do restart = True if do restart : try : self . kernel manager . restart kernel ( now = now ) except Runtime Error : self . append plain text ( 'Kernel started externally. ' 'Cannot restart.\\n' , before prompt = True ) else : self . reset ( ) else : self . kernel manager . hb channel . unpause ( ) else : self . append plain text ( 'Kernel process is either remote or ' 'unspecified. Cannot restart.\\n' , before prompt = True )", "predictions": ["restart kernel on remote process ."], "references": ["attempts to restart the running kernel ."], "bleu": 0.22236312185643822, "rouge_l": 0.45522388059701485}
{"id": 6103, "code": "def call tip ( self ) : if not self . enable calltips : return False cursor = self . get cursor ( ) cursor . move Position ( Qt Gui . Q Text Cursor . Left ) if cursor . document ( ) . character At ( cursor . position ( ) ) != '(' : return False context = self . get context ( cursor ) if not context : return False name = '.' . join ( context ) msg id = self . kernel manager . shell channel . object info ( name ) pos = self . get cursor ( ) . position ( ) self . request info [ 'call tip' ] = self . Call Tip Request ( msg id , pos ) return True", "predictions": ["call the kernel with the cursor ."], "references": ["shows a call tip if appropriate at the current cursor location ."], "bleu": 0.11434175042957104, "rouge_l": 0.40197693574958815}
{"id": 6104, "code": "def complete ( self ) : context = self . get context ( ) if context : msg id = self . kernel manager . shell channel . complete ( '.' . join ( context ) , self . get input buffer cursor line ( ) , self . get input buffer cursor column ( ) , self . input buffer ) pos = self . get cursor ( ) . position ( ) info = self . Completion Request ( msg id , pos ) self . request info [ 'complete' ] = info", "predictions": ["process incoming kernel buffer ."], "references": ["performs completion at the current cursor location ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 6105, "code": "def process execute error ( self , msg ) : content = msg [ 'content' ] if content [ 'ename' ] == 'System Exit' : keepkernel = content [ 'evalue' ] == '-k' or content [ 'evalue' ] == 'True' self . keep kernel on exit = keepkernel self . exit requested . emit ( self ) else : traceback = '' . join ( content [ 'traceback' ] ) self . append plain text ( traceback )", "predictions": ["process an error message"], "references": ["process a reply for an execution request that resulted in an error ."], "bleu": 0.05355679762998549, "rouge_l": 0.32218309859154926}
{"id": 6106, "code": "def process execute ok ( self , msg ) : payload = msg [ 'content' ] [ 'payload' ] for item in payload : if not self . process execute payload ( item ) : warning = 'Warning: received unknown payload of type %s' print ( warning % repr ( item [ 'source' ] ) )", "predictions": ["process ok message from message"], "references": ["process a reply for a successful execution request ."], "bleu": 0.12267223791558805, "rouge_l": 0.1358574610244989}
{"id": 6107, "code": "def generate ( self , * arg , * * kw ) : for p , meth in self . plugins : result = None try : result = meth ( * arg , * * kw ) if result is not None : for r in result : yield r except ( Keyboard Interrupt , System Exit ) : raise except : exc = sys . exc info ( ) yield Failure ( * exc ) continue", "predictions": ["generate the result of the given result ."], "references": ["call all plugins yielding each item in each non - none result ."], "bleu": 0.11296874775996037, "rouge_l": 0.18263473053892215}
{"id": 6108, "code": "def simple ( self , * arg , * * kw ) : for p , meth in self . plugins : result = meth ( * arg , * * kw ) if result is not None : return result", "predictions": ["return a list of plugins that have the given arg ."], "references": ["call all plugins returning the first non - none result ."], "bleu": 0.1354599427337814, "rouge_l": 0.2727272727272727}
{"id": 6109, "code": "def load Plugins ( self ) : from pkg resources import iter entry points loaded = { } for entry point , adapt in self . entry points : for ep in iter entry points ( entry point ) : if ep . name in loaded : continue loaded [ ep . name ] = True log . debug ( '%s load plugin %s' , self . class . name , ep ) try : plugcls = ep . load ( ) except Keyboard Interrupt : raise except Exception , e : warn ( \"Unable to load plugin %s: %s\" % ( ep , e ) , Runtime Warning ) continue if adapt : plug = adapt ( plugcls ( ) ) else : plug = plugcls ( ) self . add Plugin ( plug ) super ( Entry Point Plugin Manager , self ) . load Plugins ( )", "predictions": ["load the entry points from the plugin ."], "references": ["load plugins by iterating the nose . plugins entry point ."], "bleu": 0.13859150907108325, "rouge_l": 0.4093959731543625}
{"id": 6110, "code": "def load Plugins ( self ) : from nose . plugins import builtin for plug in builtin . plugins : self . add Plugin ( plug ( ) ) super ( Builtin Plugin Manager , self ) . load Plugins ( )", "predictions": ["load the plugins from the builtin plugins ."], "references": ["load plugins in nose . plugins . builtin"], "bleu": 0.25098621243978964, "rouge_l": 0.5}
{"id": 6111, "code": "def cleanup files ( self , bundle = False ) : logger . notify ( 'Cleaning up...' ) logger . indent += 2 for req in self . reqs to cleanup : req . remove temporary source ( ) remove dir = [ ] if self . pip has created build dir ( ) : remove dir . append ( self . build dir ) if bundle : remove dir . append ( self . src dir ) for dir in remove dir : if os . path . exists ( dir ) : logger . info ( 'Removing temporary dir %s...' % dir ) rmtree ( dir ) logger . indent -= 2", "predictions": ["delete all files in the build directory ."], "references": ["clean up files remove builds ."], "bleu": 0.17747405280050269, "rouge_l": 0.2932692307692307}
{"id": 6112, "code": "def name ( self ) : name = self . platform impl . get process name ( ) if os . name == 'posix' : try : cmdline = self . cmdline except Access Denied : pass else : if cmdline : extended name = os . path . basename ( cmdline [ 0 ] ) if extended name . startswith ( name ) : name = extended name self . platform impl . process name = name return name", "predictions": ["return links to the platform ext"], "references": ["the process name ."], "bleu": 0.22089591134157885, "rouge_l": 0.2074829931972789}
{"id": 6113, "code": "def exe ( self ) : def guess it ( fallback ) : cmdline = self . cmdline if cmdline and hasattr ( os , 'access' ) and hasattr ( os , 'X OK' ) : exe = cmdline [ 0 ] rexe = os . path . realpath ( exe ) if os . path . isabs ( rexe ) and os . path . isfile ( rexe ) and os . access ( rexe , os . X OK ) : return exe if isinstance ( fallback , Access Denied ) : raise fallback return fallback try : exe = self . platform impl . get process exe ( ) except Access Denied : err = sys . exc info ( ) [ 1 ] return guess it ( fallback = err ) else : if not exe : try : exe = guess it ( fallback = exe ) except Access Denied : pass return exe", "predictions": ["return directory containing the process or none if not found ."], "references": ["the process executable path . may also be an empty string ."], "bleu": 0.14709132836587344, "rouge_l": 0.25884016973125884}
{"id": 6114, "code": "def is running ( self ) : if self . gone : return False try : return self . create time == self . platform impl . get process create time ( ) except No Such Process : self . gone = True return False", "predictions": ["returns true if the process is func false otherwise ."], "references": ["return whether this process is running ."], "bleu": 0.17827531042796255, "rouge_l": 0.36454183266932266}
{"id": 6115, "code": "def suspend ( self ) : if not self . is running ( ) : name = self . platform impl . process name raise No Such Process ( self . pid , name ) if hasattr ( self . platform impl , \"suspend process\" ) : self . platform impl . suspend process ( ) else : self . send signal ( signal . SIGSTOP )", "predictions": ["construct the tray } } kwds kwds kwds kwds kwds kwds kwds kwds kwds kwds kwds kwds kwds ."], "references": ["suspend process execution ."], "bleu": 0.06439931429457924, "rouge_l": 0.09854604200323101}
{"id": 6116, "code": "def resume ( self ) : if not self . is running ( ) : name = self . platform impl . process name raise No Such Process ( self . pid , name ) if hasattr ( self . platform impl , \"resume process\" ) : self . platform impl . resume process ( ) else : self . send signal ( signal . SIGCONT )", "predictions": ["real - running this len = stopped = 0 = 0 = 0 = 1 = 0 = 1 = 1 = 0 = 0 = 1 = 1 = 1"], "references": ["resume process execution ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 6117, "code": "def kill ( self ) : if not self . is running ( ) : name = self . platform impl . process name raise No Such Process ( self . pid , name ) if os . name == 'posix' : self . send signal ( signal . SIGKILL ) else : self . platform impl . kill process ( )", "predictions": ["add the . ."], "references": ["kill the current process ."], "bleu": 0.3096787331587729, "rouge_l": 0.43571428571428567}
{"id": 6118, "code": "def init transformers ( self ) : self . transformers = [ ] for transformer cls in default transformers : transformer cls ( shell = self . shell , prefilter manager = self , config = self . config )", "predictions": ["initializes all to the kwds to be called from the shell"], "references": ["create the default transformers ."], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 6119, "code": "def register transformer ( self , transformer ) : if transformer not in self . transformers : self . transformers . append ( transformer ) self . sort transformers ( )", "predictions": ["highlight the given transformer ."], "references": ["register a transformer instance ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 6120, "code": "def unregister transformer ( self , transformer ) : if transformer in self . transformers : self . transformers . remove ( transformer )", "predictions": ["rehighlight the given transformer = true if it is not already present = false = false = false = false = false = false = false = false = false ="], "references": ["unregister a transformer instance ."], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 6121, "code": "def init checkers ( self ) : self . checkers = [ ] for checker in default checkers : checker ( shell = self . shell , prefilter manager = self , config = self . config )", "predictions": ["initialize checkers checkers checkers"], "references": ["create the default checkers ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 6122, "code": "def register checker ( self , checker ) : if checker not in self . checkers : self . checkers . append ( checker ) self . sort checkers ( )", "predictions": ["copy a checker to the checkers"], "references": ["register a checker instance ."], "bleu": 0.2907153684841096, "rouge_l": 0.3696969696969697}
{"id": 6123, "code": "def unregister checker ( self , checker ) : if checker in self . checkers : self . checkers . remove ( checker )", "predictions": ["context manager to context manager = false if it is not false"], "references": ["unregister a checker instance ."], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 6124, "code": "def init handlers ( self ) : self . handlers = { } self . esc handlers = { } for handler in default handlers : handler ( shell = self . shell , prefilter manager = self , config = self . config )", "predictions": ["initialize all filter filter filter ."], "references": ["create the default handlers ."], "bleu": 0.22089591134157885, "rouge_l": 0.18484848484848485}
{"id": 6125, "code": "def register handler ( self , name , handler , esc strings ) : self . handlers [ name ] = handler for esc str in esc strings : self . esc handlers [ esc str ] = handler", "predictions": ["insert a continuation continuation continuation continuation ."], "references": ["register a handler instance by name with esc_strings ."], "bleu": 0.15447878876032708, "rouge_l": 0.24448897795591182}
{"id": 6126, "code": "def unregister handler ( self , name , handler , esc strings ) : try : del self . handlers [ name ] except Key Error : pass for esc str in esc strings : h = self . esc handlers . get ( esc str ) if h is handler : del self . esc handlers [ esc str ]", "predictions": ["remove a complete complete complete complete complete complete complete . ."], "references": ["unregister a handler instance by name with esc_strings ."], "bleu": 0.12605968092174913, "rouge_l": 0.2036727879799666}
{"id": 6127, "code": "def find handler ( self , line info ) : for checker in self . checkers : if checker . enabled : handler = checker . check ( line info ) if handler : return handler return self . get handler by name ( 'normal' )", "predictions": ["handle a line . . . ."], "references": ["find a handler for the line_info by trying checkers ."], "bleu": 0.13391424795650428, "rouge_l": 0.22803738317757008}
{"id": 6128, "code": "def transform line ( self , line , continue prompt ) : for transformer in self . transformers : if transformer . enabled : line = transformer . transform ( line , continue prompt ) return line", "predictions": ["handle a single input input input . . . . . . . . . . . ."], "references": ["calls the enabled transformers in order of increasing priority ."], "bleu": 0.06809398432036522, "rouge_l": 0.07530864197530865}
{"id": 6129, "code": "def check ( self , line info ) : obj = self . shell . user ns . get ( line info . ifun , None ) if isinstance ( obj , I Py Autocall ) : obj . set ip ( self . shell ) return self . prefilter manager . get handler by name ( 'auto' ) else : return None", "predictions": ["handle the has been has been has been has the has the has been has the has been has been has been has the has been has been has been has"], "references": ["instances of ipyautocall in user_ns get autocalled immediately"], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 6130, "code": "def check ( self , line info ) : if line info . continue prompt and self . prefilter manager . multi line specials : if line info . esc == ESC MAGIC : return self . prefilter manager . get handler by name ( 'magic' ) else : return None", "predictions": ["handle the prefilter s prefilter . ."], "references": ["allow ! and !! in multi - line statements if multi_line_specials is on"], "bleu": 0.06628576403773602, "rouge_l": 0.0}
{"id": 6131, "code": "def check ( self , line info ) : head = line info . ifun . split ( '.' , 1 ) [ 0 ] if line info . ifun not in self . shell . alias manager or head not in self . shell . alias manager or is shadowed ( head , self . shell ) : return None return self . prefilter manager . get handler by name ( 'alias' )", "predictions": ["handle the from the from the from the from from the from the from the from the from the from the from the from the from the from the from the"], "references": ["check if the initital identifier on the line is an alias ."], "bleu": 0.04317900023606586, "rouge_l": 0.10107705053852528}
{"id": 6132, "code": "def handle ( self , line info ) : line = line info . line continue prompt = line info . continue prompt if ( continue prompt and self . shell . autoindent and line . isspace ( ) and 0 < abs ( len ( line ) - self . shell . indent current nsp ) <= 2 ) : line = '' return line", "predictions": ["handle a prompt command log ."], "references": ["handle normal input lines . use as a template for handlers ."], "bleu": 0.09663861439684919, "rouge_l": 0.31443298969072164}
{"id": 6133, "code": "def handle ( self , line info ) : transformed = self . shell . alias manager . expand aliases ( line info . ifun , line info . the rest ) line out = '%sget ipython().system(%r)' % ( line info . pre whitespace , transformed ) return line out", "predictions": ["handle the aliases command log log log log log log log log log log log log log log log log log log log log log log session log log log log"], "references": ["handle alias input lines ."], "bleu": 0.03901663112717908, "rouge_l": 0.06387434554973821}
{"id": 6134, "code": "def handle ( self , line info ) : magic handler = self . prefilter manager . get handler by name ( 'magic' ) line = line info . line if line . lstrip ( ) . startswith ( ESC SH CAP ) : new rest = line . lstrip ( ) [ 2 : ] line info . line = '%ssx %s' % ( ESC MAGIC , new rest ) line info . ifun = 'sx' line info . the rest = new rest return magic handler . handle ( line info ) else : cmd = line . lstrip ( ) . lstrip ( ESC SHELL ) line out = '%sget ipython().system(%r)' % ( line info . pre whitespace , cmd ) return line out", "predictions": ["process incoming magic if it is not already received if not ."], "references": ["execute the line in a shell empty return value"], "bleu": 0.08737167851715875, "rouge_l": 0.0}
{"id": 6135, "code": "def handle ( self , line info ) : ifun = line info . ifun the rest = line info . the rest cmd = '%sget ipython().magic(%r)' % ( line info . pre whitespace , ( ifun + \" \" + the rest ) ) return cmd", "predictions": ["call return command ."], "references": ["execute magic functions ."], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 6136, "code": "def handle ( self , line info ) : line = line info . line ifun = line info . ifun the rest = line info . the rest pre = line info . pre esc = line info . esc continue prompt = line info . continue prompt obj = line info . ofind ( self . shell ) [ 'obj' ] if continue prompt : return line force auto = isinstance ( obj , I Py Autocall ) try : auto rewrite = obj . rewrite except Exception : auto rewrite = True if esc == ESC QUOTE : newcmd = '%s(\"%s\")' % ( ifun , '\", \"' . join ( the rest . split ( ) ) ) elif esc == ESC QUOTE2 : newcmd = '%s(\"%s\")' % ( ifun , the rest ) elif esc == ESC PAREN : newcmd = '%s(%s)' % ( ifun , \",\" . join ( the rest . split ( ) ) ) else : if force auto : do rewrite = not the rest . startswith ( '(' ) else : if not the rest : do rewrite = ( self . shell . autocall >= 2 ) elif the rest . startswith ( '[' ) and hasattr ( obj , ' getitem ' ) : do rewrite = False else : do rewrite = True if do rewrite : if the rest . endswith ( ';' ) : newcmd = '%s(%s);' % ( ifun . rstrip ( ) , the rest [ : - 1 ] ) else : newcmd = '%s(%s)' % ( ifun . rstrip ( ) , the rest ) else : normal handler = self . prefilter manager . get handler by name ( 'normal' ) return normal handler . handle ( line info ) if auto rewrite : self . shell . auto rewrite input ( newcmd ) return newcmd", "predictions": ["handles the kernel of the kernel if it is not already not already not possible if not ."], "references": ["handle lines which can be auto - executed quoting if requested ."], "bleu": 0.07535838128770536, "rouge_l": 0.13832199546485258}
{"id": 6137, "code": "def enter Event ( self , event ) : super ( Call Tip Widget , self ) . enter Event ( event ) self . hide timer . stop ( )", "predictions": ["override qt method to allow user to enter the widget ."], "references": ["reimplemented to cancel the hide timer ."], "bleu": 0.1354599427337814, "rouge_l": 0.3472485768500949}
{"id": 6138, "code": "def paint Event ( self , event ) : painter = Qt Gui . Q Style Painter ( self ) option = Qt Gui . Q Style Option Frame ( ) option . init From ( self ) painter . draw Primitive ( Qt Gui . Q Style . PE Panel Tip Label , option ) painter . end ( ) super ( Call Tip Widget , self ) . paint Event ( event )", "predictions": ["override qt method to handle custom control control control options . ."], "references": ["reimplemented to paint the background panel ."], "bleu": 0.11498759556447223, "rouge_l": 0.22101449275362317}
{"id": 6139, "code": "def show tip ( self , tip ) : text edit = self . text edit document = text edit . document ( ) cursor = text edit . text Cursor ( ) search pos = cursor . position ( ) - 1 self . start position , = self . find parenthesis ( search pos , forward = False ) if self . start position == - 1 : return False self . set Text ( tip ) self . resize ( self . size Hint ( ) ) padding = 3 cursor rect = text edit . cursor Rect ( cursor ) screen rect = Qt Gui . q App . desktop ( ) . screen Geometry ( text edit ) point = text edit . map To Global ( cursor rect . bottom Right ( ) ) point . set Y ( point . y ( ) + padding ) tip height = self . size ( ) . height ( ) tip width = self . size ( ) . width ( ) vertical = 'bottom' horizontal = 'Right' if point . y ( ) + tip height > screen rect . height ( ) : point = text edit . map To Global ( cursor rect . top Right ( ) ) if point . y ( ) - tip height < padding : if 2 * point . y ( ) < screen rect . height ( ) : vertical = 'bottom' else : vertical = 'top' else : vertical = 'top' if point . x ( ) + tip width > screen rect . width ( ) : point = text edit . map To Global ( cursor rect . top Right ( ) ) if point . x ( ) - tip width < padding : if 2 * point . x ( ) < screen rect . width ( ) : horizontal = 'Right' else : horizontal = 'Left' else : horizontal = 'Left' pos = getattr ( cursor rect , '%s%s' % ( vertical , horizontal ) ) point = text edit . map To Global ( pos ( ) ) if vertical == 'top' : point . set Y ( point . y ( ) - tip height - padding ) if horizontal == 'Left' : point . set X ( point . x ( ) - tip width - padding ) self . move ( point ) self . show ( ) return True", "predictions": ["generate a tip edit for the text text p p ."], "references": ["attempts to show the specified tip at the current cursor location ."], "bleu": 0.12368857073777001, "rouge_l": 0.25884016973125884}
{"id": 6140, "code": "def cursor position changed ( self ) : cursor = self . text edit . text Cursor ( ) if cursor . position ( ) <= self . start position : self . hide ( ) else : position , commas = self . find parenthesis ( self . start position + 1 ) if position != - 1 : self . hide ( )", "predictions": ["simple method to control simple simple simple simple simple simple simple simple simple simple simple simple simple simple simple simple movement movement ."], "references": ["updates the tip based on user cursor movement ."], "bleu": 0.06964541799727335, "rouge_l": 0.13570634037819798}
{"id": 6141, "code": "def read ( * paths ) : with open ( os . path . join ( * paths ) , 'r' ) as file handler : return file handler . read ( )", "predictions": ["load the contents of a file import into a string import it import import files import"], "references": ["build a file path from * paths * and return the contents ."], "bleu": 0.12729922658368398, "rouge_l": 0.14055299539170507}
{"id": 6142, "code": "def virtualenv no global ( ) : #this mirrors the logic in virtualenv.py for locating the no-global-site-packages.txt file site mod dir = os . path . dirname ( os . path . abspath ( site . file ) ) no global file = os . path . join ( site mod dir , 'no-global-site-packages.txt' ) if running under virtualenv ( ) and os . path . isfile ( no global file ) : return True", "predictions": ["check if we need to global"], "references": ["return true if in a venv and no system site packages ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 6143, "code": "def default aliases ( ) : if os . name == 'posix' : default aliases = [ ( 'mkdir' , 'mkdir' ) , ( 'rmdir' , 'rmdir' ) , ( 'mv' , 'mv -i' ) , ( 'rm' , 'rm -i' ) , ( 'cp' , 'cp -i' ) , ( 'cat' , 'cat' ) , ] if sys . platform . startswith ( 'linux' ) : ls aliases = [ ( 'ls' , 'ls -F --color' ) , ( 'll' , 'ls -F -o --color' ) , ( 'lf' , 'ls -F -o --color %l | grep ^-' ) , ( 'lk' , 'ls -F -o --color %l | grep ^l' ) , ( 'ldir' , 'ls -F -o --color %l | grep /$' ) , ( 'lx' , 'ls -F -o --color %l | grep ^-..x' ) , ] else : ls aliases = [ ( 'ls' , 'ls -F' ) , ( 'll' , 'ls -F -l' ) , ( 'lf' , 'ls -F -l %l | grep ^-' ) , ( 'lk' , 'ls -F -l %l | grep ^l' ) , ( 'ldir' , 'ls -F -l %l | grep /$' ) , ( 'lx' , 'ls -F -l %l | grep ^-..x' ) , ] default aliases = default aliases + ls aliases elif os . name in [ 'nt' , 'dos' ] : default aliases = [ ( 'ls' , 'dir /on' ) , ( 'ddir' , 'dir /ad /on' ) , ( 'ldir' , 'dir /ad /on' ) , ( 'mkdir' , 'mkdir' ) , ( 'rmdir' , 'rmdir' ) , ( 'echo' , 'echo' ) , ( 'ren' , 'ren' ) , ( 'copy' , 'copy' ) , ] else : default aliases = [ ] return default aliases", "predictions": ["cleanup files for grep files . ."], "references": ["return list of shell aliases to auto - define ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 6144, "code": "def soft define alias ( self , name , cmd ) : try : self . define alias ( name , cmd ) except Alias Error , e : error ( \"Invalid alias: %s\" % e )", "predictions": ["set define alias ."], "references": ["define an alias but don t raise on an aliaserror ."], "bleu": 0.07425134808660917, "rouge_l": 0.3689516129032258}
{"id": 6145, "code": "def validate alias ( self , name , cmd ) : if name in self . no alias : raise Invalid Alias Error ( \"The name %s can't be aliased \" \"because it is a keyword or builtin.\" % name ) if not ( isinstance ( cmd , basestring ) ) : raise Invalid Alias Error ( \"An alias command must be a string, \" \"got: %r\" % cmd ) nargs = cmd . count ( '%s' ) if nargs > 0 and cmd . find ( '%l' ) >= 0 : raise Invalid Alias Error ( 'The %s and %l specifiers are mutually ' 'exclusive in alias definitions.' ) return nargs", "predictions": ["validates that the alias is valid ."], "references": ["validate an alias and return the its number of arguments ."], "bleu": 0.1247439242120089, "rouge_l": 0.2136602451838879}
{"id": 6146, "code": "def call alias ( self , alias , rest = '' ) : cmd = self . transform alias ( alias , rest ) try : self . shell . system ( cmd ) except : self . shell . showtraceback ( )", "predictions": ["calls the shell command with the shell ."], "references": ["call an alias given its name and the rest of the line ."], "bleu": 0.10207878682119532, "rouge_l": 0.2739520958083832}
{"id": 6147, "code": "def transform alias ( self , alias , rest = '' ) : nargs , cmd = self . alias table [ alias ] if ' ' in cmd and os . path . isfile ( cmd ) : cmd = '\"%s\"' % cmd if cmd . find ( '%l' ) >= 0 : cmd = cmd . replace ( '%l' , rest ) rest = '' if nargs == 0 : cmd = '%s %s' % ( cmd , rest ) else : args = rest . split ( None , nargs ) if len ( args ) < nargs : raise Alias Error ( 'Alias <%s> requires %s arguments, %s given.' % ( alias , nargs , len ( args ) ) ) cmd = '%s %s' % ( cmd % tuple ( args [ : nargs ] ) , ' ' . join ( args [ nargs : ] ) ) return cmd", "predictions": ["transform alias to alias ."], "references": ["transform alias to system command string ."], "bleu": 0.36015288308423526, "rouge_l": 0.6472148541114059}
{"id": 6148, "code": "def autohelp directive ( dirname , arguments , options , content , lineno , content offset , block text , state , state machine ) : config = Config ( parser Class = Opt Bucket , plugins = Builtin Plugin Manager ( ) ) parser = config . get Parser ( Test Program . usage ( ) ) rst = View List ( ) for line in parser . format help ( ) . split ( '\\n' ) : rst . append ( line , '<autodoc>' ) rst . append ( 'Options' , '<autodoc>' ) rst . append ( '-------' , '<autodoc>' ) rst . append ( '' , '<autodoc>' ) for opt in parser : rst . append ( opt . options ( ) , '<autodoc>' ) rst . append ( '   \\n' , '<autodoc>' ) rst . append ( '   ' + opt . help + '\\n' , '<autodoc>' ) rst . append ( '\\n' , '<autodoc>' ) node = nodes . section ( ) node . document = state . document surrounding title styles = state . memo . title styles surrounding section level = state . memo . section level state . memo . title styles = [ ] state . memo . section level = 0 state . nested parse ( rst , 0 , node , match titles = 1 ) state . memo . title styles = surrounding title styles state . memo . section level = surrounding section level return node . children", "predictions": ["create a autohelp directive"], "references": ["produces rst from nose help"], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 6149, "code": "def reset sgr ( self ) : self . intensity = 0 self . italic = False self . bold = False self . underline = False self . foreground color = None self . background color = None", "predictions": ["resets the sgr to its initial state ."], "references": ["reset graphics attributs to their default values ."], "bleu": 0.17747405280050269, "rouge_l": 0.25}
{"id": 6150, "code": "def split string ( self , string ) : self . actions = [ ] start = 0 last char = '\\n' if len ( string ) > 0 and string [ - 1 ] == '\\n' else None string = string [ : - 1 ] if last char is not None else string for match in ANSI OR SPECIAL PATTERN . finditer ( string ) : raw = string [ start : match . start ( ) ] substring = SPECIAL PATTERN . sub ( self . replace special , raw ) if substring or self . actions : yield substring self . actions = [ ] start = match . end ( ) groups = filter ( lambda x : x is not None , match . groups ( ) ) g0 = groups [ 0 ] if g0 == '\\a' : self . actions . append ( Beep Action ( 'beep' ) ) yield None self . actions = [ ] elif g0 == '\\r' : self . actions . append ( Carriage Return Action ( 'carriage-return' ) ) yield None self . actions = [ ] elif g0 == '\\b' : self . actions . append ( Back Space Action ( 'backspace' ) ) yield None self . actions = [ ] elif g0 == '\\n' or g0 == '\\r\\n' : self . actions . append ( New Line Action ( 'newline' ) ) yield g0 self . actions = [ ] else : params = [ param for param in groups [ 1 ] . split ( ';' ) if param ] if g0 . startswith ( '[' ) : try : params = map ( int , params ) except Value Error : pass else : self . set csi code ( groups [ 2 ] , params ) elif g0 . startswith ( ']' ) : self . set osc code ( params ) raw = string [ start : ] substring = SPECIAL PATTERN . sub ( self . replace special , raw ) if substring or self . actions : yield substring if last char is not None : self . actions . append ( New Line Action ( 'newline' ) ) yield last char", "predictions": ["split a string into a string ."], "references": ["yields substrings for which the same escape code applies ."], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 6151, "code": "def get format ( self ) : format = Qt Gui . Q Text Char Format ( ) qcolor = self . get color ( self . foreground color , self . intensity ) if qcolor is not None : format . set Foreground ( qcolor ) qcolor = self . get color ( self . background color , self . intensity ) if qcolor is not None : format . set Background ( qcolor ) if self . bold : format . set Font Weight ( Qt Gui . Q Font . Bold ) else : format . set Font Weight ( Qt Gui . Q Font . Normal ) format . set Font Italic ( self . italic ) format . set Font Underline ( self . underline ) return format", "predictions": ["returns the format for this message ."], "references": ["returns a qtextcharformat that encodes the current style attributes ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 6152, "code": "def generate ( secret , age , * * payload ) : jti = str ( uuid . uuid1 ( ) ) if not payload : payload = { } payload [ 'exp' ] = int ( time . time ( ) + age ) payload [ 'jti' ] = jti return jwt . encode ( payload , decode secret ( secret ) )", "predictions": ["generate a jwt payload from the given age ."], "references": ["generate a one - time jwt with an age in seconds"], "bleu": 0.1689983564524028, "rouge_l": 0.3929146537842191}
{"id": 6153, "code": "def mutex ( func ) : def wrapper ( * args , * * kwargs ) : \"\"\"Decorator Wrapper\"\"\" lock = args [ 0 ] . lock lock . acquire ( True ) try : return func ( * args , * * kwargs ) except : raise finally : lock . release ( ) return wrapper", "predictions": ["decorator to make a lock ."], "references": ["use a thread lock on current method if self . lock is defined"], "bleu": 0.08180282100568384, "rouge_l": 0.29611650485436897}
{"id": 6154, "code": "def clean ( self ) : now = time . time ( ) for jwt in self . jwts . keys ( ) : if ( now - self . jwts [ jwt ] ) > ( self . age * 2 ) : del self . jwts [ jwt ]", "predictions": ["clear the jwt of the jwt ."], "references": ["run by housekeeper thread"], "bleu": 0.1561969968460128, "rouge_l": 0.0}
{"id": 6155, "code": "def already used ( self , tok ) : if tok in self . jwts : return True self . jwts [ tok ] = time . time ( ) return False", "predictions": ["check if the given tok is used in the lock ."], "references": ["has this jwt been used?"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 6156, "code": "def valid ( self , token ) : now = time . time ( ) if 'Bearer ' in token : token = token [ 7 : ] data = None for secret in self . secrets : try : data = jwt . decode ( token , secret ) break except jwt . Decode Error : continue except jwt . Expired Signature Error : raise Jwt Failed ( \"Jwt expired\" ) if not data : raise Jwt Failed ( \"Jwt cannot be decoded\" ) exp = data . get ( 'exp' ) if not exp : raise Jwt Failed ( \"Jwt missing expiration (exp)\" ) if now - exp > self . age : raise Jwt Failed ( \"Jwt bad expiration - greater than I want to accept\" ) jti = data . get ( 'jti' ) if not jti : raise Jwt Failed ( \"Jwt missing one-time id (jti)\" ) if self . already used ( jti ) : raise Jwt Failed ( \"Jwt re-use disallowed (jti={})\" . format ( jti ) ) return data", "predictions": ["check if a jwt token is valid ."], "references": ["is this token valid?"], "bleu": 0.17747405280050269, "rouge_l": 0.17732558139534885}
{"id": 6157, "code": "def write ( self , nb , fp , * * kwargs ) : return fp . write ( self . writes ( nb , * * kwargs ) )", "predictions": ["write an object to a file - like object ."], "references": ["write a notebook to a file like object"], "bleu": 0.28997844147152074, "rouge_l": 0.6802973977695167}
{"id": 6158, "code": "def can cut ( self ) : cursor = self . control . text Cursor ( ) return ( cursor . has Selection ( ) and self . in buffer ( cursor . anchor ( ) ) and self . in buffer ( cursor . position ( ) ) )", "predictions": ["return whether the cursor is cut cut ."], "references": ["returns whether text can be cut to the clipboard ."], "bleu": 0.157044754112095, "rouge_l": 0.3267857142857143}
{"id": 6159, "code": "def can paste ( self ) : if self . control . text Interaction Flags ( ) & Qt Core . Qt . Text Editable : return bool ( Qt Gui . Q Application . clipboard ( ) . text ( ) ) return False", "predictions": ["returns true if the clipboard is paste false otherwise ."], "references": ["returns whether text can be pasted from the clipboard ."], "bleu": 0.18850319022747347, "rouge_l": 0.4}
{"id": 6160, "code": "def set font ( self , font ) : font metrics = Qt Gui . Q Font Metrics ( font ) self . control . set Tab Stop Width ( self . tab width * font metrics . width ( ' ' ) ) self . completion widget . set Font ( font ) self . control . document ( ) . set Default Font ( font ) if self . page control : self . page control . document ( ) . set Default Font ( font ) self . font changed . emit ( font )", "predictions": ["set font size in panel mode"], "references": ["sets the base font for the consolewidget to the specified qfont ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 6161, "code": "def print ( self , printer = None ) : if ( not printer ) : printer = Qt Gui . Q Printer ( ) if ( Qt Gui . Q Print Dialog ( printer ) . exec ( ) != Qt Gui . Q Dialog . Accepted ) : return self . control . print ( printer )", "predictions": ["prints the control for this control ."], "references": ["print the contents of the consolewidget to the specified qprinter ."], "bleu": 0.1160873020151595, "rouge_l": 0.2136602451838879}
{"id": 6162, "code": "def prompt to top ( self ) : if not self . executing : prompt cursor = self . get prompt cursor ( ) if self . get cursor ( ) . block Number ( ) < prompt cursor . block Number ( ) : self . set cursor ( prompt cursor ) self . set top cursor ( prompt cursor )", "predictions": ["prompt the user to top of the top cursor ."], "references": ["moves the prompt to the top of the viewport ."], "bleu": 0.2998221389342337, "rouge_l": 0.6}
{"id": 6163, "code": "def reset font ( self ) : if sys . platform == 'win32' : fallback = 'Courier' elif sys . platform == 'darwin' : fallback = 'Monaco' else : fallback = 'Monospace' font = get font ( self . font family , fallback ) if self . font size : font . set Point Size ( self . font size ) else : font . set Point Size ( Qt Gui . q App . font ( ) . point Size ( ) ) font . set Style Hint ( Qt Gui . Q Font . Type Writer ) self . set font ( font )", "predictions": ["reset the font to its initial state ."], "references": ["sets the font to the default fixed - width font for this platform ."], "bleu": 0.14907815372447217, "rouge_l": 0.346590909090909}
{"id": 6164, "code": "def append html ( self , html , before prompt = False ) : self . append custom ( self . insert html , html , before prompt )", "predictions": ["insert an html document into the html list ."], "references": ["appends html at the end of the console buffer ."], "bleu": 0.15019394384099988, "rouge_l": 0.31282051282051276}
{"id": 6165, "code": "def append html fetching plain text ( self , html , before prompt = False ) : return self . append custom ( self . insert html fetching plain text , html , before prompt )", "predictions": ["append html text to plain plain text ."], "references": ["appends html then returns the plain text version of it ."], "bleu": 0.16481400866629634, "rouge_l": 0.4093959731543625}
{"id": 6166, "code": "def append plain text ( self , text , before prompt = False ) : self . append custom ( self . insert plain text , text , before prompt )", "predictions": ["append a plain text to the plain text ."], "references": ["appends plain text processing ansi codes if enabled ."], "bleu": 0.19960198807747329, "rouge_l": 0.3333333333333333}
{"id": 6167, "code": "def complete with items ( self , cursor , items ) : self . cancel completion ( ) if len ( items ) == 1 : cursor . set Position ( self . control . text Cursor ( ) . position ( ) , Qt Gui . Q Text Cursor . Keep Anchor ) cursor . insert Text ( items [ 0 ] ) elif len ( items ) > 1 : current pos = self . control . text Cursor ( ) . position ( ) prefix = commonprefix ( items ) if prefix : cursor . set Position ( current pos , Qt Gui . Q Text Cursor . Keep Anchor ) cursor . insert Text ( prefix ) current pos = cursor . position ( ) cursor . move Position ( Qt Gui . Q Text Cursor . Left , n = len ( prefix ) ) self . completion widget . show items ( cursor , items )", "predictions": ["complete text with items in cursor"], "references": ["performs completion with items at the specified cursor location ."], "bleu": 0.16038842424444547, "rouge_l": 0.3588235294117647}
{"id": 6168, "code": "def fill temporary buffer ( self , cursor , text , html = False ) : current pos = self . control . text Cursor ( ) . position ( ) cursor . begin Edit Block ( ) self . append plain text ( '\\n' ) self . page ( text , html = html ) cursor . end Edit Block ( ) cursor . set Position ( current pos ) self . control . move Cursor ( Qt Gui . Q Text Cursor . End ) self . control . set Text Cursor ( cursor ) self . temp buffer filled = True", "predictions": ["fill temporary buffer with cursor ."], "references": ["fill the area below the active editting zone with text"], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 6169, "code": "def create control ( self ) : if self . custom control : control = self . custom control ( ) elif self . kind == 'plain' : control = Qt Gui . Q Plain Text Edit ( ) elif self . kind == 'rich' : control = Qt Gui . Q Text Edit ( ) control . set Accept Rich Text ( False ) control . install Event Filter ( self ) control . viewport ( ) . install Event Filter ( self ) control . custom Context Menu Requested . connect ( self . custom context menu requested ) control . copy Available . connect ( self . copy available ) control . redo Available . connect ( self . redo available ) control . undo Available . connect ( self . undo available ) layout = control . document ( ) . document Layout ( ) layout . document Size Changed . disconnect ( ) layout . document Size Changed . connect ( self . adjust scrollbars ) control . set Attribute ( Qt Core . Qt . WA Input Method Enabled , True ) control . set Context Menu Policy ( Qt Core . Qt . Custom Context Menu ) control . set Read Only ( True ) control . set Undo Redo Enabled ( False ) control . set Vertical Scroll Bar Policy ( Qt Core . Qt . Scroll Bar Always On ) return control", "predictions": ["create the basic control model ."], "references": ["creates and connects the underlying text widget ."], "bleu": 0.17516432701748888, "rouge_l": 0.2785388127853881}
{"id": 6170, "code": "def create page control ( self ) : if self . custom page control : control = self . custom page control ( ) elif self . kind == 'plain' : control = Qt Gui . Q Plain Text Edit ( ) elif self . kind == 'rich' : control = Qt Gui . Q Text Edit ( ) control . install Event Filter ( self ) viewport = control . viewport ( ) viewport . install Event Filter ( self ) control . set Read Only ( True ) control . set Undo Redo Enabled ( False ) control . set Vertical Scroll Bar Policy ( Qt Core . Qt . Scroll Bar Always On ) return control", "predictions": ["create the control control control page for the page ."], "references": ["creates and connects the underlying paging widget ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 6171, "code": "def get block plain text ( self , block ) : cursor = Qt Gui . Q Text Cursor ( block ) cursor . move Position ( Qt Gui . Q Text Cursor . Start Of Block ) cursor . move Position ( Qt Gui . Q Text Cursor . End Of Block , Qt Gui . Q Text Cursor . Keep Anchor ) return cursor . selection ( ) . to Plain Text ( )", "predictions": ["return the plain text of the plain block ."], "references": ["given a qtextblock return its unformatted text ."], "bleu": 0.16784459625186196, "rouge_l": 0.35672514619883033}
{"id": 6172, "code": "def get end cursor ( self ) : cursor = self . control . text Cursor ( ) cursor . move Position ( Qt Gui . Q Text Cursor . End ) return cursor", "predictions": ["return the end of the cursor ."], "references": ["convenience method that returns a cursor for the last character ."], "bleu": 0.1247439242120089, "rouge_l": 0.2136602451838879}
{"id": 6173, "code": "def get prompt cursor ( self ) : cursor = self . control . text Cursor ( ) cursor . set Position ( self . prompt pos ) return cursor", "predictions": ["return current prompt cursor cursor ."], "references": ["convenience method that returns a cursor for the prompt position ."], "bleu": 0.1141650334026257, "rouge_l": 0.2234432234432234}
{"id": 6174, "code": "def insert continuation prompt ( self , cursor ) : if self . continuation prompt html is None : self . insert plain text ( cursor , self . continuation prompt ) else : self . continuation prompt = self . insert html fetching plain text ( cursor , self . continuation prompt html )", "predictions": ["insert continuation prompt at cursor ."], "references": ["inserts new continuation prompt using the specified cursor ."], "bleu": 0.22172045047934616, "rouge_l": 0.5147679324894514}
{"id": 6175, "code": "def keyboard quit ( self ) : if self . temp buffer filled : self . cancel completion ( ) self . clear temporary buffer ( ) else : self . input buffer = ''", "predictions": ["cancel the keyboard buffer ."], "references": ["cancels the current editing task ala ctrl - g in emacs ."], "bleu": 0.07450619999160439, "rouge_l": 0.2190305206463196}
{"id": 6176, "code": "def prompt started ( self ) : self . control . document ( ) . set Maximum Block Count ( 0 ) self . control . set Undo Redo Enabled ( True ) self . control . set Read Only ( False ) self . control . set Attribute ( Qt Core . Qt . WA Input Method Enabled , True ) if not self . reading : self . executing = False self . prompt started hook ( ) if self . input buffer pending : self . input buffer = self . input buffer pending self . input buffer pending = '' self . control . move Cursor ( Qt Gui . Q Text Cursor . End )", "predictions": ["prompts the user to soft name name name name name name name name name name name name name name name name name name name name name name name name name name"], "references": ["called immediately after a new prompt is displayed ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 6177, "code": "def set top cursor ( self , cursor ) : scrollbar = self . control . vertical Scroll Bar ( ) scrollbar . set Value ( scrollbar . maximum ( ) ) original cursor = self . control . text Cursor ( ) self . control . set Text Cursor ( cursor ) self . control . ensure Cursor Visible ( ) self . control . set Text Cursor ( original cursor )", "predictions": ["sets the alias to the alias cursor"], "references": ["scrolls the viewport so that the specified cursor is at the top ."], "bleu": 0.09374222649442905, "rouge_l": 0.2846034214618974}
{"id": 6178, "code": "def adjust scrollbars ( self ) : document = self . control . document ( ) scrollbar = self . control . vertical Scroll Bar ( ) viewport height = self . control . viewport ( ) . height ( ) if isinstance ( self . control , Qt Gui . Q Plain Text Edit ) : maximum = max ( 0 , document . line Count ( ) - 1 ) step = viewport height / self . control . font Metrics ( ) . line Spacing ( ) else : maximum = document . size ( ) . height ( ) step = viewport height diff = maximum - scrollbar . maximum ( ) scrollbar . set Range ( 0 , maximum ) scrollbar . set Page Step ( step ) if diff < 0 and document . block Count ( ) == document . maximum Block Count ( ) : scrollbar . set Value ( scrollbar . value ( ) + diff )", "predictions": ["adjusts the alias for this widget . based on the control ."], "references": ["expands the vertical scrollbar beyond the range set by qt ."], "bleu": 0.1235622127262679, "rouge_l": 0.2629310344827586}
{"id": 6179, "code": "def dist in usersite ( dist ) : if user site : return normalize path ( dist location ( dist ) ) . startswith ( normalize path ( user site ) ) else : return False", "predictions": ["returns true if the current site is alias to the current site"], "references": ["return true if given distribution is installed in user site ."], "bleu": 0.15537125692760353, "rouge_l": 0.3505747126436781}
{"id": 6180, "code": "def print results ( distributions , list all files ) : results printed = False for dist in distributions : results printed = True logger . info ( \"---\" ) logger . info ( \"Name: %s\" % dist [ 'name' ] ) logger . info ( \"Version: %s\" % dist [ 'version' ] ) logger . info ( \"Location: %s\" % dist [ 'location' ] ) logger . info ( \"Requires: %s\" % ', ' . join ( dist [ 'requires' ] ) ) if list all files : logger . info ( \"Files:\" ) if dist [ 'files' ] is not None : for line in dist [ 'files' ] : logger . info ( \"  %s\" % line . strip ( ) ) else : logger . info ( \"Cannot locate installed-files.txt\" ) return results printed", "predictions": ["print directive for all dirname directive"], "references": ["print the informations from installed distributions found ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 6181, "code": "def main ( args = None ) : options , paths = parse options ( args ) format = getattr ( options , 'output' , 'simple' ) formatter = FORMATTERS [ format ] ( options ) for path in paths : meta = get metadata ( path , options . metadata version ) if meta is None : continue if options . download url prefix : if meta . download url is None : filename = os . path . basename ( path ) meta . download url = '%s/%s' % ( options . download url prefix , filename ) formatter ( meta ) formatter . finish ( )", "predictions": ["download up the intensity and download . ."], "references": ["entry point for pkginfo tool"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 6182, "code": "def cmp to key ( mycmp ) : class Key ( object ) : def init ( self , obj ) : self . obj = obj def lt ( self , other ) : return mycmp ( self . obj , other . obj ) < 0 def gt ( self , other ) : return mycmp ( self . obj , other . obj ) > 0 def eq ( self , other ) : return mycmp ( self . obj , other . obj ) == 0 return Key", "predictions": ["wrap the object in special special special arguments ."], "references": ["convert a cmp = function into a key = function"], "bleu": 0.10620315618312248, "rouge_l": 0.0}
{"id": 6183, "code": "def file read ( filename ) : fobj = open ( filename , 'r' ) source = fobj . read ( ) fobj . close ( ) return source", "predictions": ["format a file into a string"], "references": ["read a file and close it . returns the file source ."], "bleu": 0.10694820729788418, "rouge_l": 0.20962199312714777}
{"id": 6184, "code": "def close ( self ) : self . flush ( ) setattr ( sys , self . channel , self . ostream ) self . file . close ( ) self . closed = True", "predictions": ["generate the underlying uuid uuid * * * * * * * * * * * * * * * * * * * * * * * * * ="], "references": ["close the file and restore the channel ."], "bleu": 0.03901663112717908, "rouge_l": 0.05738476011288805}
{"id": 6185, "code": "def write ( self , data ) : self . file . write ( data ) self . ostream . write ( data ) self . ostream . flush ( )", "predictions": ["mutex mutex mutex args"], "references": ["write data to both channels ."], "bleu": 0.18325568129983205, "rouge_l": 0.0}
{"id": 6186, "code": "def add new heart handler ( self , handler ) : self . log . debug ( \"heartbeat::new heart handler: %s\" , handler ) self . new handlers . add ( handler )", "predictions": ["register a new handler handler"], "references": ["add a new handler for new hearts"], "bleu": 0.3406115671352173, "rouge_l": 0.48541114058355433}
{"id": 6187, "code": "def add heart failure handler ( self , handler ) : self . log . debug ( \"heartbeat::new heart failure handler: %s\" , handler ) self . failure handlers . add ( handler )", "predictions": ["register used failure self in the failure"], "references": ["add a new handler for heart failure"], "bleu": 0.18575057999133598, "rouge_l": 0.14285714285714285}
{"id": 6188, "code": "def handle pong ( self , msg ) : current = str to bytes ( str ( self . lifetime ) ) last = str to bytes ( str ( self . last ping ) ) if msg [ 1 ] == current : delta = time . time ( ) - self . tic self . responses . add ( msg [ 0 ] ) elif msg [ 1 ] == last : delta = time . time ( ) - self . tic + ( self . lifetime - self . last ping ) self . log . warn ( \"heartbeat::heart %r missed a beat, and took %.2f ms to respond\" , msg [ 0 ] , 1000 * delta ) self . responses . add ( msg [ 0 ] ) else : self . log . warn ( \"heartbeat::got bad heartbeat (possibly old?): %s (current=%.3f)\" , msg [ 1 ] , self . lifetime )", "predictions": ["valid pong bad ."], "references": ["a heart just beat"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 6189, "code": "def display All ( elapsed , display amt , est end , n Loops , count , num Prints ) : if num Prints > n Loops : display amt = 1 else : display amt = round ( n Loops / num Prints ) if count % display amt == 0 : avg = elapsed / count est end = round ( avg * n Loops ) ( disp elapsed , disp avg , disp est ) = time Unit ( int ( round ( elapsed ) ) , int ( round ( avg ) ) , int ( round ( est end ) ) ) print \"%s%%\" % str ( round ( count / float ( n Loops ) * 100 ) ) , \"@\" + str ( count ) , total Time = disp est [ 0 ] unit = disp est [ 1 ] if str ( unit ) == \"secs\" : remain = total Time - round ( elapsed ) remain Unit = \"secs\" elif str ( unit ) == \"mins\" : remain = total Time - round ( elapsed ) / 60 remain Unit = \"mins\" elif str ( unit ) == \"hr\" : remain = total Time - round ( elapsed ) / 3600 remain Unit = \"hr\" print \"ETA: %s %s\" % ( str ( remain ) , remain Unit ) print return", "predictions": ["write a string to a time seconds"], "references": ["displays time if verbose is true and count is within the display amount"], "bleu": 0.07882750221706718, "rouge_l": 0.0948678071539658}
{"id": 6190, "code": "def time Unit ( elapsed , avg , est end ) : minute = 60 hr = 3600 day = 86400 if elapsed <= 3 * minute : unit elapsed = ( elapsed , \"secs\" ) if elapsed > 3 * minute : unit elapsed = ( ( elapsed / 60 ) , \"mins\" ) if elapsed > 3 * hr : unit elapsed = ( ( elapsed / 3600 ) , \"hr\" ) if avg <= 3 * minute : unit avg = ( avg , \"secs\" ) if avg > 3 * minute : unit avg = ( ( avg / 60 ) , \"mins\" ) if avg > 3 * hr : unit avg = ( ( avg / 3600 ) , \"hr\" ) if est end <= 3 * minute : unit est End = ( est end , \"secs\" ) if est end > 3 * minute : unit est End = ( ( est end / 60 ) , \"mins\" ) if est end > 3 * hr : unit est End = ( ( est end / 3600 ) , \"hr\" ) return [ unit elapsed , unit avg , unit est End ]", "predictions": ["can be a decimal decimal decimal decimal decimal decimal decimal decimal decimal decimal decimal decimal"], "references": ["calculates unit of time to display"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 6191, "code": "def uncache zipdir ( path ) : from zipimport import zip directory cache as zdc uncache ( path , zdc ) uncache ( path , sys . path importer cache )", "predictions": ["paste the given self . in the given self . ."], "references": ["ensure that the importer caches dont have stale info for path"], "bleu": 0.11390778025531027, "rouge_l": 0.09090909090909091}
{"id": 6192, "code": "def nt quote arg ( arg ) : result = [ ] needquote = False nb = 0 needquote = ( \" \" in arg ) or ( \"\\t\" in arg ) if needquote : result . append ( '\"' ) for c in arg : if c == '\\\\' : nb += 1 elif c == '\"' : result . append ( '\\\\' * ( nb * 2 ) + '\\\\\"' ) nb = 0 else : if nb : result . append ( '\\\\' * nb ) nb = 0 result . append ( c ) if nb : result . append ( '\\\\' * nb ) if needquote : result . append ( '\\\\' * nb ) result . append ( '\"' ) return '' . join ( result )", "predictions": ["convert an argument to a string ."], "references": ["quote a command line argument according to windows parsing rules"], "bleu": 0.14390022429682173, "rouge_l": 0.22803738317757008}
{"id": 6193, "code": "def install script ( self , dist , script name , script text , dev path = None ) : spec = str ( dist . as requirement ( ) ) is script = is python script ( script text , script name ) def get template ( filename ) : raw bytes = resource string ( 'setuptools' , template name ) template str = raw bytes . decode ( 'utf-8' ) clean template = template str . replace ( '\"\"\"' , '' ) return clean template if is script : template name = 'script template.py' if dev path : template name = template name . replace ( '.py' , ' (dev).py' ) script text = ( get script header ( script text ) + get template ( template name ) % locals ( ) ) self . write script ( script name , to ascii ( script text ) , 'b' )", "predictions": ["print a script script script"], "references": ["generate a legacy script wrapper and install it"], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 6194, "code": "def check conflicts ( self , dist ) : return dist from imp import find module , get suffixes from glob import glob blockers = [ ] names = dict . fromkeys ( dist . get metadata ( 'top level.txt' ) ) exts = { '.pyc' : 1 , '.pyo' : 1 } for ext , mode , typ in get suffixes ( ) : exts [ ext ] = 1 for path , files in expand paths ( [ self . install dir ] + self . all site dirs ) : for filename in files : base , ext = os . path . splitext ( filename ) if base in names : if not ext : try : f , filename , descr = find module ( base , [ path ] ) except Import Error : continue else : if f : f . close ( ) if filename not in blockers : blockers . append ( filename ) elif ext in exts and base != 'site' : blockers . append ( os . path . join ( path , filename ) ) if blockers : self . found conflicts ( dist , blockers ) return dist", "predictions": ["prompt for all to the to be used in the working directory . . ."], "references": ["verify that there are no conflicting old - style packages"], "bleu": 0.06917184228205472, "rouge_l": 0.0}
{"id": 6195, "code": "def create home path ( self ) : if not self . user : return home = convert path ( os . path . expanduser ( \"~\" ) ) for name , path in self . config vars . iteritems ( ) : if path . startswith ( home ) and not os . path . isdir ( path ) : self . debug print ( \"os.makedirs('%s', 0700)\" % path ) os . makedirs ( path , 0700 )", "predictions": ["reset the font path directory platform platform platform platform platform platform platform platform platform platform platform"], "references": ["create directories under ~ ."], "bleu": 0.06468490584192432, "rouge_l": 0.0}
{"id": 6196, "code": "def is archive file ( name ) : archives = ( '.zip' , '.tar.gz' , '.tar.bz2' , '.tgz' , '.tar' , '.whl' ) ext = splitext ( name ) [ 1 ] . lower ( ) if ext in archives : return True return False", "predictions": ["return whether an html file is an html file . . . . ."], "references": ["return true if name is a considered as an archive file ."], "bleu": 0.13834368456410945, "rouge_l": 0.3900255754475704}
{"id": 6197, "code": "def new output ( output type = None , output text = None , output png = None , output html = None , output svg = None , output latex = None , output json = None , output javascript = None , output jpeg = None , prompt number = None , etype = None , evalue = None , traceback = None ) : output = Notebook Node ( ) if output type is not None : output . output type = unicode ( output type ) if output type != 'pyerr' : if output text is not None : output . text = unicode ( output text ) if output png is not None : output . png = bytes ( output png ) if output jpeg is not None : output . jpeg = bytes ( output jpeg ) if output html is not None : output . html = unicode ( output html ) if output svg is not None : output . svg = unicode ( output svg ) if output latex is not None : output . latex = unicode ( output latex ) if output json is not None : output . json = unicode ( output json ) if output javascript is not None : output . javascript = unicode ( output javascript ) if output type == u'pyout' : if prompt number is not None : output . prompt number = int ( prompt number ) if output type == u'pyerr' : if etype is not None : output . etype = unicode ( etype ) if evalue is not None : output . evalue = unicode ( evalue ) if traceback is not None : output . traceback = [ unicode ( frame ) for frame in list ( traceback ) ] return output", "predictions": ["create a append of the html html . ."], "references": ["create a new code cell with input and output"], "bleu": 0.18575057999133596, "rouge_l": 0.2222222222222222}
{"id": 6198, "code": "def new code cell ( input = None , prompt number = None , outputs = None , language = u'python' , collapsed = False , metadata = None ) : cell = Notebook Node ( ) cell . cell type = u'code' if language is not None : cell . language = unicode ( language ) if input is not None : cell . input = unicode ( input ) if prompt number is not None : cell . prompt number = int ( prompt number ) if outputs is None : cell . outputs = [ ] else : cell . outputs = outputs if collapsed is not None : cell . collapsed = bool ( collapsed ) cell . metadata = Notebook Node ( metadata or { } ) return cell", "predictions": ["create a append plain plain plain plain plain text text ."], "references": ["create a new code cell with input and output"], "bleu": 0.14991106946711685, "rouge_l": 0.2036727879799666}
{"id": 6199, "code": "def new text cell ( cell type , source = None , rendered = None , metadata = None ) : cell = Notebook Node ( ) if cell type == 'plaintext' : cell type = 'raw' if source is not None : cell . source = unicode ( source ) if rendered is not None : cell . rendered = unicode ( rendered ) cell . metadata = Notebook Node ( metadata or { } ) cell . cell type = cell type return cell", "predictions": ["create a complete with the given self self position position position position position position"], "references": ["create a new text cell ."], "bleu": 0.11633270842295028, "rouge_l": 0.21554770318021202}
{"id": 6200, "code": "def new heading cell ( source = None , rendered = None , level = 1 , metadata = None ) : cell = Notebook Node ( ) cell . cell type = u'heading' if source is not None : cell . source = unicode ( source ) if rendered is not None : cell . rendered = unicode ( rendered ) cell . level = int ( level ) cell . metadata = Notebook Node ( metadata or { } ) return cell", "predictions": ["create a fill temporary buffer from a self control control buffer control ."], "references": ["create a new section cell with a given integer level ."], "bleu": 0.14283632578659286, "rouge_l": 0.33841886269070737}
{"id": 6201, "code": "def new notebook ( name = None , metadata = None , worksheets = None ) : nb = Notebook Node ( ) nb . nbformat = nbformat nb . nbformat minor = nbformat minor if worksheets is None : nb . worksheets = [ ] else : nb . worksheets = list ( worksheets ) if metadata is None : nb . metadata = new metadata ( ) else : nb . metadata = Notebook Node ( metadata ) if name is not None : nb . metadata . name = unicode ( name ) return nb", "predictions": ["create a create create a create a create new control control . . . . . ."], "references": ["create a notebook by name id and a list of worksheets ."], "bleu": 0.10802314890908067, "rouge_l": 0.2847141190198366}
{"id": 6202, "code": "def new metadata ( name = None , authors = None , license = None , created = None , modified = None , gistid = None ) : metadata = Notebook Node ( ) if name is not None : metadata . name = unicode ( name ) if authors is not None : metadata . authors = list ( authors ) if created is not None : metadata . created = unicode ( created ) if modified is not None : metadata . modified = unicode ( modified ) if license is not None : metadata . license = unicode ( license ) if gistid is not None : metadata . gistid = unicode ( gistid ) return metadata", "predictions": ["create a new page object from the given name . . . . . . . . . . . . . . . . ."], "references": ["create a new metadata node ."], "bleu": 0.09186672262412118, "rouge_l": 0.28175519630484985}
{"id": 6203, "code": "def new author ( name = None , email = None , affiliation = None , url = None ) : author = Notebook Node ( ) if name is not None : author . name = unicode ( name ) if email is not None : author . email = unicode ( email ) if affiliation is not None : author . affiliation = unicode ( affiliation ) if url is not None : author . url = unicode ( url ) return author", "predictions": ["create a get block move block move to the given text move move move move to the cursor move move move ."], "references": ["create a new author ."], "bleu": 0.07832899330004497, "rouge_l": 0.2506849315068493}
{"id": 6204, "code": "def writable dir ( path ) : return os . path . isdir ( path ) and os . access ( path , os . W OK )", "predictions": ["return get the directory get the directory of a directory ."], "references": ["whether path is a directory to which the user has write access ."], "bleu": 0.1420146856299917, "rouge_l": 0.24629878869448185}
{"id": 6205, "code": "def unquote filename ( name , win32 = ( sys . platform == 'win32' ) ) : if win32 : if name . startswith ( ( \"'\" , '\"' ) ) and name . endswith ( ( \"'\" , '\"' ) ) : name = name [ 1 : - 1 ] return name", "predictions": ["get a prompt prompt for the given name ."], "references": ["on windows remove leading and trailing quotes from filenames ."], "bleu": 0.1262975489687145, "rouge_l": 0.10427350427350426}
{"id": 6206, "code": "def get ipython package dir ( ) : ipdir = os . path . dirname ( I Python . file ) return py3compat . cast unicode ( ipdir , fs encoding )", "predictions": ["insert the continuation prompt directory ."], "references": ["get the base directory where ipython itself is installed ."], "bleu": 0.13487005099534619, "rouge_l": 0.3588235294117647}
{"id": 6207, "code": "def update suggestions dictionary ( request , object ) : if request . user . is authenticated ( ) : user = request . user content type = Content Type . objects . get for model ( type ( object ) ) try : Object View . objects . get ( user = user , object id = object . id , content type = content type ) except : Object View . objects . create ( user = user , content object = object ) viewed = Object View . objects . filter ( user = user ) else : update dict for guests ( request , object , content type ) return if viewed : for obj in viewed : if content type == obj . content type : if not exists in dictionary ( request , object , content type , obj , True ) : if object . id != obj . object id : Object View Dictionary . objects . create ( current object = object , visited before object = obj . content object ) if not exists in dictionary ( request , obj , obj . content type , object , False ) : Object View Dictionary . objects . create ( current object = obj . content object , visited before object = object ) return", "predictions": ["create only if the cancel object is not already created"], "references": ["updates the suggestions dictionary for an object upon visiting its page"], "bleu": 0.12623203108004888, "rouge_l": 0.18885448916408668}
{"id": 6208, "code": "def get suggestions with size ( object , size ) : content type = Content Type . objects . get for model ( type ( object ) ) try : return Object View Dictionary . objects . filter ( current object id = object . id , current content type = content type ) . extra ( order by = [ '-visits' ] ) [ : size ] except : return Object View Dictionary . objects . filter ( current object id = object . id , current content type = content type ) . extra ( order by = [ '-visits' ] )", "predictions": ["return the suggestions with the given size ."], "references": ["gets a list with a certain size of suggestions for an object"], "bleu": 0.11567041937737582, "rouge_l": 0.1930379746835443}
{"id": 6209, "code": "def get suggestions ( object ) : content type = Content Type . objects . get for model ( type ( object ) ) return Object View Dictionary . objects . filter ( current object id = object . id , current content type = content type ) . extra ( order by = [ '-visits' ] )", "predictions": ["get all suggestions of an object"], "references": ["gets a list of all suggestions for an object"], "bleu": 0.23206041459353086, "rouge_l": 0.5147679324894514}
{"id": 6210, "code": "def options ( self , parser , env ) : if not self . available ( ) : return Plugin . options ( self , parser , env ) parser . add option ( '--profile-sort' , action = 'store' , dest = 'profile sort' , default = env . get ( 'NOSE PROFILE SORT' , 'cumulative' ) , metavar = \"SORT\" , help = \"Set sort order for profiler output\" ) parser . add option ( '--profile-stats-file' , action = 'store' , dest = 'profile stats file' , metavar = \"FILE\" , default = env . get ( 'NOSE PROFILE STATS FILE' ) , help = 'Profiler stats file; default is a new ' 'temp file on each run' ) parser . add option ( '--profile-restrict' , action = 'append' , dest = 'profile restrict' , metavar = \"RESTRICT\" , default = env . get ( 'NOSE PROFILE RESTRICT' ) , help = \"Restrict profiler output. See help for \" \"pstats.Stats for details\" )", "predictions": ["add options for profiler to profiler ."], "references": ["register commandline options ."], "bleu": 0.20556680845025982, "rouge_l": 0.3824451410658307}
{"id": 6211, "code": "def begin ( self ) : if not self . available ( ) : return self . create pfile ( ) self . prof = hotshot . Profile ( self . pfile )", "predictions": ["begin the task ."], "references": ["create profile stats file and load profiler ."], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 6212, "code": "def report ( self , stream ) : log . debug ( 'printing profiler report' ) self . prof . close ( ) prof stats = stats . load ( self . pfile ) prof stats . sort stats ( self . sort ) compat 25 = hasattr ( prof stats , 'stream' ) if compat 25 : tmp = prof stats . stream prof stats . stream = stream else : tmp = sys . stdout sys . stdout = stream try : if self . restrict : log . debug ( 'setting profiler restriction to %s' , self . restrict ) prof stats . print stats ( * self . restrict ) else : prof stats . print stats ( ) finally : if compat 25 : prof stats . stream = tmp else : sys . stdout = tmp", "predictions": ["report the profiler to the given stream ."], "references": ["output profiler report ."], "bleu": 0.19070828081828378, "rouge_l": 0.3546511627906977}
{"id": 6213, "code": "def finalize ( self , result ) : if not self . available ( ) : return try : self . prof . close ( ) except Attribute Error : pass if self . clean stats file : if self . fileno : try : os . close ( self . fileno ) except OS Error : pass try : os . unlink ( self . pfile ) except OS Error : pass return None", "predictions": ["close the result ."], "references": ["clean up stats file if configured to do so ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 6214, "code": "def setup partitioner ( index , num procs , gnum cells , parts ) : global partitioner p = MPI Rect Partitioner2D ( my id = index , num procs = num procs ) p . redim ( global num cells = gnum cells , num parts = parts ) p . prepare communication ( ) partitioner = p", "predictions": ["setup the partitioner for the given index ."], "references": ["create a partitioner in the engine namespace"], "bleu": 0.17747405280050269, "rouge_l": 0.26991150442477874}
{"id": 6215, "code": "def wave saver ( u , x , y , t ) : global u hist global t hist t hist . append ( t ) u hist . append ( 1.0 * u )", "predictions": ["default wave of wave"], "references": ["save the wave log"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 6216, "code": "def init db ( self ) : self . db = sqlite3 . connect ( self . hist file , detect types = sqlite3 . PARSE DECLTYPES | sqlite3 . PARSE COLNAMES ) self . db . execute ( ) self . db . execute ( ) self . db . execute ( ) self . db . commit ( )", "predictions": ["init the database tables ."], "references": ["connect to the database and create tables if necessary ."], "bleu": 0.1501861529550426, "rouge_l": 0.5030927835051546}
{"id": 6217, "code": "def new session ( self , conn = None ) : if conn is None : conn = self . db with conn : cur = conn . execute ( , ( datetime . datetime . now ( ) , ) ) self . session number = cur . lastrowid", "predictions": ["create a new session ."], "references": ["get a new session number ."], "bleu": 0.43989172475842214, "rouge_l": 0.7155425219941348}
{"id": 6218, "code": "def end session ( self ) : self . writeout cache ( ) with self . db : self . db . execute ( , ( datetime . datetime . now ( ) , len ( self . input hist parsed ) - 1 , self . session number ) ) self . session number = 0", "predictions": ["end the current session session ."], "references": ["close the database session filling in the end time and line count ."], "bleu": 0.08649595219978225, "rouge_l": 0.29611650485436897}
{"id": 6219, "code": "def name session ( self , name ) : with self . db : self . db . execute ( \"UPDATE sessions SET remark=? WHERE session==?\" , ( name , self . session number ) )", "predictions": ["name of the name of the database ."], "references": ["give the current session a name in the history database ."], "bleu": 0.17250013293422076, "rouge_l": 0.511744966442953}
{"id": 6220, "code": "def writeout cache ( self , conn = None ) : if conn is None : conn = self . db with self . db input cache lock : try : self . writeout input cache ( conn ) except sqlite3 . Integrity Error : self . new session ( conn ) print ( \"ERROR! Session/line number was not unique in\" , \"database. History logging moved to new session\" , self . session number ) try : self . writeout input cache ( conn ) except sqlite3 . Integrity Error : pass finally : self . db input cache = [ ] with self . db output cache lock : try : self . writeout output cache ( conn ) except sqlite3 . Integrity Error : print ( \"!! Session/line number for output was not unique\" , \"in database. Output will not be stored.\" ) finally : self . db output cache = [ ]", "predictions": ["cache the cache of the writeout cache"], "references": ["write any entries in the cache to the database ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 6221, "code": "def get num cpus ( ) : try : return os . sysconf ( \"SC NPROCESSORS ONLN\" ) except Value Error : num = 0 f = open ( '/proc/cpuinfo' , 'r' ) try : lines = f . readlines ( ) finally : f . close ( ) for line in lines : if line . lower ( ) . startswith ( 'processor' ) : num += 1 if num == 0 : f = open ( '/proc/stat' , 'r' ) try : lines = f . readlines ( ) finally : f . close ( ) search = re . compile ( 'cpu\\d' ) for line in lines : line = line . split ( ' ' ) [ 0 ] if search . match ( line ) : num += 1 if num == 0 : raise Runtime Error ( \"can't determine number of CP Us\" ) return num", "predictions": ["get the number of cpus from the local disk"], "references": ["return the number of cpus on the system"], "bleu": 0.4111336169005197, "rouge_l": 0.594541910331384}
{"id": 6222, "code": "def disk partitions ( all = False ) : phydevs = [ ] f = open ( \"/proc/filesystems\" , \"r\" ) try : for line in f : if not line . startswith ( \"nodev\" ) : phydevs . append ( line . strip ( ) ) finally : f . close ( ) retlist = [ ] partitions = psutil linux . get disk partitions ( ) for partition in partitions : device , mountpoint , fstype , opts = partition if device == 'none' : device = '' if not all : if device == '' or fstype not in phydevs : continue ntuple = nt partition ( device , mountpoint , fstype , opts ) retlist . append ( ntuple ) return retlist", "predictions": ["returns list of all disk partitions"], "references": ["return mounted disk partitions as a list of nameduples"], "bleu": 0.22172045047934616, "rouge_l": 0.2573839662447257}
{"id": 6223, "code": "def get system users ( ) : retlist = [ ] rawlist = psutil linux . get system users ( ) for item in rawlist : user , tty , hostname , tstamp , user process = item if not user process : continue if hostname == ':0.0' : hostname = 'localhost' nt = nt user ( user , tty or None , hostname , tstamp ) retlist . append ( nt ) return retlist", "predictions": ["return all users in the system ."], "references": ["return currently connected users as a list of namedtuples ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 6224, "code": "def get pid list ( ) : pids = [ int ( x ) for x in os . listdir ( '/proc' ) if x . isdigit ( ) ] return pids", "predictions": ["return the pid list ."], "references": ["returns a list of pids currently running on the system ."], "bleu": 0.09778809693469985, "rouge_l": 0.2341650671785029}
{"id": 6225, "code": "def short stack ( ) : stack = inspect . stack ( ) [ : 0 : - 1 ] return \"\\n\" . join ( [ \"%30s : %s @%d\" % ( t [ 3 ] , t [ 1 ] , t [ 2 ] ) for t in stack ] )", "predictions": ["short stack stack ."], "references": ["return a string summarizing the call stack ."], "bleu": 0.1739594473063345, "rouge_l": 0.31443298969072164}
{"id": 6226, "code": "def join regex ( regexes ) : if len ( regexes ) > 1 : return \"|\" . join ( [ \"(%s)\" % r for r in regexes ] ) elif regexes : return regexes [ 0 ] else : return \"\"", "predictions": ["join a regex string into a string"], "references": ["combine a list of regexes into one that matches any of them ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 6227, "code": "def file be gone ( path ) : try : os . remove ( path ) except OS Error : , e , = sys . exc info ( ) if e . errno != errno . ENOENT : raise", "predictions": ["remove a file from the filesystem"], "references": ["remove a file and don t get annoyed if it doesn t exist ."], "bleu": 0.10837364479814457, "rouge_l": 0.2798165137614679}
{"id": 6228, "code": "def update ( self , v ) : self . md5 . update ( to bytes ( str ( type ( v ) ) ) ) if isinstance ( v , string class ) : self . md5 . update ( to bytes ( v ) ) elif v is None : pass elif isinstance ( v , ( int , float ) ) : self . md5 . update ( to bytes ( str ( v ) ) ) elif isinstance ( v , ( tuple , list ) ) : for e in v : self . update ( e ) elif isinstance ( v , dict ) : keys = v . keys ( ) for k in sorted ( keys ) : self . update ( k ) self . update ( v [ k ] ) else : for k in dir ( v ) : if k . startswith ( ' ' ) : continue a = getattr ( v , k ) if inspect . isroutine ( a ) : continue self . update ( k ) self . update ( a )", "predictions": ["update the md5 with the given md5"], "references": ["add v to the hash recursively if needed ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 6229, "code": "def update profiles ( self ) : for path in [ get ipython dir ( ) , os . getcwdu ( ) ] : for profile in list profiles in ( path ) : pd = self . get profile dir ( profile , path ) if profile not in self . profiles : self . log . debug ( \"Adding cluster profile '%s'\" % profile ) self . profiles [ profile ] = { 'profile' : profile , 'profile dir' : pd , 'status' : 'stopped' }", "predictions": ["update the cluster profiles ."], "references": ["list all profiles in the ipython_dir and cwd ."], "bleu": 0.1458826981425239, "rouge_l": 0.2717149220489978}
{"id": 6230, "code": "def start cluster ( self , profile , n = None ) : self . check profile ( profile ) data = self . profiles [ profile ] if data [ 'status' ] == 'running' : raise web . HTTP Error ( 409 , u'cluster already running' ) cl , esl , default n = self . build launchers ( data [ 'profile dir' ] ) n = n if n is not None else default n def clean data ( ) : data . pop ( 'controller launcher' , None ) data . pop ( 'engine set launcher' , None ) data . pop ( 'n' , None ) data [ 'status' ] = 'stopped' def engines stopped ( r ) : self . log . debug ( 'Engines stopped' ) if cl . running : cl . stop ( ) clean data ( ) esl . on stop ( engines stopped ) def controller stopped ( r ) : self . log . debug ( 'Controller stopped' ) if esl . running : esl . stop ( ) clean data ( ) cl . on stop ( controller stopped ) dc = ioloop . Delayed Callback ( lambda : cl . start ( ) , 0 , self . loop ) dc . start ( ) dc = ioloop . Delayed Callback ( lambda : esl . start ( n ) , 1000 * self . delay , self . loop ) dc . start ( ) self . log . debug ( 'Cluster started' ) data [ 'controller launcher' ] = cl data [ 'engine set launcher' ] = esl data [ 'n' ] = n data [ 'status' ] = 'running' return self . profile info ( profile )", "predictions": ["start the cluster ."], "references": ["start a cluster for a given profile ."], "bleu": 0.1571901051328651, "rouge_l": 0.47164948453608246}
{"id": 6231, "code": "def stop cluster ( self , profile ) : self . check profile ( profile ) data = self . profiles [ profile ] if data [ 'status' ] == 'stopped' : raise web . HTTP Error ( 409 , u'cluster not running' ) data = self . profiles [ profile ] cl = data [ 'controller launcher' ] esl = data [ 'engine set launcher' ] if cl . running : cl . stop ( ) if esl . running : esl . stop ( ) result = { 'profile' : data [ 'profile' ] , 'profile dir' : data [ 'profile dir' ] , 'status' : 'stopped' } return result", "predictions": ["stop the cluster ."], "references": ["stop a cluster for a given profile ."], "bleu": 0.1571901051328651, "rouge_l": 0.47164948453608246}
{"id": 6232, "code": "def find cmd ( cmd ) : try : from win32api import Search Path except Import Error : raise Import Error ( 'you need to have pywin32 installed for this to work' ) else : PATH = os . environ [ 'PATH' ] extensions = [ '.exe' , '.com' , '.bat' , '.py' ] path = None for ext in extensions : try : path = Search Path ( PATH , cmd + ext ) [ 0 ] except : pass if path is None : raise OS Error ( \"command %r not found\" % cmd ) else : return path", "predictions": ["find the path to the executable ."], "references": ["find the full path to a . bat or . exe using the win32api module ."], "bleu": 0.09243764735735799, "rouge_l": 0.4873501997336884}
{"id": 6233, "code": "def system body ( p ) : enc = DEFAULT ENCODING for line in read no interrupt ( p . stdout ) . splitlines ( ) : line = line . decode ( enc , 'replace' ) print ( line , file = sys . stdout ) for line in read no interrupt ( p . stderr ) . splitlines ( ) : line = line . decode ( enc , 'replace' ) print ( line , file = sys . stderr ) return p . wait ( )", "predictions": ["wait for the system interrupt to be printed ."], "references": ["callback for _system ."], "bleu": 0.15619699684601276, "rouge_l": 0.3306233062330623}
{"id": 6234, "code": "def setup partitioner ( comm , addrs , index , num procs , gnum cells , parts ) : global partitioner p = ZMQ Rect Partitioner2D ( comm , addrs , my id = index , num procs = num procs ) p . redim ( global num cells = gnum cells , num parts = parts ) p . prepare communication ( ) partitioner = p", "predictions": ["sets up the partitioner for the given partitioner ."], "references": ["create a partitioner in the engine namespace"], "bleu": 0.15619699684601276, "rouge_l": 0.2557651991614256}
{"id": 6235, "code": "def init gui pylab ( self ) : if self . gui or self . pylab : shell = self . shell try : if self . pylab : gui , backend = pylabtools . find gui and backend ( self . pylab ) self . log . info ( \"Enabling GUI event loop integration, \" \"toolkit=%s, pylab=%s\" % ( gui , self . pylab ) ) shell . enable pylab ( gui , import all = self . pylab import all ) else : self . log . info ( \"Enabling GUI event loop integration, \" \"toolkit=%s\" % self . gui ) shell . enable gui ( self . gui ) except Exception : self . log . warn ( \"GUI event loop or pylab initialization failed\" ) self . shell . showtraceback ( )", "predictions": ["initializes gui for gui"], "references": ["enable gui event loop integration taking pylab into account ."], "bleu": 0.08017158404431235, "rouge_l": 0.13260869565217392}
{"id": 6236, "code": "def init code ( self ) : self . run startup files ( ) self . run exec lines ( ) self . run exec files ( ) self . run cmd line code ( ) self . run module ( ) sys . stdout . flush ( ) sys . stderr . flush ( ) self . shell . user ns hidden . update ( self . shell . user ns )", "predictions": ["initializes the user code ."], "references": ["run the pre - flight code specified via exec_lines"], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 6237, "code": "def run exec lines ( self ) : if not self . exec lines : return try : self . log . debug ( \"Running code from I Python App.exec lines...\" ) for line in self . exec lines : try : self . log . info ( \"Running code in user namespace: %s\" % line ) self . shell . run cell ( line , store history = False ) except : self . log . warn ( \"Error in executing line in user \" \"namespace: %s\" % line ) self . shell . showtraceback ( ) except : self . log . warn ( \"Unknown error in handling I Python App.exec lines:\" ) self . shell . showtraceback ( )", "predictions": ["run the shell lines in the shell ."], "references": ["run lines of code in ipythonapp . exec_lines in the user s namespace ."], "bleu": 0.1185574919557074, "rouge_l": 0.43323863636363635}
{"id": 6238, "code": "def run startup files ( self ) : startup dir = self . profile dir . startup dir startup files = glob . glob ( os . path . join ( startup dir , '*.py' ) ) startup files += glob . glob ( os . path . join ( startup dir , '*.ipy' ) ) if not startup files : return self . log . debug ( \"Running startup files from %s...\" , startup dir ) try : for fname in sorted ( startup files ) : self . exec file ( fname ) except : self . log . warn ( \"Unknown error in handling startup files:\" ) self . shell . showtraceback ( )", "predictions": ["run startup files ."], "references": ["run files from profile startup directory"], "bleu": 0.25916266987614406, "rouge_l": 0.3860759493670886}
{"id": 6239, "code": "def run exec files ( self ) : if not self . exec files : return self . log . debug ( \"Running files in I Python App.exec files...\" ) try : for fname in self . exec files : self . exec file ( fname ) except : self . log . warn ( \"Unknown error in handling I Python App.exec files:\" ) self . shell . showtraceback ( )", "predictions": ["run the shell command"], "references": ["run files from ipythonapp . exec_files"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 6240, "code": "def run cmd line code ( self ) : if self . code to run : line = self . code to run try : self . log . info ( \"Running code given at command line (c=): %s\" % line ) self . shell . run cell ( line , store history = False ) except : self . log . warn ( \"Error in executing line in user namespace: %s\" % line ) self . shell . showtraceback ( ) elif self . file to run : fname = self . file to run try : self . exec file ( fname ) except : self . log . warn ( \"Error in executing file in user namespace: %s\" % fname ) self . shell . showtraceback ( )", "predictions": ["get the user size of the user with the shell size"], "references": ["run code or file specified at the command - line"], "bleu": 0.11390778025531027, "rouge_l": 0.09606299212598425}
{"id": 6241, "code": "def run module ( self ) : if self . module to run : save argv = sys . argv sys . argv = [ sys . executable ] + self . extra args try : self . shell . safe run module ( self . module to run , self . shell . user ns ) finally : sys . argv = save argv", "predictions": ["get the suggestions of the suggestions suggestions type type type type type type type type type type type"], "references": ["run module specified at the command - line ."], "bleu": 0.06809398432036522, "rouge_l": 0.07881136950904392}
{"id": 6242, "code": "def generic ( func ) : sentinel = object ( ) def by class ( * args , * * kw ) : cls = args [ 0 ] . class for t in type ( cls . name , ( cls , object ) , { } ) . mro : f = gbt ( t , sentinel ) if f is not sentinel : return f ( * args , * * kw ) else : return func ( * args , * * kw ) by type = { object : func } try : by type [ Instance Type ] = by class except Name Error : pass gbt = by type . get def when type ( * types ) : \"\"\"Decorator to add a method that will be called for the given types\"\"\" for t in types : if not isinstance ( t , classtypes ) : raise Type Error ( \"%r is not a type or class\" % ( t , ) ) def decorate ( f ) : for t in types : if by type . setdefault ( t , f ) is not f : raise Type Error ( \"%r already has method for type %r\" % ( func , t ) ) return f return decorate by object = { } gbo = by object . get def when object ( * obs ) : \"\"\"Decorator to add a method to be called for the given object(s)\"\"\" def decorate ( f ) : for o in obs : if by object . setdefault ( id ( o ) , ( o , f ) ) [ 1 ] is not f : raise Type Error ( \"%r already has method for object %r\" % ( func , o ) ) return f return decorate def dispatch ( * args , * * kw ) : f = gbo ( id ( args [ 0 ] ) , sentinel ) if f is sentinel : for t in type ( args [ 0 ] ) . mro : f = gbt ( t , sentinel ) if f is not sentinel : return f ( * args , * * kw ) else : return func ( * args , * * kw ) else : return f [ 1 ] ( * args , * * kw ) dispatch . name = func . name dispatch . dict = func . dict . copy ( ) dispatch . doc = func . doc dispatch . module = func . module dispatch . when type = when type dispatch . when object = when object dispatch . default = func dispatch . has object = lambda o : id ( o ) in by object dispatch . has type = lambda t : t in by type return dispatch", "predictions": ["decorator to dispatch a method for from a method = dispatch"], "references": ["create a simple generic function"], "bleu": 0.11390778025531027, "rouge_l": 0.13406593406593406}
{"id": 6243, "code": "def data ( fname ) : data file = open ( data filename ( fname ) ) try : return data file . read ( ) finally : data file . close ( )", "predictions": ["read the begin of the file"], "references": ["return the contents of a data file of ours ."], "bleu": 0.13487005099534619, "rouge_l": 0.3588235294117647}
{"id": 6244, "code": "def escape ( t ) : return ( t . replace ( \"&\" , \"&amp;\" ) . replace ( \"<\" , \"&lt;\" ) . replace ( \">\" , \"&gt;\" ) . replace ( \"'\" , \"&#39;\" ) . replace ( '\"' , \"&quot;\" ) . replace ( \"  \" , \"&nbsp; \" ) . replace ( \"  \" , \"&nbsp; \" ) )", "predictions": ["report a json - safe json representation of a python string . . . . . . . . . . . . . . . . . . . ."], "references": ["html - escape the text in t ."], "bleu": 0.04317900023606586, "rouge_l": 0.1147695202257761}
{"id": 6245, "code": "def make local static report files ( self ) : for static , pkgdir in self . STATIC FILES : shutil . copyfile ( data filename ( static , pkgdir ) , os . path . join ( self . directory , static ) ) if self . extra css : shutil . copyfile ( self . config . extra css , os . path . join ( self . directory , self . extra css ) )", "predictions": ["finalize local result result result result result result ."], "references": ["make local instances of static files for html report ."], "bleu": 0.1397712139461423, "rouge_l": 0.20854700854700853}
{"id": 6246, "code": "def write html ( self , fname , html ) : fout = open ( fname , \"wb\" ) try : fout . write ( html . encode ( 'ascii' , 'xmlcharrefreplace' ) ) finally : fout . close ( )", "predictions": ["setup an partitioner partitioner to a file p p p - formatted text"], "references": ["write html to fname properly encoded ."], "bleu": 0.09552040806823771, "rouge_l": 0.10571923743500866}
{"id": 6247, "code": "def file hash ( self , source , cu ) : m = Hasher ( ) m . update ( source ) self . coverage . data . add to hash ( cu . filename , m ) return m . digest ( )", "predictions": ["return the saver saver ."], "references": ["compute a hash that changes if the file needs to be re - reported ."], "bleu": 0.040889869516541145, "rouge_l": 0.18345864661654135}
{"id": 6248, "code": "def index file ( self ) : index tmpl = Templite ( data ( \"index.html\" ) , self . template globals ) self . totals = sum ( [ f [ 'nums' ] for f in self . files ] ) html = index tmpl . render ( { 'arcs' : self . arcs , 'extra css' : self . extra css , 'files' : self . files , 'totals' : self . totals , } ) if sys . version info < ( 3 , 0 ) : html = html . decode ( \"utf-8\" ) self . write html ( os . path . join ( self . directory , \"index.html\" ) , html ) self . status . write ( self . directory )", "predictions": ["init the init init db . ."], "references": ["write the index . html file for this report ."], "bleu": 0.14390022429682173, "rouge_l": 0.34205607476635513}
{"id": 6249, "code": "def read ( self , directory ) : usable = False try : status file = os . path . join ( directory , self . STATUS FILE ) fstatus = open ( status file , \"rb\" ) try : status = pickle . load ( fstatus ) finally : fstatus . close ( ) except ( IO Error , Value Error ) : usable = False else : usable = True if status [ 'format' ] != self . STATUS FORMAT : usable = False elif status [ 'version' ] != coverage . version : usable = False if usable : self . files = status [ 'files' ] self . settings = status [ 'settings' ] else : self . reset ( )", "predictions": ["new state from disk"], "references": ["read the last status in directory ."], "bleu": 0.142719668098593, "rouge_l": 0.0}
{"id": 6250, "code": "def write ( self , directory ) : status file = os . path . join ( directory , self . STATUS FILE ) status = { 'format' : self . STATUS FORMAT , 'version' : coverage . version , 'settings' : self . settings , 'files' : self . files , } fout = open ( status file , \"wb\" ) try : pickle . dump ( status , fout ) finally : fout . close ( )", "predictions": ["end the instance to disk"], "references": ["write the current status to directory ."], "bleu": 0.20252884954471367, "rouge_l": 0.32360742705570295}
{"id": 6251, "code": "def get slice ( seq , start = 0 , stop = None , step = 1 ) : if stop == None : stop = len ( seq ) item = lambda i : seq [ i ] return map ( item , xrange ( start , stop , step ) )", "predictions": ["name of a session ."], "references": ["get a slice of a sequence with variable step . specify start stop step ."], "bleu": 0.05225260032224766, "rouge_l": 0.27518796992481204}
{"id": 6252, "code": "def chop ( seq , size ) : chunk = lambda i : seq [ i : i + size ] return map ( chunk , xrange ( 0 , len ( seq ) , size ) )", "predictions": ["with size - length chunks of size ."], "references": ["chop a sequence into chunks of the given size ."], "bleu": 0.20668251975744228, "rouge_l": 0.43571428571428567}
{"id": 6253, "code": "def read config ( ) : config = Config Parser . Config Parser ( ) config . read ( [ 'setup.cfg' ] ) if not config . has section ( 'check-manifest' ) : return if ( config . has option ( 'check-manifest' , 'ignore-default-rules' ) and config . getboolean ( 'check-manifest' , 'ignore-default-rules' ) ) : del IGNORE [ : ] if config . has option ( 'check-manifest' , 'ignore' ) : patterns = [ p . strip ( ) for p in config . get ( 'check-manifest' , 'ignore' ) . splitlines ( ) ] IGNORE . extend ( p for p in patterns if p )", "predictions": ["get configuration from num os os os os os os os os os os os os os os"], "references": ["read configuration from setup . cfg ."], "bleu": 0.08961672320242714, "rouge_l": 0.17378917378917377}
{"id": 6254, "code": "def file matches ( filename , patterns ) : return any ( fnmatch . fnmatch ( filename , pat ) for pat in patterns )", "predictions": ["check if all disk partitions match the all patterns ."], "references": ["does this filename match any of the patterns?"], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 6255, "code": "def get versioned files ( ) : encoding = 'UTF-8' if sys . platform == 'win32' else None output = run ( [ 'git' , 'ls-files' , '-z' ] , encoding = encoding ) return add directories ( output . split ( '\\0' ) [ : - 1 ] )", "predictions": ["get all users system users psutil psutil psutil psutil psutil psutil psutil psutil psutil psutil psutil"], "references": ["list all files versioned by git in the current directory ."], "bleu": 0.07692375026049747, "rouge_l": 0.07663316582914573}
{"id": 6256, "code": "def start kernel ( self , * * kwargs ) : kernel id = unicode ( uuid . uuid4 ( ) ) km = self . kernel manager factory ( connection file = os . path . join ( self . connection dir , \"kernel-%s.json\" % kernel id ) , config = self . config , ) km . start kernel ( * * kwargs ) km . start channels ( shell = True , sub = False , stdin = False , hb = False ) self . kernels [ kernel id ] = km return kernel id", "predictions": ["get a pid pid for the pid os ."], "references": ["start a new kernel ."], "bleu": 0.15619699684601276, "rouge_l": 0.3012345679012346}
{"id": 6257, "code": "def notebook for kernel ( self , kernel id ) : notebook ids = [ k for k , v in self . notebook mapping . iteritems ( ) if v == kernel id ] if len ( notebook ids ) == 1 : return notebook ids [ 0 ] else : return None", "predictions": ["2 - d short short short short short short short"], "references": ["return the notebook_id for a kernel_id or none ."], "bleu": 0.10600313379512592, "rouge_l": 0.0}
{"id": 6258, "code": "def shutdown kernel ( self , kernel id ) : self . check kernel id ( kernel id ) super ( Mapping Kernel Manager , self ) . shutdown kernel ( kernel id ) self . delete mapping for kernel ( kernel id ) self . log . info ( \"Kernel shutdown: %s\" % kernel id )", "predictions": ["join the regex % regex . ."], "references": ["shutdown a kernel and remove its notebook association ."], "bleu": 0.13958734303905354, "rouge_l": 0.12224448897795591}
{"id": 6259, "code": "def interrupt kernel ( self , kernel id ) : self . check kernel id ( kernel id ) super ( Mapping Kernel Manager , self ) . interrupt kernel ( kernel id ) self . log . info ( \"Kernel interrupted: %s\" % kernel id )", "predictions": ["file be called when a be posted"], "references": ["interrupt a kernel ."], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 6260, "code": "def restart kernel ( self , kernel id ) : self . check kernel id ( kernel id ) km = self . get kernel ( kernel id ) km . restart kernel ( ) self . log . info ( \"Kernel restarted: %s\" % kernel id ) return kernel id notebook id = self . notebook for kernel ( kernel id ) new kernel id = self . start kernel ( ) self . kill kernel ( kernel id ) self . set kernel for notebook ( notebook id , new kernel id ) self . log . info ( \"Kernel restarted: %s\" % new kernel id ) return new kernel id", "predictions": ["update the kernel of the pass kernel"], "references": ["restart a kernel while keeping clients connected ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 6261, "code": "def create iopub stream ( self , kernel id ) : self . check kernel id ( kernel id ) return super ( Mapping Kernel Manager , self ) . create iopub stream ( kernel id )", "predictions": ["update the profiles stream with the given path"], "references": ["create a new iopub stream ."], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 6262, "code": "def create shell stream ( self , kernel id ) : self . check kernel id ( kernel id ) return super ( Mapping Kernel Manager , self ) . create shell stream ( kernel id )", "predictions": ["start a cluster stream"], "references": ["create a new shell stream ."], "bleu": 0.24117803988461298, "rouge_l": 0.3860759493670886}
{"id": 6263, "code": "def create hb stream ( self , kernel id ) : self . check kernel id ( kernel id ) return super ( Mapping Kernel Manager , self ) . create hb stream ( kernel id )", "predictions": ["stop the cluster stream"], "references": ["create a new hb stream ."], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 6264, "code": "def reset ( self ) : instdict = self . dict classdict = self . class . dict for mname , mval in classdict . items ( ) : if mname in instdict and isinstance ( mval , One Time Property ) : delattr ( self , mname )", "predictions": ["find all attributes of this object . . . ."], "references": ["reset all onetimeproperty attributes that may have fired already ."], "bleu": 0.14991106946711685, "rouge_l": 0.3}
{"id": 6265, "code": "def ensure utf8 ( image tag ) : if py3compat . PY3 : return image tag def utf8 image tag ( * args , * * kwargs ) : s = image tag ( * args , * * kwargs ) if isinstance ( s , unicode ) : s = s . encode ( 'utf8' ) return s return utf8 image tag", "predictions": ["make sure the p - type is unicode"], "references": ["wrapper for ensuring image_tag returns utf8 - encoded str on python 2"], "bleu": 0.09726684100532913, "rouge_l": 0.09651898734177215}
{"id": 6266, "code": "def get unique or none ( klass , * args , * * kwargs ) : try : return klass . objects . get ( * args , * * kwargs ) except klass . Does Not Exist : return None except klass . Multiple Objects Returned : return None return None", "predictions": ["returns a partitioner class or none if not found"], "references": ["returns a unique instance of klass or none"], "bleu": 0.23356898886410005, "rouge_l": 0.4756335282651072}
{"id": 6267, "code": "def get query includes ( tokenized terms , search fields ) : query = None for term in tokenized terms : or query = None for field name in search fields : q = Q ( * * { \"%s icontains\" % field name : term } ) if or query is None : or query = q else : or query = or query | q if query is None : query = or query else : query = query & or query return query", "predictions": ["init gui names to gui from if any"], "references": ["builds a query for included terms in a text search ."], "bleu": 0.09268172804333874, "rouge_l": 0.0}
{"id": 6268, "code": "def get text query ( query string , search fields ) : include terms , exclude terms = get text tokenizer ( query string ) include q = get query includes ( include terms , search fields ) exclude q = get query excludes ( exclude terms , search fields ) query = None if include q and exclude q : query = include q & ~ exclude q elif not exclude q : query = include q else : query = ~ exclude q return query", "predictions": ["init code from query"], "references": ["builds a query for both included & excluded terms in a text search ."], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 6269, "code": "def get date greater query ( days , date field ) : query = None days = get integer ( days ) if days : past = get days ago ( days ) query = Q ( * * { \"%s gte\" % date field : past . isoformat ( ) } ) return query", "predictions": ["returns the query query query"], "references": ["query for if date_field is within number of days ago ."], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 6270, "code": "def get date less query ( days , date field ) : query = None days = get integer ( days ) if days : future = get days from now ( days ) query = Q ( * * { \"%s lte\" % date field : future . isoformat ( ) } ) return query", "predictions": ["run the files in the startup"], "references": ["query for if date_field is within number of days from now ."], "bleu": 0.06833381956448398, "rouge_l": 0.0}
{"id": 6271, "code": "def get null or blank query ( field = None ) : if not field : return field null q = get null query ( field ) blank q = get blank query ( field ) return ( null q | blank q )", "predictions": ["return files that are not exec by self ."], "references": ["query for null or blank field ."], "bleu": 0.14113991930789777, "rouge_l": 0.1278825995807128}
{"id": 6272, "code": "def case insensitive ( self , fields dict ) : if hasattr ( self . model , 'CASE INSENSITIVE FIELDS' ) : for field in self . model . CASE INSENSITIVE FIELDS : if field in fields dict : fields dict [ field + ' iexact' ] = fields dict [ field ] del fields dict [ field ]", "predictions": ["removes the field names from the model"], "references": ["converts queries to case insensitive for special fields ."], "bleu": 0.11737849637633067, "rouge_l": 0.0}
{"id": 6273, "code": "def options ( self , parser , env ) : parser . add option ( \"-a\" , \"--attr\" , dest = \"attr\" , action = \"append\" , default = env . get ( 'NOSE ATTR' ) , metavar = \"ATTR\" , help = \"Run only tests that have attributes \" \"specified by ATTR [NOSE ATTR]\" ) if compat 24 : parser . add option ( \"-A\" , \"--eval-attr\" , dest = \"eval attr\" , metavar = \"EXPR\" , action = \"append\" , default = env . get ( 'NOSE EVAL ATTR' ) , help = \"Run only tests for whose attributes \" \"the Python expression EXPR evaluates \" \"to True [NOSE EVAL ATTR]\" )", "predictions": ["add command line options for the parser ."], "references": ["register command line options"], "bleu": 0.2984745896009823, "rouge_l": 0.5319767441860466}
{"id": 6274, "code": "def want Method ( self , method ) : try : cls = method . im class except Attribute Error : return False return self . validate Attrib ( method , cls )", "predictions": ["returns true if the provided method is an actual method ."], "references": ["accept the method if its attributes match ."], "bleu": 0.14323145079400493, "rouge_l": 0.32504440497335696}
{"id": 6275, "code": "def rotate ( self ) : if self . prev yank : text = self . ring . rotate ( ) if text : self . skip cursor = True cursor = self . text edit . text Cursor ( ) cursor . move Position ( Qt Gui . Q Text Cursor . Left , Qt Gui . Q Text Cursor . Keep Anchor , n = len ( self . prev yank ) ) cursor . insert Text ( text ) self . prev yank = text", "predictions": ["rotate the cursor to the bottom of the line ."], "references": ["rotate the kill ring then yank back the new top ."], "bleu": 0.1705647399369684, "rouge_l": 0.37770897832817335}
{"id": 6276, "code": "def start hb ( self , callback ) : if not self . beating : self . kernel alive = True def ping or dead ( ) : self . hb stream . flush ( ) if self . kernel alive : self . kernel alive = False self . hb stream . send ( b'ping' ) self . hb stream . flush ( ) else : try : callback ( ) except : pass finally : self . stop hb ( ) def beat received ( msg ) : self . kernel alive = True self . hb stream . on recv ( beat received ) loop = ioloop . IO Loop . instance ( ) self . hb periodic callback = ioloop . Periodic Callback ( ping or dead , self . time to dead * 1000 , loop ) loop . add timeout ( time . time ( ) + self . first beat , self . really start hb ) self . beating = True", "predictions": ["start the kernel ."], "references": ["start the heartbeating and call the callback if the kernel dies ."], "bleu": 0.08047084086794415, "rouge_l": 0.45864661654135336}
{"id": 6277, "code": "def stop hb ( self ) : if self . beating : self . beating = False self . hb periodic callback . stop ( ) if not self . hb stream . closed ( ) : self . hb stream . on recv ( None )", "predictions": ["stop the hb and stop the hb ."], "references": ["stop the heartbeating and cancel all related callbacks ."], "bleu": 0.2116253761537182, "rouge_l": 0.465648854961832}
{"id": 6278, "code": "def fload ( self ) : if hasattr ( self , 'fobj' ) and self . fobj is not None : self . fobj . close ( ) if hasattr ( self . src , \"read\" ) : self . fobj = self . src else : self . fobj = open ( self . fname )", "predictions": ["closes the file ."], "references": ["load file object ."], "bleu": 0.3976353643835253, "rouge_l": 0.5}
{"id": 6279, "code": "def reload ( self ) : self . fload ( ) self . src = self . fobj . read ( ) src b = [ b . strip ( ) for b in self . re stop . split ( self . src ) if b ] self . silent = [ bool ( self . re silent . findall ( b ) ) for b in src b ] self . auto = [ bool ( self . re auto . findall ( b ) ) for b in src b ] if self . auto all is None : self . auto all = bool ( self . re auto all . findall ( src b [ 0 ] ) ) else : self . auto all = bool ( self . auto all ) src blocks = [ ] auto strip = lambda s : self . re auto . sub ( '' , s ) for i , b in enumerate ( src b ) : if self . auto [ i ] : src blocks . append ( auto strip ( b ) ) else : src blocks . append ( b ) src blocks [ 0 ] = self . re auto all . sub ( '' , src blocks [ 0 ] ) self . nblocks = len ( src blocks ) self . src blocks = src blocks self . src blocks colored = map ( self . ip colorize , self . src blocks ) self . reset ( )", "predictions": ["reloads all contents of the instance ."], "references": ["reload source from disk and initialize state ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 6280, "code": "def show ( self , index = None ) : index = self . get index ( index ) if index is None : return print >> io . stdout , self . marquee ( % ( self . title , index , self . nblocks - index - 1 ) ) print >> io . stdout , ( self . src blocks colored [ index ] ) sys . stdout . flush ( )", "predictions": ["show the result of the requested index ."], "references": ["show a single block on screen"], "bleu": 0.16036590969929357, "rouge_l": 0.14663461538461536}
{"id": 6281, "code": "def show all ( self ) : fname = self . title title = self . title nblocks = self . nblocks silent = self . silent marquee = self . marquee for index , block in enumerate ( self . src blocks colored ) : if silent [ index ] : print >> io . stdout , marquee ( % ( title , index , nblocks - index - 1 ) ) else : print >> io . stdout , marquee ( % ( title , index , nblocks - index - 1 ) ) print >> io . stdout , block , sys . stdout . flush ( )", "predictions": ["show all the blocks of all the blocks"], "references": ["show entire demo on screen block by block"], "bleu": 0.16036590969929357, "rouge_l": 0.125}
{"id": 6282, "code": "def reload ( self ) : self . fload ( ) lines = self . fobj . readlines ( ) src b = [ l for l in lines if l . strip ( ) ] nblocks = len ( src b ) self . src = '' . join ( lines ) self . silent = [ False ] * nblocks self . auto = [ True ] * nblocks self . auto all = True self . nblocks = nblocks self . src blocks = src b self . src blocks colored = map ( self . ip colorize , self . src blocks ) self . reset ( )", "predictions": ["reloads all lines from the contents of the instance"], "references": ["reload source from disk and initialize state ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 6283, "code": "def thread ( function , sequence , cores = None , run Series = False , quiet = False ) : if cores is None : pool = Thread Pool ( ) else : pool = Thread Pool ( cores ) tic = time . time ( ) if run Series is False : try : results = pool . map ( function , sequence ) pool . close ( ) pool . join ( ) except : print 'thread Failed... running in series :-(' results = series ( sequence , function ) else : results = series ( sequence , function ) toc = time . time ( ) elapsed = toc - tic if quiet is False : if cores is None : print \"Elapsed time: %s  :-)\\n\" % str ( elapsed ) else : print \"Elapsed time: %s  on %s threads :-)\\n\" % ( str ( elapsed ) , str ( cores ) ) return results", "predictions": ["run a thread in a sequence"], "references": ["sets up the threadpool with map for parallel processing"], "bleu": 0.1126634218241493, "rouge_l": 0.0}
{"id": 6284, "code": "def with objattrs ( * names ) : def wrap ( func ) : @ functools . wraps ( func ) def wrapper ( self , * args , * * kwargs ) : with contextlib . Exit Stack ( ) as stack : for name in names : stack . enter context ( getattr ( self , name ) ) return func ( self , * args , * * kwargs ) return wrapper return wrap", "predictions": ["decorator to enter a context with a function ."], "references": ["like with_objattr but enter context one by one ."], "bleu": 0.16784459625186196, "rouge_l": 0.3333333333333333}
{"id": 6285, "code": "def countdown ( name , date , description = '' , id = '' , granularity = 'sec' , start = None , progressbar = False , progressbar inversed = False , showpct = False ) : end date = dateparse . parse datetime ( date ) end = dateformat . format ( end date , 'U' ) content = '<div class=\"name\">' + name + '</div>' content += '<div class=\"description\">' + description + '</div>' if progressbar : if not end : raise Exception ( 'For progressbar, start date is requried.' ) parsed date = datetime . datetime . combine ( dateparse . parse date ( start ) , datetime . time ( ) ) start date = dateparse . parse datetime ( start ) or parsed date now = datetime . datetime . now ( ) pct = ( now - start date ) . total seconds ( ) / ( end date - start date ) . total seconds ( ) pct = int ( pct * 100 ) if progressbar inversed : pct = 100 - pct bar = '<div class=\"progress progress-striped active\">' bar += '<div class=\"progress-bar\"  role=\"progressbar\" aria-valuenow=\"{pct}\" aria-valuemin=\"0\" aria-valuemax=\"100\" style=\"width: {pct}%\">' bar += '<span class=\"sr-only\">{pct}% Complete</span>' bar += '</div>' bar += '</div>' if showpct : bar += '<div class=\"percentage\">{pct}%</div>' bar = bar . format ( pct = pct ) content += bar content += '<div class=\"counter\"></div>' attr = { 'class' : 'countdownbox' , 'data-datetime' : end , 'data-granularity' : granularity } if id : attr [ 'id' ] = id return html . tag ( 'div' , content , attr )", "predictions": ["return html for a specific countdown ."], "references": ["create a countdown ."], "bleu": 0.2626909894424158, "rouge_l": 0.5736677115987461}
{"id": 6286, "code": "def cleanup ( controller , engines ) : import signal , time print ( 'Starting cleanup' ) print ( 'Stopping engines...' ) for e in engines : e . send signal ( signal . SIGINT ) print ( 'Stopping controller...' ) controller . send signal ( signal . SIGINT ) time . sleep ( 0.1 ) print ( 'Killing controller...' ) controller . kill ( ) print ( 'Cleanup done' )", "predictions": ["cleanup all engines in the pool"], "references": ["cleanup routine to shut down all subprocesses we opened ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 6287, "code": "def save ids ( f , self , * args , * * kwargs ) : n previous = len ( self . client . history ) try : ret = f ( self , * args , * * kwargs ) finally : nmsgs = len ( self . client . history ) - n previous msg ids = self . client . history [ - nmsgs : ] self . history . extend ( msg ids ) map ( self . outstanding . add , msg ids ) return ret", "predictions": ["history history of history"], "references": ["keep our history and outstanding attributes up to date after a method call ."], "bleu": 0.02949347753605095, "rouge_l": 0.10099337748344371}
{"id": 6288, "code": "def sync results ( f , self , * args , * * kwargs ) : ret = f ( self , * args , * * kwargs ) delta = self . outstanding . difference ( self . client . outstanding ) completed = self . outstanding . intersection ( delta ) self . outstanding = self . outstanding . difference ( completed ) return ret", "predictions": ["sync the function with the given arguments ."], "references": ["sync relevant results from self . client to our results attribute ."], "bleu": 0.10764345432696364, "rouge_l": 0.1930379746835443}
{"id": 6289, "code": "def spin after ( f , self , * args , * * kwargs ) : ret = f ( self , * args , * * kwargs ) self . spin ( ) return ret", "predictions": ["wrap a function after a spin after the given function ."], "references": ["call spin after the method ."], "bleu": 0.22416933501922287, "rouge_l": 0.4969450101832994}
{"id": 6290, "code": "def add record ( self , msg id , rec ) : rec = self . binary buffers ( rec ) self . records . insert ( rec )", "predictions": ["add a record to the provided binary ."], "references": ["add a new task record by msg_id ."], "bleu": 0.239802967618271, "rouge_l": 0.5}
{"id": 6291, "code": "def get record ( self , msg id ) : r = self . records . find one ( { 'msg id' : msg id } ) if not r : raise Key Error ( msg id ) return r", "predictions": ["get a record by id ."], "references": ["get a specific task record by msg_id ."], "bleu": 0.2741469871939866, "rouge_l": 0.6963470319634703}
{"id": 6292, "code": "def update record ( self , msg id , rec ) : rec = self . binary buffers ( rec ) self . records . update ( { 'msg id' : msg id } , { '$set' : rec } )", "predictions": ["update the record record with the given msg ."], "references": ["update the data in an existing record ."], "bleu": 0.21105340631872635, "rouge_l": 0.4756335282651072}
{"id": 6293, "code": "def get history ( self ) : cursor = self . records . find ( { } , { 'msg id' : 1 } ) . sort ( 'submitted' ) return [ rec [ 'msg id' ] for rec in cursor ]", "predictions": ["return a list of the history of this cursor ."], "references": ["get all msg_ids ordered by time submitted ."], "bleu": 0.12605968092174913, "rouge_l": 0.11338289962825278}
{"id": 6294, "code": "def get msgs ( self ) : msgs = [ ] while True : try : msgs . append ( self . get msg ( block = False ) ) except Empty : break return msgs", "predictions": ["returns the list of items in this message ."], "references": ["get all messages that are currently ready ."], "bleu": 0.14113991930789777, "rouge_l": 0.1189083820662768}
{"id": 6295, "code": "def get msg ( self , block = True , timeout = None ) : return self . in queue . get ( block , timeout )", "predictions": ["get message from queue ."], "references": ["gets a message if there is one that is ready ."], "bleu": 0.0910020781697788, "rouge_l": 0.2341650671785029}
{"id": 6296, "code": "def parse ( url ) : config = { } if not isinstance ( url , six . string types ) : url = '' url = urlparse . urlparse ( url ) path = url . path [ 1 : ] path = path . split ( '?' , 2 ) [ 0 ] config . update ( { 'NAME' : path , 'USER' : url . username , 'PASSWORD' : url . password , 'HOST' : url . hostname , 'PORT' : url . port , } ) if url . scheme in SCHEMES : config [ 'ENGINE' ] = SCHEMES [ url . scheme ] return config", "predictions": ["parses a url ."], "references": ["parses a database url ."], "bleu": 0.4630777161991027, "rouge_l": 0.8714285714285713}
{"id": 6297, "code": "def magic run completer ( self , event ) : comps = arg split ( event . line , strict = False ) relpath = ( len ( comps ) > 1 and comps [ - 1 ] or '' ) . strip ( \"'\\\"\" ) lglob = glob . glob isdir = os . path . isdir relpath , tilde expand , tilde val = expand user ( relpath ) dirs = [ f . replace ( '\\\\' , '/' ) + \"/\" for f in lglob ( relpath + '*' ) if isdir ( f ) ] if filter ( magic run re . match , comps ) : pys = [ f . replace ( '\\\\' , '/' ) for f in lglob ( '*' ) ] else : pys = [ f . replace ( '\\\\' , '/' ) for f in lglob ( relpath + '*.py' ) + lglob ( relpath + '*.ipy' ) + lglob ( relpath + '*.pyw' ) ] return [ compress user ( p , tilde expand , tilde val ) for p in dirs + pys ]", "predictions": ["completer completer for magic run commands"], "references": ["complete files that end in . py or . ipy for the %run command ."], "bleu": 0.04928854007377984, "rouge_l": 0.08840579710144927}
{"id": 6298, "code": "def cd completer ( self , event ) : ip = get ipython ( ) relpath = event . symbol if event . line . endswith ( '-b' ) or ' -b ' in event . line : bkms = self . db . get ( 'bookmarks' , None ) if bkms : return bkms . keys ( ) else : return [ ] if event . symbol == '-' : width dh = str ( len ( str ( len ( ip . user ns [ ' dh' ] ) + 1 ) ) ) fmt = '-%0' + width dh + 'd [%s]' ents = [ fmt % ( i , s ) for i , s in enumerate ( ip . user ns [ ' dh' ] ) ] if len ( ents ) > 1 : return ents return [ ] if event . symbol . startswith ( '--' ) : return [ \"--\" + os . path . basename ( d ) for d in ip . user ns [ ' dh' ] ] relpath , tilde expand , tilde val = expand user ( relpath ) relpath = relpath . replace ( '\\\\' , '/' ) found = [ ] for d in [ f . replace ( '\\\\' , '/' ) + '/' for f in glob . glob ( relpath + '*' ) if os . path . isdir ( f ) ] : if ' ' in d : raise Try Next found . append ( d ) if not found : if os . path . isdir ( relpath ) : return [ compress user ( relpath , tilde expand , tilde val ) ] bks = self . db . get ( 'bookmarks' , { } ) . iterkeys ( ) bkmatches = [ s for s in bks if s . startswith ( event . symbol ) ] if bkmatches : return bkmatches raise Try Next return [ compress user ( p , tilde expand , tilde val ) for p in found ]", "predictions": ["cd completer completer on the ipython ipython"], "references": ["completer function for cd which only returns directories ."], "bleu": 0.15447878876032708, "rouge_l": 0.12224448897795591}
{"id": 6299, "code": "def quoteattr ( self , attr ) : attr = xml safe ( attr ) if isinstance ( attr , unicode ) and not UNICODE STRINGS : attr = attr . encode ( self . encoding ) return saxutils . quoteattr ( attr )", "predictions": ["returns an attribute of the attribute ."], "references": ["escape an xml attribute . value can be unicode ."], "bleu": 0.17112717058426782, "rouge_l": 0.34205607476635513}
{"id": 6300, "code": "def configure ( self , options , config ) : Plugin . configure ( self , options , config ) self . config = config if self . enabled : self . stats = { 'errors' : 0 , 'failures' : 0 , 'passes' : 0 , 'skipped' : 0 } self . errorlist = [ ] self . error report file = codecs . open ( options . xunit file , 'w' , self . encoding , 'replace' )", "predictions": ["configure the error report ."], "references": ["configures the xunit plugin ."], "bleu": 0.3021375397356768, "rouge_l": 0.4}
{"id": 6301, "code": "def add Error ( self , test , err , capt = None ) : taken = self . time Taken ( ) if issubclass ( err [ 0 ] , Skip Test ) : type = 'skipped' self . stats [ 'skipped' ] += 1 else : type = 'error' self . stats [ 'errors' ] += 1 tb = '' . join ( traceback . format exception ( * err ) ) id = test . id ( ) self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s time=\"%(taken).3f\">' '<%(type)s type=%(errtype)s message=%(message)s><![CDATA[%(tb)s]]>' '</%(type)s></testcase>' % { 'cls' : self . quoteattr ( id split ( id ) [ 0 ] ) , 'name' : self . quoteattr ( id split ( id ) [ - 1 ] ) , 'taken' : taken , 'type' : type , 'errtype' : self . quoteattr ( nice classname ( err [ 0 ] ) ) , 'message' : self . quoteattr ( exc message ( err ) ) , 'tb' : escape cdata ( tb ) , } )", "predictions": ["add a test test test test ."], "references": ["add error output to xunit report ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 6302, "code": "def add Failure ( self , test , err , capt = None , tb info = None ) : taken = self . time Taken ( ) tb = '' . join ( traceback . format exception ( * err ) ) self . stats [ 'failures' ] += 1 id = test . id ( ) self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s time=\"%(taken).3f\">' '<failure type=%(errtype)s message=%(message)s><![CDATA[%(tb)s]]>' '</failure></testcase>' % { 'cls' : self . quoteattr ( id split ( id ) [ 0 ] ) , 'name' : self . quoteattr ( id split ( id ) [ - 1 ] ) , 'taken' : taken , 'errtype' : self . quoteattr ( nice classname ( err [ 0 ] ) ) , 'message' : self . quoteattr ( exc message ( err ) ) , 'tb' : escape cdata ( tb ) , } )", "predictions": ["add a test message to the server ."], "references": ["add failure output to xunit report ."], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 6303, "code": "def add Success ( self , test , capt = None ) : taken = self . time Taken ( ) self . stats [ 'passes' ] += 1 id = test . id ( ) self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s ' 'time=\"%(taken).3f\" />' % { 'cls' : self . quoteattr ( id split ( id ) [ 0 ] ) , 'name' : self . quoteattr ( id split ( id ) [ - 1 ] ) , 'taken' : taken , } )", "predictions": ["add a test to the batch ."], "references": ["add success output to xunit report ."], "bleu": 0.22089591134157885, "rouge_l": 0.42857142857142855}
{"id": 6304, "code": "def register engine ( self , uid ) : self . targets . insert ( 0 , uid ) self . loads . insert ( 0 , 0 ) self . completed [ uid ] = set ( ) self . failed [ uid ] = set ( ) self . pending [ uid ] = { } self . update graph ( None )", "predictions": ["case is the insensitive insensitive"], "references": ["new engine with ident uid became available ."], "bleu": 0.1259933680597235, "rouge_l": 0.0}
{"id": 6305, "code": "def unregister engine ( self , uid ) : if len ( self . targets ) == 1 : pass self . engine stream . flush ( ) idx = self . targets . index ( uid ) self . targets . pop ( idx ) self . loads . pop ( idx ) if self . pending [ uid ] : dc = ioloop . Delayed Callback ( lambda : self . handle stranded tasks ( uid ) , 5000 , self . loop ) dc . start ( ) else : self . completed . pop ( uid ) self . failed . pop ( uid )", "predictions": ["options from the engine stream"], "references": ["existing engine with ident uid became unavailable ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 6306, "code": "def handle stranded tasks ( self , engine ) : lost = self . pending [ engine ] for msg id in lost . keys ( ) : if msg id not in self . pending [ engine ] : continue raw msg = lost [ msg id ] . raw msg idents , msg = self . session . feed identities ( raw msg , copy = False ) parent = self . session . unpack ( msg [ 1 ] . bytes ) idents = [ engine , idents [ 0 ] ] try : raise error . Engine Error ( \"Engine %r died while running task %r\" % ( engine , msg id ) ) except : content = error . wrap exception ( ) header = dict ( status = 'error' , engine = engine , date = datetime . now ( ) , ) msg = self . session . msg ( 'apply reply' , content , parent = parent , subheader = header ) raw reply = map ( zmq . Message , self . session . serialize ( msg , ident = idents ) ) self . dispatch result ( raw reply ) self . completed . pop ( engine ) self . failed . pop ( engine )", "predictions": ["handles a single stranded"], "references": ["deal with jobs resident in an engine that died ."], "bleu": 0.06741599762807414, "rouge_l": 0.0}
{"id": 6307, "code": "def audit timeouts ( self ) : now = datetime . now ( ) for msg id in self . depending . keys ( ) : if msg id in self . depending : job = self . depending [ msg id ] if job . timeout and job . timeout < now : self . fail unreachable ( msg id , error . Task Timeout )", "predictions": ["rotate the unreachable timeouts to the unreachable"], "references": ["audit all waiting tasks for expired timeouts ."], "bleu": 0.16102307266026747, "rouge_l": 0.13174946004319654}
{"id": 6308, "code": "def maybe run ( self , job ) : msg id = job . msg id self . log . debug ( \"Attempting to assign task %s\" , msg id ) if not self . targets : return False if job . follow or job . targets or job . blacklist or self . hwm : def can run ( idx ) : if self . hwm and self . loads [ idx ] == self . hwm : return False target = self . targets [ idx ] if target in job . blacklist : return False if job . targets and target not in job . targets : return False return job . follow . check ( self . completed [ target ] , self . failed [ target ] ) indices = filter ( can run , range ( len ( self . targets ) ) ) if not indices : if job . follow . all : dests = set ( ) relevant = set ( ) if job . follow . success : relevant = self . all completed if job . follow . failure : relevant = relevant . union ( self . all failed ) for m in job . follow . intersection ( relevant ) : dests . add ( self . destinations [ m ] ) if len ( dests ) > 1 : self . depending [ msg id ] = job self . fail unreachable ( msg id ) return False if job . targets : job . targets . difference update ( job . blacklist ) if not job . targets or not job . targets . intersection ( self . targets ) : self . depending [ msg id ] = job self . fail unreachable ( msg id ) return False return False else : indices = None self . submit task ( job , indices ) return True", "predictions": ["run a single callback ."], "references": ["check location dependencies and run if they are met ."], "bleu": 0.11115018927487523, "rouge_l": 0.2515463917525773}
{"id": 6309, "code": "def save unmet ( self , job ) : msg id = job . msg id self . depending [ msg id ] = job for dep id in job . after . union ( job . follow ) . difference ( self . all done ) : if dep id not in self . graph : self . graph [ dep id ] = set ( ) self . graph [ dep id ] . add ( msg id )", "predictions": ["stop the unmet graph periodic periodic periodic job"], "references": ["save a message for later submission when its dependencies are met ."], "bleu": 0.08179133792443427, "rouge_l": 0.0}
{"id": 6310, "code": "def submit task ( self , job , indices = None ) : if indices : loads = [ self . loads [ i ] for i in indices ] else : loads = self . loads idx = self . scheme ( loads ) if indices : idx = indices [ idx ] target = self . targets [ idx ] self . engine stream . send ( target , flags = zmq . SNDMORE , copy = False ) self . engine stream . send multipart ( job . raw msg , copy = False ) self . add job ( idx ) self . pending [ target ] [ job . msg id ] = job content = dict ( msg id = job . msg id , engine id = target . decode ( 'ascii' ) ) self . session . send ( self . mon stream , 'task destination' , content = content , ident = [ b'tracktask' , self . ident ] )", "predictions": ["submit a if the if it is not already connected ."], "references": ["submit a task to any of a subset of our targets ."], "bleu": 0.14709132836587344, "rouge_l": 0.25884016973125884}
{"id": 6311, "code": "def dispatch result ( self , raw msg ) : try : idents , msg = self . session . feed identities ( raw msg , copy = False ) msg = self . session . unserialize ( msg , content = False , copy = False ) engine = idents [ 0 ] try : idx = self . targets . index ( engine ) except Value Error : pass else : self . finish job ( idx ) except Exception : self . log . error ( \"task::Invaid result: %r\" , raw msg , exc info = True ) return header = msg [ 'header' ] parent = msg [ 'parent header' ] if header . get ( 'dependencies met' , True ) : success = ( header [ 'status' ] == 'ok' ) msg id = parent [ 'msg id' ] retries = self . retries [ msg id ] if not success and retries > 0 : self . retries [ msg id ] = retries - 1 self . handle unmet dependency ( idents , parent ) else : del self . retries [ msg id ] self . handle result ( idents , parent , raw msg , success ) self . mon stream . send multipart ( [ b'outtask' ] + raw msg , copy = False ) else : self . handle unmet dependency ( idents , parent )", "predictions": ["reload the result in the queue . . ."], "references": ["dispatch method for result replies"], "bleu": 0.14113991930789777, "rouge_l": 0.1506172839506173}
{"id": 6312, "code": "def handle result ( self , idents , parent , raw msg , success = True ) : engine = idents [ 0 ] client = idents [ 1 ] raw msg [ : 2 ] = [ client , engine ] self . client stream . send multipart ( raw msg , copy = False ) msg id = parent [ 'msg id' ] self . pending [ engine ] . pop ( msg id ) if success : self . completed [ engine ] . add ( msg id ) self . all completed . add ( msg id ) else : self . failed [ engine ] . add ( msg id ) self . all failed . add ( msg id ) self . all done . add ( msg id ) self . destinations [ msg id ] = engine self . update graph ( msg id , success )", "predictions": ["show a client object"], "references": ["handle a real task result either success or failure"], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 6313, "code": "def handle unmet dependency ( self , idents , parent ) : engine = idents [ 0 ] msg id = parent [ 'msg id' ] job = self . pending [ engine ] . pop ( msg id ) job . blacklist . add ( engine ) if job . blacklist == job . targets : self . depending [ msg id ] = job self . fail unreachable ( msg id ) elif not self . maybe run ( job ) : if msg id not in self . all failed : self . save unmet ( job ) if self . hwm : try : idx = self . targets . index ( engine ) except Value Error : pass else : if self . loads [ idx ] == self . hwm - 1 : self . update graph ( None )", "predictions": ["show a dependency object"], "references": ["handle an unmet dependency"], "bleu": 0.35930411196308426, "rouge_l": 0.25}
{"id": 6314, "code": "def logstate ( self ) : if self . logfile is None : print 'Logging has not been activated.' else : state = self . log active and 'active' or 'temporarily suspended' print 'Filename       :' , self . logfname print 'Mode           :' , self . logmode print 'Output logging :' , self . log output print 'Raw input log  :' , self . log raw input print 'Timestamping   :' , self . timestamp print 'State          :' , state", "predictions": ["print for the ip address"], "references": ["print a status message about the logger ."], "bleu": 0.1658165975077607, "rouge_l": 0.2953995157384988}
{"id": 6315, "code": "def log write ( self , data , kind = 'input' ) : if self . log active and data : write = self . logfile . write if kind == 'input' : if self . timestamp : write ( str to unicode ( time . strftime ( , time . localtime ( ) ) ) ) write ( data ) elif kind == 'output' and self . log output : odata = u'\\n' . join ( [ % s for s in data . splitlines ( ) ] ) write ( u'%s\\n' % odata ) self . logfile . flush ( )", "predictions": ["thread - safe write"], "references": ["write data to the log file if active"], "bleu": 0.13218059591958078, "rouge_l": 0.15721649484536082}
{"id": 6316, "code": "def new worksheet ( name = None , cells = None ) : ws = Notebook Node ( ) if name is not None : ws . name = unicode ( name ) if cells is None : ws . cells = [ ] else : ws . cells = list ( cells ) return ws", "predictions": ["create a with the current worksheet args args args args args args args args args ."], "references": ["create a worksheet by name with with a list of cells ."], "bleu": 0.1203921753741131, "rouge_l": 0.2932692307692307}
{"id": 6317, "code": "def new notebook ( metadata = None , worksheets = None ) : nb = Notebook Node ( ) nb . nbformat = 2 if worksheets is None : nb . worksheets = [ ] else : nb . worksheets = list ( worksheets ) if metadata is None : nb . metadata = new metadata ( ) else : nb . metadata = Notebook Node ( metadata ) return nb", "predictions": ["create a countdown notebook"], "references": ["create a notebook by name id and a list of worksheets ."], "bleu": 0.06876828939330318, "rouge_l": 0.34398496240601506}
{"id": 6318, "code": "def add s ( self , s , obj , priority = 0 ) : chain = self . strs . get ( s , Command Chain Dispatcher ( ) ) chain . add ( obj , priority ) self . strs [ s ] = chain", "predictions": ["cleanup a chain object"], "references": ["adds a target string for dispatching"], "bleu": 0.2179289600664314, "rouge_l": 0.1930379746835443}
{"id": 6319, "code": "def add re ( self , regex , obj , priority = 0 ) : chain = self . regexs . get ( regex , Command Chain Dispatcher ( ) ) chain . add ( obj , priority ) self . regexs [ regex ] = chain", "predictions": ["save a previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous previous"], "references": ["adds a target regexp for dispatching"], "bleu": 0.03901663112717908, "rouge_l": 0.061553985872855696}
{"id": 6320, "code": "def dispatch ( self , key ) : if key in self . strs : yield self . strs [ key ] for r , obj in self . regexs . items ( ) : if re . match ( r , key ) : yield obj else : pass", "predictions": ["sync the object with the given key . . . . ."], "references": ["get a seq of commandchain objects that match key"], "bleu": 0.10390302174233558, "rouge_l": 0.09775641025641024}
{"id": 6321, "code": "def flat matches ( self , key ) : for val in self . dispatch ( key ) : for el in val : yield el [ 1 ] return", "predictions": ["iterate over all after self ret ret ret ."], "references": ["yield all value targets without priority"], "bleu": 0.14113991930789777, "rouge_l": 0.13832199546485258}
{"id": 6322, "code": "def notebook dir changed ( self , name , old , new ) : if os . path . exists ( new ) and not os . path . isdir ( new ) : raise Trait Error ( \"notebook dir %r is not a directory\" % new ) if not os . path . exists ( new ) : self . log . info ( \"Creating notebook dir %s\" , new ) try : os . mkdir ( new ) except : raise Trait Error ( \"Couldn't create notebook dir %r\" % new )", "predictions": ["changes a add add add a add add add a new to the add"], "references": ["do a bit of validation of the notebook dir"], "bleu": 0.09782375748961449, "rouge_l": 0.18100890207715134}
{"id": 6323, "code": "def new notebook id ( self , name ) : #notebook id = unicode(uuid.uuid5(uuid.NAMESPACE URL, notebook id = unicode ( uuid . uuid4 ( ) ) self . mapping [ notebook id ] = name self . rev mapping [ name ] = notebook id return notebook id", "predictions": ["create a get id id id"], "references": ["generate a new notebook_id for a name and store its mappings ."], "bleu": 0.0812630644213965, "rouge_l": 0.10481099656357389}
{"id": 6324, "code": "def delete notebook id ( self , notebook id ) : name = self . mapping [ notebook id ] del self . mapping [ notebook id ] del self . rev mapping [ name ]", "predictions": ["deletes the record id id with the given id id = value"], "references": ["delete a notebook s id only . this doesn t delete the actual notebook ."], "bleu": 0.08955242946910898, "rouge_l": 0.14523809523809522}
{"id": 6325, "code": "def notebook exists ( self , notebook id ) : if notebook id not in self . mapping : return False path = self . get path by name ( self . mapping [ notebook id ] ) return os . path . isfile ( path )", "predictions": ["return is the get get the get directory { get }"], "references": ["does a notebook exist?"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 6326, "code": "def find path ( self , notebook id ) : try : name = self . mapping [ notebook id ] except Key Error : raise web . HTTP Error ( 404 , u'Notebook does not exist: %s' % notebook id ) return self . get path by name ( name )", "predictions": ["get the msgs by its = = name"], "references": ["return a full path to a notebook given its notebook_id ."], "bleu": 0.11021777041988566, "rouge_l": 0.10234899328859062}
{"id": 6327, "code": "def get path by name ( self , name ) : filename = name + self . filename ext path = os . path . join ( self . notebook dir , filename ) return path", "predictions": ["return up the notebook msg directory"], "references": ["return a full path to a notebook given its name ."], "bleu": 0.10624253482403696, "rouge_l": 0.2234432234432234}
{"id": 6328, "code": "def get notebook ( self , notebook id , format = u'json' ) : format = unicode ( format ) if format not in self . allowed formats : raise web . HTTP Error ( 415 , u'Invalid notebook format: %s' % format ) last modified , nb = self . get notebook object ( notebook id ) kwargs = { } if format == 'json' : kwargs [ 'split lines' ] = False data = current . writes ( nb , format , * * kwargs ) name = nb . metadata . get ( 'name' , 'notebook' ) return last modified , name , data", "predictions": ["return the update modified for the given notebook"], "references": ["get the representation of a notebook in format by notebook_id ."], "bleu": 0.12197601375336842, "rouge_l": 0.20469798657718125}
{"id": 6329, "code": "def get notebook object ( self , notebook id ) : path = self . find path ( notebook id ) if not os . path . isfile ( path ) : raise web . HTTP Error ( 404 , u'Notebook does not exist: %s' % notebook id ) info = os . stat ( path ) last modified = datetime . datetime . utcfromtimestamp ( info . st mtime ) with open ( path , 'r' ) as f : s = f . read ( ) try : nb = current . reads ( s , u'json' ) except : raise web . HTTP Error ( 500 , u'Unreadable JSON notebook.' ) nb . metadata . name = os . path . splitext ( os . path . basename ( path ) ) [ 0 ] return last modified , nb", "predictions": ["return the last completer completer completer"], "references": ["get the notebooknode representation of a notebook by notebook_id ."], "bleu": 0.11341174240707227, "rouge_l": 0.11960784313725491}
{"id": 6330, "code": "def save notebook ( self , notebook id , data , name = None , format = u'json' ) : if format not in self . allowed formats : raise web . HTTP Error ( 415 , u'Invalid notebook format: %s' % format ) try : nb = current . reads ( data . decode ( 'utf-8' ) , format ) except : raise web . HTTP Error ( 400 , u'Invalid JSON data' ) if name is not None : nb . metadata . name = name self . save notebook object ( notebook id , nb )", "predictions": ["cd the completer + name"], "references": ["save an existing notebook by notebook_id ."], "bleu": 0.15388864725803575, "rouge_l": 0.0}
{"id": 6331, "code": "def save notebook object ( self , notebook id , nb ) : if notebook id not in self . mapping : raise web . HTTP Error ( 404 , u'Notebook does not exist: %s' % notebook id ) old name = self . mapping [ notebook id ] try : new name = nb . metadata . name except Attribute Error : raise web . HTTP Error ( 400 , u'Missing notebook name' ) path = self . get path by name ( new name ) try : with open ( path , 'w' ) as f : current . write ( nb , f , u'json' ) except Exception as e : raise web . HTTP Error ( 400 , u'Unexpected error while saving notebook: %s' % e ) if self . save script : pypath = os . path . splitext ( path ) [ 0 ] + '.py' try : with io . open ( pypath , 'w' , encoding = 'utf-8' ) as f : current . write ( nb , f , u'py' ) except Exception as e : raise web . HTTP Error ( 400 , u'Unexpected error while saving notebook as script: %s' % e ) if old name != new name : old path = self . get path by name ( old name ) if os . path . isfile ( old path ) : os . unlink ( old path ) if self . save script : old pypath = os . path . splitext ( old path ) [ 0 ] + '.py' if os . path . isfile ( old pypath ) : os . unlink ( old pypath ) self . mapping [ notebook id ] = new name self . rev mapping [ new name ] = notebook id del self . rev mapping [ old name ]", "predictions": ["save the notebook self and save to the given id"], "references": ["save an existing notebook object by notebook_id ."], "bleu": 0.13950796967929133, "rouge_l": 0.22676579925650556}
{"id": 6332, "code": "def delete notebook ( self , notebook id ) : path = self . find path ( notebook id ) if not os . path . isfile ( path ) : raise web . HTTP Error ( 404 , u'Notebook does not exist: %s' % notebook id ) os . unlink ( path ) self . delete notebook id ( notebook id )", "predictions": ["configure a notebook notebook"], "references": ["delete notebook by notebook_id ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 6333, "code": "def new notebook ( self ) : path , name = self . increment filename ( 'Untitled' ) notebook id = self . new notebook id ( name ) metadata = current . new metadata ( name = name ) nb = current . new notebook ( metadata = metadata ) with open ( path , 'w' ) as f : current . write ( nb , f , u'json' ) return notebook id", "predictions": ["create a add notebook notebook"], "references": ["create a new notebook and return its notebook_id ."], "bleu": 0.17348474258688365, "rouge_l": 0.40757238307349664}
{"id": 6334, "code": "def copy notebook ( self , notebook id ) : last mod , nb = self . get notebook object ( notebook id ) name = nb . metadata . name + '-Copy' path , name = self . increment filename ( name ) nb . metadata . name = name notebook id = self . new notebook id ( name ) self . save notebook object ( notebook id , nb ) return notebook id", "predictions": ["add a notebook to the notebook"], "references": ["copy an existing notebook and return its notebook_id ."], "bleu": 0.1339801428338312, "rouge_l": 0.12869198312236285}
{"id": 6335, "code": "def make report ( self , traceback ) : sec sep = self . section sep report = [ super ( IP App Crash Handler , self ) . make report ( traceback ) ] rpt add = report . append try : rpt add ( sec sep + \"History of session input:\" ) for line in self . app . shell . user ns [ ' ih' ] : rpt add ( line ) rpt add ( '\\n*** Last line of input (may not be in above history):\\n' ) rpt add ( self . app . shell . last input line + '\\n' ) except : pass return '' . join ( report )", "predictions": ["create a report report report with the given test taken ."], "references": ["return a string containing a crash report ."], "bleu": 0.1354599427337814, "rouge_l": 0.32504440497335696}
{"id": 6336, "code": "def classes default ( self ) : return [ Interactive Shell App , self . class , Terminal Interactive Shell , Prompt Manager , History Manager , Profile Dir , Plain Text Formatter , IP Completer , Script Magics , ]", "predictions": ["the default classes for the receiver ."], "references": ["this has to be in a method for terminalipythonapp to be available ."], "bleu": 0.08723697147876523, "rouge_l": 0.1897356143079316}
{"id": 6337, "code": "def parse command line ( self , argv = None ) : argv = sys . argv [ 1 : ] if argv is None else argv if '-pylab' in argv : argv = argv [ : ] idx = argv . index ( '-pylab' ) warn . warn ( \"`-pylab` flag has been deprecated.\\n\" \"    Use `--pylab` instead, or `--pylab=foo` to specify a backend.\" ) sub = '--pylab' if len ( argv ) > idx + 1 : gui = argv [ idx + 1 ] if gui in ( 'wx' , 'qt' , 'qt4' , 'gtk' , 'auto' ) : sub = '--pylab=' + gui argv . pop ( idx + 1 ) argv [ idx ] = sub return super ( Terminal I Python App , self ) . parse command line ( argv )", "predictions": ["parse command line arguments ."], "references": ["override to allow old - pylab flag with deprecation warning"], "bleu": 0.08445588027797912, "rouge_l": 0.0}
{"id": 6338, "code": "def initialize ( self , argv = None ) : super ( Terminal I Python App , self ) . initialize ( argv ) if self . subapp is not None : return if not self . ignore old config : check for old config ( self . ipython dir ) if self . extra args and not self . something to run : self . file to run = self . extra args [ 0 ] self . init path ( ) self . init shell ( ) self . init banner ( ) self . init gui pylab ( ) self . init extensions ( ) self . init code ( )", "predictions": ["initialize the old config file ."], "references": ["do actions after construct but before starting the app ."], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 6339, "code": "def init shell ( self ) : self . shell = Terminal Interactive Shell . instance ( config = self . config , display banner = False , profile dir = self . profile dir , ipython dir = self . ipython dir ) self . shell . configurables . append ( self )", "predictions": ["initialize the shell instance ."], "references": ["initialize the interactiveshell instance"], "bleu": 0.3860973950960897, "rouge_l": 0.6802973977695167}
{"id": 6340, "code": "def init banner ( self ) : if self . display banner and self . interact : self . shell . show banner ( ) if self . log level <= logging . INFO : print", "predictions": ["init the banner for the shell ."], "references": ["optionally display the banner"], "bleu": 0.24446151121745047, "rouge_l": 0.3824451410658307}
{"id": 6341, "code": "def pylab changed ( self , name , old , new ) : if new == 'inline' : warn . warn ( \"'inline' not available as pylab backend, \" \"using 'auto' instead.\\n\" ) self . pylab = 'auto'", "predictions": ["changes the pylab of a pylab pylab ."], "references": ["replace -- pylab = inline with -- pylab = auto"], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 6342, "code": "def trait metadata ( self , traitname , key ) : try : trait = getattr ( self . class , traitname ) except Attribute Error : raise Trait Error ( \"Class %s does not have a trait named %s\" % ( self . class . name , traitname ) ) else : return trait . get metadata ( key )", "predictions": ["return the trait metadata for the given trait ."], "references": ["get metadata values for trait by key ."], "bleu": 0.17747405280050263, "rouge_l": 0.4756335282651072}
{"id": 6343, "code": "def validate ( self , obj , value ) : try : if issubclass ( value , self . klass ) : return value except : if ( value is None ) and ( self . allow none ) : return value self . error ( obj , value )", "predictions": ["check if value is a valid value ."], "references": ["validates that the value is a valid object instance ."], "bleu": 0.3638074228571147, "rouge_l": 0.5446428571428571}
{"id": 6344, "code": "def info ( self ) : if isinstance ( self . klass , basestring ) : klass = self . klass else : klass = self . klass . name result = 'a subclass of ' + klass if self . allow none : return result + ' or None' return result", "predictions": ["return the info for this class ."], "references": ["returns a description of the trait ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 6345, "code": "def info ( self ) : result = 'any of ' + repr ( self . values ) if self . allow none : return result + ' or None' return result", "predictions": ["return the info for this block ."], "references": ["returns a description of the trait ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 6346, "code": "def check ( self , completed , failed = None ) : if len ( self ) == 0 : return True against = set ( ) if self . success : against = completed if failed is not None and self . failure : against = against . union ( failed ) if self . all : return self . issubset ( against ) else : return not self . isdisjoint ( against )", "predictions": ["check if the completed value is in the proper against the completed against the completed against the completed against the completed ."], "references": ["check whether our dependencies have been met ."], "bleu": 0.0612957497932821, "rouge_l": 0.14558472553699284}
{"id": 6347, "code": "def unreachable ( self , completed , failed = None ) : if len ( self ) == 0 : return False against = set ( ) if not self . success : against = completed if failed is not None and not self . failure : against = against . union ( failed ) if self . all : return not self . isdisjoint ( against ) else : return self . issubset ( against )", "predictions": ["return true if completed is unreachable"], "references": ["return whether this dependency has become impossible ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 6348, "code": "def as dict ( self ) : return dict ( dependencies = list ( self ) , all = self . all , success = self . success , failure = self . failure )", "predictions": ["return a dict of the resource ."], "references": ["represent this dependency as a dict . for json compatibility ."], "bleu": 0.14834636222628117, "rouge_l": 0.32049036777583184}
{"id": 6349, "code": "def Ainv ( self ) : if not hasattr ( self , ' Ainv' ) : self . Ainv = self . Solver ( self . A ) return self . Ainv", "predictions": ["return the ainv object for this cursor ."], "references": ["returns a solver instance"], "bleu": 0.13485111859503685, "rouge_l": 0.0}
{"id": 6350, "code": "def Ainv ( self ) : if getattr ( self , ' Ainv' , None ) is None : self . Ainv = self . Solver ( self . A , 13 ) self . Ainv . run pardiso ( 12 ) return self . Ainv", "predictions": ["the ainv s ainv"], "references": ["returns a solver instance"], "bleu": 0.3021375397356768, "rouge_l": 0.0}
{"id": 6351, "code": "def depth ( n , tree ) : d = 0 parent = tree [ n ] while parent is not None : d += 1 parent = tree [ parent ] return d", "predictions": ["return the depth of a tree ."], "references": ["get depth of an element in the tree"], "bleu": 0.240785655451027, "rouge_l": 0.3952483801295896}
{"id": 6352, "code": "def print bintree ( tree , indent = '  ' ) : for n in sorted ( tree . keys ( ) ) : print \"%s%s\" % ( indent * depth ( n , tree ) , n )", "predictions": ["print the tree of the given tree ."], "references": ["print a binary tree"], "bleu": 0.17747405280050269, "rouge_l": 0.3546511627906977}
{"id": 6353, "code": "def disambiguate dns url ( url , location ) : if not ip pat . match ( location ) : location = socket . gethostbyname ( location ) return disambiguate url ( url , location )", "predictions": ["return the url for a url ."], "references": ["accept either ip address or dns name and return ip"], "bleu": 0.12100518276540287, "rouge_l": 0.11401869158878504}
{"id": 6354, "code": "def allreduce ( self , f , value , flat = True ) : return self . reduce ( f , value , flat = flat , all = True )", "predictions": ["wrap value in value to reduce it to a given value ."], "references": ["parallel reduce followed by broadcast of the result"], "bleu": 0.10390302174233558, "rouge_l": 0.10374149659863945}
{"id": 6355, "code": "def validate targets ( self , targets ) : if targets is None : return self . ids if isinstance ( targets , ( int , str , unicode ) ) : targets = [ targets ] targets = [ ] for t in targets : if isinstance ( t , ( str , unicode ) ) : t = self . by ident . get ( cast bytes ( t ) , t ) targets . append ( t ) targets = targets bad targets = [ t for t in targets if t not in self . ids ] if bad targets : raise Index Error ( \"No Such Engine: %r\" % bad targets ) if not targets : raise Index Error ( \"No Engines Registered\" ) return targets", "predictions": ["validate targets for targets ."], "references": ["turn any valid targets argument into a list of integer ids"], "bleu": 0.08222966016687185, "rouge_l": 0.11708253358925146}
{"id": 6356, "code": "def dispatch query ( self , msg ) : try : idents , msg = self . session . feed identities ( msg ) except Value Error : idents = [ ] if not idents : self . log . error ( \"Bad Query Message: %r\" , msg ) return client id = idents [ 0 ] try : msg = self . session . unserialize ( msg , content = True ) except Exception : content = error . wrap exception ( ) self . log . error ( \"Bad Query Message: %r\" , msg , exc info = True ) self . session . send ( self . query , \"hub error\" , ident = client id , content = content ) return #switch on message type: msg type = msg [ 'header' ] [ 'msg type' ] self . log . info ( \"client::client %r requested %r\" , client id , msg type ) handler = self . query handlers . get ( msg type , None ) try : assert handler is not None , \"Bad Message Type: %r\" % msg type except : content = error . wrap exception ( ) self . log . error ( \"Bad Message Type: %r\" , msg type , exc info = True ) self . session . send ( self . query , \"hub error\" , ident = client id , content = content ) return else : handler ( idents , msg )", "predictions": ["dispatch messages to the client ."], "references": ["route registration requests and queries from clients ."], "bleu": 0.15827883685397307, "rouge_l": 0.13926940639269406}
{"id": 6357, "code": "def save task request ( self , idents , msg ) : client id = idents [ 0 ] try : msg = self . session . unserialize ( msg ) except Exception : self . log . error ( \"task::client %r sent invalid task message: %r\" , client id , msg , exc info = True ) return record = init record ( msg ) record [ 'client uuid' ] = client id . decode ( 'ascii' ) record [ 'queue' ] = 'task' header = msg [ 'header' ] msg id = header [ 'msg id' ] self . pending . add ( msg id ) self . unassigned . add ( msg id ) try : existing = self . db . get record ( msg id ) if existing [ 'resubmitted' ] : for key in ( 'submitted' , 'client uuid' , 'buffers' ) : record . pop ( key ) for key , evalue in existing . iteritems ( ) : if key . endswith ( 'buffers' ) : continue rvalue = record . get ( key , None ) if evalue and rvalue and evalue != rvalue : self . log . warn ( \"conflicting initial state for record: %r:%r <%r> %r\" , msg id , rvalue , key , evalue ) elif evalue and not rvalue : record [ key ] = evalue try : self . db . update record ( msg id , record ) except Exception : self . log . error ( \"DB Error updating record %r\" , msg id , exc info = True ) except Key Error : try : self . db . add record ( msg id , record ) except Exception : self . log . error ( \"DB Error adding record %r\" , msg id , exc info = True ) except Exception : self . log . error ( \"DB Error saving task request %r\" , msg id , exc info = True )", "predictions": ["saves the task request to the db ."], "references": ["save the submission of a task ."], "bleu": 0.19070828081828378, "rouge_l": 0.4048672566371681}
{"id": 6358, "code": "def save task result ( self , idents , msg ) : client id = idents [ 0 ] try : msg = self . session . unserialize ( msg ) except Exception : self . log . error ( \"task::invalid task result message send to %r: %r\" , client id , msg , exc info = True ) return parent = msg [ 'parent header' ] if not parent : self . log . warn ( \"Task %r had no parent!\" , msg ) return msg id = parent [ 'msg id' ] if msg id in self . unassigned : self . unassigned . remove ( msg id ) header = msg [ 'header' ] engine uuid = header . get ( 'engine' , u'' ) eid = self . by ident . get ( cast bytes ( engine uuid ) , None ) status = header . get ( 'status' , None ) if msg id in self . pending : self . log . info ( \"task::task %r finished on %s\" , msg id , eid ) self . pending . remove ( msg id ) self . all completed . add ( msg id ) if eid is not None : if status != 'aborted' : self . completed [ eid ] . append ( msg id ) if msg id in self . tasks [ eid ] : self . tasks [ eid ] . remove ( msg id ) completed = header [ 'date' ] started = header . get ( 'started' , None ) result = { 'result header' : header , 'result content' : msg [ 'content' ] , 'started' : started , 'completed' : completed , 'received' : datetime . now ( ) , 'engine uuid' : engine uuid , } result [ 'result buffers' ] = msg [ 'buffers' ] try : self . db . update record ( msg id , result ) except Exception : self . log . error ( \"DB Error saving task request %r\" , msg id , exc info = True ) else : self . log . debug ( \"task::unknown task %r finished\" , msg id )", "predictions": ["saves the task result in the completed client ."], "references": ["save the result of a completed task ."], "bleu": 0.18575057999133596, "rouge_l": 0.4756335282651072}
{"id": 6359, "code": "def save iopub message ( self , topics , msg ) : try : msg = self . session . unserialize ( msg , content = True ) except Exception : self . log . error ( \"iopub::invalid IO Pub message\" , exc info = True ) return parent = msg [ 'parent header' ] if not parent : self . log . warn ( \"iopub::IO Pub message lacks parent: %r\" , msg ) return msg id = parent [ 'msg id' ] msg type = msg [ 'header' ] [ 'msg type' ] content = msg [ 'content' ] try : rec = self . db . get record ( msg id ) except Key Error : rec = empty record ( ) rec [ 'msg id' ] = msg id self . db . add record ( msg id , rec ) d = { } if msg type == 'stream' : name = content [ 'name' ] s = rec [ name ] or '' d [ name ] = s + content [ 'data' ] elif msg type == 'pyerr' : d [ 'pyerr' ] = content elif msg type == 'pyin' : d [ 'pyin' ] = content [ 'code' ] elif msg type in ( 'display data' , 'pyout' ) : d [ msg type ] = content elif msg type == 'status' : pass else : self . log . warn ( \"unhandled iopub msg type: %r\" , msg type ) if not d : return try : self . db . update record ( msg id , d ) except Exception : self . log . error ( \"DB Error saving iopub message %r\" , msg id , exc info = True )", "predictions": ["saves the iopub message to the db ."], "references": ["save an iopub message into the db"], "bleu": 0.2653856085536222, "rouge_l": 0.5398230088495575}
{"id": 6360, "code": "def connection request ( self , client id , msg ) : self . log . info ( \"client::client %r connected\" , client id ) content = dict ( status = 'ok' ) content . update ( self . client info ) jsonable = { } for k , v in self . keytable . iteritems ( ) : if v not in self . dead engines : jsonable [ str ( k ) ] = v . decode ( 'ascii' ) content [ 'engines' ] = jsonable self . session . send ( self . query , 'connection reply' , content , parent = msg , ident = client id )", "predictions": ["send a connection to the broker ."], "references": ["reply with connection addresses for clients ."], "bleu": 0.20556680845025982, "rouge_l": 0.2857142857142857}
{"id": 6361, "code": "def register engine ( self , reg , msg ) : content = msg [ 'content' ] try : queue = cast bytes ( content [ 'queue' ] ) except Key Error : self . log . error ( \"registration::queue not specified\" , exc info = True ) return heart = content . get ( 'heartbeat' , None ) if heart : heart = cast bytes ( heart ) \"\"\"register a new engine, and create the socket(s) necessary\"\"\" eid = self . next id self . log . debug ( \"registration::register engine(%i, %r, %r, %r)\" , eid , queue , reg , heart ) content = dict ( id = eid , status = 'ok' ) content . update ( self . engine info ) if queue in self . by ident : try : raise Key Error ( \"queue id %r in use\" % queue ) except : content = error . wrap exception ( ) self . log . error ( \"queue id %r in use\" , queue , exc info = True ) elif heart in self . hearts : try : raise Key Error ( \"heart id %r in use\" % heart ) except : self . log . error ( \"heart id %r in use\" , heart , exc info = True ) content = error . wrap exception ( ) else : for h , pack in self . incoming registrations . iteritems ( ) : if heart == h : try : raise Key Error ( \"heart id %r in use\" % heart ) except : self . log . error ( \"heart id %r in use\" , heart , exc info = True ) content = error . wrap exception ( ) break elif queue == pack [ 1 ] : try : raise Key Error ( \"queue id %r in use\" % queue ) except : self . log . error ( \"queue id %r in use\" , queue , exc info = True ) content = error . wrap exception ( ) break msg = self . session . send ( self . query , \"registration reply\" , content = content , ident = reg ) if content [ 'status' ] == 'ok' : if heart in self . heartmonitor . hearts : self . incoming registrations [ heart ] = ( eid , queue , reg [ 0 ] , None ) self . finish registration ( heart ) else : purge = lambda : self . purge stalled registration ( heart ) dc = ioloop . Delayed Callback ( purge , self . registration timeout , self . loop ) dc . start ( ) self . incoming registrations [ heart ] = ( eid , queue , reg [ 0 ] , dc ) else : self . log . error ( \"registration::registration %i failed: %r\" , eid , content [ 'evalue' ] ) return eid", "predictions": ["register a new engine ."], "references": ["register a new engine ."], "bleu": 1.0, "rouge_l": 1.0}
{"id": 6362, "code": "def unregister engine ( self , ident , msg ) : try : eid = msg [ 'content' ] [ 'id' ] except : self . log . error ( \"registration::bad engine id for unregistration: %r\" , ident , exc info = True ) return self . log . info ( \"registration::unregister engine(%r)\" , eid ) uuid = self . keytable [ eid ] content = dict ( id = eid , queue = uuid . decode ( 'ascii' ) ) self . dead engines . add ( uuid ) # handleit = lambda : self . handle stranded msgs ( eid , uuid ) dc = ioloop . Delayed Callback ( handleit , self . registration timeout , self . loop ) dc . start ( ) if self . notifier : self . session . send ( self . notifier , \"unregistration notification\" , content = content )", "predictions": ["unregister a engine id ."], "references": ["unregister an engine that explicitly requested to leave ."], "bleu": 0.1458826981425239, "rouge_l": 0.40757238307349664}
{"id": 6363, "code": "def shutdown request ( self , client id , msg ) : self . session . send ( self . query , 'shutdown reply' , content = { 'status' : 'ok' } , ident = client id ) self . session . send ( self . notifier , 'shutdown notice' , content = { 'status' : 'ok' } ) dc = ioloop . Delayed Callback ( lambda : self . shutdown ( ) , 1000 , self . loop ) dc . start ( )", "predictions": ["shutdown the request ."], "references": ["handle shutdown request ."], "bleu": 0.5081327481546147, "rouge_l": 0.75}
{"id": 6364, "code": "def resubmit task ( self , client id , msg ) : def finish ( reply ) : self . session . send ( self . query , 'resubmit reply' , content = reply , ident = client id ) content = msg [ 'content' ] msg ids = content [ 'msg ids' ] reply = dict ( status = 'ok' ) try : records = self . db . find records ( { 'msg id' : { '$in' : msg ids } } , keys = [ 'header' , 'content' , 'buffers' ] ) except Exception : self . log . error ( 'db::db error finding tasks to resubmit' , exc info = True ) return finish ( error . wrap exception ( ) ) found ids = [ rec [ 'msg id' ] for rec in records ] pending ids = [ msg id for msg id in found ids if msg id in self . pending ] if len ( records ) > len ( msg ids ) : try : raise Runtime Error ( \"DB appears to be in an inconsistent state.\" \"More matching records were found than should exist\" ) except Exception : return finish ( error . wrap exception ( ) ) elif len ( records ) < len ( msg ids ) : missing = [ m for m in msg ids if m not in found ids ] try : raise Key Error ( \"No such msg(s): %r\" % missing ) except Key Error : return finish ( error . wrap exception ( ) ) elif pending ids : pass resubmitted = { } for rec in records : header = rec [ 'header' ] msg = self . session . msg ( header [ 'msg type' ] , parent = header ) msg id = msg [ 'msg id' ] msg [ 'content' ] = rec [ 'content' ] fresh = msg [ 'header' ] header [ 'msg id' ] = fresh [ 'msg id' ] header [ 'date' ] = fresh [ 'date' ] msg [ 'header' ] = header self . session . send ( self . resubmit , msg , buffers = rec [ 'buffers' ] ) resubmitted [ rec [ 'msg id' ] ] = msg id self . pending . add ( msg id ) msg [ 'buffers' ] = rec [ 'buffers' ] try : self . db . add record ( msg id , init record ( msg ) ) except Exception : self . log . error ( \"db::DB Error updating record: %s\" , msg id , exc info = True ) finish ( dict ( status = 'ok' , resubmitted = resubmitted ) ) for msg id , resubmit id in resubmitted . iteritems ( ) : try : self . db . update record ( msg id , { 'resubmitted' : resubmit id } ) except Exception : self . log . error ( \"db::DB Error updating record: %s\" , msg id , exc info = True )", "predictions": ["re - process the task with the given id ."], "references": ["resubmit one or more tasks ."], "bleu": 0.12605968092174913, "rouge_l": 0.13090128755364808}
{"id": 6365, "code": "def extract record ( self , rec ) : io dict = { } for key in ( 'pyin' , 'pyout' , 'pyerr' , 'stdout' , 'stderr' ) : io dict [ key ] = rec [ key ] content = { 'result content' : rec [ 'result content' ] , 'header' : rec [ 'header' ] , 'result header' : rec [ 'result header' ] , 'received' : rec [ 'received' ] , 'io' : io dict , } if rec [ 'result buffers' ] : buffers = map ( bytes , rec [ 'result buffers' ] ) else : buffers = [ ] return content , buffers", "predictions": ["extract the buffers from a dict"], "references": ["decompose a taskrecord dict into subsection of reply for get_result"], "bleu": 0.1255107248036171, "rouge_l": 0.23921568627450981}
{"id": 6366, "code": "def get results ( self , client id , msg ) : content = msg [ 'content' ] msg ids = sorted ( set ( content [ 'msg ids' ] ) ) statusonly = content . get ( 'status only' , False ) pending = [ ] completed = [ ] content = dict ( status = 'ok' ) content [ 'pending' ] = pending content [ 'completed' ] = completed buffers = [ ] if not statusonly : try : matches = self . db . find records ( dict ( msg id = { '$in' : msg ids } ) ) records = { } for rec in matches : records [ rec [ 'msg id' ] ] = rec except Exception : content = error . wrap exception ( ) self . session . send ( self . query , \"result reply\" , content = content , parent = msg , ident = client id ) return else : records = { } for msg id in msg ids : if msg id in self . pending : pending . append ( msg id ) elif msg id in self . all completed : completed . append ( msg id ) if not statusonly : c , bufs = self . extract record ( records [ msg id ] ) content [ msg id ] = c buffers . extend ( bufs ) elif msg id in records : if rec [ 'completed' ] : completed . append ( msg id ) c , bufs = self . extract record ( records [ msg id ] ) content [ msg id ] = c buffers . extend ( bufs ) else : pending . append ( msg id ) else : try : raise Key Error ( 'No such message: ' + msg id ) except : content = error . wrap exception ( ) break self . session . send ( self . query , \"result reply\" , content = content , parent = msg , ident = client id , buffers = buffers )", "predictions": ["get results from client ."], "references": ["get the result of 1 or more messages ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
{"id": 6367, "code": "def get history ( self , client id , msg ) : try : msg ids = self . db . get history ( ) except Exception as e : content = error . wrap exception ( ) else : content = dict ( status = 'ok' , history = msg ids ) self . session . send ( self . query , \"history reply\" , content = content , parent = msg , ident = client id )", "predictions": ["get the history of a client ."], "references": ["get a list of all msg_ids in our db records"], "bleu": 0.14390022429682173, "rouge_l": 0.22803738317757008}
{"id": 6368, "code": "def db query ( self , client id , msg ) : content = msg [ 'content' ] query = content . get ( 'query' , { } ) keys = content . get ( 'keys' , None ) buffers = [ ] empty = list ( ) try : records = self . db . find records ( query , keys ) except Exception as e : content = error . wrap exception ( ) else : if keys is not None : buffer lens = [ ] if 'buffers' in keys else None result buffer lens = [ ] if 'result buffers' in keys else None else : buffer lens = None result buffer lens = None for rec in records : b = rec . pop ( 'buffers' , empty ) or empty if buffer lens is not None : buffer lens . append ( len ( b ) ) buffers . extend ( b ) rb = rec . pop ( 'result buffers' , empty ) or empty if result buffer lens is not None : result buffer lens . append ( len ( rb ) ) buffers . extend ( rb ) content = dict ( status = 'ok' , records = records , buffer lens = buffer lens , result buffer lens = result buffer lens ) self . session . send ( self . query , \"db reply\" , content = content , parent = msg , ident = client id , buffers = buffers )", "predictions": ["handle an incoming default default default default default default default default default default default default default default default default default default default default default default default default default default default default"], "references": ["perform a raw query on the task record database ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 6369, "code": "def cd ( self , newdir ) : prevdir = os . getcwd ( ) os . chdir ( newdir ) try : yield finally : os . chdir ( prevdir )", "predictions": ["temporarily temporarily parse the context . ."], "references": ["go to the path"], "bleu": 0.18575057999133598, "rouge_l": 0.19122257053291536}
{"id": 6370, "code": "def decode cmd out ( self , completed cmd ) : try : stdout = completed cmd . stdout . encode ( 'utf-8' ) . decode ( ) except Attribute Error : try : stdout = str ( bytes ( completed cmd . stdout ) , 'big5' ) . strip ( ) except Attribute Error : stdout = str ( bytes ( completed cmd . stdout ) . decode ( 'utf-8' ) ) . strip ( ) try : stderr = completed cmd . stderr . encode ( 'utf-8' ) . decode ( ) except Attribute Error : try : stderr = str ( bytes ( completed cmd . stderr ) , 'big5' ) . strip ( ) except Attribute Error : stderr = str ( bytes ( completed cmd . stderr ) . decode ( 'utf-8' ) ) . strip ( ) return Parsed Completed Command ( completed cmd . returncode , completed cmd . args , stdout , stderr )", "predictions": ["initialize the completed from the completed . . . . ."], "references": ["return a standard message"], "bleu": 0.09578464408619825, "rouge_l": 0.0}
{"id": 6371, "code": "def run command under r root ( self , cmd , catched = True ) : RPATH = self . path with self . cd ( newdir = RPATH ) : if catched : process = sp . run ( cmd , stdout = sp . PIPE , stderr = sp . PIPE ) else : process = sp . run ( cmd ) return process", "predictions": ["init shell shell command under"], "references": ["subprocess run on here"], "bleu": 0.2295748846661433, "rouge_l": 0.0}
{"id": 6372, "code": "def get installed version ( name ) : pattern = re . compile ( r'''Installed:\\s+(?P<version>.*)''' ) cmd = 'apt-cache policy %s' % name args = shlex . split ( cmd ) try : output = subprocess . check output ( args ) if not output : return None except Called Process Error : return None match = pattern . search ( output ) if match : version = match . groupdict ( ) [ 'version' ] if version == '(none)' : return None else : return version", "predictions": ["init the banner version"], "references": ["returns installed package version and none if package is not installed"], "bleu": 0.06243769243378541, "rouge_l": 0.12298387096774194}
{"id": 6373, "code": "def squash unicode ( obj ) : if isinstance ( obj , dict ) : for key in obj . keys ( ) : obj [ key ] = squash unicode ( obj [ key ] ) if isinstance ( key , unicode ) : obj [ squash unicode ( key ) ] = obj . pop ( key ) elif isinstance ( obj , list ) : for i , v in enumerate ( obj ) : obj [ i ] = squash unicode ( v ) elif isinstance ( obj , unicode ) : obj = obj . encode ( 'utf8' ) return obj", "predictions": ["recursively convert strings to changed"], "references": ["coerce unicode back to bytestrings ."], "bleu": 0.22352339099197038, "rouge_l": 0.1788856304985337}
{"id": 6374, "code": "def extract header ( msg or header ) : if not msg or header : return { } try : h = msg or header [ 'header' ] except Key Error : try : h = msg or header [ 'msg id' ] except Key Error : raise else : h = msg or header if not isinstance ( h , dict ) : h = dict ( h ) return h", "predictions": ["trait for message metadata"], "references": ["given a message or header return the header ."], "bleu": 0.10294235160901445, "rouge_l": 0.14386792452830188}
{"id": 6375, "code": "def check packers ( self ) : pack = self . pack unpack = self . unpack msg = dict ( a = [ 1 , 'hi' ] ) try : packed = pack ( msg ) except Exception : raise Value Error ( \"packer could not serialize a simple message\" ) if not isinstance ( packed , bytes ) : raise Value Error ( \"message packed to %r, but bytes are required\" % type ( packed ) ) try : unpacked = unpack ( packed ) except Exception : raise Value Error ( \"unpacker could not handle the packer's output\" ) msg = dict ( t = datetime . now ( ) ) try : unpacked = unpack ( pack ( msg ) ) except Exception : self . pack = lambda o : pack ( squash dates ( o ) ) self . unpack = lambda s : extract dates ( unpack ( s ) )", "predictions": ["validate the packers ."], "references": ["check packers for binary data and datetime support ."], "bleu": 0.11392443929712959, "rouge_l": 0.28773584905660377}
{"id": 6376, "code": "def object info ( * * kw ) : infodict = dict ( izip longest ( info fields , [ None ] ) ) infodict . update ( kw ) return infodict", "predictions": ["+ + print information about an info field"], "references": ["make an object info dict with all fields present ."], "bleu": 0.13821693129588736, "rouge_l": 0.21785714285714283}
{"id": 6377, "code": "def head ( self , h ) : return '%s%s%s' % ( self . color table . active colors . header , h , self . color table . active colors . normal )", "predictions": ["return info from the repr repr + h + the repr + h + h + h + h + none + h + h + 1 + h"], "references": ["return a header string with proper colors ."], "bleu": 0.04175872565419194, "rouge_l": 0.06021717670286277}
{"id": 6378, "code": "def noinfo ( self , msg , oname ) : print 'No %s found' % msg , if oname : print 'for %s' % oname else : print", "predictions": ["prints a message to stdout"], "references": ["generic message when no information is found ."], "bleu": 0.14983220973977976, "rouge_l": 0.1476997578692494}
{"id": 6379, "code": "def psource ( self , obj , oname = '' ) : linecache . checkcache ( ) try : src = getsource ( obj ) except : self . noinfo ( 'source' , oname ) else : page . page ( self . format ( py3compat . unicode to str ( src ) ) )", "predictions": ["serialize an object to the given object len len len len len len len len len len len len len len len len len len len len len len len len"], "references": ["print the source code for an object ."], "bleu": 0.055177848898164926, "rouge_l": 0.1147695202257761}
{"id": 6380, "code": "def pfile ( self , obj , oname = '' ) : lineno = find source lines ( obj ) if lineno is None : self . noinfo ( 'file' , oname ) return ofile = find file ( obj ) if ofile . endswith ( ( '.so' , '.dll' , '.pyd' ) ) : print 'File %r is binary, not printing.' % ofile elif not os . path . isfile ( ofile ) : print 'File %r does not exist, not printing.' % ofile else : page . page ( self . format ( open ( ofile ) . read ( ) ) , lineno - 1 )", "predictions": ["all . noinfo success success"], "references": ["show the whole file where an object was defined ."], "bleu": 0.10043553373039076, "rouge_l": 0.12577319587628866}
{"id": 6381, "code": "def print figure ( fig , fmt = 'png' ) : if not fig . axes and not fig . lines : return fc = fig . get facecolor ( ) ec = fig . get edgecolor ( ) fig . set facecolor ( 'white' ) fig . set edgecolor ( 'white' ) try : bytes io = Bytes IO ( ) fig . canvas . print figure ( bytes io , format = fmt , bbox inches = 'tight' ) data = bytes io . getvalue ( ) finally : fig . set facecolor ( fc ) fig . set edgecolor ( ec ) return data", "predictions": ["print figure instance of figure = 0 = 1 = 0 = 1 = 1 = 0 = 1 = 1 = 0 = 1 = 1 = 1 = 1"], "references": ["convert a figure to svg or png for inline display ."], "bleu": 0.03901663112717908, "rouge_l": 0.05209222886421862}
{"id": 6382, "code": "def activate matplotlib ( backend ) : import matplotlib if backend . startswith ( 'module://' ) : matplotlib . rc Params [ 'backend' ] = backend else : matplotlib . use ( backend ) matplotlib . interactive ( True ) import matplotlib . pylab as pylab #import matplotlib.pyplot #matplotlib.pyplot.switch backend(backend) pylab . show . needmain = False pylab . draw if interactive = flag calls ( pylab . draw if interactive )", "predictions": ["activate matplotlib to activate"], "references": ["activate the given backend and set interactive to true ."], "bleu": 0.08872444253557525, "rouge_l": 0.26521739130434785}
{"id": 6383, "code": "def import pylab ( user ns , import all = True ) : s = ( \"import numpy\\n\" \"import matplotlib\\n\" \"from matplotlib import pylab, mlab, pyplot\\n\" \"np = numpy\\n\" \"plt = pyplot\\n\" ) exec s in user ns if import all : s = ( \"from matplotlib.pylab import *\\n\" \"from numpy import *\\n\" ) exec s in user ns", "predictions": ["depth depth and depth"], "references": ["import the standard pylab symbols into user_ns ."], "bleu": 0.11115018927487523, "rouge_l": 0.0}
{"id": 6384, "code": "def trace ( self , frame , event , arg unused ) : if self . stopped : return if 0 : sys . stderr . write ( \"trace event: %s %r @%d\\n\" % ( event , frame . f code . co filename , frame . f lineno ) ) if self . last exc back : if frame == self . last exc back : if self . arcs and self . cur file data : pair = ( self . last line , - self . last exc firstlineno ) self . cur file data [ pair ] = None self . cur file data , self . last line = self . data stack . pop ( ) self . last exc back = None if event == 'call' : self . data stack . append ( ( self . cur file data , self . last line ) ) filename = frame . f code . co filename if filename not in self . should trace cache : tracename = self . should trace ( filename , frame ) self . should trace cache [ filename ] = tracename else : tracename = self . should trace cache [ filename ] #print(\"called, stack is %d deep, tracename is %r\" % ( if tracename : if tracename not in self . data : self . data [ tracename ] = { } self . cur file data = self . data [ tracename ] else : self . cur file data = None self . last line = - 1 elif event == 'line' : if self . cur file data is not None : if self . arcs : #print(\"lin\", self.last line, frame.f lineno) self . cur file data [ ( self . last line , frame . f lineno ) ] = None else : #print(\"lin\", frame.f lineno) self . cur file data [ frame . f lineno ] = None self . last line = frame . f lineno elif event == 'return' : if self . arcs and self . cur file data : first = frame . f code . co firstlineno self . cur file data [ ( self . last line , - first ) ] = None self . cur file data , self . last line = self . data stack . pop ( ) #print(\"returned, stack is %d deep\" % (len(self.data stack))) elif event == 'exception' : #print(\"exc\", self.last line, frame.f lineno) self . last exc back = frame . f back self . last exc firstlineno = frame . f code . co firstlineno return self . trace", "predictions": ["print a frame object"], "references": ["the trace function passed to sys . settrace ."], "bleu": 0.08656385444580769, "rouge_l": 0.0}
{"id": 6385, "code": "def stop ( self ) : self . stopped = True if self . thread != threading . current Thread ( ) : return if hasattr ( sys , \"gettrace\" ) and self . warn : if sys . gettrace ( ) != self . trace : msg = \"Trace function changed, measurement is likely wrong: %r\" self . warn ( msg % ( sys . gettrace ( ) , ) ) #print(\"Stopping tracer on %s\" % threading.current thread().ident) sys . settrace ( None )", "predictions": ["disambiguate the function ip address location location location location location location location location location location location location location location location location location location location location location location location location location location"], "references": ["stop this tracer ."], "bleu": 0.03280894525012139, "rouge_l": 0.0}
{"id": 6386, "code": "def start tracer ( self ) : tracer = self . trace class ( ) tracer . data = self . data tracer . arcs = self . branch tracer . should trace = self . should trace tracer . should trace cache = self . should trace cache tracer . warn = self . warn fn = tracer . start ( ) self . tracers . append ( tracer ) return fn", "predictions": ["start up the tracer instance = false if not already there"], "references": ["start a new tracer object and store it in self . tracers ."], "bleu": 0.10510262682013449, "rouge_l": 0.1641991924629879}
{"id": 6387, "code": "def installation trace ( self , frame unused , event unused , arg unused ) : sys . settrace ( None ) fn = self . start tracer ( ) if fn : fn = fn ( frame unused , event unused , arg unused ) return fn", "predictions": ["targets validate with validate and targets ids ids ids ids ids ids ids ids ids ids ids ids ."], "references": ["called on new threads installs the real tracer ."], "bleu": 0.06439931429457924, "rouge_l": 0.07634543178973717}
{"id": 6388, "code": "def start ( self ) : if self . collectors : self . collectors [ - 1 ] . pause ( ) self . collectors . append ( self ) #print(\"Started: %r\" % self. collectors, file=sys.stderr) traces0 = [ ] if hasattr ( sys , \"gettrace\" ) : fn0 = sys . gettrace ( ) if fn0 : tracer0 = getattr ( fn0 , ' self ' , None ) if tracer0 : traces0 = getattr ( tracer0 , 'traces' , [ ] ) fn = self . start tracer ( ) for args in traces0 : ( frame , event , arg ) , lineno = args try : fn ( frame , event , arg , lineno = lineno ) except Type Error : raise Exception ( \"fullcoverage must be run with the C trace function.\" ) threading . settrace ( self . installation trace )", "predictions": ["dispatch to messages with"], "references": ["start collecting trace information ."], "bleu": 0.23530495254141282, "rouge_l": 0.0}
{"id": 6389, "code": "def stop ( self ) : #print >>sys.stderr, \"Stopping: %r\" % self. collectors assert self . collectors assert self . collectors [ - 1 ] is self self . pause ( ) self . tracers = [ ] self . collectors . pop ( ) if self . collectors : self . collectors [ - 1 ] . resume ( )", "predictions": ["save the timer ."], "references": ["stop collecting trace information ."], "bleu": 0.2798263237576258, "rouge_l": 0.21785714285714283}
{"id": 6390, "code": "def pause ( self ) : for tracer in self . tracers : tracer . stop ( ) stats = tracer . get stats ( ) if stats : print ( \"\\n Coverage.py tracer stats:\" ) for k in sorted ( stats . keys ( ) ) : print ( \"%16s: %s\" % ( k , stats [ k ] ) ) threading . settrace ( None )", "predictions": ["save all the active database . . . . . ."], "references": ["pause tracing but be prepared to resume ."], "bleu": 0.11390778025531027, "rouge_l": 0.108348134991119}
{"id": 6391, "code": "def resume ( self ) : for tracer in self . tracers : tracer . start ( ) threading . settrace ( self . installation trace )", "predictions": ["save all session = paused ."], "references": ["resume tracing after a pause ."], "bleu": 0.22089591134157885, "rouge_l": 0.16666666666666666}
{"id": 6392, "code": "def new code cell ( code = None , prompt number = None ) : cell = Notebook Node ( ) cell . cell type = u'code' if code is not None : cell . code = unicode ( code ) if prompt number is not None : cell . prompt number = int ( prompt number ) return cell", "predictions": ["create a connection cell cell cell cell"], "references": ["create a new code cell with input and output"], "bleu": 0.19740631366145517, "rouge_l": 0.3667334669338677}
{"id": 6393, "code": "def new text cell ( text = None ) : cell = Notebook Node ( ) if text is not None : cell . text = unicode ( text ) cell . cell type = u'text' return cell", "predictions": ["create a register cell cell cell"], "references": ["create a new text cell ."], "bleu": 0.31239399369202553, "rouge_l": 0.5}
{"id": 6394, "code": "def new notebook ( cells = None ) : nb = Notebook Node ( ) if cells is not None : nb . cells = cells else : nb . cells = [ ] return nb", "predictions": ["create a unregister engine . . . . . . . . . . . . ."], "references": ["create a notebook by name id and a list of worksheets ."], "bleu": 0.10216198665886358, "rouge_l": 0.21353558926487748}
{"id": 6395, "code": "def render traceback ( self , excid = None ) : lines = [ ] if excid is None : for ( en , ev , etb , ei ) in self . elist : lines . append ( self . get engine str ( ei ) ) lines . extend ( ( etb or 'No traceback available' ) . splitlines ( ) ) lines . append ( '' ) else : try : en , ev , etb , ei = self . elist [ excid ] except : raise Index Error ( \"an exception with index %i does not exist\" % excid ) else : lines . append ( self . get engine str ( ei ) ) lines . extend ( ( etb or 'No traceback available' ) . splitlines ( ) ) return lines", "predictions": ["shutdown the request with the given client } } } } } } }"], "references": ["render one or all of my tracebacks to a list of lines"], "bleu": 0.07432998184513635, "rouge_l": 0.0}
{"id": 6396, "code": "def canonical dir ( self , morf ) : return os . path . split ( Code Unit ( morf , self . file locator ) . filename ) [ 0 ]", "predictions": ["return the canonical directory of the canonical"], "references": ["return the canonical directory of the module or file morf ."], "bleu": 0.47486944442513457, "rouge_l": 0.6409807355516637}
{"id": 6397, "code": "def source for file ( self , filename ) : if not filename . endswith ( \".py\" ) : if filename [ - 4 : - 1 ] == \".py\" : filename = filename [ : - 1 ] elif filename . endswith ( \"$py.class\" ) : filename = filename [ : - 9 ] + \".py\" return filename", "predictions": ["return the filename record for a file"], "references": ["return the source file for filename ."], "bleu": 0.29071536848410967, "rouge_l": 0.42857142857142855}
{"id": 6398, "code": "def warn ( self , msg ) : self . warnings . append ( msg ) sys . stderr . write ( \"Coverage.py warning: %s\\n\" % msg )", "predictions": ["get message to = = = = = = = = = = = = = = = = 1 msg"], "references": ["use msg as a warning ."], "bleu": 0.05809665204409193, "rouge_l": 0.08232118758434548}
{"id": 6399, "code": "def check for packages ( self ) : if self . source pkgs : found = [ ] for pkg in self . source pkgs : try : mod = sys . modules [ pkg ] except Key Error : continue found . append ( pkg ) try : pkg file = mod . file except Attribute Error : pkg file = None else : d , f = os . path . split ( pkg file ) if f . startswith ( ' init ' ) : pkg file = d else : pkg file = self . source for file ( pkg file ) pkg file = self . file locator . canonical filename ( pkg file ) if not os . path . exists ( pkg file ) : pkg file = None if pkg file : self . source . append ( pkg file ) self . source match . add ( pkg file ) else : self . warn ( \"Module %s has no Python source.\" % pkg ) for pkg in found : self . source pkgs . remove ( pkg )", "predictions": ["get package packages history ."], "references": ["update the source_match matcher with latest imported packages ."], "bleu": 0.13575914775035755, "rouge_l": 0.2717149220489978}
